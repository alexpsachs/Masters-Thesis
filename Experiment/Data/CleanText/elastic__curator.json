{
    "untergeek": "This issue is closed now.\n. Guys, while what you are asking for is possible it is not recommended.  Elasticsearch is not a file-system where records are like files.  Documents (or in this case, log entries) are stored in a complex, Lucene inverted index.  It's a very expensive operation to run deletes on a well-utilized ES cluster.\nFor example, imagine the disk I/O affecting a traditional RDBMS if you were to have 400,000,000 records in a database and you ran a query like:\nDELETE from DB where tag='my_tag' and type='my_type'\nThis is an extremely simple SQL statement, but the idea is that it's a full-table scan to create a list of documents to delete.  Now imagine that query matches 20% of the total number of records \u2014 that's the equivalent of feeding 80,000,000 DELETE statements to SQL.  It becomes an extremely expensive operation just finding the records for delete, then you have to delete them, and wait for transaction logging, commits, writes. etc.  What happens to disk I/O for your current indexing operations while you're running this operation?\nIf that were all that were at issue, that would be a fairly quantifiable amount of pain. However, you are running this sort of query against Elasticsearch and it is not an RDBMS.  First, off, deletes are not deletes in Elasticsearch, or at least not immediately.  A delete operation only marks/tags a document (in this case a log entry) for deletion.  Only on segment merges will delete operations be \"committed.\"  The default settings merge segments after every 5000 operations, or 200mb of data, or 30 minutes, whichever comes first.  Even after the delete is \"completed\" you'd have to wait and tell Elasticsearch to run the segment merge and/or optimize again (which is another HUGE pain point - If you've optimized the index running a delete by query is a bad idea) to make sure that the delete actually completed. This would also cause I/O delays if you were running this operation and another index was trying to index a live data stream.\nTo continue our RDBMS analogy, if I have a subset of data to eliminate, it's so much easier to run: DROP TABLE\nThe operation is nearly instantaneous and does not impact the system the same way.  The script provided here is the Elasticsearch equivalent of a \"DROP TABLE\" statement.  It is intended to delete an entire index.  This is the preferred way to eliminate old logstash indexes for similar reasons.\nIn this vein of thought it is also preferred that you separate dev and prod data into separate indexes, if that is a problem.  It is trivial to add a tag and/or a conditional in the logstash output block to send to a different index name (e.g. index => logstash-%{env}-%{+YYYY.MM.dd}\"), based on those tags.  In that way you could index to the same cluster, but with a different index name.  When you didn't want your dev logs any more, you could \"drop\" the index with this script as-is.\nBecause of all this there is a hazard in merging this feature and offering it to the logstash community as a \"safe to use\" product.  It can seriously affect performance of an Elasticsearch cluster.  It can be done, but it brings with it a lot of potential for hair-pulling and ranting.\nI will check with Jordan before merging any pull request for this.  If we do not accept the pull request you're free to fork this and publish your changes on your own.\n. @josegonzalez We're working on improving documentation.  I've only been with Elasticsearch for 2.5 weeks :)\nYour suggestion to feed non-matching records into a temp index is a good one for existing data.  Otherwise, it's still advisable to make a separate tag part of your index partitioning strategy.\nLogstash now provides a way to \"stream\" from a logstash index and do filtering/matching: http://logstash.net/docs/1.2.2/inputs/elasticsearch\nHowever, renaming indexes is less ideal.  You'd do better with a new index name, and then create an alias that points to that new index, with the alias name being the \"old\" index name.  That would preserve the index naming for you. See http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-aliases.html#alias-adding\nThe logstash cookbooks are good, but community-driven (for good or ill).  We love contributions! (not so subtle hint)\nWe are so busy trying to fix logstash and create new features that we are not able to document many use cases.  We are also working on creating a professional training course.  Re-indexing in the way described is one of the use cases we will be teaching in the training sessions (and we only spoke of it over lunch today, before I read the most recent comments).\n. Good deal.  You know what to do.\nI am closing this issue, then.\n. Looks good to me!\n. What version of Python?  I tested with 2.7.  This may not work with 3+\nI'll see if I can get a box to test that has 3+ on it.\n. I can confirm this with python 3.3.3\nI will work to see if I can work around it.  May need to fork a version specific to python3\n. This is now fixed in #8 to work with Python v3.3.3 (tested).  That PR also fixes a few other eccentricities.\n. ~~There remains another issue here.  Once closed, it doesn't see them to know they need deletion.  I'll have to test this further.~~\nNevermind.  My panic here was unwarranted.  I hit the rollover time and a new index was able to be deleted.  All's well here.\n. What version of python?\n. Better yet, which version of the logging module:\n```\n$ python\nPython 2.7.5 (default, Aug 25 2013, 00:04:04)\n[GCC 4.2.1 Compatible Apple LLVM 5.0 (clang-500.0.68)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport logging\nlogging.version\n'0.5.1.2'\n^D\n```\n. The recent pull request #13 fixes this.  Closing.\n. Curator is there to make repetitive practices easier, especially with regards to cron jobs.  If it's really a daily occurrence for people, then it makes sense to add it.  It does make some sense to have a mode to make it easier to \"open\" closed indices, or potentially to \"restore\" archived snapshots, but the latter is tricky to implement cleanly with command-line options, while the former is so simple you probably don't need curator to do it.  I mean, how often are indices getting restored?  In my experience it's 1 out of 1000 or less.  When that arrives, you have to decide, \"am I going to restore to a different index name?\" among other things.  Trying to provide all of the command-line flags for that is a yak shave I'd rather avoid, to be honest.\n\n\n\nRegardless, snapshots to filesystem and S3 will be the first goals I tackle on that front.\n. See #82 \nIt's almost here!\n. And for those who want restore, for now I suggest looking into the Kopf elasticsearch plugin's snapshot tab.\n. #82 is merged...\n. Edited: \nI appreciate what you're trying to do here.\nCan you submit your changes as a pull request instead?  That way we can comment on individual lines and give feedback before approving and merging.\n. Issue resolved by PR #40 \n. This is planned.  It just hasn't been released yet.  We'll probably name it curator-es or something like that for pypi.\n. This is done now.  You can install with: pip install elasticsearch-curator\n. Let me get some feedback from @HonzaKral and we'll see about merging this!\n. @gelim I'm sorry to ask this of you, but will you rebase your request?  I've just released 0.6.0 as a PyPI package and this will no longer automatically merge (complete re-structure).  You can even close/re-open if the rebase doesn't appeal to you.\n. @gelim We're at 0.6.1 now.  Can you rebase your additions?\n. Elasticsearch is licensed Apache 2.0.  That's what I'd prefer:\nhttps://github.com/elasticsearch/elasticsearch/blob/master/LICENSE.txt\n. The package can be called elasticsearch-curator.  That's not a problem.  Can we keep the script as curator.py?  That's a lot of changes if I can't. :frowning: \n. You can list me as the author: Aaron Mildenstein (aaron at mildensteins .com) for the email.\n. I'm fine with elasticsearch-curator.  @arieb, why did you choose curator-es?\n. @arieb While you're making this package:\n- Please prune logstash_index_cleaner.py. It's totally deprecated and was only left behind for stragglers.\n- Please update the README with the anticipated PyPI package name and command for installation.\n- Please add yourself to the CONTRIBUTORS :smile: \n- Bump the version number (in all necessary locations) to version 0.6.0 (this will be a big enough change since we're adding it to PyPI)\n- Make changes to CHANGELOG indicating version bump and your additions (follow the pattern and add your github handle).\nThanks!\n. I will merge this and submit to PyPI.  Thanks for making this project better, @arieb :+1: \n. Hmmm...\nRegistering elasticsearch-curator to http://pypi.python.org/pypi\nServer response (400): Invalid classifier \"Intended Audience :: System Administrator\"\n. I think this will actually be met by #28 which allows to use either argparse (2.7+) or optparse (2.6)\nWhat do you think?\n. I'm closing this as #40 rectifies the need for argparse.\n. It does work.  The problem is the 0.  You cannot use -d 0.  I suppose that should be better documented.\nThis was an intentional design choice so that you could not accidentally delete the current index.  If you want to delete the current index you will have to use a curl statement: curl -XDELETE localhost:9200/.marvel-2014.02.10?pretty\nPerhaps we could correct the script to allow for this in the future, but it was intentional.\nHere's what it looks like for me:\ncurator --host blackbox -c 6 -b 1 -d 7 -p .marvel-\n2014-02-10T06:55:27.821 INFO                        main:325  Job starting...\n2014-02-10T06:55:27.821 INFO                        main:345  Deleting indices older than 7 days...\n2014-02-10T06:55:27.822 INFO                   _new_conn:257  Starting new HTTP connection (1): blackbox\n2014-02-10T06:55:27.829 INFO         log_request_success:49   GET http://blackbox:9200/_settings [status:200 request:0.007s]\n2014-02-10T06:55:27.833 INFO                  index_loop:290  Attempting to delete index .marvel-2014.01.31 because it is 3 days, 0:00:00 older than cutoff.\n2014-02-10T06:55:28.040 INFO         log_request_success:49   DELETE http://blackbox:9200/.marvel-2014.01.31 [status:200 request:0.206s]\n2014-02-10T06:55:28.040 INFO                  index_loop:300  .marvel-2014.01.31: Successfully deleted.\n2014-02-10T06:55:28.040 INFO                  index_loop:290  Attempting to delete index .marvel-2014.02.01 because it is 2 days, 0:00:00 older than cutoff.\n2014-02-10T06:55:28.402 INFO         log_request_success:49   DELETE http://blackbox:9200/.marvel-2014.02.01 [status:200 request:0.362s]\n2014-02-10T06:55:28.402 INFO                  index_loop:300  .marvel-2014.02.01: Successfully deleted.\n2014-02-10T06:55:28.402 INFO                  index_loop:290  Attempting to delete index .marvel-2014.02.02 because it is 1 day, 0:00:00 older than cutoff.\n2014-02-10T06:55:29.118 INFO         log_request_success:49   DELETE http://blackbox:9200/.marvel-2014.02.02 [status:200 request:0.716s]\n2014-02-10T06:55:29.118 INFO                  index_loop:300  .marvel-2014.02.02: Successfully deleted.\n2014-02-10T06:55:29.119 INFO        find_expired_indices:188  .marvel-2014.02.03 is 0:00:00 above the cutoff.\n2014-02-10T06:55:29.119 INFO        find_expired_indices:188  .marvel-2014.02.04 is 1 day, 0:00:00 above the cutoff.\n2014-02-10T06:55:29.119 INFO        find_expired_indices:188  .marvel-2014.02.05 is 2 days, 0:00:00 above the cutoff.\n2014-02-10T06:55:29.119 INFO        find_expired_indices:188  .marvel-2014.02.06 is 3 days, 0:00:00 above the cutoff.\n2014-02-10T06:55:29.119 INFO        find_expired_indices:188  .marvel-2014.02.07 is 4 days, 0:00:00 above the cutoff.\n2014-02-10T06:55:29.119 INFO        find_expired_indices:188  .marvel-2014.02.08 is 5 days, 0:00:00 above the cutoff.\n2014-02-10T06:55:29.119 INFO        find_expired_indices:188  .marvel-2014.02.09 is 6 days, 0:00:00 above the cutoff.\n2014-02-10T06:55:29.119 INFO        find_expired_indices:188  .marvel-2014.02.10 is 7 days, 0:00:00 above the cutoff.\n2014-02-10T06:55:29.119 INFO                  index_loop:301  DELETE index operations completed.\n2014-02-10T06:55:29.120 INFO                        main:350  Closing indices older than 6 days...\n2014-02-10T06:55:29.122 INFO         log_request_success:49   GET http://blackbox:9200/_settings [status:200 request:0.002s]\n2014-02-10T06:55:29.122 INFO                  index_loop:290  Attempting to close index .marvel-2014.02.03 because it is 1 day, 0:00:00 older than cutoff.\n2014-02-10T06:55:29.130 INFO         log_request_success:49   GET http://blackbox:9200/_cluster/state?filter_nodes=true&filter_routing_table=true&filter_indices=.marvel-2014.02.03&filter_index_templates=true&filter_blocks=true [status:200 request:0.008s]\n2014-02-10T06:55:29.287 INFO         log_request_success:49   POST http://blackbox:9200/.marvel-2014.02.03/_close [status:200 request:0.153s]\n2014-02-10T06:55:29.287 INFO                  index_loop:300  .marvel-2014.02.03: Successfully closed.\n2014-02-10T06:55:29.288 INFO        find_expired_indices:188  .marvel-2014.02.04 is 0:00:00 above the cutoff.\n2014-02-10T06:55:29.288 INFO        find_expired_indices:188  .marvel-2014.02.05 is 1 day, 0:00:00 above the cutoff.\n2014-02-10T06:55:29.288 INFO        find_expired_indices:188  .marvel-2014.02.06 is 2 days, 0:00:00 above the cutoff.\n2014-02-10T06:55:29.288 INFO        find_expired_indices:188  .marvel-2014.02.07 is 3 days, 0:00:00 above the cutoff.\n2014-02-10T06:55:29.288 INFO        find_expired_indices:188  .marvel-2014.02.08 is 4 days, 0:00:00 above the cutoff.\n2014-02-10T06:55:29.288 INFO        find_expired_indices:188  .marvel-2014.02.09 is 5 days, 0:00:00 above the cutoff.\n2014-02-10T06:55:29.288 INFO        find_expired_indices:188  .marvel-2014.02.10 is 6 days, 0:00:00 above the cutoff.\n2014-02-10T06:55:29.288 INFO                  index_loop:301  CLOSE index operations completed.\n2014-02-10T06:55:29.289 INFO                        main:355  Disabling bloom filter on indices older than 1 days...\n2014-02-10T06:55:29.291 INFO         log_request_success:49   GET http://blackbox:9200/_settings [status:200 request:0.002s]\n2014-02-10T06:55:29.291 INFO                  index_loop:290  Attempting to disable bloom filter for index .marvel-2014.02.03 because it is 6 days, 0:00:00 older than cutoff.\n2014-02-10T06:55:29.299 INFO         log_request_success:49   GET http://blackbox:9200/_cluster/state?filter_nodes=true&filter_routing_table=true&filter_indices=.marvel-2014.02.03&filter_index_templates=true&filter_blocks=true [status:200 request:0.008s]\n2014-02-10T06:55:29.303 INFO                _bloom_index:267  Skipping index .marvel-2014.02.03: Already closed.\n2014-02-10T06:55:29.303 INFO                  index_loop:290  Attempting to disable bloom filter for index .marvel-2014.02.04 because it is 5 days, 0:00:00 older than cutoff.\n2014-02-10T06:55:29.312 INFO         log_request_success:49   GET http://blackbox:9200/_cluster/state?filter_nodes=true&filter_routing_table=true&filter_indices=.marvel-2014.02.04&filter_index_templates=true&filter_blocks=true [status:200 request:0.008s]\n2014-02-10T06:55:29.376 INFO         log_request_success:49   PUT http://blackbox:9200/.marvel-2014.02.04/_settings [status:200 request:0.059s]\n2014-02-10T06:55:29.376 INFO                  index_loop:300  .marvel-2014.02.04: Successfully bloom filter disabled.\n2014-02-10T06:55:29.376 INFO                  index_loop:290  Attempting to disable bloom filter for index .marvel-2014.02.05 because it is 4 days, 0:00:00 older than cutoff.\n2014-02-10T06:55:29.385 INFO         log_request_success:49   GET http://blackbox:9200/_cluster/state?filter_nodes=true&filter_routing_table=true&filter_indices=.marvel-2014.02.05&filter_index_templates=true&filter_blocks=true [status:200 request:0.008s]\n2014-02-10T06:55:29.446 INFO         log_request_success:49   PUT http://blackbox:9200/.marvel-2014.02.05/_settings [status:200 request:0.057s]\n2014-02-10T06:55:29.447 INFO                  index_loop:300  .marvel-2014.02.05: Successfully bloom filter disabled.\n2014-02-10T06:55:29.447 INFO                  index_loop:290  Attempting to disable bloom filter for index .marvel-2014.02.06 because it is 3 days, 0:00:00 older than cutoff.\n2014-02-10T06:55:29.455 INFO         log_request_success:49   GET http://blackbox:9200/_cluster/state?filter_nodes=true&filter_routing_table=true&filter_indices=.marvel-2014.02.06&filter_index_templates=true&filter_blocks=true [status:200 request:0.008s]\n2014-02-10T06:55:31.644 INFO         log_request_success:49   PUT http://blackbox:9200/.marvel-2014.02.06/_settings [status:200 request:2.185s]\n2014-02-10T06:55:31.645 INFO                  index_loop:300  .marvel-2014.02.06: Successfully bloom filter disabled.\n2014-02-10T06:55:31.645 INFO                  index_loop:290  Attempting to disable bloom filter for index .marvel-2014.02.07 because it is 2 days, 0:00:00 older than cutoff.\n2014-02-10T06:55:31.653 INFO         log_request_success:49   GET http://blackbox:9200/_cluster/state?filter_nodes=true&filter_routing_table=true&filter_indices=.marvel-2014.02.07&filter_index_templates=true&filter_blocks=true [status:200 request:0.008s]\n2014-02-10T06:55:35.807 INFO         log_request_success:49   PUT http://blackbox:9200/.marvel-2014.02.07/_settings [status:200 request:4.150s]\n2014-02-10T06:55:35.807 INFO                  index_loop:300  .marvel-2014.02.07: Successfully bloom filter disabled.\n2014-02-10T06:55:35.807 INFO                  index_loop:290  Attempting to disable bloom filter for index .marvel-2014.02.08 because it is 1 day, 0:00:00 older than cutoff.\n2014-02-10T06:55:35.816 INFO         log_request_success:49   GET http://blackbox:9200/_cluster/state?filter_nodes=true&filter_routing_table=true&filter_indices=.marvel-2014.02.08&filter_index_templates=true&filter_blocks=true [status:200 request:0.008s]\n2014-02-10T06:55:40.046 INFO         log_request_success:49   PUT http://blackbox:9200/.marvel-2014.02.08/_settings [status:200 request:4.226s]\n2014-02-10T06:55:40.047 INFO                  index_loop:300  .marvel-2014.02.08: Successfully bloom filter disabled.\n2014-02-10T06:55:40.047 INFO        find_expired_indices:188  .marvel-2014.02.09 is 0:00:00 above the cutoff.\n2014-02-10T06:55:40.047 INFO        find_expired_indices:188  .marvel-2014.02.10 is 1 day, 0:00:00 above the cutoff.\n2014-02-10T06:55:40.047 INFO                  index_loop:301  DISABLE BLOOM FILTER FOR index operations completed.\n2014-02-10T06:55:40.047 INFO                        main:365  Done in 0:00:12.228052.\n. I'm closing this since I went through the code and found I do already have a message that indicates this:\nmessages.append('Values for --delete, --close, --bloom, --optimize must be > 0')\n. Pull requests are cool :wink: \n. lgtm.  Thanks for making curator more awesome!\n. I think we're likely to merge this.  Can you add yourself to the CONTRIBUTORS and make a note in the CHANGELOG?\n. No, you won't need to do a new pull request.  All squashed into this one is just fine.\n. Meh.  I'll fix the CHANGELOG spacing later.\n. Closed by #62 \n. I believe this is now addressed with the API/script split in #159.\nIf not, we can re-open it and discuss for future releases.\n. @kimchy That's interesting to hear.  I had no problems with 0.90.x on my teeny-tiny cluster.  I had no problems with 1.0 on the same cluster.  I'm having fits with 1.1 on this same cluster.  Within 10 minutes after UTC rollover, my Logstash has stopped sending data for the last several days.  After a restart all is well.  \nThere are no indicators in either log file (though I'm admittedly not in debug for Logstash, but it didn't seem to matter there).\nFor some reason, it just stops cold after a bit in the new index.  I can't figure out why.  When Logstash is sending to disk, I have no such problems.  Only when I send to ES (and it doesn't matter if it's http or node or transport protocol from my testing).\n. Btw, that's a Logstash restart, not an ES one.  ES keeps going, but for some reason LS thinks it can't and stops.\n. This is largely moot with optimizations which have been added to Elasticsearch in the interim.  Namely, alias and allocation assignment at index creation time (including creation from template).\n. Max version of which?  Elasticsearch, or the elasticsearch-py module?\n. But this has already been merged: https://github.com/elasticsearch/curator/blob/master/curator/curator.py#L54\nAre you using an older version?\n. Are you using curator v0.6.2?  It was never meant to support Elasticsearch 1.0.  We will release a 1.0 branch of curator shortly (to coincide with Logstash 1.4), otherwise you need to clone master in order to work against an ES 1.0 instance.\n. Yeah, I anticipated releasing curator 1.0 much sooner than we are, so I wrote that in anticipation (yes, that's me).\n. It must be run daily if you wish to prune daily indices.\n. We aim to please!\n. To illustrate:\ncurator --host my-host --prefix custom-app- -d 5\nand\ncurator --host my-host -d 30\nThe second example defaults to logstash- as the prefix.\n. curator 1.0.0 has been released:\npip uninstall elasticsearch-curator\npip uninstall elasticsearch\nThen:\npip install elasticsearch-curator\nThe elasticsearch dependency will be picked up by the curator install.\n. I think this is the second installation issue that has been RHEL-specific. :frowning: \n. :+1: @HonzaKral \n:-1: RHEL's busted python implementation.\n. I will be releasing curator 1.0 later today. \u00a0It should work with ES 1.0+ (including 1.1, which was released today).\n\u2014Aaron\nOn March 24, 2014 at 3:19:08 PM, chris (notifications@github.com) wrote:\nI see the following:\ncurator --host some.com -d 500 -n\n2014-03-24T22:14:00.416 INFO                        main:332  Job starting...\n2014-03-24T22:14:00.417 INFO                   _new_conn:172  Starting new HTTP connection (1): some.com\n2014-03-24T22:14:02.584 INFO         log_request_success:49   GET http://some.com:9200/ [status:200 request:2.167s]\nExpected Elasticsearch version range > 0.19.4 < 1.0.0\nERROR: Incompatible with version 1.0.1 of Elasticsearch.  Exiting.\npython --version\nPython 2.7.3\npip --version\npip 1.0 from /usr/lib/python2.7/dist-packages (python 2.7)\n\u2014\nReply to this email directly or view it on GitHub.\n. To clean up (if you have problems):\npip uninstall elasticsearch-curator\npip uninstall elasticsearch\nTo install the updated version:\npip install elasticsearch-curator\nThe release of curator 1.0.0 should resolve this, with these steps.  If I get no updates after a week or so, I will mark this resolved.\n. pip uninstall elasticsearch will uninstall the python elasticsearch module, not your elasticsearch installation.  The following pip install elasticsearch-curator will install the newest version of the elasticsearch python module as a dependency, so it will immediately get re-installed.\n. No timeline yet.  Working on this to be out as soon as possible, though.  Too many other things are pulling me in other directions :frowning: \n. Resolved!\n. Please try these steps in order:\n1. Try re-installing the elasticsearch module\nsudo pip uninstall elasticsearch\nsudo pip install elasticsearch\nSee if curator works after this.  If not, move on to attempt 2.\n2. Complete reinstallation.\nsudo pip uninstall elasticsearch-curator\nsudo pip uninstall elasticsearch\nsudo pip install elasticsearch\nsudo pip install elasticsearch-curator\nIf this still fails, and it's possible it will, something about your python setup is disallowing discovery.  Not all Mac users have encountered this, but a few have. I use curator on my Macbook Air, and I have never had this problem.  I cannot for the life of me discover why it happens to some and not to others.  Perhaps it's because I have Xcode installed too. \nAt worst, you'll have to run it from /Library/Python/2.7/site-packages/curator/curator.py (which is where pip installs it on mine).\n. With curator 2.0, you'll have to run curator_script.py as curator.py now contains the library but not the command-line portions.\n@cdenneen, What version of python and what OS + release are you seeing this with?\n. I'm not sure what is different about my mac environment, but some people have it work, some don't.  I apologize.  I may bundle a Mac standalone at some point via pyinstaller.  Stay tuned.\nI advise migrating to a 1.x version soon.  The 0.90.x branch is being end-of-lived on 14 Oct 2014.  That will also necessitate a newer version of curator as well.\n. Let me guess, RHEL or Centos?  It's an old version of python, for one, and I've experienced this problem with entry_points with some distros of linux.  For some reason the code in the entry_point does something funny when the required module is loaded.\n```\n!/usr/bin/python\nEASY-INSTALL-ENTRY-SCRIPT: 'elasticsearch-curator==2.0.0','console_scripts','curator'\nrequires = 'elasticsearch-curator==2.0.0'\nimport sys\nfrom pkg_resources import load_entry_point\nsys.exit(\n   load_entry_point('elasticsearch-curator==2.0.0', 'console_scripts', 'curator')()\n)\n```\nThat's all that's in /usr/local/bin/curator on my MacBook Air.  It could be something with the load_entry_point method, but it's frustrating to not know why.\n. Indeed, please raise a new issue with this filter_blocks thing.  I've not seen that and would like to know what exactly triggered it.  I may need a new unit test to catch this as my current ones all pass.\n. Thank you!  I think you finally figured it out.  It's an outdated version of the setuptools!\nIf that's the case, this problem will persist in v3.  I will update the FAQ and docs in the wiki.\n. See the new FAQ\n. I will be happy to merge an addition there, but would prefer it to be more in the vein of:\n\nDistributionNotFound errors?\nSee this FAQ\n\nI want to keep people moving to the official documentation, rather than have lengthy stuff in the development README.\n. Would you mind submitting a pull request for this?\n. Marking this issue closed by #60 \n. Sorry to keep you waiting.  I need to verify this won't break our own Jenkins builds :)\n. I like this. :smile:\nMy only concern centers around the semi-repetitive use of \"require.\"  Is \"require\" the best word for the -r or --require option?  I totally get --required_rule, but I'm wondering if there isn't a better way to describe the initial -r or --require option.  \nI know.  I can't really think of one either, off-hand.  But it is sticking a bit in my mind... \n. As far as include/exclude go, we can cross that bridge when we come to it.  The prefix setting should allow for the most basic needs right now.\n. Merge now.  Fix code and add tests later :smile: \nThis is still the master branch anyway :)\n. Hmmm.  That's old enough that it appears to not have the right version of the logging module.  Is there any chance of an updated version being deployed (of logging or of Python itself)?\n. This will be closed for non-response in 7 days.\n. > I second this as an issue. Yum package manager relies on Python 2.6, so upgrading is not an option. Luckily Python is awesome. It shows you where the failing script is located so I can edit it to move past this problem.\nSurely, though, you could install python 3 as a separate, parallel installation.  You could then install curator with pip-python3, or python3 curator.py to get around this.\n. No further response.  Closing.\n. I think that could be useful.  I also think it would be useful to map out other use cases, e.g. I store logs in per-week indices, but have this script create \"daily\" indices pointed at it.\nThoughts?\n. This is potentially addressed by #82 (via #86) with the aliasing operations when combined with auto-aliasing in Elasticsearch templates.  You could automatically have each new index added to a rolling weekly and/or monthly alias, then use curator to prune with --unalias-older-than 7 for weeklies, and 30 for monthly.\nAs far as a \"yesterday\" and \"today\" that's not there yet.\n. Going to let this close unless there's a future call for it.  This is less necessary with alias assignment from template since Elasticsearch 1.something, and the new aliasing features of Curator handle the rotation after that.\n. Fixed in #66 and #67 \n. Users should be notified of the change, but I am reasonably certain this is the expected behavior from the older versions of logstash_index_cleaner.py and expire_logs\n. Thanks for catching this!\n. lgtm!\n. ~~Can you show me what your segment count looks like before and after?~~\nNevermind.  Confirmed.\n. Fixed in #75 \nThanks for catching this!\n. Indeed.  The fix is in master, but has not been released yet.  See this if you'd like to edit your own curator.py in the meanwhile.\n. Don't forget the kwargs...\n. Your date is in the wrong format (for curator).  It expects the default YYYY.MM.dd (see http://logstash.net/docs/1.4.0/outputs/elasticsearch#index) or even YYYY.MM.dd.HH (for hourly).  There is currently no other accepted formatting.  We may try to create a way to use strftime and accept defined timestamps, but that will be rather difficult as it involves matching joda-time with python's date libraries.  That will be quite a yak shave...\nEverything else looks great, though.\n. I don't think you can delete an index on alias, but it's worth trying.  Please let us know if that works.\n. Mapping will be a lot bigger than just figuring out indices.  That part is as simple as:\njoda YYYY.MM.dd vs. python %Y.%m.%d\nThe complexity happens if somebody wants to start using week numbers.  How do I delete based on elapsed days? Or months?  The masking of dailies is not a problem.  Getting tricky date math to play nice for creating complex aliases (both things like, \"yesterday\" and \"last_week\", as well as combining 7 days of regular dailies into a weekly alias, or a monthly, etc.) and other time management issues.  That's why this isn't just about adding in strftime mapping.\n. ### Marvel\n@deepakas your prefix for marvel is missing the -.  A prefix is supposed to be everything up to the date portion.  See the help output where it shows logstash-:\ncurator --help\n\u2026\n  -p PREFIX, --prefix PREFIX\n                        Prefix for the indices. Indices that do not have this\n                        prefix are skipped. Default: logstash-\n\u2026\nWith that, your marvel pruning command-line should read:\n/usr/lib/python2.6/site-packages/curator/curator.py -d 3 -p .marvel-\nFlume\nYour Flume serializer is using a different separator.  You can use the SEPARATOR configuration option to change that:\ncurator --help\n\u2026\n  -s SEPARATOR, --separator SEPARATOR\n                        Time unit separator. Default: .\n\u2026\nSo for that instance, you'd use a line more like:\n/usr/lib/python2.6/site-packages/curator/curator.py -d 3 -p raw_json- -s \\-\nBeing a dash, you may need to escape it with a backslash as above.\n. Will be closing this at the one month mark if no further comments.\n. Updated the wiki FAQ to link to and explain this better.  Closing.\n. You mention aws and ec2, but not which Linux variant.  I'm guessing RedHat or CentOS because they still use python 2.6, and they have some wonky path changes that don't get followed properly.  If this is what your environment is, this is not a huge surprise.  We've seen this in other tickets from similar environments.\nIf you find the full path to curator.py and execute that (instead of the wrapper script, curator), it should work.  That's the work-around that we have been advising.\nIf that still gives a dependency error, you could try pip install elasticsearch again and see if that fixes the dependency (which should have properly been a dependency at install time).  But I am guessing that's not the case.\n. The work-around was already posted.  It is to use the full path to the curator.py script as outlined.  This appears to be a bug primarily in RedHat/CentOS derivatives.  It works just fine in other Python 2.6 installations without having to use the full path.\nAlso, 0.6 is deprecated.  There will be no more bug fixes or patches to it as it is supporting a nearly end-of-life version of Elasticsearch (0.90.x).\n. If you attempted pip install elasticsearch-curator before pip install elasticsearch-curator==0.6.2 it is very likely that you did have that happen.\nYou should uninstall both and start over:\npip uninstall elasticsearch-curator\npip uninstall elasticsearch\npip install elasticsearch-curator==0.6.2\n. No.  The code clearly has: \ninstall_requires = [\n        'elasticsearch>=0.4.4,<1.0.0'\n    ],\nin it.\nIf pip install elasticsearch-curator==0.6.2 isn't working, then there's little I can do.  You're not using 0.6.2, that much is certain.  In 0.6.0 and 0.6.1 the same code reads:\ninstall_requires = [\n        'elasticsearch >= 0.4.4'\n    ],\nwhich could be what you're running into.\nWhat's the output of:\npip show elasticsearch\n. I think you should be able to install\npip install elasticsearch==0.4.5\nSo long as there is no other version installed.\n. Closing now as this is a month old.\n. Sorry about that.  Go ahead and just edit the line in your file to hours.  This was fixed in commit 4c4547f\nUnfortunately, 0.6.1 is deprecated now, and the 0.90.x branch of Elasticsearch will not even be supported after August 2014.  I'm don't think we'll be issuing any fixes for older versions.\n. Is this moot since it appears you installed 0.6.2 in #77?\n. Thanks!\n. This looks great, but instead of an exclude prefix, what about changing this to exclude a pattern in general?\nThis way, if I had a pattern of logstash- and a pattern of 2014.04 I would exclude all dates in April 2014, but it would still work against others.  Otherwise defining a pattern as prefix, then an exclude prefix would mean having to define logstash-2014.04 to exclude.\nThoughts?\n. I haven't heard back from you on this.  Are you still interested in making this an exclude pattern, rather than prefix?\n. I'm going to merge this anyway.  The changes in #82 are so big that I will merge this into master and refactor this into #82.  I've already added this functionality into #82 and added a simple unit test.\n. Thanks for contributing!  I love contributions. We actually discussed this feature in #46, but decided against adding it.\nDoes this solve a particular use-case you have that improvements in Elasticsearch 1.x haven't solved for you?\n. Can you send a gist of an example of your mapping?  I'm curious what you're doing.\nAlias management isn't in curator yet, but it is on the to-do list.  Deleting & closing are already functional.  Snapshots are almost ready for release.\n. Why couldn't that be handled with an Elasticsearch mapping template?\nLogstash already comes with one by default, all you'd have to do is extrapolate from there and add the fields/mappings you need.  In the referenced template, the index pattern matches logstash-* named indices, so if you need something else, or multiple templates, you can differentiate in that way.\n. @cschellenger Index mapping templates have been available since pre 0.18, at least.  Logstash started including a template with v1.3.\nAs far as alias management is concerned, if this satisfies you, let's close this PR and open an issue for alias management.  We can continue the discussion there.\n. Does this work for you? Aliasing at index creation?  If not, then let's proceed with the index pre-creation bit here.\n. Short answer: No. Curator can only delete entire indices.\nLong answer:\nAs a thought exercise, think of Elasticsearch indices as being like databases, or tablespaces within a database.  If you had hundreds of millions of rows to delete from your database, would you run a separate DELETE from TABLE where date<YYYY.MM.dd to assemble hundreds of millions of individual delete operations every day, or would you partition your tables in a way that you could simply run DROP table TABLENAME.YYYY.MM.dd?  The strain on your database would be astronomical on the former and next to nothing on the latter.  In similar fashion, it's the same way with Elasticsearch.  While Elasticsearch can technically do both methods, for use-cases with time-series data (like logging), we recommend dropping entire indices vs. the extremely I/O expensive search and delete method.  Curator was created to help fill that need.\nWhile you could store different types within different indices (e.g. syslog-2014.05.05, apache-2015.05.06), this gets very expensive, very quickly in a totally different way.  Each shard in Elasticsearch is a Lucene index.  Each index requires a portion of the heap to exist and be kept current.  If you have 3 daily indices with 5 primary shards each, you suddenly have reduced the available heap space for shard management by a factor of 3, having gone from 5 shards to 15, per index, not counting multiple indexes per day.  The ways to mitigate this (if you pursue this route) include massive daily indexing boxes and using shard allocation/routing to move indices to specific members of the cluster where they can have less effect; keeping fewer days of information; having more nodes in your cluster, and so forth.\nConclusion: While it may be desirable to have different life-cycles for your data, sometimes it's just easier and cheaper to store everything as long as the longest life-cycle you wish to maintain.\nPost-script: Even though it is neither recommended, nor best practices, it is still possible to perform these search & delete operations yourself, using the API. Curator will not be modified to perform operations such as these, however.  Curator is meant to manage at the index level, rather than the data level.\n. I'm moving this to the wiki\n. What is the output of:\n$ pip list | grep elastic\nOn mine, it looks like this:\nelasticsearch (1.0.0)\nelasticsearch-curator (1.0.0)\nAnd also, what was the command-line that generated the above?\n. I'm not sure what to make of that.  This is my own box:\n```\n$ python\nPython 2.7.5 (default, Mar  9 2014, 22:15:05)\n[GCC 4.2.1 Compatible Apple LLVM 5.0 (clang-500.0.68)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport elasticsearch\nclient = elasticsearch.Elasticsearch(host='127.0.0.1', port=9200, url_prefix='', timeout=30, use_ssl=False)\nclient.info()\n{u'status': 200, u'tagline': u'You Know, for Search', u'version': {u'lucene_version': u'4.7', u'build_hash': u'f1585f096d3f3985e73456debdc1a0745f512bbc', u'number': u'1.1.1', u'build_timestamp': u'2014-04-16T14:27:12Z', u'build_snapshot': False}, u'name': u'Blackbox'}\n```\n\n\n\nIf you're not seeing a full output from client.info() then perhaps there's something wrong with your elasticsearch instance.  u'{\"OK\":{}}' should never appear in the result from that call if the server is up and fully functional.\n. Is it possible that you have something else listening on port 9200, and elasticsearch is bound on another port?  Perhaps a client-only elasticsearch node (like the one Logstash spawns)?\n. Paste your output from this command: curl -v localhost:9200\n```\n$ curl -v localhost:9200\n Adding handle: conn: 0x7f8149803000\n Adding handle: send: 0\n Adding handle: recv: 0\n Curl_addHandleToPipeline: length: 1\n - Conn 0 (0x7f8149803000) send_pipe: 1, recv_pipe: 0\n About to connect() to localhost port 9200 (#0)\n   Trying ::1...\n Connected to localhost (::1) port 9200 (#0)\n\nGET / HTTP/1.1\nUser-Agent: curl/7.30.0\nHost: localhost:9200\nAccept: /\n< HTTP/1.1 200 OK\n< Content-Type: application/json; charset=UTF-8\n< Content-Length: 297\n<\n{\n  \"status\" : 200,\n  \"name\" : \"Blackbox\",\n  \"version\" : {\n    \"number\" : \"1.1.1\",\n    \"build_hash\" : \"f1585f096d3f3985e73456debdc1a0745f512bbc\",\n    \"build_timestamp\" : \"2014-04-16T14:27:12Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"4.7\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n* Connection #0 to host localhost left intact\n```\n\nIt should explain why you're seeing OK\n. Whatever it is, that's not an Elasticsearch server.  Do you have something else listening on 9200?  Some kind of proxy between curl and Elasticsearch?  \nIf you do netstat -na | grep 92 what do you see?  I'm wondering if you had something on 9200, and Elasticsearch chose the next available, like 9201 or something.\n. I'm stumped.  I can't explain why your elasticsearch instance can work and give that response.  Even on a client-only node (node.data & node.master are set to false) it gives a proper response.  At its very most basic, visiting the appropriate IP and port for Elasticsearch will always show:\n{\n  \"status\" : 200,\n  \"name\" : \"Ezekiel Sims\",\n  \"version\" : {\n    \"number\" : \"1.2.0\",\n    \"build_hash\" : \"c82387f290c21505f781c695f365d0ef4098b272\",\n    \"build_timestamp\" : \"2014-05-22T12:49:13Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"4.8\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\nOr something like it, with the appropriate node name, version numbers, times, etc.  That you do not see this is in error.  Curator is the least of your worries if this part of the Elasticsearch API is not functioning properly.\nPlease paste the output of curl localhost:9200/_cluster/stats?pretty and we'll see what comes out. My output is too long to paste here, but it should be fairly long.\nYou could also use jps on that pid and see what comes out:\n$ ps auwwx | grep jav\nbuh       2721  0.5  2.5 3485116 207732 pts/1  Sl+  07:21   0:22 /usr/bin/java -Xms256m -Xmx1g -Xss256k -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -Delasticsearch -Des.foreground=yes -Des.path.home=/home/buh/elasticsearch-1.2.0 -cp :/home/buh/elasticsearch-1.2.0/lib/elasticsearch-1.2.0.jar:/home/buh/elasticsearch-1.2.0/lib/*:/home/buh/elasticsearch-1.2.0/lib/sigar/* org.elasticsearch.bootstrap.Elasticsearch\nbuh       3169  0.0  0.0  16996   912 pts/3    S+   08:29   0:00 grep --color=auto jav\n$ jps\n3170 Jps\n2721 Elasticsearch\nNote that 2721 correlates in both outputs.\n. And after that, curl -v localhost:9200 still returns {\"OK\":{}}?\n. With nothing left to go on, my question is why you have the http-basic-server-plugin enabled.  That's the only thing I can think of that might do this.  I read this on the page for that plugin:\nThe plugin is disabled by default. Enabling basic authorization will disable the default HTTP Transport module.\nIf the default HTTP transport module is not there, that might explain why you only see OK.\n. YES!  Found it here\n. So that plugin is the problem.  Get rid of it and curator will work.  Either that, or create a special client node that only listens on localhost (different port even) that does NOT have that plugin, and have curator talk to that.\n. Recap: When the http-basic-server-plugin is enabled, the regular health-check output is disabled and the plugin only shows {\"OK\":{}} when a health-check call is made (curl localhost:9200) .  Since the version information is not contained in this alternate health check, curator cannot obtain the version number so it fails.\n. @amulyas Unfortunately no. Curator depends on some other health check metadata, and it seems that said plugin interferes with that.\n. @amulyas Well, that's why Elastic has the Shield security product.  Proxies are not very effective, though they are better than nothing.\nThere may be other ways to capture the version information, but this is the easiest.  If you can find another that doesn't break other functionality (or cause dramatic slowdowns), I could be persuaded to merge it.\n. @Ramlalit I'm not sure what the context is for what you're doing, even. A new issue is more appropriate.\nI can tell you, however, that I am surprised and should probably not be seeing django in the output for a Curator run. I'm guessing you installed via pip, and if possible, should instead be trying one of the independent packages.. Unfortunately, you're going to have to refactor a lot of this before it gets merged.  See #82 (which is pending merge right now)\n. That will probably be good.  I don't think there will be major changes to it.\n. I was afraid of that.  It was a bunch of big changes.\n. Because of merge conflicts, I'm closing this.  It's been refactored into #82 in yet again a new way.  A huge change to the way curator runs, mainly.  Much cleaner in some respects.  Will likely disappoint people who were doing multiple actions in one line, though.  Please look over #82 and see what you think.\n. Thanks for contributing!  I like the example a lot.  I think this information would go better in the wiki, though.  I'm actually planning on shortening the README.md and putting much of that information in the wiki too.  It's just gotten too big!\nWould you consider adding this as a wiki doc?\n. Sure.  The name can always be changed later.\n. I appreciate the feature request.  We're going to be doing a roadmap for Curator 2.0 in another week or two.  Curator's mandate is for index management, rather than data management.  It would be outside the scope of Curator to do any kind of data import/export feature.\nI agree that would be a useful tool, but it does not fit within the scope of Curator.  Perhaps you could write your own tool!\n. I've merged some copyright information into #82 \nWhen #82 is merged, it will be immediately promoted to version 1.1.0, and released, just fyi.\n. #82 is merged...\n. We do accept pull requests...\n. @HonzaKral and I will be getting together at our company meetup next week to discuss the future of Curator.  We will consider this becoming part of Curator 2.0.\n. Generic CLI tools don't deal with timeouts and things very well, unfortunately.  Elasticsearch can be local, or it might be across the datacenter, or somewhere else in AWS.\n. What command-line are you using to call this?  Where are you putting this 0? In curator or its entry_point?\n. Ah, so Python 2.6 can't handle format calls without numbers, huh?  I can fix that.\n. Fixed in #96 \n. After git wrangling, I have released 1.1.2 (don't ask) that incorporates this fix.\n. Otherwise, LGTM!\n. Of course... Windows has no concept of /dev/null. Provide a dummy log file as a temporary work-around.\u00a0\u2014Aaron\nOn Fri, Jun 27, 2014 at 11:28 PM, Brady Vidovic notifications@github.com\nwrote:\n\nUsing python 3.4 and curator-script.py 1.1.2 on Windows Server 2012\nCommand:\ncurator show --show-snapshots --repository \"Kibana_Repository\"\nResult:\nTraceback (most recent call last):\n  File \"C:\\Python34\\Scripts\\curator-script.py\", line 9, in <module>\n    load_entry_point('elasticsearch-curator==1.1.2', 'console_scripts', 'curator')()\n  File \"C:\\Python34\\lib\\site-packages\\curator\\curator.py\", line 643, in main\n    stream=open(arguments.log_file, 'a') if arguments.log_file else sys.stderr)\nFileNotFoundError: [Errno 2] No such file or directory: '/dev/null'\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/elasticsearch/curator/issues/102\n. I understand your use-case.  I don't think I will be adding this as a feature though.  \n\nWhile I sympathize with your plight, this is an exceptionally narrow use-case to add the custom coding and arguments for.  I'm already getting push-back for how many options there are, hence the recent push to make the arguments cleaner and more uniform between commands.\n. I'm sorry if you accidentally deleted something you did not mean to delete.  The functionality you are seeking already exists, after a fashion (just no prompting). \nThe --dry-run or -n flag lets you see the output of what curator Would have attempted:\n$ curator --dry-run delete --older-than 4\n2014-07-10T15:40:52.548 INFO                        main:644  Job starting...\n2014-07-10T15:40:52.548 INFO                        main:647  DRY RUN MODE.  No changes will be made.\n2014-07-10T15:40:52.549 INFO                   _new_conn:188  Starting new HTTP connection (1): localhost\n2014-07-10T15:40:52.551 INFO         log_request_success:57   GET http://localhost:9200/ [status:200 request:0.002s]\n2014-07-10T15:40:52.551 INFO                command_loop:538  Beginning DELETE operations...\n2014-07-10T15:40:52.553 INFO         log_request_success:57   GET http://localhost:9200/logstash-*/_settings?expand_wildcards=closed [status:200 request:0.001s]\n2014-07-10T15:40:52.561 INFO                command_loop:558  Would have attempted deleting index logstash-2014.07.05 because it is 2 days, 0:00:00 older than the calculated cutoff.\n2014-07-10T15:40:52.561 INFO                command_loop:558  Would have attempted deleting index logstash-2014.07.06 because it is 1 day, 0:00:00 older than the calculated cutoff.\n2014-07-10T15:40:52.561 INFO           find_expired_data:306  logstash-2014.07.07 is 0:00:00 above the cutoff.\n2014-07-10T15:40:52.561 INFO           find_expired_data:306  logstash-2014.07.08 is 1 day, 0:00:00 above the cutoff.\n2014-07-10T15:40:52.561 INFO           find_expired_data:306  logstash-2014.07.09 is 2 days, 0:00:00 above the cutoff.\n2014-07-10T15:40:52.562 INFO           find_expired_data:306  logstash-2014.07.10 is 3 days, 0:00:00 above the cutoff.\n2014-07-10T15:40:52.562 INFO                command_loop:580  DELETE index operations completed.\n2014-07-10T15:40:52.562 INFO                        main:671  Done in 0:00:00.047178.\nSince this functionality is already here, and since Curator is designed to be able to do these things without needing user input by default, I will not be adding confirmation dialogue.\n. This is technically a duplicate of #101 \nClosing.\n. Committing this to master.  Release pending acceptance testing in Jenkins.\n. You raise an interesting point.  I appreciate your effort here, but I'm going to take a different path in fixing this, as I had other logging issues I was planning to address:\n$ curator/curator.py -n --host blackbox delete --older-than 3\n2014-07-23 17:29:11,942 INFO                        main:689  Job starting...\n2014-07-23 17:29:11,943 INFO                        main:692  DRY RUN MODE.  No changes will be made.\n2014-07-23 17:29:11,946 INFO                command_loop:547  DRY RUN: Beginning DELETE operations...\n2014-07-23 17:29:11,952 INFO                command_loop:567  DRY RUN: Attempting to delete index logstash-2014.07.12.\n2014-07-23 17:29:11,953 INFO                command_loop:567  DRY RUN: Attempting to delete index logstash-2014.07.13.\n2014-07-23 17:29:11,953 INFO                command_loop:567  DRY RUN: Attempting to delete index logstash-2014.07.14.\n2014-07-23 17:29:11,953 INFO                command_loop:567  DRY RUN: Attempting to delete index logstash-2014.07.15.\n2014-07-23 17:29:11,953 INFO                command_loop:567  DRY RUN: Attempting to delete index logstash-2014.07.16.\n2014-07-23 17:29:11,953 INFO                command_loop:567  DRY RUN: Attempting to delete index logstash-2014.07.17.\n2014-07-23 17:29:11,953 INFO                command_loop:567  DRY RUN: Attempting to delete index logstash-2014.07.18.\n2014-07-23 17:29:11,953 INFO                command_loop:567  DRY RUN: Attempting to delete index logstash-2014.07.19.\n2014-07-23 17:29:11,953 INFO                command_loop:567  DRY RUN: Attempting to delete index logstash-2014.07.20.\n2014-07-23 17:29:11,953 INFO           find_expired_data:316  logstash-2014.07.21 is within the threshold period (3 days).\n2014-07-23 17:29:11,953 INFO           find_expired_data:316  logstash-2014.07.22 is within the threshold period (3 days).\n2014-07-23 17:29:11,953 INFO           find_expired_data:316  logstash-2014.07.23 is within the threshold period (3 days).\n2014-07-23 17:29:11,953 INFO                command_loop:584  DRY RUN: DELETE index operations completed.\n2014-07-23 17:29:11,953 INFO                        main:724  Done in 0:00:00.017961.\n2014-07-23 16:42:55,440 INFO                        main:724  Done in 0:00:00.017129.\nI will still add your name to the CONTRIBUTORS for raising this flag.  Watch for the fix in #111 \n. Fixed by #117 .  Also see updates in the wiki.\n. The short answer is because:\ncutoff = utc_now - timedelta(**{time_unit: (older_than - 1)})\ntimedelta doesn't work with months as a time_unit.\nI will attempt to make this work in a hacky way, like making %Y-%m be %Y-%m-01,  and multiply months times 30 days, or some such.\nSorry, it's not a quick work-around, or I'd have included it in v1.2.0 :frowning: \n. Fixed in #120 \n. Closed by #119 \n. Indeed, STDERR is the output. This is not a bug. If you do not want cron to send you an email every time it runs,  you have two options.\n1. 30 2 * * * /usr/bin/curator COMMAND [args] &> /dev/null\n2. --logfile /dev/null (or somewhere else)\nWith option 1, this is very common for cron output.  Send all output to /dev/null.  The more complicated variant is > /dev/null 2&>1, which sends STDOUT to /dev/null and redirects STDERR to STDOUT explicitly.\n. I will make this change for the next release.\n. Thanks for the additional information!\n. Fixed in #123 \n. Thanks for catching this! :+1: \n. Thanks for also reporting this!  I merged a commit this morning that already takes care of this, #126 \nI'm going to release 1.2.2 now to address this.\n. It's a misunderstanding, to be sure.  Did you use curator to create the snapshot you're seeking to delete?  Presently, the snapshot functionality in curator is rather static in its approach: One snapshot per day, one index per snapshot, and the snapshot name is the index name.  \nEffectively, curator's snapshot delete expects the snapshots to be in the format that curator snaps them, and will not delete snapshots made in any other way.  That's why it can't delete an index named test_snapshot: it expected a snapshot named testYYYY.MM.dd with the prefix you provided.\nThe reasoning here is that this is time series data.  Once an index is \"cold\" and no longer actively indexing, it is never likely to change again.  So snapshot it, save it in that state\u2014in a snapshot of the same name, containing only that index.  My future plans for snapshotting are to allow for two snapshot streams: One just like the current (full, cold indices), and another that can snapshot the current live index or two until they hit that \"cold\" state.  The delete function for the second option would potentially allow for your present use-case, and therefore a more broad ability to delete snapshots by \"snapshot\" creation date, though that can be of mixed utility because:\n\nWhen a snapshot is deleted from a repository, Elasticsearch deletes all files that are associated with the deleted snapshot and not used by any other snapshots.\n\nThe wiki shows all flags used with the snapshot command, and clearly the --timestring field is there.  If you can explain this in a way that is more clear, that would indeed be helpful.\n. Strike that.  The default will be 0 seconds.\n. The --prefix option is meant to be explicit, and not accept a wild-card.\nIf you want to make this work, your best option is:\ncurator --host 10.1.0.4 delete --older-than 1 -p .marvel-\ncurator --host 10.1.0.4 delete --older-than 1\nWhich will capture both.\nI'm not sure I want to include wild-cards for prefixes.  The idea is potentially very dangerous because it could result in commands being performed on unintended indices.\n. The difference between show and the other commands is that show doesn't have to split on the date.  There is no --older-than command for show.  Perhaps these should be unified, but my take is that show is effectively a noop and should not go through the regular flow.  It is provided as a nicety. \nThe problem is the wildcard.  See this line.  I separate the timestamp from the full index name by pruning the prefix.  If the prefix is *, this line still sees it as a single character. It doesn't know how to discover the date in those cases (hence the error message that it can't discover a date).\nThe wiki page for prefix makes plain:\n\nThe prefix in this example is logstash-, or everything up to the date string of YYYY.MM.dd.\n\nPerhaps I need to make that more visible.  The prefix is every character leading up to the date string, including the hyphen, e.g. .marvel- or logstash- or whatever else.\nIf you can think of a better way to isolate the date from the index name, I would be happy to implement/merge it.\n. That's not a bad update for the help output, something like: \nhelp='Prefix for the indices. Prefix + timestring = full index name. Default: logstash-'\nThe problem with extracting a date string from \"^(\" + prefix + \".*)(\\d{4}\\S\\d{2}\\S\\d{2})$\" is dynamically generating the second half of the regex for the pattern.  Fairly tricky to do right, and there will be a LOT of testing to make that repeatable without mistakes.  For instance, what about dates with no separator, e.g. 20140805?  Will have to think of edge cases and test it to death.  In all likelihood, I'll have to do character counts and step through whatever is given as the --timestring.\nI will see if I can coerce a way to do this and test it further.\n. This is being addressed by #136 \n. Yes, the code is committed already.\nGlobbing with the ability to filter out python strftime strings is a yak shave I don't want to pursue.  Yes, there is a segment of the population who will completely understand how to do that.  I believe that this approach is the best bridge between the two.  \nif prefix:\n        prefix = '.' + prefix if prefix[0] == '*' else prefix\n    if suffix:\n        suffix = '.' + suffix if suffix[0] == '*' else suffix\n    dateregex = get_date_regex(timestring)\n    regex = \"^\" + prefix + \"(\" + dateregex + \")\" + suffix + \"$\"\nThis is the easiest way, in my opinion.  The get_date_regex method is there to turn python strftime strings (specifically the timestring) into a regex so this works as a generalized, fully regex capable method.  \nAlso, this makes it work for cases where people want to specify *, which is a repeat character in python regex, not an \"everything\" character.\n. Done in #159\n. @t5unamie all time-series indices cannot be deleted in one shot unless they all have the same --timestring pattern.\nIf all indices I want deleted have the same --timestring (say the Logstash default of %Y.%m.%d), then I could catch all time-series indices with this command:\ncurator delete --prefix \\* --suffix \\* --older-than 30\nThis catches both .marvel- and logstash- indices on my system.\n. How does this differ from --older-than 3 time-units?  Are you suggesting deleting all indices except the 3 newest ones out of all indices (if you had logstash-, .marvel-, and something-, it would delete everything but today's index of each, meaning 3)?\nOr do you mean something more general, like, \"delete all but the 3 most recent indices of each prefix\"?\nIf you mean this most recent, it will be addressed in Curator 2.0 with the addition of wild-card support for prefixes and suffixes.  You will be able to run a command like curator delete --older-than 3 --prefix '*', and it will delete all indices older than 3 time-units (default is days), because the prefix is a wild-card.  With this, however, the time stamp on the indices must match each other to be counted.\n. I still don't see what the difference is between --older-than 3 and --more-than 3.  Curator still requires a defined time-unit, the default being days.  If you use the default time-unit, and do not provide a --timestring, the default there will be %Y.%m.%d, which would catch year-month-day formats like 2014.08.06.  If I have indices like this:\nlogstash-2014.08.01\nlogstash-2014.08.02\nlogstash-2014.08.03\nlogstash-2014.08.04\nlogstash-2014.08.05\nlogstash-2014.08.06\nlogstash-2014.08.07\nand I call delete --older-than 3 today (while it's still 2014.08.07) it will skip today's index, and delete everything older than 2014.08.04.  \nI'm very confused here, because I cannot see how a command like --more-than is any different from --older-than in this context, except for the possibility that I force curator to only consider full days when counting.  But again, in this case, adding another flag will not change the fact that you can simply decrease the count of --older-than by one to achieve the same result.\n. Thank you for clearing that up.  I understand now.\nAny reason you're doing daily indices spread so far apart?  Why not monthly or weekly indices?  Curator would then be able to work by week, or by month.\nThis is not common at all. Most of the time we see time-series data as a continuous stream.  While I see the applicability to your use-case, I don't feel it warrants adding more complexity to curator.\nThoughts?\n. Curator is expressly for time-series indices.  The tag-line for the project itself is Curator: Tending your time-series indices in Elasticsearch.  There are no plans to have Curator operate directly on non-time-series indices at any time in the future.\nYou'll have to write your own code to manage these, or do manual API calls to perform deletes, optimizes, etc., e.g.\ncurl -XDELETE localhost:9200/index_name?pretty\n. Done in #159 \n. I've known about this one for a while, but not known how to address it.  The problem is that I don't know what version number of argparse to cut off at.  Most modern *nix distributions come with a valid version of argparse, but for some strange reason (I think it's a yum dependency on Python 2.6) the RedHat variants do not.  When I tried to add argparse as a dependency, it installed a new version where one wasn't necessary.  I am trying to figure out what the version cut-off should be.  I guess I will try something arbitrary until I get a complaint and go from there :smile: \n. Done in #159\n. Done in #159\n. Thanks for catching this!\n. Already merged: bb832dc79b8b973410a3447f605bc63e18fe248a\n. I agree.  I think it should not be a failure, as it is not exiting based on a failure, but on a potentially expected condition. I'm fine with 0 or 2.  Which do you think makes more sense?  I'd argue for 0 with an INFO message.\n. This is actually a different bug.  It's not a unicode issue, it's trying to pass a tuple instead of just an index name, so it fails.  The bug is described in #141.  You can fix this by editing line 354 of /usr/lib/python2.6/site-packages/curator/curator.py on your system.\nChange:\nyield index_name, 0\nto\nyield index_name\nIt should work after that.  I will be addressing this in v2.0, which is going to be released soon.\n. I am loathe to do so, so soon before 2.0 is released.  I will perhaps do a 1.2.3 release later today.\nHonestly, I'm surprised how much interest there is in 'delete by space,' given the shortcomings it has.  This interest is the only reason I haven't removed the functionality. \n. See the wiki writeup, [Deleting indices by space].(https://github.com/elasticsearch/curator/wiki/Delete#deleting-indices-by-space)\n. Yeah.  Not doing 1.2.3. I'd be releasing some other code before I'm ready.  You'll have to wait for 2.0, or build from master yourself.\n. It has no improvement on performance whatever.  \nTechnically speaking, once an index has been optimized to 1 segment per shard, and had bloom filters disabled, there's not really anything else at the software level that will improve performance in any way.\nIt may be a useful feature for preventing writes. Elasticsearch isn't really even using this feature yet, so this will be on the back-burner for a while.\n. If you're using 1.2.2, the problem has been fixed in master by #147\n. This behavior with the newest entries is expected.  The sorting starts with the most recent and adds up the disk space from each index until you hit the threshold, and deletes everything after that (the oldest).  How many days worth of indices do you have?  It's indicating that your indices are empty, or perhaps closed.\nThere are serious reasons to not use delete by space, especially if you have multiple nodes.  Are these older indices closed?  If so, delete by space cannot delete them.  Please read the caveats for the delete by space on the wiki\n. If the 22nd (and/or any of your other indices) truly is only 495 bytes, and have 0 documents, that would explain why it appeared to be empty to curator (0.000GB).\nAre they all empty?  You created indices, but nothing was indexed?\n. Something is amiss, then.  Elasticsearch is reporting those indices as having zero size to both the head plugin and therefore curator as well.  This is not a curator problem, but an Elasticsearch-level one.\n. This feature is already working in my test branch and will be added to 2.0., though I won't make the 28th deadline for that milestone.\n. Done in #159\n. @nheid Your specific question is addressed in #196, complete with an example of using the Curator API to backup kibana-int separately.\n. Thank you for bringing this up.  This is already addressed in my current work branch, which filters by regular expression.\nPlease also know that if you have multiple indices, delete by space is not a very good idea.  You'll end up deleting things you don't want deleted because the space consumed by other indices (different prefixes) will not be taken into consideration.  Time is a better retention policy than space, particularly if you have multiple prefixes.\nSee: https://github.com/elasticsearch/curator/wiki/Delete and read the caveats at the bottom.  Delete by space is very useful for people testing on a single node setup where all space consumed is local to that one node.  Once you start having multiple nodes in your cluster where shards can be balanced very differently amongst the nodes, delete by space can have unexpected consequences and can even lead to unexpected network traffic as shard rebalancing occurs.\n. Curator will work for this use case.  \nWhat will you do if your dataset starts to grow more in one index type?  Delete by space may delete information you do not want deleted.  That's the other reason we tend to recommend delete by time.\n. Have you upgraded to the most recent curator (v2.0.2 at this moment)?  If you're satisfied that the current branch fixes this we can mark this closed.\n. This error was fixed in #147 \nPlease, please read the wiki page for the delete command.  While there are valid use-cases for delete-by-space, there are good reasons to not use the delete by space functionality.  These are the caveats at the bottom of the page.\n. You are not supposed to modify the host within curator.py.  That's what the --host directive is for.\n```\n$ curator --help\nusage: curator [-h] [-v] [--host HOST] [--url_prefix URL_PREFIX] [--port PORT]\n               [--ssl] [--auth AUTH] [--timeout TIMEOUT] [--master-only] [-n]\n               [-D] [--loglevel LOG_LEVEL] [--logfile LOG_FILE]\n               [--logformat LOGFORMAT]\n               {alias,allocation,bloom,close,delete,optimize,show,snapshot}\n               ...\nCurator for Elasticsearch indices. See\nhttp://github.com/elasticsearch/curator/wiki\noptional arguments:\n  -h, --help            show this help message and exit\n  -v, --version         show program's version number and exit\n  --host HOST           Elasticsearch host. Default: localhost\n  --url_prefix URL_PREFIX\n                        Elasticsearch http url prefix. Default: none\n  --port PORT           Elasticsearch port. Default: 9200\n  --ssl                 Connect to Elasticsearch through SSL. Default: false\n  --auth AUTH           Use Basic Authentication ex: user:pass Default: None\n  --timeout TIMEOUT     Connection timeout in seconds. Default: 30\n  --master-only         Verify that the node is the elected master before\n                        continuing\n```\nSpecifically, see this line:\n--host HOST           Elasticsearch host. Default: localhost\nAlso, see the wiki documentation for the --host flag here.\n. Thanks for catching this! The time string validation method does not yet cover monthly index patterns.\nThis will be addressed in Curator 2.0.  Until then, you can add the following code after lines https://github.com/elasticsearch/curator/blob/master/curator/curator.py#L674-L678\nelif time_unit == 'months':\n        if '%m' in timestring:\n            fail = False\nThis will should correct the behavior you've seen.\n. Indeed, it seems like a duplicate of #150.\n@cite would you care to comment further?\n. As mentioned in #150, Elasticsearch isn't even using the feature (as yet).  I got that response from Shay himself.\n. So far (from what I've heard), that feature is not properly functional.  Until it is, it doesn't make sense to add it to curator as APIs could change.\n. @cite, I do not believe there is a way to block writes to an Elasticsearch index outside of the whole-index read-only option described in #150 (which is apparently not fully functional).\nOther options include only optimizing to 2 or 3 segments per shard instead of only 1 (fully optimized).  If stray data is coming in later, then the other segments will likely be quite small.  Less than fully optimized, but still less painful to re-optimize to only that level than to 1 segment per shard.\nThoughts?\n. Hopefully this functionality works properly in ES in some future releases.\n. The functionality is still not there.  When it is, this can be added.\n. I've tested this and it does work as expected in recent versions.  This will be back on the menu for a future release of Curator.\n. Thanks for the info.  I know that the bloom filter cache would explain the behavior you described, all by itself, even.\nBloom is irrelevant to this issue as there's already a bloom feature in Curator, and bloom filter cache is disabled by default since 1.4.x.  I'm actually wondering what the change would be with just the blocks.write, since bloom is no longer an issue.  Do you have any data on that front?\n. Update: There's a problem with setting indices to read-only (at least in Elasticsearch 1.6 and using the elasticsearch python module):\nCreate index\n``` python\n\n\n\nclient.indices.create(index=\"foo\")\n{u\u2019acknowledged\u2019: True}\n```\n\n\n\nIndex a document\n``` python\n\n\n\nclient.index(index=\"foo\", doc_type=\"mytype\", body={\"name\":\"James\"})\n{u\u2019_type\u2019: u\u2019mytype\u2019, u\u2019_id\u2019: u\u2019AU345CoLYzProjwnt2EQ\u2019, u\u2019created\u2019: True, u\u2019_version\u2019: 1, u\u2019_index\u2019: u\u2019foo\u2019}\n```\n\n\n\nSet index to read-only\n``` python\n\n\n\nclient.indices.put_settings(index=\"foo\", body={\"index.blocks.write\":True})\n{u\u2019acknowledged\u2019: True}\n```\n\n\n\nAttempt index operation to read-only index (will fail)\n``` python\n\n\n\nclient.index(index=\"foo\", doc_type=\"mytype\", body={\"name\":\"Steve\"})\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/Users/buh/Library/Python/2.7/lib/python/site-packages/elasticsearch/client/utils.py\", line 73, in _wrapped\n    return func(args, params=params, *kwargs)\n  File \"/Users/buh/Library/Python/2.7/lib/python/site-packages/elasticsearch/client/init.py\", line 300, in index\n    _make_path(index, doc_type, id), params=params, body=body)\n  File \"/Users/buh/Library/Python/2.7/lib/python/site-packages/elasticsearch/transport.py\", line 318, in perform_request\n    status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)\n  File \"/Users/buh/Library/Python/2.7/lib/python/site-packages/elasticsearch/connection/http_urllib3.py\", line 128, in perform_request\n    self._raise_error(response.status, raw_data)\n  File \"/Users/buh/Library/Python/2.7/lib/python/site-packages/elasticsearch/connection/base.py\", line 124, in _raise_error\n    raise HTTP_EXCEPTIONS.get(status_code, TransportError)(status_code, error_message, additional_info)\nelasticsearch.exceptions.AuthorizationException: TransportError(403, u'cluster_block_exception', u'blocked by: [FORBIDDEN/8/index write (api)];')\n```\n\n\n\nThis much is expected.  I set blocks.write to True and I can\u2019t write.  \nBut then I set it to False, which should allow writes again, and I still can\u2019t write\u2026\nSet index to allow writes\n``` python\n\n\n\nclient.indices.put_settings(index=\"foo\", body={\"index.blocks.write\":False})\n{u\u2019acknowledged\u2019: True}\n```\n\n\n\nI still can't write\n``` python\n\n\n\nclient.index(index=\"foo\", doc_type=\"mytype\", body={\"name\":\"Steve\"})\nTraceback (most recent call last):\n  File \u201c\u201d, line 1, in \n  File \u201c/Library/Python/2.7/site-packages/elasticsearch/client/utils.py\u201d, line 69, in _wrapped\n    return func(args, params=params, *kwargs)\n  File \u201c/Library/Python/2.7/site-packages/elasticsearch/client/init.py\u201d, line 254, in index\n    _make_path(index, doc_type, id), params=params, body=body)\n  File \u201c/Library/Python/2.7/site-packages/elasticsearch/transport.py\u201d, line 307, in perform_request\n    status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)\n  File \u201c/Library/Python/2.7/site-packages/elasticsearch/connection/http_urllib3.py\u201d, line 89, in perform_request\n    self._raise_error(response.status, raw_data)\n  File \u201c/Library/Python/2.7/site-packages/elasticsearch/connection/base.py\u201d, line 105, in _raise_error\n    raise HTTP_EXCEPTIONS.get(status_code, TransportError)(status_code, error_message, additional_info)\nelasticsearch.exceptions.AuthorizationException: TransportError(403, u\u2019ClusterBlockException[blocked by: [FORBIDDEN/8/index write (api)];]\u2019)\n```\n\n\n\nUntil this is reversible, this feature will not be incorporated into Curator.\n. This is now reversible:\n```\n\n\n\nclient.indices.put_settings(index=\"foo\", body={\"index.blocks.write\":False})\n{u'acknowledged': True}\nclient.index(index=\"foo\", doc_type=\"mytype\", body={\"name\":\"Steve\"})\n{u'_type': u'mytype', u'_shards': {u'successful': 1, u'failed': 0, u'total': 2}, u'_index': u'foo', u'_version': 1, u'created': True, u'result': u'created', u'_id': u'AVtY857GaUWcUuzD7SWs'}\n```\n\n\n\nAs such, this feature will now be added as part of #656, rather than be its own dedicated feature. I'm sorry you hit this bug.  Thank you for taking the time to report it.\nI am happy to announce, though, that it's been fixed.  This was documented in #146, fixed in #147, and a work-around is here if you'd like to edit your current installation.\nThe fix will be in Curator 2.0, which should be coming soon.\n. This is remarkably strange.  The timeout switch is indeed being set.  I validated that by way of this code:\nlogging.info(\"arguments.timeout = {0}\".format(arguments.timeout))\n    client = elasticsearch.Elasticsearch(host=arguments.host, http_auth=arguments.auth, port=arguments.port, url_prefix=arguments.url_prefix, timeout=arguments.timeout, use_ssl=arguments.ssl)\nI only added the extra logging information so the standard output would show the result.  Here it is:\n$ python curator.py --timeout 1111 delete --older-than 90\n2014-09-19 15:38:52,825 INFO      Job starting...\n2014-09-19 15:38:52,826 INFO      arguments.timeout = 1111\n2014-09-19 15:38:52,828 INFO      Beginning DELETE operations...\n2014-09-19 15:38:52,838 INFO      logstash-2014.09.05 is within the threshold period (90 days).\n...\nI'm not sure why you're getting this error.  There is even an override if you're doing optimize or snapshot operations to set arguments.timeout to 21600.  This has been thoroughly tested because a snapshot or optimize will fail quickly because they take longer than 30 seconds.  This override works, so I'm not sure what you're running into, but let's see if we can find out.\nFirst, I'm curious why you're getting a timeout on a delete operation.  Deletes should be nearly instantaneous.  Even if you have the default 30 seconds, you should never hit that for a delete operation.  The timeout you're seeing seems to be a ProcessClusterEventTimeoutException.  A timeout from a low-set arguments.timeout looks like this:\n$ python curator.py --timeout 11 optimize --older-than 12 --max_num_segments 1\n2014-09-19 15:52:29,264 INFO      Job starting...\n2014-09-19 15:52:29,264 INFO      arguments.timeout = 11\n2014-09-19 15:52:29,267 INFO      Beginning OPTIMIZE operations...\n2014-09-19 15:52:29,277 INFO      Attempting to optimize index logstash-2014.09.05.\n2014-09-19 15:52:29,280 INFO      Skipping index logstash-2014.09.05: Already closed.\n2014-09-19 15:52:29,280 INFO      Attempting to optimize index logstash-2014.09.06.\n2014-09-19 15:52:29,286 INFO      Optimizing index logstash-2014.09.06 to 1 segments per shard.  Please wait...\nTraceback (most recent call last):\n  File \"curator.py\", line 737, in <module>\n    main()\n  File \"curator.py\", line 732, in main\n    arguments.func(client, **argdict)\n  File \"curator.py\", line 585, in command_loop\n    skipped = op(client, index_name, **kwargs)\n  File \"curator.py\", line 445, in _optimize_index\n    client.indices.optimize(index=index_name, max_num_segments=max_num_segments)\n  File \"/usr/local/lib/python2.7/dist-packages/elasticsearch/client/utils.py\", line 68, in _wrapped\n    return func(*args, params=params, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/elasticsearch/client/indices.py\", line 734, in optimize\n    _, data = self.transport.perform_request('POST', _make_path(index, '_optimize'), params=params)\n  File \"/usr/local/lib/python2.7/dist-packages/elasticsearch/transport.py\", line 276, in perform_request\n    status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)\n  File \"/usr/local/lib/python2.7/dist-packages/elasticsearch/connection/http_urllib3.py\", line 51, in perform_request\n    raise ConnectionError('N/A', str(e), e)\nelasticsearch.exceptions.ConnectionError: ConnectionError(HTTPConnectionPool(host='localhost', port=9200): Read timed out. (read timeout=11)) caused by: ReadTimeoutError(HTTPConnectionPool(host='localhost', port=9200): Read timed out. (read timeout=11))\nNote that we get a ReadTimeoutError here rather than the ProcessClusterEventTimeoutException you're seeing.  It also says Read timed out. (read timeout=11), which matches what was set.\nI'm not sure what's going on in your cluster, but the unusual ProcessClusterEventTimeoutException message in addition to a delete operation taking longer than a few milliseconds indicates to me that something else is going on that is not related to curator's timeout.\nThoughts?\n. By the way, the TransportError(503 (an HTTP 503 is \"service unavailable\") is suggesting that something is wrong.\n. Feel free to re-open this if you have more comments.  Closing for now.\n. Do you see a use-case where you'll be opening multiple indices at once, older than a specific timestamp?  Curator is typically for handling indices based on relative age.  Opening closed indices is not typically something that one would do regularly, so it is not part of curator as the API call is really simple.\nThe API call to open a closed index looks like:\ncurl -XPOST 'localhost:9200/my_index/_open'\nYou can also open indices within an ES plugin like ES Head or Kopf.\nThoughts?\n. I like this!  If it had come a day sooner, it probably would have made it into the 2.0 release (tomorrow), but I'm in code-freeze now. :frowning: \nIt will be in master soon thereafter, though.  Thanks for making Curator more awesome!\n. Having some reservations now that I'm testing.  With most log lines output by curator, syslog output would be just fine.  However there's a problem. With debug turned on, many lines will be too big for syslog (>1K).\nTraceback (most recent call last):\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/logging/handlers.py\", line 861, in emit\n    self.socket.send(msg)\nerror: [Errno 40] Message too long\nLogged from file base.py, line 60\nThoughts?  I can't merge this if it's going to break for users.\n. Closing this as v3 has a completely changed code base.  If you'd like to try this in v3, please submit a new PR.\n. Thanks for clearing that up!\n. Thanks for opening this issue!  I really appreciate feedback and suggestions as they drive me to make improvements.\nThe behavior you're describing more closely resembles the behavior in curator 1.2.0.  I changed from this behavior after I came to understand snapshots better.\nWhen I initially coded snapshot behavior, I thought that snapshots would pile up additional copies of indices, somehow, even though I knew they supported incremental backups.  What I learned is that snapshots capture at the segment level.  As long as the segment is referenced by any other snapshot in the system, deleting a snapshot will not delete the referenced data.  Subsequent snapshots of the same indices simply back up any new segments since the last snapshot in that repository (incrementals).\nBut how does that work in practice?\nThought experiment\nFor the sake of this thought experiment, let's presume I am capturing daily indices, that my oldest index is 2014.06.01, and that we run daily snapshots on indices older than 1 day.\nIf I run curator to capture a snapshot on 2014.06.02 and tell it to capture the last 30 days, I will have a snapshot called something like curator-20140601023004 which will contain the index logstash-2014.06.01.  The next night I'd have a snapshot called something like curator-20140602023006 which would contain the indices logstash-2014.06.01, & logstash-2014.06.02.\nThis pattern would continue as long as we let it, incrementally adding indices and snapshot names.\nOn 2014.07.01, my snapshot will be called curator-20140701023003 and will contain each index for the month of June.  On 2014.07.02, curator deletes index logstash-2014.06.01 as it is older than 30 days.  The next snapshot would contain all indices (omitting the now deleted logstash-2014.06.01) from June, and logstash-2014.07.01.\nLet's leap forward in time to 90 days after 2014.06.02.  If I attempt to prune snapshots older than 90 days, then the snapshot taken 2014.06.02 will be deleted.  Because index logstash-2014.06.01 is in that snapshot, you would think this would remove the snapshot data for index logstash-2014.06.01, but it doesn't.  Why not?  Because index logstash-2014.06.01 continued to appear in snapshots until 2014.07.02.  You would not actually delete the snapshot data for logstash-2014.06.01 until another 30 days had passed.\nNo such thing as duplicates\nWhile it may make sense from an operational standpoint to have them separated, snapshots should be approached differently from a tape-backup-like, solid copy per backup way of thinking.  Your indices are simply collections of segments.  If fully optimized, they are only 1 segment per shard.  The snapshot repository simply stores the segments and the metadata needed to reference them as \"indices\" (or collections of segments) when you restore.  It is important to understand that if you take two snapshots of the same, fully-optimized index (and therefore the same segments) with different names (e.g. snapshot-a & snapshot-b), the repository will only have 1 copy of the shards in that index, and both snapshots will reference those segments.  If I try to delete snapshot-a, the referenced segments will not be deleted because snapshot-b still references those segments.  Until all snapshots referencing the segments are deleted, the segments will persist in the repository.\nRestoring snapshots\nYou can restore individual indices from any snapshot, which implies the ability to restore to a given point in time.\nSince the restore operation can pull individual indices from a snapshot (implying the referenced segments at that moment of time in history), there is no longer much reason to have snapshots be one index each.  In fact, with incrementals, it's easier to figure out what point in time you want to target by having the rough timestamp in the snapshot name.  Of course, even without this, the snapshot metadata indicates the time the snapshot was taken.\nFailed snapshots\nThis is something that perhaps curator could handle better, particularly with warning the user.  You will see curator log messages if there is a failure, though, unless you set wait_for_completion to false.  You would still see a failure in the Elasticsearch logs, however.  If a segment cannot be captured in a snapshot, it may be corrupted (which is another problem entirely), in which case it would not be able to be captured ever, or until it is repaired (if possible).  Again, curator currently does not have a way to protect against this case.\nIn the event of a failure, the optimistic view is that Curator will simply try again the next time you run.  In the noted case of a corrupted segment, there is not much curator can do.\nMy suggested use case for time-series indices\nBecause optimize can potentially improve search speed and recovery time, it makes sense to back up the indices after they've been optimized to a low number of segments per shard.  Typically this is after a day or two, however, meaning there are no backups in the short term.\nOne solution I can recommend is to have two separate repositories (which need to have separate file system paths, otherwise you'd be writing the data to the same place), one being for short-term data, one for long-term data.\nThe short term snapshots should perhaps capture _all (or at least the indices you care most about), and could be run cyclicly, even as short an interval as every 5 minutes.  No joke!  Because only segments that change would be updated, this would keep live data backed up very quickly.  You'd perhaps only keep 4 to 7 days of this data, at most because you'd be capturing indices --older-than 2 days to the long-term snapshot repository, after they were optimized.  The long-term snapshots would have fully optimized segments and incrementals would not likely happen because new data is not likely to stream in for a date in the past.\nConclusion\nIf you still want the old behavior, please feel free to make use of curator 1.2.2.  There's nothing wrong with this approach if you want to do so, but curator will support the current methodology going forward.\nYou're also free to write your own script using the create_snapshot() method from the Elasticsearch Curator Python API to create named snapshots the way you want them.\n. --older-than will only work on full day indices.  Since it's the 27th, it is therefore an incomplete index, and would be skipped.\n. Right!  And because the method snapshot() doesn't call _op_loop(), you'll only see exclusions.  I suppose in snapshot() I should add something to show what indices have been selected, if only in the debug output.\n. @trompx \nUntil all snapshots which reference an index (and therefore by extension, a segment) are deleted, the segment (or index) is not deleted.\nSo, short answer: Yes.  As long as the snapshot exists (and is not corrupted), you can restore.\n. No. Not even Elasticsearch can do that. And since Curator is really only an index selection wrapper around the Elasticsearch API calls, Curator cannot do this. . @syndy1989 please ask usage questions in the Elasticsearch area of https://discuss.elastic.co\nThat question is totally unrelated to Curator . :+1: This is a feature I have already discussed internally and intended to add.\n. This is now up for your review, if you like.  I'll merge it tomorrow if there are no complaints or bugs discovered.\n. It does, indeed.  However, I typically don't update the wiki until an official version release is out.  Watch for the update when we release 2.1.0\n. I remember why I didn't do this before...\n$ curator/curator_script.py --host blackbox snapshot --all-indices --repository TESTING\n2014-10-01 17:35:36,190 INFO      Job starting...\n2014-10-01 17:35:36,190 INFO      Default timeout of 30 seconds is too low for command SNAPSHOT.  Overriding to 21,600 seconds (6 hours).\n2014-10-01 17:35:36,196 INFO      Capturing snapshots of specified indices...\n2014-10-01 17:51:29,214 INFO      Snapshots captured for specified indices.\n2014-10-01 17:51:29,214 INFO      Done in 0:15:53.034072.\nBecause it can take an eternity to get a response to the test while a big initial backup is running!\nPerhaps it's better to catch the 503 and notify about that instead, as it's only a 30 second wait.\n. Not necessarily another curator execution, but definitely a snapshot in progress already.. What would this look like for you, if you could have anything?\nCurator can loop over indices and have partial success (e.g. 9 out of 10 indices were successfully optimized, or 4 out of 5 indices were closed).  Snapshots are sort of all-or-nothing.  The snapshot may not be recorded as a success, but some of the segments may have been backed up and be referenced.\n. Enabling this is desirable, but cannot be done without changes that would potentially change the API.  Bumping this to Milestone 3.0.0\n. I don't think I will require a special flag.  This will be the default behavior.\n. I can't make 2.1.0 have proper exit codes at all because it requires me to fundamentally change how the API calls are working (i.e., they have to return pass/fail somehow). This is why it forces a major release to add the feature.\n. Exit codes should be working in v3.  If I have missed any, just submit a issue and/or PR and we'll fix it.\n. Short answer, I welcome pull requests :smile:\nTL;DR\nIt will require a non-trivial amount of extra coding to get Python to do this across multiple releases (i.e. 2.5 -> 3.4).  Even the most recent releases of Python do not have native ISO week support, but use custom modules.  Adding someone else's custom code to an open source project can present licensing issues, at the very least, if the licenses don't match up well.  So, we would have to use someone else's implementation, or write our own.\nThe second obstacle is Logstash.  I think you'll find that JODA suffers from a similar problem with regard to ISO week numbers, ergo Logstash hasn't supported them yet.  It would become imperative for Curator if Logstash supported ISO week numbers, otherwise not so much.  Curator has followed Logstash's lead with this and will likely continue to do so.\n. A more thorough explanation, plus a refutation of my assertion that Joda does not support ISO weeks (it does, so I am in error in my previous comment) is at https://github.com/elasticsearch/logstash/issues/1848.  The issue at hand is that without naming the index in full ISO Week format (which Elasticsearch does not support properly because indices cannot contain capital letters), there is ambiguity in whether a week is in ISO or locale format.  See https://github.com/elasticsearch/logstash/issues/1848 for more information.\n. At the very least, now, Curator should remove the ability for weeks to start on Sundays (%U vs. %W) as it is clear that Logstash + Joda time will never send weeks starting on Sundays\nRef: http://stackoverflow.com/questions/1801907/joda-time-first-day-of-week/1808242#1808242\n. This may be a 3.1 feature...\n. This may be moot now with Curator 4 (alpha releases now available), because it simply counts the number of seconds in  seconds, minutes, hours, days, weeks, months or years from a point of reference.  Everything is converted to epoch time, and then it's just a matter of older or younger than the point of reference.  Curator is no longer trying to do complicated date math.\nIf you feel I'm closing this in error, feel free to open a new ticket, or re-open this one.\n. Thank you for taking the time to submit this pull request!\n. It won't be released for a bit, but I was planning on merging this today.  If you're running an ES beta, you can probably handle cloning and running curator from master :smile: \n. Are you by chance using a self-signed certificate?  From the error, this error is clearly not from within curator itself, but in the urllib3 package.\n. It may be that you need to include the root certificates for rapidssl somewhere for urllib3.  I'm totally stabbing in the dark here because it's not something I can easily replicate here for testing.  You're the first to raise this issue, directly or indirectly.  The InsecureRequestWarning itself references https://urllib3.readthedocs.org/en/latest/security.html, which says it's likely a case where root certificates need updating.  It has a brief how-to for using certifi to get Mozilla's most recent certificates.\n. As I cannot reproduce this, I'm closing it.  If needed, please reopen it.\n. I know, I know.  Everyone has been expecting 2.1.0 for over a month now.  I've been tasked with working on Logstash 1.5, though, so my time to work on Curator has been limited.\n. Please see if my response to #174 answers your question.  If not, please tell me what your use case is that still requires the old way.\n. @rhealitycheck I appreciate where you're coming from.  However, the previous functionality was too restrictive and limiting in comparison to the full functionality that the snapshot/restore API allows.  That's what I was trying to indicate in #174.\nHave you heard of the kopf plugin? Among many features, the kopf plugin makes it easy to see what's in any given repository as well as browse through all snapshots in a repository.\n\nUsing Kopf it is easy to restore from snapshots using the little inbox looking icon to the left of the red x under each snapshot name.  You can pick individual indices from within snapshots and restore the index/indices to an alternate naming pattern, or if the original index is gone, to the original name itself.  It looks something like this:\n\nI understand this doesn't look exactly like what you want, but it is more in line with the full breadth of features the snapshot/restore API offers.\nAlternate solutions include using the older version of curator or writing your own code using the Elasticsearch Curator API, like the curator_script.py does.\n. As this is addressed in #174 as well, I'm closing this.  Please feel free to reopen if needed.\n. Thanks for contributing to the project!  I'm glad you dug in and found the problem and offered a fix.  As noted, there is a fix in master for this already.\n@MosheZada I plan to release Curator 2.1.0 by the end of this month.  Until then, you can run the master version or patch your pip-installed version with the method from master.\n. I see this when I run this command (my beta instance is on 9222 to not collide with my other instances):\n/testing \u00bb curl localhost:9222\n{\n  \"status\" : 200,\n  \"name\" : \"BetaMax\",\n  \"cluster_name\" : \"untergeekbeta\",\n  \"version\" : {\n    \"number\" : \"1.4.0.Beta1\",\n    \"build_hash\" : \"1f25669f3299b0680b266c3acaece43774fb59ae\",\n    \"build_timestamp\" : \"2014-10-01T14:58:15Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"4.10.1\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\nAnd I do a fresh clone of curator from git, and run a show, I see this:\n/testing/git/curator/curator \u00bb ./curator_script.py --port 9222 show --show-indices\nlogstash-2014.10.09\nlogstash-2014.10.10\nlogstash-2014.10.11\nlogstash-2014.10.12\nlogstash-2014.10.13\nlogstash-2014.10.14\nlogstash-2014.10.15\nlogstash-2014.10.16\nlogstash-2014.10.17\n/testing/git/curator/curator \u00bb ./curator_script.py -v\ncurator_script.py 2.1.0-dev\nI can't quite understand how your run isn't working yet.  Do you have the most recent master?\nAlso, are you sure that's what you wanted to do with --url_prefix?  Your entire elasticsearch cluster is behind the name myindex-2014.10.17.18?  That means a mapping query to index logstash-2014.10.17 would look like this in a curl statement:\ncurl myhost/myindex-2014.10.17.18/logstash-2014.10.17/_mapping?pretty\nIs that what you're trying to do?  It seems possible to me that if it's a non-existing URL prefix it could alter the results send back to this call. See the wiki documentation for --url_prefix for more information.\nThoughts?\n. Closing since I haven't heard back.  Please feel free to reopen if needed.\n. Thanks for the feature request.  This feature has been requested before and was not added for the reasons enumerated in #83 and in the wiki FAQ.\n. Closed by ac96ad77b112000df5554db3d4c31a7c923b1a25\n. Thanks for catching this!\n. This is because with --prefix set to .marvel you are missing the hyphen.  Please see the example at the bottom of the wiki page for prefix:\n\nDelete indices older than 7 days with prefix .marvel-:\ncurator delete --older-than 7 --prefix .marvel-\n\nTo curator, an index name must fit the pattern prefix + timestring + suffix, where prefix and suffix can be empty.  In the case of your entry above you have prefix with a value of .marvel which would mean the index would have to be .marvel20141003.  If you add the hyphen back in, the index name will be recognized properly.\n. When using the curator script, the kibana-int index can only be captured into a snapshot via the --all-indices flag as it is not a time-series index (no timestamp in the index name).  \nI will put up a quick and dirty example of how you can use the Elasticsearch Curator API to take snapshots of kibana-int or any other single index.  \nPlease wait until tomorrow or so for me to do this.\n. By the way, I noticed that you called curator with sudo.  There is no need for root-level access with curator.\n. This simple example presumes that:\n1. You have already created a repository (in this case, TESTING)\n2. You have a recent version of Python (2.6+)\n3. You have installed both the elasticsearch and elasticsearch-curator python packages.\n``` python\n!/usr/bin/env python\nimport elasticsearch\nimport curator\nrepo_name = 'TESTING'\nindex_list = [ 'kibana-int' ]\nsnap_prefix = 'kibana-int-'\nclient = elasticsearch.Elasticsearch(host=\"localhost\", port=9200, timeout=21600)\ncurator.create_snapshot(client, indices=index_list, snapshot_prefix=snap_prefix, repository=repo_name)\n```\nThis is about as bare-bones as it can get.  There's no logging or verification.  This script will, however, create a snapshot in repository TESTING with a prefix of kibana-int- followed by a datestamp.\nThis can be verified in the Elasticsearch log output:\n[2014-10-15 11:27:02,668][INFO ][snapshots                ] [Blackbox] snapshot [TESTING:kibana-int-20141015162702] is done\nSince the create_snapshot method expects a single index or a list, you can extend index_list by adding more index names, e.g. index_list = [ 'kibana-int', 'index1', 'index2' ], etc.\nTo reiterate, this is a very simple example.  There are many other options that can be passed.  This passes many of the defaults.  Truthfully, host=\"localhost\" and port=9200 aren't even necessary as they are the defaults.\n. This is no longer the case:\ncurator -n snapshot --repository TESTING --snapshot-prefix foo --most-recent 1\n2014-10-17 15:34:27,527 INFO      Job starting...\n2014-10-17 15:34:27,527 INFO      DRY RUN MODE.  No changes will be made.\n2014-10-17 15:34:27,527 INFO      Default timeout of 30 seconds is too low for command SNAPSHOT.  Overriding to 21,600 seconds (6 hours).\n2014-10-17 15:34:27,529 INFO      DRY RUN: Capturing snapshots of specified indices...\n2014-10-17 15:34:27,534 INFO      DRY RUN: Snapshot will capture indices: logstash-2014.10.17\n2014-10-17 15:34:27,534 INFO      DRY RUN: Snapshots captured for specified indices.\n2014-10-17 15:34:27,534 INFO      Done in 0:00:00.040161.\nHowever, I found a bug that I will need to address:\ncurator -n snapshot --repository TESTING --snapshot-prefix foo --most-recent 1 --prefix \\*\n2014-10-17 15:34:38,725 INFO      Job starting...\n2014-10-17 15:34:38,726 INFO      DRY RUN MODE.  No changes will be made.\n2014-10-17 15:34:38,726 INFO      Default timeout of 30 seconds is too low for command SNAPSHOT.  Overriding to 21,600 seconds (6 hours).\n2014-10-17 15:34:38,728 INFO      DRY RUN: Capturing snapshots of specified indices...\n2014-10-17 15:34:38,732 INFO      DRY RUN: Snapshot will capture indices: logstash-2014.10.17\n2014-10-17 15:34:38,732 INFO      DRY RUN: Snapshots captured for specified indices.\n2014-10-17 15:34:38,732 INFO      Done in 0:00:00.036912.\nThis should have also picked up .marvel-2014.10.17, but it doesn't.  If I use --prefix .marvel- it works.  This must be addressed.\n. Never mind.  The --prefix behavior when used with --most-recent is expected.\nIt should be documented that --most-recent does not use regular expressions.  Perhaps it should, but that will be a milestone 3.0.0 event.\n. @bflad Curator, specifically the curator_script.py portion of it, is for time-series indices which have date-stamps as part of the index name.  The very simple example above shows how the Curator API can be used to accomplish what you're trying to do.\n. Even though your indices are 6 hours apart, I don't see why a date string of %Y-%m-%d.%H wouldn't work.  Curator simply matches indices by that pattern.  It won't care that there are gaps.\nBe sure to use something like:\ncurator delete --time-unit hours --timestring %Y-%m-%d.%H --older-than n\nwhere n is a number of hours (can be in the hundreds or thousands) older than the current hour.\nThoughts?\n. On a completely separate note, if your cluster is that sensitive to both size and query, it is likely quite undersized.  A cluster this sensitive will likely require constant babysitting, which is no fun, indeed.\n. Curator does not iterate against hypothetical index names. Instead, it uses --older-than to perform date cutoff calculations against existing indices matching the provided patterns.\nWhen you run curator, it pulls the list of all indices from Elasticsearch and immediately filters them by --prefix, --timestring, and --suffix, but not by timestamp.  This part is simple regular expression pattern matching.  This leaves curator with a list of indices with known --timestring patterns to work with.  With the --older-than number, curator then calls the filter_by_timestamp method which calculates a cutoff date by multiplying the --older-than number by the --time-units. The method then subtracts that date from now to get a fixed cut-off time in the past.  It then compares this past timestamp against the timestamp extracted from the index name (by way of --timestring) of each index in the previously filtered list.  In this way, the method then returns the list of indices --older-than n --time-units.\n. Thoughts?  If no response, I will close this issue in a few days.\n. So the last column is not an hour, but an interval?  Yes, that's an edge case we won't likely address.  You will likely have to run curator at 6 hour intervals and just let it presume hours inaccurately.  With such a maneuver, you should be able to get the functionality.\nEither that, or you'll just have to write your own script that uses the API and you select indices your own way (perhaps by using the code you've written, in conjunction with the API).  Once selected, you should be able to send those to the delete_index method.\n. Please see the documentation wiki entry for --prefix\n\nTo curator, an index name fits the pattern prefix + timestring + suffix, where prefix and suffix can be empty.\n\nElasticsearch (and by extension, curator) has no way of knowing how old a time-series index unless a timestamp is part of the index name.\nPotential solutions are to name your indices with timestamps, or to manually delete your indices with the delete API.\n. Can you provide links to the unclear documentation?  I'd be happy to take this as an action item.\nIf we can do something to improve documentation, we like to be informed.\n. I did some checking.  The new index.creation_date exists only in Elasticsearch version 1.4.0+, which is currently in beta and not yet generally available.\nIt will be another flag available in Curator 2.1.0 or 3.0.\n. I seriously need a weekend.  The issue at hand is by design.\npython\n        if not all_indices:\n            index_list = get_object_list(client, **kwargs)\n            if most_recent:\n                matching_indices = index_list[-kwargs['most_recent']:]\nIt will only get the last # of indices in index_list, regardless of --prefix.  If I want the 2 most recent logstash indices, I would do --most-recent 2 and --prefix logstash-.  Even the documentation mocks me for thinking I intended this to be --most-recent n days:\npython\n    :arg most_recent: Most recent *n* indices will be operated on.\nSo, to wrap up:\nYou can still use --prefix, --timestring and --suffix to populate index_list, but in the end, --most_recent will only catch the specified number of entries from what's in the list, regardless.\n. The idea of combining --older-than and --newer-than differs from --most-recent in that this uses the same regexes to constrain a date range.\n. This is totally one of the best new features in v3.0.0!\n. Thanks for providing this code.  I appreciate the time and thought that have gone into this.\nIt's the complexity this adds that concerns me.  Feature creep and an overabundance of command-line options are already getting a bit out of control.  I understand your use case as you've outlined in #197, but why can't you just add 6 to the value you provide in --older-than?  The only trade-off is that at pruning time, you'd have built up to an extra 6 hours of data.  You wouldn't delete more than you wanted in a single pass at this point.  No extra complexity would be needed in curator.  In my mind, that makes this request an edge case because the improvement can already be addressed in other ways.\nIf your cluster is so sensitive to shard count and disk space that you have no leeway for 6 extra hours of data, you might need to increase your node count.\nThat said, I encourage you to use the API to accomplish your needs in a way that makes sense to you.  It is this kind of edge case that prompted me to provide an API so people with different or specialized needs could build their own to suit their requirements.  You could import the API, but keep this port of the get_index_time() method in your own script so you can call as needed (or alternately just keep your own fork of curator.py).\nThoughts?\n. Please be advised that there is a limit to how many shards an instance of Elasticsearch can address that is independent of disk space.  Once that threshold is crossed, Elasticsearch could suddenly stop indexing without warning, even if you have significant amounts of disk space remaining.\nI can't tell you exactly when that will be with your data and usage pattern, but I will point out that by default, an ES data node will set aside 10% of the heap for indexing (caching, operations, etc.).  Each active shard will want no less than 250M of that space.  Each inactive shard will want no less than 4MB of that space.  Elasticsearch will still force the division to result in values lower than those numbers in the event that this heap space becomes more crowded.  If it pushes too far, you will be unable to index.  The only way out when this happens is to delete or close indices and usually a node restart.  If you need indices to persist beyond this, your options are one or more of the following: a) bigger heap [not to exceed 31G per node], b)  closing infrequently accessed indices, c) adding more nodes to the system to rebalance shard count per node.\n. This is a known issue with some installations.  See #56 & #77.\nThe best workaround is to run /usr/lib/python2.6/site-packages/curator/curator_script.py (or whatever path it is in on your system).  The entry_point at /usr/bin/curator really just calls this script anyway.  Something about your python install does not allow the entry point to find the elasticsearch-py dependency which curator installs for you.\nThe RPM bit is a red herring.  As @electrical pointed out, the RPM installs Elasticsearch, and curator installs a python module called elasticsearch-py as a dependency.\n. Multiple prefixes is not supported.  However, regular expressions for --prefix are supported:\n$ curator -n delete --older-than 10 --prefix '(?:logstash-|.marvel-)'\n2014-10-27 11:20:31,116 INFO      Job starting...\n2014-10-27 11:20:31,117 INFO      DRY RUN MODE.  No changes will be made.\n2014-10-27 11:20:31,120 INFO      DRY RUN: Deleting indices...\n2014-10-27 11:20:31,131 INFO      DRY RUN: of delete_index operation on .marvel-2014.10.13\n2014-10-27 11:20:31,131 INFO      DRY RUN: of delete_index operation on .marvel-2014.10.14\n2014-10-27 11:20:31,131 INFO      DRY RUN: of delete_index operation on .marvel-2014.10.15\n2014-10-27 11:20:31,131 INFO      DRY RUN: of delete_index operation on .marvel-2014.10.16\n2014-10-27 11:20:31,131 INFO      DRY RUN: of delete_index operation on .marvel-2014.10.17\n2014-10-27 11:20:31,131 INFO      .marvel-2014.10.18 is within the threshold period (10 days).\n2014-10-27 11:20:31,132 INFO      .marvel-2014.10.19 is within the threshold period (10 days).\n2014-10-27 11:20:31,132 INFO      .marvel-2014.10.20 is within the threshold period (10 days).\n2014-10-27 11:20:31,132 INFO      .marvel-2014.10.21 is within the threshold period (10 days).\n2014-10-27 11:20:31,132 INFO      .marvel-2014.10.22 is within the threshold period (10 days).\n2014-10-27 11:20:31,132 INFO      .marvel-2014.10.23 is within the threshold period (10 days).\n2014-10-27 11:20:31,132 INFO      .marvel-2014.10.24 is within the threshold period (10 days).\n2014-10-27 11:20:31,132 INFO      .marvel-2014.10.25 is within the threshold period (10 days).\n2014-10-27 11:20:31,133 INFO      .marvel-2014.10.26 is within the threshold period (10 days).\n2014-10-27 11:20:31,133 INFO      .marvel-2014.10.27 is within the threshold period (10 days).\n2014-10-27 11:20:31,133 INFO      DRY RUN: of delete_index operation on logstash-2014.10.13\n2014-10-27 11:20:31,133 INFO      DRY RUN: of delete_index operation on logstash-2014.10.14\n2014-10-27 11:20:31,133 INFO      DRY RUN: of delete_index operation on logstash-2014.10.15\n2014-10-27 11:20:31,133 INFO      DRY RUN: of delete_index operation on logstash-2014.10.16\n2014-10-27 11:20:31,133 INFO      DRY RUN: of delete_index operation on logstash-2014.10.17\n2014-10-27 11:20:31,134 INFO      logstash-2014.10.18 is within the threshold period (10 days).\n2014-10-27 11:20:31,134 INFO      logstash-2014.10.19 is within the threshold period (10 days).\n2014-10-27 11:20:31,134 INFO      logstash-2014.10.20 is within the threshold period (10 days).\n2014-10-27 11:20:31,134 INFO      logstash-2014.10.21 is within the threshold period (10 days).\n2014-10-27 11:20:31,134 INFO      logstash-2014.10.22 is within the threshold period (10 days).\n2014-10-27 11:20:31,134 INFO      logstash-2014.10.23 is within the threshold period (10 days).\n2014-10-27 11:20:31,134 INFO      logstash-2014.10.24 is within the threshold period (10 days).\n2014-10-27 11:20:31,135 INFO      logstash-2014.10.25 is within the threshold period (10 days).\n2014-10-27 11:20:31,135 INFO      logstash-2014.10.26 is within the threshold period (10 days).\n2014-10-27 11:20:31,135 INFO      logstash-2014.10.27 is within the threshold period (10 days).\n2014-10-27 11:20:31,135 INFO      DRY RUN: Speficied indices deleted.\n2014-10-27 11:20:31,135 INFO      Done in 0:00:00.051619.\nSo you can accomplish this with --prefix '(?:logstash-|.marvel-)' which has a logical OR between logstash- and .marvel-.\nIf this is an acceptable answer, please go ahead and close the issue.  I'll close it in a few days otherwise.\n. Thanks for this!\n. Can you add a test to catch this?  Clearly my testing does not catch this scenario.\n. Do you need me to add a test?\n. No worries, then.  I'll just update that and merge it in here.  The change needs to make it in.\n. By default, Elasticsearch will do a tiered approach to segment merges.  It does a marvelous job of this and it is strongly recommended that end-users never tamper with the default merge patterns.  This has been heavily tested and vetted. \nWith that said, optimize is not optimizing anything in a way that benefits an active index.  It is a Lucene forceMerge() operation.  This means that it is attempting to merge segments\u2014of which there may be 50 or more on any given shard\u2014into only n per shard.  The I/O load on a system during a full optimize (1 segment per shard) is enormous.  It can take hours to finish for large indices.  Not only this, it can result in strange segment sizes which will continue to make for sub-optimal future segment merges which is a performance hit for the rest of the life of your index.\nDue to the performance robbing implications of this, you do NOT want to do this on an active index, ever, under any circumstances.  The official recommendation from Elasticsearch is to let Elasticsearch handle your segment merges automatically for live indices.  Once they're \"cold\" and not having new data added is when it makes sense to \"optimize\" to fewer segments per shard.  This is why you only have --older-than as an option here.\nThe API allows you to force an optimization in spite of these recommendations.  But to prevent end-user pain, that option will not be added to Curator.\n. The behavior with monthly indices is as designed, with the chief reason being delete.  Let's say I wanted to delete a monthly index --older-than 1 month. When October 2 rolls around I still expect to be able to see data back to at least September 2.  In order to preserve that functionality, I must not act on a whole month index until another whole month has passed, otherwise I only have 1 day of data.\nThis is the tricky part about monthly indices, unfortunately.  In your case, it seems you're better off using the Curator API to get done what you want.  Curator can't protect your old data if it compromises the integrity of a \"whole month.\"\n. That's also why 0 works in --older-than with monthly indices, to allow for optimize calls on the most recently changed monthly index, and delete can work against --older-than 1 so as not to violate the integrity.\nIf you insist on optimizing the current index, you will have to do it outside of the curator script, using the curator API.  That functionality, being not recommended, will not be added.\n. The indicated check is there because snapshot can work against more options than --older-than, while optimize only has the --older-than flag.\n. Did you perhaps mean these lines?  https://github.com/elasticsearch/curator/blob/4d16dd67a462e7ed45cbd1923962436f306b0ced/curator/curator_script.py#L283-L285\n. If you want to change the logic in curator_script.py, by all means submit a pull request.  It has to accept a 0 in order to work the way you would like.\n. Please test it and see.  I will do so also.\n. It seemed to work with the existing tests without breaking anything, so we may be good here.\n. This behavior seems limited to monthly indices.  Investigating.\n. 2 digit years have not been supported.  Try adding them here:\n``` diff\ndiff --git a/curator/curator.py b/curator/curator.py\nindex 9c34310..9265a64 100755\n--- a/curator/curator.py\n+++ b/curator/curator.py\n@@ -11,6 +11,7 @@ logger = logging.getLogger(name)\nDATE_REGEX = {\n     'Y' : '4',\n+    'y' : '2',\n     'm' : '2',\n     'W' : '2',\n     'U' : '2',\n``\n. I tested it, it passes tests.  LGTM.\n. Closing per discussion in #213\n. Thanks for taking the time to add these and submit them for review. I'm quite curious.  What's the use case for Curator to open closed indices based on--older-than`?  Is opening closed indices a scheduled phenomenon for you?\nI'm not opposed to having \"open\" methods in Curator.  You did a great job converting the basic \"close\" methodology and the close loop into matching methods for opening.  Re-opening an already open index has no penalty and yields no errors though:\n``` python\n\n\n\nclient.indices.open(index='logstash-2014.11.04')\n{u'acknowledged': True}\nclient.indices.open(index='logstash-2014.11.04')\n{u'acknowledged': True}\n```\n\n\n\nBecause opening indices is usually ad hoc I didn't include any methods for opening.  The command-line API calls are also quite simple:\nshell\n$ curl -XPOST localhost:9200/logstash-2014.11.04/_open?pretty\n{\n  \"acknowledged\" : true\n}\nThe eshead and kopf plugins for Elasticsearch also have interfaces which make it as simple as a few clicks to open closed indices.\nIf you have a legitimate use case for re-opening indices at a specified time, I'm open to the suggestion.\n. @stuart-warren I am in agreement with this.  Curator 3.0 (if it ever makes it that far) is intended to be a pipeline.  This would allow any number of \"filters\" to be applied: --older-than, --newer-than, etc. in combination.  This would be a milestone 3.0.0 fix, I think.\n. I'm not going to merge this for the reasons above.  I think that it's an edge case that can just as easily be covered by use of the indices.open() method in the elasticsearch python module where needed.  In the meanwhile, the mentioned kopf and eshead plugins for Elasticsearch can make re-opening closed indices go quickly. \n. I have also addressed this in #239.  This feature will not be added before Curator 3.0.  Curator needs to be able to pipeline all of the indices through any number of filters, including multiple date criteria.  This will be a bigger change than just merging an \"open\" statement. \n. Thanks for catching this!\n. Closed by d6d3368b52cbef26e91027a5f31e0016c6b79d63\n. Oh, this is nice.  Thanks!  I'll merge this soon.\n. Closed by 008a198425a72ad977e2c4e7b31a36c1fc5eaf6f\n. I'm sorry you had a hard time.\nWould you consider adding this to the Curator wiki?  For now, that's as good a place as any.\n. Closing this since you added the information.  Thanks!\n. While I appreciate the effort and the functionality it offers, I'm not sure that this belongs as part of the project.\nFirst, it's relatively simple to use pip to install Curator and all its dependencies, even offline.  I understand that some people prefer using rpm, deb, etc., but I'm not looking to make these packages at this time.\nAdditionally, the not-commonly-known fact is that Curator is destined to go away.  It was a stop-gap project from the beginning.  Elasticsearch is working on including most, if not all, of Curator's current functionality as a functional part of Elasticsearch.  Curator will continue to exist as people transition from it to the new way of doing things.  \nWith news like this, it's just not a pressing need for me to offer RPMs or DEBs that are installed to a semi-arbitrary path.  I'll end up having to maintain the packaging, rather than the core code.  That's not something I'm interested in pursuing with the future looming.\nI do recommend you create your own repository for this, however.  It is useful, and doubtless there are others who would put it to good use.\nThoughts?\n. This should be doable with the current configuration options.  The 3rd example in the wiki shows how to delete expired snapshots.\nYou'd probably want a different command for each type, e.g.\ncurator snapshot --delete-older-than 24 --time-unit hours --snapshot-prefix hourly- --repository REPOSITORY_NAME\ncurator snapshot --delete-older-than 7 --time-unit days --snapshot-prefix daily- --repository REPOSITORY_NAME\ncurator snapshot --delete-older-than 30 --time-unit days --snapshot-prefix weekly- --repository REPOSITORY_NAME\nPlease note that with snapshots, the expiration time calculation is figured by the time of snapshot creation, as recorded in the snapshot.  In this way, it makes sense to use 30 days for deleting weekly snapshots rather than 4 weeks, which is not exactly a month.\nThese methods do require that you isolate snapshots from each other with snapshot prefixes.\nDoes this answer your question?\n. It is true that the current implementation would require 3 separate cron entries or a wrapper script.  --delete-older-than will not cause a snapshot to be taken, however.  These 3 deletes will take mere moments to complete, even on a busy server.\nThe functionality you are requesting may be added to Curator 3.0 (should we get to that release).\n. There is much new functionality for filtering snapshots (and indices) in v3.  This will still not be possible in a single command, however.\n. This is known in Elasticsearch as a delete by query.  This is an extremely expensive operation in terms of I/O and CPU, and it results in messy, unoptimized indices.  While I appreciate the use case you face, the best solution to this would be to have named indices and use curator in its present form to delete entire indices.  This systemic pain is why this feature has not been added to curator.\nYou can get around the kibana multiple index issue by creating an alias (logstash-YYYY.MM.dd) for each day and assigning each new index to it.  It's not an easy work-around, but it is the best choice for performance.  There are still other concerns with multiple indices per day (shard count per node can exhaust memory quicker).\n. Sorry for a slow response.  I'm in the process of moving.\n@quasipedia: Running an optimize will merge segments, but that is perhaps just as costly as the delete would be in the first place.  It does no other housekeeping, as you put it.  It is, however, the only way to heal an index after a delete by query.\nTo respond to @neoice, you wouldn't necessarily have to have all log levels in independent indices.  Using conditionals, you could extract just one or two into a debug index, and use the else statement to send the rest to a different index.\n. @quasipedia I'm sorry.  Curator can't compensate for this, and using Elasticsearch calls to delete huge volumes of docs is also not recommended.  We can keep the discussion alive here, but I am going to close the ticket.\n. Please see #196\n. Rendering _, a, l, l is an artifact of the code.  A change in logging will change the display.\nI will see what, if anything, has changed in the elasticsearch python module to change this behavior since ES 1.1.\n. There is nothing preventing _all from working.  I will create a PR to fix the incorrect display.\n. The logging issue has been fixed in #225 \nI think you may have a different issue with capturing --all-indices.  Have you looked in the Elasticsearch logs?\n. The prefix option must come after a command. \n. A fix for this is already in master. See #183\n. Those flags need to come after the command (show).\u00a0\n\u2014Aaron\nOn Tue, Dec 2, 2014 at 8:29 AM, Matt Hughes notifications@github.com\nwrote:\n\nTrying to specify the timeout as 8 hours using the following command I derived from the README:\nbash-4.1# curator --version\ncurator 2.0.2\nbash-4.1# curator --time=8 --time-unit=hours show  --show-indices\n               ...\ncurator: error: unrecognized arguments: --time-unit=hours\nbash-4.1# curator --time=3 --time-unit=days show  --show-indices\n               ...\ncurator: error: unrecognized arguments: --time-unit=days\nAlso tried with -T and -t but same errors.\nReply to this email directly or view it on GitHub:\nhttps://github.com/elasticsearch/curator/issues/227\n. Timeout should be before, but that suggests that you expect to wait up to 8 hours for Elasticsearch to respond. It should respond in less than 1, certainly less than 10.\u00a0\n\n\u2014Aaron\nOn Tue, Dec 2, 2014 at 8:43 AM, Matt Hughes notifications@github.com\nwrote:\n\nLike this?\nbash-4.1# curator show -t 1 -T days --show-indices\nusage: curator [-h] [-v] [--host HOST] [--url_prefix URL_PREFIX] [--port PORT]\n               [--ssl] [--auth AUTH] [--timeout TIMEOUT] [--master-only] [-n]\n               [-D] [--loglevel LOG_LEVEL] [--logfile LOG_FILE]\n               [--logformat LOGFORMAT]\n               {show,allocation,alias,snapshot,close,bloom,optimize,delete}\n               ...\ncurator: error: unrecognized arguments: -t 1 -T days\nIt doesn't look like they should according to the usage docs.  Although, note that only --timeout is shown; for some reason --time-unit is not there.\nReply to this email directly or view it on GitHub:\nhttps://github.com/elasticsearch/curator/issues/227#issuecomment-65250355\n. Seconds, that is. Not hours, certainly.\u00a0\n\n\u2014Aaron\nOn Tue, Dec 2, 2014 at 8:43 AM, Matt Hughes notifications@github.com\nwrote:\n\nLike this?\nbash-4.1# curator show -t 1 -T days --show-indices\nusage: curator [-h] [-v] [--host HOST] [--url_prefix URL_PREFIX] [--port PORT]\n               [--ssl] [--auth AUTH] [--timeout TIMEOUT] [--master-only] [-n]\n               [-D] [--loglevel LOG_LEVEL] [--logfile LOG_FILE]\n               [--logformat LOGFORMAT]\n               {show,allocation,alias,snapshot,close,bloom,optimize,delete}\n               ...\ncurator: error: unrecognized arguments: -t 1 -T days\nIt doesn't look like they should according to the usage docs.  Although, note that only --timeout is shown; for some reason --time-unit is not there.\nReply to this email directly or view it on GitHub:\nhttps://github.com/elasticsearch/curator/issues/227#issuecomment-65250355\n. The show command doesn't need a timeout. The snapshot command should.\u00a0\n\n\u2014Aaron\nOn Tue, Dec 2, 2014 at 8:50 AM, Matt Hughes notifications@github.com\nwrote:\n\nThis is for doing a snapshot with the default behavior of waiting for the snapshot to complete.  By default, I believe Curator makes this 6 hours so 8 doesn't seem that far fetched to me.  Am I missing something?\nEither way, 2.0.2 doesn't appear to accept --timeout before or after.\nReply to this email directly or view it on GitHub:\nhttps://github.com/elasticsearch/curator/issues/227#issuecomment-65251616\n. Time-unit must come after the command. The wiki documentation on GitHub may help out.\u00a0\n\n\u2014Aaron\nOn Tue, Dec 2, 2014 at 9:06 AM, Matt Hughes notifications@github.com\nwrote:\n\nOdd that discriminates based on the command.  I would think any communication with ES server should just use the --timeout passed to it.\nAnyway, got --timeout working for snapshot, but it won't accept --time-unit.\nbash-4.1# curator --timeout=6 --time-unit=hours --master-only --host localhost --port 9200 snapshot --repository test --all-indices\nusage: curator [-h] [-v] [--host HOST] [--url_prefix URL_PREFIX] [--port PORT]\n               [--ssl] [--auth AUTH] [--timeout TIMEOUT] [--master-only] [-n]\n               [-D] [--loglevel LOG_LEVEL] [--logfile LOG_FILE]\n               [--logformat LOGFORMAT]\n               {show,allocation,alias,snapshot,close,bloom,optimize,delete}\n               ...\ncurator: error: unrecognized arguments: --time-unit=hours\nbash-4.1#\nIs README.md just wrong?  Considering I don't see time-unit in the usage printed out there, I'm guessing yes.\nReply to this email directly or view it on GitHub:\nhttps://github.com/elasticsearch/curator/issues/227#issuecomment-65254574\n. Elasticsearch-level flags go before the command, command-level flags go after the command\u00a0\n\n\u2014Aaron\nOn Tue, Dec 2, 2014 at 9:07 AM, Aaron Mildenstein aaron@mildensteins.com\nwrote:\n\nTime-unit must come after the command. The wiki documentation on GitHub may help out.\u00a0\n\u2014Aaron\nOn Tue, Dec 2, 2014 at 9:06 AM, Matt Hughes notifications@github.com\nwrote:\n\nOdd that discriminates based on the command.  I would think any communication with ES server should just use the --timeout passed to it.\nAnyway, got --timeout working for snapshot, but it won't accept --time-unit.\nbash-4.1# curator --timeout=6 --time-unit=hours --master-only --host localhost --port 9200 snapshot --repository test --all-indices\nusage: curator [-h] [-v] [--host HOST] [--url_prefix URL_PREFIX] [--port PORT]\n               [--ssl] [--auth AUTH] [--timeout TIMEOUT] [--master-only] [-n]\n               [-D] [--loglevel LOG_LEVEL] [--logfile LOG_FILE]\n               [--logformat LOGFORMAT]\n               {show,allocation,alias,snapshot,close,bloom,optimize,delete}\n               ...\ncurator: error: unrecognized arguments: --time-unit=hours\nbash-4.1#\nIs README.md just wrong?  Considering I don't see time-unit in the usage printed out there, I'm guessing yes.\nReply to this email directly or view it on GitHub:\nhttps://github.com/elasticsearch/curator/issues/227#issuecomment-65254574\n. That's a worthy idea.  Thank you for submitting it.  As this would represent a (minor) API change, it will likely not happen until Curator 3.0, though being additive, it could appear sooner.\n. The --request-timeout flag is now available for the optimize and snapshot commands with v3.\n. Can you paste an actual example?\n. Thank you.  Can you also send the command-line used to run this?\n. There it is.  You omitted the hyphen after logstash.  It should read --prefix logstash-.  Please see https://github.com/elasticsearch/curator/wiki/Prefix, namely:\n\nThe prefix in this example is logstash-, or everything up to the timestring of YYYY.MM.dd. \n\nIf your indices are all logstash-YYYY.MM.dd then you don't even need the prefix directive as the default is already logstash-.\n. The same thing will appear with Marvel indices, but the prefix will be .marvel-\n. The bottom line is that the prefix must be exact or the regular expressions used to split prefix/timestamp/suffix won't know where the division is.\n. Glad to help! Feel free to open another issue if you have trouble.\n. Closed by #233 \n. This passes tests against 1.4.1 and 1.5.0-SNAPSHOT\n. Can you show me an example of your command-line and a few of the index names you're trying to process?\n. @bhennigar Can you provide a few of the index names you expect curator to prune?  UTC can be blamed at the hourly level, but not the daily level, unless you only have 3 days of data.\n. @bhennigar you have 3 indices there, and they have hours in them.  Are they daily or hourly indices?  I see bro-%Y%m%d%H%M, or year, month, day, hour, minute. You will have to tell Curator to expect that pattern because by default, the timestring is %Y.%m.%d.  Curator also can't handle minutes, so you may need to put 00 at the end of your timestring.\n```\ncurator delete --help\nusage: curator delete [-h] [--timestring TIMESTRING] [--prefix PREFIX]\n...\noptional arguments:\n...\n  --timestring TIMESTRING\n                        Python strftime string to match your index definition,\n                        e.g. 2014.07.15 would be %Y.%m.%d\n...\n  --time-unit TIME_UNIT\n                        Unit of time to reckon by: [hours|days|weeks|months]\n                        Default: days\n...\n```\nYour command-line should look more like:\ncurator delete --time-unit hours --timestring '%Y%m%d%H00' --older-than 3 --prefix bro-\n. @bhennigar please add the --debug flag before the delete command.  Paste the output in a comment.\n. And include the command-line, please.\n. @Grauen thanks for the report.  Glad you got things figured out.\n. Ah, my bad.  @bhennigar The 00 shouldn't be part of the timestamp.  Try this command-line:\ncurator delete --time-unit hours --timestring '%Y%m%d%H' --older-than 3 --prefix bro- --suffix 00\n. Encapsulate long output in \\```` triple back ticks on their own line above & below the trace.  Read up on quotes in the github markdown documentation.\n. Found it.  It appears your operations are taking a rather long time, so curator is timing out.[status:N/A request:30.035s]Try adding a--timeoutflag (before the command) with a value of 900 or something.  The default 30 second timeout is too slow.\n. Glad to help!\n. I contemplated adding a yearly option to Curator, but that initially seemed highly unlikely to be used.  If you really have indices that are rotated that infrequently (every 2 years), I advise using the Curator API and writing a custom script.  Something that infrequently performed is more easily handled by a bare curl command, e.g.curl -XDELETE localhost:9200/my-index-2012?pretty`.  It's not as handy as the curator command, perhaps, but you are only having to run that every year or two.\n. Some operations are supported with aliases.  Operations like delete and close need to be performed on the actual index and not the alias.\n. It's possible that there's a UTC collision somewhere.\nCan you paste an example command-line and your index name pattern?\n. This is definitely not supposed to happen.  What version of curator are you using?\nIn the current version of curator, the delete method calls either filter_by_timestamp or filter_by_space (in your case, by space).  filter_by_space calls get_indices which filters by prefix.  The get_indices method filters by prefix and suffix flawlessly for all other use cases, so I'm quite curious as to why yours is not working.  Reference code.\nCan you re-run your command with -D or --debug and paste the output? \nAs a side note, I see that you have multiple indices.  This is a red flag for me.  If you have multiple indices and/or multiple nodes you should not be using delete by space.  Delete by space is a hack that is really only present for very small (preferably one node) clusters.  It becomes astronomically harder to accurately free up space across multiple nodes, to say nothing of multiple indices.  Please see the Caveats on the wiki for more information.\n. kibana-int is already filtered, as is .marvel-kibana.\nCan you provide the debug output, as requested?\n. I cannot replicate the problem described.\nSteps to replicate\n\nInstall clean elasticsearch 1.4.1 instance\n\n\nbuh@Aironaut (10:32 AM) [~/logstash-1.4.2] \uf8ff curl localhost:9200\n{\n  \"status\" : 200,\n  \"name\" : \"Thing\",\n  \"cluster_name\" : \"untergeek_testing\",\n  \"version\" : {\n    \"number\" : \"1.4.1\",\n    \"build_hash\" : \"89d3241d670db65f994242c8e8383b169779e2d4\",\n    \"build_timestamp\" : \"2014-11-26T15:49:29Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"4.10.2\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n- Create dummy indices\nfor index in \"test-2014.12.18 mesos-info-2014.12.18 mesos-offers-2014.12.18 mesos-warning-2014.12.18\"; do\n  for ((i=1;i<1000;i++)); do curl -XPUT localhost:9200/$index/logs/$i -d '{ \"doc\": \"foo\",\"bar\" : \"baz\" }'; done\ndone\n- Show indices \nbuh@Aironaut (10:33 AM) [~/logstash-1.4.2] \uf8ff curator show --show-indices --prefix \\*\nmesos-info-2014.12.18\nmesos-offers-2014.12.18\nmesos-warning-2014.12.18\ntest-2014.12.18\n- Run curator as provided\nbuh@Aironaut (10:33 AM) [~/logstash-1.4.2] \uf8ff curator delete --prefix test- --disk-space 0.000001\n2014-12-30 10:33:37,577 INFO      Job starting...\n2014-12-30 10:33:37,580 INFO      Deleting indices...\n2014-12-30 10:33:37,580 INFO      Deleting by space rather than time.\n2014-12-30 10:33:37,628 INFO      delete_index operation succeeded on test-2014.12.18\n2014-12-30 10:33:37,628 INFO      Speficied indices deleted.\n2014-12-30 10:33:37,628 INFO      Done in 0:00:00.058232.\n\nWhat version of Elasticsearch are you using?  Which python version?  What operating system?\n. I apologize for the difficulty you faced.  Are you sure you're using the most recent version?  The error you encountered was reported in #254 and patched in #256 and was released in version 2.1.2.\nParsing your debug log I found that you had these indices:\nintape-rsyslog-2015-02-14\nintape-rsyslog-2015-02-13\nintape-rsyslog-2015-02-12\nintape-rsyslog-2015-02-11\nrsyslog-2015-02-14\nrsyslog-2015-02-12\nrsyslog-2015-02-11\nrsyslog-2015-02-13\nkibana-int\nvideo\nand you passed a --prefix of logs-.  The error was triggered by using a prefix that was not found at all (#254).\nThe error in question is closed by #256, so I'm closing the ticket.\n. This has been requested before, but you've provided a use case I can get behind.\nThis is something that, if added, will not be added until Curator 3.0.  The reason for the delay is that the goal of Curator 3.0 is to use a \"pipeline\" for configuration options.  This will allow all indices to be filtered by all criteria.  For instance, an option would be to use --older-than 60 in conjunction with --younger-than 90.\n. Opening indices is now in v3.0.0\n. This will get some attention as beta releases of Elasticsearch 2.0 become available.\n. As this is dependent on Lucene 5.0, this will only be doable when I start testing with Elasticsearch 2.0 builds.\n. It looks like this will be addressed by Curator, but not directly by enabling or disabling this feature.\nFrom this post:\n\nAlternatively, we expect many users will want to utilize a \u201chot/warm\u201d architecture using the shard allocation filtering feature. In this scenario, time-based indexes on hot nodes can be configured to be created with the default LZ4 compression; when they\u2019re migrated to the warm nodes (configured with index.codec: best_compression in elasticsearch.yml), the indexes can be compressed by optimizing that index. It may be preferable to pay the CPU penalty of compression during this optimize process (which we often recommend executing when the cluster is known to be less utilized) than at the time of initial indexing.\n\nWith this being the case, it seems that Curator's existing shard allocation functionality will address this need.\n. I addressed this at length in #174 with this comment.\nAnother issue is #190 \nThe bottom-line is that unless your indices change between snapshots, you will not be re-backing them up, because snapshots are at the segment level.\nIf you still require single-index snapshots, I suggest writing your own script that uses the Elasticsearch Curator API.\n. But this is why with snapshots it's more common to snapshot --older-than and let them just stack up.  This way you're not \"re-\"snapshotting your older indices (they've been captured previously), and are only adding the most recent snapshots --older-than the time specified.\nBest of luck.\n. See if the changes in v3 help.  Closing for now.\n. Try increasing the timeout value --timeout.  The default 30 seconds does not appear to be enough in your use case.\n. optimize would work fine because it automatically implements a 21600 second timeout (if you don't override with your own).  close probably works because Elasticsearch may not have to worry about shard allocation after closing.\n. Any report?\n. Connection aborted indicates something outside of curator (note that it's http_urllib3).\n2 minutes (120 seconds) may yet be too short, or something else is going on here which is beyond the scope of curator itself.\n. Interesting.  Does it work if you try a delete by time? Test with the -n flag to prevent it from actually deleting anything.\n. #252 is an import error and is unrelated to this. \n. Please retry this with v3.\n. Going to close this for now.  If issues persist, please feel free to re-open.\n. Your workaround is valid.  You have a huge number of indices.\nBecause this is coming from Elasticsearch, I have no way of addressing this directly.  @HonzaKral may know something about this I do not, as it comes from the elasticsearch-py module.\n. I may write in a way to check the length of the index list before acting, then.\n. @AlexeyKupershtokh please share the conditions which led to this.  It should not be possible in 3.0.3 because of line 112, which reads:\npython\nif len(to_csv(working_list)) > 3072 and not ctx.parent.info_name == 'snapshot':\nIf it's bigger than 3072 bytes, it calls the chunk_index method.  I chose this number because it seemed implausible to hit 4k with nearly 1k of room to breathe.  \nWhat kind of call did this?  Did you have some extra URL parameters?  A really long url or extension?  Are your index names just huge? If I have to reduce this number further, I can.  I'd like some hard data to back it up, though.\n. You're using the --disk-space flag. That's why this is getting triggered.  You have too many indices for the query for the delete by space, and I did not apply the logic to prevent over-sized lists of indices to that method.\nHere's the offending line:\nhttps://github.com/elastic/curator/blob/master/curator/api/filter.py#L337\nIt will need to be updated to batch the results in smaller chunks, then re-assemble them into a single array again.  I don't like having to add this (but I will to keep it from breaking) because it suggests a massive-scale usage of delete by space, which is NotGood\u2122, for a variety of reasons, including the ones quoted below.  Please see: http://www.elastic.co/guide/en/elasticsearch/client/curator/current/delete.html#_deleting_indices_by_space\n\nDeleting Indices By Space\nThis option is for those who want to retain indices based on disk consumption, rather than by a set number of days. There are some important caveats surrounding this choice.\nCaveats\n- Elasticsearch cannot calculate the size of closed indices. Elasticsearch does not keep tabs on how much disk-space closed indices consume. If you close indices, your space calculations will be inaccurate.\n- Indices consume resources just by existing. You could run into performance and/or operational snags in Elasticsearch as the count of indices climbs.\n- You need to manually calculate how much space across all nodes. The total you give will be the sum of all space consumed across all nodes in your cluster. If you use shard allocation to put more shards or indices on a single node, it will not affect the total space reported by the cluster, but you may still run out of space on that node.\nThese are only a few of the caveats. This is still a valid use-case, especially for those running a single-node test box, however, so we include this option for your convenience.\n\nEven if you change your shard allocation per node to be by space, you have no idea which indices will keep what shards on what nodes.  Deleting the \"oldest\" indices could free up space on 2 nodes, but leave several others completely full, which could result in a lot of extra shard reallocation just to balance the cluster.  \nIt is much wiser to use monitoring software to see what size n days worth of indices can accommodate and then keep that many indices by time, rather than by space.\nYou also are feeding a list of both your stage and production indices to be acted on.  Delete by space will simply feed the indices alphabetically (or reverse, if that flag is chosen), as it comes after the other filters.  This means you could potentially delete entire indices you don't want to delete, and not free the space you actually care about.\nWhile I will fix this, because I don't like the potential of this bug to keep coming up, the fact that you are hitting it is indicative of a potentially dangerous approach to index management.\n. > I don't like situations when it stops writing, start losing newest data and require additional ops to recover from this state.\nThis actually results more often from having too many shards per node, rather than too much data on the server.  See https://github.com/elastic/curator/pull/201#issuecomment-59767589\n\nI can't tell you exactly when that will be with your data and usage pattern, but I will point out that by default, an ES data node will set aside 10% of the heap for indexing (caching, operations, etc.). Each active shard will want no less than 250M of that space. Each inactive shard will want no less than 4MB of that space. Elasticsearch will still force the division to result in values lower than those numbers in the event that this heap space becomes more crowded. If it pushes too far, you will be unable to index. The only way out when this happens is to delete or close indices and usually a node restart. If you need indices to persist beyond this, your options are one or more of the following: a) bigger heap [not to exceed 31G per node], b) closing infrequently accessed indices, c) adding more nodes to the system to rebalance shard count per node.\n\nYou have one node.  If you are using the default number of shards per index, which is 5, you have 5 shards multiplied by a rather large number of indices.  You are likely putting a great deal of pressure on that 10% of the heap, perhaps even to the point that it will, as mentioned, stop indexing altogether, until Elasticsearch is restarted.  Any production, time-series use of Elasticsearch should be prepared to be extended horizontally by adding more nodes as needed.  This is the other reason why delete by space is NotGood\u2122, because Elasticsearch should be spread across multiple nodes.\n. > Though I wonder what else approaches could be used to automatically free space in cluster environments with many index series (like stage and prod multiplied by different projects writing logs) ?\nThis is why deleting by time and filtering by prefix is available in Curator.  You can easily keep n days of staging indices, and x days of production indices in this way.\n. @bobrik This will be easily addressed with the pipelining feature pending in 3.0.\n. However, if you want to do a short-term PR for count validation, I will accept it and merge it to master.\n. While checking replica count could help, most of the need for this is obviated with better index filtering, in v3.  Also, all matching indices get the replica count updated simultaneously in v3.\nI have removed these checks for now.  If they are desired, we can re-add them to v3.\n. LGTM!\n. That is why the code was updated with json.dumps in the places where that could happen.\nhttps://github.com/elasticsearch/curator/search?utf8=\u2713&q=json.dumps\nIt's one of the changes in Curator 2.1.0\nIf there are other places this can happen that I have missed, I would prefer to keep the usage similar.\n. Looks like json.dumps didn't wrap that properly.  I wonder why not. Looks like a better solution is in order.\nSo, what does your PR look like?\n. Yes, the CLA covers Curator as well.\n. Closed by #250\n. --all-indices is to enable Curator to snapshot all indices, time-series or not.  This is by design.  If you are trying to snapshot a collection of non-timeseries indices, your current options are to snapshot everything via the --all-indices option, or to write your own script that uses the Curator API so you don't have to wrap your own REST calls in something else.\n. lgtm!\n. When all was unified, it did work with python 3.  Since the split of the API to curator.py and the script functions to curator_script.py, some of the args in curator_script.py do not like Python 3.  It will require a refactor.\n. The API passes all tests with python 3, by the way.\n. I have tested curator v3.0.0-beta2 with Python 3.4.2 and all tests are passing.\n. The ReadTimeoutError suggests you should increase the --timeout parameter. The default is 39 seconds. Try increasing that to 60 or 120.\u00a0\n\u2014Aaron\nOn Sat, Jan 17, 2015 at 5:04 AM, bhennigar notifications@github.com\nwrote:\n\nCurator will no longer run after successfully running nightly for a couple weeks. There has been no change to the system. No updates run. Ubuntu 14.04 64bit. ElasticSearch 1.4.2. \n```\ncurator --debug --host localhost --port 9200 --timeout 900 delete --time-unit days --timestring '%Y%m%d%H' --older-than 10 --prefix bro- --suffix 00\nTraceback (most recent call last):\n  File \"./curator\", line 9, in \n    load_entry_point('elasticsearch-curator==2.1.1', 'console_scripts', 'curator')()\n  File \"/var/www/Brownian/local/lib/python2.7/site-packages/pkg_resources/init.py\", line 474, in load_entry_point\n    return get_distribution(dist).load_entry_point(group, name)\n  File \"/var/www/Brownian/local/lib/python2.7/site-packages/pkg_resources/init.py\", line 2582, in load_entry_point\n    return ep.load()\n  File \"/var/www/Brownian/local/lib/python2.7/site-packages/pkg_resources/init.py\", line 2265, in load\n    return self._load()\n  File \"/var/www/Brownian/local/lib/python2.7/site-packages/pkg_resources/init.py\", line 2268, in _load\n    module = import(self.module_name, fromlist=['name'], level=0)\n  File \"/var/www/Brownian/local/lib/python2.7/site-packages/curator/init.py\", line 1, in \n    from .curator import *\n  File \"/var/www/Brownian/local/lib/python2.7/site-packages/curator/curator.py\", line 7, in \n    import elasticsearch\n  File \"/var/www/Brownian/local/lib/python2.7/site-packages/elasticsearch/init.py\", line 18, in \n    from .client import Elasticsearch\n  File \"/var/www/Brownian/local/lib/python2.7/site-packages/elasticsearch/client/init.py\", line 5, in \n    from ..transport import Transport\n  File \"/var/www/Brownian/local/lib/python2.7/site-packages/elasticsearch/transport.py\", line 4, in \n    from .connection import Urllib3HttpConnection\n  File \"/var/www/Brownian/local/lib/python2.7/site-packages/elasticsearch/connection/init.py\", line 3, in \n    from .http_urllib3 import Urllib3HttpConnection\n  File \"/var/www/Brownian/local/lib/python2.7/site-packages/elasticsearch/connection/http_urllib3.py\", line 3, in \n    from urllib3.exceptions import ReadTimeoutError, SSLError as UrllibSSLError\nImportError: cannot import name ReadTimeoutError\n```\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/elasticsearch/curator/issues/252\n. Sorry, my phone wasn't a big enough screen to see everything at once.\n\nOn closer inspection, the ReadTimeoutError is actually an ImportError.  It can't import something necessary.  I'm not sure what changed on your system, but something had to have changed.  \nIn the elasticsearch python module, identified in your path as: /var/www/Brownian/local/lib/python2.7/site-packages/elasticsearch/connection/http_urllib3.py, there's an import line:\npython\nfrom urllib3.exceptions import ReadTimeoutError, SSLError as UrllibSSLError\nIt can't find the ReadTimeoutError module within the urllib3.exceptions .  That makes this error part of the URLLib3 package which is not controlled by the curator installation, except being installed as a dependency of the elasticsearch module installation.\nYou could perhaps uninstall/reinstall the elasticsearch module to see if dependent modules get re-installed.  Chances are, however, you may need to uninstall/reinstall the urllib3 python module with\npip uninstall urllib3\npip install urllib3\n\u2026and see what happens.\n. I'm doing a clean install on Ubuntu 14.04 64-bit as well.\n```\nbuh@Elasticbox (12:05 PM) [~] \u00bb head -1 /etc/apt/sources.list\ndeb cdrom:[Lubuntu 14.04 LTS Trusty Tahr - Release amd64 (20140416.2)]/ trusty main multiverse restricted universe\nbuh@Elasticbox (12:05 PM) [~] \u00bb uname -a\nLinux Elasticbox 3.13.0-40-generic #69-Ubuntu SMP Thu Nov 13 17:53:56 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux\nbuh@Elasticbox (12:05 PM) [~] \u00bb curator\ncurator: command not found\nbuh@Elasticbox (12:05 PM) [~] \u00bb sudo pip2 install elasticsearch-curator\nDownloading/unpacking elasticsearch-curator\n  Downloading elasticsearch-curator-2.1.1.tar.gz\n  Running setup.py (path:/tmp/pip_build_root/elasticsearch-curator/setup.py) egg_info for package elasticsearch-curator\nwarning: no files found matching 'CHANGELOG'\nwarning: no previously-included files matching '__pycache__' found under directory '*'\nwarning: no previously-included files matching '*.py[co]' found under directory '*'\n\nDownloading/unpacking elasticsearch>=1.0.0,<2.0.0 (from elasticsearch-curator)\n  Downloading elasticsearch-1.3.0-py2.py3-none-any.whl (55kB): 55kB downloaded\nDownloading/unpacking urllib3>=1.8,<2.0 (from elasticsearch>=1.0.0,<2.0.0->elasticsearch-curator)\n  Downloading urllib3-1.10-py2-none-any.whl (75kB): 75kB downloaded\nInstalling collected packages: elasticsearch-curator, elasticsearch, urllib3\n  Running setup.py install for elasticsearch-curator\nwarning: no files found matching 'CHANGELOG'\nwarning: no previously-included files matching '__pycache__' found under directory '*'\nwarning: no previously-included files matching '*.py[co]' found under directory '*'\nInstalling curator script to /usr/local/bin\nInstalling es_repo_mgr script to /usr/local/bin\n\nSuccessfully installed elasticsearch-curator elasticsearch urllib3\nCleaning up...\nbuh@Elasticbox (12:06 PM) [~] \u00bb curator --debug --host localhost --port 9200 --timeout 900 delete --time-unit days --timestring '%Y%m%d%H' --older-than 10 --prefix bro- --suffix 00\n2015-01-17 12:06:06,530 INFO                        root                   main:334  Job starting...\n2015-01-17 12:06:06,530 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-01-17 12:06:06,530 INFO      urllib3.connectionpool              _new_conn:188  Starting new HTTP connection (1): localhost\n2015-01-17 12:06:06,531 DEBUG     urllib3.connectionpool          _make_request:368  \"GET / HTTP/1.1\" 200 334\n2015-01-17 12:06:06,531 INFO               elasticsearch    log_request_success:62   GET http://localhost:9200/ [status:200 request:0.001s]\n2015-01-17 12:06:06,531 DEBUG              elasticsearch    log_request_success:64   > None\n2015-01-17 12:06:06,531 DEBUG              elasticsearch    log_request_success:65   < {\n  \"status\" : 200,\n  \"name\" : \"Elasticbox\",\n  \"cluster_name\" : \"untergeek\",\n  \"version\" : {\n    \"number\" : \"1.4.2\",\n    \"build_hash\" : \"927caff6f05403e936c20bf4529f144f0c89fd8c\",\n    \"build_timestamp\" : \"2014-12-16T14:11:12Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"4.10.2\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n2015-01-17 12:06:06,531 INFO         elasticsearch.trace    log_request_success:72   curl -XGET 'http://localhost:9200/?pretty' -d ''\n2015-01-17 12:06:06,531 DEBUG        elasticsearch.trace    log_request_success:75   #[200] (0.001s)\n{\n\"cluster_name\": \"untergeek\",\n\"name\": \"Elasticbox\",\n\"status\": 200,\n\"tagline\": \"You Know, for Search\",\n\"version\": {\n\"build_hash\": \"927caff6f05403e936c20bf4529f144f0c89fd8c\",\n\"build_snapshot\": false,\n\"build_timestamp\": \"2014-12-16T14:11:12Z\",\n\"lucene_version\": \"4.10.2\",\n\"number\": \"1.4.2\"\n}\n}\n2015-01-17 12:06:06,531 DEBUG     curator.curator_script          check_version:228  Detected Elasticsearch version 1.4.2\n2015-01-17 12:06:06,531 DEBUG                       root                   main:359  Matching indices with pattern: bro-%Y%m%d%H\n2015-01-17 12:06:06,531 DEBUG                       root                   main:363  argdict = {'url_prefix': '', 'func': , 'prefix': 'bro-', 'log_level': 'INFO', 'timestring': '%Y%m%d%H', 'dry_run': False, 'exclude_pattern': None, 'logformat': 'Default', 'auth': None, 'ssl': False, 'host': 'localhost', 'command': 'delete', 'time_unit': 'days', 'timeout': 900, 'debug': True, 'disk_space': None, 'log_file': None, 'master_only': False, 'port': 9200, 'older_than': 10, 'suffix': '00'}\n2015-01-17 12:06:06,531 INFO                        root                 delete:1017 Deleting indices...\n2015-01-17 12:06:06,531 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-01-17 12:06:06,536 DEBUG     urllib3.connectionpool          _make_request:368  \"GET //_settings?expand_wildcards=open%2Cclosed HTTP/1.1\" 200 12709\n2015-01-17 12:06:06,536 INFO               elasticsearch    log_request_success:62   GET http://localhost:9200//_settings?expand_wildcards=open%2Cclosed [status:200 request:0.005s]\n2015-01-17 12:06:06,537 DEBUG              elasticsearch    log_request_success:64   > None\n...\n...\n...\n2015-01-17 12:06:06,539 INFO             curator.curator                 delete:1026 Specified indices deleted.\n2015-01-17 12:06:06,539 INFO      curator.curator_script                   main:366  Done in 0:00:00.019744.\n```\nI do not know what you're encountering, but it is not likely to be part of curator itself.\n. Fascinating.  I wonder what's going on with this.\n. :+1: \n. I appreciate this!  Thank you for making Curator more awesome.\nUnfortunately, I am changing the whole pipeline for Curator 3.0 and this functionality will be included in a different way.  As such, I will not be merging this into master right now because it will be in conflict with the future release.  By all means, keep using your forked branch to have this functionality until then.\nThe new 3.0 pipeline will allow multiple filtration methods simultaneously.  For example,  --older-than and --newer-than could both be used at once to have a custom filtered list of indices.\n. I'm spending most of my time on Logstash 1.5 with the rest of my team.  I wish I could give you more information, but it will be a month or two, I fear.\n. v3 is at RC1 right now.  Please see if the combination of --older-than and --newer-than work for you now, along with the other pipelined filters available.\n. Can you provide the command-line that ran this, so I can attempt to replicate the scenario that caused the error?\nAt first glance, it appears that it's trying to perform an operation on an index that no longer exists.  Do you have curator running as a cron job?  Did the previous day's cron job finish before it ran again?\n. It's not a race condition occurring within curator.  If your cluster is really busy the cluster updates may not be fully propagated from the elected master to all of the other nodes.  In such a case, it is possible that the node curator is polling has an out-of-date cluster state, and it shows the index as being present when it has already been deleted.\nOne possible solution is to run curator on all nodes, but with the --master-only flag set.  This will force curator to only perform operations on the elected master, and it will start, but quietly exit on the non-elected master nodes.  If curator is only ever run against the elected master, this \"race\" condition cannot occur because the elected master will always have the most up-to-date cluster state.\n. @rk295 Thanks for helping to debug this.\nI cannot duplicate this, so I'm not sure what's going on.  You could go through a process to test this same thing:\n```\nPython 2.7.6 (default, Sep  9 2014, 15:04:36)\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.39)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport curator\nimport elasticsearch\nclient = elasticsearch.Elasticsearch(host=\"192.168.123.204\")\nindex_name = \"logstash-2015.01.30\"\nclient.indices.segments(index=index_name)['indices'][index_name]['shards']\n{u'1': [{u'segments': {u'_ev4': {u'search': True, u'num_docs': 2921899, u'deleted_docs': 0, u'generation': 19264, u'size_in_bytes': 261485794, u'compound': False, u'version': u'4.10.2', u'committed': True, u'memory_in_bytes': 2053962}, u'_ev5': {u'search': True, u'num_docs': 1, u'deleted_docs': 0, u'generation': 19265, u'size_in_bytes': 6122, u'compound': True, u'version': u'4.10.2', u'committed': True, u'memory_in_bytes': 19778}}, u'num_search_segments': 2, u'routing': {u'node': u'pREk39dmQCCYZvTWzHx_sg', u'state': u'STARTED', u'primary': True}, u'num_committed_segments': 2}, {u'segments': {u'_euk': {u'search': True, u'num_docs': 2921899, u'deleted_docs': 0, u'generation': 19244, u'size_in_bytes': 261460273, u'compound': False, u'version': u'4.10.2', u'committed': True, u'memory_in_bytes': 2053954}, u'_eul': {u'search': True, u'num_docs': 1, u'deleted_docs': 0, u'generation': 19245, u'size_in_bytes': 6122, u'compound': True, u'version': u'4.10.2', u'committed': True, u'memory_in_bytes': 19778}}, u'num_search_segments': 2, u'routing': {u'node': u'jVKQ2johRn6AZmGrkvXWXA', u'state': u'STARTED', u'primary': False}, u'num_committed_segments': 2}], u'0': [{u'segments': {u'_ev4': {u'search': True, u'num_docs': 1, u'deleted_docs': 0, u'generation': 19264, u'size_in_bytes': 5835, u'compound': True, u'version': u'4.10.2', u'committed': True, u'memory_in_bytes': 19778}, u'_ev2': {u'search': True, u'num_docs': 1, u'deleted_docs': 0, u'generation': 19262, u'size_in_bytes': 6121, u'compound': True, u'version': u'4.10.2', u'committed': True, u'memory_in_bytes': 19778}, u'_ev3': {u'search': True, u'num_docs': 1, u'deleted_docs': 0, u'generation': 19263, u'size_in_bytes': 5827, u'compound': True, u'version': u'4.10.2', u'committed': True, u'memory_in_bytes': 19778}, u'_ev0': {u'search': True, u'num_docs': 2921117, u'deleted_docs': 0, u'generation': 19260, u'size_in_bytes': 261317239, u'compound': False, u'version': u'4.10.2', u'committed': True, u'memory_in_bytes': 2057674}, u'_ev1': {u'search': True, u'num_docs': 1, u'deleted_docs': 0, u'generation': 19261, u'size_in_bytes': 6122, u'compound': True, u'version': u'4.10.2', u'committed': True, u'memory_in_bytes': 19778}}, u'num_search_segments': 5, u'routing': {u'node': u'pREk39dmQCCYZvTWzHx_sg', u'state': u'STARTED', u'primary': False}, u'num_committed_segments': 5}, {u'segments': {u'_euq': {u'search': True, u'num_docs': 1, u'deleted_docs': 0, u'generation': 19250, u'size_in_bytes': 5835, u'compound': True, u'version': u'4.10.2', u'committed': True, u'memory_in_bytes': 19778}, u'_eup': {u'search': True, u'num_docs': 1, u'deleted_docs': 0, u'generation': 19249, u'size_in_bytes': 5827, u'compound': True, u'version': u'4.10.2', u'committed': True, u'memory_in_bytes': 19778}, u'_euo': {u'search': True, u'num_docs': 1, u'deleted_docs': 0, u'generation': 19248, u'size_in_bytes': 6121, u'compound': True, u'version': u'4.10.2', u'committed': True, u'memory_in_bytes': 19778}, u'_eun': {u'search': True, u'num_docs': 1, u'deleted_docs': 0, u'generation': 19247, u'size_in_bytes': 6122, u'compound': True, u'version': u'4.10.2', u'committed': True, u'memory_in_bytes': 19778}, u'_eum': {u'search': True, u'num_docs': 2921117, u'deleted_docs': 0, u'generation': 19246, u'size_in_bytes': 261364679, u'compound': False, u'version': u'4.10.2', u'committed': True, u'memory_in_bytes': 2057170}}, u'num_search_segments': 5, u'routing': {u'node': u'jVKQ2johRn6AZmGrkvXWXA', u'state': u'STARTED', u'primary': True}, u'num_committed_segments': 5}], u'3': [{u'segments': {u'_euu': {u'search': True, u'num_docs': 1, u'deleted_docs': 0, u'generation': 19254, u'size_in_bytes': 5913, u'compound': True, u'version': u'4.10.2', u'committed': True, u'memory_in_bytes': 19778}, u'_eut': {u'search': True, u'num_docs': 2921717, u'deleted_docs': 0, u'generation': 19253, u'size_in_bytes': 261563657, u'compound': False, u'version': u'4.10.2', u'committed': True, u'memory_in_bytes': 2056682}}, u'num_search_segments': 2, u'routing': {u'node': u'pREk39dmQCCYZvTWzHx_sg', u'state': u'STARTED', u'primary': True}, u'num_committed_segments': 2}, {u'segments': {u'_euu': {u'search': True, u'num_docs': 1, u'deleted_docs': 0, u'generation': 19254, u'size_in_bytes': 5913, u'compound': True, u'version': u'4.10.2', u'committed': True, u'memory_in_bytes': 19778}, u'_eut': {u'search': True, u'num_docs': 2921717, u'deleted_docs': 0, u'generation': 19253, u'size_in_bytes': 261529369, u'compound': False, u'version': u'4.10.2', u'committed': True, u'memory_in_bytes': 2057210}}, u'num_search_segments': 2, u'routing': {u'node': u'jVKQ2johRn6AZmGrkvXWXA', u'state': u'STARTED', u'primary': False}, u'num_committed_segments': 2}], u'2': [{u'segments': {u'_ev6': {u'search': True, u'num_docs': 2921177, u'deleted_docs': 0, u'generation': 19266, u'size_in_bytes': 261469390, u'compound': False, u'version': u'4.10.2', u'committed': True, u'memory_in_bytes': 2060394}}, u'num_search_segments': 1, u'routing': {u'node': u'pREk39dmQCCYZvTWzHx_sg', u'state': u'STARTED', u'primary': False}, u'num_committed_segments': 1}, {u'segments': {u'_eus': {u'search': True, u'num_docs': 2921177, u'deleted_docs': 0, u'generation': 19252, u'size_in_bytes': 261473201, u'compound': False, u'version': u'4.10.2', u'committed': True, u'memory_in_bytes': 2059530}}, u'num_search_segments': 1, u'routing': {u'node': u'jVKQ2johRn6AZmGrkvXWXA', u'state': u'STARTED', u'primary': True}, u'num_committed_segments': 1}], u'4': [{u'segments': {u'_evb': {u'search': True, u'num_docs': 2921902, u'deleted_docs': 0, u'generation': 19271, u'size_in_bytes': 261494542, u'compound': False, u'version': u'4.10.2', u'committed': True, u'memory_in_bytes': 2061994}}, u'num_search_segments': 1, u'routing': {u'node': u'pREk39dmQCCYZvTWzHx_sg', u'state': u'STARTED', u'primary': False}, u'num_committed_segments': 1}, {u'segments': {u'_euo': {u'search': True, u'num_docs': 2921902, u'deleted_docs': 0, u'generation': 19248, u'size_in_bytes': 261482203, u'compound': False, u'version': u'4.10.2', u'committed': True, u'memory_in_bytes': 2060682}}, u'num_search_segments': 1, u'routing': {u'node': u'jVKQ2johRn6AZmGrkvXWXA', u'state': u'STARTED', u'primary': True}, u'num_committed_segments': 1}]}\n```\n\n\n\nIf it works standalone but not with curator, there is a strong possibility that curator is not getting results back properly from your node, hence the empty key.  How busy is your single node?\nAlso, your elasticsearch python module is a version behind.  I think the most recent is 1.3.0.  Also, which version of Elasticsearch are you running against?\n. This is a fascinating case, that's for sure.\nThis is beyond atypical.  You apparently have an index called logstash-2015.01.14, and Logstash thinks it has 10 shards (5 primary, +1 replica each), but exactly 0 of those were successful.   Even more confusing is that 0 of those are listed as failed as well.\nIndeed, this can be worked around by checking to ensure that indices is not empty.  This is such a rare case that while I can add a check for this and continue, I think it would be wiser to show an error that a strange, empty index exists.\nI will file a bug for this.\n. @rk295 See the change in #265 \n. Have you tried encapsulating in single quotes?  The . in api.transactions may be interpreted as a regular expression.\n--prefix 'api.transactions-'\nAlso, can you attach the run output with the --debug flag set?  That will help me see what else is/may be going on.\nAlso, what is the timestring?  Does it differ from the default?\n. Also possible, since it's interpreted as a regex, is using --prefix 'api\\.transactions-'\n. Hmmm.  I have tried to duplicate this and I am not having a problem with delete operations:\nI created a pile of indices and use curator to show them (it's curator_script.py because I'm in my local github branch):\nbuh@Aironaut (11:08 AM) [~/WORK/curator/curator] \uf8ff ./curator_script.py show --show-indices --prefix api.transactions-\napi.transactions-2015.01.21\napi.transactions-2015.01.22\napi.transactions-2015.01.23\napi.transactions-2015.01.24\napi.transactions-2015.01.25\napi.transactions-2015.01.26\napi.transactions-2015.01.27\nAnd then I try to delete the indices older than 3 days:\nbuh@Aironaut (11:08 AM) [~/WORK/curator/curator] \uf8ff ./curator_script.py delete --older-than 3 --prefix api.transactions-\n2015-01-28 11:09:13,657 INFO      Job starting...\n2015-01-28 11:09:13,661 INFO      Deleting indices...\n2015-01-28 11:09:13,706 INFO      delete_index operation succeeded on api.transactions-2015.01.21\n2015-01-28 11:09:13,726 INFO      delete_index operation succeeded on api.transactions-2015.01.22\n2015-01-28 11:09:13,744 INFO      delete_index operation succeeded on api.transactions-2015.01.23\n2015-01-28 11:09:13,762 INFO      delete_index operation succeeded on api.transactions-2015.01.24\n2015-01-28 11:09:13,772 INFO      delete_index operation succeeded on api.transactions-2015.01.25\n2015-01-28 11:09:13,772 INFO      api.transactions-2015.01.26 is within the threshold period (3 days).\n2015-01-28 11:09:13,772 INFO      api.transactions-2015.01.27 is within the threshold period (3 days).\n2015-01-28 11:09:13,772 INFO      Specified indices deleted.\n2015-01-28 11:09:13,772 INFO      Done in 0:00:00.122511.\nEven trying to use the close command with the --debug flag enabled, I don't see what you are seeing.\n2015-01-28 11:14:19,666 DEBUG                   __main__          check_version:250  Detected Elasticsearch version 1.4.1\n2015-01-28 11:14:19,666 DEBUG                       root                   main:377  Setting default timestring for days to %Y.%m.%d\n2015-01-28 11:14:19,666 DEBUG                       root                   main:378  Matching indices with pattern: api.transactions-%Y.%m.%d\n2015-01-28 11:14:19,666 DEBUG                       root                   main:382  argdict = {'url_prefix': '', 'prefix': 'api.transactions-', 'log_level': 'INFO', 'timestring': '%Y.%m.%d', 'dry_run': False, 'exclude_pattern': None, 'logformat': 'Default', 'auth': None, 'ssl': False, 'host': 'localhost', 'command': 'close', 'time_unit': 'days', 'timeout': 30, 'debug': True, 'func': <function close at 0x109b78e60>, 'log_file': None, 'master_only': False, 'port': 9200, 'older_than': 3, 'suffix': ''}\n2015-01-28 11:14:19,666 INFO                        root                  close:979  Closing indices...\n...\n2015-01-28 11:14:20,285 INFO                     curator               _op_loop:786  close_index operation succeeded on api.transactions-2015.01.25\n2015-01-28 11:14:20,285 INFO                     curator    filter_by_timestamp:396  api.transactions-2015.01.26 is within the threshold period (3 days).\n2015-01-28 11:14:20,329 INFO                     curator    filter_by_timestamp:396  api.transactions-2015.01.27 is within the threshold period (3 days).\n2015-01-28 11:14:20,329 INFO                     curator                  close:983  Closed specified indices.\n2015-01-28 11:14:20,329 INFO                    __main__                   main:385  Done in 0:00:00.674704.\nI even ran it to see what would happen if no indices were present:\n```\nbuh@Aironaut (11:16 AM) [~/WORK/curator/curator] \uf8ff ./curator_script.py show --show-indices --prefix api.transactions-\nbuh@Aironaut (11:17 AM) [~/WORK/curator/curator] \uf8ff ./curator_script.py --debug close --older-than 3 --prefix api.transactions-\n2015-01-28 11:17:09,399 INFO                        root                   main:353  Job starting...\n2015-01-28 11:17:09,400 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-01-28 11:17:09,400 INFO      urllib3.connectionpool              _new_conn:188  Starting new HTTP connection (1): localhost\n2015-01-28 11:17:09,404 DEBUG     urllib3.connectionpool          _make_request:368  \"GET / HTTP/1.1\" 200 342\n2015-01-28 11:17:09,404 INFO               elasticsearch    log_request_success:62   GET http://localhost:9200/ [status:200 request:0.005s]\n2015-01-28 11:17:09,405 DEBUG              elasticsearch    log_request_success:64   > None\n2015-01-28 11:17:09,405 DEBUG              elasticsearch    log_request_success:65   < {\n  \"status\" : 200,\n  \"name\" : \"Strong Guy\",\n  \"cluster_name\" : \"untergeek_testing\",\n  \"version\" : {\n    \"number\" : \"1.4.1\",\n    \"build_hash\" : \"89d3241d670db65f994242c8e8383b169779e2d4\",\n    \"build_timestamp\" : \"2014-11-26T15:49:29Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"4.10.2\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n2015-01-28 11:17:09,405 INFO         elasticsearch.trace    log_request_success:72   curl -XGET 'http://localhost:9200/?pretty' -d ''\n2015-01-28 11:17:09,405 DEBUG        elasticsearch.trace    log_request_success:75   #[200] (0.005s)\n{\n\"cluster_name\": \"untergeek_testing\",\n\"name\": \"Strong Guy\",\n\"status\": 200,\n\"tagline\": \"You Know, for Search\",\n\"version\": {\n\"build_hash\": \"89d3241d670db65f994242c8e8383b169779e2d4\",\n\"build_snapshot\": false,\n\"build_timestamp\": \"2014-11-26T15:49:29Z\",\n\"lucene_version\": \"4.10.2\",\n\"number\": \"1.4.1\"\n}\n}\n2015-01-28 11:17:09,405 DEBUG                   main          check_version:250  Detected Elasticsearch version 1.4.1\n2015-01-28 11:17:09,405 DEBUG                       root                   main:377  Setting default timestring for days to %Y.%m.%d\n2015-01-28 11:17:09,406 DEBUG                       root                   main:378  Matching indices with pattern: api.transactions-%Y.%m.%d\n2015-01-28 11:17:09,406 DEBUG                       root                   main:382  argdict = {'url_prefix': '', 'prefix': 'api.transactions-', 'log_level': 'INFO', 'timestring': '%Y.%m.%d', 'dry_run': False, 'exclude_pattern': None, 'logformat': 'Default', 'auth': None, 'ssl': False, 'host': 'localhost', 'command': 'close', 'time_unit': 'days', 'timeout': 30, 'debug': True, 'func': , 'log_file': None, 'master_only': False, 'port': 9200, 'older_than': 3, 'suffix': ''}\n2015-01-28 11:17:09,406 INFO                        root                  close:979  Closing indices...\n2015-01-28 11:17:09,406 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-01-28 11:17:09,408 DEBUG     urllib3.connectionpool          _make_request:368  \"GET //_settings?expand_wildcards=open%2Cclosed HTTP/1.1\" 200 2\n2015-01-28 11:17:09,409 INFO               elasticsearch    log_request_success:62   GET http://localhost:9200//_settings?expand_wildcards=open%2Cclosed [status:200 request:0.003s]\n2015-01-28 11:17:09,409 DEBUG              elasticsearch    log_request_success:64   > None\n2015-01-28 11:17:09,409 DEBUG              elasticsearch    log_request_success:65   < {}\n2015-01-28 11:17:09,409 INFO         elasticsearch.trace    log_request_success:72   curl -XGET 'http://localhost:9200/*/_settings?pretty&expand_wildcards=open%2Cclosed' -d ''\n2015-01-28 11:17:09,409 DEBUG        elasticsearch.trace    log_request_success:75   #[200] (0.003s)\n{}\n2015-01-28 11:17:09,410 INFO                     curator                  close:983  Closed specified indices.\n2015-01-28 11:17:09,410 INFO                    main                   main:385  Done in 0:00:00.021696.\n```\nI'm not sure what to make of your output as I am unable to replicate the problem.  \nCan you paste in your entire command-line?\n. What version of Python are you using?\n. That show works is indeed, odd.  What happens if you use the -n flag (dry-run) and do delete operations?  The dry-run means it won't actually delete anything.  But we'd at least see if this behavior is limited to close operations.\nI also wonder what version of the re module is in your setup.\n```\n$ python\nPython 2.7.6 (default, Sep  9 2014, 15:04:36)\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.39)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport re\nre.version\n'2.2.1'\n```\n\n\n\nMine is 2.2.1.  What version is yours?\n. Ah! I missed this before.  It's right there!\napi.transactions-2015.1.26\nYour month is a single-digit. The date pattern is expecting 2 digits: ^api.transactions-(\\d{4}.\\d{2}.\\d{2})$\nAnd the show command doesn't do any date matching, just prefix + .* + suffix, which is why that worked.\nWhatever is creating your indices is creating them with a single-digit month instead of a 2 digit month.  If you fix that, it should work.\n. This fix is in curator v3.\n. Thanks for adding this.\n. Thanks for the addition, and for making curator more awesome.\nI'm about to totally change-up how curator works for 3.0.  I'm happy to merge your changes to curator.py as the API can use these methods. I have twice denied adding an \"open\" command in the past as it is not typical for people to do this frequently. But I am going to want it for 3.0, so thanks for your addition.\nHowever, anything you add to curator_script.py will likely be obliterated.  I recommend keeping your own fork of curator_script.py for the time being and removing it from this PR.\n. As \"open\" is now in v3, I'm going to close this with an invitation to do another pull request for the shards-per-node functionality against the v3 code base.\n. This now works in curator v3.  See: https://github.com/elasticsearch/curator/blob/master/docs/asciidoc/index.asciidoc#index\n. Yeah.  I made the change for a 1.x dev branch thinking they weren't going to release it until 1.5.  I never changed it after it was merged into 1.4.\n. This is the error message in the new beta of curator 3:\ncurator --host x.x.x.x show snapshots --repository foo\n2015-03-03 20:56:34,059 INFO      Job starting...\n2015-03-03 20:56:34,081 ERROR     Unable to find all snapshots in repository: foo\nERROR. Unable to get snapshots from Elasticsearch.\n. The new beta does this:\n$ es_repo_mgr create s3 --repository curatortest --bucket curator-test --base_path Repository --access_key REDACTED --secret_key REDACTED\n2015-03-03 20:59:38,468 INFO      Checking if repository curatortest already exists...\n2015-03-03 20:59:38,470 INFO      Repository curatortest not in Elasticsearch. Continuing...\n2015-03-03 20:59:38,474 ERROR     Unable to create repository curatortest.  Exception   Check logs for more information.\n$ echo $?\n1\n. This is, unfortunately, all too common.  Some setups (notably anything CentOS) have a hard time with the entry_point.  For some odd reason, it's kind of hit or miss with OS X, as is evidenced by the following:\n```\n$ sudo pip install elasticsearch-curator\nPassword:\nDownloading/unpacking elasticsearch-curator\n  Downloading elasticsearch-curator-2.1.2.tar.gz\n  Running setup.py (path:/private/tmp/pip_build_root/elasticsearch-curator/setup.py) egg_info for package elasticsearch-curator\nwarning: no files found matching 'CHANGELOG'\nwarning: no previously-included files matching '__pycache__' found under directory '*'\nwarning: no previously-included files matching '*.py[co]' found under directory '*'\n\nRequirement already satisfied (use --upgrade to upgrade): elasticsearch>=1.0.0,<2.0.0 in /Library/Python/2.7/site-packages (from elasticsearch-curator)\nInstalling collected packages: elasticsearch-curator\n  Running setup.py install for elasticsearch-curator\nwarning: no files found matching 'CHANGELOG'\nwarning: no previously-included files matching '__pycache__' found under directory '*'\nwarning: no previously-included files matching '*.py[co]' found under directory '*'\nInstalling curator script to /usr/local/bin\nInstalling es_repo_mgr script to /usr/local/bin\n\nSuccessfully installed elasticsearch-curator\nCleaning up...\n$ /usr/local/bin/curator\nusage: curator [-h] [-v] [--host HOST] [--url_prefix URL_PREFIX] [--port PORT]\n               [--ssl] [--auth AUTH] [--timeout TIMEOUT] [--master-only] [-n]\n               [-D] [--loglevel LOG_LEVEL] [--logfile LOG_FILE]\n               [--logformat LOGFORMAT]\n           {alias,allocation,bloom,close,delete,optimize,replicas,show,snapshot}\n           ...\n\ncurator: error: too few arguments\n```\n$ uname -a\nDarwin Aironaut.local 14.1.0 Darwin Kernel Version 14.1.0: Mon Dec 22 23:10:38 PST 2014; root:xnu-2782.10.72~2/RELEASE_X86_64 x86_64\nOn OS X, I'm not sure if it's because I have the OS X dev tools in place, or because I've steadily upgraded in-place since OS X 10.8.  In any case, I don't have this problem, but others do, and I can't explain it.  \nThe workaround is to call the curator_script.py directly:\n$ python /Library/Python/2.7/site-packages/curator/curator_script.py\nYou could also sudo chmod +x /Library/Python/2.7/site-packages/curator/curator_script.py and symlink it: sudo ln -s /Library/Python/2.7/site-packages/curator/curator_script.py /usr/local/bin/curator_script.py\n. Unfortunately, they both are.  This is from a misunderstanding, which I am addressing in Curator 3.0, namely that I didn't necessarily want to prevent the showing of all indices by combining prefix with a timestamp.\nThe show command only shows indices (or snapshots), but the formation of the arguments only adds --prefix and --suffix, and no --time-unit or --timestring.  (See: https://github.com/elasticsearch/curator/blob/master/curator/curator_script.py#L153-L163 and note the conspicuous absence of calls to add_common_args(subparser) where --timestring and --time-unit are referenced).\nWhen --time-unit and --timestring are present, indices are obtained by regex, symbolically ^(prefix)(timestamp)(suffix)$.  Because the show command does not use a timestamp, it can only pull ^(prefix).*(suffix)$.\nAs a result, the current version of show will match a prefix of logstash- by default and show logstash-.* (since no suffix is provided), while delete will have a --timestring and will not match logstash-api-....\nThe Curator 3 branch I am working on will allow for arbitrary prefixes and timestamps and suffixes, such that a prefix of log will match anything beginning with log, regardless of the remainder of the index name--with or without a timestamp (it will act similarly with suffixes). It will match a timestamp (by timestring) anywhere in the index name, symbolically ^.(timestamp).$, and extract that value for comparison.  You will no longer be fixed to ^(prefix)(timestamp)(suffix)$ in the way that Curator has been up to this point.\n. This should be consistent now with the new beta.\n. Hi @cfeio.  There are couple of ways to achieve this.\n1. The --exclude flag.  This would allow you to use an exclusion regex like ^.*anotherindex.*$ to exclude matching indices.\n2. The --regex flag.  This would allow you to more fully define the index expression as ^logstash-\\d{4}\\d{2}\\d{2}$.  With this you would still need to use --timestring if you planned on doing time-based calculations.\n. Thanks for making curator more awesome!  This looks great!\nYes, please sign the CLA.\nOnce that's done:\n- I will merge into the current master\n- I will put your name into the contributors (if it's not already there).\n- I will be reimplementing the method in my curator v3 branch in a way that makes sense there.\n. Hmmm.  Did you use @ferki as the username you signed with?  I'm still getting a \"not signed\" message.\n. I'm checking right now, but I'm fairly certain that you cannot take any action against a closed index except delete and open.\n. And when it opened, it had the correct number of replicas?  And the correct allocation?\n. If replicas can be altered on a closed index, the solution is to alter line 20 to use ensure_list instead of prune_closed.  \nAnd if true for allocation, the same applies to the prune_allocated command.\nThere would be no reason to add a flag for these.\nHowever, these alterations would have to be vetted against older versions of Elasticsearch, and potentially have a client verification test done if they only work on newer versions.\n. I would have to write tests to apply changes to closed indices, then open them again and check settings.  Our CI test sled covers versions as old as 1.0, so it'd get tested against that.\nIf the behavior is allowed with closed indices, I won't filter them.  I only filter closed indices to prevent errors which might result from taking action against a closed index.  A flag to turn this feature on/off would be unnecessary, in my opinion.\n. The changes are only those 2 lines of code I mentioned above. But I can only accept a PR with tests, so if you're not comfortable with writing tests, I'm happy to take it up.\n. Be sure to add them in the test/integration folder, as those are the ones which actually test against a local Elasticsearch.\n. Please add the change to ensure_list to https://github.com/elasticsearch/curator/blob/master/curator/api/replicas.py#L20 as well.\nI don't think a method to skip indices with the correct number of replicas is a bad idea per se, but I don't think it's as necessary now that the replica count is applied to all indices at once.  In the past it iterated the indices one at a time.  I'm not 100%, but I don't think anything untoward will happen telling an index with 0 replicas to have 0 again.  In any case, the settings are applied simultaneously now for all but alias and optimize operations.  It will also iterate with bloom filter disabling, but only if you specify a --delay argument.  But since bloom filters are disabled since 1.4, this is a non-issue for most.\n. I'll have to ask about the CCLA.  What's your organization?\nAre the tests all passing for you now?  I'll download and test myself if necessary.\nAlso, are you fortunate enough to have both a 2.7 and 3.x python instance installed?  Our CI tests 2.7 and 3.3.  I test on my own with 2.7.2 and 3.4.2.\n. Hmmm.  I'm trying to manually verify. Our manual verification tool prompts me:\n\nType user's name, e-mail or Github nickname and press Enter...\n\nI've entered ferki, @ferki, Ferenc Erki, and adjust GmbH and none of those are working.  Was there a different email entered that I can try?\n. Strange.  Please send the transaction ID and anything else to my email, aaron.mildenstein at elasticsearch.com\nWe'll get to the bottom of this.\n. Manually confirmed for now.  Will investigate why the CCLA isn't being recognized later.\n. This build is taking me to my happy place, with nearly complete code coverage!\n```\nXML: /Users/buh/WORK/curator/nosetests.xml\nName                             Stmts   Miss  Cover   Missing\n\ncurator                              3      0   100%\ncurator._version                     1      0   100%\ncurator.api                         12      0   100%\ncurator.api.alias                   56      0   100%\ncurator.api.allocation              23      0   100%\ncurator.api.bloom                   31      0   100%\ncurator.api.close                   15      0   100%\ncurator.api.delete                  17      0   100%\ncurator.api.filter                 119      0   100%\ncurator.api.opener                  14      0   100%\ncurator.api.optimize                35      0   100%\ncurator.api.replicas                18      0   100%\ncurator.api.show                     5      0   100%\ncurator.api.snapshot                47      0   100%\ncurator.api.utils                  134      0   100%\ncurator.cli                         14      0   100%\ncurator.cli.alias                   13      0   100%\ncurator.cli.allocation              13      0   100%\ncurator.cli.bloom                    8      0   100%\ncurator.cli.cli                     47      5    89%   12-17\ncurator.cli.close                    7      0   100%\ncurator.cli.delete                  11      0   100%\ncurator.cli.index_selection         64      0   100%\ncurator.cli.opener                   7      0   100%\ncurator.cli.optimize                10      0   100%\ncurator.cli.replicas                12      0   100%\ncurator.cli.show                     9      0   100%\ncurator.cli.snapshot                20      0   100%\ncurator.cli.snapshot_selection      60      1    98%   94\ncurator.cli.utils                  109      0   100%\n\nTOTAL                              934      6    99%\nRan 208 tests in 10.907s\n```\n. I'm so sorry for the inconvenience!  I intend to release 3.0.0 early tomorrow, barring any failures.\nIn the meanwhile, please make elaborate use of the --help flag and ask any questions here you may need to know.\n. That's a valid concern.  I'll draft up a change for this.\nI know this is a valid use case, but I hate it for this reason.  It forces the flow to be different.  I don't want to have to pass the dry_run flag to the delete method for any reason, ever.  C'est la vie.\n. I believe that #290 also addresses this by putting the disk-space filter block ahead of the final test for indices. See logger.warn('No indices matched provided args.')\n. What does the output reveal if you issue the show command (without the --dry-run flag) instead of delete?  It seems to me that the output is correct in both examples.  It's showing which snapshots would be deleted, and announcing that it will not actually delete them.\n. If you were looking for log output similar to Curator v2, I apologize.  It is a major point release and much has been changed, including this.\nBecause of the new filtered pipeline approach to index and snapshot selection, I do not do the logging the same as in the previous version.\n. I should add a log line to explain that the following output is the list of indices.  I'll file that as an enhancement.\n. Thanks for catching the omission. I intended to document this, but in the swarm of other things I neglected to do so. \n. I added a page on exit codes to the wiki: https://github.com/elastic/curator/wiki/Exit-Codes\nI also added the command-line output to the wiki: \n- https://github.com/elastic/curator/wiki/index-selection\n- https://github.com/elastic/curator/wiki/snapshot-selection\nSince the wiki is user-editable, I'd love to have you contribute by adding the --repository and other flags.  Those were an omission on my part because of my haste to push out docs.  I merely copied the Index Selection page over and edited, but did not add the new stuff.\nI will totally add you to the CONTRIBUTORS if you do this for Curator.\n. http://www.elastic.co/guide/en/elasticsearch/client/curator/current/snapshot.html\nhttp://www.elastic.co/guide/en/elasticsearch/client/curator/current/exit-codes.html\nGoing to close this now, as documentation has been moved to the main Elastic documentation.\n. The command structure is different in v3. Can you share the command-line you used? I don't see it in the output. \n. --time-unit must come after indices or snapshots\n. You are missing the new sub-command indices after the delete command. This sub-command is what now drives index selection.\nTry:\ncurator --help\nThen:\ncurator delete --help\nAnd finally:\ncurator delete indices --help\nHopefully this shows the change I'm trying to explain. \n. You won't need suffix '*' any more. \nPlease read up on the command changes in the wiki here on github. \n. You are correct in your assessment that it is not well documented.  I shall have to explain this better.\nLet me outline the process that happens now in 3.x. The flow is in index_selection.py. Using the Click API, every argument/option is a decorator above the method.  Each of these decorators allows for a callback.  The callback for these options appends filters to an array (in the special context object: ctx.obj['filters']).  Once all of these filters are added, it then iterates through the list of filters against the working list of indices, sending the filter & current working list to the regex_iterate method.  This iteratively filters the list of indices in one method, rather than in each method as in v2.\nOf course, the index_selection module has a lot of if statements because there are a ton of edge cases.  I started with a considerably cleaner method, but the edge cases made it uglier.  I'd still rather have it ugly and working, though.\n. That's actually a decent idea.  Since I'd be adding API calls rather than deleting or altering them, I think this might work.  Let me think about how to accomplish this and get back to it.\n. What would you like to see available in an API call?  What in the index_selection would you like to see separated?\n. I love it!  If I try to include this now, however, I worry about versioning, as the 3.0 API has been released, and this would result in a major overhaul of several components.  The only major change I had planned on making to the 3.0 API is renaming regex_iterate (an admittedly horrible choice of names) to apply_filter, and I'm only doing so because 3.0 was so recently released.  I can add a similar build_filter method, though.\nAs a result--and to more closely mimic the CLI behavior--I put this together, and was prepared to release it alongside these usage examples\nI would love to work toward something of this nature for 3.1 (at the earliest) or a 4.0 version in the future.\nThoughts?  Do you think the potentially breaking nature of this worth pursuing for a rapid 3.0.1 release?\n. The more I think about this the wider the pendulum swings in both directions.\nI really like the simplicity of the Class/Object, but its complexity grows quickly if you do not specify default values.  I do not want any default values.  This is one of the reasons for the endless if statements in index_selection and the associated methods.  I want fairly strident controls forcing the user to specify things like days, and timestring.  If I'm already going to have that, as complex as it is, it makes some sense to keep the formatting the way it is.  I also want to have the process mimic the way the CLI is filtering (callbacks are adding the filters one at a time), which is slightly different from how the class handles it (and I suppose you could have the class instance be part of the obj dictionary, but that's a future discussion).\nOn the other hand, it makes some sense to add the other method as an alternative way to add items.  It's clean, it's simple, and many users may wind up preferring it.\nSo, rather than choose one or the other, why not both?  There's no reason not to have both!  Use the API call that you like.\nBecause it so closely mimics the way the command-line works, I'm including the build_filter method as shown, including the examples.  \nIn addition, I would absolutely love for you, @SegFaultAX, to add this class as an alternative method to add index filters, complete with defaults as you have them.  Please add the unit tests in the appropriate files.  I definitely want to keep the 99% code coverage the project currently has.  Please also thoroughly document the class such that sphinx will pick it up.  We can go over any and all changes before merging.\nIn this way I hope we can pave a way for the future, while over-filling the needs of the present.\n. please test this\n. test this please\n. Thanks for bringing this to my attention.  What is really going on is that this is not used.  It's a carry-over from Curator 2.x.  The is_flag option makes this True if set.\nHowever, it makes sense to change this to follow the pattern shown for --wait-for-completion, since include_global_state should be True by default.\nIt can be further cleaned up as default=DEFAULT_ARGS['ignore_unavailable'] is not needed either.\n. Please refer to the documentation wiki:\nDelete examples\nDelete command\nIndex selection\nThere is a new command structure, which is discoverable at each subcommand via the --help flag:\ncurator --help\n...\ncurator delete --help\n...\ncurator delete indices --help\n...\nIn  your case, what you would use is likely something like:\ncurator delete indices --older-than 7 --timestring %Y.%m.%d --time-unit days --prefix logstash\nThis will delete indices beginning with \"logstash\", which have a date pattern of YEAR.MONTH.DAY, and are older than 7 days.\n. Try using \ncurator show indices --all-indices\nYou'll see all indices. \n. Any updates?\n. Now that you have all indices showing, try adding the filtering:\ncurator show indices --older-than 7 --timestring %Y.%m.%d --time-unit days --prefix logstash\n. closed by #314 \n. sigh\nI forgot to take into account snapshots when I did the index splitting.  This will be a quick fix.\n. After I address the issue you brought up again with index selection and older-than, I will release 3.0.2\n. Hey, @peterskim12, how is this for a draft?\n\n. Or this one:\n\n. I'm working on the hand + feather duster, since they converted so poorly.  The final version will look better.\n. I could shrink it:\n\nAnd make it easier to see by reducing it to 2 shelves, with 2 Logstash logos each\n. That's 200x200, btw.\n. Here's another 200x200:\n\n. I cleaned up everything but the feathers, which are ridiculously hard to create for a novice like me.\n. And a 400px version:\n\n. @peterskim12 I've been told the internal logo team will eventually handle this.  I'm closing the issue for now.\n. The dry-run output has changed since 2.x.  Also, rather than be excessively verbose, Curator now only notifies when there's been an error, rather than telling you at every step what is happening.\nWhen you say that dry-run returns incorrect results, what do you mean?\n. You're not the first to complain about the change in dry-run output.  See if the changes in #329 will satisfy you.\n. I see what you mean.  This is because filtering out closed indices is done in the action methods (alias, delete, snapshot, etc).  \nTruthfully, this is cosmetic in your case.  Closing a closed index doesn't hurt anything.\nI can make the dry-run output the index name and its current state (open or closed).  Would that suffice?\n. Would output like this suffice?\nShow\n$ ./run_curator.py --host 192.168.123.204 show indices --all-indices\n2015-03-27 17:16:02,753 INFO      Job starting: show indices\n2015-03-27 17:16:02,777 INFO      Matching all indices. Ignoring flags other than --exclude.\n.kibana\n.marvel-2015.03.13 (CLOSED)\n.marvel-2015.03.14\n.marvel-2015.03.15\n.marvel-2015.03.16\n.marvel-2015.03.17\n.marvel-2015.03.18\n.marvel-2015.03.19\n.marvel-2015.03.20\n.marvel-2015.03.21\n.marvel-2015.03.22\n.marvel-2015.03.23\n.marvel-2015.03.24\n.marvel-2015.03.25\n.marvel-2015.03.26\n.marvel-2015.03.27\n.marvel-kibana\nkibana-int\nlogstash-2015.03.13 (CLOSED)\nlogstash-2015.03.14\nlogstash-2015.03.15\nlogstash-2015.03.16\nlogstash-2015.03.17\nlogstash-2015.03.18\nlogstash-2015.03.19\nlogstash-2015.03.20\nlogstash-2015.03.21\nlogstash-2015.03.22\nlogstash-2015.03.23\nlogstash-2015.03.24\nlogstash-2015.03.25\nlogstash-2015.03.26\nlogstash-2015.03.27\nDry Run\n$ ./run_curator.py --host 192.168.123.204 --dry-run bloom indices --all-indices\n2015-03-27 17:16:10,155 INFO      Job starting: bloom indices\n2015-03-27 17:16:10,172 INFO      Matching all indices. Ignoring flags other than --exclude.\n2015-03-27 17:16:10,172 INFO      DRY RUN MODE.  No changes will be made.\n2015-03-27 17:16:10,175 INFO      DRY RUN: bloom: .kibana\n2015-03-27 17:16:10,194 INFO      DRY RUN: bloom: .marvel-2015.03.13 (CLOSED)\n2015-03-27 17:16:10,209 INFO      DRY RUN: bloom: .marvel-2015.03.14\n2015-03-27 17:16:10,221 INFO      DRY RUN: bloom: .marvel-2015.03.15\n2015-03-27 17:16:10,231 INFO      DRY RUN: bloom: .marvel-2015.03.16\n2015-03-27 17:16:10,241 INFO      DRY RUN: bloom: .marvel-2015.03.17\n2015-03-27 17:16:10,250 INFO      DRY RUN: bloom: .marvel-2015.03.18\n2015-03-27 17:16:10,259 INFO      DRY RUN: bloom: .marvel-2015.03.19\n2015-03-27 17:16:10,268 INFO      DRY RUN: bloom: .marvel-2015.03.20\n2015-03-27 17:16:10,280 INFO      DRY RUN: bloom: .marvel-2015.03.21\n2015-03-27 17:16:10,291 INFO      DRY RUN: bloom: .marvel-2015.03.22\n2015-03-27 17:16:10,302 INFO      DRY RUN: bloom: .marvel-2015.03.23\n2015-03-27 17:16:10,313 INFO      DRY RUN: bloom: .marvel-2015.03.24\n2015-03-27 17:16:10,323 INFO      DRY RUN: bloom: .marvel-2015.03.25\n2015-03-27 17:16:10,334 INFO      DRY RUN: bloom: .marvel-2015.03.26\n2015-03-27 17:16:10,344 INFO      DRY RUN: bloom: .marvel-2015.03.27\n2015-03-27 17:16:10,348 INFO      DRY RUN: bloom: .marvel-kibana\n2015-03-27 17:16:10,349 INFO      DRY RUN: bloom: kibana-int\n2015-03-27 17:16:10,353 INFO      DRY RUN: bloom: logstash-2015.03.13 (CLOSED)\n2015-03-27 17:16:10,355 INFO      DRY RUN: bloom: logstash-2015.03.14\n2015-03-27 17:16:10,357 INFO      DRY RUN: bloom: logstash-2015.03.15\n2015-03-27 17:16:10,359 INFO      DRY RUN: bloom: logstash-2015.03.16\n2015-03-27 17:16:10,362 INFO      DRY RUN: bloom: logstash-2015.03.17\n2015-03-27 17:16:10,365 INFO      DRY RUN: bloom: logstash-2015.03.18\n2015-03-27 17:16:10,367 INFO      DRY RUN: bloom: logstash-2015.03.19\n2015-03-27 17:16:10,370 INFO      DRY RUN: bloom: logstash-2015.03.20\n2015-03-27 17:16:10,375 INFO      DRY RUN: bloom: logstash-2015.03.21\n2015-03-27 17:16:10,379 INFO      DRY RUN: bloom: logstash-2015.03.22\n2015-03-27 17:16:10,383 INFO      DRY RUN: bloom: logstash-2015.03.23\n2015-03-27 17:16:10,386 INFO      DRY RUN: bloom: logstash-2015.03.24\n2015-03-27 17:16:10,389 INFO      DRY RUN: bloom: logstash-2015.03.25\n2015-03-27 17:16:10,392 INFO      DRY RUN: bloom: logstash-2015.03.26\n2015-03-27 17:16:10,395 INFO      DRY RUN: bloom: logstash-2015.03.27\n. Interesting.  This is obviously an edge case.  An index with no active primary shards is not something the every-day user is likely to encounter.\nWhat kind of resolution would you like to see?\n. This is a severe edge case.  @jjfalling and I discussed this in the Elastic HipChat privately.  While catching this can be done, it is perhaps worthwhile to leave the full stack trace to show what's going on.\nIf the scenario which has the index with no active primaries is limited to that one index, Curator now provides a way to re-run the command and exclude that index:\n--exclude logstash-2015.03.21\nAgain, this is an extremely uncommon situation.  The verbose stack-trace log is appropriate here, so no changes will be made at this time.\n. Thanks for submitting this.  \nFirst, you mention that you're trying to delete metadata-curator-[timestamp] and snapshot-curator-[timestamp] and it deletes these.  However, the name in the error is curator-20150213001500.  Was this correct?  If so, is that snapshot supposed to be deleted?\nRegardless, I have racked my brain to try to guess how this scenario could happen.  One of three explanations I can guess at are as follows.\nDuplicate snapshots in the working list.\nThis theory falls apart because of https://github.com/elastic/curator/blob/master/curator/cli/snapshot_selection.py#L81\nBefore any work is done, the list is turned into a set (only one of anything in a set), then back into a list again.  This basically guarantees a unique list.\nIndividual nodes are not connected to the repository properly.\nThe error, (Service: Amazon S3; Status Code: 404; Error Code: NoSuchKey; Request ID: 5531B70DBC6BD25B), indicates that the Elasticsearch S3 plugin can't find the file.\nWhile this is concerning, it's not something Curator itself has any control over.  It makes a call via the Elasticsearch python module, and if that call cannot be completed due to a remote exception, there's really nothing Curator can do.  Curator can neither create this scenario, nor correct it.\nI can try to put some better error handling in to catch this sort of failure, but that will only be cosmetic to this scenario.\nIdeas to troubleshoot/fix this include updating the S3 plugin and/or your Elasticsearch version.\nSnapshot is in a bad state\nThe snapshot in the error, curator-20150213001500, may be incomplete, or corrupted in some way.  It shows up in the snapshot metadata, but the file has been deleted.  This could also happen if it were incompletely deleted in a previous attempt (i.e. the files were deleted, but the metadata was not, so it appeared that the snapshot was still there), or the snapshot was malformed when created.\nCluster issues\nThis may indicate something amiss with your cluster.  One node may be able to see something another cannot.  \nConclusion\nIn any case, with this not caused by having the snapshot name in the list 2x, this is outside the scope of Curator.  It is possible I've missed something, but these are my best guesses as to how this state could have happened.\n. I'm sorry you're having a bad time with Curator.  You're having a bad experience, and it's my goal to fix it.  From what I can tell, though, it can only be indirectly Curator's fault from what the errors are indicating.\nFlow\nLet's look at the flow again: \n- Curator creates a client connection to Elasticsearch using the elasticsearch-py module\n- The command to delete a snapshot is issued to Elasticsearch using elasticsearch-py calls\n- Elasticsearch, via the Amazon S3 plugin, issues the call to Amazon\n- The response you're getting is coming from Amazon, back to Elasticsearch, back to elasticsearch-py, up the chain to Curator again.\nWhat this means is that the error:\nelasticsearch.exceptions.NotFoundError: TransportError(404, u'RemoteTransportException[[Lemuel Dorcas][inet[/10.11.102.53:9300]][cluster/snapshot/delete]]; nested: SnapshotMissingException[[my_s3_repository:curator-20150213001500] is missing]; nested: FileNotFoundException[The specified key does not exist. (Service: Amazon S3; Status Code: 404; Error Code: NoSuchKey; Request ID: 5531B70DBC6BD25B)]; ')\n...is, at its root, coming from Elasticsearch.  You should actually be able to find a corresponding error in your Elasticsearch log files (in fact, I'd love to see them inline in a response here).  Neither Curator nor the elasticsearch-py module have any ability of themselves to connect to S3, which is handled by Elasticsearch and the S3 plugin.  This is why I was suggesting a client update on that end might be a potential fix.  Can you tell me what version of the S3 plugin you are using in your Elasticsearch? And with which Elasticsearch version?\nPossible causes\n\nThe calls to delete are happening too quickly in succession\n  This is, at best, a guess.  Elasticsearch is, indeed, unable to find the file.  There could be no 404 otherwise.  Amazon is sending the 404\u2014(Service: Amazon S3; Status Code: 404; Error Code: NoSuchKey; Request ID: 5531B70DBC6BD25B)]; ')\u2014complete with a request ID.  Why Amazon would do this, I can only speculate.  My guess is that it's an incorrect 404 because too many calls are hitting at a time, and it's not able to search the file-system quickly enough so the request actually results in a timeout of some kind on their end.  That's why you can still find it later with a curl call (things aren't hammering the server).\nThere is some kind of background retry logic happening in the S3 plugin\n  This ties in to the \"too many requests in rapid succession\" case above.  The call may go through to delete, but Amazon (or Elasticsearch) issue their own retry.  As you noted, sometimes the file gets deleted anyway.  This could be the case here, and we're getting a 404 because the file did get deleted, but we're getting a retry bounce-back.\n\nProposed resolutions\nOn the simple side, add a --delay.  Make it wait in between each delete.  This is too passive, though, to my thinking.\nAdd retry and verification logic. This is more involved. I can add some code to double-check that a snapshot is present, and attempt to perform the action again. This might warrant some \"test for snapshot, attempt delete, and retry if fail,\" kind of logic.\n. It's good to know that the AWS plugin isn't encountering an error at the ES level.  That means that Elasticsearch is getting a legitimate 404 from Amazon.  Why Amazon is giving a 404 is still not clear.\nThere is no retry within Curator.  It only tries once.  See https://github.com/elastic/curator/blob/master/curator/api/snapshot.py#L60-L64\nThat means the retry is probably inside Elasticsearch and/or the AWS plugin.\n- What version of Elasticsearch are you running?\n- What version of the AWS plugin are you running?\n. If you're using Curator 3.0.2 or 3.0.3 you shouldn't be experiencing #307 (fixed in #308).  That was one of the issues with snapshots \"retrying\" because the index list was getting segmented.\n. Which version of Curator are you using right now?\n. The portions you are showing are only revealing methods within the elasticsearch-py and urllib3 modules.  Note the 2nd column, which contains the name of the module, while the 3rd column shows the method being called, and on what line number.  Only the very bottom method is a curator.api.snapshot call.  \nLook at what's happening.  The first _make_request line shows a 504 error, which is a gateway timeout, which is\n\nA server (not necessarily a Web server) is acting as a gateway or proxy to fulfil the request by the client  to access the requested URL. This server did not receive a timely response from an upstream server it accessed to deal with your HTTP request.\nThis usually means that the upstream server is down (no response to the gateway/proxy)\n\nBut that 504 is probably indicating the same disconnect you saw with your curl call, in that wait_for_completion did not seem to wait for the complete time. It seems that the urllib3 code that the elasticsearch library is using is just as confused by the odd timeout.  In both cases, the snapshot has been started, and is still going, but wait_for_timeout is not waiting.\nAnd because of the 504, it thinks it failed, hence the retries and the failure messages, but these are not from Curator, but from the urllib3 calls in the elasticsearch-py module. It never goes back to Curator until it has retried 3 times and failed with each. Each failed retry within the elasticsearch module results in a call to the log_request_fail method.  This confirms what I have repeatedly stated: Curator itself is not retrying, but rather something farther up the chain.  \nMy suspicions at this point are two:\n1. There is some transport layer (gateway or proxy) between you and S3 that is perhaps causing some of these issues.  For example, an aggressive firewall rule that disallows HTTP keepalive connections, or those longer than 60 seconds, might do something like this.\n2. Since you have an older version of Elasticsearch (1.3.2)\u2014and correspondingly, the S3 plugin, there's a bug in your version of the Elasticsearch code that is signaling an improper failure, so the elasticsearch python library is instigating a retry, but that retry fails.  I did not see your exact issue in the Changelogs from Elasticsearch 1.3.3 up to 1.3.9, but there were many snapshot related fixes merged in those versions.  Updating may solve the problem, and certainly wouldn't hurt if it didn't. \n. Happy Easter to you as well!  I'm glad we may have pinned this down.  I've looked into the elasticsearch-py module API docs and cannot see a way to set up a keep-alive to work around this.  I don't know if you'd be amenable to a special port, firewalled off and all that, that isn't behind the ELB, just for snapshots, but that's another possibility.\nYou could also set up a dedicated wrapper that uses the regular Curator command-line to do snapshots, but with --wait_for_completion set to False.  The other part would be calls to the Elasticsearch API to keep polling periodically until the snapshot is complete, which would let the wrapper complete successfully when the snapshot successfully completed.\n. Any update on this?\n. Thanks for the update.  I'm going to add this scenario as an FAQ to the documentation.\n. Thanks for responding @pkr1234.  @trompx, The answer right above your issue was closed by PR #363 where this FAQ was added to the official documentation.\nGlad you have it sorted out.\n. Also document #348 and the timeout feature in #349 to fix it.\n. The --debug flag will show you the same thing without having to edit the code. \nI think the problem is that I have the wrong level of ctx in the fs and s3 methods.\nTry changing:\nclient = get_client(**ctx.parent.params)\non line 149 to\nclient = get_client(**ctx.parent.parent.params)\nThat should pass the variables through properly.\n. That's in curator/cli/es_repo_mgr.py, by the way.\n. An integration test or two for this should be added (and I apologize, that's probably why this happened at all--this is the one piece of code not tested).  I can help out with this if you like.\nAny integration test would belong in https://github.com/elastic/curator/blob/master/test/integration/test_cli_commands.py\nThe other classes there show how to mimic command-line options in testing.  You'll want to create a repository, then test for its existence (I know, redundant since it's validated, but you can re-use the same call in the test).\nFeel free to ask any questions.\n. Excellent.  You have two more edits to make.\n1. Add yourself to the CONTRIBUTORS file.\n2. Add a blurb to the Changelog.rst file.\n. lgtm!\n. Hi @bobrik. v3.0.3 should correct the timeout. #323 restores the --timeout override, rather than trust in the unfortunately ineffective --request-timeout flag.\nEven so, v3.0.2 should still work if you use the --timeout flag with a suitably large number.\n. This should be addressed by #595 and #596 with Curator 4.0 imminent.\nIf you feel this is in error, please feel free to reopen this ticket or open another.\n. @aliostad I'm sorry to hear that. I explained in the blog post that there are some features that simply could not be added to the old CLI, like atomic aliases and snapshot restore.\nI do intend to release scripts that have much of the 3.x CLI functionality but use the new API.  Not all features will be supported, but much, if not most, of what could be done in 3.x will be possible with the newer scripts.  Beyond that, it should still be possible to use the curator.yml client/logging config file with these (pending) scripts, which should simplify the command-lines for them.\n. I'm sorry you couldn't find an example that suited you.\nIf the examples in the guide do not satisfy you, this would be the correct place to add one, rather than as an FAQ:\nhttp://www.elastic.co/guide/en/elasticsearch/client/curator/current/examples.html\nAlso, please see http://www.elastic.co/guide/en/elasticsearch/client/curator/current/_how_to_contribute.html for the contribution guide.\n. There's also the site corrections page.\n. And the Examples page is a sub-page under Getting Started, if you were wondering.\n. That would be a bit redundant, since there is little difference between what you've added and the first example on the Examples page, which reads:\n\nDelete all indices with matching --timestring which are older than 30 days\ncurator --host 10.0.0.2 delete indices --older-than 30 --time-unit days --timestring '%Y.%m.%d'`\nIn previous versions of Curator, you would have needed as many runs of the command-line as you had prefixes, no matter the matching pattern. Not so any more! With this example, all indices with a pattern of %Y.%m.%d inside will be deleted. This is useful for managing both Logstash and Marvel indices with a single command, if you want the same retention period.\n\nThe only differences are that the existing example has a --host flag, and it's 30 days vs. 35.\nI'd be amenable to another example, adding logstash as a --prefix.\n. I will close this issue, then.  Feel free to re-open, or open a new one if you like.\n. Curator v3 likely fixes this as the --prefix is no longer forced to be \"everything leading up to the date\" portion of an index name.  In fact, v3 can support any number of different index naming styles now, not just time-series indices.  v3 is also much more thoroughly tested to prevent this sort of thing from happening, including command-line level integration testing of various options.\nThe fix is to upgrade to v3.  Bear in mind that the command-line syntax is different from v2.  Be sure to read the documentation\nI will keep this open for another day or two to allow you to upgrade and test, but re-do the title to indicate the problem is with Curator v2.\n. I'm sorry for the inconvenience you experienced.  This does seem like it should be the case.  You can do selection with just the --timestring flag, but apparently adding --time-unit makes it think you're doing a delete by time.\nI will add a check to ensure that --time-unit without --newer-than/--older-than does not result in this scenario.\n. What's really going on is that it didn't care about --time-unit at all.  It simply matched all indices with a timestring pattern.  I can add a confirmation dialog to prevent accidentally using a --timestring without --[older|newer]-than.  Would that suffice?\nThis behavior is documented: http://www.elastic.co/guide/en/elasticsearch/client/curator/current/examples.html#_show_only_indices_with_a_timestring\n. Will do!  Confirmation dialog coming up!\n. Please see the code changes in #349 \nThe command-line output will look like this:\n\n. The warning will only display if it is not doing the show command or a --dry-run, by the way.\n. Considering this fixed by #349 \n. ## Dependency issue?\n\nraise DistributionNotFound(req) pkg_resources.DistributionNotFound: click>=3.3\n\nThis indicates that the click dependency did not get installed.  If it did, and this error is still happening, you'll probably have to upgrade setuptools.\nUpgrade setuptools\nI also see that you're using python 2.6.  It is often necessary is to install the latest version of setuptools when running python 2.6.  See https://github.com/elastic/curator/issues/56#issuecomment-77843587 for more details there.\nYou will have to download the most recent version of setuptools if you're doing this manually.  I do not know if there are other dependencies it will need.\nLaunching as a module\nIn versions prior to v3, you could execute the curator.py file as a script. Because Curator is now a proper python package, the only way to launch Curator v3 is via the entry_point or as a module.  \nThe entry_point typically gets installed into /usr/bin or /usr/local/bin as curator--which you tried running first.  The other way to make it run is to call it as a python module:\npython -m curator\nThis will execute as though you called the entry_point, but the command-line help will show this oddity:\nUsage: __main__.py [OPTIONS] COMMAND [ARGS]...\nNote the __main__.py.  This is because you called the module with no other instructions, so it called main.\n. See the new FAQ\n. Part of the reason you may be having read issues with your indices is that they're hourly.  Elasticsearch automatically \"times out\" an index after 30 minutes, reclaiming the resources allocated to \"active\" indices.  This would leave a decent chunk of memory unavailable for 30 minutes of the next index's hour.  That said, how do you know you've reduced overhead by setting indices to read-only? \nThis feature has been requested before, so this is actually a duplicate of #160 \nIt is, maybe, perhaps, finally time to add the feature.   I am reluctant to do so because there will be those who set recent indices to read-only and blow-up their Logstash instances because old data sometimes straggles in from the past.  This will cause errors and all kinds of fun as Elasticsearch pushes back and says it can't write to that index.\nI will close this issue shortly, as all other discussion should be on #160 \n. Please sign the CLA.  Yes, it may seem silly, but even documentation changes have copyrights.\n. Thanks for signing.  Any future PRs will not have the warning.\n. I love it!  Thanks for contributing.  Before I merge, feel free to add yourself to the CONTRIBUTORS file, following the pattern there.\n. Weird.  That seems more like an Elasticsearch issue than a Curator one.  Clearly it is matching the indices properly, and sending the DELETE properly, and getting an acknowledged.\nWhats' in the Elasticsearch logs at that same time?\n. Hmmm.  The other issue is this:\n\nlog_request_success:75   #[200] (60.856s)\n\nIt's taking almost exactly 60 seconds to perform a delete.  This seems quite high.  How many indices are there?  How busy is the cluster?  Perhaps something is timing out on the other end.\n. You may be hitting this FAQ item:\nhttp://www.elastic.co/guide/en/elasticsearch/client/curator/current/_q_snapshots_seem_to_be_working_why_am_i_getting_literal_snapshotmissingexception_literal_messages.html\n. Do you have a load balancer in front of your client nodes?\n. This timeout is not from Curator or Elasticsearch, or you'd see the error in both sets of logs.  Do you see anything in the Elasticsearch logs?\nIs there a proxy or a gateway of some kind?  A firewall that will only keep state with a 60 second timeout? \n. Whatever else may be going on, your Elasticsearch cluster is overloaded.  It should never take even 30 seconds to delete a batch of indices.  You may simply be suffering from master/client node communication timeouts (though I'm surprised there's not logs to indicate such) due to the overloaded nature of your cluster. 1250 indices, presuming the default 5+1 shard count, divided among 20 nodes is 625 shards per node, which is a rather high number.  By default, Elasticsearch allocates 10% of the heap it gets to index caching and management. Each active shard will want 250M of that 10%, and each inactive shard will want 4M of that 10%.  Even if you had zero active shards across your nodes, 625*4M = 2.5GB of that 10%.  That would suggest you would need 31G heaps on each node (10% would be 3.1G).  If you are not using 31G heaps, you are possibly putting a high amount of memory pressure on the index cache.  Elasticsearch will try to be smart about how it handles this memory.  A shard becomes inactive 30 minutes after the last indexing activity.  Elasticsearch can probably squeeze the memory down to about 47M per active shard (though indexing performance is degraded) and just under 1M for inactive shards.  When Elasticsearch can't squeeze that memory any more, it will suddenly, and without warning, stop being able to index new data.  The cluster will still be up, and search will still work.  You just won't be able to index any more.  This typically only happens in time-series index situations as shard count per node becomes an issue.\nAlso of concern, with regard to shard count per node, is that deleting indices will result in a massive cluster rebalance.  Elasticsearch, by default, will try to keep a similar number of shards per node.  If you delete a bunch in a big batch, it will try to rebalance, which could also result in severely degraded disk and network I/O.\nYou may simply have to try pruning your indices in much smaller batches and wait for the cluster to recover in between.\n. Just because the masters aren't overloaded doesn't mean that there isn't a heavy load on the network and disk I/O with shard shuffling.  You're still trying to remove some 73 indices at once. Deleting all primary and replica shards for that many indices in a single batch is going to cause a lot of cluster updating, and you already have a 250Mb cluster state.  Sending updates for that much is not going to be easy.  While your masters may not be taxed, consider how much data has to move with that update, between the master and every other node.  That's a lot of traffic and updating!\nCurator's batching is not configurable.  It's there to defeat the 4k URL limit imposed by Elasticsearch.\nAt the bottom of all of this, the reality is that the problem isn't Curator's, but Elasticsearch's.  Curator simply sends the command\n. (and the command is getting there intact).\n. This is worth considering.  The [un]alias is atomic in the end, but the selection criteria would be exceptionally tricky to implement at the command-line.\nHow do you select the indices to remove, and add from the alias?  It would almost necessitate a 2 level index selection: one for the add, and one for the remove.  This could make for a fairly long command-line.\n. This will be available in the next major release.  It will be delayed because that will be the release that allows for a configuration file, which this level of abstraction surely requires.\n. This should be addressed by #595 and #596 with Curator 4.0 imminent.\nIf you feel this is in error, please feel free to reopen this ticket or open another.\n. Thanks for catching the cut/paste error!\nPlease be sure to sign the CLA as mentioned in the README. \n. No problem. I can fix that now. \n. Manually added your name.  Future pull requests should not fail the CLA.\n. --host isn't required because it has a default value of localhost.  It's only required if you are using something other than that.\nThe idea of using a .curator.rc file is an interesting one, but it kind of falls into duplicate territory with #343 \n. Closing in favor of #343\n. I appreciate that you're in a tough spot.  I was asked to include exit codes in #180.  Now my non-failure-but-still-a-failure exit code 99, which is ONLY for when there are no matching indices, is in the cross-hairs.\nI think that I will lump this in to the work on #343.  If I can make that work, then exit code 99 will be moot for your use case.  You'll have a single config file which will run everything.\n. I will be releasing soon.  Please stay tuned.\n. Lucene works by lumping segments into logical groupings called indices.  Elasticsearch extends this by calling a lucene index a \"shard,\" and then logically groups those into \"indices.\"  A segment is completely immutable when written.  Lucene automatically merges segments into bigger segments.  The \"optimize\" call is actually a forceMerge operation.\nSnapshots are collections of segments, nothing more.  If a segment hasn't changed since the last snapshot, the new snapshot only points to the existing segment.  No segment which is referenced by a \"newer\" snapshot will be deleted when you delete an \"older\" snapshot.\nIn this sense, snapshots are not a typical backup.  You're far better off naming snapshots accordingly:\n- 20 minute intervals could be named: rapid-TIMESTAMP\n- hourly-TIMESTAMP\n- daily-TIMESTAMP\n- weekly-TIMESTAMP\nCurator would handle these easily, now that you have a varied approach.\n- Keep no more than 2 hours of the rapid-TIMESTAMP snapshots\n- Keep no more than 12 hours of hourly-TIMESTAMP\n- Keep no more than 7 days of daily-TIMESTAMP\n- Keep no more than 30 days (or 28) of weekly-TIMESTAMP\nIn this way, you'll have an easy to read set of snapshots, recognizable by name, and an easy way to keep retention organized.  And fewer segments will be redundant at this point, as the more \"merged\" segments will tend to percolate up, especially if you use Curator's optimize to forceMerge segments on \"older\" indices for the weekly or daily snapshots.\n. I totally understand that with snapshot count.  \nSince snapshots are just Lucene segment copies with pointers, I do recommend named snapshots.  The current version of Curator does make it \"messier,\" but the results are very clean and manageable. \n[update: added \"with pointers.\"]\n. The --exclude flag will allow you to exclude snapshots.  It's not deterministic, but it would allow you to preserve certain snapshots.\n. That's why tiered, named snapshots is my recommendation.  If both rapid-TIMESTAMP and hourly-TIMESTAMP refer to some of the same segments, then those segments will be referenced by a restore, even if rapid-TIMESTAMP is deleted later on.  This is both the blessing and the curse of snapshots.  The more you have, the more segment pointer checks have to be made, but the less storage space you consume, and the less network/shared-disk-i/o you have to worry about.\nI will try to add an exponential backoff feature, but it won't be available for a bit.\n. In analyzing what would need to be changed to permit this at the CLI level, it will either be a breaking change, or I will have to create another \"delete\" sub-command.  Currently the delete subcommands are indices and snapshots.  Adding another sub-command, like \"snapshots-tiered,\" will be a bit odd and confusing, but I may still do it.  I also may opt not to, as it will be short-lived and removed when 4.0 comes out.\n4.0 will have some major new features, like file-based configuration, which will make something like this easier.\n. I'm not sure this is necessary any more, since Elasticsearch 2.x improved the slowdown that used to occur when there were many snapshots in the repository.\nI'm going to close it.  If someone still wants this feature, they can make a new issue, or reopen this one.. Some kind of metadata seems to have been lost.  Could you re-run both commands with the --debug flag?  The results will reveal more.\n. By way of explanation, Curator must check for existing snapshots so it is certain to not try to create a snapshot with the same name as an existing snapshot.  The fact that Curator cannot find the snapshots indicates not that there are no snapshots present (which it would detect and then happily create snapshots), but that it cannot get snapshots for some reason.  This is why I suggest that some metadata may have been deleted, corrupted or otherwise munged.\n. That's actually a tricky question.  It will be in the data path of your elasticsearch cluster on each of your nodes.  I cannot think of a clean way to delete/purge it without deleting all of your data.  That might be a good question to ask in the https://discuss.elastic.co forums.\n. Duplicate of #372\n. _all is not acceptable either:\nTransportError(404, 'RepositoryMissingException[[_all] missing]')\n. Bah!\nTransportError(404, u'SnapshotMissingException[[TEST_REPOSITORY:_status] is missing]; nested: FileNotFoundException[/tmp/tmpIHZ7RXWBY99L16/snapshot-_status (No such file or directory)]; ')\nThe _status endpoint doesn't even EXIST in these older versions.  I'll bypass this and let the old behavior work for these versions.\n. Did not fix :frowning: \n. Also, update Changelog with date.\n. YES! Finally fixed!\n. --loglevel debug should be replaced with --debug to get all of the upstream messages from the Elasticsearch module.  Not sure why --loglevel debug is not showing more, but there should be tons and tons of logs spewing.\nAlso, you should look at your Elasticsearch logs to see if errors are in there.\n. This kind of timeout can happen when ES is heavily overburdened.  In this case, it appears you're trying to delete many indices at once (could you elaborate? provide an approximate count?), which may contribute to that overburdening.  An increased timeout value will likely help with this.  Reducing the number of indices being deleted simultaneously might also help.  This could be accomplished by reducing the time window (--older-than X, and slowly reduce X until it is the desired number).\nIf you hardly have any indices (seeing that you're deleting older than 3), this might also indicate an overburdened cluster (or single node).\nAs far as reporting the error goes, I made a deliberate choice not to complicate the code here.  Trying to capture one of multiple potential errors here, and responding correctly to each kind, is a lot of extra code for each method making calls\u2013especially when trying to make both Python 2.x and 3.x play nice with the same code base.  Since --debug output will show the actual error, I leave it to the end user to use --debug when they hit errors.\n. I will make an issue to suggest using the --debug flag in these error messages.\n. Deleting one by one would not be a problem.  If it is it really indicates something wrong with your cluster.\nDeleting en masse is what's enabled by default now, as doing it one by one actually can cause more headache for the cluster, as it needs to figure out how to rebalance shards.  If it takes more time for you to get a sane cluster state, then a higher timeout should be the solution.  \nThis may have to do with your refresh interval.  This interval is actually counter-productive.  On of our top Lucene committers did some performance tests and found that refresh intervals longer than 10s did not help, and actually slowed things down.  The sweet spot is between 1s and 10s in nearly every scenario he created, at high and low indexing volumes.  I recommend going back to at least 10s, if not the 5s that Logstash defaults to.\n. Starting in version 1.4 of Elasticsearch, there's an internal test\u2014which is accessed by this API\u2014that verifies that all nodes of the cluster have write access to the snapshot repository.  Curator uses this to prevent you from attempting a snapshot which would have automatically been PARTIAL (because not all nodes could write to the shared repository).\nWhat kind of repository are you using?  fs? s3?\n. Or possibly that one of the nodes temporarily was unable to read/write to the shared filesystem.\nCan you close this if you're satisfied?\n. Thanks for reporting this.  Can you run this with the --debug flag and attach the output?  I am not able to test s3 repositories very easily.\n. Hi, @lbjay.  It's been a bit.  Have you been able to run this with the --debug flag and attach the output?\n. I appreciate your problem.  I will probably remove exit code 99 in 3.1.1 or 3.2.0.  Closing this as it's a duplicate of #371 \n. Hi, @petitout \nThe reason day of year isn't recognized is that it is not (yet) included as a viable choice.\nSee https://github.com/elastic/curator/blob/master/curator/api/filter.py#L16-L26\n\nDATE_REGEX = {\n    'Y' : '4',\n    'y' : '2',\n    'm' : '2',\n    'W' : '2',\n    'U' : '2',\n    'd' : '2',\n    'H' : '2',\n    'M' : '2',\n    'S' : '2',\n}\n\nDay of year is a curious choice for index naming.  If you want to add it, you'll have to find a way to add it.  Does %j always send a 3-digit number when used in python?  If so, you could add a pull request to both add it to the DATE_REGEX constant, and write a test or two.\nIf you don't want to create a pull request, we can treat this issue as a feature request.  Just edit the title and add the label \"enhancement.\"\n. I haven't taken the time to add it because restore is a much less common use-case (or at least, it was) than backup is.  I have an issue now requesting it, so it's on my plate now :smile: \n. This feature has been delayed.  It has way more parts to test than it first appears.  I have a work-in-progress branch, but it will not make 3.3.0.\n. I'm finally adding this, everyone.\n. This is absolutely on my radar and will be added quickly.\n. I will begin work on this as soon as @HonzaKral adds sync-flush to the elasticsearch-py module.  He tells me he's just coded it and will be releasing a version today.\n. Super excited for this, @HonzaKral.  This will be one of the best new features for time-series users of Elasticsearch.  Ever.\n. @pickypg I am already doing that one.  We should make them available through our regular download channels, though.\n. .deb and .rpm are not going to happen soon.  They just won't.  I spent days working on making prototypes, and while I can make debs that would be installable, they depend on the other dependencies being turned into packages as well: elasticsearch-py (the official elasticsearch python module), click (the CLI library module), and urllib3.\nI think we could make elasticsearch-py into a module in-house, and add it to our repository, and a sufficient urllib3 is probably already available.  Click, however, is the problem.  I don't own that project, and I don't want to become responsible for the upkeep of RPM/DEB versions of Click for RedHat/CentOS, or Debian/Ubuntu, etc.  And without a Click package, there can be no Curator package.  Bundling their source inside the package is a big no-no, so that won't work either.\n. The officially recommended way to install Curator will remain pip install elasticsearch-curator for the time being.\nHowever, I can use nuitka to compile Curator and its dependencies into bytecode, and then share those as .tar.gz and .zip (for windows).  The problem with this approach is that it is extremely system dependent, as it bundles the libc and other libraries from the host system.  I have built Docker images which will create Curator binaries for Ubuntu 12.04 & 14.04, CentOS 6 & 7 (latest on each).  The resulting binaries will work on the system they came from, but are not necessarily guaranteed to work on minor versions in the RHEL/CentOS camp.  It would be trivial to make another image for those point versions, however, now that I have good Dockerfiles and scripts.  \nWe can make these binaries available for download through our regular channels.  While it might be possible to make RPM/DEB packages of these tarballs, because the binary is so system specific, I don't think that's a wise approach.  Too many possibilities of library conflicts.  It would lead to major headaches in attempting to guarantee compatibility which are easily rectified by building a new binary from a compatible Docker image.\n. Some digging has shown that only unstable and very current builds for Debian and Ubuntu have python-click (or worse, python-click-cli, causing a name conflict with the dependencies), and they're a major point version behind current, though 3.3 should still work with Curator.\n. We could\u2014at least there's nothing technical stopping us\u2014host our own \"Curator PPA\" (or yum repository) and include our own build of Click.  That's a discussion that needs to happen internally, though, as there are licensing and security considerations.  If we do go that route, it may not even \"fix\" the issue users are having where they are in an air-gapped datacenter and wouldn't be able to download from the PPA directly.  They'd need 3 or 4 RPM/DEB packages to complete the install, rather than one pretty RPM/DEB.\n. I have proof-of-concept (well, they actually work rather well) DEB packages for python2/3 on Debian/Ubuntu.  The problem is that it starts getting sticky rather quickly.  These were all built using @jordansissel's remarkable fpm\nRequired\n\npython-click_4.0_all.deb\npython-elasticsearch_1.6.0_all.deb\npython-elasticsearch-curator_3.1.0_all.deb\n\nDependencies (if necessary)\n\npython-setuptools_18.0.1_all.deb\npython-urllib3_1.10.4_all.deb\n\nOptional (for HTTPS certificate verification)\n\npython-certifi_2015.04.28_all.deb\n. @pickypg @peterskim12 See if these options satisfy you: https://www.elastic.co/guide/en/elasticsearch/client/curator/current/installation.html\n. Yay!\n. Have you signed the CLA?  I can merge this after you've signed.\n. For this PR it won't show up.  If you submit to any Elastic owned repository in the future, it will show as signed.\n. lgtm!\n. I'm not sure why that's failing for you.  My own daily script contains these lines:\n\n/usr/local/bin/curator --logfile /var/log/curator/curator.log delete indices   --time-unit days --timestring %Y.%m.%d --prefix .marvel --older-than 15\n/usr/local/bin/curator --logfile /var/log/curator/curator.log close  indices   --time-unit days --timestring %Y.%m.%d --prefix .marvel --older-than 14\n/usr/local/bin/curator --logfile /var/log/curator/curator.log delete snapshots --time-unit days --older-than 30 --repository Untergeek --prefix marvel-\n/usr/local/bin/curator --logfile /var/log/curator/curator.log optimize --max_num_segments 1 --delay 5 indices --time-unit days --timestring %Y.%m.%d --older-than 1 --prefix .marvel-\n/usr/local/bin/curator --logfile /var/log/curator/curator.log snapshot --repository Untergeek --prefix marvel- indices --time-unit days --timestring %Y.%m.%d --older-than 2 --prefix .marvel-\nThat first line doesn't look substantially different from yours.  Could you enable the --debug flag and see what is in the output?  Hopefully it will reveal more about why it's not working.\n. Hmmm.  It appears that there's something wrong with the regular expressions:\n170 REGEX = ^.marvel.$\nThat regular expression should be:\n170 REGEX = ^.marvel.*$\nThe regex is also completely wrong with the time string: 170 REGEX = (?P\\m.\\d).  Those should have been interpolated to numerics!\nIs this just a case of special characters failing to be read properly? I have no explanation as to why it is failing for you.  When I run this test on my windows machine with Python 3.4, it works just fine.  I run Curator in a plain CMD window, no batch files or power shell. \nTo rule out your python install, you may want to try using this pre-built package for 64-bit Windows (do NOT install it in your python directory or it may not work--this has been repeatedly observed).\n. The more I think about it, the more certain I am that executing curator from the batch file has something to do with it.  Try running manually from the CMD window and see if that works.\n. Now for a head scratcher...\nFirst, I created indices to match the names in your setup (yellow state because I have only one node):\nbuh@aironaut (08:57 PM) ~\n\uf8ff curl localhost:9200/_cat/indices\nyellow open content_2015_06_13 5 1 0 0 575b 575b\nyellow open content_2015_06_07 5 1 0 0 575b 575b\nyellow open grafana-dash       5 1 0 0 575b 575b\nyellow open content_2015_06_10 5 1 0 0 575b 575b\nyellow open foo                5 1 1 0  3kb  3kb\nyellow open foo1               5 1 0 0 575b 575b\nyellow open test               5 1 0 0 575b 575b\nyellow open content_2015_06_08 5 1 0 0 575b 575b\nyellow open content_2015_06_09 5 1 0 0 575b 575b\nyellow open content_2015_06_12 5 1 0 0 575b 575b\nyellow open content_2015_06_16 5 1 0 0 575b 575b\nyellow open content_2015_06_15 5 1 0 0 575b 575b\nyellow open .kibana            5 1 0 0 575b 575b\nyellow open content_2015_06_11 5 1 0 0 575b 575b\nyellow open content_2015_06_14 5 1 0 0 575b 575b\nyellow open content_2015_06_06 5 1 0 0 575b 575b\nThey're yellow because it's a single node.\nbuh@aironaut (08:58 PM) ~\n\uf8ff curator --version\ncurator, version 3.1.0\n```\nbuh@aironaut (08:58 PM) ~\n\uf8ff curator --debug show indices --all-indices\n2015-06-15 20:58:37,733 INFO      curator.cli.index_selection                indices:53   Job starting: show indices\n2015-06-15 20:58:37,733 DEBUG     curator.cli.index_selection                indices:54   Params: {'url_prefix': u'', 'http_auth': None, 'dry_run': False, 'loglevel': u'INFO', 'logformat': u'default', 'host': u'localhost', 'timeout': 30, 'debug': True, 'use_ssl': False, 'logfile': None, 'master_only': False, 'port': 9200}\n2015-06-15 20:58:37,733 DEBUG          curator.cli.utils             get_client:109  kwargs = {'url_prefix': u'', 'http_auth': None, 'dry_run': False, 'loglevel': u'INFO', 'host': u'localhost', 'logformat': u'default', 'timeout': 30, 'debug': True, 'use_ssl': False, 'logfile': None, 'master_only': False, 'port': 9200}\n2015-06-15 20:58:37,734 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-06-15 20:58:37,734 INFO      urllib3.connectionpool              _new_conn:203  Starting new HTTP connection (1): localhost\n2015-06-15 20:58:37,742 DEBUG     urllib3.connectionpool          _make_request:383  \"GET / HTTP/1.1\" 200 339\n2015-06-15 20:58:37,742 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.008s]\n2015-06-15 20:58:37,742 DEBUG              elasticsearch    log_request_success:65   > None\n2015-06-15 20:58:37,742 DEBUG              elasticsearch    log_request_success:66   < {\n  \"status\" : 200,\n  \"name\" : \"Starfox\",\n  \"cluster_name\" : \"untergeek_testing\",\n  \"version\" : {\n    \"number\" : \"1.6.0\",\n    \"build_hash\" : \"cdd3ac4dde4f69524ec0a14de3828cb95bbb86d0\",\n    \"build_timestamp\" : \"2015-06-09T13:36:34Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"4.10.4\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n2015-06-15 20:58:37,743 DEBUG          curator.cli.utils          check_version:90   Detected Elasticsearch version 1.6.0\n2015-06-15 20:58:37,743 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-06-15 20:58:37,750 DEBUG     urllib3.connectionpool          _make_request:383  \"GET //_settings?expand_wildcards=open%2Cclosed HTTP/1.1\" 200 3008\n2015-06-15 20:58:37,751 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200//_settings?expand_wildcards=open%2Cclosed [status:200 request:0.008s]\n2015-06-15 20:58:37,751 DEBUG              elasticsearch    log_request_success:65   > None\n2015-06-15 20:58:37,751 DEBUG              elasticsearch    log_request_success:66   < {\"content_2015_06_10\":{\"settings\":{\"index\":{\"creation_date\":\"1434423436815\",\"number_of_shards\":\"5\",\"number_of_replicas\":\"1\",\"version\":{\"created\":\"1060099\"},\"uuid\":\"vo1Y1G1QRjidEo95BU5rpw\"}}},\"content_2015_06_12\":{\"settings\":{\"index\":{\"creation_date\":\"1434423437352\",\"number_of_shards\":\"5\",\"number_of_replicas\":\"1\",\"version\":{\"created\":\"1060099\"},\"uuid\":\"3ouSeODFQmSgLA6j2Nln8w\"}}},\"content_2015_06_11\":{\"settings\":{\"index\":{\"creation_date\":\"1434423437634\",\"number_of_shards\":\"5\",\"number_of_replicas\":\"1\",\"version\":{\"created\":\"1060099\"},\"uuid\":\"9bPk6Ug1QMaHycs-6PIEaQ\"}}},\"test\":{\"settings\":{\"index\":{\"creation_date\":\"1434423437070\",\"number_of_shards\":\"5\",\"number_of_replicas\":\"1\",\"version\":{\"created\":\"1060099\"},\"uuid\":\"xW1R2P02SlyboD11SgmUxg\"}}},\"content_2015_06_14\":{\"settings\":{\"index\":{\"creation_date\":\"1434423436696\",\"number_of_shards\":\"5\",\"number_of_replicas\":\"1\",\"version\":{\"created\":\"1060099\"},\"uuid\":\"fE6bseR9Q96nFqBg3CKjdg\"}}},\"content_2015_06_13\":{\"settings\":{\"index\":{\"creation_date\":\"1434423436585\",\"number_of_shards\":\"5\",\"number_of_replicas\":\"1\",\"version\":{\"created\":\"1060099\"},\"uuid\":\"PxvVdaZdQM6V_knnQWsWuQ\"}}},\"content_2015_06_16\":{\"settings\":{\"index\":{\"creation_date\":\"1434423437423\",\"number_of_shards\":\"5\",\"number_of_replicas\":\"1\",\"version\":{\"created\":\"1060099\"},\"uuid\":\"3C6DtOpzTm6s19_DCw_FPQ\"}}},\"foo\":{\"settings\":{\"index\":{\"creation_date\":\"1434399801715\",\"number_of_shards\":\"5\",\"number_of_replicas\":\"1\",\"version\":{\"created\":\"1060099\"},\"uuid\":\"fzt4JPo9SlmTjs-hufvzQw\",\"blocks\":{\"write\":\"False\"}}}},\"content_2015_06_15\":{\"settings\":{\"index\":{\"creation_date\":\"1434423437489\",\"number_of_shards\":\"5\",\"number_of_replicas\":\"1\",\"version\":{\"created\":\"1060099\"},\"uuid\":\"1DdHgWoXTY20y11t2atzkg\"}}},\"content_2015_06_07\":{\"settings\":{\"index\":{\"creation_date\":\"1434423437726\",\"number_of_shards\":\"5\",\"number_of_replicas\":\"1\",\"version\":{\"created\":\"1060099\"},\"uuid\":\"k_DVc4GoSkyRG-Z5gAkzmg\"}}},\"content_2015_06_06\":{\"settings\":{\"index\":{\"creation_date\":\"1434423436437\",\"number_of_shards\":\"5\",\"number_of_replicas\":\"1\",\"version\":{\"created\":\"1060099\"},\"uuid\":\"NEdCkL7LRYijYvo5RbSuVg\"}}},\"content_2015_06_09\":{\"settings\":{\"index\":{\"creation_date\":\"1434423437276\",\"number_of_shards\":\"5\",\"number_of_replicas\":\"1\",\"version\":{\"created\":\"1060099\"},\"uuid\":\"4lnBZh0bTM-SPvAIgbBe3Q\"}}},\"content_2015_06_08\":{\"settings\":{\"index\":{\"creation_date\":\"1434423437189\",\"number_of_shards\":\"5\",\"number_of_replicas\":\"1\",\"version\":{\"created\":\"1060099\"},\"uuid\":\"JICFzub2SUKrm8XPn_xObA\"}}},\"grafana-dash\":{\"settings\":{\"index\":{\"creation_date\":\"1434423437802\",\"number_of_shards\":\"5\",\"number_of_replicas\":\"1\",\"version\":{\"created\":\"1060099\"},\"uuid\":\"06rAqx0LRMqs4N9qA4bhuw\"}}},\".kibana\":{\"settings\":{\"index\":{\"creation_date\":\"1434423437558\",\"number_of_shards\":\"5\",\"number_of_replicas\":\"1\",\"version\":{\"created\":\"1060099\"},\"uuid\":\"TS3cBO8SRhSM3hU6oVj7Ug\"}}},\"foo1\":{\"settings\":{\"index\":{\"creation_date\":\"1434423436972\",\"number_of_shards\":\"5\",\"number_of_replicas\":\"1\",\"version\":{\"created\":\"1060099\"},\"uuid\":\"NCSGKRzERu2lwnEjs_sJFg\"}}}}\n2015-06-15 20:58:37,753 DEBUG          curator.api.utils            get_indices:27   All indices: [u'content_2015_06_08', u'content_2015_06_09', u'grafana-dash', u'content_2015_06_16', u'content_2015_06_15', u'content_2015_06_14', u'content_2015_06_13', u'content_2015_06_12', u'content_2015_06_11', u'content_2015_06_10', u'content_2015_06_06', u'content_2015_06_07', u'test', u'foo', u'foo1', u'.kibana']\n2015-06-15 20:58:37,753 DEBUG     curator.cli.index_selection                indices:60   Full list of indices: [u'content_2015_06_08', u'content_2015_06_09', u'grafana-dash', u'content_2015_06_16', u'content_2015_06_15', u'content_2015_06_14', u'content_2015_06_13', u'content_2015_06_12', u'content_2015_06_11', u'content_2015_06_10', u'content_2015_06_06', u'content_2015_06_07', u'test', u'foo', u'foo1', u'.kibana']\n2015-06-15 20:58:37,753 INFO      curator.cli.index_selection                indices:72   Matching all indices. Ignoring flags other than --exclude.\n2015-06-15 20:58:37,753 DEBUG     curator.cli.index_selection                indices:74   All filters: []\n2015-06-15 20:58:37,753 DEBUG     curator.cli.index_selection                indices:110  ACTION: show. INDICES: [u'.kibana', u'content_2015_06_06', u'content_2015_06_07', u'content_2015_06_08', u'content_2015_06_09', u'content_2015_06_10', u'content_2015_06_11', u'content_2015_06_12', u'content_2015_06_13', u'content_2015_06_14', u'content_2015_06_15', u'content_2015_06_16', u'foo', u'foo1', u'grafana-dash', u'test']\n2015-06-15 20:58:37,753 INFO      curator.cli.index_selection                indices:114  Matching indices:\n2015-06-15 20:58:37,753 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-06-15 20:58:37,756 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/.kibana HTTP/1.1\" 200 297\n2015-06-15 20:58:37,757 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/.kibana [status:200 request:0.003s]\n2015-06-15 20:58:37,757 DEBUG              elasticsearch    log_request_success:65   > None\n2015-06-15 20:58:37,757 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"untergeek_testing\",\"metadata\":{\"templates\":{},\"indices\":{\".kibana\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1434423437558\",\"number_of_shards\":\"5\",\"number_of_replicas\":\"1\",\"version\":{\"created\":\"1060099\"},\"uuid\":\"TS3cBO8SRhSM3hU6oVj7Ug\"}},\"mappings\":{},\"aliases\":[]}}}}\n.kibana\n2015-06-15 20:58:37,758 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-06-15 20:58:37,759 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/content_2015_06_06 HTTP/1.1\" 200 308\n2015-06-15 20:58:37,760 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/content_2015_06_06 [status:200 request:0.002s]\n2015-06-15 20:58:37,760 DEBUG              elasticsearch    log_request_success:65   > None\n2015-06-15 20:58:37,760 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"untergeek_testing\",\"metadata\":{\"templates\":{},\"indices\":{\"content_2015_06_06\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1434423436437\",\"number_of_shards\":\"5\",\"number_of_replicas\":\"1\",\"version\":{\"created\":\"1060099\"},\"uuid\":\"NEdCkL7LRYijYvo5RbSuVg\"}},\"mappings\":{},\"aliases\":[]}}}}\ncontent_2015_06_06\n2015-06-15 20:58:37,760 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-06-15 20:58:37,762 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/content_2015_06_07 HTTP/1.1\" 200 308\n2015-06-15 20:58:37,762 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/content_2015_06_07 [status:200 request:0.002s]\n2015-06-15 20:58:37,762 DEBUG              elasticsearch    log_request_success:65   > None\n2015-06-15 20:58:37,763 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"untergeek_testing\",\"metadata\":{\"templates\":{},\"indices\":{\"content_2015_06_07\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1434423437726\",\"number_of_shards\":\"5\",\"number_of_replicas\":\"1\",\"version\":{\"created\":\"1060099\"},\"uuid\":\"k_DVc4GoSkyRG-Z5gAkzmg\"}},\"mappings\":{},\"aliases\":[]}}}}\ncontent_2015_06_07\n2015-06-15 20:58:37,763 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-06-15 20:58:37,765 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/content_2015_06_08 HTTP/1.1\" 200 308\n2015-06-15 20:58:37,765 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/content_2015_06_08 [status:200 request:0.002s]\n2015-06-15 20:58:37,765 DEBUG              elasticsearch    log_request_success:65   > None\n2015-06-15 20:58:37,765 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"untergeek_testing\",\"metadata\":{\"templates\":{},\"indices\":{\"content_2015_06_08\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1434423437189\",\"number_of_shards\":\"5\",\"number_of_replicas\":\"1\",\"version\":{\"created\":\"1060099\"},\"uuid\":\"JICFzub2SUKrm8XPn_xObA\"}},\"mappings\":{},\"aliases\":[]}}}}\ncontent_2015_06_08\n2015-06-15 20:58:37,766 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-06-15 20:58:37,767 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/content_2015_06_09 HTTP/1.1\" 200 308\n2015-06-15 20:58:37,768 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/content_2015_06_09 [status:200 request:0.002s]\n2015-06-15 20:58:37,768 DEBUG              elasticsearch    log_request_success:65   > None\n2015-06-15 20:58:37,768 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"untergeek_testing\",\"metadata\":{\"templates\":{},\"indices\":{\"content_2015_06_09\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1434423437276\",\"number_of_shards\":\"5\",\"number_of_replicas\":\"1\",\"version\":{\"created\":\"1060099\"},\"uuid\":\"4lnBZh0bTM-SPvAIgbBe3Q\"}},\"mappings\":{},\"aliases\":[]}}}}\ncontent_2015_06_09\n2015-06-15 20:58:37,768 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-06-15 20:58:37,771 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/content_2015_06_10 HTTP/1.1\" 200 308\n2015-06-15 20:58:37,771 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/content_2015_06_10 [status:200 request:0.003s]\n2015-06-15 20:58:37,772 DEBUG              elasticsearch    log_request_success:65   > None\n2015-06-15 20:58:37,772 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"untergeek_testing\",\"metadata\":{\"templates\":{},\"indices\":{\"content_2015_06_10\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1434423436815\",\"number_of_shards\":\"5\",\"number_of_replicas\":\"1\",\"version\":{\"created\":\"1060099\"},\"uuid\":\"vo1Y1G1QRjidEo95BU5rpw\"}},\"mappings\":{},\"aliases\":[]}}}}\ncontent_2015_06_10\n2015-06-15 20:58:37,772 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-06-15 20:58:37,774 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/content_2015_06_11 HTTP/1.1\" 200 308\n2015-06-15 20:58:37,784 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/content_2015_06_11 [status:200 request:0.011s]\n2015-06-15 20:58:37,784 DEBUG              elasticsearch    log_request_success:65   > None\n2015-06-15 20:58:37,784 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"untergeek_testing\",\"metadata\":{\"templates\":{},\"indices\":{\"content_2015_06_11\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1434423437634\",\"number_of_shards\":\"5\",\"number_of_replicas\":\"1\",\"version\":{\"created\":\"1060099\"},\"uuid\":\"9bPk6Ug1QMaHycs-6PIEaQ\"}},\"mappings\":{},\"aliases\":[]}}}}\ncontent_2015_06_11\n2015-06-15 20:58:37,784 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-06-15 20:58:37,786 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/content_2015_06_12 HTTP/1.1\" 200 308\n2015-06-15 20:58:37,786 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/content_2015_06_12 [status:200 request:0.002s]\n2015-06-15 20:58:37,786 DEBUG              elasticsearch    log_request_success:65   > None\n2015-06-15 20:58:37,837 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"untergeek_testing\",\"metadata\":{\"templates\":{},\"indices\":{\"content_2015_06_12\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1434423437352\",\"number_of_shards\":\"5\",\"number_of_replicas\":\"1\",\"version\":{\"created\":\"1060099\"},\"uuid\":\"3ouSeODFQmSgLA6j2Nln8w\"}},\"mappings\":{},\"aliases\":[]}}}}\ncontent_2015_06_12\n2015-06-15 20:58:37,837 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-06-15 20:58:37,839 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/content_2015_06_13 HTTP/1.1\" 200 308\n2015-06-15 20:58:37,839 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/content_2015_06_13 [status:200 request:0.002s]\n2015-06-15 20:58:37,839 DEBUG              elasticsearch    log_request_success:65   > None\n2015-06-15 20:58:37,839 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"untergeek_testing\",\"metadata\":{\"templates\":{},\"indices\":{\"content_2015_06_13\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1434423436585\",\"number_of_shards\":\"5\",\"number_of_replicas\":\"1\",\"version\":{\"created\":\"1060099\"},\"uuid\":\"PxvVdaZdQM6V_knnQWsWuQ\"}},\"mappings\":{},\"aliases\":[]}}}}\ncontent_2015_06_13\n2015-06-15 20:58:37,884 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-06-15 20:58:37,886 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/content_2015_06_14 HTTP/1.1\" 200 308\n2015-06-15 20:58:37,886 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/content_2015_06_14 [status:200 request:0.002s]\n2015-06-15 20:58:37,886 DEBUG              elasticsearch    log_request_success:65   > None\n2015-06-15 20:58:37,886 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"untergeek_testing\",\"metadata\":{\"templates\":{},\"indices\":{\"content_2015_06_14\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1434423436696\",\"number_of_shards\":\"5\",\"number_of_replicas\":\"1\",\"version\":{\"created\":\"1060099\"},\"uuid\":\"fE6bseR9Q96nFqBg3CKjdg\"}},\"mappings\":{},\"aliases\":[]}}}}\ncontent_2015_06_14\n2015-06-15 20:58:37,886 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-06-15 20:58:37,888 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/content_2015_06_15 HTTP/1.1\" 200 308\n2015-06-15 20:58:37,934 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/content_2015_06_15 [status:200 request:0.047s]\n2015-06-15 20:58:37,934 DEBUG              elasticsearch    log_request_success:65   > None\n2015-06-15 20:58:37,934 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"untergeek_testing\",\"metadata\":{\"templates\":{},\"indices\":{\"content_2015_06_15\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1434423437489\",\"number_of_shards\":\"5\",\"number_of_replicas\":\"1\",\"version\":{\"created\":\"1060099\"},\"uuid\":\"1DdHgWoXTY20y11t2atzkg\"}},\"mappings\":{},\"aliases\":[]}}}}\ncontent_2015_06_15\n2015-06-15 20:58:37,935 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-06-15 20:58:37,936 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/content_2015_06_16 HTTP/1.1\" 200 308\n2015-06-15 20:58:37,982 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/content_2015_06_16 [status:200 request:0.047s]\n2015-06-15 20:58:37,982 DEBUG              elasticsearch    log_request_success:65   > None\n2015-06-15 20:58:37,983 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"untergeek_testing\",\"metadata\":{\"templates\":{},\"indices\":{\"content_2015_06_16\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1434423437423\",\"number_of_shards\":\"5\",\"number_of_replicas\":\"1\",\"version\":{\"created\":\"1060099\"},\"uuid\":\"3C6DtOpzTm6s19_DCw_FPQ\"}},\"mappings\":{},\"aliases\":[]}}}}\ncontent_2015_06_16\n2015-06-15 20:58:37,983 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-06-15 20:58:37,986 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/foo HTTP/1.1\" 200 370\n2015-06-15 20:58:37,986 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/foo [status:200 request:0.003s]\n2015-06-15 20:58:37,986 DEBUG              elasticsearch    log_request_success:65   > None\n2015-06-15 20:58:37,986 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"untergeek_testing\",\"metadata\":{\"templates\":{},\"indices\":{\"foo\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1434399801715\",\"number_of_shards\":\"5\",\"number_of_replicas\":\"1\",\"version\":{\"created\":\"1060099\"},\"uuid\":\"fzt4JPo9SlmTjs-hufvzQw\",\"blocks\":{\"write\":\"False\"}}},\"mappings\":{\"mytype\":{\"properties\":{\"name\":{\"type\":\"string\"}}}},\"aliases\":[]}}}}\nfoo\n2015-06-15 20:58:38,028 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-06-15 20:58:38,030 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/foo1 HTTP/1.1\" 200 294\n2015-06-15 20:58:38,030 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/foo1 [status:200 request:0.002s]\n2015-06-15 20:58:38,030 DEBUG              elasticsearch    log_request_success:65   > None\n2015-06-15 20:58:38,030 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"untergeek_testing\",\"metadata\":{\"templates\":{},\"indices\":{\"foo1\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1434423436972\",\"number_of_shards\":\"5\",\"number_of_replicas\":\"1\",\"version\":{\"created\":\"1060099\"},\"uuid\":\"NCSGKRzERu2lwnEjs_sJFg\"}},\"mappings\":{},\"aliases\":[]}}}}\nfoo1\n2015-06-15 20:58:38,031 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-06-15 20:58:38,081 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/grafana-dash HTTP/1.1\" 200 302\n2015-06-15 20:58:38,081 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/grafana-dash [status:200 request:0.050s]\n2015-06-15 20:58:38,081 DEBUG              elasticsearch    log_request_success:65   > None\n2015-06-15 20:58:38,081 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"untergeek_testing\",\"metadata\":{\"templates\":{},\"indices\":{\"grafana-dash\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1434423437802\",\"number_of_shards\":\"5\",\"number_of_replicas\":\"1\",\"version\":{\"created\":\"1060099\"},\"uuid\":\"06rAqx0LRMqs4N9qA4bhuw\"}},\"mappings\":{},\"aliases\":[]}}}}\ngrafana-dash\n2015-06-15 20:58:38,081 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-06-15 20:58:38,083 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/test HTTP/1.1\" 200 294\n2015-06-15 20:58:38,128 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/test [status:200 request:0.047s]\n2015-06-15 20:58:38,128 DEBUG              elasticsearch    log_request_success:65   > None\n2015-06-15 20:58:38,129 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"untergeek_testing\",\"metadata\":{\"templates\":{},\"indices\":{\"test\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1434423437070\",\"number_of_shards\":\"5\",\"number_of_replicas\":\"1\",\"version\":{\"created\":\"1060099\"},\"uuid\":\"xW1R2P02SlyboD11SgmUxg\"}},\"mappings\":{},\"aliases\":[]}}}}\ntest\n```\n@HonzaKral can you imagine a scenario where the python client isn't pulling the indices properly, but the _cat API is?  This makes no sense to me.  When I try to reproduce it, I see the indices properly.\n. @lakomiec if you'd be so kind as to replicate these steps and paste the results into here:\nRun python at the command-line.  You'll be presented with something rather like:\n\n```\n$ python\nPython 2.7.6 (default, Mar 22 2014, 22:59:56)\n[GCC 4.8.2] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n```\n\n\n\nAt the >>> prompt, please run these steps:\n1. import elasticsearch\n2. client = elasticsearch.Elasticsearch(host='127.0.0.1', port='9200')\n3. client.cat.indices().split('\\n')\nThe result for me looks something like this.  It should resemble the output from the cat API call you do via curl.  If it doesn't, then we have something new to investigate.\n[u'yellow open content_2015_06_13 5 1 0 0 720b 720b ', u'yellow open content_2015_06_07 5 1 0 0 720b 720b ', u'yellow open grafana-dash       5 1 0 0 720b 720b ', u'yellow open content_2015_06_10 5 1 0 0 720b 720b ', u'yellow open foo                5 1 1 0  3kb  3kb ', u'yellow open foo1               5 1 0 0 720b 720b ', u'yellow open test               5 1 0 0 720b 720b ', u'yellow open content_2015_06_08 5 1 0 0 720b 720b ', u'yellow open content_2015_06_09 5 1 0 0 720b 720b ', u'yellow open content_2015_06_12 5 1 0 0 720b 720b ', u'yellow open content_2015_06_16 5 1 0 0 720b 720b ', u'yellow open content_2015_06_15 5 1 0 0 720b 720b ', u'yellow open .kibana            5 1 0 0 720b 720b ', u'yellow open content_2015_06_11 5 1 0 0 720b 720b ', u'yellow open content_2015_06_14 5 1 0 0 720b 720b ', u'yellow open content_2015_06_06 5 1 0 0 720b 720b ', u'']\n. If you could also run this one:\npython\nlist(client.indices.get_settings(index='*', params={'expand_wildcards': 'open,closed'}))\nIt should show a list of all indices, open and closed.\n. I tested with 1.5.2 and found that it was the same in either case for me.\nNow to pin it down:\ncurl -XGET http://localhost:9200/*/_settings?expand_wildcards=open%2Cclosed\nWhat's your output from this?  I'm guessing you will only see test, .kibana and grafana-dash here.\n. What if we remove the expand_wildcards portion?\ncurl -XGET http://localhost:9200/*/_settings?pretty\nIf that one works, but the wildcards portion doesn't, then we're making progress!  @HonzaKral was right.  This problem is actually not within Curator, but Elasticsearch itself.  This query should be returning all indices, open or closed, but it isn't.  Clearly the cat API is showing the indices, but this call isn't for some reason I cannot fathom.  But when I create identically named indices on a 1.5.2 cluster, I can see them, so it's not a name issue.  I will ask some of the Elasticsearch devs to help figure this one out.\n. Do you get the same result from each node in the cluster?  How many nodes do you have?\nAlso, what happens if you run:\ncurl -XGET http://localhost:9200/content_2015_06_08/_settings?pretty\nThis eliminates the wildcard and only targets that specific index.\n. I just noticed that you have 20 shards + 1 replica, which translates into 40 shards per index.  Why are you going with that choice?\n. Generally speaking, the default 5+1 is more than enough for performance and scalability.  I would never recommend going beyond 1 shard per node (ignoring replicas).  The reason is that we recommend never having more than 300 shards per node (any combination of primaries & replicas to equal that number).\nWe recommend no more than 30.5G heap per ES instance.  10% of the heap is, by default, set aside for index caching and management.  Active shards each want 250M of that 10%, and inactive (shards which haven't indexed in the last 5 minutes [for recent versions of ES]) 4M of that 10%.  Presuming 5 active shards and 295 inactive shards, that's 1.25G + 1.18G for a total of 2.43G.  That would mean you'd need a heap greater than 24G to handle that many shards, cleanly. \nAdditionally, higher shard counts will increase recovery time as there are more pieces to move (by default, ES only relocates or rebuilds 2 shards at a time, per node).  This also increases the cluster management overhead.\nMoreover, higher shard counts will actually increase search time as there are more shards a search has to look through.  If fast search is part of your plan, consider fewer shards but more replicas.  More replicas increase the search speed because there are more copies of the data to search from.  Higher shard counts will improve indexing performance, but at the costs outlined.\n. Trying another, what comes out if you try this?\ncurl -XGET http://localhost:9200/_all/_settings?pretty\n. Please show every not commented line in your elasticsearch.yml file (you can mask IPs or anything sensitive).\nWhat plugins have you installed?  It's possible a plugin is interfering.\n. @burtonator Shard splitting doesn't work because document<->shard mapping is handled by something like this (we teach this in Core ES training):\nhash(_id) % shard_count\nShard count has to be static for this to work.  If you could split shards, you'd ruin the math there and not be able to find documents by _id in that index again.   Yes, if you anticipate needing that many shards, starting with that number makes sense.  However, there are, as mentioned, trade-offs.  There are reasons to have more shards, like if your index is so big that the shards would be bigger than 50G (the maximum recommended size).  \nElasticity can be increased by using aliases.  You can re-index into another index with more shards, if needed, and use aliases to point from A->B in an atomic API call, keeping your data available constantly.\nIf you have a 5G heap, I really don't recommend using 20+1 shards per index unless you only have a few indices.  Those recommendations I left are best case scenarios.  Elasticsearch will squash those desired amounts to work with whatever it can get.  I've seen the 250M for active shards compressed down to 47M, and inactive shards down to 760K.  It will eventually reach the point where it simply cannot allocate any more shards, however, and it will fail to index any more data.  It will do this silently, and without warning.  Search will still work, however.\nOut of curiosity, why would you want shard splitting?  What advantage does 20 shards give over 10?  More shards, as mentioned, increases the number of places a node has to look for the data it wants.  An Elasticsearch user I recently did some troubleshooting with at a meetup did something similar.  They counted the number of threads in their system and thought that more shards would more effectively use CPU cores.  They were doing over 1,000,000 queries per second on their 20+ node cluster, but it was beginning to have outages when certain queries would happen.  When it came out that they had 16+1 shards across some 300 indices, my colleague and I immediately suggested they take those indices down to 2+2 (or 3) shards for the < 10G indices, and 3+2 for the 10G > 20G indices, and a similar pattern for other indices.  More replicas means faster searches, more shards means faster indexing.\nMy documentation sources cannot be shared as it is from the Core ES training slides (which are only available to training attendees) and the Knowledge Base we share with our support customers.  The memory limits are especially mentioned in time-series indexing cases, because the index count always grows.  If you don't plan on having continuous index growth, you would likely never hit those limits. \nIf you are planning on having such a big cluster, I recommend sending devs to our Core ES training and engineers to our soon-to-be-released ops training.  These concepts, and more, are taught in these courses.  We also discuss cluster design in support kick-off calls.\n. I guess it's live.  Our Core Elasticsearch Operations course is now available at http://purchases.elastic.co\n. Though functionally they should be no different (which has been confirmed by multiple ES devs), I have replaced * with _all on https://github.com/elastic/curator/blob/3.x/curator/api/utils.py#L26\nIf you find this file in your installation and change that line, Curator should work as expected.\nWe cannot explain why the wildcard expansion is only partial for you.  If you submit a ticket at https://github.com/elastic/elasticsearch with the details of the failure to expand, they may take a deeper look at it.  With this unusual circumstance, there's no telling what else may be off in your cluster.\n. That change has not been merged yet, but is in my current branch, btw.\n. What do you see if you run it with the --debug flag?\n. @robin13 Would you see if you get more information from Curator 3.2.0?  I tried to put some more logging in place.\n. Yep!  See this FAQ in the official documentation.\nBottom line: RHEL ships with an ancient (in computer terms) version of Python, and the setuptools package needs to be upgraded for the installation to work.  The FAQ doc for this issue has all the details. \n. This is the expected behavior.  The countdown only happens if you specify --timestring without other time-identifying flags.  See #348 and #349 for more information.  This edge case is documented here:\n\nStarting in v3.1.0, if you specify --timestring without --older-than or --newer-than, the process will continue, but with a message warning you that the operation will delete all indices with a time string, followed by a 10 second countdown timer, which provides the opportunity to cancel the operation.\n. I didn't realize you were addressing another issue.  The --all-indices flag supersedes all other flags.  It will always return all indices.  You can only use the --exclude flag to remove things from an --all-indices list.  See the --all-indices documentation\n. To address this question:\nSo my question is: Is this expected? Eg \"--older-than\" does not work with \"--all-indices\" and I'd have to use regex to delete all indices older than 30 days for example.\n\nThe answer is still with the --timestring flag documentation:\ncurator delete indices --timestring '%Y.%m.%d'\n2015-06-19 09:54:53,728 INFO      Job starting: delete indices\n2015-06-19 09:54:53,740 INFO      Pruning Kibana-related indices to prevent accidental deletion.\nYou are using --timestring without --older-than or --newer-than.\nThis could result in actions being performed on all indices matching %Y.%m.%d\nPress CTRL-C to exit Curator before the timer expires:\n10 ^C\nAborted!\nIn your above example, using the --timestring flag triggered the countdown, even if it erroneously was going to select all indices.  Regexes are implied.  Using --timestring %Y.%m.%d will match all indices that match that time pattern, which in my case would be all logstash-YYYY.MM.dd and .marvel-YYYY.MM.dd indices.\n. Delete indices older than 30 days that have a %Y.%m.%d pattern:\ncurator delete indices --timestring %Y.%m.%d --older-than 30 --time-unit days\n. Omitting --prefix, --suffix, or --regex will simply match on the pattern provided, which in this case would be (not a real regex): ^.*[date pattern].*$\n. It is a bit confusing.  The filters are just regexes. They're built in this method: https://github.com/elastic/curator/blob/master/curator/api/filter.py#L29-L89\nThe --all-indices behavior is here: https://github.com/elastic/curator/blob/master/curator/api/filter.py#L29-L89\nAs mentioned, it simply ignores all other flags.  You can see that the --exclude option is applied later.\n. Each filter you use, --prefix, --suffix, etc. is applied iteratively to the list of indices, the results being used for the next filter iteration.\nFor example, if I have a --prefix, one of the filters (which is a list of regexes) will be ^prefix.*\nIf I have a --suffix, one of the filters will be ^.*suffix$\n. These package will not be bundled in a way that requires the license files.\n. Thanks for catching that!\nFeel free to add yourself to the CONTRIBUTORS file.  I'll be happy to merge when that's done!\n. And after you've signed the contributor's agreement, too.\n. Thanks for making Curator more awesome!\n. Thanks for being willing to contribute to Curator! I'd be more than happy to merge any additional utility methods you add to the API.\nMy expectations are:\n- The methods are well annotated/documented following the existing pattern.\n- Lots of unit tests (they'll need to pass both python2 and python3).  If you need help writing tests, please ask.\n- Add the methods to the API documentation (again, if you annotate/document the methods well, this is as simple as following the pattern in the docs, they'll self-build).\n- Add yourself to the CONTRIBUTORS file.\n- Sign the contributor agreement \u2014 No merging will happen without this step.\n. Packages come later :(\nOtherwise, we're good here.\n. I appreciate that this is a bug for you.  This is extremely unusual, except in cases where the cluster is heavily overburdened.  The delete API call checks for exceptions, and if there are none, it returns True.  This is why it's considered a successful operation.  Elasticsearch itself is capable of deleting hundreds of indices simultaneously when not under egregious loads.  The test suite itself does this with many indices simultaneously.  If your cluster is failing to do this, you should look at the Elasticsearch logs to see what's happening.  Even the time elapsed in the log you provided suggests it sent all of the deletes in a 2 second window.\nWhat happens if you run this with the --debug flag?  If you're trying to delete older than 7 days, why are you waiting until there are 5 indices of every kind, rather than running daily to prune the number of indices more regularly?  How many nodes in your cluster?  How many shards per node?  The count of indices you have is concerning and could be a contributing factor.\n. Please attach or link any corresponding Elasticsearch node logs.  The client you connected to on port 9217 is the most important, but all others need to be looked at.\nWhy 9217, by the way?  Are you really running 18 or more local nodes so the number is bumped?\n. I count 94 indices you're trying to delete at once.  This translates into 940 shards, at the default 5+1.  The first pass is 66 indices, which would be 660 shards at the default 5+1.\nIs 5+1 your index template?  Or are you creating these with more or fewer shards/replicas?\n. Disk space is only part of the equation.  It's actually the least important part.  Each shard in Elasticsearch comes with a management cost.  The master node(s) have to ensure that all nodes keep a current cluster state.  The index cache portion defaults to 10% of the allocated heap.  Each active shard wants 250MB of that 10%, and each inactive shard wants 4M, and you have some indices with 40+0 shards. If a node cannot keep all of the shards in the 10% with these values, it begins to compress them.  This compression slows down indexing and is tied to overburdening of the cluster.  In fact, with too many shards per index, I've seen those values compressed to the point that there was only 47M of cache available for each active shard and 750K for each inactive shard.  At or around that point, Elasticsearch stopped indexing outright, though search still worked.\nThis is why I asked how many nodes you have, and how many shards per node.  Also important to add to the equation is how big your heap is on each node.  The sheer number of indices and shards you have could be leading to this pain point.\n. How big is your cluster state?  (curl -XGET 'http://localhost:9200/_cluster/state', then check how big the resulting file is). With so many indices (and shards), and each having their own types & mappings, it could be quite large.  Trying to keep such a large cluster state in sync between all of your nodes could be contributing to the problem.\nAdding a verification step is possible, but tedious.  I hesitate to add verifications to things for edge cases like this where your cluster state is likely a contributing factor.\n. How many nodes, and what is the heap size?\n. What's the disk I/O at?  How many shards per node?\n. Also, how many master nodes?\n. You can also try the code change in #423 to see if that works for you.  This is a work-around for an edge case.  Elasticsearch should never fail in this way.  Something in your cluster is not right.\n. Each box has 16 drives?  RAID6 is certainly not known for its write speed, regardless of how underutilized it may appear.  For an indexing heavy operation (which yours appears to be, based on the writes), this would appear to be an interesting choice.  Is that to mitigate the absence of replica shards?\n. @TinLe any news on this?\n. I've just released v3.2.3 which adds master_timeout to the delete command only.  It will match the --timeout value, but only up to 300 seconds.  Hopefully this addresses the issue here.\nI'm still concerned to see whether 300 seconds will be sufficient for some of these cases.  If that happens, my advice will be to try to delete in smaller batches.\n. I need an \"edge_case\" label :smiley: \nWhile this is technically a bug, it's also been fairly rare.\n. In order to prevent snapshots from being incomplete (i.e. one of the nodes isn't able to write to the shared filesystem), Curator runs a repository verification before each run if you have a supported version of Elasticsearch (1.4+).\nWhat this suggests is that adding the VPC endpoint has altered Elasticsearch's ability to write and/or verify snapshots, or it's too slow to respond, resulting in a false positive (or negative, as the case may be).  With the VPC enabled, you should probably run the API call manually and see what shakes out, both in the returned message, and in the Elasticsearch log files:\ncurl -XPOST localhost:9200/_snapshot/REPOSITORY_NAME/_verify?pretty\nBecause this is a safety feature, I do not recommend trying to work around it without knowing for sure that all of your nodes are truly backing up properly.  That said, if this VPC endpoint is timing out, you can use the --skip-repo-validation flag, if you're using 3.2.0\n. I'm closing this as there has been no further comment.  If you believe this is in error, please feel free to reopen the issue, or open another new one.\n. :+1: \n. I hate merge conflicts...\n. I see what's happened here. All of the indices are closed, and with no indices, the seal operation is trying to seal all of the indices.  I'll put in a quick fix for this.\n. That's not a bad idea.  I will not likely implement it before 4.0, however.  In the meantime, it's super-trivial to add &> /dev/null to the end of a cron job.\n. This should be addressed by #595 and #596 with Curator 4.0 imminent.\nIf you feel this is in error, please feel free to reopen this ticket or open another.\n. This is a bug.  You've encountered an error I have not anticipated.\nThe flow here is that when you send a command to close indices, and you are using Elasticsearch 1.6+, it will try to seal (synced flush) open indices before closing them.  \nWhat's happened is that, in trying to seal an index, Curator got an AttributeError exception, where I only ever anticipated TransportError exceptions.  I cannot explain why you received an AttributeError exception, however.  That sounds like something else is going on.  Better error-type capturing will allow me to display the results of the AttributeError.\nAre all nodes in your cluster running 1.6.0?  Are your logstash instances using the node protocol, or the http protocol?  If node, it means that not all of your cluster is running ES 1.6+ (as Logstash's client node is still probably ES 1.5-ish, depending on what version of Logstash you're using.  That's my current stab in the dark.\n. Curator 3.2.1 was just released, and it should at least show a better error for this case.  Please test it and let me know what you find.\n. That issue has nothing to do with the one raised.  A recent change to the pbr python module has broken mock, and this has caused a chain of failures.  I will investigate changing the dependencies to get around this, at least until it's resolved.\n. Actually, it's not the pbr module that's causing problems.  It's mock itself: They officially dropped support for Python 2.6 https://github.com/testing-cabal/mock/commit/8a2e8709502c30e2dd0838125bc5c34fec35f451\nAs a significant number of Curator users are on RHEL/CentOS versions which are stuck on 2.6, this presents a problem.  I will pin mock to 1.0.1 as a result.\n. I'd love to see what error came out, if you got an Exception with closing indices during the seal operation (supposed to be non-fatal with seal/synced flush).  Thanks for the update that all is working for you.\n. Is every node in your cluster running Elasticsearch 1.6 or higher?\nThe error:\nfails = [ i for i in sorted(results) if results[i]['failed'] > 0 ]\nTypeError: sequence index must be integer, not 'str'\nMeans that results[i] is not a dictionary object, but a list, for some reason.  I'd like to know why.\nPlease run curator --debug close indices --time-unit days --older-than 30 --timestring %Y.%m.%d --prefix logstash-netflow5- and attach the results.\n. @romale, because the error you are getting is different from the one in this closed issue, please open a new issue.\nAnd I think I know what is happening, as you were taking a huge chunk of indices, but it worked with a smaller one.  It looks like I'll need to add the master_timeout flag to the seal command.\n. Your timestring is improperly formatted.  It's supposed to be a python strftime string, not an actual date.\nPlease see the --timestring documentation for more information.\n. Closing...\n. Self-signed certificates will not be recognized by Certifi.  The certs used by Certifi are in a different location, so those other modules will not do anything, nor will installing system-level certificates.  \nSee http://certifi.io/en/latest/ for more information on how Certifi works.\nNow, it may be possible to add the CA from your self-signed certificates to the ones Certifi uses.  First, you'll have to find where Certifi's cacert.pem file is.  On my laptop, it's here: /usr/local/Cellar/python3/3.4.2_1/libexec/pip/pip/_vendor/certifi/cacert.pem\nIn theory, all you'd have to do is append your certificate information in the matching format.\nThe last entry in mine looks like this: \n```\nIssuer: CN=Atos TrustedRoot 2011 O=Atos\nSubject: CN=Atos TrustedRoot 2011 O=Atos\nLabel: \"Atos TrustedRoot 2011\"\nSerial: 6643877497813316402\nMD5 Fingerprint: ae:b9:c4:32:4b:ac:7f:5d:66:cc:77:94:bb:2a:77:56\nSHA1 Fingerprint: 2b:b1:f5:3e:55:0c:1d:c5:f1:d4:e6:b7:6a:46:4b:55:06:02:ac:21\nSHA256 Fingerprint: f3:56:be:a2:44:b7:a9:1e:b3:5d:53:ca:9a:d7:86:4a:ce:01:8e:2d:35:d5:f8:f9:6d:df:68:a6:f4:1a:a4:74\n-----BEGIN CERTIFICATE-----\nMIIDdzCCAl+gAwIBAgIIXDPLYixfszIwDQYJKoZIhvcNAQELBQAwPDEeMBwGA1UE\nAwwVQXRvcyBUcnVzdGVkUm9vdCAyMDExMQ0wCwYDVQQKDARBdG9zMQswCQYDVQQG\nEwJERTAeFw0xMTA3MDcxNDU4MzBaFw0zMDEyMzEyMzU5NTlaMDwxHjAcBgNVBAMM\nFUF0b3MgVHJ1c3RlZFJvb3QgMjAxMTENMAsGA1UECgwEQXRvczELMAkGA1UEBhMC\nREUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCVhTuXbyo7LjvPpvMp\nNb7PGKw+qtn4TaA+Gke5vJrf8v7MPkfoepbCJI419KkM/IL9bcFyYie96mvr54rM\nVD6QUM+A1JX76LWC1BTFtqlVJVfbsVD2sGBkWXppzwO3bw2+yj5vdHLqqjAqc2K+\nSZFhyBH+DgMq92og3AIVDV4VavzjgsG1xZ1kCWyjWZgHJ8cblithdHFsQ/H3NYkQ\n4J7sVaE3IqKHBAUsR320HLliKWYoyrfhk/WklAOZuXCFteZI6o1Q/NnezG8HDt0L\ncp2AMBYHlT8oDv3FdU9T1nSatCQujgKRz3bFmx5VdJx4IbHwLfELn8LVlhgf8FQi\neowHAgMBAAGjfTB7MB0GA1UdDgQWBBSnpQaxLKYJYO7Rl+lwrrw7GWzbITAPBgNV\nHRMBAf8EBTADAQH/MB8GA1UdIwQYMBaAFKelBrEspglg7tGX6XCuvDsZbNshMBgG\nA1UdIAQRMA8wDQYLKwYBBAGwLQMEAQEwDgYDVR0PAQH/BAQDAgGGMA0GCSqGSIb3\nDQEBCwUAA4IBAQAmdzTblEiGKkGdLD4GkGDEjKwLVLgfuXvTBznk+j57sj1O7Z8j\nvZfza1zv7v1Apt+hk6EKhqzvINB5Ab149xnYJDE0BAGmuhWawyfc2E8PzBhj/5kP\nDpFrdRbhIfzYJsdHt6bPWHJxfrrhTZVHO8mvbaG0weyJ9rQPOLXiZNwlz6bb65pc\nmaHFCN795trV1lpFDMS3wrUU77QR/w4VtfX128a961qn8FYiqTxlVMYVqL2Gns2D\nlmh6cYGJ4Qvh6hEbaAjMaZ7snkGeRDImeuKHCnE96+RapNLbxc3G3mB/ufNPRJLv\nKrcYPqcZ2Qt9sTdBQrC6YB3y/gkRsPCHe6ed\n-----END CERTIFICATE-----\n```\n. Since Certifi is super-simple (it's very few lines of code + a cacert.pem file), I may be able to hack in a way to provide your own cacert file for verification, and fall back to Certifi when that is not provided.\n. Working on adding this feature for 3.3.0.\nNew flags:\n--certificate will be a path to a ca_cert of some kind.\n--ssl-no-validate will disable ssl certificate validation.\nSo, if you provide a certificate, it should check with that.  If not, it will try the certifi certs, unless you use the --ssl-no-validate flag.\n. I will chalk this up to another vote for #428 \nI will silence non-log output if --quiet is provided.\n. And/or force --quiet mode if --logformat is set to logstash.\n. Yes.  It will only silence places in the code where the Click API is used to output directly to the console (so you'd see those messages even if you were logging to a file).\n. Continuing the discussion from #429, it seems I'm at least half-right.  It's a timeout issue, but it doesn't seem to be a master_timeout issue (thought that may yet come into play).\nNote the error: \nReadTimeoutError: HTTPConnectionPool(host=u'localhost', port=9200): Read timed out. (read timeout=30)\nThis means it took longer than 30 seconds for the client to respond to Curator, so it timed out.  30 seconds is the default value.  I will change the error output from being DEBUG to ERROR so this is more visible.  \nNow, the way to effectively test if this is merely a client timeout, or also a server-side master_timeout, is to re-run your command (keeping the --debug flag) with an increased --timeout, say 60 or 90 seconds:\ncurator --debug --timeout 60 close indices --time-unit days --older-than 30 --timestring %Y.%m.%d --prefix logstash-netflow5-\n. Run that command with --debug and attach, please.  It should show closed indices with (CLOSED) to the side.\n. We didn't hit the timeout this time, but we also wouldn't have hit the master_timeout either:\n2015.06.17/_flush/synced [status:200 request:27.812s]\n27 seconds is underneath both thresholds.\n. An example of (CLOSED):\ncurator show indices --time-unit days --older-than 14 --timestring %Y.%m.%d\n2015-07-17 08:50:33,420 INFO      Job starting: show indices\n2015-07-17 08:50:33,449 INFO      Matching indices:\n.marvel-2015.07.03 (CLOSED)\nlogstash-2015.07.03 (CLOSED)\n. It's been a while since there has been any updates here. I'm going to close this.  \nFeel free to re-open the issue if you have more to add.\n. This was addressed by version 3.2.3\n\u2014Aaron\nOn Sat, Jul 18, 2015 at 4:57 PM, allenmchan notifications@github.com\nwrote:\n\nI am trying to delete really old indices that have been closed for awhile. I cannot tell if this is issue with curator or elasticsearch. I see no new log entries in the master node's elasticsearch log\nMy command is\nsudo curator --debug --logfile=/var/log/curator/delete.log delete indices --older-than 200 --time-unit days --timestring '%Y.%m.%d'\nHere is the part in the curator log where the error occurs. \nhttps://gist.github.com/allenmchan/bfc1a98e6067619afe3b\nThanks\nReply to this email directly or view it on GitHub:\nhttps://github.com/elastic/curator/issues/442\n. Have you tried increasing the --timeout value? \u00a0You should be able to go as high as 300 with the current build of Curator.\u00a0\n\n\u2014Aaron\nOn Sat, Jul 18, 2015 at 5:15 PM, allenmchan notifications@github.com\nwrote:\n\nClosed #442.\nReply to this email directly or view it on GitHub:\nhttps://github.com/elastic/curator/issues/442#event-359332240\n. Thanks for adding this functionality.  Honestly, I'm kind of surprised that nobody has requested it before.\n\nCan you respond to the suggestions I've made, and perhaps look into adding some integration tests as well?  If not, I will add those later, but I definitely want an integration test per type.\n. Please be sure to also:\n- Add yourself to the CONTRIBUTORS file\n- Add something to the Changelog.rst file\n. Sorry for the delay.  Was in Portland for OSCON and training.  I've been catching up ever since.\n. The --reverse flag is only for use with the --disk-space flag:\n--reverse BOOLEAN   Only valid with --disk-space. Affects sort order of the\n                      indices.  True means reverse-alphabetical (if dates are\n                      involved, older is deleted first).  [default: True]\nYou're indicating that you want to delete older than 3 months, but are providing a daily timestamp.  The --time-unit is not merely a unit of time, but should be the unit of time by which your indices are reckoned.  As a result, you should convert months to days, then delete older than 90 days instead:\ncurator delete indices --older-than 90 --time-unit days --timestring %Y.%m.%d --prefix \"logstash-\"\n. Also, when using a wildcard with your regular expression, you need to encapsulate in single-quotes, and it's good practice to fully define your regex, whether with ^ and $, or use the .* notation to indicate a wildcard matching anything.  The following example will match indices beginning with (^) logstash-2015 and ending with anything else (.*$):\ncurator delete indices --regex '^logstash-2015.*$' --older-than 90 --time-unit days --timestring %Y.%m.%d\n. Using the wildcard without the single-quotes left it free to match much more.\n. That's an excellent question.  Without enabling a logging instance, you won't be able to troubleshoot this very well.\nI can tell you that if you don't increase the timeout in your client definition, it will likely return with either a failure, or keep running in the background, but with the client timed out (I've learned not to trust the request_timeout flag).  This is because an optimize is a very long-running operation, longer than the default 30 second timeout that comes with a client.\n. The flag is just timeout:\nclient = Elasticsearch( hosts='es_in_production', timeout=20 * 60 )\nI suppose the 20 * 60 works, but I'd still just put 1200. \n. Since you haven't raised any new issues, I'm presuming this resolved things for you.\nIf you find another bug or issue, please open another ticket.\n. I suppose I could add this.  It's a lot of work for a binary feature, though, because it will result in some API changes (which means it will not make it in until Curator 4.0 at the earliest).\nIn the meantime, I simply have 2 shell scripts for this:\nDisable allocation\n``` bash\n!/bin/bash\ncurl -XPUT localhost:9200/_cluster/settings -d '{\n  \"transient\" : {\n    \"cluster.routing.allocation.enable\" : \"none\"\n  }\n}'\necho\n/usr/local/bin/curator --loglevel ERROR seal indices --all-indices\n```\nEnable allocation (after restart)\n``` bash\n!/bin/bash\ncurl -XPUT localhost:9200/_cluster/settings -d '{\n  \"transient\" : {\n    \"cluster.routing.allocation.enable\" : \"all\"\n  }\n}'\n```\n. This will require a different approach now with 4.0, but it may be of some benefit between some actions.  I'll see if I can add this as an action to 4.0 now that an alpha is out.\n. This feature makes no sense in the current incarnation of Curator.  It may be resurrected when I re-add the command-line tools (as separate scripts) in the future.  Re-flagging as on_hold.\n. This actually does make sense to me now.  As a simple, filterless action, this works to disable allocation before something happens, and then re-enable after another action is completed.\n. Sorry for the inconvenience! This is because I failed to increase the dependency on the elasticsearch-py module to be 1.6.0 or higher.\nIf you installed with pip, then you can fix this oversight on my part by running:\npip install -U elasticsearch\nWhich should install the most recent version of the Elasticsearch python module, which has the flush_synced API call.\n. And this shouldn't be related to #441 \n. That doesn't make sense, because the only elasticsearch module in yum is python-elasticsearch-1.6.0-1.noarch.rpm\nCan you do this for me?\n```\n$ python\nPython 2.7.6 (default, Jun 22 2015, 17:58:13)\n[GCC 4.8.2] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport elasticsearch\nelasticsearch.version\n(1, 6, 0)\n```\n\n\n\nYour version may vary, but you should be able to run the import elasticsearch and then the elasticsearch.__version__ commands at the python interactive shell.  Use exit() or a CTRL-D to exit the shell.\nPlease paste your results here.\n. That (1, 0, 1) indicates that you have a previously installed version of the elasticsearch python module (1.0.1), which is incompatible with the flush_synced API call.  Perhaps you can upgrade via yum?  Barring that, uninstall that 1.0.1 version, then install the one in the Curator repository.\nAlso, this may be related to #441, if you have such an old version of the elasticsearch python module.\n. @pricecarl, did you install with pip first, and then switch to yum/RPM?  If not, is there some other repository you're using that has a version of the elasticsearch python module?  There could be a conflict, if so.\n. Some other repository you're subscribed to must provide elasticsearch, then.  The Curator repository only provides 1.6.0.\n. Which is why I need to update the dependency to require v1.6.0+ of the elasticsearch python module.\n. Not sure.  Some RPM repository you are subscribed to is providing 1.0.1.  If Curator's dependency was on 1.6.0, the Curator repo should have provided that.\n. The 409 error in sealing (synced flush) one or more indices is non-fatal.  It just means the index was busy merging new segments or indexing new documents so it couldn't seal.  Or in your case, something else was going on because your 409 error was because \"all shards failed to commit on pre-sync\".  It did take 20.911 seconds to perform the synced flush against proxylog-2015-07-29 and proxylog-2015-07-30 combined (which could mean 5 seconds for the one that succeeded, and 15.911 for the one that didn't successfully seal.\nIn any case, they both successfully closed.  Not sealing properly before closing means that it will take longer to open (if you ever re-open) the index.\n. @imreFitos please see my comment on #517.\n. This should be addressed by #595 and #596 with Curator 4.0 imminent.\nIf you feel this is in error, please feel free to reopen this ticket or open another.\n. Thanks for the report.  This is indeed an oversight.\n. I tried that.  I got well-reasoned pushback in #371 so I am only using 0 and 1 for the most part.\n. Thanks for the patch!  Have you signed the CLA yet?\n. Confirmed.\n. Thanks for reporting your experience.  The error and workaround are documented in the install documents:\n\nThere are some pitfalls you may encounter:\n- ImportError: No module named pkg_resources\nIf you see this error:\nTraceback (most recent call last):\n  File \"/usr/bin/curator\", line 5, in <module>\n    from pkg_resources import load_entry_point\nImportError: No module named pkg_resources\nThen you will need to install python-setuptools (provided in the Curator repository):\nyum install python-setuptools\nThe repository provides python-setuptools-18.0.1-1.noarch.rpm\n. If there are no other comments, I will be closing this.\n. Mostly because the workaround is for ancient versions of Python on CentOS/RedHat boxes.  Every other native distribution tends to have it.  I don't want to stomp on the shipped versions, so I do nothing here.\n. Well, CentOS 6.7 isn't ancient, but the python version shipped with CentOS 5 and 6 is.  I believe it's because of a dependency issue related to how their package management works.  Upgrading python on those boxes is a no-no, though you may be able to run your own python3.\n. Good observation!  I didn't write this code, but didn't think about it at the time I merged it.\n\nBy all means, submit a pull request and replace primary_size_in_bytes with size_in_bytes:\n{u'size_in_bytes': 5722926301, u'primary_size_in_bytes': 2861508827}\nIt will give the total consumed by primary + all replicas.\n. Had to put in a conditional.  size_in_bytes only appears if there are replicas.\n. If you'd like to write this in, and add tests for it, I'd be happy to merge it.\n. The more I look at this the stranger it feels.  Maybe I'm not understanding you correctly.\nYou want to ensure that a single index does not consume more than 200GB?  What happens if it gets bigger than 200GB?  You're just going to delete an index if it's too big?  Is that what you're requesting?\n. Closing this for non-response.  I have other ideas for index rollover at size threshold, so that base will eventually be covered.\n. You could add the --debug flag to see more.\n7200 seconds is extremely short for a snapshot.  I do not recommend using the --request_timeout flag as it is most commonly used in conjunction with\u2014but opposition to\u2014the --timeout flag.  --timeout is for the client timeout (the connection to Elasticsearch), where --request_timeout should be less than the --timeout flag.  --request_timeout is used to force Elasticsearch to return in the event a request is long running.  It's not for managing a client-level timeout.\nThose things said, it looks like you have almost exactly 4 minutes between creating the client and the timeout, which is less than the client timeout of 21600 and the request_timeout of 7200.  Chances are you're hitting a gateway timeout (504) error, described in this FAQ.  You can see for sure if you use the --debug flag.\n. Well, it's not a 504, but you are getting several 503 errors, which are \"Service unavailable\" errors.\n2015-08-17 12:07:25,544 WARNING            elasticsearch       log_request_fail:82   PUT /_snapshot/my_s3_repository/mdp-2015.08.04?wait_for_completion=true [status:503 request:0.007s]\nYou're also seeing a 60 second timeout:\n2015-08-17 12:07:25,443 WARNING            elasticsearch       log_request_fail:82   PUT http://haproxystaging:9200/_snapshot/my_s3_repository/mdp-2015.08.04?wait_for_completion=true [status:N/A request:60.002s]\nThis timeout is what the **ProtocolError: ('Connection aborted.' part is reporting.\nJust from the hostname, haproxystaging, I presume you have your Elasticsearch clients behind some kind of proxy gateway.  This is why you're timing out.  Your HAProxy is forcing the client connection to timeout at 60 seconds, just like in the FAQ I referenced in my last comment.\nYour options include:\n1. Don't connect to the HAProxy for snapshot operations, but instead have a direct client connection.\n2. Use --wait_for_completion False (it will run in the background, and you'll have to use some other tool to check for completion).\n3. Increase the HAProxy timeout to a ludicrous number somehow (not recommended unless it can be limited to this use case).\n4. Something else I haven't thought of...\n. Glad that ironed it out for you.  We definitely recommend using client nodes where possible, instead of connecting directly to data nodes.\n. With regards to your timestring, it's only a regex.  Without providing any additional filters than timestring, you're effectively sending a matching regex like: ^.*\\d\\d\\d\\d.*$, and looking for 4 numbers in a row.  This is why it's matching any index with 4 numbers in a row.  Though you've only provided %Y, you are specifying days as your time unit.  However, there is nothing currently checking to ensure that you have a day in your timestring.  This is also why you're picking up indices much newer than the 365 day cutoff, because your timestring specifies only years, but you want to filter by days.\nWith regards to Kibana indices, Curator only prunes the 3 default Kibana index names: .kibana (K4), kibana-int (K3), and .marvel-kibana (Marvel).  Thus kibana-int-foo is not a match.  This is documented here in the Note: section.\nThat said, I cannot understand why kibana-int-foo is even matching your search criteria at all.  If you run with --debug, it will provide much more insight into the regex used, and other information.\n. No more information, @faxm0dem ?\n. Send it to aaron at mildensteins period com\n. @faxm0dem I never received anything...\n. Well, this seems to explain it.  You have 4 digit \"dates\" in your kibana-int-foo:\nkibana-int-ccosvms0053\nYou're doing a date regex on %Y, which is a 4 digit number.\n2015-08-28 16:14:02,656 DEBUG     curator.cli.index_selection                indices:94   Filter: {'pattern': '(?P<date>\\\\d{4})', 'value': 365, 'groupname': 'date', 'time_unit': 'days', 'timestring': u'%Y', 'method': 'older_than'}\nAs you can see, the filter is only looking for \\d{4), or 4 digits.  You don't have kibana-int-foo, but rather kibana-int-foo\\d{4}, which is a match, where 0053 is a much older year than 2015 :smile: \nYou can still use the year number as a pattern.   I would simply add --exclude '^kibana' as a means to prevent those from being otherwise matched. (It may need to be a full regex, like '^kibana-int-.*$').\n. I should note that it is clearly omitting kibana-int-foo entries where there is not a 4-digit number.\n. Perhaps adding the word, ago, would help: --newer-than n --time-unit's ago. \nIt filters indices newer than n days ago, and it uses UTC, so remember that date math in your calculations.  Also, it only does full days, so that's probably partly why you are seeing some oddities.\n. Rather than throw a more readable exception, perhaps we should just fix the casting of the variable?\n. If we expect the variable to be sent as int, it will raise a different exception and not proceed with the method.\n. We can try to cast if it fails the int type check, and hard-fail the method if it fails at that point.\n. Actually, we only need to fix the casting.  You'll get a very different exception if the variable passed is not able to be cast.\n. And it can only be cast in the timestamp_check method, as value can be non-integer in other places.\nI'm fixing the casting of disk_space as well.\n. Sigh\nWhile all tests were passing on my machine, for some reason they stopped passing on our Jenkins setup.  Refactoring this change to hopefully patch Jenkins.\n. First, the patches you're submitting seem like they should be at the https://github.com/evenup-curator repo, for the job.pp file, rather than for Curator proper.\nSecond, I'm not sure which version you're using, but it should run just fine. The warning is just that, plus a 10 second delay before continuing as normal.  Why would that prevent cron from running the command?  I understand that this may add lines to stdout, but that shouldn't prevent cron from functioning.\nThe Python code in question is here:\nif timestring and not newer_than and not older_than \\\n          and not (ctx.parent.info_name == 'show') \\\n          and not ctx.parent.parent.params['dry_run']:\n            click.echo(click.style('You are using --timestring without --older-than or --newer-than.', fg='yellow', bold=True))\n            click.echo('This could result in actions being performed on all indices matching {0}'.format(timestring))\n            click.echo(click.style('Press CTRL-C to exit Curator before the timer expires:', fg='red', bold=True))\n            countdown(10)\nIt would be extremely trivial to add another line: \nand not (ctx.parent.info_name == 'snapshot')\nto this conditional, but I see no reason to do so until it can be proven why it won't work in cron as-is.\n. It is possible, but that may depend on the platform.  In some cases, it may work if you put curator in a wrapper script, but not if you call it directly.  I'm not sure, but would love for more details.\nIn any case, #428 will likely fix this, when I get around to adding it.\n. Nevermind.  It appears these regressions are actually problems with Jenkins itself, not my code.\n. As strange as this may sound, --loglevel DEBUG does not behave the same way as --debug.  You may see more with the --debug flag.\nThe reason for the discrepancy is that there's a check for the --debug flag that enables other logging, where this is not true for merely setting --loglevel DEBUG.  With the --debug flag, you get the urllib3 and elasticsearch python module debug information.  As you can see above, --loglevel DEBUG only shows Curator's debug information.\n. I put them in debug logging because the volume was quite long.  To turn on \"debug\" for Curator only (but not include all of the debug content of the --debug flag), you can set --loglevel DEBUG.\nAgain, I'm not opposed to bringing this back into INFO, but it can be a lot of spam to go through to find the ham.\n. Probably.  I am disappointed that there's not anything between INFO and WARN.  I either have to throw everything in DEBUG, and little or nothing in INFO, or make more things WARN, which as you've noted, is not a great idea either.\n. I think making the skips DEBUG makes sense.  I will definitely pursue that.\nI will also re-introduce an INFO log line that shows every index that will be acted on.  Then any failures can be logged as WARN or ERROR.\n. It's an interesting idea.  But wouldn't other age-based criteria fit the requirement just as well, seeing as how \"closed\" are also likely older by the same, measurable index naming criteria?\nI won't add the feature myself any time soon, as I have bigger fish to fry.  However, if you, or someone in the community were to add it, I would happily merge the feature.\n. This may be the case if you created the snapshots with something other than Curator, and they have a timestamp in the name that differs from what Curator assigns by default.\nYou'll note that the --timestring definition for snapshots differs from the one for indices:\nSnapshots\nThe code which yields this is here: https://github.com/elastic/curator/blob/master/curator/cli/snapshot_selection.py#L24-L25\n--timestring TEXT               Python strftime string to match your\n                                  snapshot's definition, e.g. 20140715020304\n                                  would be %Y%m%d%H%M%S\nIndices\nThe code which yields this is here: https://github.com/elastic/curator/blob/master/curator/cli/index_selection.py#L23-L24\n--timestring TEXT               Python strftime string to match your index\n                                  definition, e.g. 2014.07.15 would be\n                                  %Y.%m.%d\nNote that there is a default value for snapshots, but no default for indices.  This is likely why you had no matches.  I don't think an update to the code is necessary here.  I could add show_default=True to that line, to indicate that there is a default timestring, but that would make long output even longer.  Education and documentation are perhaps better solutions.\n. That timeout could need to be much higher.  You can see more in the output if you run it with the --debug flag.  S3 can particularly be an issue with insufficiently high timeout values.  If you want a zero exit code, you should perhaps give a timeout of at least 300 seconds, perhaps more.\nAnother issue to consider is in the FAQ, here: https://www.elastic.co/guide/en/elasticsearch/client/curator/current/_q_snapshots_seem_to_be_working_why_am_i_getting_literal_snapshotmissingexception_literal_messages.html\n. There's a dependency on the elasticsearch-py module supporting 2.0.  As soon as that's available, I will start work on Curator 4.0-alpha.\n. Tests are not yet passing.  It will be some time as I have more pressing matters to attend.\n. Fixed in #488.  That was a lot of API crawling.\n. Add these options from Curator: https://github.com/elastic/curator/blob/master/curator/cli/cli.py#L37-L38\n. Curator is intended for index-level actions only. You're welcome to use the Curator API (http://curator.readthedocs.org) to accomplish this, or even maintain your own fork, but that functionality will not be merged into Curator.\nAnother part of the reason is that Elasticsearch 2.0+ will have changes regarding the use of \"types.\"\n. The very first line explains what happened:\n2015-10-02 06:00:03,844 WARNING elasticsearch log_request_fail:82 POST http://localhost:9200/logstash-2015.10.01/_optimize?max_num_segments=1 [status:N/A request:21600.056s]\nrequest 21600.056s means that it timed out, which is evident from several other indicators in the logs, e.g. ReadTimeoutError and Read timed out. (read timeout=21600).  See the optimize command documentation regarding timeouts for more information.\nThis all means that that one, single optimize call took over 6 hours to return, but the job continued in the background and completed in spite of the client timeout. \nWhy was it unable to succeed in the 6 hour default timeout?  This could be for several reasons, including:\n1. Indexing is heavy\n2. Too few nodes to handle the write load\n3. Using slow spinning disks instead of SSDs\n4. ???\nOptions include adding nodes, switching to SSDs, or the cheaper fix of increasing the --timeout and --request_timeout options both to something larger even than 6 hours (6 x 3600 seconds = 21600).  If your indices are very large, consider optimizing to a larger number segments per shard rather than 1, or the default of 2.\n. Curator only uses Elasticsearch API calls.  Chances are very good that you'd see the same node dropping behavior if you made the same API call that Curator uses.\nHave you looked through the Elasticsearch logs?  Particularly the client that Curator hits, and the elected master node's logs.  Evidence of what is transpiring may be found there.\nHow many shards per node do you have?  This may sound like a strange question, but the answer may have some bearing on what's going on.\n. I have no idea which operations you're using Curator to complete, but I highly discourage the use of load-balancers between Curator and client node(s).  Several operations can take a long time to process (particularly optimize and snapshot), which has caused many user much grief.  Your problem seems to be with delete operations, which is a different story, though.\nI have seen delete operations take a toll on master nodes, but usually only when the cluster state is huge, and updating the cluster state takes enough time to cause issues.\n\nWe have 3-5 shards per node and 9 shards per index.\n\nIf I interpret this correctly, it seems like you meant \"we have 3-5 indices per node, and 9 shards per index.\"  If not so, this seems to indicate that you have 3 or 4 nodes per primary shard (to say nothing of replica shards), to spread them around that far.\nIf you meant 3-5 indices per node, and 9 shards per index, that would mean something different.  Either way, it doesn't seem like you have over 300 shards per node.  This is the point at which I would expect deletes to potentially begin affecting the cluster state.\nI can't see Curator as an influencing factor in your problem here with the information you've shared.\n. These logs are helpful, but it'd be much better to see what, if anything came before that.  I see master_left, and then the resulting failure to forward the delete request to the master:\n\u2026 connection exception while trying to forward request to master node \u2026\nBut I do not see the actual delete command come in, or what transpired in the moments before Curator made the delete call.  Clearly something is prodding a network or node failure (with the master), but I can't tell what.  What do the master node logs say?\nWhen I send a delete through Curator, the elected master node log reports this:\n[2015-10-04 02:30:01,278][INFO ][cluster.metadata         ] [Elasticbox] [logstash-2015.09.19] deleting index\nDo you see deleting index messages in your elected master node logs?  Those would be helpful to track down.\n. Strange.  It seems you do not have DEBUG mode on the master node, so we're left with only INFO.\nThe bit that interests me is that you do not see the delete request, only the node re-initializing.\nAlso that you're on 1.7.0.  There were a few bugs with 1.7.0 which may or may not have anything to do with what you're seeing.  It can't hurt to upgrade to 1.7.2, though.\n. Log settings are in the logging.yml file in the config directory.  If they can be set by API (which I'm not sure they can), that would only be per node, rather than cluster-wide.\n. The urllib3 errors are a red herring.  The problem actually appears to be the fault of version 1.7.0 of the elasticsearch-py module.  1.7.0 is forward-looking to Elasticsearch 2.0, and it may be an incompatibility between versions.  I will, at some point, have to do some dependency refactoring.\nYou can verify you have version 1.7.0 by running:\n$ pip list | grep elasticsearch\nelasticsearch (1.7.0)\nor you can test at the command-line:\n```\n$ python\n\n\n\nimport elasticsearch\nelasticsearch.version\n(1, 7, 0)\n```\n\n\n\nThis can be addressed by running:\npip install -U elasticsearch==1.6.0\nThis command will uninstall version 1.7.0 of the elasticsearch-py module and replace it with version 1.6.0.\n. Thanks for diving in!  Your python looks just fine, and the test addition looks great.  :smile: \nLooking at what you're trying to accomplish, though, I'd prefer to see this applied as a filter.  I really don't want to have the get_indices method changed, even though what you've created does work.  It's an opportunity to extend the API that is behind my motivation.\nIf you look at https://github.com/elastic/curator/blob/master/curator/api/utils.py#L287-L302, you'll see that there's already a prune_closed method.  It should be pretty simple to clone that, but make it prune_opened.  This extends the API, which is why I'd prefer it if you went in that direction.\nRemove the not from https://github.com/elastic/curator/blob/master/curator/api/utils.py#L298, and change Closed to Opened in https://github.com/elastic/curator/blob/master/curator/api/utils.py#L301.\nYou won't have to change the config option you created.  You'd only need to put an if statement after https://github.com/elastic/curator/blob/master/curator/cli/index_selection.py#L84-L87 to filter out the \"open\" indices.  Something like this, perhaps:\nif closed_only:\n    logger.info(\"Pruning open indices, leaving only closed indices.\")\n    working_list = prune_opened(client, working_list)\nHow does that sound?\n. @Basster \nThe only things I need from you now are to update the CONTRIBUTORS file and Changelog.rst.\nUpdate the Changelog with what you've changed, and add yourself to CONTRIBUTORS.\n. @univerio I'm sorry to hear you're having difficulty with AWS ES.  An endpoint-limited version of ES is not a scenario I could have planned for.  Curator was designed to run with full-service versions of Elasticsearch because there was never an expectation there would ever be anything but.\nCurator depends on the index_closed method in more places than just the optimize method, and is also referenced by the prune_closed method.  I can't just turn off or ignore exceptions for this check without repercussions.  These methods were added because of the program-crashing results of trying to perform operations on closed indices. The only 2 things you can do to closed indices are open and delete (and delete actually briefly opens the index\u2014internally to Elasticsearch\u2014before deleting).  While I appreciate that AWS ES does not allow users to even close indices, I'm concerned that the path you recommend\u2014making Curator regard the exception as only a warning\u2014could have some unintended consequences for Elasticsearch deployments which can close indices.\nIf you (or someone else) were to code up a way around this, and add tests to guarantee reverse compatibility with all versions of Elasticsearch from 1.0.x - 2.0.x, I'd be willing to merge that in, if it passes code review.\n. I suppose you could parse that out.  I'd think it'd be easier to use the _cat API with the JSON output.  The AWS ES version is 1.5+, and the _cat API will show index state (open or closed), but only in Elasticsearch versions > 1.4.\nModifying the index_closed function to do an ES version check (Curator does this in several places already), then use the _cat API first if an acceptable version of ES is found would solve this in a less fragile way.\n. ``` python\n$ python\nPython 2.7.10 (default, Aug 22 2015, 20:33:39)\n[GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.0.59.1)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport elasticsearch\nclient = elasticsearch.Elasticsearch(host='172.19.73.2')\nclient.cat.indices(format='json',h='index,status')\n[{u'status': u'open', u'index': u'logstash-2015.10.14'}, {u'status': u'open', u'index': u'.marvel-2015.10.05'}, {u'status': u'open', u'index': u'logstash-2015.10.10'}, {u'status': u'close', u'index': u'.marvel-2015.10.01'}, {u'status': u'open', u'index': u'.marvel-2015.10.07'}, {u'status': u'open', u'index': u'.marvel-2015.10.06'}, {u'status': u'open', u'index': u'.marvel-2015.10.10'}, {u'status': u'open', u'index': u'.marvel-2015.10.02'}, {u'status': u'open', u'index': u'logstash-2015.10.06'}, {u'status': u'open', u'index': u'test_batch'}, {u'status': u'close', u'index': u'logstash-2015.10.01'}, {u'status': u'open', u'index': u'logstash-2015.10.15'}, {u'status': u'open', u'index': u'.marvel-2015.10.11'}, {u'status': u'open', u'index': u'logstash-2015.10.04'}, {u'status': u'open', u'index': u'.marvel-2015.10.08'}, {u'status': u'open', u'index': u'.marvel-2015.10.15'}, {u'status': u'open', u'index': u'logstash-2015.10.02'}, {u'status': u'open', u'index': u'.marvel-2015.10.13'}, {u'status': u'open', u'index': u'logstash-2015.10.12'}, {u'status': u'open', u'index': u'logstash-2015.10.07'}, {u'status': u'open', u'index': u'logstash-2015.10.08'}, {u'status': u'open', u'index': u'.marvel-2015.10.14'}, {u'status': u'open', u'index': u'.marvel-2015.10.04'}, {u'status': u'open', u'index': u'logstash-2015.10.05'}, {u'status': u'open', u'index': u'.marvel-2015.10.03'}, {u'status': u'open', u'index': u'.marvel-2015.10.12'}, {u'status': u'open', u'index': u'.marvel-kibana'}, {u'status': u'open', u'index': u'.marvel-2015.10.09'}, {u'status': u'open', u'index': u'logstash-2015.10.13'}, {u'status': u'open', u'index': u'.kibana'}, {u'status': u'open', u'index': u'logstash-2015.10.09'}, {u'status': u'open', u'index': u'logstash-2015.10.03'}, {u'status': u'open', u'index': u'logstash-2015.10.11'}]\n```\n. fixed in #499 \n. packages.elasticsearch.org and packages.elastic.co have different IPs:\n\n\n\n```\n\u00bb host packages.elasticsearch.org\npackages.elasticsearch.org is an alias for dualstack.downloadlb-1287877610.us-east-1.elb.amazonaws.com.\ndualstack.downloadlb-1287877610.us-east-1.elb.amazonaws.com has address 184.72.222.192\ndualstack.downloadlb-1287877610.us-east-1.elb.amazonaws.com has address 107.22.185.174\ndualstack.downloadlb-1287877610.us-east-1.elb.amazonaws.com has address 184.72.233.150\ndualstack.downloadlb-1287877610.us-east-1.elb.amazonaws.com has IPv6 address 2406:da00:ff00::b848:e996\ndualstack.downloadlb-1287877610.us-east-1.elb.amazonaws.com has IPv6 address 2406:da00:ff00::6b16:b9ae\ndualstack.downloadlb-1287877610.us-east-1.elb.amazonaws.com has IPv6 address 2406:da00:ff00::b848:dec0\n\u00bb host packages.elastic.co\npackages.elastic.co is an alias for dualstack.download-colb-770446651.us-east-1.elb.amazonaws.com.\ndualstack.download-colb-770446651.us-east-1.elb.amazonaws.com has address 184.73.247.78\ndualstack.download-colb-770446651.us-east-1.elb.amazonaws.com has address 184.73.196.98\ndualstack.download-colb-770446651.us-east-1.elb.amazonaws.com has address 50.19.101.59\ndualstack.download-colb-770446651.us-east-1.elb.amazonaws.com has IPv6 address 2406:da00:ff00::b849:f74e\ndualstack.download-colb-770446651.us-east-1.elb.amazonaws.com has IPv6 address 2406:da00:ff00::3213:653b\ndualstack.download-colb-770446651.us-east-1.elb.amazonaws.com has IPv6 address 2406:da00:ff00::b849:c462\n```\nWe're transitioning to a new repository.  Until that's complete, this will need to remain as-is.\n. Yes.\n. Heh... Never mind.  They're using the same AWS bucket, so I guess it is okay to switch.  Thanks for pointing it out.\n. fixes #492\n. I should make it more clear that --quiet isn't the same thing as no output.  The --quiet flag suppresses stdout output, but not log output.  If you do not specify a log file, then Curator defaults to use stdout as its logging target.  This is likely what you're experiencing: stdout log output.\nIf you want it to be completely quiet, you will need to use a --logfile.  Without it, you'll still be logging to stdout, hence output.\nWhy would I have both stdout and logfile output?  So that users will get some messages at the command-line no matter what (except in the case of the new --quiet flag).\n. @cameronkerrnz please see the above comment\n. @mizeng if you are using Curator as it ships, it already does automatically exclude all standard Kibana index types:\n``` python\n    if ctx.parent.info_name == \"delete\": # Protect against accidental delete\n        logger.info(\"Pruning Kibana-related indices to prevent accidental deletion.\")\n        working_list = prune_kibana(working_list)\n# If there are manually added indices, we will add them here\nworking_list.extend(in_list(index, indices))\n\nif working_list and ctx.parent.info_name == 'delete':\n    # If filter by disk space, filter the working_list by space:\n    if ctx.parent.params['disk_space']:\n        logger.info(\"Filtering to keep disk usage below {0} gigabytes\".format(ctx.parent.params['disk_space']))\n        working_list = filter_by_space(\n                            client, working_list,\n                            disk_space=ctx.parent.params['disk_space'],\n                            reverse=ctx.parent.params['reverse']\n                       )\n\n```\nThe prune_kibana method is used to remove these indices from the working list before any action is taken.\nIf you are using the Curator API, you'd be correct.  filter_by_space does not, by itself, automatically prune Kibana indices.  It should be simple to add the prune_kibana method before or after running the filter_by_space method.\nNOTE: If you are using an older version of Curator, the filter_by_space method was called differently and did not act as a special filter, but was called delete_by_space.  With one of these older versions, it may have been possible for Curator to have deleted a Kibana index in a delete-by-space operation.  You will automatically have this protection if you upgrade to the most recent version of Curator.\n. This is an interesting feature request.  Unfortunately, it's not as simple as merely \"merging\" indices.  It involves a full reindexing of all indices into another index.\nThis is not a lightweight operation.  It potentially could be very taxing to clusters, especially if you're already concerned about memory consumption.\nYou'd consume double the space on your cluster by the time the re-index completed (or more, depending on the number of replicas).\nThere are many other issues that could arise from this.  I'll ponder on it.\n. @bgagnon and @tuckerpm, these are still one-off use cases.  Curator is meant for index management tasks done on a regular time table.  While Curator may be metaphorically likened to a swiss army knife with lots of functionality, the more you add, the less easily used it becomes:\n\nI don't want Curator to be like that knife.  The feature you are requesting is not something that would be done on a daily basis, nor is it something likely to be done on a weekly or monthly basis either.  In the event that you want to merge indices, it's actually relatively trivial to run a simple logstash instance to do what you just described:\n```\ninput {\n  elasticsearch {\n    # ... elasticsearch connection args here...\n    index => \"logstash-2015.11.*\"\n  }\n}\nfilter {}\noutput {\n  elasticsearch {\n    # ... elasticsearch connection args here...\n    # Make Logstash re-index them to monthly indices with this addition\n    index => \"logstash-%{+YYYY.MM}\"\n  }\n}\n```\nThis example will ingest all documents from all indices from November 2015 and merge them into a single index.  You can tweak the wildcard to get bigger or smaller swaths of indices at one time.\n@davmrtl Your use case could actually be solved in a cleaner and less painful way by sending your documents to two indices simultaneously, rather than by using an ex post facto merging process (which is actually just reindexing).  If you still want to do ex post facto reindexing, you can use a similar process to the one outlined above.  However, unless you're heavily constrained by I/O and disk space, it would likely be easier to index to two destinations at once:\noutput {\n  # Weekly indices\n  elasticsearch {\n    # ... other config args ...\n    index => \"logstash-%{+YYYY.ww}\"\n  }\n  elasticsearch {\n    # ... other config args ...\n    # This is the default index pattern, merely represented here for visibility\n    index => \"logstash-%{+YYYY.MM.dd}\"\n  }\n}\nIt is important to note that in either case, you are still indexing twice.  You just choose when and how that happens.\nIn this way you'd already have weekly indices, but would also have the flexibility of daily indices for fast access in Kibana.  When they outlive their usefulness, you use Curator to delete them, but your data is still preserved in the weekly indices.\nFor the present, I don't think I'll be adding reindexing functionality to Curator for the aforementioned reason that it is an uncommon use case, and that there are tools better suited to that use case.\n. @referup-tarantegui What version of Logstash? There would be a lot of logs in recent versions of Logstash explaining such a failure.\nWith regards to the original post, the _reindex API will be useful for this, but it is only available in the most recent versions of Elasticsearch (2.1+, I think).  It will, at some point, be added to Curator.. @referup-tarantegui then you should check the Logstash logs.  Which solution did you use? Double-indexing or re-indexing?. 30 to 1 is a tall order.  A very tall order. Are you trying to do that all at once?  If you're really, seriously trying to do that in a single pass, I'm kind of not surprised it's not able to effectively query all of that.\nTwo options that may make this easier would be to make an alias that encompasses all 30 source indices and use that as the source \"index\" in the input.\nThe other option is to use the _reindex API.. The alias would only be to change index => \"logstash-2015.11.*\" to index => \"alias_name\".  You can delete the alias afterwards.  I think the issue here is that some part of the chain can't keep up with demand.. LGTM.\n. Vanilla 1.5.1 returns a list.  The elasticsearch module does the JSON handling.  AWS Elasticsearch is seriously screwed up if you can't get a proper response (text/json)  for the _cat API such that the Elasticsearch module works.\n. I would be too!  Thanks for catching this, it seems I missed a step.\nI just pushed them out, signed.  Would you please verify?\n. Yes, Curator assumes UTC.  There is not presently a setting which allows this to be tuned.  I'd be happy to merge a PR that adds this functionality, but it's not on my to-do list.\n. Without changing anything, your best bet right now is to simply keep 1 more --time-unit of indices to prevent acting before the desired time frame.\n. Everything Elasticsearch does with time is in UTC. Recalculation is done for display only, as in Kibana. \nI made Curator use UTC as well. The creation_date is in UTC, so for the sake of consistency, time strings are also treated the same. . Thanks for this!\n. Can you add a test case for this?\n. No worries.  I don't think this needs a test case.  Thanks for adding to Curator.\n. I currently have no way to test this.  If you need me to expedite work on this, you'll have to go through the chain, to Suyog first.\n. @gmoscovicz, no. --ssl-no-validate means that no validation of the certificate will be performed, which is inherently insecure. That's all the warning messages are saying: This isn't secure because no validation took place. \nThis option allows the user to connect over SSL in a way that works, but violates the security features of SSL. So, yes. If this is a problem for users, they should get and use valid certificates, even if they have to create their own certificate authority to do so. \n. @nellicus I'm guessing that the error there is because the end user expected their certificate to simply be accepted, even though unspecified.  Did they use a self-signed certificate?  If so, it appears that they did not specify the CA certificate to allow the chain to be complete.  Just because you're not verifying the authenticity of the certificate doesn't mean it doesn't still have to match.\n. Closing from lack of activity, and Curator 4.0 is imminent.  If this continues to be an issue, please feel free to open a new issue.\n. Thanks for catching this :smile: \n. I won't be adding this feature myself, but would be happy to merge it if someone else were to add it, with tests.\n. @imphasing I've done a bit of digging.  It does not appear simple to add this, at first glance.\n1. elasticsearch-py (the parent module Curator uses) does not have an out-of-the-box way to enable this.\n2. There is this, which gives a possible way to do proxying, but with the caveat that the proposed solution will not proxy HTTPS requests, which is explained in detail here.\nAs a result, Curator will not likely be adding proxy support any time soon, as the burden of having to write tedious work-arounds that will not work with HTTPS is not something I want to tackle.  I'm going to close this for now, and reference it if the request comes up again.\n. There are some changes to this for 4.0 that allow this to work, but it's very naive right now.  It lacks any magic for creating \"future\" indices, or at least indices with future names.\nI would like to add the ability to specify an offset in seconds in the future for a name to be, and have Curator add the timedelta to the original.  That would allow for creation of future indices.  It's not as pressing right now, so if someone wants to look at the code and docs and figure out a way to add that, the feature will arrive sooner.\nOther ideas for \"future\" index creation include looking at name pattern and incrementing it, i.e. index1, index2, index3. It could also be index_001, index_002, index_003, or something like that.  This, combined with aliasing, could be very useful.\n. #614 adds some more functionality here.\n. https://www.elastic.co/guide/en/elasticsearch/reference/5.2/date-math-index-names.html\nThis is the answer! And I tested it with Curator 4.2.6 against Elasticsearch 5.2.0 (should work in the entire 5.x branch.  Haven't tested the 2.x branch yet).\nUsing this file:\n```\n\nactions:\n  1:\n    action: create_index\n    description: Create logstash index for tomorrow using Elasticsearch date math\n    options:\n      name: ''\nI get this DEBUG output:\n2017-03-27 14:21:34,074 DEBUG              curator.utils     parse_date_pattern:1034 Fully rendered name: \n2017-03-27 14:21:34,074 DEBUG                curator.cli         process_action:98   Doing the action here.\n2017-03-27 14:21:34,074 INFO      curator.actions.create_index              do_action:391  Creating index \"\" with settings: {}\n2017-03-27 14:21:35,088 INFO                 curator.cli                    cli:203  Action ID: 1, \"create_index\" completed.\n2017-03-27 14:21:35,089 INFO                 curator.cli                    cli:204  Job completed.\n``\nNote that it says it's2017-03-27 14:21:35`. I'm UTC-6, so in UTC terms, it's still the 27th of March.\nSo this action file created this output:\ncurator_cli show_indices --ignore_empty_list --filter_list '{\"filtertype\": \"pattern\", \"kind\": \"prefix\", \"value\":\"logstash-\"}'\n...\nlogstash-2017.03.26\nlogstash-2017.03.27\nlogstash-2017.03.28\nIt created a logstash index for March 28th.  This should solve your problems.. http://build-eu-00.elastic.co/job/es-curator_core/\nIt was in the past, but something broke.  They haven't fixed it since ES 2.0 was released.\n. Raise an issue with @electrical and/or the infra team\n. Thanks for catching the typo.\n. Hmm. You should only see errors like this if there is a library conflict somewhere in your path. Everything in the zip package is self-contained and does not require any Python to be installed. It tends to cause issues if you have Python (2.7 or 3.4) installed. \nHave you tried running from a Windows box with no Python on it?\n. @wespday Sorry it's taken me so long to get back to you.  Building a Windows 7 VM and patching it fully, installing VisualStudio\u2014and all other dependencies\u2014is a fairly involved process.\nPlease test this version here and let me know if you still have problems.  It was built, as mentioned, in Windows 7 64 bit, with Visual Studio 2013 Express (just the compiler part), the latest version of nuitka, and Python 2.7.10.\n. This is seriously strange.  What version of Windows are you using, @hwwin?\n. I'm sorry about the difficulty, everyone.  \nI use Nuitka to compile the windows binary, and there's an actual issue with included files in recent versions (and I updated the ticket with my own results).  I'm rebuilding the binary right now with the last version of Python and Nuitka that made usable binaries.\n. @wespday @gschuager @hwwin @bwinterseg @samjudson please try this version.  It works for me on a python-less install\u2014and bonus\u2014should work on win32 or win64.\n. I will publish this version if I get a few LGTMs.\n. @samjudson The --quiet option is to prevent stdout messages when you have selected a log file via the --logfile option.  If you have not selected a log file, logging will continue to stdout, and --quiet will have no effect.\n. @dparkar, did you set a higher timeout?  --dry-run isn't likely to increase timeouts if that's needed.  And it appears that you have a full 30 second timeout (and 30 seconds is the default).\n. @dparkar Yes, --dry-run is different: https://github.com/elastic/curator/blob/master/curator/cli/index_selection.py#L151-L152\nand\nhttps://github.com/elastic/curator/blob/master/curator/cli/utils.py#L72-L81\nIt you're having 30 second timeouts in the show_dry_run method, that's a problem.  It implies that it's taking more than 30 seconds to check if a single index is closed.  I'm not sure what's going on in your cluster, but that's indicative of either a huge number of indices (which is addressed in 59bbf8a), or perhaps an overburdened cluster.\n. @styfle Thanks for reporting back.\nI'm actually finishing up a new release in the next few days.  The new release will include new instructions because I'm including an MSI for Windows, and instructions on how to build your own binary, if necessary.\n. @MonDeveloper Please hold on a few more hours.  I'm trying to release an update.\n. Thank you all for your patience.  3.4.1 is released, and the Windows download has been updated.  Please see the docs.\n. Because it's a 32bit binary now.  I switched from using nuitka to using cx_Freeze to generate the binaries (and now, MSI).\n. @sts What would this feature look like?  The command-line API to add/remove templates is pretty straightforward.  Adding this functionality to Curator would perhaps just add complexity to an already crowded command-line.  If you're really set on this, you could add the methods to the API and then make a separate script, like the es_repo_mgr endpoint, to handle templates.\nIn any case, it's not something I'm likely to add myself.  I'd be happy to merge this if someone were to add it in (preferably with tests).\n. Before you start, understand that Curator v4 is very different from Curator v3. It may not be a good fit for v4. \nAs such, I had intended to replicate the command-line structure of v3 in a parallel project, and this would probably fit there better. \n. This might be something to add later when I begin re-adding versions of the v3-style command-line scripts.\n. This issue has been worked around in 3.5.x with the ability to use certificates for authentication, removing the need for passwords.  Please give that a try and let me know what you think.\n. This should be addressed by #595 and #596 with Curator 4.0 imminent.\nIf you feel this is in error, please feel free to reopen this ticket or open another.\n. I apologize, but in its current state, I will not be merging this pull request.\nI appreciate the effort that went into this.  Unfortunately, it's very limited in its current utility, in that it only filters by position from the right.  This works for your use case, but it does not solve the bigger problem of sorting by date after sorting by space.  I'd much rather see a change to delete --disk-space that adds flags and first sorts by date, and then sorts by disk space.  It's a much bigger problem than it first appears (which is why it has not been given much love).\nFortunately, there are regexes and methods already in the API that could hypothetically be applied to the indices to sort them by age before their sizes are calculated.  It would require a partial reimplementation of the filtering criteria in https://github.com/elastic/curator/blob/master/curator/cli/index_selection.py, but it could be done.  It could even take options given in the indices sub-command to help with the filtering.\n. Currently, you can do delete --disk-space and then indices to further filter.  I propose adding an extra flag to go along with --disk-space that would sort by date.  Date bits would have to go into the indices section, but the filter-by-space method could theoretically extract that data from the click context, and use  it as well.\nThat's just the rough view at this point.\n. I'm not sure it will be easy.  Perhaps it will require re-using the --timestring flag, though I would hope it could be passed along from the context.  It may require extra args and pass-throughs to accomplish this, however.\n. This is obviated by the pending release of Curator 4.0.\nSee the changes in #595 and #596\n. Thanks, @corey-hammerton.  @KunX, he is correct.  You do need to use the --index flag to select individual indices.\n. Sorry to keep you waiting for so long.\n. @idsvandermolen It passes tests with elasticsearch 1.x (see the build results link on the repository home page), though there are some exceptions that don't get caught properly for 1.x.  But it is still functional.\nThis will be the last release branch to be reverse compatible with ES 1.x.  There will not be a 3.6.0 release.  4.0 is coming soon.\n. @idsvandermolen The version lockout is really not as complete as it might seem.  There are features in the es-py 2.x branch that won't work against a 1.x elasticsearch cluster, of course.  But so much of the underlying API is still identical that for the feature set that Curator uses, I can still use es-py 2.x with 1.x elasticsearch clusters.  Until the most recent version, you could still get away with using es-py 1.8 with Curator.  In order to make use of the PKI structure, though, es-py 2.3.0 is required.  As I pointed out, the tests with Curator all pass in 1.x clusters, even with es-py 2.3.0.\n. I don't rely on Travis, but on our internal CI\n. I am curious about the use case.  Why does a fixed number of newest/oldest matter?\n. What I mean to say, is when will you delete the n oldest, when theoretically, that number could include today's index?  How is this an improvement over --newer-than and --older-than?\nPlease sell me on why this functionality ought to be included.\n. I'm still not sold.  While I can appreciate your use case, this adds complexity and more flags to an already large option tree but adds very little additional functionality.  It's not terribly hard to use show to see everything, look at the dates, and then use --older-than.\n. Thank you for contributing to Curator.  While I appreciate the thought and effort that went into submitting this pull request, I have decided not to merge this code as it is too similar to existing functionality, and adds unnecessary complexity.\nPlease continue to support this project, and other open source projects, with your time, ideas, code, and comments.  Thank you!\n. Ah, so I should check that --older-than is not empty, rather than boolean, which catches the zero as a false positive in this case.\nI'll see what I can do.\n. In the meantime, you could add --newer-than 1040, which would effectively add a null filter that includes all weekly indices newer than 20 years in the future.  That will remove the warning. :smile: \n. Well, you can't do days with your time unit as weeks. \nThe functionality has two things to keep in mind. \n1. It counts days from 00:00 UTC to 00:00 UTC the next day\n2. Weeks are counted slightly more awkwardly because of differences in calculation. That 2 week thing is the result. \n. @rhoml Yes, I apologize for the awkwardness, but this is because the way calculations are done, you can't accurately calculate 1 week old until the current week has become 1 full week, which results in --older-than 1 not acting until the week is basically 2 weeks old.  This is why --older-than 0 was added, so this oddity can be worked around. \nThe --newer-than 1040 workaround will still quiet the warning, while it should yield the desired result.\n. In order to accurately reproduce this, can you send me the list of index names you were trying to work on?  I can attempt to create a test to replicate this behavior.\n. I guess I can try to generate some of those.  How many do you have?\n. This should be addressed by #595 and #596 with Curator 4.0 imminent.\nIf you feel this is in error, please feel free to reopen this ticket or open another.\n. Probably.\n. Without seeing your log file, I can't tell for sure, but I'm guessing that already optimized indices are being skipped.  There is no pause between those.  You can see that in the code:\npython\n    if optimized(client, index_name, max_num_segments):\n        return True\n    else:\n        logger.info('Optimizing index {0} to {1} segments per shard.  Please wait...'.format(index_name, max_num_segments))\n        try:\n            client.indices.optimize(\n                index=index_name, max_num_segments=max_num_segments,\n                request_timeout=request_timeout\n            )\n            time.sleep(delay)\n            return True\nRegardless, I'm improving logging so you can see this take place:\npython\n    if optimized(client, index_name, max_num_segments):\n        return True\n    else:\n        logger.info('Optimizing index {0} to {1} segments per shard.  Please wait...'.format(index_name, max_num_segments))\n        try:\n            client.indices.optimize(\n                index=index_name, max_num_segments=max_num_segments,\n                request_timeout=request_timeout\n            )\n            if delay > 0:\n                logger.info(\"Pausing for {0} seconds before continuing...\".format(delay))\n                time.sleep(delay)\n            return True\n. After consideration, I thought that it was somehow not passing the delay option when the \"breaking it up\" flag was hit (and it somehow may not be catching it, but it doesn't look like it): https://github.com/elastic/curator/blob/master/curator/cli/index_selection.py#L156-L165\ndo_command runs the same way regardless of how it's called: https://github.com/elastic/curator/blob/master/curator/cli/utils.py#L232-L236\nPlease gist the output here with --debug enabled so I can see some more troubleshooting information.\n. I'd need to see at least 2 of the optimize calls.  Only one doesn't show me quite enough.\n. Better, but do you have an example where 2 indices are actually being optimized in a row, with evidence of no delay between them?  All of these appear to be already optimized and are skipped.\n. Thanks for this.  I see the 120 second delay here:\n2016-01-13 02:02:23,118 DEBUG              elasticsearch    log_request_success:66   < {\"_shards\":{\"total\":36,\"successful\":36,\"failed\":0}}\n2016-01-13 02:04:23,201 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\nand here:\n2016-01-13 02:04:23,975 DEBUG              elasticsearch    log_request_success:66   < {\"_shards\":{\"total\":36,\"successful\":36,\"failed\":0}}\n2016-01-13 02:06:24,010 INFO           curator.cli.utils               exit_msg:67   Job completed successfully.\nIt seems to be operating exactly as expected.  You can only see the delay in the time differences, but it's there.  I've already improved logging to show when it's happening for the next release.\n. What surprises me more is the speed at which your cluster optimizes.  It's almost instantaneous.  On my own cluster (with 3 data nodes, and 5 shards per index), it takes a fair amount of time to optimize my daily, 1.3G indices.  Granted, I'm not using SSDs, but still:\n2016-01-12 10:04:14,412 INFO      Optimizing index logstash-2016.01.11 to 1 segments per shard.  Please wait...\n2016-01-12 10:05:57,135 INFO      Job completed successfully.\nAnd 24 and 36 shard indices?  That's a rather high shard count.  How many nodes is that spread across?\n. And SSDs, I presume?\n. It's a marvelous cluster, by the way!\n. @rhoml have you ever been able to replicate this?  It's been a while and I never heard back.\n. Interesting.  Sorry for the weirdness.  Thankfully, it shouldn't matter for Curator 4.0 (working in my secret lab on it right now).\n. This should be addressed by #595 and #596 with Curator 4.0 imminent.\nIf you feel this is in error, please feel free to reopen this ticket or open another.\n. ~~It does not.  The allocation rules, as presently coded, are based on tags only.  _ip is not recognized as a valid tag.  If you wanted to add this in, I'd merge it.  This is the first time this has been requested, so there doesn't appear to have been a high demand for it.~~\nUpdate:\n@makeyang, I stand corrected, though I haven't tried it yet.\nHaving read through the shard allocation filtering documentation better, it appears that these special types should work without any additional changes.\nIn the current release of Curator (3.5.0), that would mean that your rule should be _ip=XXX, exactly as you've stated.  Any of the special attributes should work with that formula.\n| Attribute | Description |\n| --- | --- |\n| _name | Match nodes by node name |\n| _host_ip | Match nodes by host IP address (IP associated with hostname) |\n| _publish_ip | Match nodes by publish IP address |\n| _ip | Match either _host_ip or _publish_ip |\n| _host | Match nodes by hostname |\nIf you'd please test and respond back to me, I'd love to add this to the documentation.\n. This should be addressed by #595 and #596 with Curator 4.0 imminent.\nIf you feel this is in error, please feel free to reopen this ticket or open another.\n. I manually verified your CLA signature.  Thanks for adding this.\n. When Elastic releases \"official,\" Elastic-provided docker images, then we might release this.  Until then, it's on-hold. \n. For anyone else looking here, until further notice, I would recommend using https://hub.docker.com/r/bobrik/curator/. Sorry everyone, for the prolonged delay in response. Yes, I would like to offer an official Elastic Curator image. Not sure yet when this will happen. It probably won\u2019t be until the next major version is released. And I\u2019m not sure when that will be. \nAt the end of the day, Curator is still a one-man show (me), and I am a consultant now, rather than a full-time developer, so my development time is more limited. Coming soon will be Index Lifecycle Management, which will cover the most common Curator use cases, complete with a UI in Kibana. For anything requiring more complicated flows, Curator will continue to be the go-to tool for index management. As such, yes, an official Docker image is an important step. I will get to it when I can, as it involves me working with the infra team that does image publishing, etc. I will get to it when I can, but it will not be the next few weeks, what with the holidays coming.. I appreciate the offer, @frittentheke. However, if it were as simple as that, I would already have done it. There are internal and external politics and multiple systems involved. Our infra team manages the docker hub images. I have to go through them. Curator is not deployed or managed on those Jenkins servers, which manage the automated building process. I have to go through a process to get things on their servers if I want this to happen. It's not high enough on the priority list to escalate and get Curator both on the internal build system and in Docker Hub in an expedited time frame\u2014especially not since Index Lifecycle Management is almost ready for release.. You\u2019re right, but that\u2019s not the point. My Infra team manages all docker building and publishing. I can\u2019t be a one-off. . Thanks for this.  Can you rebase and add a blurb to the changelog and add yourself to the contributors list, please?\n. @zzugg if you don't want to add yourself to the Contributors file, and update the Changelog, let me know.  I'll still merge your changes and add these myself.\n. @zzugg I'll take care of it.  Thanks for the code change.\n. Yeah, I can fix that.  Sorry for the inconvenience.\n. That is correct. Python strftime is not the same as strftime.org and requires zero padded days and months.   The linked documentation indicates that:\n\n'%d Day of the month as a zero-padded decimal number'\n. @gfrankliu This is not an easy task here, as it would involve a great deal of tokenizing and testing.  Instead, in Curator 4.0 (still not pushed to master) you can filter by age in 3 different ways:\n1. By index name (which is the only way the current version of Curator works)\n2. By index creation_date.  This is the timestamp of when the index was created.\n3. By use of the field_stats API.  You use a timestamp field, and it can tell when the oldest and newest entries are in an index.\n\nThese are likely more desirable than hacking a way in to allow non-zero-padded strftime dates.\n. @gfrankliu https://www.elastic.co/guide/en/elasticsearch/client/curator/4.0/index.html\nYou can give the alpha release a try.  I'll be pushing out a8 in a little while (there's a forcemerge/optimize bug in a7 for ES versions < 5.0 that a8 fixes).\n. What happens if you run:\ncurator show indices --prefix 'gbs\\.'\nI'll add a test to the suite to see what happens with the long index name.\n. Hmm.  I put the index with:\ncurl -XPUT localhost:9200/gbs.evenlogger.logstash-2016.01.04\nAnd then when I ran curator (3.4.0) I saw this:\ncurator show indices --all-indices\n2016-01-05 11:53:40,041 INFO      Job starting: show indices\n2016-01-05 11:53:40,054 INFO      Matching all indices. Ignoring flags other than --exclude.\n2016-01-05 11:53:40,054 INFO      Action show will be performed on the following indices: [u'.kibana', u'.marvel-es-2015.12.22', u'.marvel-es-2015.12.29', u'.marvel-es-2015.12.30', u'.marvel-es-2015.12.31', u'.marvel-es-2016.01.01', u'.marvel-es-2016.01.02', u'.marvel-es-2016.01.03', u'.marvel-es-2016.01.04', u'.marvel-es-2016.01.05', u'.marvel-es-data', u'.marvel-kibana', u'gbs.evenlogger.logstash-2016.01.04', u'logstash-2015.12.22', u'logstash-2015.12.29', u'logstash-2015.12.30', u'logstash-2015.12.31', u'logstash-2016.01.01', u'logstash-2016.01.02', u'logstash-2016.01.03', u'logstash-2016.01.04', u'logstash-2016.01.05', u'test_batch']\n2016-01-05 11:53:40,054 INFO      Matching indices:\n.kibana\n.marvel-es-2015.12.22 (CLOSED)\n.marvel-es-2015.12.29\n.marvel-es-2015.12.30\n.marvel-es-2015.12.31\n.marvel-es-2016.01.01\n.marvel-es-2016.01.02\n.marvel-es-2016.01.03\n.marvel-es-2016.01.04\n.marvel-es-2016.01.05\n.marvel-es-data\n.marvel-kibana\ngbs.evenlogger.logstash-2016.01.04\nlogstash-2015.12.22 (CLOSED)\nlogstash-2015.12.29\nlogstash-2015.12.30\nlogstash-2015.12.31\nlogstash-2016.01.01\nlogstash-2016.01.02\nlogstash-2016.01.03\nlogstash-2016.01.04\nlogstash-2016.01.05\ntest_batch\nAnd when I run with --prefix gbs I see this:\ncurator show indices --prefix gbs\n2016-01-05 11:55:45,999 INFO      Job starting: show indices\n2016-01-05 11:55:46,013 INFO      Action show will be performed on the following indices: [u'gbs.evenlogger.logstash-2016.01.04']\n2016-01-05 11:55:46,013 INFO      Matching indices:\ngbs.evenlogger.logstash-2016.01.04\nI'm not sure what the problem is for you.\n. To test with 3.2.3, I had to install an older Elasticsearch instance:\n\u00bb curator --version\ncurator, version 3.2.3\n\u00bb curator show indices --all-indices\n2016-01-05 12:10:12,583 INFO      Job starting: show indices\n2016-01-05 12:10:12,592 INFO      Matching all indices. Ignoring flags other than --exclude.\n2016-01-05 12:10:12,592 INFO      Matching indices:\ngbs.evenlogger.logstash-2016.01.04\n\u00bb curator show indices --prefix gbs\n2016-01-05 12:10:18,670 INFO      Job starting: show indices\n2016-01-05 12:10:18,681 INFO      Matching indices:\ngbs.evenlogger.logstash-2016.01.04\nIt seems to be working with 3.2.3 and Elasticsearch 1.7 also.\nWhat does your environment look like?  OS and version? Python version? Elasticsearch version?\n. Glad it was a simple fix!\n. I would start by upgrading to 3.4.1\n. It was not a bug, but was to report a known error code for this case:\nhttps://github.com/elastic/curator/blob/master/curator/cli/utils.py#L96-L102\npython\ndef check_master(client, master_only=False):\n    \"\"\"\n    Check if master node.  If not, exit with error code\n    \"\"\"\n    if master_only and not is_master_node(client):\n        logger.info('Master-only flag detected. Connected to non-master node. Aborting.')\n        sys.exit(9)\nI see no problem changing this to exit with a 0 code, if you need this.  Feel free to submit a pull request with that change.  If you do, be sure to also update the Changelog with a blurb about what's been updated.\n. Closing via #541 \n. I forgot to have you add yourself to the Contributors file.  Can you do that, too?\n. Thanks for this!\n. Hmmm.  That kind of paints you into a corner.\nCurator requires http access as it is API-driven.  It does not need to be run only on the masters, or anything like that.  It only needs to run against a single client, which can be any node at all, provided you haven't disabled http access.\nI would simply run it against one of the clients (removing the --master-only option) and monitor the exit code so you can tell it ran successfully each day.\n. @panda87 Understood.  Rather than set http.enabled to false, perhaps you could set it to only listen on 127.0.0.1.  In that way, nothing external could query the masters, but a localhost API call, like Curator, would work.\n. That's one thing snapshots actually make easier: sorting by date. They have an epoch timestamp that curator uses for this. While not impossible, it's more work to add this, and more flags to manage. I'll add the adopt me tag for now. \n. Closing in favor of #649, which will add filter by count.\n. Thanks for signing the CLA and for patching this doc.\n. The CLI works with AWS ES.  Just be sure to use v3.4.0\n. The error is in the logs:\nFailed to parse setting [master_timeout] with value [30000] as a time value: unit is missing or unrecognized\nYou're using Curator 3.2.3 with Elasticsearch 2.0.0.  You should be using Curator 3.4.0, as I believe there are a few fixes to address things like this.\n. By the way, the unit in question has to do with a hidden, internal setting called master_timeout.  It has nothing to do with the --time-unit setting.\n. Here are the lines of code merged on October 8 to fix the issue you hit:\npython\n    if get_version(client) >= (2,0,0):\n        if type(master_timeout) == type(int()):\n            master_timeout = str(master_timeout/1000) + 's'\nTo reiterate, you need at least Curator 3.3.0 to run against Elasticsearch 2.x.  The most recent version is 3.4.0.\n. I'm closing this, as the issue has already been addressed in a more recent version of Curator.\n. The YUM repo has 3.4.0.  Perhaps something else is amiss.\n. @SteveDevOps \n@corey-hammerton is right.  Your snapshot name pattern is quite different from the default, which is some form of NAME and includes an strftime string like this: marvel-20160126093236\nThis is shown in the help output:\n```\n$ curator show snapshots --help\nUsage: curator show snapshots [OPTIONS]\nGet a list of snapshots to act on from the provided arguments, then\n  perform the command [delete, show] on the resulting list.\nOptions:\n  --newer-than INTEGER            Include only snapshots newer than n\n                                  time_units\n...\n  --timestring TEXT               Python strftime string to match your\n                                  snapshot's definition, e.g. 20140715020304\n                                  would be %Y%m%d%H%M%S\n...\n```\nSo the default is YearMonthDayHourMinuteSecond with no spaces or punctuation between them.\nYour snapshot names are of a pattern ol-close-%m%d%Y-%H%M%S-nnn.  I'm not sure about the last three numbers, but you get the idea.\n. To the API?  The es_repo_mgr endpoint that comes with Curator does this, and there are a few modules in its code you'd be able to use, if you follow the code and use them.  Otherwise, it's not something I originally planned on adding.\n. > I was using the curator object inside an Ansible module\nI'm not trying to be dismissive, but I'm kind of surprised by this.  You are automating/scripting the creation of repositories?  This is something that happens regularly enough that you need to automate it?\nI might be able to migrate that method into the API, but I'm not sure when I'll get to it.\n. Methods moved and documented :smile: \n. @fiunchinho See the updated docs.\n. @erik-stephens I've had the same problem with my repository disappearing.  Thanks for the report. I'll add in some improved logging to cover this.\n. Thanks for touching up the docs.  Could you please sign the CLA, as this is a requirement for any code (or documentation) submission.\n. @iz4blue any update here?\n. @iz4blue I can't merge your changes without you signing the CLA.\n. I'm a little worried about feature creep.  While I understand the utility, how do you define this on a command-line?  This is the kind of feature that requires a config file, rather than command-line options.  Also, what is the reasoning to add a filter after the fact, instead of using an index template to add it at index creation time?  There's a need to add a filter after the fact?\nI'd like input from the product team on this.  I'm not sure if Tanya, Steve, or Alvin is the right person.\n. Fixed by #614\n. Is there more to the error message?\n. Are you building on a Windows machine?\n. You probably don't need the ./.  I did a clean build with a fresh git clone of master on a brand new Ubuntu 16.04 box:\n```\npython setup.py build_exe\nrunning build_exe\ncreating directory build/exe.linux-x86_64-2.7\ncopying /usr/local/lib/python2.7/dist-packages/cx_Freeze/bases/Console -> build/exe.linux-x86_64-2.7/curator\ncopying /usr/lib/x86_64-linux-gnu/libpython2.7.so.1.0 -> build/exe.linux-x86_64-2.7/libpython2.7.so.1.0\nwriting zip file build/exe.linux-x86_64-2.7/lib/python27.zip\nName                      File\n  ----                      ----\nm BUILD_CONSTANTS\nm StringIO                  /usr/lib/python2.7/StringIO.py\n... LOTS OF OUTPUT\n... ENDS SUCCESSFULLY\n```\nI did have to hack and install cx_Freeze from source, as it wouldn't build from pip.  But it did build.  But the beta release of cx_Freeze 5.0 still has some bugs:\n\u00bb cd build/exe.linux-x86_64-2.7/\nbuh@macbuntu1 (06:32 PM) ~/git/curator/build/exe.linux-x86_64-2.7\n\u00bb ./curator --help\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/cx_Freeze/initscripts/Console.py\", line 7, in <module>\n    import os\nImportError: No module named os\nbuh@macbuntu1 (06:32 PM) ~/git/curator/build/exe.linux-x86_64-2.7\n\u00bb ls lib\npython2.7  python27.zip\nbuh@macbuntu1 (06:32 PM) ~/git/curator/build/exe.linux-x86_64-2.7\n\u00bb file curator\ncurator: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, for GNU/Linux 2.6.32, BuildID[sha1]=8f1852d4de0f340b37478fc11e43746e01e3bed6, stripped\n. Can I close this now?\n. Or you can \ud83d\ude04 \n. Never mind.  It's not an issue.\n. > 2016-02-11 01:23:29,905 DEBUG elasticsearch log_request_success:66 < {\"OK\":{}}\nThis appears to be because you're using an incompatible plugin.  Curator is not at fault.  I will be closing this as a result.\n. @alexandrz A good question.  The project that actually matters is https://github.com/elastic/elasticsearch-py, as that's what Curator uses.  Also, elasticsearch-py uses the urllib3 module, so it becomes a question of how that would be handled up the chain.  If elasticsearch-py and urllib3 can do it, then it would be possible to build support for this into Curator.  I've sent a private message to the maintainer of elasticsearch-py regarding this.\n. @alexandrz Would this do it? http://elasticsearch-py.readthedocs.org/en/master/#running-with-aws-elasticsearch-service\n. I would need you to add/test the functionality, pretty much exactly as described there.  If you cannot easily do that, I can try to create a branch with it, and roughly add the args to the command-line and let you test that.\n. You may need to hold off a bit on a pull request as I will not be merging any new features into the Curator 3.x branch. \nI'm in the middle of release-prep for Curator 4.0, which is a near-complete rewrite.  Most of your changes to get_client() should still transfer over as I've largely kept that method intact, so that's a good thing.  The rewrite uses configuration files instead of an ever-growing number of command-line arguments, so that should also make your additions simpler there.  \nPlease continue to use and test your forked version against ES in AWS, though.  This will be important usability testing.\nWe'll have to be careful about how we name and refer to this functionality.  AWS ES, at least in my mind, refers to Amazon's Elasticsearch offering, rather than Elasticsearch deployed to AWS instances.  Curator 4.0 will not be reverse compatible with Elasticsearch versions less than 2.0, and therefore will be wholly incompatible with the Amazon Elasticsearch offering, which is presently stuck at Elasticsearch v1.6.\n. This should be addressed by #595 and #596 with Curator 4.0 imminent.\nIf you feel this is in error, please feel free to reopen this ticket or open another.\n. @thinkspill, please try:\ncurl -XGET localhost:9200/_cluster/state/master_node?pretty\nIf your client is able to perform a master_node request, you should see a response sort of like this:\n{\n  \"cluster_name\" : \"untergeek_testing\",\n  \"master_node\" : \"h4HrNs5fTnqWiYgBhTzQVg\"\n}\nHowever, I expect you to see a 401 still. I'm pretty sure AWS ES does not allow for any /_cluster/state calls, which is why using the --master-only flag is causing you headaches.\n@alexandrz, I'm not sure what you're seeing, but if you're using the --master-only flag, the same applies.\n. @thinkspill That is correct, IIRC. You cannot close indices in AWS ES.\n. Please sign the CLA. I will merge after that.\n. I'm sorry this is frustrating, but it is deliberate.  Were it not so, the Curator command line:\ncurator delete indices --timestring %Y.%m --time-unit months --older-than 1\nwould delete the January index on February 1.  Hence, Curator counts in full months as a safety precaution against accidental deletion.  Of course, this causes confusion, and frustration for someone who wants to optimize the January index on February 1.  Because of this, you can use --older-than 0 to get around this.\n. > One consequence is that if you have a cron job running curator every day, it won't delete anything for a whole month, then on the first day of the month, it will delete a months worth of indices\nThis is incorrect. When using --time-unit months, each index is exactly 1 month, rather than counting days or weeks in a month.\nOf course, this will become semi-moot for future releases of Curator (but only with Elasticsearch 2.0+) where index ages will be determined by the Stats API, rather than only by the index name pattern.\n. I'm sorry you're all having a less than ideal experience. This issue should be completely obviated in the upcoming 4.0 release of Curator, by way of the following changes:\nIndex timestamps can be calculated in 3 different ways:\n1. Timestring in the index name\n2. Index creation time \n3. Using the Elasticsearch stats_api to look at the youngest and oldest documents in an index, based on a timestamp field.\nRather than calculate to a month, day, or hour, Curator will use epoch time for these 3 methods, and will normalize epoch + milliseconds (or any other higher resolution) down to just epoch seconds.  The calculation of what is older than, or younger than a given epoch time will be by way of a reference point, also calculated as epoch time.  There will no longer be any concept of \"whole months,\" which is what led to this issue.  If an index is older or younger than the reference point in time, it will be acted upon.\nIn order to make this work for users, more filtering options will almost certainly be necessary.  As a result, Curator 4.0 will no longer have any command-line options other than a version output, but will instead use a YAML configuration file.\nI hope to have a beta version of 4.0 out in the next few weeks.  Curator 4.0 will not be reverse compatible with any version of Elasticsearch older than 2.0.  It will also not work with AWS ES, as Curator requires access to the cluster metadata, which endpoint is blocked there.\n. I hope to release 4.0 Beta before the end of the week.\n. This should be addressed by #595 and #596 with Curator 4.0 imminent.\nIf you feel this is in error, please feel free to reopen this ticket or open another.\n. Doesnt this patch require an updated version of the client?  Won't it fail without that?\n. Unfortunately you can't. @HonzaKrall is probably traveling home from our engineering all-hands right now. He'll merge it after he's home, settled, and had a chance to review it. \n. No auto-fill for names in the iPhone version of GitHub. That's @HonzaKral, I think. \n. @richm Can you please update your PR with \n1. requirements.txt, line 2.\n   - elasticsearch>=2.3.0,<3.0.0\n2. setup.py, line 25\n   - res = ['elasticsearch>=2.3.0,<3.0.0' ]\nWhen you submit, it should re-test with the new requirement, which hopefully gets picked up properly.\n. @richm sorry for the delay.  I've been busy taking care of my family the last few days.  \nI've tested the updates locally.  I'm not sure what's up with Travis, but it works on Elasticsearch versions 1.4.4 and 2.2.0.\nI note that there is no update to the Changelog.  Can you please rebase (since your version is behind) and add something to the Changelog (in the General section)?  Should be ready to merge at that point.\n. Thanks, @richm, but it looks like you forgot to rebase before adding your changes to the Changelog.  I'm seeing merge conflicts now in GitHub.\nCan you please rebase against master and try again?\n. > ```\n\nAdd support for the --client-cert and --client-key command line parameters and client_cert and client_key parameters to the get_client() call. #520 (rich)\n```\nThis needs to be under the 3.5.x release in the General section, rather than the 3.4.1 release, since that's already out.  Just mimic the pattern of the others.\n. Tiny nitpick here, but since this is a feature add, and not a bug fix, could you just delete the *Bug Fixes* section so it fits under *General*?  We're good to go after that.\n\nThanks for indulging me in my pickiness.\n. LGTM\u2122\n. Ordering by age with delete by space is addressed by #595 and #596 with Curator 4.0 imminent.\nPlease open a separate feature request for select by alias.\n. I presume you meant 3.4.1 as the Curator version. \nI'd run it again with the --debug flag to see more of the error. \n. > The operation is not valid for the object's storage class\nIf you are trying to snapshot to Glacier, this is not supported by Elasticsearch.  Only to a regular S3 file store.   You could theoretically move the files from there to a Glacier storage, but not directly.\n. Thanks for the report.  Closing this now.\n. This will be possible in a future release, but only for newer versions of Elasticsearch. This is because Curator will make use of the Field Stats API. \n. This should be addressed by #595 and #596 with Curator 4.0 imminent.\nIf you feel this is in error, please feel free to reopen this ticket or open another.\n. @berglh Please address usage questions to https://discuss.elastic.co/c/elasticsearch where I, other Elastic engineers, and community members can help answer your question, and leave it in a place where others can find it later.. @catalincatana I'm not sure what happened on the Elasticsearch side that resulted in both a failed (empty) snapshot and a successful one.  It may be that this is expected behavior.  I've never actually looked at the files after a failed snapshot attempt.  I use the API to check and see what's there.  Chances are good that if you check the API, or use curator show snapshots --all-snapshots --repository REPONAME that you'll only see the valid snapshot.  If Elasticsearch is doing attempting to create a snapshot, then abandoning the empty file/directory, that may be a bug.  Without knowing more about your environment, cluster, version(s), repository type, etc, I can only guess what happened.  Remember, Curator only makes API calls, and responds to the response codes it receives.  Everything else is at the Elasticsearch level.\n. @catalincatana what is the full command-line (minus any connection credentials) you're using with Curator?  What version of Elasticsearch are you using?\n. @catalincatana Also, after taking the snapshot, do you see both the complete and incomplete snapshots in the show snapshots command?\n. > Elasticsearch version is: 1.1.1\nThat may be some of the issue.  Elasticsearch is on version 2.2.0 right now, with 1.1.1 being a year or so old.  Snapshots were only added in 1.0.0, and many bugs and issues have been squashed since then.  \nVersions less than 1.4 are deprecated and no longer supported, so I highly recommend upgrading\u2014carefully and methodically\u2014to more recent versions in stages.  The reason for this is that your 1.1.1 indices may not be recognized in a direct upgrade to 2.2.0.  You may have to go from 1.1.1 to 1.4.4, to 1.7.3, to 2.2.0, or something like that.  You may also have to run an optimize (don't forceMerge to 1 on the first time, unless they're already optimized) in between to get new segments created which will be recognized in newer versions of Elasticsearch.\nI'm not 100% sure of your upgrade path, but you can certainly find out by asking at https://discuss.elastic.co/c/elasticsearch\n. Thanks!\n. Thanks for catching this!  Don't know why I didn't think of it when I merged the original PR, but clearly there aren't many people using this or I'd have heard about it sooner (or they just didn't check until today :smile: )\nCan you please:\n1. Sign the CLA\n2. Update the Contributors file\n3. Update the Changelog with a brief synopsis of what you've done (under Bug Fixes)\nIf you sign the CLA before merging other commits, it should pick up the change then.  If not, I can manually verify.\n. Very strange.  I wonder why it doesn't recognize you. A manual CLA check says you signed the CLA on 2015-06-09.  Did you commit to a different Elastic repository last year?  Did you change the email address associated with your Github account by chance?\n. No worries.  Must be something up with our CLA checker.\n. @niemyjski I'm sorry to hear your experience has been less than stellar.\nBefore I looked at the link you provided, I saw this in the log data you pasted:\nFile \"/usr/local/lib/python2.7/dist-packages/elasticsearch/connection/http_urllib3.py\", line 99, in perform_request\n    raise ConnectionTimeout('TIMEOUT', str(e), e)\nelasticsearch.exceptions.ConnectionTimeout: ConnectionTimeout caused by - ReadTimeoutError(HTTPConnectionPool(host=u'localhost', port=9200): Read timed out. (read timeout=120))\nA timeout, such as this one, means that Curator is not able to get the data or response it expects from Elasticsearch in the given time.  In this case, it's 120 seconds.  This differs from what you have in that wiki link, which says 600 seconds.\nHaving perused the contents of that wiki entry, I see that you're using Azure for your snapshot repository.  Remote repositories can take a lot longer to respond,  I suggest at least a 600 second timeout, if not 900.  The reality is that Curator isn't a special client tool.  It just does API calls to Elasticsearch and reports the results.  If Curator timed out at 120 seconds, chances are very good that a curl call of the same would suffer similarly.\nSome questions to help troubleshoot this further would be to ascertain how many snapshots are in these repositories, and other related questions, like, \"how often are they captured?\"  One reason for timeout issues could be the large volume of segments having to be tracked and accounted for.  You're deleting snapshots after 7 days, but are you only capturing one every day, or more frequently?  How many indices are there in a given snapshot?\n. That's kind of what I expected.  Hourly (and 12 hourly) snapshots on a cluster with time-series data can result in a lot of segments that need to be kept.  Lots of segment checks is the subject of this Elasticsearch issue, which was corrected in this merge.\nI do not believe the fix made it into the 1.7.x branch of Elasticsearch, but is only available in Elasticsearch 2+.  I looked through all of the release notes for all 1.7.x versions and could not find this issue, though I may still be wrong and they figured out how to back-port it.\nI invite you to try snapshots from the command-line with curl and see if the same delay/timeout occurs.  I'm confident that the duration of time required for snapshots will be equally visible using this method.  Users of AWS and Azure have experienced these kinds of delays, especially associated with frequent snapshots, seemingly tied to the I/O latency associated with the remote filesystem, at least until the fix listed above was merged into the 2.x branch.\nRelated, informative bits:\nhttps://github.com/elastic/elasticsearch-cloud-aws/issues/150\nhttps://github.com/elastic/curator/issues/174#issuecomment-57056621 (Specifically the Thought Experiment and \"No such thing as duplicates\").\n. I can only point to the issues mentioned.  I don't know that separate containers/folders matters much.  It's the count of segments in a given repository that matters.  The more there are, the longer things take, as in the previously linked issue, Snapshot deletion and creation slow down as number of snapshots in repository grows\nThe fix is in Elasticsearch 2.0+.  \n\nI've tried directly from sense and they do take a while to delete but getting the list is pretty quick. but is kinda slow when doing it from curator.\n\nThe slowness of Curator pulling the list is something that can be improved, but is not the real issue here.  Look for improvements in the forthcoming Curator 4.0.  Unfortunately, it will not fix the slow delete issue described in this ticket.\n. The --dry-run flag will only show what would be deleted, but you're welcome to give that a try.\n. Hmmm.  There could be more information available from the TransportError, but it's not showing what it is because the exception isn't being fully logged.\nA TransportError is likely to imply a 400 or 500 HTTP response, though.  Something upstream telling this request thread that it cannot proceed.  It could be simultaneously running snapshots causing a collision, but it's hard to say without the error code.\n. I can try.  There's another bug or two I need to address to release 3.5.1, and I may get to it as early as Monday, if I'm lucky.\n. You are definitely hitting something strange.  I'm not even attempting to pull an indexed item.  Line 126 of snapshot.py looks like this:\nlogger.error(\"Unable to delete snapshot {0} from repository {1}. Error message: {2}.   Run with --debug flag and/or check Elasticsearch logs for more information.\".format(snapshot, repository, e))\nWhat this means is that the elasticsearch python module is having a hard time parsing the response, as it received something it did not expect.\n. Do you see any errors logged in Elasticsearch (particularly the elected master node, and/or the client that Curator connected to) at the time this ran?\nI'm wondering if any plugins/modules you have extended Elasticsearch with may have changed responses, and it's breaking the elasticsearch python module's rules, since it is not what Elasticsearch should be sending back.  What plugins/modules have you added to Elasticsearch (besides Azure)?\n. I just spoke with the developer/maintainer of the elasticsearch python module.  He says this is a minor reverse incompatibility issue between es-py module 2.x when using a 1.x version of Elasticsearch.  As you can see, Curator does everything it can to be fully reverse compatible, but this is a limitation specifically with this release of Curator, which depends on es-py 2.3.0.  We won't be able to see what's really going on until you're on Elasticsearch 2.x\nAs far as this goes, I can put checks in to see if a snapshot is in progress before attempting a delete, but this feature will not go into Curator 3.x.  The 3.x branch is bug fixes only now, and I'm working on a 4.0 release.  4.0 will not have guaranteed reverse compatibility with all 1.x versions of ES, but will hopefully work with 1.7 still.  When Curator 5.0 comes out, it will be version-tied to Elasticsearch 5.0, and all subsequent releases will be major-version-bound.\nIn the meanwhile, you could do some shell scripting to see if a snapshot is in progress using curl calls (or something like it) before running Curator to achieve the same effect.\n. 4.0 is a few weeks away, at least, so I recommend finding some way to handle this in the meanwhile.\n. Thanks for reporting this.  It is quite likely that there needs to be a higher timeout for this operation.  Have you tried running with a higher timeout value than the default of 30 seconds?\n. @ferki 3.5.1 is now in PyPI and the official elasticsearch repositories (CentOS and Debian-styles).  The Windows binaries are also updated, but the links will not show up in the documentation until the next doc-build cycle (could be an hour or so).\n. @rhorii Thanks for reporting it.  3.5.1 is released now, so please download and make use of it.\n. Presuming this is closed by #595 and #596 with Curator 4.0 imminent.\nIf you feel this is in error, please feel free to reopen this ticket or open another.\n. I'm pretty bound by the strictures of Python's datetime and strftime implementations, so this isn't likely to happen.  However...\nCurator 4.0 should address this concern by allowing you to identify index age by field_stats API and creation_date, and not just trying to extract a timestamp from the index name.\n. Presuming this is closed by #595 and #596 with Curator 4.0 imminent.\nIf you feel this is in error, please feel free to reopen this ticket or open another.\n. This is unfortunately unsupportable in the 3.x branch of Curator.  I will see if I can get it to work in the 4.x dev branch I'm working on right now.\n. This should be addressed by #595 and #596 with Curator 4.0 imminent.\nIf you feel this is in error, please feel free to reopen this ticket or open another.\n. I'm pretty bound by the strictures of Python's datetime and strftime implementations. \nCurator 4.0 should address these by allowing you to identify index age by field_stats API and creation_date, and not just trying to extract a timestamp from the index name.\n. Presuming this is closed by #595 and #596 with Curator 4.0 imminent.\nIf you feel this is in error, please feel free to reopen this ticket or open another.\n. You're really creating and deleting hundreds of indices per day?  How many active indices do you retain on an ongoing basis (per node, the whole cluster)? Each shard, each index has a management cost to the cluster.  If you are creating hundreds of indices per day, and then deleting hundreds of \"expired\" indices per day, this is a tremendous overhead for an Elasticsearch cluster.  \nIt may be possible to do what you're requesting, but I'm not sure retooling Curator is the proper solution to this issue.  If you must use a heavy index/shard count per day, you may be better served with a custom script using the Curator API that retries indefinitely until all deletes are completed.  Curator retries 3 times because that's all it should take.  Even that concession was debated hotly at the time it was requested.\nCurator is just a specialized API frontend.   Curator sends API requests to Elasticsearch, and Elasticsearch responds.  So why does Curator time out for your use case?  Because Elasticsearch rebalances the cluster by shard count per node by default.  If you're trying to delete so many at once, Elasticsearch is trying to rebalance the cluster.  So many shards and indices shuffling around is taxing your cluster quite heavily, and it is unable to simultaneously rebalance and process new delete (or even create requests, I'd wager) in a timely manner.  The timeouts are an indication of a very busy cluster, as described, and Curator is only responding with what it received from Elasticsearch.\n. Closing in favor of #649, which will add filter by count.\n. There are two potential answers.\n1. Escape the percent signs: https://github.com/elastic/curator/issues/151#issuecomment-88252653\n2. If systemd, double the percents: https://discuss.elastic.co/t/curator-how-to-escape-characters-of-timestring-in-systemd/25222\n. Yes, but is it systemd?  If so, it would need to have double percents instead.\nOther alternatives are to use a wrapper script and call that in cron instead.  Some cron variants choke on the length of the command line.\n. I recommend trying it sooner with the --dry-run flag (in your cron line) before just hoping it works for tonight's run.\n. This should be addressed by #595 and #596 with Curator 4.0 imminent.\nIf you feel this is in error, please feel free to reopen this ticket or open another.\n. Can you share your whole command line?\n. This debug output seems to be missing something.  Could you paste the first part, where the client gets built?\n. That's actually somewhat of a relief.  I'll investigate my build system and see what I can find.  I may share test binaries with you once I've built them.  It won't be before next week, though.\n. They should be out on Monday, though they may suffer from the same SSL issue as I haven't updated my build computer yet.\n. I've updated the Windows downloads for Curator 3.5.1.  I tested them against Shield so they should work for you.\nWait until about 20 minutes from now and the links for Curator 4.0.0a7 should be available at the bottom of the page: https://www.elastic.co/guide/en/elasticsearch/client/curator/4.0/installation.html\nIf they still say 4.0.0a2, then it hasn't updated yet.\n. Since I've pushed the fixed binaries, I'm closing this.\n. Yes.  This is what I tried with my test Shield + SSL setup with the fixed win32 binary.  It most certainly connects over SSL because that's all it allows.  Note that it still says it's using http.  This is not the case.  I'm not sure why it still says SSL, but it's not.  If I try to curl this host with http it fails with an empty response, but with an https it does connect.\nHere's the output from mine:\n```\nC:\\Users\\Admin User\\Downloads\\curator-3.5.1-win32>curator.exe --debug --host steiny --use_ssl --ssl-no-validate --http_auth buh:changemenow show indices --all-indices\n2016-04-26 18:12:54,523 INFO      curator.cli.index_selection                indices:62   Job starting: show indices\n2016-04-26 18:12:54,539 DEBUG     curator.cli.index_selection                indices:65   Params: {'url_prefix': u'', 'http_auth': u'buh:changemenow', 'dry_run': False, 'certificate': None, 'loglevel': u'INFO', 'logformat': u'default', 'client_cert': None, 'host': u'steiny', 'quiet': False, 'timeout': 30, 'debug': True, 'use_ssl': True, 'logfile': None, 'master_only': False, 'port': 9200, 'ssl_no_validate': True, 'client_key': None}\n2016-04-26 18:12:54,571 DEBUG          curator.cli.utils             get_client:114  kwargs = {'url_prefix': u'', 'http_auth': u'buh:changemenow', 'dry_run': False, 'certificate': None, 'loglevel': u'INFO', 'host': u'steiny', 'quiet': False, 'port': 9200, 'logformat': u'default', 'timeout': 30, 'debug': True, 'use_ssl': True, 'logfile': None, 'master_only': False, 'client_cert': None, 'ssl_no_validate': True, 'client_key': None}\nC:\\Users\\Admin User\\Downloads\\curator-3.5.1-win32\\library.zip\\elasticsearch\\connection\\http_urllib3.py:70: UserWarning: Connecting to steiny using SSL with verify_certs=False is insecure.\n2016-04-26 18:12:54,632 DEBUG         urllib3.util.retry               from_int:164  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-04-26 18:12:54,648 INFO      urllib3.connectionpool              _new_conn:788  Starting new HTTPS connection (1): steiny\nC:\\Users\\Admin User\\Downloads\\curator-3.5.1-win32\\library.zip\\urllib3\\connectionpool.py:821: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html\n2016-04-26 18:12:55,648 DEBUG     urllib3.connectionpool          _make_request:395  \"GET / HTTP/1.1\" 200 333\n2016-04-26 18:12:55,664 INFO               elasticsearch    log_request_success:63   GET http://steiny:9200/ [status:200 request:1.031s]\n2016-04-26 18:12:55,680 DEBUG              elasticsearch    log_request_success:65   > None\n2016-04-26 18:12:55,680 DEBUG              elasticsearch    log_request_success:66   < {\n  \"name\" : \"Gammenon the Gatherer\",\n  \"cluster_name\" : \"untergeek-shield\",\n  \"version\" : {\n    \"number\" : \"2.3.1\",\n    \"build_hash\" : \"bd980929010aef404e7cb0843e61d0665269fc39\",\n    \"build_timestamp\" : \"2016-04-04T12:25:05Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"5.5.0\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n2016-04-26 18:12:55,710 DEBUG          curator.cli.utils          check_version:90   Detected Elasticsearch version 2.3.1\n2016-04-26 18:12:55,726 DEBUG         urllib3.util.retry               from_int:164  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\nC:\\Users\\Admin User\\Downloads\\curator-3.5.1-win32\\library.zip\\urllib3\\connectionpool.py:821: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html\n2016-04-26 18:12:55,773 DEBUG     urllib3.connectionpool          make_request:395  \"GET /_all/_settings?expand_wildcards=open%2Cclosed HTTP/1.1\" 200 180\n2016-04-26 18:12:55,789 INFO               elasticsearch    log_request_success:63   GET http://steiny:9200/_all/_settings?expand_wildcards=open%2Cclosed [status:200 request:0.062s]\n2016-04-26 18:12:55,805 DEBUG              elasticsearch    log_request_success:65   > None\n2016-04-26 18:12:55,821 DEBUG              elasticsearch    log_request_success:66   < {\"myindex\":{\"settings\":{\"index\":{\"creation_date\":\"1461626021151\",\"number_of_shards\":\"1\",\"number_of_replicas\":\"0\",\"uuid\":\"eeFVMuijT46W_m7PR8MGkQ\",\"version\":{\"created\":\"2030199\"}}}}}\n2016-04-26 18:12:55,851 DEBUG          curator.api.utils            get_indices:29   All indices: [u'myindex']\n2016-04-26 18:12:55,851 DEBUG     curator.cli.index_selection                indices:81   Full list of indices: [u'myindex']\n2016-04-26 18:12:55,868 INFO      curator.cli.index_selection                indices:101  Matching all indices. Ignoring flags other than --exclude.\n2016-04-26 18:12:55,882 DEBUG     curator.cli.index_selection                indices:103  All filters: []\n2016-04-26 18:12:55,882 INFO      curator.cli.index_selection                indices:149  Action show will be performed on the following indices: [u'myindex']\n2016-04-26 18:12:55,898 INFO      curator.cli.index_selection                indices:153  Matching indices:\n2016-04-26 18:12:55,914 DEBUG         urllib3.util.retry               from_int:164  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\nC:\\Users\\Admin User\\Downloads\\curator-3.5.1-win32\\library.zip\\urllib3\\connectionpool.py:821: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html\n2016-04-26 18:12:55,976 DEBUG     urllib3.connectionpool          _make_request:395  \"GET /_cluster/state/metadata/myindex HTTP/1.1\" 200 318\n2016-04-26 18:12:55,993 INFO               elasticsearch    log_request_success:63   GET http://steiny:9200/_cluster/state/metadata/myindex [status:200 request:0.079s]\n2016-04-26 18:12:55,993 DEBUG              elasticsearch    log_request_success:65   > None\n2016-04-26 18:12:56,007 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"untergeek-shield\",\"metadata\":{\"cluster_uuid\":\"_na\",\"templates\":{},\"indices\":{\"myindex\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1461626021151\",\"number_of_shards\":\"1\",\"number_of_replicas\":\"0\",\"uuid\":\"eeFVMuijT46W_m7PR8MGkQ\",\"version\":{\"created\":\"2030199\"}}},\"mappings\":{},\"aliases\":[]}}}}\nmyindex\nC:\\Users\\Admin User\\Downloads\\curator-3.5.1-win32>\n``\n. Also, if you note the logs, it says,Starting new HTTPS connection (1): elasticsearch.contoso.com`, so it was trying to start an HTTPS connection.\n. Sounds reasonable.\n. The current behavior would automatically chunk lists into acceptable URL lengths:\npython\n        self.loggit.info('Updating index setting {0}'.format(self.body))\n        try:\n            index_lists = chunk_index_list(self.index_list.indices)\n            for l in index_lists:\n                self.client.indices.put_settings(\n                    index=to_csv(l), body=self.body\n                )\nAs you can see, the current action is to act on a list of chunked lists.  I guess the question is whether it would be desirable to loop through the entire list one index at a time and pausing after each one, or in batches, and pause after each batch.  \nThoughts? @corey-hammerton what would you like to see here?\n. Yeah, the code wasn't a problem, but timeout becomes one, if you had a low default timeout value.\nI've already updated the default options for each action to allow for \"timeout_override\", so actions like this will not timeout after 30 seconds.  My local test revealed that is a likely outcome, and not all actions should have high timeouts, so this was a stepping stone to being able to allow for this to \"complete\" before continuing.\n. Are the hours in UTC or local tie?\n. That might explain the discrepancy. Calculations are all in UTC. \n. Not in 3.x. I haven't added it to the 4.0 alpha yet either, but would appreciate the feature request being added. It will not be back ported to 3.x\n. Have you tried running this with the --debug flag?  The output there will be much more informative.\n. The problem is that it's treating the t as a special character, like a . or a -.  This is the first time I've encountered anyone using this syntax, as most people have used hyphens or dots to separate things. \nI may back port the fix to 3.x, but chances are good that I will not.  I'm adding the fix to the 4.x branch now.\n. This is a duplicate of #584, which issue has been fixed in version 3.5.1.\nSince you're on 3.5.0, an upgrade should fix that.\n. https://www.elastic.co/guide/en/elasticsearch/client/curator/4.0/index.html\nSpecifically: \n- https://www.elastic.co/guide/en/elasticsearch/client/curator/4.0/actionfile.html\n- https://www.elastic.co/guide/en/elasticsearch/client/curator/4.0/configfile.html\n- https://www.elastic.co/guide/en/elasticsearch/client/curator/4.0/examples.html (There was an asciidoc bug here that should be fixed in about 10 minutes or so.  Some of the examples are under a different trunk, titled \"create_index\")\n. Sorry for the confusion, but the links to the usage documentation are in the main README for the project.\n. @Gueust are you trying to use Curator 4.0?  If not, be sure to look at the 3.5 documentation on the above site links.  There's a drop-down to select the proper version, as 4.0 is still a pre-release.\n. Master is 4.x now, so this is not incongruous.  The 3.x branch has its own documentation.  When 4.0 is officially released, it will be \"current,\" so the URLs will work then.  I didn't want to change the README to point to the 4.0 tree.\n. This is already possible with Curator 4.0.  Here's an example configuration.  The description field explains what is going on.\nYou could try this configuration with the --dry-run flag first so nothing would actually get deleted.\n```\nRemember, leave a key empty if there is no value.  None will be a string,\nnot a Python \"NoneType\"\n\nAlso remember that all examples have 'disable_action' set to True.  If you\nwant to use this action as a template, be sure to set this to False after\ncopying it.\nactions:\n  1:\n    action: delete_indices\n    description: >-\n      Delete indices.  Find which to delete by first limiting the list to logstash- \n      prefixed indices. Next filter by space, to those indices in excess of \n      20g of usage.  Then further filter those to prevent deletion of anything \n      less than 30 days old.\n    options:\n      continue_if_exception: False\n      disable_action: False\n    filters:\n    - filtertype: pattern\n      kind: prefix\n      value: logstash-\n    - filtertype: space\n      disk_space: 20\n      reverse: True\n      use_age: True\n      source: creation_date\n      timestring:\n      field:\n      stats_result:\n    - filtertype: age\n      source: creation_date\n      direction: older\n      timestring:\n      unit: days\n      unit_count: 30\n```\nYou could just as easily change the age types in the space and age filters to use the index name, instead of the index creation date.  Just look at the online documentation for Curator 4.0 to see more on how that would be done.\n. This feature will not be back ported or added to the 3.x branch, by the way.\n. Documentation has been heavily updated in #614 to improve readability, and also reflect changes in functionality.  The exclude filter element is now valid for each filtertype, with default values for this setting described in each filter's documentation (which may take another 20 minutes before being available).\n. Going to close this now.  If you feel this is in error, please feel free to re-open.\n. There is no relationship between direction and use_age.  The space filter does not use the direction filter element, as space is always calculated based on keeping the youngest indices, and deleting the oldest indices based on space consumption.  I'm working on better wording and explanation for these filters, so thank you for the pull request and suggestion.  It's a work in progress, while it's not yet fully released.\n. Of course it did.  RHEL/Centos never provide comprehensive python packages.\nI'll make sure it gets added.  Thanks.\n. I've updated the repositories with this.  4.0.0a9 is up now, too, though the docs won't catch up with it for another 20 minutes.\n. What do you mean?  Encryption of the data on the disk?  Encryption of the data as it's streaming to S3?\nCurator talks to Elasticsearch via the https://github.com/elastic/elasticsearch-cloud-aws plugin.  If encryption is disk side, and Elasticsearch and the plugin both support it, then Curator will just make it work.  \nCurator v4 experimentally allows request signing with AWS IAM credentials, if that's more of what you were asking.  See more here: https://www.elastic.co/guide/en/elasticsearch/client/curator/4.0/configfile.html\n. Thanks for the clarification.\nIt's important to note that Curator doesn't have any control over the S3 repository.  Curator sends API requests to Elasticsearch, and Elasticsearch does the snapshotting.  The AWS plugin documentation shows that an S3 repository can support server side encryption: https://www.elastic.co/guide/en/elasticsearch/plugins/current/cloud-aws-repository.html\nYou have to configure this on the Elasticsearch side of things, however.  Also, Curator cannot create an S3 repository with this option at present, but once the repository is created, it will snapshot to it through Elasticsearch's API as mentioned.  If I ever repackage the es_repo_mgr tool for use with Curator v4, I'll be sure to add support for this, but that tool is not being included at present.\n. Closing since it can be completely handled on the Elasticsearch side with the linked settings.\n. Which version of Curator did you install?  The command line you're attempting to use is from Curator 1.x., but from .cli import cli indicates a much more recent version.\nAlso, how did you install Curator?  You're attempting to launch curator.py, which is not how it's supposed to be run.  Curator installs an entry_point at /usr/local/bin/curator or on some systems, /usr/bin/curator.  You will always see the relative import message if you try to execute curator.py from the module path, rather than using the endpoint.\nYou should try just running: curator --help from the command line and see what you get.\n. As mentioned, you must be reading a very outdated set of instructions for Curator.  The command-line flags are very different from whatever you've been reading.  Have a look at the output of curator --help to see what I mean.\nThere is a link to the Curator documentation in the output of curator --help, but I'll include it here as well: https://www.elastic.co/guide/en/elasticsearch/client/curator/current/index.html\n. Closing since this is a non-issue.\n. It does need to be a cron job.\n. Don't need microseconds at all, as we're using whole seconds only.  Curator doesn't need more resolution than that.  \nI'll just change it so that all Pythons use the older format.\n. My previous patch to prune_nones tests for variable existence, instead of value, which fails to capture zeroes.  I'm releasing a patch for this shortly.\n. Thanks for catching this.  It was so much documentation, I'm sure there are tiny mistakes like that in other places too \ud83d\ude26 \n. Thanks again.  By all means, add yourself to the Contributors file in another PR.\n. Curator 4's configuration doesn't mesh with this, but I intend to put some of the v3-style command-line features back in as separate scripts.  Perhaps this is one that would make sense to have in that bucket.\n. @danielmotaleite Changing the refresh_interval only makes sense for indices actively indexing documents because refreshes only happen when new documents are coming in.  For time-series indices, it makes sense to set this in the index template, rather than with an external tool.  Because of that, it is uncommon to need to change this setting on the fly.\nSee: https://www.elastic.co/guide/en/elasticsearch/reference/5.1/docs-refresh.html\n\nElasticsearch automatically refreshes shards that have changed every index.refresh_interval which defaults to one second. \n\n(Emphasis added)\n. It's not quite as simple as that, unfortunately.  If you'd like to take a stab at adding that feature, I accept pull requests, and I'd be happy to help with tests and such.. Since newer indices automatically have the higher priority (according to the documentation), this is desirable, but less pressing.. Also going to lump this under #656. The Elasticsearch API does not allow for this, so Curator does not either.  The only way to accomplish this would be to get all repositories, and iterate over them.  This could be done using the Curator API, though it would be unwieldy.\nI would invite you to consider re-thinking your backup strategy a bit.  Snapshots are not the same as other incremental backups.\nSnapshots are incremental at the segment level, not at the file or document level.  When a snapshot is taken, it backs up every new segment it has no record of, and points to every segment already snapshotted. No segment is ever deleted as long as a single snapshot points to it. This means that every single snapshot is like a full snapshot, because it points to every segment necessary to restore the cluster to that point in time. You shouldn't need to have a new repository every week.  You could just do weekly snapshots and those would be your full \"restore to that point in time\" backups.  You could delete any daily snapshots in between these weekly snapshots and that would cover things for you.  Just name them differently (you could prefix them with daily- and weekly-) so they are easily found and pruned by Curator.  You could then keep daily incremental snapshots as long as you deemed necessary, and weekly ones at a different retention.\n. Going to close this as I do not believe it's a usage pattern that, as described, neither Elasticsearch nor Curator are intended to address.\nIf you feel this is in error, please feel free to re-open this issue, and elaborate on your reasoning.\n. Indeed. Not going to fix it because v4 has completely different behavior. \n. This is expected behavior.  If you have not specified a --logfile, it will still log to STDOUT.  The --quiet flag was to prevent other console output from being delivered.  Logging output will still go wherever you tell it, with STDOUT being the default.\nThis is documented here\n. I used to have different exit codes for different actions.  It was a huge headache, and I got a lot of flak for it, so I removed that feature.\nCurator v4 is in alpha right now, and it's not really as command-line oriented as v3.  It uses YAML config files now.  You will need to read through logs to see what kinds of failures happened, as the object-oriented approach is just catching exceptions and exiting with a 1 on any failure, still.  I do not anticipate this changing for Curator v4, due to this change.  No other changes will be back-ported to Curator v3.\n. Not presently, but that can be a feature request for the newer 4.x branch.\n. 3.x works with older versions of Elasticsearch.\n. Hmmm.  That sounds suspicious.  Can you attach --debug output ?\n. Curator at one point did timestring/time-unit vetting, e.g. \"If you say --time-unit is weeks, ensure that  timestring contains %W\".  I think that got lost during the 3.x upgrade.  It was re-added after 3.4.0.\nThis is moot in Curator 4, because all timestamps are converted to epoch time.\n. @reqless, the title suggest that it worked at one point, then stopped working.  Is this the case?\nAlso, which version of Curator are you running?  I don't see that information in any of the comments.\nFor troubleshooting, using the --debug flag will be useful, as will looking in the Elasticsearch logs to see if there are any errors or extra information stored there.\n. @reqless I'd also be grateful if you'd dry this with Curator v4, to see if it still has the same problem.\n. Closing for lack of follow-up.  If you feel this is in error, please feel free to re-open, or open another issue.\n. Indeed, as @iainlbc pointed out.  You should increase your --timeout setting.\n. While I can, and will likely, make this change, the problem is that Curator 4 does not support versions of ES < 2.0.  That won't help you with AWS ES.  I don't know that I have a great desire to make changes to Curator 3.x at this point, either.\n. And the check in the line you mention is because if you send a snapshot request to Elasticsearch, and a snapshot is already in progress, it will fail.  Rather than try to process the \"why\" of all possible fails, Curator first eliminates this potential fail.  This is disturbingly common with slower repositories, with people trying to do hourly snapshots, so the check is there to prevent that from causing issues.\n. Truthfully, I don't think I can safely remove this functionality, as I look at it.  While you correctly read that the check for /_snapshot/_status is to ensure no other snapshot is running, it's not limited to the current repository.  Elasticsearch might have multiple repositories configured, but only one snapshot can be run at any given time, to any of them.  That's why Curator checks for snapshots across all possible repositories.  \nWhile I could theoretically test my way around this by just not testing for existing snapshots and try to catch failures with a try/except block, it's a monkey patch approach to a problem that doesn't exist anywhere outside of AWS ES.  As such, I'm not going to be fixing this.  I apologize for the inconvenience.\n. Thanks for the update, @cjuroz. Please open a separate issue and I will update the docs accordingly. . It's not a bad idea.  Have you hit an error with a version it's not working with?  I mean, it should work with 2.6+, generally speaking.\n. Is this for the 3.x tree or the 4.x tree?\n. This has been fixed on my test system, which is Centos 6.8:\nCentOS release 6.8 (Final)\nPython has done some odd things, so if you still have issues with setuptools being weird, you can also try \nsudo yum install python-distribute\nwhich should install setuptools in a suitable version (I've included it in the repo).\nThe version of setuptools in the repository seems to work:\n```\n[vagrant@localhost ~]$ python\nPython 2.6.6 (r266:84292, Jul 23 2015, 15:22:56)\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-11)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport setuptools\nsetuptools.version\n'20.3'\n```\n\n\n\nbut I upgraded it anyway and put a 20.10 version in as well:\n```\n[vagrant@localhost ~]$ python\nPython 2.6.6 (r266:84292, Jul 23 2015, 15:22:56)\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-11)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport setuptools\nsetuptools.version\n'20.10.1'\n```\n\n\n\nIt appears to work with both Curator 3.5.1 and my patched version of Curator 4.0.0a9 (because the released version had a python6 incompatibility, which will be fixed in the a10 release).\n. Would love to hear if the fixes I've put in work for you.  I fixed the 3.x and 4.x trees, and they seem to install properly on my clean vagrant instances.\n. In the absence of further response, I'm going to close this.  If you believe it is closed in error, feel free to re-open, or open a new ticket.\n. Currently, Curator 4 allows for this feature.  It is in alpha release right now.  I am working to get the first beta out this week, if all goes well.\nCurator 4 documentation is at https://www.elastic.co/guide/en/elasticsearch/client/curator/4.0/index.html\nEssentially, you'd add extra filters.  Filter stacking is not possible in Curator 3.x, but it is in 4.\nSnapshots are not monitored by size in Curator, by the way.  Only indices.\n. I should have said that Elasticsearch doesn't show how much space is consumed by snapshots.\nSee:\n{\n      \"snapshot\": \"marvel-20160513083214\",\n      \"version_id\": 2020199,\n      \"version\": \"2.2.1\",\n      \"indices\": [\n        \".marvel-es-2016.05.08\",\n        \".marvel-es-2016.05.07\",\n        \".marvel-es-2016.05.09\",\n        \".marvel-es-2016.05.04\",\n        \".marvel-es-2016.05.03\",\n        \".marvel-es-2016.05.06\",\n        \".marvel-es-2016.05.05\",\n        \".marvel-es-2016.05.11\",\n        \".marvel-es-2016.05.10\",\n        \".marvel-es-2016.05.02\",\n        \".marvel-es-2016.05.01\",\n        \".marvel-es-2016.04.30\"\n      ],\n      \"state\": \"SUCCESS\",\n      \"start_time\": \"2016-05-13T08:32:14.271Z\",\n      \"start_time_in_millis\": 1463128334271,\n      \"end_time\": \"2016-05-13T08:32:26.934Z\",\n      \"end_time_in_millis\": 1463128346934,\n      \"duration_in_millis\": 12663,\n      \"failures\": [],\n      \"shards\": {\n        \"total\": 12,\n        \"failed\": 0,\n        \"successful\": 12\n      }\n    },\n. Curator 4 will work with ES 2.x - 5.0\n. See the sample action file configuration for a similar (if not identical) request in #611 \nThe 4.0 documentation explains how to use the command-line.  You need to have a configuration file and an action file, as you pointed out.\n. Since this functionality has already been added to 4.x, I'm closing this issue.\n. Funny you should ask.  I'm adding it right now.  Yes, it's slated for 4.1.0\n. @kdubezerra \nSo you want to remove not older, but newer indices?\nYour config might look like this:\n```\nactions:\n  1:\n    action: delete_indices\n    description: Delete the newest 10 indices by creation_date\n    options:\n      ignore_empty_list: True\n      continue_if_exception: False\n    filters:\n    - filtertype: count\n      count: 10\n      reverse: False\n      use_age: True\n      source: creation_date\n```\nPlease keep in mind that this is a very simplistic configuration.  There are no other filters, so this will delete any index recently created, including .kibana and the .marvel-* indices.  You should probably prepend the count filter with a pattern filter to select only indices matching a pattern.\nOut of idle curiosity, why would you want to delete the newest indices instead of older ones?  What's the use case?\n. Ah, makes sense.  In that case, your config might look like this:\n```\nactions:\n  1:\n    action: delete_indices\n    description: Delete all logstash prefixed indices but the 10 most recent, by creation_date\n    options:\n      ignore_empty_list: True\n      continue_if_exception: False\n    filters:\n    - filtertype: pattern\n      kind: prefix\n      value: 'logstash-'\n    - filtertype: count\n      count: 10\n      use_age: True\n      source: creation_date\n```\nNote that reverse is omitted.  That's because the default is True, so it doesn't need to be there.  Also note the use of single quotes for the pattern filter value, 'logstash-'.  This is because it's used as part of a regular expression.  It's best practices to single-quote anything in the value field.\n. Actually, no.  Setting reverse to True does the opposite of what you're thinking, though with exclude set to True, the configuration you shared will still have the desired effect.  It would just get there in a more roundabout way.\nA sort operation will ordinarily go in alphabetical or numerically ascending order.  This would have logstash-2016.08.30 selected before logstash-2016.08.31, either by alphanumeric sorting or by converted date stamp.  Setting reverse to True reverses this order, and allows \"newer\" or \"younger\" indices to be selected first.  It counts from this order, and fills the count you selected, and drops everything else from the actionable list.  \nThe unit tests verify this behavior, so even if it doesn't make sense, it's how it works.  If you're still skeptical, you can always launch with --dry-run and see that the output does what you expect without changing anything.\n. @ebuildy It's an interesting idea, but I'm not sure if it's worth pursuing within the context of the newer Curator 4 configuration.  It would further complicate the YAML configuration to add \"today|thisweek|thismonth\" type configuration aliases, and then properly tokenize them.  Also, because Curator 4 converts all timestamps to epoch internally, I don't know that this is the best approach. \"Today\" is subjective, based on your timezone, and it could be very confusing for users, where epoch time is by definition, UTC only.\n. I'm closing this for lack of comment, and that it doesn't make much sense in Curator 4.  If you feel like this is in error, feel free to reopen the ticket, or open a new one.\n. It's on the roadmap to put them into Elasticsearch as a document, since ES can ingest YAML.  If someone wanted to add other ways/means of ingesting those files, I'm happy to merge PRs (must have tests).\n. Yes, but this is why the client configuration file curator.yml file and the action configuration file my_actions.yml file are separate.  With a local configuration for the client present, then the actions can be stored in other locations, such as the same Elasticsearch cluster you're Curating.\nAre you suggesting that pulling even the client configuration should be possible over URL?  That becomes a much more interesting question, especially since there could be authentication credentials in that file: user/pass, AWS tokens, etc.  You'd still need local access to any SSL certificates, if you were authenticating in that way, too.\n. If someone wants to add it, and provide tests, then I'm happy to merge that.  I won't likely be adding it myself, though.\n. What system are you running this on?  If your system is running systemd, try this FAQ answer about systemd.\nMost of the time, I don't use Curator directly in cron.  I use a wrapper script, and call the script via cron.\n. I should also mention that Curator v4 solves this by using configuration files instead of using lots of complicated, nested flags.  See https://www.elastic.co/guide/en/elasticsearch/client/curator/4.0/index.html\n. Sure:\nIn crontab: \n30 2 * * * /home/username/bin/curator.sh &> /dev/null\nAnd /home/username/bin/curator.sh would be:\n```\n!/bin/bash\n/usr/local/bin/curator --logfile /tmp/curator.log --host localhost delete indices --older-than 14 --time-unit days --timestring %Y.%m.%d\n```\nNote that there is no need for the single quotes or escaping of percents in the shell script.\n. I'm closing this since 4.0 resolves most\u2014if not all\u2014of the crontab issues by removing flags.  If you feel that more discussion is necessary, feel free to reopen the issue.\n. Replica counts are already supported in the 3.x and 4.0 branches.  I will look into the others.\n. I think that changing compression settings for sure should be added.  It may require a forceMerge immediately after changing (if it isn't auto-triggered).\n. But with command chaining in Curator 4, you could close the indices, change compression, and then reopen by using the same filters. It can be done!\n. I can guarantee you I won't likely be able to get to it before you, so I'll be happy to merge this in after you add it.\n. It won't make it into 4, but probably will in a later 5.x release.. The JSON output was designed to go straight into Logstash using the json_lines codec. It is supposed to provide the @timestamp field with the time the Curator event occurred. Why are you trying to remove/replace the event time?\n. Ah. You should let Logstash overwrite the timestamp. If Curator isn't sending the correct ISO8601, then either the server isn't UTC with an offset, or I need to coerce the time properly. \n. The Beats timestamp is not the event timestamp. \n. @ryancurrah Please see if you can patch/modify your existing Curator install with the changes in #662 \nI'd be interested to know if that addresses your concerns.\n. Interesting that your installation didn't pick up the timezone automatically.  Good to know.  This will make the patch very small.\n. See if the changes I just pushed meet with your approval.  There's no need to send an offset if we're always sending in UTC (Zulu) time, so I reverted back to the older Z notation.\n. I'm going to merge this now, as I have some other bits I want to push in another PR, and I think I've addressed your concerns.\n. Can you paste the debug output instead, please?\n. Thanks.  I'm looking it over right now.\nAs an aside, I took your config files, exactly as they were and replaced only the host and repository name, and it worked flawlessly for me.  I'm still extracting the cluster state from the log file, but I'm guessing that the 'key error \"creation_date\"' bit is indicating something unusual in your index metadata.\n. Yep:\n```\n\n\n\nfor idx in testinge.indices.keys():\n...     try:\n...         foo = testinge.indices[idx]['settings']['index']['creation_date']\n...     except:\n...         print('Index: {0} does not have a creation_date!'.format(idx))\n...\nIndex: kibana-int-demo1 does not have a creation_date!\nIndex: .marvel-kibana does not have a creation_date!\nIndex: kibana-int does not have a creation_date!\n```\n\n\n\nThis seems to indicate that these are old indices and do not have a creation_date.  If you're not still using them, you can quickly get around this issue by deleting them.  kibana-int and .marvel-kibana are for pre-2.x versions of ES.  Since I can see in your log output, Detected Elasticsearch version 2.3.2, these can probably be safely deleted, unless you're still trying to make Kibana 3 work with this version of ES.\nI will have to code a way to catch this, log it to death to ensure that the end user knows this is going on, and remove these indices from the actionable list (if creation_date is selected).\n. It pre-populates all of the values, yes.  I have a workaround that will skip these old messages with a warning. I will push a PR soon.\n. By the way, creation_date was added in Elasticsearch v1.4, so those are pretty old indices.\n. Glad to hear it, @leonhoffman \nThanks for the report!\n. #664 should address this for future users.\n. Oooh.  That's an unfortunate one.  I'll have a fix for you in a second.\n. Please see if you can add the line patched in #666 ( \ud83d\ude06 ) to your existing install and see if that covers the problem.\n. I am planning on re-bundling those separately in the future.  If scripting against the API is undesirable, I recommend installing the 3.x branch in a virtualenv if you still need it now:\n(sudo) pip install virtualenv\nOnce this is installed:\n1. mkdir curator-3.5.1\n2. cd curator-3.5.1\n3. virtualenv venv\n4. source venv/bin/activate.  Your command-line prompt will be prepended with (venv) after this, to show you are in a virtual env.\n5. pip install elasticsearch-curator==3.5.1\n6. Test your work: curator --version\ncurator, version 3.5.1\nAny time you need this, you can cd curator-3.5.1 and source venv/bin/activate, and curator 3.5.1 will be available in that directory.\nYou're really creating new repositories that frequently?\n. path.repo must be set in your elasticsearch.yml on each master and/or data node.\nAfter you've set this (and the user running Elasticsearch must have read/write privileges for that file system/path) and restarted Elasticsearch, then you should be able to create the repository with that command.\n. Since this is pretty clear-cut, I'm going to mark this as closed.  If you believe there needs to be more discussion, feel free to re-open the issue.\n. If I understand correctly, you're trying to remove indices from indices_needing_snapshot based on whether they are or are not in snapshots.snapshots?  And, based on your snapshot creation here, you're trying to create a snapshot per index?\n. I'm puzzled by the approach.  Why not snapshot all of the indices in one?  Retention?  Having multiple snapshots point to the same index does not duplicate data.  Snapshots work at the segment level, and any snapshot attempting to back up an already snapshotted segment will just point to it, so it's not duplicating data.  And when deleting snapshots, segments are only deleted when there are no more snapshots pointing to them.\n. Please read this comment from the past.  Curator used to do per-index snapshots, but it no longer does because it is inefficient to do so.  Please note, you can restore individual indices from any snapshot at any time.  It's super easy to do so.\n. I think that's worth pursuing. \nAs a rule, I advise using the --dry-run flag liberally when testing new configurations. The --dry-run flag is there so you can catch even legitimate configuration errors. I highly recommend it. \n. Duplicate of #668 \nFixed in #670 \n. I plan on releasing 4.0.1 today, if all goes well.\n. Thanks for reporting.  The feature is experimental, as it was listed as such in the 4.0.0 release notes.\nCan you please run with the loglevel set to DEBUG and attach the file here?\nOne thing I noticed is that you have use_ssl set to False, but if I recall correctly, it must be set to True, as the AWS signing is SSL.\n. Also, see: https://www.elastic.co/guide/en/elasticsearch/client/curator/current/configfile.html#aws_key\n\nYou must set your hosts to the proper hostname with port. It may not work setting port and hosts to only a host name due to the different connection module used.\n\nThis means your configuration file should look like:\n\n``` ---\nRemember, leave a key empty if there is no value.  None will be a string,\nnot a Python \"NoneType\"\nclient:\n  hosts:\n    - \"search-sl-es-cluster-xyz.ap-southeast-2.es.amazonaws.com:9200\"\n  port:\n```\n\nNote that port is empty.\n. The original issue that lead to this feature says that the port needs to be 443.  It also appears that I did not get the host and port formatting correct.\nThis suggests that the hosts entry in the config needs to be a single host, with no port assigned, the port entry needs to be 443, and kwargs['hosts'] = [{'host': kwargs['host'], 'port': 443}] needs to be inserted after line 530 in utils.py in the source.\nThis is one of the parts of the code I could not test properly, as I do not have access to AWS instances for Curator, hence it was marked Experimental.  I appreciate you helping to track it down.\nIf this doesn't work, then it suggests to me that I need more debug log collection because the TypeError: update() message seems to indicate that \nFile \"/usr/lib/python2.7/site-packages/curator/utils.py\", line 537, in get_client\n    kwargs['region'], 'es')\nis not getting the entire configuration line for some reason (though it's perfectly valid python):\nkwargs['http_auth'] = (\n                AWS4Auth(\n                    kwargs['aws_key'], kwargs['aws_secret_key'],\n                    kwargs['region'], 'es')\n            )\nIt may only be getting kwargs['region'], 'es') according to the error.  I am on holiday right now, and can only offer this much in a short break before heading back to the festivities.  I'll be back full time on Thursday, but may take more looks as time permits.\nThanks again!\n. It will still need the code modifications I suggested.  I will try to get you a patched version of utils.py when I get back so you can test it.\n. Sorry to keep everyone waiting.  I'm at OpenWest the rest of this week.  I've been putting together a whopping 4 presentations for it, which has kept me completely busy. \nIn the meanwhile, if someone wants to attempt some of the changes I outlined and submit a PR, or at least a branch to try, that would be terrific.\n. What's in this comment\nAnd thanks!\n. Try changing:\nkwargs['http_auth'] = (\n            AWS4Auth(\n                kwargs['aws_key'], kwargs['aws_secret_key'],\n                kwargs['region'], 'es')\n        )\nto:\nkwargs['http_auth'] = (AWS4Auth(kwargs['aws_key'], kwargs['aws_secret_key'],kwargs['region'], 'es'))\nIt appeared that one error was showing an incomplete line.\n. er, make that:\nkwargs['http_auth'] = AWS4Auth(kwargs['aws_key'], kwargs['aws_secret_key'],kwargs['region'], 'es')\n. What's the rest of the error?\n. Hmmm.  If you open a python console, would you please try:\nfrom requests_aws4auth import AWS4Auth\nAWS4Auth(kwargs['aws_key'], kwargs['aws_secret_key'],kwargs['region'], 'es')\nBut substitute your values where the kwargs go.  Please respond with sanitized output (I don't want you to put your sensitive keys here).  I'm curious what the output is.\n. Looking at the requests_aws4auth PyPI page, it may be that the requests module needs to be imported:\nimport requests\nabove the from requests_aws4auth import AWS4Auth section\n. Now, in utils.py, add import requests above from requests_aws4auth import AWS4Auth and see if it's any different.\n. This implies that the fault is upstream. Either I'm lacking some data that needs to be sent, or something is off in the requests area. I'll do more digging as I get time. \nCan you use Curator on the inside of your AWS cluster, rather than from remote?\n. That's correct. The AWS option is for connecting from outside of AWS. An internal instance of Curator will work by connecting directly to ES. \n. But you don't need an instance of Curator per node.   If you do that, be sure to set master_only to True, so it only runs on the elected master node. Otherwise it only needs a single client connection to manage the whole cluster\n. It only needs access to the client port, so within the VPC is fine \n. No, it seems there is something fundamentally missing or wrong with this experimental feature.  I will be trying to get it to work still, but I don't yet have the cycles to fully troubleshoot it.\n. Fixed by #722, presumably.\n. It seems you are still using Curator v3.x if you are looking for an --is-alias flag.  Curator is now at version 4.0.1 and has a new configuration syntax, and has an option that should do what you want: https://www.elastic.co/guide/en/elasticsearch/client/curator/4.0/close.html\nPlease see if the delete_aliases option covers your use case.\n. @ynux do you have any feedback here?  I'm going to close this and consider it moot for the \"close indices\" use-case if I don't hear a response soon.\n. > I wouldn't call it \"aliases will be deleted from indices\" but \"index will be removed from aliases\".\nWhile technically accurate, too many new users (and even old ones) do not understand the distinction.  I applaud you for understanding it so well.\nHowever, it's important to note that it's also accurate this way as aliases is a key in the index metadata, with the aliases that the given key/index is associated with as the value.  In that light,  \"deleting aliases from indices\" still works, and since that's what it looks like after it's done, it's rather moot how it's described, so long as users know that the index and alias will be disassociated after the action.\nI look forward to hearing whether the delete_aliases option works for your use case.\n. Unfortunately, I can't add a feature that I can't test myself.  This is further complicated by the words, \"only partially supported.\"  That makes it difficult to schedule time to work out coding a way to make this work, only to find that it might not, or worse, doesn't work.\nI've added the adopt_me label in case someone wants to take a stab at this.  Until it's officially supported by requests-aws4auth, I don't see myself adding it.\n. It would have to be in addition, otherwise it would break existing functionality. \nAlso, someone else will have to add it, test it, and submit a PR. I haven't the means to test it myself. . Thanks for catching this. I'm on vacation until Thursday. I can help with tests then, if you don't figure out a way before then. \n. @sherzberg, sorry for the long wait.  After vacation I was caught up in preparing 4 presentations for OpenWest.  I'm on the clock again.\nI put a sample test together in https://github.com/untergeek/curator/tree/test/682\nParticularly, the tests are here, and the test variables are here.\nWithout the new line you proposed, the test_snapshot_ignore_empty_list test fails.  That's the most important one.\nFeel free to take it all and use it however you'd like.  Once you submit a PR, I'll merge it and will likely be ready to release 4.0.2 immediately afterwards.  Also, feel free to add yourself to the CONTRIBUTORS file.\nThanks for catching this and submitting your findings!\n. By the way, be sure to rebase your current branch against master.  I've merged several PRs since this started.\n. @sherzberg Because I need to move forward with a new release, I'm going to merge in my test branch if I don't hear from you soon.\n. @sherzberg I added you to CONTRIBUTORS.  Thanks for catching and reporting bugs!\nhttps://github.com/elastic/curator/pull/699/commits/15f836eb49921f5da5a287da9c2c0bd6feb1c1fa\n. How did you install Curator?  Via pip?  By package?  Because the error here is talking about a python problem, not an Elasticsearch problem.  I know it looks like an Elasticsearch problem, but that elasticsearch>=2.3.0,<5.1.0 is actually referring to the elasticsearch python module, not your version of Elasticsearch.\nThe fix is well described in the FAQ for Curator: https://www.elastic.co/guide/en/elasticsearch/client/curator/3.5/entrypoint-fix.html\nYou need to upgrade setuptools for Python to fix this, then it will start working again. \n. That command-line syntax will not work with 4.  You will need to read up on the 4.0 docs: https://www.elastic.co/guide/en/elasticsearch/client/curator/4.0/index.html\nAnd you will still need to upgrade setuptools for your Python before it will work.  The instructions for that are at https://www.elastic.co/guide/en/elasticsearch/client/curator/current/entrypoint-fix.html\n. While I can reduce the log level and make more information DEBUG level, part of the drive to make more at level INFO is that there are plans to put the logs into Elasticsearch in a future version.  I feel like I would be taking a step backwards there.  I can probably make the not actionable bits DEBUG level. I wish there were something other than INFO and DEBUG, as there isn't anything in between, and above that is WARNING:\n| Level | Numeric value |\n| --- | --- |\n| CRITICAL | 50 |\n| ERROR | 40 |\n| WARNING | 30 |\n| INFO | 20 |\n| DEBUG | 10 |\n| NOTSET | 0 |\n. This isn't a regression.  Curator 4.0 only supports ES versions 2.x and up. See Versioning in the README.  It seems you're using the API and creating your own client connection, as the get_client method supplied by Curator would have detected this and disallowed it.\nThe longer reason is that there is a ton of new functionality that is simply not supported by older versions of Elasticsearch.  Much of this new functionality depends on using the cluster metadata calls, which are disallowed by AWS ES.  \nCurator 3.x persists for those who are compelled, for whatever reason, to stay in that space.  Trying to straddle major versions, and document & instruct users which actions and filters do or do not work based on Elasticsearch versions is a support nightmare.  As a result, a decision was made to cut off support for older versions of ES with the newer version of Curator.\nIf you're using AWS ES, you'll need to use the Curator 3.x branch.  You can install this via pip with pip install -U elasticsearch-curator==3.5.1\n. @flybd5 When I released Curator 4, it was completely incompatible with AWS ES because they had not yet released their version of Elasticsearch 2.x.  I'm sorry you've had a bad experience, but it's not my fault that AWS ES doesn't support the API calls necessary for Curator to work.  That's 100% on Amazon for releasing an incomplete version of Elasticsearch.  Please see the conversations in #717 regarding this, including the links to AWS forum posts on the subject, and the request for help from a core developer of the AWS ES product.\nI'll see about adding a blurb to the README.\n. > What is the point to have aws key and secret options in the config if Curator is not compatible with AWS ES at all?\nThey were added before Curator 4 was released, as part of some feature adding being done by others.  Having no way to properly mock the calls, or to do proper integration testing, I merged pull requests submitted by others after they reported a functional outcome.  The get_client method was carried over from 3.x with little modification, so that is partly why those options persist, even if they've been renamed in places.\n\nyou decided for some reason to make optional API required and you refuse to support the way Curator 3 worked?\n\nThe /_cluster/state/metadata endpoint is not optional for Curator 4.  Curator 4 is a major version release, and in many ways is a full API rewrite.  So many of the things that Curator 3 did were suboptimal, and frequently repeated calls that didn't need to be repeated, if data was stored once.  As stated in a previous comment on this ticket, when Curator 4 was being developed, there was no such thing as AWS ES 2.x, so I had no way of knowing that Amazon would release a 2.x version, nor that if they did that it still would not support the metadata endpoint.  I developed for the unaltered, open-source releases of Elasticsearch, with the 2.x APIs and 5.x APIs in mind.  Another important note is that earlier versions of Curator 3 did not support AWS ES 1.x either, for the same reason.  A user submitted a method that would pull some necessary details using the _cat API, and thereby make Curator work (for that call) with AWS ES.  #717 has some interesting details here, including one of the AWS ES developers reaching out to ask how they can make their version compatible with Curator.  Another developer tried to hack a way to make it work, but found that there were no substitute calls for the metadata one, which also covers index state (open/closed) and routing information, both of which are considered essential for Curator.\nThe IndexList object in Curator 4 pulls the full list of indices, and all of their associated metadata at object instantiation time.  It escapes the loops and frequent API calls that were in Curator 3.x.  To suggest that I'm refusing to support the way Curator 3 worked is to miss the mark.  It was, and is, a rewrite for optimization and to support newer API calls, rather than a deliberate maneuver to hurt 3.x or AWS ES users. \nPlease also remember that I wrote the documentation before AWS ES 2.x was released.  This has clearly resulted in some frustration for you, and I'm sorry for that.  I will alter the docs for all references to IAM credentials to point out that AWS ES is not supported, or perhaps remove them altogether from the documentation.\n. Curator 4.2.3 is the current release. It re-introduced 3.x style singletons via the curator_cli. Check the current documentation for more information. \nIn either case, you will need to make some adjustments as the filter syntax for the new singleton actions is very different from the many flags in 3.x. See https://www.elastic.co/guide/en/elasticsearch/client/curator/current/singleton-cli.html. If AWS doesn't open the cluster state metadata endpoint, it still won't be supported by Curator when/if they release a 5.x version . Somewhat similarly, yes. Run curator_cli --help and curator_cli snapshot --help for more information. Specifically, the --filter_list flag replaces --prefix, --time-unit, --newer-than, --timestring, etc. \nYou'll need to create a filter list that is a JSON representation of the YAML filters block from the new configuration style. There are some simple examples in the online documentation. You can test your --filter_list with curator_cli show_indices --filter_list ... to ensure it works as expected. . Please open a new issue or issues for these findings.\n-- Aaron Mildenstein\nOn November 24, 2016 at 6:39:32 PM, trompx (notifications@github.com) wrote:\n\nThanks, I was already converting my scripts and managed to make my first\nbackup with curator 4.2.3.post1\ncurator_cli $ES_OPTS snapshot --repository $S3_REPOSITORY --name\nmy-nice-name-hourly-%Y%m%d%H%M%S --filter_list\n'{\"filtertype\":\"pattern\",\"kind\":\"prefix\",\"value\":\"myindice\"}'\nFirst with curator 3.x, the snapshot backup on the s3 repository had the\nsame name specified in command line with --prefix. With new version I have\nnames like meta-0UH9cMZsR1Df5v68IaDPEQ.dat. Is it possible to still get the\nnice formatted name as specified in --name my-nice-name-hourly-%Y%m%d%H%M%S\n?\nSecond, doing the first backup, when I did a show_snapshots right after,\nI got the following output:\nmy-nice-name-hourly-20161125002650\nmy-nice-name-hourly-20161125002650\nIs it normal that it lists the snapshot twice?\nThen I indexed some more data, did another snapshot and show_snapshots\nagain, I got:\nmy-nice-name-hourly-20161125010047\nmy-nice-name-hourly-20161125010047\nWhile a new .dat file has been added to the s3 repository, the\nshow_snapshots show no trace of the previous backups...\nThe whole point (at least in my case) of the backup is to be able to\nrestore it, however the restore singleton is not available. Will it be\nimplemented anytime soon? Otherwise I will need to look for other solutions\n:(\nAnyway, thank you very much for making thoses singletons available!\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/elastic/curator/issues/685#issuecomment-262864530,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AA-R31aznVeVjJO7A-FaagyasinomxRpks5rBjxUgaJpZM4JIOkn\n.\n. Restore works great in regular Curator 4 (YAML configuration file). There are tons of options to configure, so I haven't made a restore singleton. If someone wants to add it, I'll merge. Curator 3 never had restore due to its complexity. . Snapshot to s3 is not the same as using Amazon AWS ES (i.e. their hosted Elasticsearch offering). S3 repositories are available to any and all ES installations. \n\nThe metadata endpoint is only needed with Amazon's own AWS ES hosted service.  . The singletons do use the metadata endpoint, by the way. . A pull request would be welcome, if you'd like to add it.\n. It's pretty much:\n| Curator Version | ES 1.x | ES 2.x | ES 5.x |\n| --- | --- | --- | --- |\n| 3 | yes | yes | no |\n| 4 | no | yes | yes |\n. Filter settings do not persist beyond the boundaries of the filter configuration they reside in.  As I look at the data provided (thanks!) it's clear that everything is as it should be.\nHere are the creation_dates of the indices in question\nIndex \"logstash-2016.28\" age (1468317737), which is Tue, 12 Jul 2016 10:02:17 GMT\nIndex \"logstash-2016.27\" age (1467712937), which is Tue, 05 Jul 2016 10:02:17 GMT\nIndex \"logstash-2016.26\" age (1467108137), which is Tue, 28 Jun 2016 10:02:17 GMT\nIndex \"logstash-2016.25\" age (1466503337), which is Tue, 21 Jun 2016 10:02:17 GMT\nIndex \"logstash-2016.24\" age (1465898537), which is Tue, 14 Jun 2016 10:02:17 GMT\nIndex \"logstash-2016.23\" age (1465293737), which is Tue, 07 Jun 2016 10:02:17 GMT\nLet's look at your snapshot action, where you are including indices younger than 2 weeks old:\n- filtertype: age\n      source: creation_date\n      direction: younger\n      unit: weeks\n      unit_count: 2\n      exclude:\nand excluding indices younger than 1 week old:\n- filtertype: age\n      source: creation_date\n      direction: younger\n      unit: weeks\n      unit_count: 1\n      exclude: True\nHere's a truncated view of the logs you provided.  The first age filter that includes indices younger than 2 weeks old does the following:\n2016-07-11 11:45:29,203 INFO      Remains in actionable list: Index \"logstash-2016.27\" age (1467712937), direction: \"younger\", point of reference, (1467020729)\n2016-07-11 11:45:29,203 INFO      Remains in actionable list: Index \"logstash-2016.26\" age (1467108137), direction: \"younger\", point of reference, (1467020729)\n2016-07-11 11:45:29,203 INFO      Removed from actionable list: Index \"logstash-2016.25\" age (1466503337), direction: \"younger\", point of reference, (1467020729)\n2016-07-11 11:45:29,203 INFO      Removed from actionable list: Index \"logstash-2016.24\" age (1465898537), direction: \"younger\", point of reference, (1467020729)\n2016-07-11 11:45:29,203 INFO      Removed from actionable list: Index \"logstash-2016.23\" age (1465293737), direction: \"younger\", point of reference, (1467020729)\n2016-07-11 11:45:29,203 INFO      Remains in actionable list: Index \"logstash-2016.28\" age (1468317737), direction: \"younger\", point of reference, (1467020729)\nThe point of reference 1467020729, is Mon, 27 Jun 2016 09:45:29 GMT\n2016 weeks 23, 24 and 25 are not included in the actionable list because they're not younger than the point of reference, which is 2 weeks ago.  This leaves weeks 26, 27, and 28.\nThe next filter is going to exclude indices younger than 1 week ago.  Let's see what happens here:\n2016-07-11 11:45:29,203 INFO      Removed from actionable list: Index \"logstash-2016.27\" age (1467712937), direction: \"younger\", point of reference, (1467625529)\n2016-07-11 11:45:29,204 INFO      Remains in actionable list: Index \"logstash-2016.26\" age (1467108137), direction: \"younger\", point of reference, (1467625529)\n2016-07-11 11:45:29,204 INFO      Removed from actionable list: Index \"logstash-2016.28\" age (1468317737), direction: \"younger\", point of reference, (1467625529)\nThe point of reference, 1467625529 is Mon, 04 Jul 2016 09:45:29 GMT.  Remember that your filter is set to exclude indices younger than this date, which is 1 week ago (from the time of execution).\n1468317737 is Tue, 12 Jul 2016 10:02:17 GMT\n1467712937 is Tue, 05 Jul 2016 10:02:17 GMT\nYounger than the point of reference.\n1467108137 is Tue, 28 Jun 2016 10:02:17 GMT\nNot younger than the point of reference, so it remains in the actionable list.  \nSo far as your configuration and provided logs are telling me, Curator is doing exactly what you've configured it to do.\n. You're right, I'm sorry.  I didn't catch what you were saying until now.\nThe code shouldn't allow for filter bleed through.  See https://github.com/elastic/curator/blob/master/curator/indexlist.py#L704\nAs you can see, on each iteration of the list of filters, it resets the filter args.  There is more output in debug logging.  Could you please run in debug and attach that file?\n. I think I might need to use copy.deepcopy() here, as in other places...\n. The debug output will confirm my suspicions.\nYou can work around this right now by not trusting the default on exclude, and manually putting True or False.\n. If that works, please let me know, and I'll patch it.\n. Thanks for adding it!\n. > I read today that ES doesn't reccomand using same repo for long time since the recovery will take longer than without snapshots.\nPlease let me know where you read this.  I would like to see this recommendation in print, as it is not something we commonly recommend.  \nRecovery only takes a long time when there are hundreds of thousands of segments.  This can easily be avoided by using a 2, or perhaps 3 tiered approach.  Frequent repository rollover should not be necessary if this is handled correctly.  In fact, it can result in a lot of extra data storage cost, as data may be duplicated in multiple locations.\nWhen using time-series data, we do recommend having a \"fast moving\" snapshot repository.  This would have hourly or daily snapshots, but only persist them for a few days.  After your time-series indices rollover, they're unlikely to be changed, so you can do a \"forceMerge,\" which is an optimize call in Curator 3.  It is recommended to forceMerge to 1 or 2 segments per shard, where feasible.  For snapshotting force merged segments, a separate repository would be recommended, because it only contains segments unlikely to change, so incremental segment creep wouldn't happen.\n. @panda87 can you please provide an update here?\n. The only ES employees in that thread are Igor and myself, and the dates are two years ago.  The Elasticsearch google user group was deprecated some time ago, and migrated to https://discuss.elastic.co/c/elasticsearch, alongside forums for our other software.  That recommendation addresses a limited use case:\nWhat Igor said:\n\nI would recommend pruning old snapshots as time goes by or starting snapshots into a new bucket/directory if you really need to maintain 2 hour resolution for 2 months old snapshots.\n\nThis is still true if you \"really need to maintain 2 hour resolution for 2 month old snapshots.\"  This is not common for any kind of time-series data (which I presume you're using, as you're using Curator for snapshots).  If you're using time-series data, or you do not need 2 hour resolution for 2 month old snapshots, there's a better way.\nThe slowness you're trying to avoid with lots of snapshots is actually a function of segment count.  \nMy comment in that referenced thread:\n\nSnapshots are at the segment level.  The more segments stored in the repository, the more segments will have to be compared to those in each successive snapshot.  With merges taking place continually in an active index, you may end up with a considerable number of \"orphaned\" segments stored in your repository, i.e. segments \"backed up,\" but no longer directly correlating to a segment in your index.  Checking through these may be contributing to the increased amount of time between snapshots.  \nConsider pruning older snapshots.  \"Orphaned\" segments will be deleted, and any segments still referenced will be preserved.\n\nA way to visualize what's constantly happening in indices with the Lucene segments is here, as demonstrated by one of our core Lucene committers.\nA snapshot:\n1. Prevents segment merges from happening until complete\n2. Copies all segments as they were at snapshot initiation time to the shared file store\n3. Associates the snapshot with the segments via pointers and metadata\nThe pointers point to the segments necessary to restore to a given point in time.  This becomes problematic when snapshots are taken at frequent intervals, or on time-series data.  Unchanged segments will not be re-copied, but with time-series indices, or frequently changing indices, these segments are constantly being updated by segment merges of new/changed data (as seen in the above video).  This means that there can be segments copied which might contain duplicated data because the former segments have been merged one or more times, and are therefore considered a new segment (the \"old\" segments which reference the same data are the \"orphans\" I'm talking about in my comment in that old thread).  A single, daily index can easily have over 300 segments, before a forceMerge.\nThe reason snapshots get slower over time is that each new segment to be snapshotted is compared to the other segments already stored in the repository.  With more segments come more tests.  This does not scale.  Adding repositories does solve this problem by reducing the number of segments in a single repository, but it should only be used, as Igor said, \"if you really need to maintain 2 hour resolution for 2 months old snapshots.\"  Elasticsearch still needs to maintain a list of all repositories and all snapshots in them, so this greatly pads the amount of metadata that Elasticsearch has to track.\nThe best way to reduce segment creep with time-series indices is to use a tiered repository approach in combination with forceMerge.  For example:\n- Tier 1 (hourly snapshots).\n- Tier 2 (daily snapshots). \n- Tier 3 (weekly snapshots) optional\nEach tier has its own repository.  The thinking is that Tier 1 only needs to persist for 48 to 72 hours (2 to 3 days).  By this time, a daily index rotation will have occurred, and you can forceMerge (optimize, in older versions of the API) segments per shard down to a small number, or even 1.  At this point, these optimized indices are ready to be snapshotted into tier 2.  This results in fewer segments to be compared \nUsing Curator, you are able to perform all steps of this tiered approach.  It has the beauty of keeping a limited number of short-term snapshots in case a restore is immediately necessary, but a much larger number of long-term snapshots, after they've been forceMerged, so that the number of segments in the repository\u2014which is the cause of the slowness you're trying to avoid\u2014is a non-issue.\n. @panda87 if this answers your question, please let me know, or close the ticket.\n. Thanks!\n. Thanks for updating the docs!\n. I appreciate the suggestion, but to me it makes no sense to add a Watch index to a Kibana-specific filter.  In any case, preventing any index starting in .watch from being acted on is as simple as adding another filter block:\n- filtertype: pattern\n  kind: prefix\n  value: .watch\n  exclude: True\nIt wouldn't be too hard to add another filtertype for watches, but that seems a lot of work to replicate the above functionality for a single index, or index pattern. The Kibana filtertype exists because it existed in previous versions (and as such, it would be expected), and there are multiple, dissimilar indices which are used by Kibana/Marvel, so this prevents the user from having to add multiple filter blocks to catch all of those.\n. It sounds like you have some other package which depends on a specific version of python-urllib3 + RHEL's backported security fixes.  Unfortunately, I can't force Curator to use plugins from the official RHEL repository.  \nI am obliged to ship updated plugins for whatever Curator depends on in the Elasticsearch repository.  This is for security reasons.  If there are collisions, you'll have to do some manual work to install using other repositories.  \nThe reason you're getting conflicts is that the Elasticsearch repository has newer versions of the plugins available, and yum/rpm wants to use the newest by default.  Curator should work with the older versions, but I can't put old versions of the plugins in the repository just to satisfy RHEL/CentOS.\nMy recommendation is to remove the Elasticsearch Curator repository and use the default repository.  If there are dependencies not met, install each manually until Curator works.\nThis is why I recommend using pip instead of packages in the official documentation.  It saves a lot of headache in this vein.\n. I've never tried this, but I do not think it would be possible without string replacement.  What's the use case that you're using environment variables?  Docker, or some other container-style?\n. As an aside, I will be re-adding some of the CLI functionality from 3.x in the future.  It will not be able to do all of the things that the YAML-based configuration can do, but much of it, plus a few other command-line only things (like show).\n. I recommend writing a wrapper for now.  It should be fairly easy to do string replacement, even if it's done every time it's called (e.g. source file -> string replacement -> run against target).\n. Thanks!\n. Yeah, I caught this a few minutes before you reported it during release testing.  I've fixed it and 4.0.3 is already out on pip.\n. It was a MANIFEST.in error.\n. No problem.  Sorry for the trouble.  I caught it immediately when doing a pip sanity test after pushing the new version there.  That was me doing a PyPI release test, as it passed all of the regular Travis CI testing already.  It was a packaging error that Travis did not catch.  I'll have to see if I can get that figured out for the future.\nI'm actually surprised you caught it in the few minutes it was there after my test, but that's on me for not marking it unavailable on PyPI sooner.\n. With this quoted text, I can't see if the YAML configuration indents are done properly.\nIt could be that your description says indices older than one day but your filter block says younger.\n. Here's an example of one of my snapshot configurations:\n8:\n    action: snapshot\n    description: >-\n      Snapshot daily indices older than 1 day (based on index\n      creation_date) with the default snapshot name pattern of\n      'daily-%Y%m%d%H%M%S'.  Wait for the snapshot to complete.  Do not skip\n      the repository filesystem access check.  Use the other options to create\n      the snapshot.\n    options:\n      ignore_empty_list: True\n      repository: Untergeek\n      # Leaving name blank will result in the default 'curator-%Y%m%d%H%M%S'\n      name: 'daily-%Y%m%d%H%M%S'\n      ignore_unavailable: False\n      include_global_state: True\n      partial: False\n      wait_for_completion: True\n      skip_repo_fs_check: False\n      timeout_override: 7200\n      continue_if_exception: False\n      disable_action: False\n    filters:\n    - filtertype: pattern\n      kind: timestring\n      value: '%Y.%m.%d'\n      exclude: False\n    - filtertype: age\n      source: creation_date\n      direction: older\n      unit: days\n      unit_count: 1\n      exclude:\n    - filtertype: closed\n. Please attach DEBUG level logs for further analysis.  I just completed two tests of the exact action I posted from my Windows test machine using v4.0.3: once with DEBUG logging, and once in INFO.  It worked both times (and the snapshot was practically instantaneous the second time, since the segments didn't change much).  The INFO level output looks like this:\n2016-07-25 18:08:17,461 INFO      Action #8: snapshot\n2016-07-25 18:08:17,739 INFO      Creating snapshot \"daily-20160726000817\" from indices: [u'.marvel-es-1-2016.07.20', u'logstash-2016.07.12', u'logstash-2016.07.13', u'.marvel-es-1-2016.07.19', u'logstash-2016.07.15', u'logstash-2016.07.16', u'.marvel-es-1-2016.07.21', u'logstash-2016.07.18', u'logstash-2016.07.19', u'.marvel-es-1-2016.07.23', u'.marvel-es-1-2016.07.24', u'logstash-2016.07.14', u'logstash-2016.07.20', u'topbeat-2016.07.13', u'topbeat-2016.07.12', u'topbeat-2016.07.15', u'topbeat-2016.07.17', u'topbeat-2016.07.16', u'topbeat-2016.07.19', u'topbeat-2016.07.18', u'logstash-2016.07.17', u'logstash-2016.07.25', u'logstash-2016.07.24', u'.marvel-es-1-2016.07.22', u'logstash-2016.07.21', u'.marvel-es-1-2016.07.25', u'logstash-2016.07.23', u'logstash-2016.07.22', u'topbeat-2016.07.24', u'topbeat-2016.07.25', u'topbeat-2016.07.20', u'topbeat-2016.07.21', u'topbeat-2016.07.22', u'topbeat-2016.07.23', u'topbeat-2016.07.14']\n2016-07-25 18:08:19,578 INFO      Snapshot daily-20160726000817 successfully completed.\n2016-07-25 18:08:19,578 INFO      Action #8: completed\n2016-07-25 18:08:19,578 INFO      Job completed.\nI note that your logs include the elasticsearch and urllib3 log lines.  This should not be happening unless you have set blacklist: [] in your client configuration file (this is a brand new setting in 4.0.3, and it is not widely known yet, so I'd be surprised if you did this).  This suggests to me that you may not be running v4.0.3, or that the code between versions is mixed.\nAre you running the curator.exe file?  Did you install from pip?  Did you use the MSI installer?  There is a known issue with the MSI installer needing old versions to be uninstalled before new ones are installed, otherwise there will be 2 versions installed at once.\n. @vidhyarti Please respond with the files and answers requested.  I am unable to duplicate this behavior on Windows with the Elastic-provided binary.\n. I just ran your config (against my repository):\n\u00bb curator --config ~/.curator/test.yml test.yml\n2016-07-26 11:02:00,167 DEBUG                curator.cli                    cli:165  default_timeout = 30\n2016-07-26 11:02:00,173 DEBUG                curator.cli                    cli:170  Full list of actions: {1: {'action': 'snapshot', 'description': \"Snapshot logstash- prefixed indices older than 1 day (based on index creation_date) with the default snapshot name pattern of 'curator-%Y%m%d%H%M%S'.  Wait for the snapshot to complete.  Do not skip the repository filesystem access check.  Use the other options to create the snapshot.\", 'filters': [{'direction': 'younger', 'stats_result': None, 'source': 'creation_date', 'filtertype': 'age', 'field': None, 'epoch': None, 'exclude': None, 'unit_count': 1, 'unit': 'days'}], 'options': {'ignore_unavailable': False, 'partial': False, 'name': 'shan', 'repository': 'Untergeek', 'continue_if_exception': False, 'skip_repo_fs_check': False, 'timeout_override': None, 'disable_action': False, 'include_global_state': True, 'wait_for_completion': True}}}\n2016-07-26 11:02:00,173 INFO                 curator.cli                    cli:177  Action #1: snapshot\n2016-07-26 11:02:00,173 DEBUG                curator.cli                    cli:190  continue_if_exception = False\n2016-07-26 11:02:00,173 DEBUG                curator.cli                    cli:191  timeout_override = None\n2016-07-26 11:02:00,173 DEBUG              curator.utils             get_client:503  kwargs = {'aws_secret_key': None, 'url_prefix': '', 'http_auth': None, 'certificate': None, 'aws_key': None, 'aws_region': None, 'client_cert': None, 'hosts': ['172.19.73.13'], 'timeout': 30, 'use_ssl': False, 'master_only': False, 'port': 9200, 'ssl_no_validate': False, 'client_key': None}\n2016-07-26 11:02:00,173 DEBUG              curator.utils             get_client:548  Not using \"requests_aws4auth\" python module to connect.\n2016-07-26 11:02:00,177 DEBUG              curator.utils          check_version:412  Detected Elasticsearch version 2.3.3\n2016-07-26 11:02:00,177 DEBUG                curator.cli                    cli:216  client is <class 'elasticsearch.client.Elasticsearch'>\n2016-07-26 11:02:00,177 DEBUG                curator.cli                    cli:222  TRY: actions: {'action': 'snapshot', 'description': \"Snapshot logstash- prefixed indices older than 1 day (based on index creation_date) with the default snapshot name pattern of 'curator-%Y%m%d%H%M%S'.  Wait for the snapshot to complete.  Do not skip the repository filesystem access check.  Use the other options to create the snapshot.\", 'filters': [{'direction': 'younger', 'stats_result': None, 'source': 'creation_date', 'filtertype': 'age', 'field': None, 'epoch': None, 'exclude': None, 'unit_count': 1, 'unit': 'days'}], 'options': {'ignore_unavailable': False, 'partial': False, 'name': 'shan', 'repository': 'Untergeek', 'skip_repo_fs_check': False, 'include_global_state': True, 'wait_for_completion': True}} kwargs: {'master_timeout': 30, 'timeout': 30, 'dry_run': False}\n2016-07-26 11:02:00,177 DEBUG                curator.cli         process_action:46   Configuration dictionary: {'action': 'snapshot', 'description': \"Snapshot logstash- prefixed indices older than 1 day (based on index creation_date) with the default snapshot name pattern of 'curator-%Y%m%d%H%M%S'.  Wait for the snapshot to complete.  Do not skip the repository filesystem access check.  Use the other options to create the snapshot.\", 'filters': [{'direction': 'younger', 'stats_result': None, 'source': 'creation_date', 'filtertype': 'age', 'field': None, 'epoch': None, 'exclude': None, 'unit_count': 1, 'unit': 'days'}], 'options': {'ignore_unavailable': False, 'partial': False, 'name': 'shan', 'repository': 'Untergeek', 'skip_repo_fs_check': False, 'include_global_state': True, 'wait_for_completion': True}}\n2016-07-26 11:02:00,177 DEBUG                curator.cli         process_action:47   kwargs: {'master_timeout': 30, 'dry_run': False, 'timeout': 30}\n2016-07-26 11:02:00,177 DEBUG                curator.cli         process_action:50   opts: {'ignore_unavailable': False, 'partial': False, 'name': 'shan', 'repository': 'Untergeek', 'skip_repo_fs_check': False, 'include_global_state': True, 'wait_for_completion': True}\n2016-07-26 11:02:00,178 DEBUG                curator.cli         process_action:68   MYKWARGS = {'ignore_unavailable': False, 'partial': False, 'name': 'curator-%Y%m%d%H%M%S', 'repository': None, 'skip_repo_fs_check': False, 'include_global_state': True, 'wait_for_completion': True}\n2016-07-26 11:02:00,178 DEBUG                curator.cli         process_action:72   Action kwargs: {'ignore_unavailable': False, 'partial': False, 'name': 'shan', 'repository': 'Untergeek', 'skip_repo_fs_check': False, 'include_global_state': True, 'wait_for_completion': True}\n2016-07-26 11:02:00,178 DEBUG              curator.utils            verify_args:1018 Arguments for action \"snapshot\": {'ignore_unavailable': False, 'partial': False, 'name': 'shan', 'repository': 'Untergeek', 'skip_repo_fs_check': False, 'include_global_state': True, 'wait_for_completion': True}\n2016-07-26 11:02:00,178 DEBUG              curator.utils           matches_keys:1022 options.keys = ['ignore_unavailable', 'partial', 'name', 'repository', 'skip_repo_fs_check', 'include_global_state', 'wait_for_completion'] mydict.keys = ['ignore_unavailable', 'partial', 'name', 'repository', 'skip_repo_fs_check', 'include_global_state', 'wait_for_completion']\n2016-07-26 11:02:00,178 DEBUG                curator.cli         process_action:102  Running \"SNAPSHOT\"\n2016-07-26 11:02:00,178 DEBUG          curator.indexlist          __get_indices:64   Getting all indices\n2016-07-26 11:02:00,187 DEBUG              curator.utils            get_indices:369  All indices: [u'.marvel-es-1-2016.07.20', u'logstash-2016.07.12', u'logstash-2016.07.13', u'logstash-2016.07.14', u'logstash-2016.07.15', u'logstash-2016.07.16', u'.marvel-es-1-2016.07.21', u'logstash-2016.07.18', u'logstash-2016.07.19', u'.marvel-es-1-2016.07.23', u'.kibana', u'.marvel-es-1-2016.07.24', u'logstash-2016.07.20', u'topbeat-2016.07.13', u'topbeat-2016.07.12', u'topbeat-2016.07.15', u'.marvel-es-1-2016.07.26', u'topbeat-2016.07.17', u'topbeat-2016.07.16', u'topbeat-2016.07.19', u'topbeat-2016.07.18', u'logstash-2016.07.17', u'squid-2016.07', u'squid-2016.06', u'squid-2016.05', u'logstash-2016.07.25', u'logstash-2016.07.24', u'.marvel-es-1-2016.07.22', u'logstash-2016.07.26', u'logstash-2016.07.21', u'.marvel-es-1-2016.07.25', u'logstash-2016.07.23', u'logstash-2016.07.22', u'nxfilter-2016.06', u'nxfilter-2016.07', u'topbeat-2016.07.24', u'topbeat-2016.07.25', u'topbeat-2016.07.26', u'topbeat-2016.07.20', u'topbeat-2016.07.21', u'topbeat-2016.07.22', u'topbeat-2016.07.23', u'.marvel-es-data-1', u'topbeat-2016.07.14', u'packetbeat-2016.06', u'packetbeat-2016.07', u'packetbeat-2016.04', u'packetbeat-2016.05', u'packetbeat-2016.03', u'packetbeat-2016.01']\n2016-07-26 11:02:00,187 DEBUG          curator.indexlist       empty_list_check:178  Checking for empty list\n2016-07-26 11:02:00,187 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,187 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,187 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,187 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,187 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,187 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,187 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,187 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,188 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,188 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,188 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,188 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,188 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,188 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,188 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,188 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,188 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,188 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,188 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,188 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,188 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,188 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,188 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,189 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,189 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,189 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,189 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,189 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,189 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,189 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,189 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,189 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,189 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,189 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,189 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,189 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,189 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,189 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,190 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,190 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,190 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,190 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,190 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,190 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,190 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,190 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,190 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,190 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,190 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,190 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:02:00,190 DEBUG          curator.indexlist          _get_metadata:141  Getting index metadata\n2016-07-26 11:02:00,190 DEBUG          curator.indexlist       empty_list_check:178  Checking for empty list\n2016-07-26 11:02:00,746 DEBUG          curator.indexlist       _get_index_stats:109  Getting index stats\n2016-07-26 11:02:00,746 DEBUG          curator.indexlist       empty_list_check:178  Checking for empty list\n2016-07-26 11:02:00,746 DEBUG          curator.indexlist           working_list:189  Generating working list of indices\n2016-07-26 11:02:00,746 DEBUG          curator.indexlist           working_list:189  Generating working list of indices\n2016-07-26 11:02:00,760 DEBUG          curator.indexlist     iterate_over_stats:118  Index: .marvel-es-1-2016.07.20  Size: 625.4MB  Docs: 1073808\n2016-07-26 11:02:00,761 DEBUG          curator.indexlist     iterate_over_stats:118  Index: logstash-2016.07.13  Size: 782.0MB  Docs: 5868498\n2016-07-26 11:02:00,761 DEBUG          curator.indexlist     iterate_over_stats:118  Index: logstash-2016.07.14  Size: 781.4MB  Docs: 5867804\n2016-07-26 11:02:00,761 DEBUG          curator.indexlist     iterate_over_stats:118  Index: logstash-2016.07.15  Size: 780.1MB  Docs: 5866694\n2016-07-26 11:02:00,761 DEBUG          curator.indexlist     iterate_over_stats:118  Index: logstash-2016.07.16  Size: 772.7MB  Docs: 5865100\n2016-07-26 11:02:00,761 DEBUG          curator.indexlist     iterate_over_stats:118  Index: logstash-2016.07.17  Size: 766.9MB  Docs: 5864406\n2016-07-26 11:02:00,761 DEBUG          curator.indexlist     iterate_over_stats:118  Index: logstash-2016.07.18  Size: 770.4MB  Docs: 5865226\n2016-07-26 11:02:00,761 DEBUG          curator.indexlist     iterate_over_stats:118  Index: logstash-2016.07.19  Size: 769.6MB  Docs: 5865392\n2016-07-26 11:02:00,761 DEBUG          curator.indexlist     iterate_over_stats:118  Index: .marvel-es-1-2016.07.21  Size: 638.2MB  Docs: 1092220\n2016-07-26 11:02:00,761 DEBUG          curator.indexlist     iterate_over_stats:118  Index: .marvel-es-1-2016.07.23  Size: 619.1MB  Docs: 1061902\n2016-07-26 11:02:00,761 DEBUG          curator.indexlist     iterate_over_stats:118  Index: .kibana  Size: 249.8KB  Docs: 114\n2016-07-26 11:02:00,761 DEBUG          curator.indexlist     iterate_over_stats:118  Index: .marvel-es-1-2016.07.24  Size: 615.0MB  Docs: 1071450\n2016-07-26 11:02:00,761 DEBUG          curator.indexlist     iterate_over_stats:118  Index: .marvel-es-1-2016.07.25  Size: 616.1MB  Docs: 1072454\n2016-07-26 11:02:00,762 DEBUG          curator.indexlist     iterate_over_stats:118  Index: .marvel-es-1-2016.07.26  Size: 450.1MB  Docs: 769836\n2016-07-26 11:02:00,762 DEBUG          curator.indexlist     iterate_over_stats:118  Index: topbeat-2016.07.13  Size: 460.3MB  Docs: 1457302\n2016-07-26 11:02:00,762 DEBUG          curator.indexlist     iterate_over_stats:118  Index: topbeat-2016.07.15  Size: 452.5MB  Docs: 1434886\n2016-07-26 11:02:00,762 DEBUG          curator.indexlist     iterate_over_stats:118  Index: topbeat-2016.07.14  Size: 449.5MB  Docs: 1435384\n2016-07-26 11:02:00,762 DEBUG          curator.indexlist     iterate_over_stats:118  Index: topbeat-2016.07.17  Size: 452.1MB  Docs: 1435102\n2016-07-26 11:02:00,762 DEBUG          curator.indexlist     iterate_over_stats:118  Index: topbeat-2016.07.16  Size: 452.5MB  Docs: 1435326\n2016-07-26 11:02:00,762 DEBUG          curator.indexlist     iterate_over_stats:118  Index: topbeat-2016.07.19  Size: 464.6MB  Docs: 1464818\n2016-07-26 11:02:00,762 DEBUG          curator.indexlist     iterate_over_stats:118  Index: topbeat-2016.07.18  Size: 458.3MB  Docs: 1448342\n2016-07-26 11:02:00,762 DEBUG          curator.indexlist     iterate_over_stats:118  Index: squid-2016.07  Size: 210.1MB  Docs: 674788\n2016-07-26 11:02:00,762 DEBUG          curator.indexlist     iterate_over_stats:118  Index: squid-2016.06  Size: 1007.8MB  Docs: 4045432\n2016-07-26 11:02:00,762 DEBUG          curator.indexlist     iterate_over_stats:118  Index: squid-2016.05  Size: 153.7MB  Docs: 845050\n2016-07-26 11:02:00,762 DEBUG          curator.indexlist     iterate_over_stats:118  Index: logstash-2016.07.25  Size: 788.8MB  Docs: 5854906\n2016-07-26 11:02:00,762 DEBUG          curator.indexlist     iterate_over_stats:118  Index: logstash-2016.07.24  Size: 791.2MB  Docs: 5865156\n2016-07-26 11:02:00,763 DEBUG          curator.indexlist     iterate_over_stats:118  Index: .marvel-es-1-2016.07.22  Size: 639.7MB  Docs: 1092084\n2016-07-26 11:02:00,763 DEBUG          curator.indexlist     iterate_over_stats:118  Index: logstash-2016.07.26  Size: 557.8MB  Docs: 4170652\n2016-07-26 11:02:00,763 DEBUG          curator.indexlist     iterate_over_stats:118  Index: logstash-2016.07.21  Size: 781.9MB  Docs: 5865306\n2016-07-26 11:02:00,763 DEBUG          curator.indexlist     iterate_over_stats:118  Index: logstash-2016.07.20  Size: 771.5MB  Docs: 5866468\n2016-07-26 11:02:00,763 DEBUG          curator.indexlist     iterate_over_stats:118  Index: logstash-2016.07.23  Size: 781.6MB  Docs: 5864324\n2016-07-26 11:02:00,763 DEBUG          curator.indexlist     iterate_over_stats:118  Index: logstash-2016.07.22  Size: 783.9MB  Docs: 5866724\n2016-07-26 11:02:00,763 DEBUG          curator.indexlist     iterate_over_stats:118  Index: nxfilter-2016.06  Size: 80.7MB  Docs: 725678\n2016-07-26 11:02:00,763 DEBUG          curator.indexlist     iterate_over_stats:118  Index: nxfilter-2016.07  Size: 156.3MB  Docs: 721248\n2016-07-26 11:02:00,763 DEBUG          curator.indexlist     iterate_over_stats:118  Index: topbeat-2016.07.24  Size: 644.6MB  Docs: 1557788\n2016-07-26 11:02:00,763 DEBUG          curator.indexlist     iterate_over_stats:118  Index: topbeat-2016.07.25  Size: 632.5MB  Docs: 1558610\n2016-07-26 11:02:00,763 DEBUG          curator.indexlist     iterate_over_stats:118  Index: topbeat-2016.07.26  Size: 438.6MB  Docs: 1109014\n2016-07-26 11:02:00,763 DEBUG          curator.indexlist     iterate_over_stats:118  Index: topbeat-2016.07.20  Size: 631.8MB  Docs: 1569118\n2016-07-26 11:02:00,763 DEBUG          curator.indexlist     iterate_over_stats:118  Index: topbeat-2016.07.21  Size: 656.9MB  Docs: 1588514\n2016-07-26 11:02:00,764 DEBUG          curator.indexlist     iterate_over_stats:118  Index: topbeat-2016.07.22  Size: 649.5MB  Docs: 1567138\n2016-07-26 11:02:00,764 DEBUG          curator.indexlist     iterate_over_stats:118  Index: topbeat-2016.07.23  Size: 655.2MB  Docs: 1580592\n2016-07-26 11:02:00,764 DEBUG          curator.indexlist     iterate_over_stats:118  Index: .marvel-es-data-1  Size: 4.5MB  Docs: 30\n2016-07-26 11:02:00,764 DEBUG          curator.indexlist     iterate_over_stats:118  Index: packetbeat-2016.06  Size: 516.1MB  Docs: 271240\n2016-07-26 11:02:00,764 DEBUG          curator.indexlist     iterate_over_stats:118  Index: packetbeat-2016.07  Size: 278.4MB  Docs: 166288\n2016-07-26 11:02:00,764 DEBUG          curator.indexlist     iterate_over_stats:118  Index: packetbeat-2016.04  Size: 290.9MB  Docs: 275570\n2016-07-26 11:02:00,764 DEBUG          curator.indexlist     iterate_over_stats:118  Index: packetbeat-2016.05  Size: 422.7MB  Docs: 270432\n2016-07-26 11:02:00,764 DEBUG          curator.indexlist     iterate_over_stats:118  Index: packetbeat-2016.03  Size: 424.9MB  Docs: 336976\n2016-07-26 11:02:00,764 DEBUG          curator.indexlist     iterate_over_stats:118  Index: packetbeat-2016.01  Size: 1.2GB  Docs: 193962\n2016-07-26 11:02:00,764 DEBUG          curator.indexlist        iterate_filters:678  Iterating over a list of filters\n2016-07-26 11:02:00,764 DEBUG          curator.indexlist        iterate_filters:684  All filters: [{'direction': 'younger', 'stats_result': None, 'source': 'creation_date', 'filtertype': 'age', 'field': None, 'epoch': None, 'exclude': None, 'unit_count': 1, 'unit': 'days'}]\n2016-07-26 11:02:00,764 DEBUG          curator.indexlist        iterate_filters:686  Top of the loop: [u'.marvel-es-1-2016.07.20', u'logstash-2016.07.12', u'logstash-2016.07.13', u'logstash-2016.07.14', u'logstash-2016.07.15', u'logstash-2016.07.16', u'.marvel-es-1-2016.07.21', u'logstash-2016.07.18', u'logstash-2016.07.19', u'.marvel-es-1-2016.07.23', u'.kibana', u'.marvel-es-1-2016.07.24', u'logstash-2016.07.20', u'topbeat-2016.07.13', u'topbeat-2016.07.12', u'topbeat-2016.07.15', u'.marvel-es-1-2016.07.26', u'topbeat-2016.07.17', u'topbeat-2016.07.16', u'topbeat-2016.07.19', u'topbeat-2016.07.18', u'logstash-2016.07.17', u'squid-2016.07', u'squid-2016.06', u'squid-2016.05', u'logstash-2016.07.25', u'logstash-2016.07.24', u'.marvel-es-1-2016.07.22', u'logstash-2016.07.26', u'logstash-2016.07.21', u'.marvel-es-1-2016.07.25', u'logstash-2016.07.23', u'logstash-2016.07.22', u'nxfilter-2016.06', u'nxfilter-2016.07', u'topbeat-2016.07.24', u'topbeat-2016.07.25', u'topbeat-2016.07.26', u'topbeat-2016.07.20', u'topbeat-2016.07.21', u'topbeat-2016.07.22', u'topbeat-2016.07.23', u'.marvel-es-data-1', u'topbeat-2016.07.14', u'packetbeat-2016.06', u'packetbeat-2016.07', u'packetbeat-2016.04', u'packetbeat-2016.05', u'packetbeat-2016.03', u'packetbeat-2016.01']\n2016-07-26 11:02:00,765 DEBUG              curator.utils        iterate_filters:687  Un-parsed filter args: {'direction': 'younger', 'stats_result': None, 'source': 'creation_date', 'filtertype': 'age', 'field': None, 'epoch': None, 'exclude': None, 'unit_count': 1, 'unit': 'days'}\n2016-07-26 11:02:00,765 DEBUG              curator.utils        iterate_filters:713  Filter args: {'direction': 'younger', 'stats_result': 'min_value', 'field': None, 'source': 'creation_date', 'epoch': None, 'timestring': None, 'exclude': False, 'unit_count': 1, 'unit': 'days'}\n2016-07-26 11:02:00,765 DEBUG              curator.utils        iterate_filters:714  Pre-instance: [u'.marvel-es-1-2016.07.20', u'logstash-2016.07.12', u'logstash-2016.07.13', u'logstash-2016.07.14', u'logstash-2016.07.15', u'logstash-2016.07.16', u'.marvel-es-1-2016.07.21', u'logstash-2016.07.18', u'logstash-2016.07.19', u'.marvel-es-1-2016.07.23', u'.kibana', u'.marvel-es-1-2016.07.24', u'logstash-2016.07.20', u'topbeat-2016.07.13', u'topbeat-2016.07.12', u'topbeat-2016.07.15', u'.marvel-es-1-2016.07.26', u'topbeat-2016.07.17', u'topbeat-2016.07.16', u'topbeat-2016.07.19', u'topbeat-2016.07.18', u'logstash-2016.07.17', u'squid-2016.07', u'squid-2016.06', u'squid-2016.05', u'logstash-2016.07.25', u'logstash-2016.07.24', u'.marvel-es-1-2016.07.22', u'logstash-2016.07.26', u'logstash-2016.07.21', u'.marvel-es-1-2016.07.25', u'logstash-2016.07.23', u'logstash-2016.07.22', u'nxfilter-2016.06', u'nxfilter-2016.07', u'topbeat-2016.07.24', u'topbeat-2016.07.25', u'topbeat-2016.07.26', u'topbeat-2016.07.20', u'topbeat-2016.07.21', u'topbeat-2016.07.22', u'topbeat-2016.07.23', u'.marvel-es-data-1', u'topbeat-2016.07.14', u'packetbeat-2016.06', u'packetbeat-2016.07', u'packetbeat-2016.04', u'packetbeat-2016.05', u'packetbeat-2016.03', u'packetbeat-2016.01']\n2016-07-26 11:02:00,765 DEBUG          curator.indexlist          filter_by_age:338  Filtering indices by age\n2016-07-26 11:02:00,765 DEBUG          curator.indexlist           working_list:189  Generating working list of indices\n2016-07-26 11:02:00,765 DEBUG          curator.indexlist       __not_actionable:38   Index .marvel-es-1-2016.07.20 is not actionable, removing from list.\n2016-07-26 11:02:00,765 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \".marvel-es-1-2016.07.20\" age (1468972802), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,765 DEBUG          curator.indexlist       __not_actionable:38   Index logstash-2016.07.12 is not actionable, removing from list.\n2016-07-26 11:02:00,765 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"logstash-2016.07.12\" age (1468281605), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,765 DEBUG          curator.indexlist       __not_actionable:38   Index logstash-2016.07.13 is not actionable, removing from list.\n2016-07-26 11:02:00,765 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"logstash-2016.07.13\" age (1468368005), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,765 DEBUG          curator.indexlist       __not_actionable:38   Index logstash-2016.07.14 is not actionable, removing from list.\n2016-07-26 11:02:00,766 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"logstash-2016.07.14\" age (1468454405), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,766 DEBUG          curator.indexlist       __not_actionable:38   Index logstash-2016.07.15 is not actionable, removing from list.\n2016-07-26 11:02:00,766 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"logstash-2016.07.15\" age (1468540805), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,766 DEBUG          curator.indexlist       __not_actionable:38   Index logstash-2016.07.16 is not actionable, removing from list.\n2016-07-26 11:02:00,766 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"logstash-2016.07.16\" age (1468627205), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,766 DEBUG          curator.indexlist       __not_actionable:38   Index .marvel-es-1-2016.07.21 is not actionable, removing from list.\n2016-07-26 11:02:00,766 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \".marvel-es-1-2016.07.21\" age (1469059201), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,766 DEBUG          curator.indexlist       __not_actionable:38   Index logstash-2016.07.18 is not actionable, removing from list.\n2016-07-26 11:02:00,766 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"logstash-2016.07.18\" age (1468800003), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,766 DEBUG          curator.indexlist       __not_actionable:38   Index logstash-2016.07.19 is not actionable, removing from list.\n2016-07-26 11:02:00,766 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"logstash-2016.07.19\" age (1468886403), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,766 DEBUG          curator.indexlist       __not_actionable:38   Index .marvel-es-1-2016.07.23 is not actionable, removing from list.\n2016-07-26 11:02:00,766 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \".marvel-es-1-2016.07.23\" age (1469232000), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,766 DEBUG          curator.indexlist       __not_actionable:38   Index .kibana is not actionable, removing from list.\n2016-07-26 11:02:00,766 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \".kibana\" age (1424476118), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,767 DEBUG          curator.indexlist       __not_actionable:38   Index .marvel-es-1-2016.07.24 is not actionable, removing from list.\n2016-07-26 11:02:00,767 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \".marvel-es-1-2016.07.24\" age (1469318400), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,767 DEBUG          curator.indexlist       __not_actionable:38   Index logstash-2016.07.20 is not actionable, removing from list.\n2016-07-26 11:02:00,767 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"logstash-2016.07.20\" age (1468972803), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,767 DEBUG          curator.indexlist       __not_actionable:38   Index topbeat-2016.07.13 is not actionable, removing from list.\n2016-07-26 11:02:00,767 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"topbeat-2016.07.13\" age (1468368007), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,767 DEBUG          curator.indexlist       __not_actionable:38   Index topbeat-2016.07.12 is not actionable, removing from list.\n2016-07-26 11:02:00,767 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"topbeat-2016.07.12\" age (1468281607), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,767 DEBUG          curator.indexlist       __not_actionable:38   Index topbeat-2016.07.15 is not actionable, removing from list.\n2016-07-26 11:02:00,767 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"topbeat-2016.07.15\" age (1468540808), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,767 DEBUG          curator.indexlist           __actionable:34   Index .marvel-es-1-2016.07.26 is actionable and remains in the list.\n2016-07-26 11:02:00,767 DEBUG          curator.indexlist            __excludify:57   Remains in actionable list: Index \".marvel-es-1-2016.07.26\" age (1469491202), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,767 DEBUG          curator.indexlist       __not_actionable:38   Index topbeat-2016.07.17 is not actionable, removing from list.\n2016-07-26 11:02:00,768 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"topbeat-2016.07.17\" age (1468713608), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,768 DEBUG          curator.indexlist       __not_actionable:38   Index topbeat-2016.07.16 is not actionable, removing from list.\n2016-07-26 11:02:00,768 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"topbeat-2016.07.16\" age (1468627208), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,768 DEBUG          curator.indexlist       __not_actionable:38   Index topbeat-2016.07.19 is not actionable, removing from list.\n2016-07-26 11:02:00,768 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"topbeat-2016.07.19\" age (1468886408), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,768 DEBUG          curator.indexlist       __not_actionable:38   Index topbeat-2016.07.18 is not actionable, removing from list.\n2016-07-26 11:02:00,768 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"topbeat-2016.07.18\" age (1468800008), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,768 DEBUG          curator.indexlist       __not_actionable:38   Index logstash-2016.07.17 is not actionable, removing from list.\n2016-07-26 11:02:00,768 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"logstash-2016.07.17\" age (1468713603), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,768 DEBUG          curator.indexlist       __not_actionable:38   Index squid-2016.07 is not actionable, removing from list.\n2016-07-26 11:02:00,768 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"squid-2016.07\" age (1467331205), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,768 DEBUG          curator.indexlist       __not_actionable:38   Index squid-2016.06 is not actionable, removing from list.\n2016-07-26 11:02:00,768 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"squid-2016.06\" age (1464739202), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,768 DEBUG          curator.indexlist       __not_actionable:38   Index squid-2016.05 is not actionable, removing from list.\n2016-07-26 11:02:00,768 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"squid-2016.05\" age (1463693537), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,769 DEBUG          curator.indexlist       __not_actionable:38   Index logstash-2016.07.25 is not actionable, removing from list.\n2016-07-26 11:02:00,769 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"logstash-2016.07.25\" age (1469404805), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,769 DEBUG          curator.indexlist       __not_actionable:38   Index logstash-2016.07.24 is not actionable, removing from list.\n2016-07-26 11:02:00,769 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"logstash-2016.07.24\" age (1469318405), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,769 DEBUG          curator.indexlist       __not_actionable:38   Index .marvel-es-1-2016.07.22 is not actionable, removing from list.\n2016-07-26 11:02:00,769 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \".marvel-es-1-2016.07.22\" age (1469145602), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,769 DEBUG          curator.indexlist           __actionable:34   Index logstash-2016.07.26 is actionable and remains in the list.\n2016-07-26 11:02:00,769 DEBUG          curator.indexlist            __excludify:57   Remains in actionable list: Index \"logstash-2016.07.26\" age (1469491205), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,769 DEBUG          curator.indexlist       __not_actionable:38   Index logstash-2016.07.21 is not actionable, removing from list.\n2016-07-26 11:02:00,769 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"logstash-2016.07.21\" age (1469059205), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,769 DEBUG          curator.indexlist       __not_actionable:38   Index .marvel-es-1-2016.07.25 is not actionable, removing from list.\n2016-07-26 11:02:00,769 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \".marvel-es-1-2016.07.25\" age (1469404800), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,769 DEBUG          curator.indexlist       __not_actionable:38   Index logstash-2016.07.23 is not actionable, removing from list.\n2016-07-26 11:02:00,769 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"logstash-2016.07.23\" age (1469232005), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,770 DEBUG          curator.indexlist       __not_actionable:38   Index logstash-2016.07.22 is not actionable, removing from list.\n2016-07-26 11:02:00,770 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"logstash-2016.07.22\" age (1469145605), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,770 DEBUG          curator.indexlist       __not_actionable:38   Index nxfilter-2016.06 is not actionable, removing from list.\n2016-07-26 11:02:00,770 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"nxfilter-2016.06\" age (1464899541), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,770 DEBUG          curator.indexlist       __not_actionable:38   Index nxfilter-2016.07 is not actionable, removing from list.\n2016-07-26 11:02:00,770 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"nxfilter-2016.07\" age (1467331203), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,770 DEBUG          curator.indexlist       __not_actionable:38   Index topbeat-2016.07.24 is not actionable, removing from list.\n2016-07-26 11:02:00,770 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"topbeat-2016.07.24\" age (1469318408), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,770 DEBUG          curator.indexlist       __not_actionable:38   Index topbeat-2016.07.25 is not actionable, removing from list.\n2016-07-26 11:02:00,770 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"topbeat-2016.07.25\" age (1469404807), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,770 DEBUG          curator.indexlist           __actionable:34   Index topbeat-2016.07.26 is actionable and remains in the list.\n2016-07-26 11:02:00,770 DEBUG          curator.indexlist            __excludify:57   Remains in actionable list: Index \"topbeat-2016.07.26\" age (1469491208), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,770 DEBUG          curator.indexlist       __not_actionable:38   Index topbeat-2016.07.20 is not actionable, removing from list.\n2016-07-26 11:02:00,770 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"topbeat-2016.07.20\" age (1468972807), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,771 DEBUG          curator.indexlist       __not_actionable:38   Index topbeat-2016.07.21 is not actionable, removing from list.\n2016-07-26 11:02:00,771 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"topbeat-2016.07.21\" age (1469059206), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,771 DEBUG          curator.indexlist       __not_actionable:38   Index topbeat-2016.07.22 is not actionable, removing from list.\n2016-07-26 11:02:00,771 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"topbeat-2016.07.22\" age (1469145608), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,771 DEBUG          curator.indexlist       __not_actionable:38   Index topbeat-2016.07.23 is not actionable, removing from list.\n2016-07-26 11:02:00,771 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"topbeat-2016.07.23\" age (1469232007), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,771 DEBUG          curator.indexlist       __not_actionable:38   Index .marvel-es-data-1 is not actionable, removing from list.\n2016-07-26 11:02:00,771 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \".marvel-es-data-1\" age (1464979408), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,771 DEBUG          curator.indexlist       __not_actionable:38   Index topbeat-2016.07.14 is not actionable, removing from list.\n2016-07-26 11:02:00,771 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"topbeat-2016.07.14\" age (1468454407), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,771 DEBUG          curator.indexlist       __not_actionable:38   Index packetbeat-2016.06 is not actionable, removing from list.\n2016-07-26 11:02:00,771 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"packetbeat-2016.06\" age (1464739226), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,771 DEBUG          curator.indexlist       __not_actionable:38   Index packetbeat-2016.07 is not actionable, removing from list.\n2016-07-26 11:02:00,771 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"packetbeat-2016.07\" age (1467331208), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,772 DEBUG          curator.indexlist       __not_actionable:38   Index packetbeat-2016.04 is not actionable, removing from list.\n2016-07-26 11:02:00,772 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"packetbeat-2016.04\" age (1459468824), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,772 DEBUG          curator.indexlist       __not_actionable:38   Index packetbeat-2016.05 is not actionable, removing from list.\n2016-07-26 11:02:00,772 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"packetbeat-2016.05\" age (1462060888), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,772 DEBUG          curator.indexlist       __not_actionable:38   Index packetbeat-2016.03 is not actionable, removing from list.\n2016-07-26 11:02:00,772 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"packetbeat-2016.03\" age (1456790407), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,772 DEBUG          curator.indexlist       __not_actionable:38   Index packetbeat-2016.01 is not actionable, removing from list.\n2016-07-26 11:02:00,772 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"packetbeat-2016.01\" age (1452209738), direction: \"younger\", point of reference, (1469466120)\n2016-07-26 11:02:00,772 DEBUG              curator.utils        iterate_filters:716  Post-instance: [u'.marvel-es-1-2016.07.26', u'logstash-2016.07.26', u'topbeat-2016.07.26']\n2016-07-26 11:02:00,774 DEBUG              curator.utils      repository_exists:911  Repository Untergeek exists.\n2016-07-26 11:02:00,774 DEBUG              curator.utils     parse_date_pattern:995  Partially rendered name: s\n2016-07-26 11:02:00,774 DEBUG              curator.utils     parse_date_pattern:995  Partially rendered name: sh\n2016-07-26 11:02:00,774 DEBUG              curator.utils     parse_date_pattern:995  Partially rendered name: sha\n2016-07-26 11:02:00,774 DEBUG              curator.utils     parse_date_pattern:995  Partially rendered name: shan\n2016-07-26 11:02:00,774 DEBUG              curator.utils     parse_date_pattern:997  Fully rendered name: shan\n2016-07-26 11:02:00,774 DEBUG                curator.cli         process_action:110  Doing the action here.\n2016-07-26 11:02:00,774 DEBUG          curator.indexlist       empty_list_check:178  Checking for empty list\n2016-07-26 11:02:00,846 DEBUG              curator.utils           test_repo_fs:927  All nodes can write to the repository\n2016-07-26 11:02:00,846 DEBUG              curator.utils           test_repo_fs:929  Nodes with verified repository access: {u'HD9y66SKRleRoj82_eK2bA': {u'name': u'MacBuntu1'}, u'7DVvG6djQFuYxxK-4sIdHQ': {u'name': u'Elasticbox'}, u'-LDbj7sGRmC7zdirMA82rA': {u'name': u'MacBuntu2'}, u'TKH6UI_VQk6GnMk5DE3mbQ': {u'name': u'esmaster'}}\n2016-07-26 11:02:00,848 INFO      curator.actions.snapshot              do_action:741  Creating snapshot \"shan\" from indices: [u'.marvel-es-1-2016.07.26', u'logstash-2016.07.26', u'topbeat-2016.07.26']\nThis was from Linux.  I will test against the MSI package when my windows box comes up.\n. This is the output from the Windows MSI package:\n2016-07-26 11:22:08,719 DEBUG                curator.cli                    cli:165  default_timeout = 30\n2016-07-26 11:22:08,726 DEBUG                curator.cli                    cli:170  Full list of actions: {1: {'action': 'snapshot', 'description': \"Snapshot logstash- prefixed indices older than 1 day (based on index creation_date) with the default snapshot name pattern of 'curator-%Y%m%d%H%M%S'.  Wait for the snapshot to complete.  Do not skip the repository filesystem access check.  Use the other options to create the snapshot.\", 'filters': [{'direction': 'younger', 'stats_result': None, 'source': 'creation_date', 'filtertype': 'age', 'field': None, 'epoch': None, 'exclude': None, 'unit_count': 1, 'unit': 'days'}], 'options': {'ignore_unavailable': False, 'partial': False, 'name': 'shan', 'repository': 'Untergeek', 'continue_if_exception': False, 'skip_repo_fs_check': False, 'timeout_override': 7200, 'disable_action': False, 'include_global_state': True, 'wait_for_completion': True}}}\n2016-07-26 11:22:08,726 INFO                 curator.cli                    cli:177  Action #1: snapshot\n2016-07-26 11:22:08,726 DEBUG                curator.cli                    cli:190  continue_if_exception = False\n2016-07-26 11:22:08,726 DEBUG                curator.cli                    cli:191  timeout_override = 7200\n2016-07-26 11:22:08,726 DEBUG              curator.utils             get_client:503  kwargs = {'aws_secret_key': None, 'url_prefix': '', 'http_auth': None, 'certificate': None, 'aws_key': None, 'aws_region': None, 'client_cert': None, 'hosts': ['172.19.73.13'], 'timeout': 7200, 'use_ssl': False, 'master_only': False, 'port': 9200, 'ssl_no_validate': False, 'client_key': None}\n2016-07-26 11:22:08,726 DEBUG              curator.utils             get_client:548  Not using \"requests_aws4auth\" python module to connect.\n2016-07-26 11:22:08,734 DEBUG              curator.utils          check_version:412  Detected Elasticsearch version 2.3.3\n2016-07-26 11:22:08,736 DEBUG                curator.cli                    cli:216  client is <class 'elasticsearch.client.Elasticsearch'>\n2016-07-26 11:22:08,736 DEBUG                curator.cli                    cli:222  TRY: actions: {'action': 'snapshot', 'description': \"Snapshot logstash- prefixed indices older than 1 day (based on index creation_date) with the default snapshot name pattern of 'curator-%Y%m%d%H%M%S'.  Wait for the snapshot to complete.  Do not skip the repository filesystem access check.  Use the other options to create the snapshot.\", 'filters': [{'direction': 'younger', 'stats_result': None, 'source': 'creation_date', 'filtertype': 'age', 'field': None, 'epoch': None, 'exclude': None, 'unit_count': 1, 'unit': 'days'}], 'options': {'ignore_unavailable': False, 'partial': False, 'name': 'shan', 'repository': 'Untergeek', 'skip_repo_fs_check': False, 'include_global_state': True, 'wait_for_completion': True}} kwargs: {'master_timeout': 300, 'timeout': 7200, 'dry_run': False}\n2016-07-26 11:22:08,736 DEBUG                curator.cli         process_action:46   Configuration dictionary: {'action': 'snapshot', 'description': \"Snapshot logstash- prefixed indices older than 1 day (based on index creation_date) with the default snapshot name pattern of 'curator-%Y%m%d%H%M%S'.  Wait for the snapshot to complete.  Do not skip the repository filesystem access check.  Use the other options to create the snapshot.\", 'filters': [{'direction': 'younger', 'stats_result': None, 'source': 'creation_date', 'filtertype': 'age', 'field': None, 'epoch': None, 'exclude': None, 'unit_count': 1, 'unit': 'days'}], 'options': {'ignore_unavailable': False, 'partial': False, 'name': 'shan', 'repository': 'Untergeek', 'skip_repo_fs_check': False, 'include_global_state': True, 'wait_for_completion': True}}\n2016-07-26 11:22:08,736 DEBUG                curator.cli         process_action:47   kwargs: {'master_timeout': 300, 'dry_run': False, 'timeout': 7200}\n2016-07-26 11:22:08,736 DEBUG                curator.cli         process_action:50   opts: {'ignore_unavailable': False, 'partial': False, 'name': 'shan', 'repository': 'Untergeek', 'skip_repo_fs_check': False, 'include_global_state': True, 'wait_for_completion': True}\n2016-07-26 11:22:08,736 DEBUG                curator.cli         process_action:68   MYKWARGS = {'ignore_unavailable': False, 'partial': False, 'name': 'curator-%Y%m%d%H%M%S', 'repository': None, 'skip_repo_fs_check': False, 'include_global_state': True, 'wait_for_completion': True}\n2016-07-26 11:22:08,736 DEBUG                curator.cli         process_action:72   Action kwargs: {'ignore_unavailable': False, 'partial': False, 'name': 'shan', 'repository': 'Untergeek', 'skip_repo_fs_check': False, 'include_global_state': True, 'wait_for_completion': True}\n2016-07-26 11:22:08,736 DEBUG              curator.utils            verify_args:1018 Arguments for action \"snapshot\": {'ignore_unavailable': False, 'partial': False, 'name': 'shan', 'repository': 'Untergeek', 'skip_repo_fs_check': False, 'include_global_state': True, 'wait_for_completion': True}\n2016-07-26 11:22:08,736 DEBUG              curator.utils           matches_keys:1022 options.keys = ['ignore_unavailable', 'partial', 'name', 'repository', 'skip_repo_fs_check', 'include_global_state', 'wait_for_completion'] mydict.keys = ['ignore_unavailable', 'partial', 'name', 'repository', 'skip_repo_fs_check', 'include_global_state', 'wait_for_completion']\n2016-07-26 11:22:08,736 DEBUG                curator.cli         process_action:102  Running \"SNAPSHOT\"\n2016-07-26 11:22:08,736 DEBUG          curator.indexlist          __get_indices:64   Getting all indices\n2016-07-26 11:22:08,749 DEBUG              curator.utils            get_indices:369  All indices: [u'.marvel-es-1-2016.07.20', u'logstash-2016.07.12', u'logstash-2016.07.13', u'logstash-2016.07.14', u'logstash-2016.07.15', u'logstash-2016.07.16', u'.marvel-es-1-2016.07.21', u'logstash-2016.07.18', u'logstash-2016.07.19', u'.marvel-es-1-2016.07.23', u'.kibana', u'.marvel-es-1-2016.07.24', u'logstash-2016.07.20', u'topbeat-2016.07.13', u'topbeat-2016.07.12', u'topbeat-2016.07.15', u'.marvel-es-1-2016.07.26', u'topbeat-2016.07.17', u'topbeat-2016.07.16', u'topbeat-2016.07.19', u'topbeat-2016.07.18', u'logstash-2016.07.17', u'squid-2016.07', u'squid-2016.06', u'squid-2016.05', u'logstash-2016.07.25', u'logstash-2016.07.24', u'.marvel-es-1-2016.07.22', u'logstash-2016.07.26', u'logstash-2016.07.21', u'.marvel-es-1-2016.07.25', u'logstash-2016.07.23', u'logstash-2016.07.22', u'nxfilter-2016.06', u'nxfilter-2016.07', u'topbeat-2016.07.24', u'topbeat-2016.07.25', u'topbeat-2016.07.26', u'topbeat-2016.07.20', u'topbeat-2016.07.21', u'topbeat-2016.07.22', u'topbeat-2016.07.23', u'.marvel-es-data-1', u'topbeat-2016.07.14', u'packetbeat-2016.06', u'packetbeat-2016.07', u'packetbeat-2016.04', u'packetbeat-2016.05', u'packetbeat-2016.03', u'packetbeat-2016.01']\n2016-07-26 11:22:08,749 DEBUG          curator.indexlist       empty_list_check:178  Checking for empty list\n2016-07-26 11:22:08,749 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,749 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,749 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,749 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,750 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,750 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,750 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,750 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,750 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,750 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,750 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,750 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,750 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,750 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,750 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,750 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,750 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,750 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,750 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,750 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,750 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,750 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,750 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,750 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,750 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,750 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,750 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,752 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,752 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,752 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,752 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,752 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,752 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,752 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,752 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,752 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,752 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,752 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,753 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,753 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,753 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,753 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,753 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,753 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,753 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,753 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,753 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,753 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,753 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,753 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 11:22:08,753 DEBUG          curator.indexlist          _get_metadata:141  Getting index metadata\n2016-07-26 11:22:08,753 DEBUG          curator.indexlist       empty_list_check:178  Checking for empty list\n2016-07-26 11:22:09,397 DEBUG          curator.indexlist       _get_index_stats:109  Getting index stats\n2016-07-26 11:22:09,397 DEBUG          curator.indexlist       empty_list_check:178  Checking for empty list\n2016-07-26 11:22:09,397 DEBUG          curator.indexlist           working_list:189  Generating working list of indices\n2016-07-26 11:22:09,397 DEBUG          curator.indexlist           working_list:189  Generating working list of indices\n2016-07-26 11:22:09,414 DEBUG          curator.indexlist     iterate_over_stats:118  Index: .marvel-es-1-2016.07.20  Size: 625.4MB  Docs: 1073808\n2016-07-26 11:22:09,414 DEBUG          curator.indexlist     iterate_over_stats:118  Index: logstash-2016.07.13  Size: 782.0MB  Docs: 5868498\n2016-07-26 11:22:09,414 DEBUG          curator.indexlist     iterate_over_stats:118  Index: logstash-2016.07.14  Size: 781.4MB  Docs: 5867804\n2016-07-26 11:22:09,414 DEBUG          curator.indexlist     iterate_over_stats:118  Index: logstash-2016.07.15  Size: 780.1MB  Docs: 5866694\n2016-07-26 11:22:09,414 DEBUG          curator.indexlist     iterate_over_stats:118  Index: logstash-2016.07.16  Size: 772.7MB  Docs: 5865100\n2016-07-26 11:22:09,414 DEBUG          curator.indexlist     iterate_over_stats:118  Index: logstash-2016.07.17  Size: 766.9MB  Docs: 5864406\n2016-07-26 11:22:09,414 DEBUG          curator.indexlist     iterate_over_stats:118  Index: logstash-2016.07.18  Size: 770.4MB  Docs: 5865226\n2016-07-26 11:22:09,414 DEBUG          curator.indexlist     iterate_over_stats:118  Index: logstash-2016.07.19  Size: 769.6MB  Docs: 5865392\n2016-07-26 11:22:09,414 DEBUG          curator.indexlist     iterate_over_stats:118  Index: .marvel-es-1-2016.07.21  Size: 638.2MB  Docs: 1092220\n2016-07-26 11:22:09,414 DEBUG          curator.indexlist     iterate_over_stats:118  Index: .marvel-es-1-2016.07.23  Size: 619.1MB  Docs: 1061902\n2016-07-26 11:22:09,414 DEBUG          curator.indexlist     iterate_over_stats:118  Index: .kibana  Size: 249.8KB  Docs: 114\n2016-07-26 11:22:09,415 DEBUG          curator.indexlist     iterate_over_stats:118  Index: .marvel-es-1-2016.07.24  Size: 615.0MB  Docs: 1071450\n2016-07-26 11:22:09,415 DEBUG          curator.indexlist     iterate_over_stats:118  Index: .marvel-es-1-2016.07.25  Size: 616.1MB  Docs: 1072454\n2016-07-26 11:22:09,415 DEBUG          curator.indexlist     iterate_over_stats:118  Index: .marvel-es-1-2016.07.26  Size: 458.5MB  Docs: 786982\n2016-07-26 11:22:09,415 DEBUG          curator.indexlist     iterate_over_stats:118  Index: topbeat-2016.07.13  Size: 460.3MB  Docs: 1457302\n2016-07-26 11:22:09,415 DEBUG          curator.indexlist     iterate_over_stats:118  Index: topbeat-2016.07.15  Size: 452.5MB  Docs: 1434886\n2016-07-26 11:22:09,415 DEBUG          curator.indexlist     iterate_over_stats:118  Index: topbeat-2016.07.14  Size: 449.5MB  Docs: 1435384\n2016-07-26 11:22:09,415 DEBUG          curator.indexlist     iterate_over_stats:118  Index: topbeat-2016.07.17  Size: 452.1MB  Docs: 1435102\n2016-07-26 11:22:09,415 DEBUG          curator.indexlist     iterate_over_stats:118  Index: topbeat-2016.07.16  Size: 452.5MB  Docs: 1435326\n2016-07-26 11:22:09,415 DEBUG          curator.indexlist     iterate_over_stats:118  Index: topbeat-2016.07.19  Size: 464.6MB  Docs: 1464818\n2016-07-26 11:22:09,417 DEBUG          curator.indexlist     iterate_over_stats:118  Index: topbeat-2016.07.18  Size: 458.3MB  Docs: 1448342\n2016-07-26 11:22:09,417 DEBUG          curator.indexlist     iterate_over_stats:118  Index: squid-2016.07  Size: 221.8MB  Docs: 676778\n2016-07-26 11:22:09,417 DEBUG          curator.indexlist     iterate_over_stats:118  Index: squid-2016.06  Size: 1007.8MB  Docs: 4045432\n2016-07-26 11:22:09,417 DEBUG          curator.indexlist     iterate_over_stats:118  Index: squid-2016.05  Size: 153.7MB  Docs: 845050\n2016-07-26 11:22:09,417 DEBUG          curator.indexlist     iterate_over_stats:118  Index: logstash-2016.07.25  Size: 788.8MB  Docs: 5854906\n2016-07-26 11:22:09,417 DEBUG          curator.indexlist     iterate_over_stats:118  Index: logstash-2016.07.24  Size: 791.2MB  Docs: 5865156\n2016-07-26 11:22:09,417 DEBUG          curator.indexlist     iterate_over_stats:118  Index: .marvel-es-1-2016.07.22  Size: 639.7MB  Docs: 1092084\n2016-07-26 11:22:09,417 DEBUG          curator.indexlist     iterate_over_stats:118  Index: logstash-2016.07.26  Size: 645.1MB  Docs: 4252144\n2016-07-26 11:22:09,417 DEBUG          curator.indexlist     iterate_over_stats:118  Index: logstash-2016.07.21  Size: 781.9MB  Docs: 5865306\n2016-07-26 11:22:09,417 DEBUG          curator.indexlist     iterate_over_stats:118  Index: logstash-2016.07.20  Size: 771.5MB  Docs: 5866468\n2016-07-26 11:22:09,417 DEBUG          curator.indexlist     iterate_over_stats:118  Index: logstash-2016.07.23  Size: 781.6MB  Docs: 5864324\n2016-07-26 11:22:09,417 DEBUG          curator.indexlist     iterate_over_stats:118  Index: logstash-2016.07.22  Size: 783.9MB  Docs: 5866724\n2016-07-26 11:22:09,417 DEBUG          curator.indexlist     iterate_over_stats:118  Index: nxfilter-2016.06  Size: 80.7MB  Docs: 725678\n2016-07-26 11:22:09,417 DEBUG          curator.indexlist     iterate_over_stats:118  Index: nxfilter-2016.07  Size: 156.4MB  Docs: 722068\n2016-07-26 11:22:09,417 DEBUG          curator.indexlist     iterate_over_stats:118  Index: topbeat-2016.07.24  Size: 644.6MB  Docs: 1557788\n2016-07-26 11:22:09,417 DEBUG          curator.indexlist     iterate_over_stats:118  Index: topbeat-2016.07.25  Size: 632.5MB  Docs: 1558610\n2016-07-26 11:22:09,417 DEBUG          curator.indexlist     iterate_over_stats:118  Index: topbeat-2016.07.26  Size: 467.2MB  Docs: 1131094\n2016-07-26 11:22:09,417 DEBUG          curator.indexlist     iterate_over_stats:118  Index: topbeat-2016.07.20  Size: 631.8MB  Docs: 1569118\n2016-07-26 11:22:09,417 DEBUG          curator.indexlist     iterate_over_stats:118  Index: topbeat-2016.07.21  Size: 656.9MB  Docs: 1588514\n2016-07-26 11:22:09,417 DEBUG          curator.indexlist     iterate_over_stats:118  Index: topbeat-2016.07.22  Size: 649.5MB  Docs: 1567138\n2016-07-26 11:22:09,418 DEBUG          curator.indexlist     iterate_over_stats:118  Index: topbeat-2016.07.23  Size: 655.2MB  Docs: 1580592\n2016-07-26 11:22:09,418 DEBUG          curator.indexlist     iterate_over_stats:118  Index: .marvel-es-data-1  Size: 4.5MB  Docs: 30\n2016-07-26 11:22:09,418 DEBUG          curator.indexlist     iterate_over_stats:118  Index: packetbeat-2016.06  Size: 516.1MB  Docs: 271240\n2016-07-26 11:22:09,418 DEBUG          curator.indexlist     iterate_over_stats:118  Index: packetbeat-2016.07  Size: 278.5MB  Docs: 166468\n2016-07-26 11:22:09,418 DEBUG          curator.indexlist     iterate_over_stats:118  Index: packetbeat-2016.04  Size: 290.9MB  Docs: 275570\n2016-07-26 11:22:09,418 DEBUG          curator.indexlist     iterate_over_stats:118  Index: packetbeat-2016.05  Size: 422.7MB  Docs: 270432\n2016-07-26 11:22:09,418 DEBUG          curator.indexlist     iterate_over_stats:118  Index: packetbeat-2016.03  Size: 424.9MB  Docs: 336976\n2016-07-26 11:22:09,418 DEBUG          curator.indexlist     iterate_over_stats:118  Index: packetbeat-2016.01  Size: 1.2GB  Docs: 193962\n2016-07-26 11:22:09,418 DEBUG          curator.indexlist        iterate_filters:678  Iterating over a list of filters\n2016-07-26 11:22:09,418 DEBUG          curator.indexlist        iterate_filters:684  All filters: [{'direction': 'younger', 'stats_result': None, 'source': 'creation_date', 'filtertype': 'age', 'field': None, 'epoch': None, 'exclude': None, 'unit_count': 1, 'unit': 'days'}]\n2016-07-26 11:22:09,420 DEBUG          curator.indexlist        iterate_filters:686  Top of the loop: [u'.marvel-es-1-2016.07.20', u'logstash-2016.07.12', u'logstash-2016.07.13', u'logstash-2016.07.14', u'logstash-2016.07.15', u'logstash-2016.07.16', u'.marvel-es-1-2016.07.21', u'logstash-2016.07.18', u'logstash-2016.07.19', u'.marvel-es-1-2016.07.23', u'.kibana', u'.marvel-es-1-2016.07.24', u'logstash-2016.07.20', u'topbeat-2016.07.13', u'topbeat-2016.07.12', u'topbeat-2016.07.15', u'.marvel-es-1-2016.07.26', u'topbeat-2016.07.17', u'topbeat-2016.07.16', u'topbeat-2016.07.19', u'topbeat-2016.07.18', u'logstash-2016.07.17', u'squid-2016.07', u'squid-2016.06', u'squid-2016.05', u'logstash-2016.07.25', u'logstash-2016.07.24', u'.marvel-es-1-2016.07.22', u'logstash-2016.07.26', u'logstash-2016.07.21', u'.marvel-es-1-2016.07.25', u'logstash-2016.07.23', u'logstash-2016.07.22', u'nxfilter-2016.06', u'nxfilter-2016.07', u'topbeat-2016.07.24', u'topbeat-2016.07.25', u'topbeat-2016.07.26', u'topbeat-2016.07.20', u'topbeat-2016.07.21', u'topbeat-2016.07.22', u'topbeat-2016.07.23', u'.marvel-es-data-1', u'topbeat-2016.07.14', u'packetbeat-2016.06', u'packetbeat-2016.07', u'packetbeat-2016.04', u'packetbeat-2016.05', u'packetbeat-2016.03', u'packetbeat-2016.01']\n2016-07-26 11:22:09,420 DEBUG              curator.utils        iterate_filters:687  Un-parsed filter args: {'direction': 'younger', 'stats_result': None, 'source': 'creation_date', 'filtertype': 'age', 'field': None, 'epoch': None, 'exclude': None, 'unit_count': 1, 'unit': 'days'}\n2016-07-26 11:22:09,420 DEBUG              curator.utils        iterate_filters:713  Filter args: {'direction': 'younger', 'stats_result': 'min_value', 'field': None, 'source': 'creation_date', 'epoch': None, 'timestring': None, 'exclude': False, 'unit_count': 1, 'unit': 'days'}\n2016-07-26 11:22:09,420 DEBUG              curator.utils        iterate_filters:714  Pre-instance: [u'.marvel-es-1-2016.07.20', u'logstash-2016.07.12', u'logstash-2016.07.13', u'logstash-2016.07.14', u'logstash-2016.07.15', u'logstash-2016.07.16', u'.marvel-es-1-2016.07.21', u'logstash-2016.07.18', u'logstash-2016.07.19', u'.marvel-es-1-2016.07.23', u'.kibana', u'.marvel-es-1-2016.07.24', u'logstash-2016.07.20', u'topbeat-2016.07.13', u'topbeat-2016.07.12', u'topbeat-2016.07.15', u'.marvel-es-1-2016.07.26', u'topbeat-2016.07.17', u'topbeat-2016.07.16', u'topbeat-2016.07.19', u'topbeat-2016.07.18', u'logstash-2016.07.17', u'squid-2016.07', u'squid-2016.06', u'squid-2016.05', u'logstash-2016.07.25', u'logstash-2016.07.24', u'.marvel-es-1-2016.07.22', u'logstash-2016.07.26', u'logstash-2016.07.21', u'.marvel-es-1-2016.07.25', u'logstash-2016.07.23', u'logstash-2016.07.22', u'nxfilter-2016.06', u'nxfilter-2016.07', u'topbeat-2016.07.24', u'topbeat-2016.07.25', u'topbeat-2016.07.26', u'topbeat-2016.07.20', u'topbeat-2016.07.21', u'topbeat-2016.07.22', u'topbeat-2016.07.23', u'.marvel-es-data-1', u'topbeat-2016.07.14', u'packetbeat-2016.06', u'packetbeat-2016.07', u'packetbeat-2016.04', u'packetbeat-2016.05', u'packetbeat-2016.03', u'packetbeat-2016.01']\n2016-07-26 11:22:09,420 DEBUG          curator.indexlist          filter_by_age:338  Filtering indices by age\n2016-07-26 11:22:09,420 DEBUG          curator.indexlist           working_list:189  Generating working list of indices\n2016-07-26 11:22:09,420 DEBUG          curator.indexlist       __not_actionable:38   Index .marvel-es-1-2016.07.20 is not actionable, removing from list.\n2016-07-26 11:22:09,420 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \".marvel-es-1-2016.07.20\" age (1468972802), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,420 DEBUG          curator.indexlist       __not_actionable:38   Index logstash-2016.07.12 is not actionable, removing from list.\n2016-07-26 11:22:09,420 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"logstash-2016.07.12\" age (1468281605), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,421 DEBUG          curator.indexlist       __not_actionable:38   Index logstash-2016.07.13 is not actionable, removing from list.\n2016-07-26 11:22:09,421 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"logstash-2016.07.13\" age (1468368005), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,421 DEBUG          curator.indexlist       __not_actionable:38   Index logstash-2016.07.14 is not actionable, removing from list.\n2016-07-26 11:22:09,421 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"logstash-2016.07.14\" age (1468454405), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,421 DEBUG          curator.indexlist       __not_actionable:38   Index logstash-2016.07.15 is not actionable, removing from list.\n2016-07-26 11:22:09,421 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"logstash-2016.07.15\" age (1468540805), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,421 DEBUG          curator.indexlist       __not_actionable:38   Index logstash-2016.07.16 is not actionable, removing from list.\n2016-07-26 11:22:09,421 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"logstash-2016.07.16\" age (1468627205), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,421 DEBUG          curator.indexlist       __not_actionable:38   Index .marvel-es-1-2016.07.21 is not actionable, removing from list.\n2016-07-26 11:22:09,421 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \".marvel-es-1-2016.07.21\" age (1469059201), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,421 DEBUG          curator.indexlist       __not_actionable:38   Index logstash-2016.07.18 is not actionable, removing from list.\n2016-07-26 11:22:09,421 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"logstash-2016.07.18\" age (1468800003), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,421 DEBUG          curator.indexlist       __not_actionable:38   Index logstash-2016.07.19 is not actionable, removing from list.\n2016-07-26 11:22:09,421 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"logstash-2016.07.19\" age (1468886403), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,421 DEBUG          curator.indexlist       __not_actionable:38   Index .marvel-es-1-2016.07.23 is not actionable, removing from list.\n2016-07-26 11:22:09,421 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \".marvel-es-1-2016.07.23\" age (1469232000), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,421 DEBUG          curator.indexlist       __not_actionable:38   Index .kibana is not actionable, removing from list.\n2016-07-26 11:22:09,421 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \".kibana\" age (1424476118), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,421 DEBUG          curator.indexlist       __not_actionable:38   Index .marvel-es-1-2016.07.24 is not actionable, removing from list.\n2016-07-26 11:22:09,421 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \".marvel-es-1-2016.07.24\" age (1469318400), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,421 DEBUG          curator.indexlist       __not_actionable:38   Index logstash-2016.07.20 is not actionable, removing from list.\n2016-07-26 11:22:09,421 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"logstash-2016.07.20\" age (1468972803), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,423 DEBUG          curator.indexlist       __not_actionable:38   Index topbeat-2016.07.13 is not actionable, removing from list.\n2016-07-26 11:22:09,423 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"topbeat-2016.07.13\" age (1468368007), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,423 DEBUG          curator.indexlist       __not_actionable:38   Index topbeat-2016.07.12 is not actionable, removing from list.\n2016-07-26 11:22:09,423 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"topbeat-2016.07.12\" age (1468281607), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,423 DEBUG          curator.indexlist       __not_actionable:38   Index topbeat-2016.07.15 is not actionable, removing from list.\n2016-07-26 11:22:09,423 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"topbeat-2016.07.15\" age (1468540808), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,423 DEBUG          curator.indexlist           __actionable:34   Index .marvel-es-1-2016.07.26 is actionable and remains in the list.\n2016-07-26 11:22:09,423 DEBUG          curator.indexlist            __excludify:57   Remains in actionable list: Index \".marvel-es-1-2016.07.26\" age (1469491202), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,423 DEBUG          curator.indexlist       __not_actionable:38   Index topbeat-2016.07.17 is not actionable, removing from list.\n2016-07-26 11:22:09,423 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"topbeat-2016.07.17\" age (1468713608), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,423 DEBUG          curator.indexlist       __not_actionable:38   Index topbeat-2016.07.16 is not actionable, removing from list.\n2016-07-26 11:22:09,424 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"topbeat-2016.07.16\" age (1468627208), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,424 DEBUG          curator.indexlist       __not_actionable:38   Index topbeat-2016.07.19 is not actionable, removing from list.\n2016-07-26 11:22:09,424 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"topbeat-2016.07.19\" age (1468886408), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,424 DEBUG          curator.indexlist       __not_actionable:38   Index topbeat-2016.07.18 is not actionable, removing from list.\n2016-07-26 11:22:09,424 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"topbeat-2016.07.18\" age (1468800008), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,424 DEBUG          curator.indexlist       __not_actionable:38   Index logstash-2016.07.17 is not actionable, removing from list.\n2016-07-26 11:22:09,424 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"logstash-2016.07.17\" age (1468713603), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,424 DEBUG          curator.indexlist       __not_actionable:38   Index squid-2016.07 is not actionable, removing from list.\n2016-07-26 11:22:09,424 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"squid-2016.07\" age (1467331205), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,424 DEBUG          curator.indexlist       __not_actionable:38   Index squid-2016.06 is not actionable, removing from list.\n2016-07-26 11:22:09,424 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"squid-2016.06\" age (1464739202), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,424 DEBUG          curator.indexlist       __not_actionable:38   Index squid-2016.05 is not actionable, removing from list.\n2016-07-26 11:22:09,424 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"squid-2016.05\" age (1463693537), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,424 DEBUG          curator.indexlist       __not_actionable:38   Index logstash-2016.07.25 is not actionable, removing from list.\n2016-07-26 11:22:09,424 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"logstash-2016.07.25\" age (1469404805), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,424 DEBUG          curator.indexlist       __not_actionable:38   Index logstash-2016.07.24 is not actionable, removing from list.\n2016-07-26 11:22:09,424 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"logstash-2016.07.24\" age (1469318405), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,424 DEBUG          curator.indexlist       __not_actionable:38   Index .marvel-es-1-2016.07.22 is not actionable, removing from list.\n2016-07-26 11:22:09,424 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \".marvel-es-1-2016.07.22\" age (1469145602), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,424 DEBUG          curator.indexlist           __actionable:34   Index logstash-2016.07.26 is actionable and remains in the list.\n2016-07-26 11:22:09,424 DEBUG          curator.indexlist            __excludify:57   Remains in actionable list: Index \"logstash-2016.07.26\" age (1469491205), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,424 DEBUG          curator.indexlist       __not_actionable:38   Index logstash-2016.07.21 is not actionable, removing from list.\n2016-07-26 11:22:09,426 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"logstash-2016.07.21\" age (1469059205), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,426 DEBUG          curator.indexlist       __not_actionable:38   Index .marvel-es-1-2016.07.25 is not actionable, removing from list.\n2016-07-26 11:22:09,426 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \".marvel-es-1-2016.07.25\" age (1469404800), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,426 DEBUG          curator.indexlist       __not_actionable:38   Index logstash-2016.07.23 is not actionable, removing from list.\n2016-07-26 11:22:09,426 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"logstash-2016.07.23\" age (1469232005), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,426 DEBUG          curator.indexlist       __not_actionable:38   Index logstash-2016.07.22 is not actionable, removing from list.\n2016-07-26 11:22:09,426 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"logstash-2016.07.22\" age (1469145605), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,426 DEBUG          curator.indexlist       __not_actionable:38   Index nxfilter-2016.06 is not actionable, removing from list.\n2016-07-26 11:22:09,426 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"nxfilter-2016.06\" age (1464899541), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,426 DEBUG          curator.indexlist       __not_actionable:38   Index nxfilter-2016.07 is not actionable, removing from list.\n2016-07-26 11:22:09,426 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"nxfilter-2016.07\" age (1467331203), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,427 DEBUG          curator.indexlist       __not_actionable:38   Index topbeat-2016.07.24 is not actionable, removing from list.\n2016-07-26 11:22:09,427 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"topbeat-2016.07.24\" age (1469318408), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,427 DEBUG          curator.indexlist       __not_actionable:38   Index topbeat-2016.07.25 is not actionable, removing from list.\n2016-07-26 11:22:09,427 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"topbeat-2016.07.25\" age (1469404807), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,427 DEBUG          curator.indexlist           __actionable:34   Index topbeat-2016.07.26 is actionable and remains in the list.\n2016-07-26 11:22:09,427 DEBUG          curator.indexlist            __excludify:57   Remains in actionable list: Index \"topbeat-2016.07.26\" age (1469491208), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,427 DEBUG          curator.indexlist       __not_actionable:38   Index topbeat-2016.07.20 is not actionable, removing from list.\n2016-07-26 11:22:09,427 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"topbeat-2016.07.20\" age (1468972807), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,427 DEBUG          curator.indexlist       __not_actionable:38   Index topbeat-2016.07.21 is not actionable, removing from list.\n2016-07-26 11:22:09,427 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"topbeat-2016.07.21\" age (1469059206), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,427 DEBUG          curator.indexlist       __not_actionable:38   Index topbeat-2016.07.22 is not actionable, removing from list.\n2016-07-26 11:22:09,427 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"topbeat-2016.07.22\" age (1469145608), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,427 DEBUG          curator.indexlist       __not_actionable:38   Index topbeat-2016.07.23 is not actionable, removing from list.\n2016-07-26 11:22:09,427 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"topbeat-2016.07.23\" age (1469232007), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,427 DEBUG          curator.indexlist       __not_actionable:38   Index .marvel-es-data-1 is not actionable, removing from list.\n2016-07-26 11:22:09,427 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \".marvel-es-data-1\" age (1464979408), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,427 DEBUG          curator.indexlist       __not_actionable:38   Index topbeat-2016.07.14 is not actionable, removing from list.\n2016-07-26 11:22:09,427 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"topbeat-2016.07.14\" age (1468454407), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,427 DEBUG          curator.indexlist       __not_actionable:38   Index packetbeat-2016.06 is not actionable, removing from list.\n2016-07-26 11:22:09,427 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"packetbeat-2016.06\" age (1464739226), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,427 DEBUG          curator.indexlist       __not_actionable:38   Index packetbeat-2016.07 is not actionable, removing from list.\n2016-07-26 11:22:09,427 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"packetbeat-2016.07\" age (1467331208), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,428 DEBUG          curator.indexlist       __not_actionable:38   Index packetbeat-2016.04 is not actionable, removing from list.\n2016-07-26 11:22:09,428 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"packetbeat-2016.04\" age (1459468824), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,428 DEBUG          curator.indexlist       __not_actionable:38   Index packetbeat-2016.05 is not actionable, removing from list.\n2016-07-26 11:22:09,428 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"packetbeat-2016.05\" age (1462060888), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,428 DEBUG          curator.indexlist       __not_actionable:38   Index packetbeat-2016.03 is not actionable, removing from list.\n2016-07-26 11:22:09,428 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"packetbeat-2016.03\" age (1456790407), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,428 DEBUG          curator.indexlist       __not_actionable:38   Index packetbeat-2016.01 is not actionable, removing from list.\n2016-07-26 11:22:09,428 DEBUG          curator.indexlist            __excludify:57   Removed from actionable list: Index \"packetbeat-2016.01\" age (1452209738), direction: \"younger\", point of reference, (1469467329)\n2016-07-26 11:22:09,428 DEBUG              curator.utils        iterate_filters:716  Post-instance: [u'.marvel-es-1-2016.07.26', u'logstash-2016.07.26', u'topbeat-2016.07.26']\n2016-07-26 11:22:09,431 DEBUG              curator.utils      repository_exists:911  Repository Untergeek exists.\n2016-07-26 11:22:09,431 DEBUG              curator.utils     parse_date_pattern:995  Partially rendered name: s\n2016-07-26 11:22:09,431 DEBUG              curator.utils     parse_date_pattern:995  Partially rendered name: sh\n2016-07-26 11:22:09,431 DEBUG              curator.utils     parse_date_pattern:995  Partially rendered name: sha\n2016-07-26 11:22:09,431 DEBUG              curator.utils     parse_date_pattern:995  Partially rendered name: shan\n2016-07-26 11:22:09,431 DEBUG              curator.utils     parse_date_pattern:997  Fully rendered name: shan\n2016-07-26 11:22:09,431 DEBUG                curator.cli         process_action:110  Doing the action here.\n2016-07-26 11:22:09,431 DEBUG          curator.indexlist       empty_list_check:178  Checking for empty list\n2016-07-26 11:22:09,497 DEBUG              curator.utils           test_repo_fs:927  All nodes can write to the repository\n2016-07-26 11:22:09,497 DEBUG              curator.utils           test_repo_fs:929  Nodes with verified repository access: {u'HD9y66SKRleRoj82_eK2bA': {u'name': u'MacBuntu1'}, u'7DVvG6djQFuYxxK-4sIdHQ': {u'name': u'Elasticbox'}, u'-LDbj7sGRmC7zdirMA82rA': {u'name': u'MacBuntu2'}, u'TKH6UI_VQk6GnMk5DE3mbQ': {u'name': u'esmaster'}}\n2016-07-26 11:22:09,513 INFO      curator.actions.snapshot              do_action:741  Creating snapshot \"shan\" from indices: [u'.marvel-es-1-2016.07.26', u'logstash-2016.07.26', u'topbeat-2016.07.26']\n2016-07-26 11:22:42,859 INFO      curator.actions.snapshot           report_state:715  Snapshot shan successfully completed.\n2016-07-26 11:22:42,859 INFO                 curator.cli                    cli:252  Action #1: completed\n2016-07-26 11:22:42,859 INFO                 curator.cli                    cli:253  Job completed.\nAgain, I cannot replicate this error.  What version of Elasticsearch are you using?  If pip, which version of Python?  What is Elasticsearch running in?  Windows?\nAlso, please attach debug logs with blacklist: [], as I might be able to see some of the calls to Elasticsearch that way.  It seems to me that there's something else going on here, since I cannot replicate it with Elasticsearch 2.3.3 and Curator 4.0.3.\n. Python 3.6 is a development branch, isn't it?  It's only at 3.6.0a3 as of July 12, 2016, from what I can tell.  I be able to easily test against pre-release versions in  Travis CI.  The most recent stable branch is 3.5.2, as of 2016-06-27.  You had the problem with the MSI, which should have been using an isolated, localized and precompiled version of Python 2.7.  If you're using a development release of Python, that might have something to do with it.  I confess that the error <class 'TypeError' at 0x000000001D8AC400> seems to not match other errors I am accustomed to seeing.\nThe code that raises this TypeError is at: https://github.com/elastic/curator/blob/master/curator/utils.py#L88.  It works in Python 2.6, 2.7, 3.4, and 3.5.  It may be some change in 3.6 (if you really are using a pre-release of Python) is not working with this change.\n. Thanks for catching the typo.\n. The answer is just to employ another filter.  You can exclude a pattern filtertype with a value of '%Y.%m.%d':\n- filtertype: pattern\n      kind: timestring\n      value: '%Y.%m.%d'\n      exclude: True\nThis filter should work anywhere in the chain, but it might be easier to visualize if it appears before the age filter with the weekly timestring.\n. Since there's a simple fix for this, I'm going to mark it complete.  If you believe this is in error, please feel free to reopen this with a comment as to why, or open another ticket.\n. @jakommo the problem is that dates do not always appear at the end of an index name.  While it is very common for that to be the case, I have also seen 2016.12.29-suffix, and prod-2016.12.29-logtype and other variations.  As a result, the regexes that match dates are all preceded and followed by .*.  So, for example, the year-month-day regex is ^.*\\d{4}\\.\\d{2}\\.\\d{2}.*$.  \nThis is why you need a second filter to prevent year-month-day indices if you only want year-week.  While it may be possible to write in an implied filter blocking year-month-day indices, I'm not sure I like the implications of an invisible filter.  I'd rather instruct users with the why and the how of working around it.. Duplicate of #674 \n. This will be coupled with parallel work for #511 \n. #511 will not be performed because Elasticsearch date math is sufficient. Adding strftime for aliases will be possible, but it will not support future index names.  Does that present an obstacle?. Actually, there's nothing I can do here at all.  I finally read the fine print.\nYou called this ticket, \"allow use of date params in alias keys\", but what you're asking for is for Curator to parse the extra_settings, which can be arbitrary.  Curator does not parse the extra_settings because it can't predict with any reliability what will be there.  It's just not a feasible feature request.\nIf you know what you're up against, you can always use environment variables and pass those to Curator.. Thanks for adding this!\n. Can you run this with loglevel: DEBUG, sanitize (remove any sensitive information) and attach?  I'd like to see what's going on, but the method name and line numbers are only provided in DEBUG level.\n. Also, please share your action.yml, so I can attempt to replicate locally.\n. Here it is:\nif not indices:\n        logger.error('Missing required repository parameter')\n        return False\nWhile this is clearly a typo (it should be logging that no array of indices was provided), that's not even the larger error.\nThis is actually a multifaceted error.  It shouldn't even get to this code if the index list object has no indices.  I'll be pushing a fix for this soon.\n. The creation_date is index metadata that did not get added until Elasticsearch 1.4.  That's why that message is in Curator.  Your index selection criteria is probably using an age filter with creation_date, and this is the result if the index has no creation_date metadata (and that metadata is in the cluster state, not the index itself).  Curator tries to do the safe thing and not act on an index in that case.\n. You're right.  That's not needed for this action to occur.  Curator 4 differs from the previous versions in that it pulls the index metadata for all indices when it starts. The message you report happens during that index metadata discovery step:\nif not 'creation_date' in wl['settings']['index']:\n                        self.loggit.warn(\n                            'Index: {0} has no \"creation_date\"! This implies '\n                            'that the index predates Elasticsearch v1.4. For '\n                            'safety, this index will be removed from the '\n                            'actionable list.'.format(index)\n                        )\n                        self.__not_actionable(index)\nAs I mentioned already, this is because Curator 4 determines to do the safe thing with older indices.  The README shows that Curator 4 is meant for Elasticsearch versions 2+.  I will add another bit to the README to indicate that indices created before ES 1.4 are not supported. \n. I'd prefer not to make exceptions to a rule.  There are several other good and important reasons to have indices upgraded to at least a 2.x version (which is crucial if an eventual upgrade to Elasticsearch 5.0 is planned, as older indices cannot be used in place).  \nIf you still want to use Curator, use can use version 3.5.1 which has no such restrictions and according to the compatibility matrix supports all ES versions 0.x through 2.x.  Curator 4 is the first version which is not fully backwards compatible.\n. If you're already using Elasticsearch 2.3, you could try the new _reindex API: https://www.elastic.co/guide/en/elasticsearch/reference/2.3/docs-reindex.html\nWhile I can sympathize with the pain of reindexing data, as mentioned, if you plan on upgrading to newer versions of Elasticsearch in the future, this will still be required at that point.\n. As there is no real solution other than reindexing, I'm marking this closed.  If you feel this is in error, please feel free to reopen the ticket, or open a new one.\n. If you read this comment in #685, you'll see that it's not a regression.  Curator 4 depends on the _cluster/state endpoint, and AWS ES does not provide it.\nWhen I wrote Curator 4, which is a major change over Curator 3:\n1. Due to new APIs, Curator 4 was not going to be reverse compatible with pre 2.0 versions of Elasticsearch.\n2. AWS ES was tied to ES version 1.5\n3. I hoped that if they ever released a newer version it would support the _cluster/state endpoint.\nAWS ES now supports a newer version of Elasticsearch, but it still doesn't support the _cluster/state endpoint.\nWhy does Curator 4 depend on the _cluster/state endpoint, when Curator 3 did not?  Because Curator 4 extends a lot of functionality that Curator 3 did not have, like the ability to filter by age with the index creation_date.  The method in question is IndexList._get_metadata.  Other code that depends on the cluster state includes the master_only testing and checking whether indices are open or closed. A workaround to not use cluster state would mean having to complicate the code, considerably, and it would result in a fragmented user experience, as it would mean that some users simply couldn't use some functionality.  I got into that in Curator 3, and it was a pain to maintain and document, and I was constantly having to explain that feature X was not supported in use case Y.  I want to keep the experience uniform, so I am using the simplest way forward, which is to use _cluster/state.\nAs mentioned in that linked comment, the best case scenario for you would be to use Curator 3 still, as it supports versions of ES up through 2.x.  I also suggest asking AWS ES to change the policy of disallowing the _cluster/state endpoint, since it was their decision to remove that.  Perhaps they could be persuaded to reverse that decision.\n. > I think there is no point in support the AWS Credentials in Curator 4 if there is no way this tool can work with AWS ES.\n@basex Not necessarily.  This may be useful for people who want to run Curator remotely against self-hosted ES clusters residing in AWS\n. @malpani \nSee https://github.com/elastic/curator/blob/v4.0.0/curator/indexlist.py#L146-L149 \nself.client.cluster.state(\n                    index=to_csv(l),metric='metadata'\n                )['metadata']['indices']\nand https://github.com/elastic/curator/blob/v4.0.0/curator/utils.py#L400\nmaster_node_id = client.cluster.state(metric='master_node')['master_node']\nThe biggest part is really the index metadata in https://github.com/elastic/curator/blob/v4.0.0/curator/indexlist.py#L137-L166, where you see that Curator currently uses (this may expand at any time):\n- number_of_replicas\n- number_of_shards\n- state\n- creation_date\n- routing\nMost of these are under cluster.state['metadata']['indices'][INDEXNAME]['settings']['index'], though the index state (open/closed) is at cluster.state['metadata']['indices'][INDEXNAME]['state']\n. @chetandhembre I can't find that API call in the elasticsearch-py.readthedocs.io documentation.  I'm not looking to hack the call directly.\n. It's currently not solvable due to AWS ES not supporting the required API call. \n. Yes.  It lacks some of the newer features in 4, but it should work with AWS ES.\n. @RyanBowlby-Reflektion I'm sorry you had a bad experience.  I keep putting notices everywhere I can think of.  \nThis isn't a huge banner, but it's in the README:\n\nIt is also important to note that Curator 4 requires access to the /_cluster/state/metadata endpoint. Forks of Elasticsearch which do not support this endpoint (such as AWS ES, see #717) will not be able to use Curator version 4.\n\nIn the official documentation, I added the following warning to each of the AWS configuration flags here, here, and here.\n\nThis feature allows connection to AWS using IAM credentials, but Curator 4 does not currently work with AWS.\n\nWhich links to this FAQ response.\nI also removed the AWS IAM flags from the configuration file example. Note the difference in the older 4.0 documentation here.\nI'm not sure where else I can put notices to make it more plain.\n. @oji It was never officially supported. . I would first advise adding a : after exclude where they are missed.  The YAML configuration language requires keys to be followed by colons.\n. Proper indentation is also necessary.\n. You have to create a repository first. It's not just a file system path. It must be a shared file system that all data and master nodes have write access to. There's an Elasticsearch API for that, as well as the es_repo_mgr script that ships with Curator. \n. That'll teach me to do bad cut/paste and not validate my args.  Thanks for submitting this.\nI need a few more things from you:\n1. Please sign the CLA\n2. Please add yourself to the CONTRIBUTORS file.\n3. Please add a blurb to the Changelog.rst in the doc directory under a **Bug Fixes** header, like other releases show.\n. @basex What email did you sign with?  It should be the same email as is associated with your GitHub account, basex, or it will not show up in our system properly.\n. Duplicate of #717. \n. fixed by #725 \n. What happens if you set value: '-2016.30' instead of without quotes?\n. I ask because value is part of a regex.  The .3 may not be a literal . without putting the whole string in quotes.  I will try to replicate locally.\n. Also, it does fail for a snapshot action, but passes for a close action:\n2016-08-09 12:25:25,692 DEBUG              curator.utils        iterate_filters:721  Pre-instance: [u'test-2016.30', u'test-2016.33']\n2016-08-09 12:25:25,692 DEBUG          curator.indexlist        filter_by_regex:280  Filtering indices by regex\n2016-08-09 12:25:25,692 DEBUG          curator.indexlist       empty_list_check:178  Checking for empty list\n2016-08-09 12:25:25,692 DEBUG          curator.indexlist           working_list:189  Generating working list of indices\n2016-08-09 12:25:25,692 DEBUG          curator.indexlist        filter_by_regex:301  Filter by regex: Index: test-2016.30\n2016-08-09 12:25:25,692 DEBUG          curator.indexlist           __actionable:34   Index test-2016.30 is actionable and remains in the list.\n2016-08-09 12:25:25,692 DEBUG          curator.indexlist        filter_by_regex:301  Filter by regex: Index: test-2016.33\n2016-08-09 12:25:25,692 DEBUG          curator.indexlist       __not_actionable:38   Index test-2016.33 is not actionable, removing from list.\n2016-08-09 12:25:25,692 DEBUG              curator.utils        iterate_filters:723  Post-instance: [u'test-2016.30']\n2016-08-09 12:25:25,692 INFO               curator.utils           show_dry_run:599  DRY-RUN MODE.  No changes will be made.\n2016-08-09 12:25:25,692 INFO               curator.utils           show_dry_run:602  (CLOSED) indices may be shown that may not be acted on by action \"close\".\n2016-08-09 12:25:25,692 INFO               curator.utils           show_dry_run:609  DRY-RUN: close: test-2016.30 with arguments: {'delete_aliases': False}\n2016-08-09 12:25:25,692 INFO                 curator.cli                    cli:252  Action #1: completed\n2016-08-09 12:25:25,693 INFO                 curator.cli                    cli:253  Job completed.\nI will dig some more into why.\n. No, my bad.  It does work.  You need to use single quotes around your value: expression, as it's part of a regular expression:\nWithout quotes:\n2016-08-09 12:28:38,646 DEBUG              curator.utils        iterate_filters:721  Pre-instance: [u'test-2016.30', u'test-2016.33']\n2016-08-09 12:28:38,646 DEBUG          curator.indexlist        filter_by_regex:280  Filtering indices by regex\n2016-08-09 12:28:38,646 DEBUG          curator.indexlist       empty_list_check:178  Checking for empty list\n2016-08-09 12:28:38,646 DEBUG          curator.indexlist           working_list:189  Generating working list of indices\n2016-08-09 12:28:38,646 DEBUG          curator.indexlist        filter_by_regex:301  Filter by regex: Index: test-2016.30\n2016-08-09 12:28:38,646 DEBUG          curator.indexlist       __not_actionable:38   Index test-2016.30 is not actionable, removing from list.\n2016-08-09 12:28:38,646 DEBUG          curator.indexlist        filter_by_regex:301  Filter by regex: Index: test-2016.33\n2016-08-09 12:28:38,646 DEBUG          curator.indexlist       __not_actionable:38   Index test-2016.33 is not actionable, removing from list.\n2016-08-09 12:28:38,646 DEBUG              curator.utils        iterate_filters:723  Post-instance: []\n2016-08-09 12:28:38,646 DEBUG          curator.indexlist       empty_list_check:178  Checking for empty list\n2016-08-09 12:28:38,646 ERROR                curator.cli                    cli:236  Unable to complete action \"snapshot\".  No actionable items in list: <class 'curator.exceptions.NoIndices'>\nWith quotes:\n2016-08-09 12:30:06,081 DEBUG              curator.utils        iterate_filters:721  Pre-instance: [u'test-2016.30', u'test-2016.33']\n2016-08-09 12:30:06,081 DEBUG          curator.indexlist        filter_by_regex:280  Filtering indices by regex\n2016-08-09 12:30:06,081 DEBUG          curator.indexlist       empty_list_check:178  Checking for empty list\n2016-08-09 12:30:06,081 DEBUG          curator.indexlist           working_list:189  Generating working list of indices\n2016-08-09 12:30:06,081 DEBUG          curator.indexlist        filter_by_regex:301  Filter by regex: Index: test-2016.30\n2016-08-09 12:30:06,081 DEBUG          curator.indexlist           __actionable:34   Index test-2016.30 is actionable and remains in the list.\n2016-08-09 12:30:06,081 DEBUG          curator.indexlist        filter_by_regex:301  Filter by regex: Index: test-2016.33\n2016-08-09 12:30:06,081 DEBUG          curator.indexlist       __not_actionable:38   Index test-2016.33 is not actionable, removing from list.\n2016-08-09 12:30:06,082 DEBUG              curator.utils        iterate_filters:723  Post-instance: [u'test-2016.30']\n2016-08-09 12:30:06,082 DEBUG          curator.indexlist       empty_list_check:178  Checking for empty list\n2016-08-09 12:30:06,082 ERROR                curator.cli                    cli:242  Failed to complete action: snapshot.  <class 'curator.exceptions.MissingArgument'>: No value for \"repository\" provided\nThe quoted expression yields: 2016-08-09 12:30:06,082 DEBUG              curator.utils        iterate_filters:723  Post-instance: [u'test-2016.30'],  where as the other just has [] as it's post-instance output.\n. I will document this better.  The string doesn't get interpreted correctly if it isn't quoted, but only because it's part of a regular expression, so the . and any other special characters could be interpreted.\n. How is Elasticsearch secured?  Is it using the Shield plugin?  Or is it behind an SSL proxy or something like that?\n. I don't know if it's relevant, but Curator has been tested with Shield and is known to work with that. \n. Also, running with loglevel: DEBUG and attaching the resulting file will help with debugging.\n. And you may try setting ssl_no_validate: True, as that is what helped with this older ticket.\n. I do not recommend downgrading to 3.5.1.  The linked example was simply using 3.5.1.  The message, as you have seen, is the same.\n/usr/lib/python2.7/site-packages/urllib3/connectionpool.py:770: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html\nInsecureRequestWarning)\nThis is a non-fatal warning.  You likely cannot get rid of it if you are using self-signed certificates, or there are missing intermediate certificates, or any other number of possible permutations.  It simply says, \"I'm not verifying the certificate, just so you know,\" and then it continues on its merry way.  I have no way of knowing how your certificates were generated, or by whom they were signed.  Curator itself only delegates that testing to the urllib3 python module, and that module says there's something wrong in the certificate chain.  It's that simple.  Perhaps urllib3 is pickier than Kibana.  I don't know.  Kibana may still be generating errors in the log file related to the certificate chain, but it keeps working anyway.  I don't have that information handy as it's in your environment.\nUsing --master-only is probably not needed in your use case.  It is used when deploying Curator to all nodes in the cluster, but only having it complete the run on the elected master.\n. Any update, @sagh0900 ?\n. @sagh0900  Have you tried installing the certifi module?  It is supposed to provide those CAs.  Yes, the OS-provided one can work, but the certifi python module is also supposed to provide those.  If certificate is not specified, and the certifi module is installed, Curator should use that.\nThis behavior is in these lines of code.\n. @inqueue Thanks for reporting your findings.  That helps.\n. With the upgrade to Curator 4.2, and providing self-contained versions for linux and windows, most of the issues with certificates seem to have been resolved.  This is chiefly because older versions of Python are not being used (with their SSL issues), and certifi is bundled.  I'm going to close this.. logfomat: logstash\nTry respelling it, logformat, with an r in it.\n. Did that fix your problem?  If so, please feel free to close this.\n. By the way, #674 is my number one priority right now.\n. \ud83d\ude04 \n. That's an interesting use of epoch.  It was included primarily for testing.  It was always intended that Curator use time relative to the time of execution to determine which indices are actionable, rather than absolute time.  This is evident that even with the epoch option, you are still obligated to use those relative timings, e.g. older than 1 second.\nIn any case, this feature described in this ticket should otherwise be added to #697 \n. Fixed by #734.  See https://www.elastic.co/guide/en/elasticsearch/client/curator/current/fe_unit.html\n. My idea for this is:\n- filtertype: period\n  unit: [second|minute|hour|day|week|month|year]\n  range_from: 1\n  range_to: 1\n  source: [name|creation_date|field_stats]\n  timestring: \n  field:\n  stats_value:\n  exclude: False\nThis would get the entire period identified by unit, unit_count instances ago.  \nFor example, with unit set to month:\n- A range_from and range_to of 0 would get the current month (even incomplete).\n- A range_from and range_to of 1 would get the previous month (first through last day of month, however many days it might be)\n- A range_from 1 and range_to of 2 would get the previous month and the month before that (first through last day of each month, however many days they might be)\nOne potential caveat is that all time values are in UTC, as this is how they're all calculated in Curator right now, but that's also how all dates are stored in Elasticsearch, so that shouldn't be a big surprise.\n. Changing this up to be less weird. -1 would be the previous period, and 1 (positive one) would be in the future.  Should make the math easier.. The option is in the configuration file now, not the command line.  Almost everything is a breaking change if you've upgraded to 4.0.\nSee: https://www.elastic.co/blog/curator_v4_release\nand also https://www.elastic.co/guide/en/elasticsearch/client/curator/4.0/configfile.html#master_only\n. This has been fixed by #742\n. I'm adding a similar logging functionality to the schema validation requested in #674.  It shouldn't be hard to add to the main body.\n. Whoa!  That's some old stuff there.  Curator has not used those versions in a very long time.  Curator is now on version 4.0.6, and has some very different configuration options than you are probably seeing.  \n\nName: elasticsearch\nVersion: 0.4.5\n\nThe current version of the Elasticsearch python module is 2.4.0.  If you're using a version of Curator old enough to depend on something that old, you are definitely running an outdated version of Curator.\nAs far as the DistributionNotFound error is concerned, there is a FAQ answer for that here in the official documentation.\nI haven't seen the Python 2.6 deprecation notice before today, but it makes sense that you would need to upgrade to 3.5 or 2.7 if you want to be on a supported level.  Otherwise, I will do my best to continue to support 2.6 via the RPM/DEB packages described in the documentation.\n. @elJoeyJojo Elasticsearch the java application and elasticsearch the python module are different from each other.  One is the search engine, the other is the python client library.  \nHow you upgrade the python module depends on how it was installed.  How did you install it on your system?\n. Current release versions are:\nElasticsearch: 2.3.5\nLogstash: 2.3.4\nKibana: 4.5.4\nLogstash forwarder is deprecated in favor of filebeat\nIt seems that the tutorial is nearly 1 year old.  The Elastic stack is growing very quickly.\nIf you truly installed python-pip and did pip install elasticsearch-curator, I'd be curious to know what version it was.\nPlease paste the output of pip list | grep elastic \n. And you installed via pip install elasticsearch-curator?  That suggests that DigitalOcean has its own mirror of pip, and it's very behind.\nPlease paste the output of pip install -U elasticsearch elasticsearch-curator\n. That's the syntax for a very old version of Curator (2.x, I think).  The 3.x syntax was different from that, and the 4.x syntax uses YAML configuration files.\nIf you want command-line syntax for Curator, you will need to do pip install -U elasticsearch-curator==3.5.1 to go back to that version.\nAfter that, you'll need to run:\ncurator --host localhost delete indices --timestring '%Y.%m.%d' --older-than 10 --time-unit days\nSee: https://www.elastic.co/guide/en/elasticsearch/client/curator/3.5/time-unit.html\nOther than that, the documentation is all online.  Please refer to current documentation instead of older, unreliable information on blog posts.  The documentation for the most recent versions is at https://www.elastic.co/guide/en/elasticsearch/client/curator/index.html\n. Did you not follow the instructions in my first comment to fix the DistributionNotFound error?\nhttps://www.elastic.co/guide/en/elasticsearch/client/curator/4.0/entrypoint-fix.html (same fix for any version of Curator)\n. You're welcome.\n. This kind of falls under #511, and the extended support for this.  Please comment there, as that's where this belongs.\n. It's hard for me to tell what's going on here. I can't tell if it's properly formatted (indentation matters, and it is in the examples).  \nIn order to read it properly, please encapsulate each block of text in triple back-tick characters on their own line, with nothing else on that line e.g.\nformatted text goes here\n. Also, I think it's not a good idea to put your curator client configuration in your logstash conf.d directory.  Logstash will likely try to read it and parse it as a configuration file, and that will definitely yield an error.\n. I believe I've found your problem:\nhosts:\n    - 127.0.0.1:\nRemove the colon at the end of 127.0.0.1: and it will hopefully work.\n. you're also missing the keyword, client: at the beginning of the config.yml file:\n```\nRemember, leave a key empty if there is no value.  None will be a string,\nnot a Python \"NoneType\"\nclient:\n  hosts:\n    - 127.0.0.1\n```\nSomehow you put it on the line before:\n```\nRemember, leave a key empty if there is no value.  None will be a string,\nnot a Python \"NoneType\"client:\nhosts:\n    - 127.0.0.1\n```\n. This was first reported in #674, and addressed in #742, merged yesterday.  It will be released in Curator 4.1.0, which will hopefully be out soon.\n. There is limited use for the index filtering nature of Curator with this API.  I'm sure there are those out there who will be reindexing stuff nightly, but how will they determine which indices?\nI need some use case feedback before I start blindly coding this.\n. I think combining #733 with this will actually be pretty cool.  It would allow users to reindex one or more hours|days|weeks|months down to a single index.\nI think the use case has been found.\n. Just to give you a heads-up, I do not intend to add the reindex action to the 4.x branch, even though the API technically is supported for 2.4.\nThe reindex action will only be available for Curator 5, and therefore will be version locked to ES 5.. Shrink will likely come in a later release of 5.x, but not in 5.0. It's planned.  I just rushed to re-release it, so it's still \"old\" style.  Thanks for putting in the request.\n. This is RPM hell, as it is colloquially known.  Your options are to:\n1. Install the RPM manually, instead of from the curator repo.  You would also need the other dependent RPMs in this case (click, urllib3, elasticsearch, etc.).\n2. Install via pip, as per the instructions in the documentation.\n\nNow I am able to only install curator python library but not the curator cli interface. Any hint on how to solve this?\n\nCurator 4 does not separate the API and CLI.  They are the same.  If you can install Curator 4, you have the CLI.  It is just very different from Curator 3 as it uses YAML configuration files instead of a bajillion command-line flags.   Start here for more information.\n. There is a way around this, and that is to compile Curator.\nI just built this and verified it against a clean VM with nothing pre-installed.  You don't even necessarily need to have python installed at all, since it's bundled.\nCompile Curator into a binary with included libraries\nErrata\nNOTE: This how-to is only verified for CentOS 6.  There are extra steps/workarounds involved to enable cx-Freeze on other platforms. Also, this build is x86-64 only.  You should be able to modify the steps/VMs to work for other platforms, but that is not covered here.\nThis requires either vagrant, or a CentOS 6 installation with internet access.  These instructions use Vagrant, so you can build it anywhere\nMake a curator build directory (replace /path/to with whatever you would like, that you have access to):\nmkdir /path/to/curator_build\nBuild a Vagrant VM:\nmkdir centosVM\ncd centosVM\nPut the following in a file called Vagrantfile:\nVagrant.configure(2) do |config|\n  config.vm.box = \"elastic/centos-6-x86_64\"\n  config.vm.synced_folder \"/path/to/curator_build\", \"/curator_build\", create: true, owner: \"vagrant\", group: \"vagrant\"\n  config.vm.provider \"virtualbox\" do |v|\n    v.customize [\"modifyvm\", :id, \"--nictype1\", \"virtio\"]\n  end\nend\nNote that /path/to/curator_build is in this configuration.  Be sure to replace it with whatever you used in that previous step.\nRun:\nvagrant up; vagrant ssh\nInside the VM, run:\n```\nsudo -i\nyum groupinstall \"Development Tools\" \"Development Libraries\" python-devel\npip install -U setuptools\nwget https://github.com/elastic/curator/archive/v4.1.1.tar.gz \ntar zxf v4.1.1.tar.gz\ncd curator-v4.1.1\npip install --user cx-Freeze\npip install --user -r requirements.txt\npython setup.py build_exe\nmv build/exe.linux-x86_64-2.6 /curator_build/curator-4.1.1\n```\nTest the compiled version from this directory:\ncd /curator_build/curator-4.1.1\n./curator --version\nYou should see:\ncurator, version 4.1.1\nExit the vm now:\nshutdown -h now\nConclusion\nWhen you exit the VM, the fully compiled-for-CentOS 6 version of Curator will be available in /path/to/curator_build/curator-4.1.1\nYou can now tar/zip this directory and copy it to the target machines.  You can also delete the entire VM directory afterward.\n. I now publish these binaries directly to the APT and YUM repositories:\nhttps://www.elastic.co/guide/en/elasticsearch/client/curator/current/apt-repository.html#apt-binary\nhttps://www.elastic.co/guide/en/elasticsearch/client/curator/current/yum-repository.html#yum-binary\n. @JD557 can you sign our CLA please?  I can't merge even a doc change without that.\n. I manually verified.  Thanks for signing. LGTM\u2122.\n. Actually, the error message results from a problem with the order of operations.  This is revealed here:\n```\nfrom voluptuous import *\nschemas = [\n    Schema({Optional('exclude'): All(Any(int, bool), Coerce(bool))}),\n    Schema({Optional('exclude'): All(Any(bool, int), Coerce(bool))}),\n]\ntest = [\n    {'exclude':0},\n    {'exclude':1},\n    {'exclude':'0'},\n    {'exclude':'1'},\n    {'exclude':'false'},\n    {'exclude':'true'},\n    {'exclude':'False'},\n    {'exclude':'True'},\n    {'exclude':False},\n    {'exclude':True},\n]\nfor s in schemas:\n    print('{0}'.format(s))\n    for t in test:\n        try:\n            result = s(t)\n            print('   PASS: Schema({0}) = {1}'.format(t, s(t)))\n        except Exception as e:\n            print('   FAIL: Schema({0}). Exception: {1}'.format(t, e))\n    print\n```\nHere are the results:\n```\n, ]), Coerce(bool, msg=None), msg=None)}, extra=PREVENT_EXTRA, required=False) object at 0x10ef56e90>\n   PASS: Schema({'exclude': 0}) = {'exclude': False}\n   PASS: Schema({'exclude': 1}) = {'exclude': True}\n   FAIL: Schema({'exclude': '0'}). Exception: expected int for dictionary value @ data['exclude']\n   FAIL: Schema({'exclude': '1'}). Exception: expected int for dictionary value @ data['exclude']\n   FAIL: Schema({'exclude': 'false'}). Exception: expected int for dictionary value @ data['exclude']\n   FAIL: Schema({'exclude': 'true'}). Exception: expected int for dictionary value @ data['exclude']\n   FAIL: Schema({'exclude': 'False'}). Exception: expected int for dictionary value @ data['exclude']\n   FAIL: Schema({'exclude': 'True'}). Exception: expected int for dictionary value @ data['exclude']\n   PASS: Schema({'exclude': False}) = {'exclude': False}\n   PASS: Schema({'exclude': True}) = {'exclude': True}\n, ]), Coerce(bool, msg=None), msg=None)}, extra=PREVENT_EXTRA, required=False) object at 0x10ef5f310>\n   PASS: Schema({'exclude': 0}) = {'exclude': False}\n   PASS: Schema({'exclude': 1}) = {'exclude': True}\n   FAIL: Schema({'exclude': '0'}). Exception: expected bool for dictionary value @ data['exclude']\n   FAIL: Schema({'exclude': '1'}). Exception: expected bool for dictionary value @ data['exclude']\n   FAIL: Schema({'exclude': 'false'}). Exception: expected bool for dictionary value @ data['exclude']\n   FAIL: Schema({'exclude': 'true'}). Exception: expected bool for dictionary value @ data['exclude']\n   FAIL: Schema({'exclude': 'False'}). Exception: expected bool for dictionary value @ data['exclude']\n   FAIL: Schema({'exclude': 'True'}). Exception: expected bool for dictionary value @ data['exclude']\n   PASS: Schema({'exclude': False}) = {'exclude': False}\n   PASS: Schema({'exclude': True}) = {'exclude': True}\n```\nAs you can see, strings are not coerced into booleans, but integers are.  With the proper order, Any(bool, int), it gives the proper message, expected bool.  That's what actually needs to change here, as it otherwise does what it is supposed to do.  It's supposed to cast a boolean, and 0 and 1 can be boolean, as can False and True, but \"0\" and \"1\", and \"true\", and \"false\" cannot, capitalized or not.\n. Fixed in #760 \n. Ah. That's a holdover from the 3.x code. Back then I expected you to test for a value or a False before proceeding. \nWith v4 already released, I can't really change this part of the API without breaking \"existing\" functionality. I can change this to be a raise then. \nIn the meantime, test for False before proceeding:\nvalue = curator.get_repository(...)\nif value:\n    # do something\nSorry for the inconvenience. \n. Please feel free to submit a pull request with the correction.\n-- Aaron Mildenstein\nOn September 20, 2016 at 7:05:34 AM, Rostislav (notifications@github.com)\nwrote:\n\nLooks like a typo in\nhttps://github.com/elastic/curator/blob/master/curator/utils.py#L544\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/elastic/curator/issues/762, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AA-R37jICieSHJ_d_CQjt5YtKHr0t_Ufks5qr9oegaJpZM4KBmWn\n.\n. Have you signed the CLA?\n. Are you using the delete_aliases: True option?  That fixes this.\n. Are you using 4.1.0? Or an older version?\n. @elliott-davis can you give an example?  This has been addressed in the master branch, I think.\n. Hmmm.  This is an awkward edge case.  The Voluptuous schema validator cannot coerce a string value of \"True\" to being a boolean value of True.  This is because Voluptuous' coerce method doesn't care about string boolean values:\n\n```\ndef coerce(type, msg=None):\n\"\"\"Coerce a value to a type.\nIf the type constructor throws a ValueError, the value will be marked as\nInvalid.\n\"\"\"\n    def f(v):\n        try:\n            return type(v)\n        except ValueError:\n            raise Invalid(msg or ('expected %s' % type.name))\n    return f\n```\nYou can see what happens at any python command line:\n```\n\n\n\nl = ['True', 'true', True, 'False', 'false', False]\nprint('{0}'.format(l))\n['True', 'true', True, 'False', 'false', False]\nfor v in l:\n...     print('Value: {0} is of type: {1}'.format(v, type(v)))\n...\nValue: True is of type: \nValue: true is of type: \nValue: True is of type: \nValue: False is of type: \nValue: false is of type: \nValue: False is of type: \n```\n\n\n\nSo because environment variables are inherently strings, Voluptuous cannot coerce.  What this means is that in order for it to coerce a string value of 'true'|'True' or 'false'|'False' into a true bool, I need to do it myself.  I can do this, and I apologize for the inconvenience.\n. Indeed.  I should have to be angry with the developer for not listing this in any of the examples I found, but rather the ones I used.\nI'll test this out and see what I get.  It may not work, still, for some of the cases, as None is still valid (then it takes the default), so I may yet have to use something similar, but with an added if statement for the None case.\n. The count filtertype was unintentionally omitted from the __map_method method in the SnapshotList class.  I'll add that back in and roll a 4.1.1 soon.\n. Not at this time, though I'm trying to create singleton cli bits (i.e. would only delete_indices, or delete_snapshots).\n. @geek876 \nI'm curious what use case prohibits you from using a configuration file of any kind.  Can you elaborate?\n. If this is what you're trying to do, I recommend using Curator 3.x in the meanwhile.  It can be installed via pip by doing pip install -U elasticsearch-curator==3.5.1 (the -U guarantees that whatever is there is replaced by the version after the ==).\nCan you tell me specifically which actions are crucial to your operation?  I can perhaps prioritize those.\n. I'm going to disagree that it's a regression (I had to maintain that code).  I will agree that there are use cases that the current, file-based configuration does not make easy. But for each of those use cases you can find, I can see several more that are possible with this format that would have been impossible with the 3.x code base.  Inconvenience? Yes. Regression? Not from my perspective.\n. So, what if there were a command that ran like this?\n```\n$ curator_delete_indices --dry-run --filter_list '[{\"filtertype\":\"age\",\"source\":\"creation_date\",\"direction\":\"older\",\"unit\":\"days\",\"unit_count\":13},{\"filtertype\":\"pattern\",\"kind\":\"prefix\",\"value\":\"logstash\"}]'\n2016-11-02 12:58:35,195 INFO      DRY-RUN MODE.  No changes will be made.\n2016-11-02 12:58:35,195 INFO      (CLOSED) indices may be shown that may not be acted on by action \"delete_indices\".\n2016-11-02 12:58:35,195 INFO      DRY-RUN: delete_indices: logstash-2016.10.19 (CLOSED) with arguments: {}\n2016-11-02 12:58:35,195 INFO      DRY-RUN: delete_indices: logstash-2016.10.20 with arguments: {}\n```\nThe full range of filters for a given singleton operation becomes available, so long as you encapsulate it as JSON within single quotes.\nAt present, it requires that you either:\n1. Use --config to point to an actual config (or use ~/.curator/curator.yml), or\n2. Use --config /dev/null to use all of the defaults: logging is INFO level out to the console, Elasticsearch is at 127.0.0.1, etc.\nI will investigate overriding options with config flags.\n. Curator 4.2.1 is out, and it has the curator_cli endpoint (or actual binary, if you use that package).\nIt should look somewhat familiar to users of Curator 3.x:\n```\n\u00bb curator_cli\nUsage: curator_cli [OPTIONS] COMMAND [ARGS]...\nOptions:\n  --config PATH       Path to configuration file. Default:\n                      ~/.curator/curator.yml\n  --host TEXT         Elasticsearch host.\n  --url_prefix TEXT   Elasticsearch http url prefix.\n  --port TEXT         Elasticsearch port.\n  --use_ssl           Connect to Elasticsearch through SSL.\n  --certificate TEXT  Path to certificate to use for SSL validation.\n  --client-cert TEXT  Path to file containing SSL certificate for client auth.\n  --client-key TEXT   Path to file containing SSL key for client auth.\n  --ssl-no-validate   Do not validate SSL certificate\n  --http_auth TEXT    Use Basic Authentication ex: user:pass\n  --timeout INTEGER   Connection timeout in seconds.\n  --master-only       Only operate on elected master node.\n  --dry-run           Do not perform any changes.\n  --loglevel TEXT     Log level\n  --logfile TEXT      log file\n  --logformat TEXT    Log output format [default|logstash|json].\n  --version           Show the version and exit.\n  --help              Show this message and exit.\nCommands:\n  allocation        Shard Routing Allocation\n  close             Close indices\n  delete_indices    Delete indices\n  delete_snapshots  Delete snapshots\n  forcemerge        forceMerge index/shard segments\n  open              Open indices\n  replicas          Change replica count\n  show_indices      Show indices\n  show_snapshots    Show snapshots\n  snapshot          Snapshot indices\n```\nLearn more about it here.\n. You can't specify multiple hosts with a --host directive, as it only reads in a string and not a list (array).  \nYou can use a YAML configuration file with the singleton command-line, however, and that allows you the flexibility to do that.\nWhy do you need multiple hosts?  Are you trying to round-robin the requests? Because that's all this achieves.... You can also do: curator_cli --config myconfig.yml ..., so that should solve your issue:\n```\n$ curator_cli --help\nUsage: curator_cli [OPTIONS] COMMAND [ARGS]...\nOptions:\n  --config PATH       Path to configuration file. Default:\n                      ~/.curator/curator.yml\n...\n```\nWhen multiple hosts are specified, they are used in a round-robin manner. If Curator is unable to connect to one node, it will attempt up to three connections to a missing/failed node before removing that node from the pool.  It will continue to use the other nodes specified in a round-robin manner.. I am so sorry.  Your issue came just as I was pushing a the documentation change for #769 \nThe bottom line is that Curator cannot work with AWS ES because AWS ES does not support the /_cluster/state/metadata endpoint (see #685).  There is an AWS forum link in #685, and even a comment from one of the core AWS ES developers.\nHow did this happen?  Well, the get_client method is largely carried over from Curator 3.x, and some PRs were submitted to add AWS IAM functionality there.  Curator 4 was developed to support only Elasticsearch 2.0 and higher.  At the time, AWS ES was only on version 1.5, so it was not compatible at all.  After Curator 4 was released, then Amazon released their 2.x version, and it did not support the /_cluster/state/metadata endpoint, which Curator 4 depends on.\nI am improving the documentation in #771 to reflect these things.  I apologize for the inconvenience.\nThis ticket is being closed as a duplicate of #685\n. Can you provide more context? I've never seen that error before. \n. Ah. Curator is not an Elasticsearch plugin. It is a standalone script \n. Since this is a non-issue, I am closing this.\n. What version of the API are you using?  The current version (Curator 4) handles these cases for you already.\nSee https://github.com/elastic/curator/blob/master/curator/utils.py#L349-L370:\ndef chunk_index_list(indices):\n    \"\"\"\n    This utility chunks very large index lists into 3KB chunks\n    It measures the size as a csv string, then converts back into a list\n    for the return value.\n    :arg indices: A list of indices to act on.\n    :rtype: list\n    \"\"\"\n    chunks = []\n    chunk = \"\"\n    for index in indices:\n        if len(chunk) < 3072:\n            if not chunk:\n                chunk = index\n            else:\n                chunk += \",\" + index\n        else:\n            chunks.append(chunk.split(','))\n            chunk = index\n    chunks.append(chunk.split(','))\n    return chunks\nWhich is used in each action, like this example https://github.com/elastic/curator/blob/master/curator/actions.py#L489-L494:\ntry:\n            index_lists = chunk_index_list(self.index_list.indices)\n            for l in index_lists:\n                self.client.indices.open(index=to_csv(l))\n        except Exception as e:\n            report_failure(e)\nCurator 4 does this for you, for all actions.  Curator 3 tries to do this for most of the actions at the command-line level, but you'd have to do it yourself if you were using the API.  The chunk_index_list method is in Curator 3 in some form.\n. So how are you calling close indices?  I'm not following how you could hit this if you're using the proper methods.  Here's a simplified example (no filter args are actually shown):\n```\nclient = elasticsearch.Elasticsearch(host='127.0.0.1')\nilo = curator.IndexList(client)\nilo.filter_by_age(**kwargs)\nadd any other filters as necessary here\n...\nclose = curator.Close(ilo)\nclose.do_action()\n```\nThe do_action method for class Close looks like this:\ndef do_action(self):\n        \"\"\"\n        Close open indices in `index_list.indices`\n        \"\"\"\n        self.index_list.filter_closed()\n        self.index_list.empty_list_check()\n        self.loggit.info(\n            'Closing selected indices: {0}'.format(self.index_list.indices))\n        try:\n            index_lists = chunk_index_list(self.index_list.indices)\n            for l in index_lists:\n                if self.delete_aliases:\n                    self.loggit.info(\n                        'Deleting aliases from indices before closing.')\n                    self.loggit.debug('Deleting aliases from: {0}'.format(l))\n                    try:\n                        self.client.indices.delete_alias(\n                            index=to_csv(l), name='_all')\n                    except Exception as e:\n                        self.loggit.warn(\n                            'Some indices may not have had aliases.  Exception:'\n                            ' {0}'.format(e)\n                        )\n                self.client.indices.flush(\n                    index=to_csv(l), ignore_unavailable=True)\n                self.client.indices.close(\n                    index=to_csv(l), ignore_unavailable=True)\n        except Exception as e:\n            report_failure(e)\nSee: https://github.com/elastic/curator/blob/master/curator/actions.py#L232-L260\nAs you can see, it uses the chunk_index_list method to prevent what you are seeing.  How are you calling your close?\n. Can you please attach the log file?  Preferably a DEBUG level log file.  This should not be happening.  As mentioned in my first response, the index list is truncated at the 3072 byte range.  An additional index name can only be added if the length of all indices is less than 3072 bytes.  They should not be much bigger than 3072 bytes, unless you have very long index names.  That would make yours an edge case.\n. It may be a call somewhere else in the chain that is not chunking.  I know that chunking works, as it's been around since 3.x.  That debug log will be essential to finding where that happened.  At the very least, the original log that had the 4096 error in it will be helpful.\nI am trying to replicate this on my end and cannot :(\nWithout your data I won't know where to look.\n. Thanks for that.  I forgot to update that step.\n. https://www.elastic.co/guide/en/elasticsearch/client/curator/current/python-source.html#_voluptuous covers it now.\n. Thank you for this report.  I will be investigating tomorrow.  In the meanwhile, could you please provide some additional information?\n- Which version of Linux are you using? \n- Which patch level?\n- Which version(s) of Python are you using to test? Both 2 and 3.\n- How are the certificates signed? By which provider?\nKnowing which version of Linux and Python are shipping without the SNI (Subject Name Indication)\nextension to TLS will help me a great deal.  Truthfully, I'm surprised that this error would crop up with the binary package, as it was built with Python 2.7.12 on Ubuntu 16.04, but I will do what I can to verify this.\nAlso, with regards to this error:\n\nTraceback (most recent call last):\nFile \"/usr/local/bin/curator\", line 5, in \nfrom pkg_resources import load_entry_point\nImportError: No module named 'pkg_resources'\n\nPlease install setuptools, per this FAQ entry in the documentation.  Up to date versions are provided in the Curator apt repository, e.g. python3-setuptools.  That will allow you to test this with Python3.\n. > ```\n\n$ apt-get install -y python3-pip python3-elasticsearch-curator=4.1.2\n$ pip3 install -U setuptools\nSuccessfully installed setuptools-28.3.0\n```\n\nThis is unusual.  I suggested the way to do it was to install the version of setuptools I compiled into a package at the same time as the Curator package, e.g.\n$ apt-get install -y python3-setuptools python3-elasticsearch-curator\nI do know that that works, as I have tested it on 16.04 VMs.\nIt's either that, or install everything via pip:\n$ apt-get install -y python3-pip\n$ pip3 install -U setuptools elasticsearch-curator\nMixing those methods may be the issue.\n. @floragunncom \nAre you using Shield, or some other method to secure Elasticsearch?\n. I couldn't even get Curator 4.1.2 to build via pip3 on my 16.04 install.  It seems that something weird happened with the most recent version of setuptools.  It fails during the wheel build, because the wheel build seems intent on downloading the most recent version of setuptools, regardless of what's installed locally.\nThat said, with the current binary package (Python 2.7.12 based), I can confirm part of the behavior you saw above with regards to the SNI error:\n2016-10-10 16:17:11,171 DEBUG              curator.utils             get_client:524  kwargs = {'url_prefix': '', 'aws_secret_key': None, 'http_auth': None, 'certificate': '/home/buh/ca/certs/cacert.pem', 'aws_key': None, 'aws_region': None, 'client_cert': '/home/buh/ca/aaronm-signed.crt', 'hosts': ['steiny.untergeek.net'], 'timeout': 30, 'use_ssl': True, 'master_only': False, 'port': 9200, 'ssl_no_validate': False, 'client_key': '/home/buh/ca/private/aaronm-key.pem'}\n2016-10-10 16:17:11,171 DEBUG              curator.utils             get_client:530  Attempting to verify SSL certificate.\n2016-10-10 16:17:11,174 DEBUG              curator.utils             get_client:567  \"requests_aws4auth\" module present, but not used.\n/opt/elasticsearch-curator/library.zip/urllib3/util/ssl_.py:334: SNIMissingWarning: An HTTPS request has been made, but the SNI (Subject Name Indication) extension to TLS is not available on this platform. This may cause the server to present an incorrect TLS certificate, which can cause validation failures. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n/opt/elasticsearch-curator/library.zip/urllib3/util/ssl_.py:132: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\nEnter PEM pass phrase:\nbut it still connects in the end:\n2016-10-10 16:11:28,359 DEBUG              curator.utils          check_version:427  Detected Elasticsearch version 2.4.1\n2016-10-10 16:11:28,359 DEBUG                curator.cli                    cli:166  client is <class 'elasticsearch.client.Elasticsearch'>\n2016-10-10 16:11:28,359 INFO                 curator.cli                    cli:172  Trying Action ID: 1, \"delete_indices\": Delete indices older than 15 days (based on index name), for any Year.month.day indices. Ignore the error if the filter does not result in an actionable list of indices (ignore_empty_list) and exit cleanly.\n2016-10-10 16:11:28,360 DEBUG                curator.cli         process_action:39   Configuration dictionary: {'action': 'delete_indices', 'description': 'Delete indices older than 15 days (based on index name), for any Year.month.day indices. Ignore the error if the filter does not result in an actionable list of indices (ignore_empty_list) and exit cleanly.', 'filters': [{'direction': 'older', 'stats_result': 'min_value', 'filtertype': 'age', 'source': 'name', 'epoch': None, 'timestring': '%Y.%m.%d', 'exclude': False, 'unit_count': 15, 'unit': 'days'}], 'options': {}}\nSo I have an SNI error, even with a cx_Freeze compiled version from Python 2.7.12.\nI did a test build of the binary package with Python3 3.5.2 and the error goes away:\n2016-10-10 16:16:29,321 DEBUG              curator.utils             get_client:524  kwargs = {'master_only': False, 'client_key': '/home/buh/ca/private/aaronm-key.pem', 'timeout': 30, 'client_cert': '/home/buh/ca/aaronm-signed.crt', 'url_prefix': '', 'ssl_no_validate': False, 'hosts': ['steiny.untergeek.net'], 'certificate': '/home/buh/ca/certs/cacert.pem', 'aws_key': None, 'http_auth': None, 'aws_region': None, 'use_ssl': True, 'port': 9200, 'aws_secret_key': None}\n2016-10-10 16:16:29,322 DEBUG              curator.utils             get_client:530  Attempting to verify SSL certificate.\n2016-10-10 16:16:29,327 DEBUG              curator.utils             get_client:567  \"requests_aws4auth\" module present, but not used.\nEnter PEM pass phrase:\n2016-10-10 16:16:32,043 DEBUG              curator.utils          check_version:427  Detected Elasticsearch version 2.4.1\n2016-10-10 16:16:32,043 DEBUG                curator.cli                    cli:166  client is <class 'elasticsearch.client.Elasticsearch'>\n2016-10-10 16:16:32,043 INFO                 curator.cli                    cli:172  Trying Action ID: 1, \"delete_indices\": Delete indices older than 15 days (based on index name), for any Year.month.day indices. Ignore the error if the filter does not result in an actionable list of indices (ignore_empty_list) and exit cleanly.\nIn both cases, the certificate and keys are connecting to Shield just fine, even with the SNI error.  I'm not sure why SearchGuard isn't playing nice.\n. I know this isn't the news you'd like to hear, but if it works for Shield right now, with Curator functional out of the box using SSL + PKI, then I'm satisfied that the base functionality works as expected with the official Elastic plugins, and there is no major bug needing to be addressed.\nI'm not opposed to merging a pull request to address your issue, but I will not be writing a fix myself. I can only accept a pull request to merge new functionality so long as it doesn't break any existing behaviors.  It would have to test properly in Python 2 and Python 3 on RHEL/CentOS 6 & 7, and Ubuntu 12.04, 14.04, & 16.04, and work with ES versions 2.x and 5.0.\n. Yes.  It works the same as with it set to optional.\nChange this:\n```\nshield.http.ssl.client.auth: optional\nshield.http.ssl.client.auth: required\n```\nRestart, and test again.\nSame result:\n2016-10-10 17:25:25,605 DEBUG              curator.utils             get_client:524  kwargs = {'hosts': ['steiny.untergeek.net'], 'http_auth': None, 'client_key': '/home/buh/ca/private/aaronm-key.pem', 'port': 9200, 'url_prefix': '', 'client_cert': '/home/buh/ca/aaronm-signed.crt', 'master_only': False, 'aws_key': None, 'certificate': '/home/buh/ca/certs/cacert.pem', 'aws_secret_key': None, 'use_ssl': True, 'timeout': 30, 'ssl_no_validate': False, 'aws_region': None}\n2016-10-10 17:25:25,605 DEBUG              curator.utils             get_client:530  Attempting to verify SSL certificate.\n2016-10-10 17:25:25,610 DEBUG              curator.utils             get_client:567  \"requests_aws4auth\" module present, but not used.\nEnter PEM pass phrase:\n2016-10-10 17:25:30,271 DEBUG              curator.utils          check_version:427  Detected Elasticsearch version 2.4.1\n2016-10-10 17:25:30,271 DEBUG                curator.cli                    cli:166  client is <class 'elasticsearch.client.Elasticsearch'>\n2016-10-10 17:25:30,271 INFO                 curator.cli                    cli:172  Trying Action ID: 1, \"delete_indices\": Delete indices older than 15 days (based on index name), for any Year.month.day indices. Ignore the error if the filter does not result in an actionable list of indices (ignore_empty_list) and exit cleanly.\n. #783 addresses the situation where older versions of Python lack the necessary libraries for SSL/TLS support. It does not fix Curator's usability with regards to Search Guard SSL\n. That's easily addressed by upgrading to Curator 4. Filters are processed in the order they appear in the configuration file. \n. I'd change the title to, \"Curator 3 doesn't process filters in a predictable order\"\n. No, not without drastically changing the API.  Curator 3 is pretty dead at this point. I'd only update a security fix.\nI am planning on adding some singleton CLI actions as add-ons to Curator 4, but these are not yet part of any release schedule.\nIs there some reason a user is resistant to Curator 4? \n. I'm changing the label because the issue is resolved in Curator 4.  Yes, it's a bug in 3, but it has been fixed in a subsequent version.\n. @gmoskovicz You should also note that 2016.09.39 is not %Y.%m.%d, as there is no 39th of September.  That's more like %Y.%m.%W, for week number. \n. Actually, it doesn't care.  The pattern is just ^.*(timestamp pattern).*$.  And because it's just matching digits: 4 numbers, a period, 2 numbers, a period, and 2 numbers; Then weeks match the pattern, but it can't be reverse translated to %d for days.  That's what the error is actually doing.   Yes, prefix is not being filtered first, but that's why it sees the pattern there.\nSo, to your point, if you were to have monthly-%Y.%m and daily-%Y.%m.%d, without applying prefix first, the above timestamp pattern would match because 4 digits followed by a period, followed by 2 digits still matches the daily index type too.  Curator 4 allows you to get around this by using an extra filter with exclude set to True, e.g.\n- filtertype: pattern\n  kind: timestring\n  value: '%Y.%m'\n  exclude: False\n- filtertype: pattern\n  kind: timestring\n  value: '%Y.%m.%d'\n  exclude: True\nWithout calculating any dates, this would filter to keep only 4 digit + . + 2 digit timestring patterns.\nOf course, it could also be handled by prefix first, too, should the prefixes differ.\n. I'm kind of confused that you are trying to connect to localhost in your configuration file, but are using AWS IAM credentials.  I will address each of the scenarios I can conceive of, based on what I'm seeing.\nIAM Credentials and AWS ES\nThe first thing you should know, before you go any farther, is that Curator 4 does not work with AWS ES and IAM credentials: https://www.elastic.co/guide/en/elasticsearch/client/curator/current/faq_aws_iam.html\nThey remain as place-holders in case Amazon AWS ES ever allows the cluster state metadata endpoint in the future.\nThe main reason for your observation is at https://github.com/elastic/curator/blob/v4.1.2/curator/utils.py#L558\nThe override only happens if the AWS IAM key, secret key and region (all three) are set. If you're connecting with IAM credentials, it forces use_ssl to True, because that's how those work, and that is required.  That's why it is forcing SSL, even though you haven't set that.\nElasticsearch on own EC2 instances\nIf you are using your own Elasticsearch nodes in your own AWS EC2 instances, then you do not need IAM credentials, and they won't work for you in this case anyway.  Don't populate those fields in the configuration YAML file if this is the case. \n. If you run it again with loglevel: DEBUG and attach the result, I'll be able to tell you.\n. Thanks.  Here's what I'm finding.\nThe pattern filter is working exactly as expected.  Only logstash-* indices are kept.\nThen it begins with the age filter, using creation date, and keeping everything older than 5 days from right now.  That is in epoch time: 1476037644, which is visible in the log file as the point of reference, (1476037644).  Converted to a readable format, that looks like Sun, 09 Oct 2016 18:27:24 GMT.\nSo let's look at one that you would think would be excluded from the actionable list: logstash-2016.10.11. Clearly, an index with a date stamp only 3 days in the past should be removed, and it is.  But the date stamp should surprise you:\nRemoved from actionable list: Index \"logstash-2016.10.11\" age (1476443012), direction: \"older\", point of reference, (1476037644)\nWhen I convert 1476443012 to a readable format, it says: Fri, 14 Oct 2016 11:03:32 GMT.  Wait.  That index says it was created at 11:03 GMT, which is roughly 7 hours ago.  The creation_date was only seven hours ago, not the expected number of days in the past.\nLet's take one that should have remained in the actionable list and try the same process.\nRemoved from actionable list: Index \"logstash-2016.10.03\" age (1476443142), direction: \"older\", point of reference, (1476037644)\nThe age, based on creation_date here, is 1476443142, which translates to a readable date of Fri, 14 Oct 2016 11:05:42 GMT\nWhat's going on is that whatever is creating these indices is doing it all at once, and it only started about 7 to 8 hours ago.\nNow, if you want to change the number of replicas still on these indices, use name instead of creation_date.  Your age filter would look like:\n- filtertype: age\n  source: name\n  timestring: %Y.%m.%d\n  direction: older\n  unit: days\n  unit_count: 5\nYou don't need to have exclude there if it is empty.  This should yield the expected result because it calculates age based on the timestamp in the index name, not when Elasticsearch created the index.\n. I'm guessing you have Logstash reading a bunch of older files all at once and are back-filling your data.  That would explain why creation_date is so recent, instead of the date of the first document entry.\n. Hi, @syndy1989 \nAs this is a usage question, please feel free to ask it at https://discuss.elastic.co/c/elasticsearch where the community can benefit from helping answer your question, and future users can benefit from the answer you receive.. Please ask this question at https://discuss.elastic.co/c/elasticsearch. What's happening is that when it attempts to do the \"close\" action, there are no indices that need to be closed that match the filters provided, hence the No actionable items in the list message.\nIt may be that this is the case, that they're already closed.  This would result in an empty list of indices.  This raises an exception, internally, because you can't close indices if there aren't any in the list.  Curator provides a way to ignore an empty list condition.  For example, you are setting up Curator to close indices older than 30 days, but you only have 10 days of indices right now.  This would result in an empty list condition, but it is expected.  Anytime where an empty list is a non-fatal exception, you can skip the current action and to move on to the next action in the configuration by setting ignore_empty_list: True in the options section.\nIf you switch to log in DEBUG mode, you'll have a much more complete list of what is going on.  If you want me to help continue troubleshooting this, please attach a debug log (sanitize it of sensitive information if needed).\n. Try running with loglevel: DEBUG in  your curator.yml file.  You should see which indices are found matching your pattern for action id 2.\n. A few more lines might be more instructive.  You can also attach the file by dragging & dropping or selecting them (see the message at the bottom of the comment entry area).\nIt appears, however, that it can only find one index that is older than 10 days that scheduled for deletion.\n. So, here's an example.\n2016-10-19 08:29:18,983 DEBUG          curator.indexlist            __excludify:58   Removed from actionable list: Index \"***-all-***-2016-09-29\" age (1476283462), direction: \"older\", point of reference, (1476016158)\nThis entry has a date stamp on the index name of 2016-09-29.  But the creation date is 1476283462, which is Wed, 12 Oct 2016 14:44:22 GMT.  Your point of reference, which is 10 days before the time of execution, is 1476016158, or Sun, 09 Oct 2016 12:29:18 GMT.\nI can see by this that your data is older than the index it was put into.  If you need to delete this older data, perhaps you should change your age filter to be:\n- filtertype: age\n      source: name\n      direction: older\n      timestring: '%Y-%m-%d'\n      unit: days\n      unit_count: 10\nTry that with --dry-run and see if that does what you expect.   Using source: name changes the source of age to be derived from the index name, rather than creation_date, which is when the index was created in Elasticsearch.  It's apparent that you've ingested some older data.\n. Curator doesn't consider either to be old enough to be acted on.\nPoint of reference: 1476016158, which is Sun, 09 Oct 2016 12:29:18 GMT. The point of reference is unit_count * unit * # of seconds in a \"unit\" subtracted from the epoch time at Curator run time.  \nThe times are measured exactly, and a strict greater than or less than marks the cutoff.  By that measure, those indices are not going to be caught by Curator, as is indicated in the log file:\n\"***-***-en-2016-10-09\" age (1476022761) is Sun, 09 Oct 2016 14:19:21 GMT, which is almost 2 hours younger than the cutoff, so it doesn't get deleted.\n\"***-***-en-2016-10-09\" age (1476019759) is Sun, 09 Oct 2016 13:29:19 GMT, which is one hour younger than the cutoff, so it doesn't get deleted.\n. As it's been 2 hours since my last comment, if you re-run with no changes, you should see these indices acted on.  Is that the case?\n. Thanks for catching that! I did so many examples at 4.0 release time that I must have overlooked that one.\n. Long story short: Yes, the different exceptions are acted on differently.\nThe exceptions caught are acted on differently.  First, Curator catches all exceptions.  Then it tests to see if it was an empty list exception.  If it is an empty list exception, it will act according to the conditionals.  Otherwise, regardless of exception, it will test for continue_if_exception and act accordingly.\nSo if your case is not fatal from an empty list, you should use ignore_empty_list: True, rather than continue_if_exception: True, as the two are mutually exclusive.\n. The readonlyrest-plugin is not an officially supported/recognized plugin of the Elastic Stack.  As such, Curator is not designed to handle its altered behavior.  There's nothing I can do to help you here.\n. Not necessarily.  What OS are you using?  I see that you're showing a 3.5.2 version of Python, but Curator is the binary package (which was originally compiled using Python 2.7)\n\n/opt/elasticsearch-curator/library.zip/urllib3/util/ssl_.py:334: SNIMissingWarning:\n/home/vagrant/.local/lib/python2.7/site-packages/click/core.py\n\nOlder versions of Python are at fault here.\nSee #780 and #783.\nI haven't pushed out the new Python 3.5-based packages yet.  I can make an updated one for you to test, if you like.  I perfected the build process already, but haven't updated Curator, so saw no reason to do a version bump just for this (unless a customer needs it).\n. Because of these changes, when I release Curator 5, I may only release a binary package.  It saves me the grief of having to support older versions of Python.\n. I have builds from a few days ago for Centos 6 & 7, and All recent LTS versions of Ubuntu (12,14,16.04), and it should also work on Debian 8\n. I tested against Shield with self-signed PKI certs with these packages, so I assume they should be okay.\n. Python wasn't pointing at 2.7.  That's how the binary package works.  It's a bundled version.\nTry installing this over the top with dpkg: https://dl.dropboxusercontent.com/u/1085030/elasticsearch-curator_4.1.2_amd64.deb\nEither that, or uninstall and then install this with dpkg.\n. > Either that, or uninstall and then install this with dpkg.\nTry doing apt purge elasticsearch-curator.  You may need to force reinstall first.\nAfter a full uninstall, then try installing the new package.\n. Yes!  Thanks for confirming!\n. @nellicus is this for a customer? I plan to release 4.2.0 really soon, and the new binary packages will be based on this build method.\n. Cool.  Thanks for helping me test on one more platform.  I feel confident moving forward with the binary package solution now.  Feel free to close this if you're satisfied.\n. I understand the pain, but the difficulty here is that Curator will never know what may or may not have happened from any other API call somewhere else.  It's a dangerous assumption that Curator is the only thing that may have caused a change in the cluster state metadata endpoint.  How can Curator guarantee that nothing changed?  By pulling it again and comparing it to the cached version, which kind of invalidates the time and data savings.\nI'm very wary of the potential to mess something up in the cluster with this approach, especially since some current actions, and some future planned ones, have the potential to change the cluster state in exactly the ways described (rollover API, reindex API, shrink API, etc).\n. For the record, I'm not saying no here.  I'm still puzzling out a way to potentially do this, allowing the end user to take responsibility for the inherent risks.\n. My biggest worry is that it would be an overhaul of the IndexList API, as each action creates a new IndexList, ergo a new pull of the metadata.\n. There are two things in play here: Cluster state metadata and whether an index is present or not.\nI have some ideas on how to preserve metadata, and check for index presence if using cached metadata.\n. > That's an interesting idea. Cache the metadata, potentially, but perform existence and openness checks (since they are cheaper). Is that what you're thinking?\nExactly, except that openness is in the metadata check.  I could potentially \"update\" the state in the stored metadata according to what actions were taken against which indices in the instance of open or close actions.\nI'm not sure that task history will come into play much, but I will experiment.  It would be a 5.0 only feature, though.\n. TransportError(401, u'{\"Message\":\"Your request: \\'/_snapshot/_status\\' is not allowed.\"}')\nIt appears that the snapshot status endpoint is not available in AWS ES 2.x.  This is outside the control of Curator.  I don't recall anyone complaining about this in AWS ES 1.x, but I may just not have had any reports of it.\n. I've added a note about this to the compatibility matrix in the README.\n. @yissachar I'm sorry I didn't connect the dates with that.  You're right, and I'll update the compatibility matrix.\n. This is apparently still true as of #987, even with AWS ES 5.3. It's a fair question.  The reason everything is in UTC is because internal to Elasticsearch, everything is in UTC.  The snapshot metadata and index creation date are in epoch format, which is UTC.  Logstash pushing year/month/day index names does rollover at UTC 0:00.  Another reason is that snapshot names have a narrow list of what's acceptable, and adding a timezone information would result in being human readable, but not easily re-parsed back, as only lower-case letters and limited punctuation are allowed in snapshot names.\nCurator now does everything in UTC as well.  It would introduce a lot of complexity to handle timezones.  I would have to do it for everything in Curator, and I'm not sure that's something I want to pursue.  This is admittedly in part because it will be a lot of support work trying to answer questions about time zone support, native time zones, people who have things start breaking because they did some weird hack to get around this, etc.  It's been both convenient and predictable to keep things at UTC, as it's not unfamiliar to users of Curator.\nI understand it's inconvenient to have to reckon against UTC, but I don't plan on changing this.  If someone wanted to undertake the monumental task of adding timezone support, not just for a narrow use case but across the board, I would probably merge it.  I'm not anticipating that happening, though.\n. That's because of a python version mismatch. I'm guessing someone installed\nvia pip and it installed v5.0.0 of the Elasticsearch python module.\nThis is already fixed in master. It can be fixed by running\npip install -U elasticsearch==2.4.0\n-- Aaron Mildenstein\nOn October 28, 2016 at 1:07:37 PM, Jay Greenberg (notifications@github.com)\nwrote:\n\nRunning Curator 4.1.2 Against ES 2.3 yields the following error (\nRequirement.parse('elasticsearch<5.1.0,>=2.4.0')), even though docs\nhttps://github.com/elastic/curator#compatibility-matrix suggest\notherwise.\nTraceback (most recent call last):\n  File \"c:\\users\\kramases\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\pkg_resources__init__.py\", line 635,\nin _build_master\n    ws.require(requires)\n  File \"c:\\users\\kramases\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\pkg_resources__init__.py\", line 943,\nin require\n    needed = self.resolve(parse_requirements(requirements))\n  File \"c:\\users\\kramases\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\pkg_resources__init__.py\", line 834,\nin resolve\n    raise VersionConflict(dist, req).with_context(dependent_req)\npkg_resources.ContextualVersionConflict: (elasticsearch 2.3.0 (c:\\users\\kramases\\appdata\\local\\programs\\python\\python35\\\nlib\\site-packages), Requirement.parse('elasticsearch<5.1.0,>=2.4.0'), {'elasticsearch-curator'})\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/elastic/curator/issues/798, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AA-R35dFlVX8bzEo3EII3Niv3KgnhWmjks5q4kf5gaJpZM4Kjwla\n.\n. Duplicate of #787, fixed in #788.\n. huh?  Who told you that Curator doesn't work with ES 5.0?  Did you see the compatibility matrix on the home page here?  https://github.com/elastic/curator\n. You need to sign the CLA before I can merge this\n. Yes.  You are supposed to create your own yaml files.  \n\nThe configuration file can be in ~/.curator/curator.yml, in which case you don't have to specify it on the command line.  The example for this file is at https://www.elastic.co/guide/en/elasticsearch/client/curator/current/configfile.html.  You could cut/paste the example file right there at the top and change the values to your own, e.g.\n```\nRemember, leave a key empty if there is no value.  None will be a string,\nnot a Python \"NoneType\"\nclient:\n  hosts:\n    - 127.0.0.1\n  port: 9200\n  url_prefix:\n  use_ssl: False\n  certificate:\n  client_cert:\n  client_key:\n  ssl_no_validate: False\n  http_auth:\n  timeout: 30\n  master_only: False\nlogging:\n  loglevel: INFO\n  logfile:\n  logformat: default\n  blacklist: ['elasticsearch', 'urllib3']\n```\nChange 127.0.0.1 to be the IP of your Elasticsearch client node.\nAs for the action.yml file, you can use the example at the URL you provided, https://www.elastic.co/guide/en/elasticsearch/client/curator/current/ex_delete_indices.html and edit as needed:\n```\nRemember, leave a key empty if there is no value.  None will be a string,\nnot a Python \"NoneType\"\n\nAlso remember that all examples have 'disable_action' set to True.  If you\nwant to use this action as a template, be sure to set this to False after\ncopying it.\nactions:\n  1:\n    action: delete_indices\n    description: >-\n      Delete indices older than 45 days (based on index name), for logstash-\n      prefixed indices. Ignore the error if the filter does not result in an\n      actionable list of indices (ignore_empty_list) and exit cleanly.\n    options:\n      ignore_empty_list: True\n      timeout_override:\n      continue_if_exception: False\n      disable_action: True\n    filters:\n    - filtertype: pattern\n      kind: prefix\n      value: logstash-\n      exclude:\n    - filtertype: age\n      source: name\n      direction: older\n      timestring: '%Y.%m.%d'\n      unit: days\n      unit_count: 45\n      exclude:\n```\nThis bare example would need disable_action: False, or it will ignore the action (this is a safety feature so users copying the example won't accidentally do something they do not intend).  This action will purge all indices starting with logstash- and older than 45 based on the date in the index name.  If you have logstash- prefixed indices, you could leave this as-is, otherwise replace logstash- with the prefix of your choice, and unit_count: 45 should perhaps be changed to unit_count: 60 for 60 days.\nSave this file to whatever name and path you want, e.g. /path/to/my_action.yml.  You would then call:\n$ curator --dry-run /path/to/my_action.yml\nAnd make sure the indices set to be deleted are exactly what you expect, and no others.  You may need to add or tweak the filters for your delete action a bit.  Once you're satisfied, re-run the above command, omitting the --dry-run portion to actually execute the action.\n. Sure. Try the patterns, minus the *, from Kibana.\n-- Aaron Mildenstein\nOn November 7, 2016 at 6:36:28 PM, elJoeyJojo (notifications@github.com)\nwrote:\n\nhey @untergeek https://github.com/untergeek\nthanks for your quick response! I really appreciate it.\nall looks pretty clear except I don't use Logstash, it simply outputs with\nElasticSearch, so would the value be according to my metrics? For example I\nam only really using metricbeat, so would my value be \"metricbeat-*\" as it\nis in the index patterns of Kibana?\nmy indices in /var/lib/elasticsearch/nodes/0/indices how as this:\n4.0GiB [##########] /XIKf-5NJQR-6rqxg3ql9gQ\n3.8GiB [######### ] /Ob5z2GU3SiWRpy2_oCh2BA\n3.8GiB [######### ] /2ye87jZfQO2U1Rtc5z5Ckg\n3.7GiB [######### ] /ZZ7uG0K6QrqnE3ibkxD8ew\n3.6GiB [######### ] /FAxFrGoHR5Wcu_VpQTdUbg\n3.6GiB [######### ] /8mANDmBoSIGrPik65918SA\n3.6GiB [######### ] /eDMhPPjnT3WS9EC55LBiLA\n3.6GiB [######### ] /qAFjD_IOTVek6yjTN7UIAA\n3.6GiB [######### ] /8IyOE-DTT_Cs6AQZr_RF8g\n2.9GiB [#######   ] /SZLcXCyQSjatitIpUdyONQ\n1.2GiB [###       ] /OyRlR3C9Q0uXU5xq_jAEVg\n200.2MiB [ ] /UH9ixRCgS0iuIxEEagyA6A\n252.0KiB [ ] /xHiYJWY4T7WEJKHefYYoow\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/elastic/curator/issues/804#issuecomment-259019432,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AA-R30sdhIDwd72nPkjemt6JMHILCWlsks5q79IcgaJpZM4Kr6gV\n.\n. Going to close this now.  Please ask other questions at https://discuss.elastic.co\n. I apologize for the hassle.  As you've discovered, it is because I neglected to use rm -rf in the pre-remove script.  Hopefully this will not be an issue for future releases.  The better workaround for the (hopefully) few users who are using the binary package is to uninstall, then reinstall.  If an upgrade fails, sudo dpkg --purge elasticsearch-curator followed by a reinstall (as with your workaround) does the job.\n. Another reason it failed is that the files are completely different.  4.2.1 is built using Python 3.5.2, where 4.1.2 was still Python 2.7.12 (which still did not address the SSL problems).\n. Curator doesn't support single-digit month notation, even though Python does, due to the regular expression hoops that would be required to verify and properly extract the dates and re-translate them back again.  You might be able to just disable the safety checks that ensure %m is present for --time-unit months, but I would not test that theory without running a --dry-run\n\nIf you were using Elasticsearch 2.x or newer, I would suggest upgrading to the current version of Curator (which just recently re-added 3.x style command-line actions) and using the index creation_date to filter.\nSince you aren't using Elasticsearch 2.x or newer, then I'm afraid I can't do anything else to help you.\n. Yes, that line is the one. You have to comment that out to bypass the check. \n. Yeah.  The parser is kind of limited in that way.  It was expected that the people naming things would remember to make them uniform, so they are parsable.\n. Sigh.\nThis is what I get for migrating to precompiled stuff, you know?  I miss silly bits like this.\n. You'll probably have to wait for me to get back to the 'States to do a release with this.\nI'll try to do it remotely, though.  The windows build will be the hard one...\n. It shows /home/vagrant because it was compiled in a vagrant instance.  It's running everything from a virtual directory.  \nIt appears to be looking for something in a directory, and not finding it, which I don't think is necessarily related to the virtual paths, as I have tested this binary against Shield/Security enabled instances manually (though not against Elastic Cloud).  When I've tested it, it's been with self-signed certificates and I've specified --certificate, --client-cert, and --client-key.  You have specified none of these, so I wonder if that's related, that it's not able to find the certifi-provided certificates in the virtual filesystem, which should be bundled.\nCould you please run this with --loglevel DEBUG and attach the output?  Hopefully it will have more information.\n. Confirmed (you don't have to send logs).  Troubleshooting, though I'm traveling today.  It may be next week before I get to it.\n. It's not able to find the cacert.pem that is bundled with Certifi in the virtual path.  While I find a way around this, please use the certifi-cacert.pem this zip file with --certificate /path/to/certifi-cacert.pem.\n. Have you looked at using source: field_stats (see here) with the age filter?  So long as you are looking for a timestamp field, this may do what you want.\nOtherwise, the filter you describe would require a query, as that data does not appear in the cluster state metadata. Here's a sample of the data in the 'settings' key:\n{u'index': {u'number_of_replicas': u'1', u'uuid': u'GV0wgrQSSrOrcGo5kGTGNg', u'number_of_shards': u'3', u'creation_date': u'1472688005429', u'version': {u'upgraded': u'5000099', u'created': u'2030399'}, u'refresh_interval': u'5s'}}\nThis is where creation_date comes from, as well as the number of shards and replicas.  There is no \"last touched\" date in here, unfortunately.  If you wanted to write an extra filter that does queries, I suppose it could be merged.  It's not something that's on my radar, though.\n. Yep!\n. Keep in mind, this only checks for the max value. Hypothetically, if you kept on getting younger values, somehow, it would still appear as though it hadn't been touched since max_value. This is highly unlikely, but thought you should know. \n. Yes, that's correct. But in super-rare situations, I've seen a tailed syslog file send old data. That's the hypothetical I mentioned. It's unusual, and probably redundant/repeated data. \n. Maybe. I'll see what I can do\n-- Aaron Mildenstein\nOn November 18, 2016 at 3:38:43 PM, Christian Winther (\nnotifications@github.com) wrote:\n\nHi,\nWould it be possible to expose ignore_empty_list for the \"new\" CLI ?\nOn a new cluster, the delete_indices fail since there currently is no\nindices older than the $x of filter_list, and it's not possible to\nconfigure that as expected from the CLI tool.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/elastic/curator/issues/812, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AA-R32J0qLPVsLVrojrGdPc0O_g_kZA0ks5q_caDgaJpZM4K2nE2\n.\n. Curator 4.2.1 is compiled and comes with an embedded version of Python 3.5.2 to remove dependencies and prevent SSL errors that come from using old versions of Python. \n\nI will document this for the upcoming release. . Thanks for reporting the fix. . Serious weirdness with this branch. It should be clean now, though.. This is because I wrongly assumed PyPI would pick up the dependency from requirements.txt first.  Fix coming to master soon (though subsequent installs via pip won't have the problem since they'll already have certifi installed).. v4.2.3.post1 is in PyPI now. Yes, it should.  Something is off, I will look into it.. Sorry for the long wait here.  This filter is doing what it is supposed to do, just not in the way you are perhaps expecting.  It hinges on the meaning of exclude in the context of the filter. The tricky thing about the term exclude is that it is in reference to matching values.\nIf exclude: True, which is the default for the count filter, exclude matching values from the actionable list.  Using your configuration, that will remove the 10 most recent indices from the actionable list, leaving everything else older than 45 days.\nIf exclude: False, then exclude non-matching values from the actionable list (which is figuratively an include).  If you added exclude: False to your configuration, then the result would be that only the 10 most recent indices of those older than 45 days would be selected for snapshot.\nDoes that help?\nHere's a real-world example output. \nFirst, I'm looking at my snapshots here with this filter block:\nfilters:\n  - filtertype: pattern\n    kind: prefix\n    value : daily\n  - filtertype: age\n    source: creation_date\n    direction: older\n    unit: days\n    unit_count: 45\n  - filtertype: count\n    count: 10\nAnd this is in the debug output:\n2016-12-07 15:43:54,107 DEBUG     curator.validators.SchemaCheck               __init__:27   \"filter\" config: {u'count': 10, 'reverse': True, 'use_age': False, u'filtertype': u'count', 'timestring': None, 'exclude': True}\n2016-12-07 15:43:54,108 DEBUG       curator.snapshotlist        iterate_filters:425  Parsed filter args: {u'count': 10, 'reverse': True, 'use_age': False, u'filtertype': u'count', 'timestring': None, 'exclude': True}\n2016-12-07 15:43:54,108 DEBUG              curator.utils        iterate_filters:434  Filter args: {u'count': 10, 'reverse': True, 'use_age': False, 'timestring': None, 'exclude': True}\n2016-12-07 15:43:54,108 DEBUG              curator.utils        iterate_filters:435  Pre-instance: [u'daily-20160701031655', u'daily-20160701083002', u'daily-20160702083900', u'daily-20160703084843', u'daily-20160705083855', u'daily-20160708085232', u'daily-20160709090033', u'daily-20160710083907', u'daily-20160712083855', u'daily-20160713084629', u'daily-20160714083833', u'daily-20160715083822', u'daily-20160716084106', u'daily-20160717084108', u'daily-20160718083830', u'daily-20160719083827', u'daily-20160720084338', u'daily-20160721083901', u'daily-20160722083839', u'daily-20160722221125', u'daily-20160723083830', u'daily-20160724083900', u'daily-20160725083839', u'daily-20160726000633', u'daily-20160726000817', u'daily-20160726083841', u'daily-20160727083833', u'daily-20160728084118', u'daily-20160729083912', u'daily-20160730083834', u'daily-20160731084055', u'daily-20160801083854', u'daily-20160801221454', u'daily-20160802084515', u'daily-20160803083833', u'daily-20160804084346', u'daily-20160805083832', u'daily-20160806084350', u'daily-20160807083851', u'daily-20160808084112', u'daily-20160809083904', u'daily-20160810083911', u'daily-20160811083843', u'daily-20160812083849', u'daily-20160813083824', u'daily-20160814083836', u'daily-20160815083844', u'daily-20160816083837', u'daily-20160817083905', u'daily-20160818084124', u'daily-20160819083955', u'daily-20160820083931', u'daily-20160821083850', u'daily-20160822084136', u'daily-20160823084147', u'daily-20160903094901', u'daily-20160904084103', u'daily-20160905084007', u'daily-20160906084027', u'daily-20160907083804', u'daily-20160908083818', u'daily-20160909083757', u'daily-20160910083758', u'daily-20160911083751', u'daily-20160912083747', u'daily-20160912151125', u'daily-20160913083747', u'daily-20160914083757', u'daily-20160915083801', u'daily-20160916083749', u'daily-20160917083801', u'daily-20160918083749', u'daily-20160919083744', u'daily-20160920083746', u'daily-20160921083800', u'daily-20160922083745', u'daily-20160923083801', u'daily-20160924083756', u'daily-20160925083810', u'daily-20160926083757', u'daily-20160927083807', u'daily-20160928084400', u'daily-20160929083950', u'daily-20160930083758', u'daily-20161001195928', u'daily-20161002083003', u'daily-20161003085357', u'daily-20161004083805', u'daily-20161005083800', u'daily-20161006083809', u'daily-20161007084018', u'daily-20161008083805', u'daily-20161009083753', u'daily-20161010083805', u'daily-20161014165527', u'daily-20161015083531', u'daily-20161016083525', u'daily-20161017083535', u'daily-20161018083522', u'daily-20161019083537', u'daily-20161020083523', u'daily-20161021083516', u'daily-20161022083525', u'daily-20161023083514']\n2016-12-07 15:43:54,108 DEBUG       curator.snapshotlist        filter_by_count:362  Filtering snapshots by count\n2016-12-07 15:43:54,108 DEBUG       curator.snapshotlist       __not_actionable:51   Snapshot daily-20161023083514 is not actionable, removing from list.\n2016-12-07 15:43:54,108 DEBUG       curator.snapshotlist            __excludify:71   Removed from actionable list: daily-20161023083514 is 1 of specified count of 10.\n2016-12-07 15:43:54,108 DEBUG       curator.snapshotlist       __not_actionable:51   Snapshot daily-20161022083525 is not actionable, removing from list.\n2016-12-07 15:43:54,108 DEBUG       curator.snapshotlist            __excludify:71   Removed from actionable list: daily-20161022083525 is 2 of specified count of 10.\n2016-12-07 15:43:54,109 DEBUG       curator.snapshotlist       __not_actionable:51   Snapshot daily-20161021083516 is not actionable, removing from list.\n2016-12-07 15:43:54,109 DEBUG       curator.snapshotlist            __excludify:71   Removed from actionable list: daily-20161021083516 is 3 of specified count of 10.\n2016-12-07 15:43:54,109 DEBUG       curator.snapshotlist       __not_actionable:51   Snapshot daily-20161020083523 is not actionable, removing from list.\n2016-12-07 15:43:54,109 DEBUG       curator.snapshotlist            __excludify:71   Removed from actionable list: daily-20161020083523 is 4 of specified count of 10.\n2016-12-07 15:43:54,109 DEBUG       curator.snapshotlist       __not_actionable:51   Snapshot daily-20161019083537 is not actionable, removing from list.\n2016-12-07 15:43:54,109 DEBUG       curator.snapshotlist            __excludify:71   Removed from actionable list: daily-20161019083537 is 5 of specified count of 10.\n2016-12-07 15:43:54,109 DEBUG       curator.snapshotlist       __not_actionable:51   Snapshot daily-20161018083522 is not actionable, removing from list.\n2016-12-07 15:43:54,109 DEBUG       curator.snapshotlist            __excludify:71   Removed from actionable list: daily-20161018083522 is 6 of specified count of 10.\n2016-12-07 15:43:54,109 DEBUG       curator.snapshotlist       __not_actionable:51   Snapshot daily-20161017083535 is not actionable, removing from list.\n2016-12-07 15:43:54,109 DEBUG       curator.snapshotlist            __excludify:71   Removed from actionable list: daily-20161017083535 is 7 of specified count of 10.\n2016-12-07 15:43:54,109 DEBUG       curator.snapshotlist       __not_actionable:51   Snapshot daily-20161016083525 is not actionable, removing from list.\n2016-12-07 15:43:54,109 DEBUG       curator.snapshotlist            __excludify:71   Removed from actionable list: daily-20161016083525 is 8 of specified count of 10.\n2016-12-07 15:43:54,109 DEBUG       curator.snapshotlist       __not_actionable:51   Snapshot daily-20161015083531 is not actionable, removing from list.\n2016-12-07 15:43:54,109 DEBUG       curator.snapshotlist            __excludify:71   Removed from actionable list: daily-20161015083531 is 9 of specified count of 10.\n2016-12-07 15:43:54,109 DEBUG       curator.snapshotlist       __not_actionable:51   Snapshot daily-20161014165527 is not actionable, removing from list.\n2016-12-07 15:43:54,110 DEBUG       curator.snapshotlist            __excludify:71   Removed from actionable list: daily-20161014165527 is 10 of specified count of 10.\n2016-12-07 15:43:54,110 DEBUG       curator.snapshotlist           __actionable:46   Snapshot daily-20161010083805 is actionable and remains in the list.\n2016-12-07 15:43:54,110 DEBUG       curator.snapshotlist            __excludify:71   Remains in actionable list: daily-20161010083805 is 11 of specified count of 10.\n2016-12-07 15:43:54,110 DEBUG       curator.snapshotlist           __actionable:46   Snapshot daily-20161009083753 is actionable and remains in the list.\n2016-12-07 15:43:54,110 DEBUG       curator.snapshotlist            __excludify:71   Remains in actionable list: daily-20161009083753 is 12 of specified count of 10.\n2016-12-07 15:43:54,110 DEBUG       curator.snapshotlist           __actionable:46   Snapshot daily-20161008083805 is actionable and remains in the list.\nWhat results afterward is:\n```\nlots and lots of other snapshots here...\n...\ndaily-20161007084018\ndaily-20161008083805\ndaily-20161009083753\ndaily-20161010083805\n``\nNotably missing are:u'daily-20161014165527', u'daily-20161015083531', u'daily-20161016083525', u'daily-20161017083535', u'daily-20161018083522', u'daily-20161019083537', u'daily-20161020083523', u'daily-20161021083516', u'daily-20161022083525', u'daily-20161023083514'`, which were removed from the actionable list.\nIn order to have only the most recent 10, I would set exclude: False, which will select only the matches by excluding non-matching values.  This is what pops out if I do that:\ndaily-20161014165527\ndaily-20161015083531\ndaily-20161016083525\ndaily-20161017083535\ndaily-20161018083522\ndaily-20161019083537\ndaily-20161020083523\ndaily-20161021083516\ndaily-20161022083525\ndaily-20161023083514\nAnd if I wanted only the oldest 10, I'd add reverse: False and exclude: False, which gives these (the oldest I have):\ndaily-20160701031655\ndaily-20160701083002\ndaily-20160702083900\ndaily-20160703084843\ndaily-20160705083855\ndaily-20160708085232\ndaily-20160709090033\ndaily-20160710083907\ndaily-20160712083855\ndaily-20160713084629\nDoes that help?  Hopefully this makes it more understandable.  exclude is tricky, but I needed the behavior to be consistent between filters, so it defines behavior relative to matching values.. I'm going to close this for now.  If you feel that I'm doing so in error, please feel free to re-open it, or open a new issue.. If you look at the compatibility matrix, you'll see that Elasticsearch 5.0 is fully supported with all current actions and filters. The elasticsearch-py module, version 2.4, supports all necessary API calls to allow Curator to work with Elasticsearch versions 2.0 through 5.0. \nThe python module version shift you saw is because version 5.0 of the python module has some API changes that caused integration tests to fail. When Curator v5 comes out it will be refactored to use the v5 python module. . Indeed. Curator is for index-level management. A feature to delete data within an index is outside the scope of Curator's intended reach. . Is there an API call that does pull the .security index in 2.4.1?  Curator can only show what the Elasticsearch API reveals. . @PhaedrusTheGreek I'm not sure this is an easy fix, though it should be doable.  It will require an ugly workaround that may not even be effective unless the provided credentials have all required permissions.  This will likely result in a lot of ugly extra error and/or warning messages suggesting that .security may or may not be there, and Curator can't tell exactly.  The steps \n\nFigure out which version of Elasticsearch (2.x or 5.x)\nIf 2.x, then test for the presence of .security manually.  This can fail for so many reasons I'm not sure if my tests will be conclusive. Log warnings/errors if not found or it cannot be accessed by the provided credentials.\nManually add the .security index to the IndexList object and its metadata.. The proper way to fix this is to upgrade to 2.4.2, which mimics the 5.x behavior of including .security in a wildcard search.  \n\nThere is no other workaround, as earlier 2.x versions disallow closing and deleting of the .security index, making it impossible to restore from a snapshot.. My misunderstanding. I'll reopen this. . This has nothing to do with Curator and everything to do with the Elasticsearch 5 API. The name you assign is only visible via API call. The same thing applies to indices. You should not be looking at the file system for information about either, but only the API. . See https://www.elastic.co/guide/en/elasticsearch/client/curator/current/restore.html\nNote that leaving name blank for the restore action automatically selects the most recent snapshot. . Thanks for reporting back that it worked after the cluster restart.\nI'm not sure why it shows two running.  I'll take a look at adding the status in another column with --verbose output at some point.  It probably means one of them is flagged IN_PROGRESS and the other is something else.\n. I'm not seeing this issue in the master branch anymore.  It's probably because I'm using elasticsearch-py version 5.1.0, so it's a better API matchup.. 1 request, 1 question:\nCan you attach more logs from Curator?  The error alone does not paint a complete picture.\nWhat was the timeout value you used, or the timeout override?. Not sure how you're doing it, but you do seem to have concurrent snapshots running.\nYour curl statement shows a snapshot with a name of: backup-2016-11-24-v0.3, which differs from the naming scheme defined in your Curator snapshot action file.\nCurator is naming its snapshot with the default of: curator-%Y%m%d%H%M%S (determined by leaving the name blank) as evidenced by this log line:\n2016-11-28 10:01:04,780 ERROR                curator.cli                    cli:193  Failed to complete action: snapshot.  <class 'curator.exceptions.FailedExecution'>: Exception encountered.  Rerun with loglevel DEBUG and/or check Elasticsearch logs for more information. Exception: TransportError(503, u'concurrent_snapshot_execution_exception', u'[es-snapshots:curator-20161128100004] a snapshot is already running')\nYour curator snapshot is named curator-20161128100004\nDo you have multiple, concurrent Curator and/or scripted snapshots running?  You cannot run multiple snapshots at once.  The reason is that Elasticsearch is obliged to mark and hold all segments to-be-snapshotted and prevent them from being merged or otherwise altered.  Elasticsearch cannot do this to two snapshots simultaneously, so it will give you an error if you attempt to launch a snapshot while another is in progress.  \nCurator is supposed to catch this by testing whether a snapshot is running, and raise a SnapshotInProgress exception.  I'm not sure why it's not happening here, but it could be that they're both being launched/started simultaneously from separate scripts, or something like that, and Curator doesn't immediately detect another snapshot running because there isn't one running at the time of the check.  It's interesting to note that it takes 59 seconds before the 503 error is returned:\n2016-11-28 10:00:05,277 INFO      curator.actions.snapshot              do_action:834  Creating snapshot \"curator-20161128100004\" from indices: [u'', ..... ]\n2016-11-28 10:01:04,780 ERROR                curator.cli                    cli:193  Failed to complete action: snapshot.  <class 'curator.exceptions.FailedExecution'>: Exception encountered.  Rerun with loglevel DEBUG and/or check Elasticsearch logs for more information. Exception: TransportError(503, u'concurrent_snapshot_execution_exception', u'[es-snapshots:curator-20161128100004] a snapshot is already running')\nThe difference between 10:00:05,277 and 10:01:04,780 is almost a perfect 60 seconds.\nI'm not sure what may be going on in your environment, but it seems that there is more here than just a Curator snapshot job taking place.  There may be more information in the Elasticsearch logs during that same timeframe.  I suggest taking a look in the master node log files, as well as whichever client Curator was connecting to for what will hopefully be more information.. Aha! That's ringing a bell now.  I'm guessing this would either have to be something on the Elasticsearch side (not likely), or you are connecting through a proxy or load balancer (or timed firewall, or some other kind of intermediary).  \nCurator itself does not attempt retries with Snapshots.  The Failed to complete error you see there is just Curator reporting what it received back from the Elasticsearch API.  The precise timing here of 10 minutes, like the 60 seconds of the previous post, suggests there's an intermediary between Curator and the client node.  The timeout value may be 21600, but the intermediary (be it a load balancer, proxy, firewall, etc.) times out the connection at an exact value.\nAs stated, Curator itself has no control over this, and does not attempt a retry as part of the code (see the code in question here.  The urllib3 python module, which is used by the Elasticsearch python module that Curator depends on, will do exactly what is described here if an intermediary interrupts the connection.  It will retry/re-send the last request.  That would explain why the code that tests whether a snapshot is already running is not catching anything.\nIf there is an intermediary of some kind, I can't solve that for you with code changes to Curator.  If you must use Curator, you can set wait_for_completion: False and it will just return immediately, and the snapshot will run in the background.  No timeout will be encountered.  \nIt may be possible to write a special \"wait_for_completion\" action that will cover cases like this.  It will simply check that a snapshot, forceMerge, or whatever task is running, and keep checking every so many seconds until either a timeout is reached, or the action completes successfully.  Since 5.0 supports this kind of task architecture already, I may put that into Curator 5.  It will be a bit hacky to add that into Curator 4, but I might do it anyway.  Otherwise, your best bet is to either use wait_for_completion: False or get a connection to Elasticsearch that isn't behind a proxy or load balancer (or whatever the intermediary is), or set a ridiculously high timeout for the Curator box's connection to said intermediary.. You may be able to see more information if you change your configuration to match these lines:\nlogging:\n  loglevel: DEBUG\n  blacklist: []\nThis will show the normally omitted and verbose urllib3 logs.  I'm confident you'll see urllib3 lose a connection and immediately retry the connection, which results in the \"you already have a snapshot running\" message and error.. For more historical information about this issue, along with samples of some of those more verbose logs, see #333 (which took a long time to finally diagnose and confirm that it was a load balancer) and #457 . The continue_if_exception option catches all exceptions, save one only: An empty list (indices or snapshots), which is exactly what you hit with the \"no actionable items\" error.  \nFor empty lists, you need the ignore_empty_list option.. Elasticsearch 5.1.0 is not yet officially released. It will be supported when it is released. . Yes. It will work with 5.0.2.  I'll be updating Curator to support 5.2+ this week.. Not high on my list of priorities, especially with ILM coming in 6.4.  I'd love to add it, but with me being the only dedicated resource, it's not likely to make it in soon, unless a customer contracts with us for me to add it as a consultant.. There's an issue upgrading from elasticsearch-curator 4.1.2.  See #806 \nI upgraded to 4.2.4 from 4.2.3 just fine, using APT:\n$ sudo apt list | grep curator | grep installed\nelasticsearch-curator/stable,stable,now 4.2.4 amd64 [installed]. Sorry, that should have been #806. @ppf2 Check it out now: https://www.elastic.co/guide/en/elasticsearch/client/curator/current/filters.html\nIt's also redundantly added here and here as well.. Try using 127.0.0.1 instead of localhost \n. The code properly recognizes when a given host and port are used. The test is mostly to ensure that defaults are used when things are empty.  If I read in the passed host and port, then those defaults would be just as wrong.  I'm not sure I can do what you're asking. I will see if I can change the test to behave more like a unit test, even though it's an integration test. \nIn the meanwhile, you can safely ignore this error as it's a false positive. It is doing what it's supposed to do: use localhost and 9200 as the connection criteria if none are provided. . I'll use an if statement. If TEST_ES_SERVER is set to something other than localhost:9200, expect a fail value, otherwise expect success. . Upgrade to Curator 4.2.4. See https://github.com/elastic/curator/issues/832. Curator doesn't support OAuth2 or the OAuth2 proxy. That seems to be your\nproblem here.\n-- Aaron Mildenstein\nOn December 19, 2016 at 2:49:35 AM, gilzellner (notifications@github.com)\nwrote:\n\nthis is copy pasted from my issue on server fault\nhttp://serverfault.com/questions/821445/elasticsearch-curator-unable-to-connect-with-aws-elasticsearch-service\n:\nFrom a server on AWS, I push data into AWS ES service.\nThis data appears on Kibana just fine.\nI am also able to perform cleanup via cURL commands from this machine.\nHowever, no machine of mine is able to run ElasticSearch Curator\nsuccessfully and connect to AWS ES.\n...jects/elasticsearch-curator\u276f curator --host HOSTNAME.us-east-1.es.amazonaws.com delete indices --older-than 12 --time-unit days --timestring %Y.%m.%d\n2016-12-18 16:54:08,631 INFO      Job starting: delete indices\n2016-12-18 16:58:10,143 ERROR     Connection failure.\nMy access policy allows me to access this machine:\n~\u276f curl HOSTNAME.us-east-1.es.amazonaws.com\n{\n  \"name\" : \"Exodus\",\n  \"cluster_name\" : \"########\",\n  \"version\" : {\n    \"number\" : \"2.3.2\",\n    \"build_hash\" : \"##########\",\n    \"build_timestamp\" : \"2016-11-14T15:59:50Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"5.5.0\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\nand run commands:\n~\u276f curl -XDELETE 'HOSTNAME.us-east-1.es.amazonaws.com/2016-11*'\n{\"acknowledged\":true}%\nI am using the version of ES curator stated to be working with AWS ES\nhttps://github.com/elastic/curator#compatibility-matrix:\n...jects/elasticsearch-curator\u276f pip list | grep curator\nelasticsearch-curator (3.5.1)\nAs per answer (1) I added a port setting: \"--port 80\"\nthe connection still fails, just much faster:\ncurator --host HOSTNAME --port 80 delete indices --older-than 12 --time-unit days --timestring %Y.%m.%d\n2016-12-19 08:28:18,309 INFO      Job starting: delete indices\n2016-12-19 08:28:18,324 ERROR     Connection failure.\nand here is the output for debug mode:\n2016-12-19 11:25:59,247 DEBUG         curator.api.filter         get_date_regex:157  regex = \\l\\o\\g\\s\\t\\a\\s\\h-\\d{4}.\\d{2}.\\d{2}\n2016-12-19 11:25:59,247 DEBUG          curator.cli.utils        filter_callback:195  REGEX = (?P\\l\\o\\g\\s\\t\\a\\s\\h-\\d{4}.\\d{2}.\\d{2})\n2016-12-19 11:25:59,247 DEBUG          curator.cli.utils        filter_callback:198  Added filter: {'pattern': '(?P\\l\\o\\g\\s\\t\\a\\s\\h\\-\\d{4}\\.\\d{2}\\.\\d{2})', 'value': 12, 'groupname': 'date', 'time_unit': 'days', 'timestring': u'logstash-%Y.%m.%d', 'method': 'older_than'}\n2016-12-19 11:25:59,247 DEBUG          curator.cli.utils        filter_callback:199  New list of filters: [{'pattern': '(?P\\l\\o\\g\\s\\t\\a\\s\\h\\-\\d{4}\\.\\d{2}\\.\\d{2})', 'value': 12, 'groupname': 'date', 'time_unit': 'days', 'timestring': u'logstash-%Y.%m.%d', 'method': 'older_than'}]\n2016-12-19 11:25:59,248 INFO      curator.cli.index_selection                indices:62   Job starting: delete indices\n2016-12-19 11:25:59,248 DEBUG     curator.cli.index_selection                indices:65   Params: {'url_prefix': u'', 'http_auth': None, 'dry_run': False, 'certificate': None, 'loglevel': u'INFO', 'logformat': u'default', 'client_cert': None, 'host': u'HOSTNAME', 'quiet': False, 'timeout': 30, 'debug': True, 'use_ssl': False, 'logfile': None, 'master_only': False, 'port': 80, 'ssl_no_validate': False, 'client_key': None}\n2016-12-19 11:25:59,248 DEBUG          curator.cli.utils             get_client:114  kwargs = {'url_prefix': u'', 'http_auth': None, 'dry_run': False, 'certificate': None, 'loglevel': u'INFO', 'host': u'HOSTNAME', 'quiet': False, 'port': 80, 'logformat': u'default', 'timeout': 30, 'debug': True, 'use_ssl': False, 'logfile': None, 'master_only': False, 'client_cert': None, 'ssl_no_validate': False, 'client_key': None}\n2016-12-19 11:25:59,248 DEBUG         urllib3.util.retry               from_int:191  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-12-19 11:25:59,248 DEBUG     urllib3.connectionpool              _new_conn:212  Starting new HTTP connection (1): HOSTNAME\n2016-12-19 11:25:59,584 DEBUG     urllib3.connectionpool          _make_request:400  http://HOSTNAME:80 \"GET / HTTP/1.1\" 200 None\n2016-12-19 11:25:59,586 INFO               elasticsearch    log_request_success:66   GET http://HOSTNAME:80/ [status:200 request:0.338s]\n2016-12-19 11:25:59,586 DEBUG              elasticsearch    log_request_success:68   > None\n2016-12-19 11:25:59,587 DEBUG              elasticsearch    log_request_success:69   <\n<!DOCTYPE html>\n\n\nSign In\n\n\nbody {\n         font-family: \"Helvetica Neue\",Helvetica,Arial,sans-serif;\nfont-size: 14px;\nline-height: 1.42857143;\ncolor: #333;\nbackground: #f0f0f0;\n}\n.signin {\n    display:block;\nmargin:20px auto;\nmax-width:400px;\nbackground: #fff;\nborder:1px solid #ccc;\nborder-radius: 10px;\npadding: 20px;\n}\n.center {\n    text-align:center;\n}\n.btn {\n    color: #fff;\n        background-color: #428bca;\nborder: 1px solid #357ebd;\n            -webkit-border-radius: 4;\n-moz-border-radius: 4;\nborder-radius: 4px;\nfont-size: 14px;\npadding: 6px 12px;\ntext-decoration: none;\ncursor: pointer;\n}</p>\n<p>.btn:hover {\n    background-color: #3071a9;\n        border-color: #285e8e;\next-decoration: none;\n}\nlabel {\n    display: inline-block;\nmax-width: 100%;\nmargin-bottom: 5px;\nfont-weight: 700;\n}\ninput {\n    display: block;\nwidth: 100%;\nheight: 34px;\npadding: 6px 12px;\nfont-size: 14px;\nline-height: 1.42857143;\ncolor: #555;\nbackground-color: #fff;\nbackground-image: none;\nborder: 1px solid #ccc;\nborder-radius: 4px;\n-webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075);\nbox-shadow: inset 0 1px 1px rgba(0,0,0,.075);\n-webkit-transition: border-color ease-in-out .15s,-webkit-box-shadow ease-in-out .15s;\n-o-transition: border-color ease-in-out .15s,box-shadow ease-in-out .15s;\ntransition: border-color ease-in-out .15s,box-shadow ease-in-out .15s;\nmargin:0;\nbox-sizing: border-box;\n}\nfooter {\n    display:block;\nfont-size:10px;\ncolor:#aaa;\ntext-align:center;\nmargin-bottom:10px;\n}\nfooter a {\n    display:inline-block;\nheight:25px;\nline-height:25px;\ncolor:#aaa;\ntext-decoration:underline;\n}\nfooter a:hover {\n    color:#aaa;\n}\n\n\n\n\n\n\nAuthenticate using @crosswise.com\nSign in with a Google Account\n\n\n\n\n\nUsername:\nPassword:\nSign In\n\n\n\nif (window.location.hash) {\n(function() {\nvar inputs = document.getElementsByName('rd');\nfor (var i = 0; i &lt; inputs.length; i++) {\ninputs[i].value += window.location.hash;\n}\n})();\n}\n\n\nSecured with OAuth2 Proxy version 2.2.0-alpha\n\n\n\n2016-12-19 11:25:59,587 ERROR          curator.cli.utils             get_client:140  Connection failure.\nSo I added a few diagnostic lines in the appropriate place in utils.py:\ntry:\n    client = elasticsearch.Elasticsearch(**kwargs)\n    check_version(client)\n    # Verify the version is acceptable.\n    logger.debug('version is acceptable')\n    check_master(client, master_only=master_only)\n    # Verify \"master_only\" status, if applicable\n    logger.debug('Verify \"master_only\" status')\n    return client\nexcept Exception as e:\n    import traceback\n    logger.error('Connection failure.')\n    logger.error(e)\n    traceback.print_exc()\n    sys.exit(1)\nand we get:\n2016-12-19 11:42:37,345 ERROR          curator.cli.utils             get_client:143  Connection failure.\n2016-12-19 11:42:37,345 ERROR          curator.cli.utils             get_client:144  Unknown mimetype, unable to deserialize: text/html\nTraceback (most recent call last):\n  File \"/Users/gilzellner/.virtualenvs/temp/lib/python2.7/site-packages/curator/cli/utils.py\", line 134, in get_client\n    check_version(client)\n  File \"/Users/gilzellner/.virtualenvs/temp/lib/python2.7/site-packages/curator/cli/utils.py\", line 89, in check_version\n    version_number = get_version(client)\n  File \"/Users/gilzellner/.virtualenvs/temp/lib/python2.7/site-packages/curator/api/utils.py\", line 202, in get_version\n    version = client.info()['version']['number']\n  File \"/Users/gilzellner/.virtualenvs/temp/lib/python2.7/site-packages/elasticsearch/client/utils.py\", line 69, in _wrapped\n    return func(args, params=params, *kwargs)\n  File \"/Users/gilzellner/.virtualenvs/temp/lib/python2.7/site-packages/elasticsearch/client/init.py\", line 220, in info\n    return self.transport.perform_request('GET', '/', params=params)\n  File \"/Users/gilzellner/.virtualenvs/temp/lib/python2.7/site-packages/elasticsearch/transport.py\", line 357, in perform_request\n    data = self.deserializer.loads(data, headers.get('content-type'))\n  File \"/Users/gilzellner/.virtualenvs/temp/lib/python2.7/site-packages/elasticsearch/serializer.py\", line 74, in loads\n    raise SerializationError('Unknown mimetype, unable to deserialize: %s' % mimetype)\nSerializationError: Unknown mimetype, unable to deserialize: text/html\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/elastic/curator/issues/845, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AA-R339JxhIiDrRzwvw0ly9JjwkAUW57ks5rJlMvgaJpZM4LQirv\n.\n. Secured with <a href=\"https://github.com/bitly/oauth2_proxy#oauth2_proxy\">OAuth2 Proxy</a> version 2.2.0-alpha\nClosing as this is not supported.. 4.2.5 is released, and this has been corrected.. Have you tried setting this in the extra_settings option for the allocation action?  Just a thought.. Duplicate of #656 . I'm not sure what you're doing here, but I can't replicate your results:\n\n$ curator_cli --version\ncurator_cli, version 4.2.4\nCommand without --ignore_empty_list:\n$ curator_cli --loglevel INFO --logfile '' delete_indices --filter_list '{\"filtertype\":\"pattern\",\"kind\":\"prefix\",\"value\":\"not_here_dude\"}'\nOutput and exit code:\n```\n{\"@timestamp\": \"2016-12-21T19:57:20.354Z\", \"function\": \"_check_empty_list\", \"linenum\": 101, \"loglevel\": \"ERROR\", \"message\": \"Singleton action failed due to empty index list\", \"name\": \"curator.singletons\"}\n$ echo $?\n1\n```\nCommand with --ignore_empty_list:\n$ curator_cli --loglevel INFO --logfile '' delete_indices --ignore_empty_list --filter_list '{\"filtertype\":\"pattern\",\"kind\":\"prefix\",\"value\":\"not_here_dude\"}'\nOutput and exit code:\n```\n{\"@timestamp\": \"2016-12-21T19:57:53.779Z\", \"function\": \"_check_empty_list\", \"linenum\": 96, \"loglevel\": \"INFO\", \"message\": \"Singleton action not performed: empty index list\", \"name\": \"curator.singletons\"}\n$ echo $?\n0\n```\nEven when I try to run multiple filters:\n```\n$ curator_cli --loglevel INFO --logfile '' show_indices --ignore_empty_list --filter_list '[{\"filtertype\":\"none\"},{\"filtertype\":\"pattern\",\"kind\":\"prefix\",\"value\":\"not_here_dude\"}]'\n{\"@timestamp\": \"2016-12-21T20:03:27.054Z\", \"function\": \"_check_empty_list\", \"linenum\": 96, \"loglevel\": \"INFO\", \"message\": \"Singleton action not performed: empty index list\", \"name\": \"curator.singletons\"}\n$ curator_cli --loglevel INFO --logfile '' show_indices --ignore_empty_list --filter_list '[{\"filtertype\":\"pattern\",\"kind\":\"prefix\",\"value\":\"logstash\"},{\"filtertype\":\"pattern\",\"kind\":\"prefix\",\"value\":\"not_here_dude\"}]'\n{\"@timestamp\": \"2016-12-21T20:04:20.392Z\", \"function\": \"_check_empty_list\", \"linenum\": 96, \"loglevel\": \"INFO\", \"message\": \"Singleton action not performed: empty index list\", \"name\": \"curator.singletons\"}\n```\nOr even with your filters, both before and after:\n$ curator_cli --loglevel INFO --logfile '' delete_indices --ignore_empty_list --filter_list '[{\"filtertype\":\"age\",\"source\":\"name\",\"direction\":\"older\",\"unit\":\"days\",\"unit_count\":5,\"timestring\": \"%Y.%m.%d\"},{\"filtertype\":\"pattern\",\"kind\":\"prefix\",\"value\":\"logstash\"}]'\n{\"@timestamp\": \"2016-12-21T20:05:17.297Z\", \"function\": \"do_action\", \"linenum\": 478, \"loglevel\": \"INFO\", \"message\": \"Deleting selected indices: ['logstash-2016.12.15', 'logstash-2016.12.11', 'logstash-2016.09.30', 'logstash-2016.12.16', 'logstash-2016.11.21', 'logstash-2016.12.12', 'logstash-2016.12.08', 'logstash-2016.12.14', 'logstash-2016.12.09', 'logstash-2016.12.13', 'logstash-2016.12.10', 'logstash-2016.12.07']\", \"name\": \"curator.actions.delete_indices\"}\n{\"@timestamp\": \"2016-12-21T20:05:17.298Z\", \"function\": \"__chunk_loop\", \"linenum\": 453, \"loglevel\": \"INFO\", \"message\": \"---deleting index logstash-2016.12.15\", \"name\": \"curator.actions.delete_indices\"}\n{\"@timestamp\": \"2016-12-21T20:05:17.298Z\", \"function\": \"__chunk_loop\", \"linenum\": 453, \"loglevel\": \"INFO\", \"message\": \"---deleting index logstash-2016.12.11\", \"name\": \"curator.actions.delete_indices\"}\n{\"@timestamp\": \"2016-12-21T20:05:17.298Z\", \"function\": \"__chunk_loop\", \"linenum\": 453, \"loglevel\": \"INFO\", \"message\": \"---deleting index logstash-2016.09.30\", \"name\": \"curator.actions.delete_indices\"}\n{\"@timestamp\": \"2016-12-21T20:05:17.298Z\", \"function\": \"__chunk_loop\", \"linenum\": 453, \"loglevel\": \"INFO\", \"message\": \"---deleting index logstash-2016.12.16\", \"name\": \"curator.actions.delete_indices\"}\n{\"@timestamp\": \"2016-12-21T20:05:17.298Z\", \"function\": \"__chunk_loop\", \"linenum\": 453, \"loglevel\": \"INFO\", \"message\": \"---deleting index logstash-2016.11.21\", \"name\": \"curator.actions.delete_indices\"}\n{\"@timestamp\": \"2016-12-21T20:05:17.298Z\", \"function\": \"__chunk_loop\", \"linenum\": 453, \"loglevel\": \"INFO\", \"message\": \"---deleting index logstash-2016.12.12\", \"name\": \"curator.actions.delete_indices\"}\n{\"@timestamp\": \"2016-12-21T20:05:17.298Z\", \"function\": \"__chunk_loop\", \"linenum\": 453, \"loglevel\": \"INFO\", \"message\": \"---deleting index logstash-2016.12.08\", \"name\": \"curator.actions.delete_indices\"}\n{\"@timestamp\": \"2016-12-21T20:05:17.298Z\", \"function\": \"__chunk_loop\", \"linenum\": 453, \"loglevel\": \"INFO\", \"message\": \"---deleting index logstash-2016.12.14\", \"name\": \"curator.actions.delete_indices\"}\n{\"@timestamp\": \"2016-12-21T20:05:17.298Z\", \"function\": \"__chunk_loop\", \"linenum\": 453, \"loglevel\": \"INFO\", \"message\": \"---deleting index logstash-2016.12.09\", \"name\": \"curator.actions.delete_indices\"}\n{\"@timestamp\": \"2016-12-21T20:05:17.298Z\", \"function\": \"__chunk_loop\", \"linenum\": 453, \"loglevel\": \"INFO\", \"message\": \"---deleting index logstash-2016.12.13\", \"name\": \"curator.actions.delete_indices\"}\n{\"@timestamp\": \"2016-12-21T20:05:17.299Z\", \"function\": \"__chunk_loop\", \"linenum\": 453, \"loglevel\": \"INFO\", \"message\": \"---deleting index logstash-2016.12.10\", \"name\": \"curator.actions.delete_indices\"}\n{\"@timestamp\": \"2016-12-21T20:05:17.299Z\", \"function\": \"__chunk_loop\", \"linenum\": 453, \"loglevel\": \"INFO\", \"message\": \"---deleting index logstash-2016.12.07\", \"name\": \"curator.actions.delete_indices\"}\n{\"@timestamp\": \"2016-12-21T20:05:20.112Z\", \"function\": \"_actionator\", \"linenum\": 82, \"loglevel\": \"INFO\", \"message\": \"Singleton \\\"delete_indices\\\" action completed.\", \"name\": \"curator.singletons\"}\nI figured I might be able to get the error if the list is empty afterward, but I can't:\n$ curator_cli --loglevel INFO --logfile '' delete_indices --ignore_empty_list --filter_list '[{\"filtertype\":\"age\",\"source\":\"name\",\"direction\":\"older\",\"unit\":\"days\",\"unit_count\":5,\"timestring\": \"%Y.%m.%d\"},{\"filtertype\":\"pattern\",\"kind\":\"prefix\",\"value\":\"logstash\"}]'\n{\"@timestamp\": \"2016-12-21T20:06:03.646Z\", \"function\": \"_check_empty_list\", \"linenum\": 96, \"loglevel\": \"INFO\", \"message\": \"Singleton action not performed: empty index list\", \"name\": \"curator.singletons\"}\nI'm not sure what's going on in your environment, but I cannot replicate it on my end with 4.2.4.\nI do see you're using Python 3.4.  Is there a reason you're not using the pre-compiled binaries I provide now?. > Here we go! - looks like it is a problem with the ordering of the filters.\nWell, of course.  You created an empty list condition with the first filter.  \nIt's not about the ordering as much as it's about the fact that Curator's trying to filter an already empty list.  The ordering only serves to make that happen sooner.\nI found where --ignore_empty_list was not being caught and have a fix coming.\n. Thanks for correcting the typo.\n. Alias is avoided because it's very different and a terrific pain to try to accurately create the add and remove filter blocks as JSON on the command-line.  Sure, it could be done if someone really wanted to do it, mostly because the schema checking would validate the options.  It still makes for a very ugly command line, that's for sure.\nRestore is not implemented because it also is a pain with so many flags and options.  I'd happily merge a restore action if someone wanted to put it in and manually test it.. Curator 4 supports environment variables.  You could still have a script that generates them, then calls Curator.. The new period filter will help with the alias issue.. Alias was added in #1214.. I can't merge any code (even documentation changes) if you do not sign the CLA. Also, simply removing required=True is not optimal.  I recommend replacing that with default='{\"filtertype\":\"none\"}'.. And since you're going to the trouble to do this, I would ask that you make that change to show_indices in addition to snapshot and show_snapshot.  Thanks.. And please rebase against the current master so you can properly add a line to the current Changelog, if you don't mind.. Probably a duplicate of https://github.com/elastic/curator/issues/828#issuecomment-265589719. It will probably be fixed in Curator 5.0, as I believe it's an artifact of using the 2.x API calls to 5.0.. This does not appear to happen with the master branch, or the 5.x branch.  I'd love it if you could test with the 5.x branch and see if it still appears this way on your end.. Now that Curator 5.0 is officially released, I hope to hear back on this, officially.. The snapshots are supposed to have these unique ids.  This is a feature, not a bug.  This is a change made in elasticsearch.  Indices, too, are not named in the filesystem as before.. @Rumbles It is a size thing.  An optimize, or more appropriately a forceMerge, is effectively a transparent, localized reindex operation.  \nSegments are immutable, once created.  In order to forceMerge segments, you have to take the data on them and re-index them into new, larger segments.  Regular segment merging takes place constantly and transparently under the hood so long as new documents are being indexed. A forceMerge is a manually initiated segment merge, usually done to reduce the management overhead associated with having hundreds of segments per shard, and to speed up snapshot operations as there are fewer segments to compare.\nA forceMerge takes all of the existing segments and re-indexes them to fit in the specified number of segments per shard.  As such, enough space is required to fit all segments into the \"new\" larger segment (or segments).  If optimizing to 1 segment per shard, you would need as much free space as all segments for that shard consume.  After the operation is complete, the smaller segments are purged, freeing the space again.  If you do not have sufficient space, it will fail with a message, like the one you shared.  Effectively, this is not a Curator error as you'd get the same error if you manually initiated a forceMerge from a REST call (which is why @corey-hammerton suggested you run that curl call manually, as it is expected the same error would manifest).  Curator is really just a wrapper for API calls that helps select the indices you want to work on.  If an API call fails, Curator just passes the message back.  At the end of the day, this is not an error.  It's Curator passing along the response from Elasticsearch informing you that it cannot perform the desired operation due to space constraints.. Since this is a usage question, rather than an issue with Curator itself, please redirect any further questions to https://discuss.elastic.co/c/elasticsearch. Addressed in 5.0.1. Addressed in 5.0.1. Your end. Two digit weeks are the ISO standard. . Feel free to submit a PR. It has to pass all tests. You might need to change one of the unit tests accordingly. . Closed by #862 . LGTM\u2122\nThanks!. I'm pretty sure this is a usage question rather than a bug, and I responded to the identical question you put up at https://discuss.elastic.co/t/curator-ssl-connection-issue/70800. I've left a comment.  I cannot merge your changes until you've signed the CLA (yes, even for documentation changes), and make a few revisions.. @abraxxa I also note that the bottom of the YUM installation page has the following warning block:\n\nThe elasticsearch-curator binary package cannot coexist with the older python-elasticsearch-curator packages. This collision is explicitly defined in packages since 4.1.2, but not in previous packages. If it is installed, you must uninstall the python-elasticsearch-curator package before installing the elasticsearch-curator binary package.\n\nPerhaps reword accordingly, since there is a warning there with regard to the older python-elasticsearch-curator package.. I'm closing this as it's been a month without feedback or response.  In this time period, I've also added https://www.elastic.co/guide/en/elasticsearch/client/curator/current/version-compatibility.html and issue #874 was added, which seeks to address similar concerns differently.. It will be updated for Curator v5. Many of the tests do not pass because the API calls have changed slightly. . Not sure, honestly. No firm date yet. . This change is already in master.. Have you tried using the --dry-run flag? It's a great way to see what's going to be acted on before you do it for real. \nIf you're only filtering by creation_date, it will find all of your indices that age. The timestring is ignored in such a case. \nYou could use the kibana filter type, but you would need to set exclude: True to exclude the Kibana indices. . If you change the link to the About page, it should be roughly the same info: https://www.elastic.co/guide/en/elasticsearch/client/curator/current/about.html. And I'd be happy to merge that change.. @basex, did you want to make those changes, or just close the PR?  Feel free to close it yourself if you don't want to make those changes.. If you note the line above, it's actually accurate:\nStarting new HTTPS connection (1): localhost\nThe python Elasticsearch module is logging in a way that is less than 100% informative is why it says \"http\" instead of \"https\", but it is connecting via HTTPS (or at least attempting to).\nI see that you have --certificate /etc/mycert.  What exactly is in that file?  Is that a directory?  Curator requires a full path to an actual certificate (usually PEM formatted).  The --certificate flag is for a certificate like the ones that ship with a browser, so that Curator can verify that the SSL keys on the server side match what is expected.  If you've made a self-signed certificate, you'd likely use the CA certificate (not key) here.\nAs a side note, are you using an older version of Elasticsearch?  Why are you still using 3.5.1?  I mention that because all of the 1.x versions of Elasticsearch are now past the end of life notice.\n. Don't use 4.1.2. The most recent version is 4.2.5. And you can use singleton actions again, like in 3.x. Without running with loglevel set to DEBUG, and attaching a sanitized version of the output (i.e. no passwords or public IPs), I will have a very hard time debugging or helping find the root cause. \nIf I had to guess, though, I suspect you may have an index that starts with filebeat but somehow has an atypical value for the timestring.. Sorry for the long wait.  I've been dealing with a kidney stone and am just getting back to things.\nI did some digging, and the long and short of it is that Python will not let you use an hour value of 24 in a timestring to extract a timestamp. I know, it was a huge surprise to me, and I'll be having to document this.  You're the first to report it (lucky you, right?).  The value should be 00 for that hour.\nI found these in your log file:\nfilebeat-2017.01.18.24\nfilebeat-2017.01.19.24\nfilebeat-2017.01.20.24\nfilebeat-2017.01.21.24\nfilebeat-2017.01.22.24\nfilebeat-2017.01.23.24\nThere's a much better way of calculating age now in Elasticsearch.\nYou can still use this timestring with %H hours to filter indices like this:\n- filtertype: pattern\n  kind: timestring\n  value: '%Y.%m.%d.%H'\nSince you cannot use a value of 24 for %H in a timestring to calculate age, I recommend using source: creation_date, or if you've been ingesting old data and the creation date would be inaccurate, the most accurate is \n source: field_stats.  \ncreation_date is simply the epoch timestamp recorded at the time the index was created.  With time-series data this is usually accurate, but it can be inaccurate if you've ingested some old syslog line, and Logstash creates an index for 2017.01.01 on the 20th of the month. Based on the content of the log line, Logstash makes the index name's date in the past, but the creation_date would still be 2017.01.20.  To avoid scenarios like this, use source: field_stats to calculate index age.\nThe field_stats API in Elasticsearch will tell you what the min and max values of a field are in an Elasticsearch index.  For Curator, presuming you're using the @timestamp field, the configuration might look like:\n```\n'@timestamp' is the default value for 'field', so the 'field' line can be omitted if that is the case\n'min_value' is the default for 'stats_result', so the 'stats_result' line can be omitted if that is the case\n\nfiltertype: age\n  source: field_stats\n  field: '@timestamp'\n  stats_result: min_value \n  direction: older\n  unit: days\n  unit_count: 3\n``\nThis will calculate an index's age based on the minimum value found for@timestamp` in the index.  If that is older than 3 days ago, it will remain in the actionable list.  \n\nYou could also use max_value for stats_result, which would calculate index age based on the \"newest\" value in the index.  It is important to remember that for time calculation, min_value and max_value are going to be evaluating epoch time.  As such, a bigger value indicates a more recent time stamp.. For the record, this is what happens:\n```\n\n\n\nimport datetime\nindex_timestamp = '2017.01.22.24'\ntimestring = '%Y.%m.%d.%H'\ndatetime.datetime.strptime(index_timestamp, timestring)\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/_strptime.py\", line 328, in _strptime\n    data_string[found.end():])\nValueError: unconverted data remains: 4\nIf I set the hour to 23, it works just fine:\ni_t = '2017.01.22.23'\nts = '%Y.%m.%d.%H'\ndatetime.datetime.strptime(i_t, ts)\ndatetime.datetime(2017, 1, 22, 23, 0)\n```\n\n\n\nThat's why there's a 4 remaining as unconverted data: 2 is a valid hour, but 24 is not, so it tries to grab the 2, leaving the 4.. Did you use %H in your filebeat.yml file? Or %k?  See https://godoc.org/github.com/elastic/beats/libbeat/common/dtfmt. Yes, %H is usable in Logstash. It always has been, but there you would use 2 H's to indicate 2 digits, just like you have in your example. . I'm going to mark this as closed.. Chances are very good that this will not be repaired in Curator 4.x, but will be in 5.x.  It will be documented in the 4.x tree (soonish) that this feature does not work when using Curator 4.x with 5.x\nThis is what makes cross-version support hard \ud83d\ude41 . Fixed by #896. @sherry-ger Rather than bundle actions, what if there were a few template actions, like add and remove (and show, for the cli version).  Since actions can be chained, it makes more sense to me to keep the actions cleaner, and more singularly purposed.. How did you create the repository?\nThe repository must be a shared file system, usually NFS for Linux systems, but s3 and HDFS are also available. Each master and data node must be able to both read and write to this shared file system, so with NFS this also requires matching user is permissions. \nThe repository must be registered with elasticsearch via an API call and the path.repo set to the shared file system mount point. In your case it seems that you've defined the path.repo as a sub folder in your regular elasticsearch data path. This would only work in a single node cluster, as that is the only way a local path is shared by all nodes, since there's only one. It defeats the purpose of a backup, too, since the storage is local. \nI'm not sure what your aiming for, but you'll need to figure out how to mount NFS (or some kind of shared fs) in your Docker containers to do snapshots. . You have to create the repository for Elasticsearch to recognize using the API.  See https://www.elastic.co/guide/en/elasticsearch/reference/5.2/modules-snapshots.html#_repositories. You can also use the es_repo_mgr tool that ships with Curator.  It does the API call for you, using the command-line.. > But more naturally a setting in the elasticsearch.yml file. Regardless of filesystem.\nI can see why that would be helpful in your case, and why you'd want it.  Unfortunately, it doesn't work that way.  Remember that Elasticsearch is a distributed system.  You would have to guarantee that all nodes had the exact same line in the elasticsearch.yml file.  But the problem remains, that as nodes are added and removed, things can be changed.  Elasticsearch strives to do everything via API.  As a repository must be recognized on all nodes, even in cases where you are only running a single node, it makes more sense to be able to add that via API call and have it applied to all nodes in the cluster.  You simply must add the repository to your node via the API call (or the es_repo_mgr script, which does the API call) or Elasticsearch cannot make use of it.\nSure, you could make use of ENTRYPOINT or some such, but it is moot if Elasticsearch is not already up and running.  You can't make an API call to a system that isn't running.. This has never been a problem before.  I'm not sure what you're up against, but that command has worked for many others (myself included) when using Curator 3.x.\nThe issue at hand here seems to be revealed when you say that\n\nCurator will clear-up the metadata, but not the data in the indices directory.\n\nThis is revealing because Curator only issues API calls to Elasticsearch and returns a response.  It seems that Curator is sending the API call, and Elasticsearch is saying, \"I got this,\" and responding that it is deleting the data.  If Elasticsearch is not deleting the data, that is not going to be Curator's fault, unfortunately.  Curator behaves as if you ran:\ncurl -XDELETE http://hostname:port/.marvel-2016.01.31\nmultiple times, one line for each index.\nTo recap, if Elasticsearch is deleting anything\u2014including metadata\u2014it suggests that Curator made the API call to Elasticsearch successfully.  If Elasticsearch failed to complete the deletion, I don't know what to tell you.\nDo you have more in the logs?  Debug from Curator, and/or from Elasticsearch (the client and/or the elected master node)?. If you're running Shield, it probably has something to do with that.  In the earlier versions, dot-prefixed index names were protected, if I recall correctly.  I'm not sure what can be done to address that.. In any case, it's not Curator itself that can correct the problem.  . Closing.. That's not a common occurrence. If you have a snapshot running for over 24 hours, chances are good one of these is true:\n\nYou are running a very large snapshot (i.e. terabytes of data in one shot)  This can be a problem because segment merges are affected while a snapshot is running.  It's perhaps better to take smaller chunks of indices to avoid super-long snapshot run times.\nYou have a huge number of snapshots in your repository. Elasticsearch has to compare your current indices/segments to compare them with what's already stored so as not to duplicate segments.  This can be time consuming.  Ways around this include performing forceMerges on older time series data before snapshotting so the segment count is less.\nYour cluster is under some duress.  This could be from any number of causes.  \n\nIn any case, there is a filter that can detect the state of snapshots: https://www.elastic.co/guide/en/elasticsearch/client/curator/current/filtertype_state.html\nThis could be used as your filter with the curator_cli binary and the show_snapshots action to keep tabs on snapshots from the command-line.  I do not recall offhand whether the snapshot start date was included in the verbose output, but I do not think I did.  That could be added, potentially.. Closing this as I do not feel it is a necessary feature.  Feel free to reopen and start the discussion again if you feel that it should be.. This is a duplicate of #831 \nYou need the ignore_empty_list option .  There are two kinds of exceptions:\n\nSomething failed with either Curator or Elasticsearch. This is covered by the continue_if_exception option.\nThe list to act on is empty. This is covered by the ignore_empty_list option.\n\nThe reason these are separate is that there are cases where an empty list will occur, but it shouldn't be treated as a regular exception.  Because Curator can catch an empty list as a separate exception, it won't accidentally continue if a type 1 exception was the cause (because in such a case, you wouldn't normally want it to continue).  \nEmpty list conditions can occur when you first start using Curator.  You might want to purge indices older than 30 days, but you only have 3 or 4 right now.  This would trigger the exception above, but it's not really an error.  This is why the separate ignore_empty_list option exists.. See the version compatibility matrix.\nAs it appears that AWS ES 5.x now supports the /cluster/state endpoint, it _should work with Curator version 4.  There are API calls in ES 5.1 that are different from 5.0 and 2.x, and do not work (namely waiting for shard allocation to complete), but otherwise I have no reason to suspect that it won't work.. Interesting.  I wonder what AWS included in the metadata.  They apparently included the /_cluster/state endpoint, which is necessary for Curator, but may not have included all of the necessary information in that endpoint.  It does not appear that the individual index settings (which contain the metadata) are there, at least not under the settings key.\nI'd love it if you could run with --loglevel DEBUG and attach the resulting output (after sanitizing anything sensitive).\nAs a side note, are you 100% sure you have 4.2.6?  It looks like you have 4.2.4 from the traceback output:\nload_entry_point('elasticsearch-curator==4.2.4', 'console_scripts', 'curator_cli')()\nYou can tell by running curator --version.  It does look like you installed via pip, as you're running with Python 2.7.. @volcomism I'm sorry it still doesn't work.  It does seem that while AWS ES does now support the /_cluster/state endpoint, it still does not contain the necessary information for Curator to populate its internal metadata store.  It seems that they should have tested that more carefully, as I know there are tickets open with regards to supporting Curator.. @streylight do you have a link to an AWS ticket/issue/forum page?. There are some missing pieces in _settings, so it works for only a limited number of use cases. . That was attempted in another PR, and it won't pass the unit and integration tests, so it was rejected as a workaround. I understand that AWS is aware of the issue and is working on a fix. . Please let me know how it goes.  I will have to change the support matrix and document how to use IAM credentials once it is confirmed.. @jitran is that with IAM credentials in your curator.yml file, then?. @jimmycuadra it does require the IAM credentials to be added to the configuration file.  It cannot pull the credentials from Elasticsearch.. @jimmycuadra @jitran I have no instance of AWS ES to play with, so I will be more than happy to merge any pull requests with fixes to the current setup.  I otherwise have no way to develop and test this. \ud83d\ude22 . With the change from #1172, I'm going to close this.  Amazon has improved the API call availability, and Curator seems to work just fine with everything 5.1 and up now.. Interesting.  What do you see in the Elasticsearch logs?  I'd be interested to see what is in the logs on the node you connected to (localhost?) and the elected master at the time Curator ran.\nI've checked my own Curator logs, and it appears that forceMerges are working (since it doesn't attempt to forceMerge again the next day, that indicates that they've been squashed down to 1 segment per shard).\nYesterday:\n{\"@timestamp\": \"2017-02-07T09:30:11.105Z\", \"function\": \"cli\", \"linenum\": 173, \"loglevel\": \"INFO\", \"message\": \"Trying Action ID: 6, \\\"forcemerge\\\": forceMerge Year.month.day indices older than 2 days (based on index creation_date) to 1 segment per shard.  Delay 120 seconds between each forceMerge operation to allow the cluster to quiesce. This action will ignore indices already forceMerged to the same or fewer number of segments per shard, so the 'forcemerged' filter is unneeded.\", \"name\": \"curator.cli\"}\n{\"@timestamp\": \"2017-02-07T09:30:11.292Z\", \"function\": \"do_action\", \"linenum\": 527, \"loglevel\": \"INFO\", \"message\": \"forceMerging selected indices\", \"name\": \"curator.actions.forcemerge\"}\n{\"@timestamp\": \"2017-02-07T09:30:11.292Z\", \"function\": \"do_action\", \"linenum\": 532, \"loglevel\": \"INFO\", \"message\": \"forceMerging index logstash-2017.02.06 to 1 segments per shard.  Please wait...\", \"name\": \"curator.actions.forcemerge\"}\n{\"@timestamp\": \"2017-02-07T09:30:12.437Z\", \"function\": \"do_action\", \"linenum\": 543, \"loglevel\": \"INFO\", \"message\": \"Pausing for 120.0 seconds before continuing...\", \"name\": \"curator.actions.forcemerge\"}\n{\"@timestamp\": \"2017-02-07T09:32:12.533Z\", \"function\": \"do_action\", \"linenum\": 532, \"loglevel\": \"INFO\", \"message\": \"forceMerging index metricbeat-2017.02.06 to 1 segments per shard.  Please wait...\", \"name\": \"curator.actions.forcemerge\"}\n{\"@timestamp\": \"2017-02-07T09:35:17.195Z\", \"function\": \"do_action\", \"linenum\": 543, \"loglevel\": \"INFO\", \"message\": \"Pausing for 120.0 seconds before continuing...\", \"name\": \"curator.actions.forcemerge\"}\n{\"@timestamp\": \"2017-02-07T09:37:17.294Z\", \"function\": \"do_action\", \"linenum\": 532, \"loglevel\": \"INFO\", \"message\": \"forceMerging index .monitoring-logstash-2-2017.02.06 to 1 segments per shard.  Please wait...\", \"name\": \"curator.actions.forcemerge\"}\n{\"@timestamp\": \"2017-02-07T09:37:18.455Z\", \"function\": \"do_action\", \"linenum\": 543, \"loglevel\": \"INFO\", \"message\": \"Pausing for 120.0 seconds before continuing...\", \"name\": \"curator.actions.forcemerge\"}\n{\"@timestamp\": \"2017-02-07T09:39:18.550Z\", \"function\": \"do_action\", \"linenum\": 532, \"loglevel\": \"INFO\", \"message\": \"forceMerging index .monitoring-kibana-2-2017.02.06 to 1 segments per shard.  Please wait...\", \"name\": \"curator.actions.forcemerge\"}\n{\"@timestamp\": \"2017-02-07T09:39:18.780Z\", \"function\": \"do_action\", \"linenum\": 543, \"loglevel\": \"INFO\", \"message\": \"Pausing for 120.0 seconds before continuing...\", \"name\": \"curator.actions.forcemerge\"}\n{\"@timestamp\": \"2017-02-07T09:41:18.831Z\", \"function\": \"do_action\", \"linenum\": 532, \"loglevel\": \"INFO\", \"message\": \"forceMerging index .monitoring-es-2-2017.02.06 to 1 segments per shard.  Please wait...\", \"name\": \"curator.actions.forcemerge\"}\n{\"@timestamp\": \"2017-02-07T09:43:14.845Z\", \"function\": \"do_action\", \"linenum\": 543, \"loglevel\": \"INFO\", \"message\": \"Pausing for 120.0 seconds before continuing...\", \"name\": \"curator.actions.forcemerge\"}\nToday:\n{\"@timestamp\": \"2017-02-08T09:30:10.333Z\", \"function\": \"cli\", \"linenum\": 173, \"loglevel\": \"INFO\", \"message\": \"Trying Action ID: 6, \\\"forcemerge\\\": forceMerge Year.month.day indices older than 2 days (based on index creation_date) to 1 segment per shard.  Delay 120 seconds between each forceMerge operation to allow the cluster to quiesce. This action will ignore indices already forceMerged to the same or fewer number of segments per shard, so the 'forcemerged' filter is unneeded.\", \"name\": \"curator.cli\"}\n{\"@timestamp\": \"2017-02-08T09:30:10.526Z\", \"function\": \"do_action\", \"linenum\": 527, \"loglevel\": \"INFO\", \"message\": \"forceMerging selected indices\", \"name\": \"curator.actions.forcemerge\"}\n{\"@timestamp\": \"2017-02-08T09:30:10.526Z\", \"function\": \"do_action\", \"linenum\": 532, \"loglevel\": \"INFO\", \"message\": \"forceMerging index .monitoring-es-2-2017.02.07 to 1 segments per shard.  Please wait...\", \"name\": \"curator.actions.forcemerge\"}\n{\"@timestamp\": \"2017-02-08T09:31:45.799Z\", \"function\": \"do_action\", \"linenum\": 543, \"loglevel\": \"INFO\", \"message\": \"Pausing for 120.0 seconds before continuing...\", \"name\": \"curator.actions.forcemerge\"}\n{\"@timestamp\": \"2017-02-08T09:33:45.898Z\", \"function\": \"do_action\", \"linenum\": 532, \"loglevel\": \"INFO\", \"message\": \"forceMerging index logstash-2017.02.07 to 1 segments per shard.  Please wait...\", \"name\": \"curator.actions.forcemerge\"}\n{\"@timestamp\": \"2017-02-08T09:33:47.344Z\", \"function\": \"do_action\", \"linenum\": 543, \"loglevel\": \"INFO\", \"message\": \"Pausing for 120.0 seconds before continuing...\", \"name\": \"curator.actions.forcemerge\"}\n{\"@timestamp\": \"2017-02-08T09:35:47.424Z\", \"function\": \"do_action\", \"linenum\": 532, \"loglevel\": \"INFO\", \"message\": \"forceMerging index .monitoring-logstash-2-2017.02.07 to 1 segments per shard.  Please wait...\", \"name\": \"curator.actions.forcemerge\"}\n{\"@timestamp\": \"2017-02-08T09:35:47.823Z\", \"function\": \"do_action\", \"linenum\": 543, \"loglevel\": \"INFO\", \"message\": \"Pausing for 120.0 seconds before continuing...\", \"name\": \"curator.actions.forcemerge\"}\n{\"@timestamp\": \"2017-02-08T09:37:47.901Z\", \"function\": \"do_action\", \"linenum\": 532, \"loglevel\": \"INFO\", \"message\": \"forceMerging index metricbeat-2017.02.07 to 1 segments per shard.  Please wait...\", \"name\": \"curator.actions.forcemerge\"}\n{\"@timestamp\": \"2017-02-08T09:40:34.789Z\", \"function\": \"do_action\", \"linenum\": 543, \"loglevel\": \"INFO\", \"message\": \"Pausing for 120.0 seconds before continuing...\", \"name\": \"curator.actions.forcemerge\"}\n{\"@timestamp\": \"2017-02-08T09:42:34.829Z\", \"function\": \"do_action\", \"linenum\": 532, \"loglevel\": \"INFO\", \"message\": \"forceMerging index .monitoring-kibana-2-2017.02.07 to 1 segments per shard.  Please wait...\", \"name\": \"curator.actions.forcemerge\"}\n{\"@timestamp\": \"2017-02-08T09:42:35.838Z\", \"function\": \"do_action\", \"linenum\": 543, \"loglevel\": \"INFO\", \"message\": \"Pausing for 120.0 seconds before continuing...\", \"name\": \"curator.actions.forcemerge\"}. I'm on 5.2.0 and Curator 4.2.6, btw.\nHere's action number 6 from my actions.yml file:\n6:\n    action: forcemerge\n    description: >-\n      forceMerge Year.month.day indices older than 2 days (based on index\n      creation_date) to 1 segment per shard.  Delay 120 seconds between each\n      forceMerge operation to allow the cluster to quiesce.\n      This action will ignore indices already forceMerged to the same or fewer\n      number of segments per shard, so the 'forcemerged' filter is unneeded.\n    options:\n      ignore_empty_list: True\n      max_num_segments: 1\n      delay: 120\n      timeout_override: 7200\n      continue_if_exception: False\n      disable_action: False\n    filters:\n    - filtertype: pattern\n      kind: timestring\n      value: '%Y.%m.%d'\n      exclude: False\n    - filtertype: age\n      source: creation_date\n      direction: older\n      unit: days\n      unit_count: 1. Curious.  What happens if you run a forceMerge on an index via curl or the Console in Kibana?. Hmm.  You might want to specify a number of segments with that call, first:\ncurl -XPOST 'localhost:9200/rumor_test/_forcemerge?max_num_segments=1&pretty'. You created a test index with 84 shards.  What about segments?. It stalls? What do you mean by that?  It just sits there?  It should sit there while it's performing the merge.  You can see this time delay in the timestamps in the logs I posted earlier.  Your Curator logs show no delay at all (millisecond, maybe).\nOut of curiosity, how many nodes are in your cluster?  42 shards is quite a few, and 1 replica per primary making that 84 is very considerable.  That's only recommended if you have at least nearly as many nodes as shards to spread the I/O around.. Ah, that's the issue.  You didn't include the wait_for_completion option.  It is performing the forceMerges in the background.  If you want it to wait for one to complete before proceeding to the next, you need to use the wait_for_completion option, and set it to True.. Without adding wait_for_completion, it will not pause. \nTry this on an unmerged index:\ncurl -XPOST 'localhost:9200/INDEXNAME/_forcemerge?max_num_segments=1&wait_for_completion=true&pretty'. My bad.  I forgot this action doesn't require that flag.\nAt this point I can tell you two things for sure:\n\nThis isn't Curator's problem, as you are seeing the same behavior via curl or the Console.  Since Curator is only really an index selection and filtering wrapper for API calls, you seeing the same behavior with generic API calls means...(see 2):\nSomething else is at work here at the Elasticsearch level, and I do not know what it may be.  \n\nI run the same forceMerge call in my cluster and it works, and has every time (it's even part of the integration testing run on every build of Curator).  I recommend asking the community support forum for Elasticsearch for more answers.. I might be pointing out the obvious here, but the log you're showing is in DRY-RUN mode, so no actions were actually taken.  \nDid you run it without the --dry-run flag?. If you run it with the --dry-run flag, it will only show what it would do.  It won't actually do the action.  Run it again without the --dry-run flag and it will actually delete the indices.. Apologies, but the Kibana filter is exclude-only, since it filters multiple kinds of indices.  You can tell it exclude: False, but it won't do any different:\nfor index in self.working_list():\n            if index in [\n                    '.kibana', '.marvel-kibana', 'kibana-int', '.marvel-es-data'\n                ]:\n                self.__excludify(True, exclude, index)\nIt is a hold-over from Curator 3.  It's exclusively there to prevent you from accidentally deleting .kibana indices.  I should probably document this functionality better.\nThat said, what you want to do can be achieved like this:\nactions:\n  1:\n    action: snapshot\n    description: Backup the .kibana index\n    options:\n      ignore_empty_list: True\n      repository: YOUR_REPO_NAME\n      name: 'kibana-%Y%m%d%H%M%S'\n    filters:\n    - filtertype: pattern\n      kind: regex\n      value: '^\\.kibana$'\nYou could also keep the prefix filtertype you're using now, but you need to escape the ., as all of the pattern types are actually part of regular expressions:\nThis works:\nprefix: '\\.kibana'\nIt also works with curator_cli, but you have to escape the back-slash (rather than the period) in order for it to appear in the resulting JSON:\nBAD:\n```\n$ curator_cli show_indices --filter_list '{\"filtertype\":\"pattern\", \"kind\":\"prefix\", \"value\":\".kibana\"}'\nUsage: curator_cli show_indices [OPTIONS]\nError: Invalid value for \"--filter_list\": Invalid JSON: {\"filtertype\":\"pattern\", \"kind\":\"prefix\", \"value\":\".kibana\"}\n```\nGOOD:\n$ curator_cli show_indices --filter_list '{\"filtertype\":\"pattern\", \"kind\":\"prefix\", \"value\":\"\\\\.kibana\"}'\n.kibana. The curator version has already been bumped and this issue addressed. 4.2.5 isn't the latest version. . @jakommo Hmm.  It seems from the error that the issue is that alias today does not exist:\nException: TransportError(404, 'aliases_not_found_exception', 'aliases [today] missing')\nThis is a different problem from not being able to remove an index from an alias it's not assigned to.  In my testing, you could issue a remove as part of the alias update body and it wouldn't complain.  Now, you can't issue a remove for an alias that doesn't exist...\nMight not the issue be that in removing the last index from alias today that the alias got deleted?\n. I'm not sure how to proceed.  It seems that when a \"remove\" action is present, there should be a test to ensure an alias exists before attempting to remove something from it.. Thanks for the deeper dive.  The 404 was certainly coming from Elasticsearch, but as you point out, aliases_not_found is kind of a misleading error.  Now that I see what's going on, I should be able to address this.. There are only two lines of code that call this function, and they are both in repomgrcli.py (which is just the one for the fs repo type).\nAs you read this, you can see that it does, in fact, pass kwargs, including repository.  This function should probably be updated, as it's really only a holdover from Curator 3, with few (if any) modifications.  You can model your call after the one in repomgrcli.py, which are the click.option parameters/decorators above the main fs function.. You are correct, though.  Somehow the repository arg is omitted from those docs.  I'll try to fix that.. That would be because your timestring has an unsupported flag in it (%-j).  The supported flags are described here.  %j is supported, but not %-j.  You can see this on line 559 of your debug output:\n559:2017-02-17 14:29:47,056 DEBUG              curator.utils         get_date_regex:153  regex = \\d{4}\\.\\d{2}\\.\\-j\nHowever...\nIn #397, support for %j was added.  In #578, the regex map for %j was modified to support 1 to 3 digits, effectively making %j have a compatible regular expression to one for %-j.  I would try using %j instead.  I haven't had any follow-ups or complaints after the changes in #578, so I presume it should work for you.\nOtherwise, I'd recommend using source: creation_date or source: stats_result instead of the index name.  \nAlso, you should update to the latest version of Curator, 4.2.6.. I don't know what's naming your indices, but for parsing's sake, I always advise using zero-padded numbers to avoid this kind of confusion.  Just my 2\u00a2 . Please close this with a follow-up if it works for you, @jhammerman79 . Closing in expectation the issue is resolved.  Please re-open if this is not the case.. It doesn't just add a 1, it adds a 1 to the timestamp and a %d to the timestring:\ntimestring becomes: %Y.%m -> %Y.%m%d\nand\nindex_timestamp becomes: 2017.02 -> 2017.021\n```\n\n\n\nfrom datetime import datetime\ntimestring = '%Y.%m'\nindex_timestamp = '2017.02'\ndatetime.strptime(index_timestamp, timestring)\ndatetime.datetime(2017, 2, 1, 0, 0)\ntimestring += '%d'\nindex_timestamp += '1'\ndatetime.strptime(index_timestamp, timestring)\ndatetime.datetime(2017, 2, 1, 0, 0)\n```\n\n\n\nThis admittedly silly workaround was added to support older versions of Python, as without it, some crufty old versions of datetime didn't parse it properly (I don't remember the PR that fixed this issue).  \nIn any case, it doesn't change the result in any way, so it shouldn't be acting as a block to you.\n. This method also only returns the datetime timestamp of the local-only modifications.  It doesn't alter the index_timestamp or the timestring globally.. Closing due to lack of follow up.  Feel free to reopen if you need to.. I think your issue is wholly resolved by using the new period filter, available in Curator 5.0.. You mentioned that you have x-pack.  \n(Never mind, you're sending your monitoring data from remote). The Monitoring data will auto-prune at 7 days, so unless you want it for less than that, Curator is perhaps unnecessary.  Also, that duration is configurable. See xpack.monitoring.history.duration on https://www.elastic.co/guide/en/x-pack/current/monitoring-settings.html for more information \nThe other likelihood is that you have some permissions issues that are protecting these indices.  I see the monitoring indices when I run this:\n```\ncurator_cli show_indices\n.kibana\n.monitoring-data-2\n.monitoring-es-2-2017.02.23\n.monitoring-es-2-2017.02.24\n.monitoring-es-2-2017.02.25\n.monitoring-es-2-2017.02.26\n.monitoring-es-2-2017.02.27\n.monitoring-es-2-2017.02.28\n.monitoring-es-2-2017.03.01\n.monitoring-kibana-2-2017.02.23\n.monitoring-kibana-2-2017.02.24\n.monitoring-kibana-2-2017.02.25\n.monitoring-kibana-2-2017.02.26\n.monitoring-kibana-2-2017.02.27\n.monitoring-kibana-2-2017.02.28\n.monitoring-kibana-2-2017.03.01\n.monitoring-logstash-2-2017.02.23\n.monitoring-logstash-2-2017.02.24\n.monitoring-logstash-2-2017.02.25\n.monitoring-logstash-2-2017.02.26\n.monitoring-logstash-2-2017.02.27\n.monitoring-logstash-2-2017.02.28\n.monitoring-logstash-2-2017.03.01\n.security\n...(all of my other indices come after)\n```\nIf I do it from the python console, it looks like this:\n```\n\n\n\nimport elasticsearch\nimport curator\nclient = elasticsearch.Elasticsearch(host='esclient', http_auth='user:pass')\nilo = curator.IndexList(client)\nprint (sorted(ilo.working_list()))\n[u'.kibana', u'.monitoring-data-2', u'.monitoring-es-2-2017.02.23', u'.monitoring-es-2-2017.02.24',...]\n```\nThat's why I'm guessing it's a permissions issue.  It works for me.. @kylegoch Auto-pruning won't work if you're sending the indices to another cluster.  I re-read and noticed that, so don't rely on auto-prune when you're sending the monitoring data to another cluster.. Please upgrade to 4.2.6\n\n\n\nThere is a new option that catches a NoIndices or NoSnapshots condition called ignore_empty_list. continue_if_exception catches every exception but an empty list condition. . Please read about it in the documentation: https://www.elastic.co/guide/en/elasticsearch/client/curator/current/option_ignore_empty.html. This is not similar to #848, which is that --ignore-empty-list was not being properly caught.  You are not making use of this flag in the sample you pasted above.  That issue was also concerning errors being caught differently due to filter ordering. In your example above, you have only one filter, so ordering does not come into play.\nI'm only seeing a 404 error, indicating that an index is not found.  There are multiple possibilities. I would need to see debug-level logs to know what is going on here.  Please do not paste the logs as an image.  That makes it impossible to search the text of the logs.. I spun up a local instance of Elasticsearch 2.3.2 (didn't have 2.3.1) and created a movies index with 1 document:\n```\n$ curl -XPUT http://localhost:9200/movies -d '\n\n{ \"doc\": \"the movies index\" }\n'\n{\"acknowledged\":true}\n```\n\nThen I waited two minutes and tried to fire off the same cli command as you have above:\n$ curator_cli --host 127.0.0.1 --port 9200 delete_indices --filter_list '[{\"filtertype\": \"age\", \"source\": \"creation_date\", \"direction\": \"older\", \"unit\": \"minutes\", \"unit_count\": 2}]'\n2017-03-03 14:26:00,962 DEBUG              curator.utils             get_client:535  kwargs = {'url_prefix': '', 'aws_secret_key': None, 'http_auth': 'elastic:notachance', 'certificate': None, 'aws_key': None, 'aws_region': None, 'port': 9200, 'hosts': [u'127.0.0.1'], 'timeout': 30, 'use_ssl': False, 'master_only': False, 'client_cert': None, 'ssl_no_validate': False, 'client_key': None}\n2017-03-03 14:26:00,962 DEBUG              curator.utils             get_client:585  Not using \"requests_aws4auth\" python module to connect.\n2017-03-03 14:26:00,966 DEBUG              curator.utils          check_version:438  Detected Elasticsearch version 2.3.2\n2017-03-03 14:26:00,966 DEBUG         curator.singletons delete_indices_singleton:262  Validating provided filters: [{u'source': u'creation_date', u'direction': u'older', u'filtertype': u'age', u'unit_count': 2, u'unit': u'minutes'}]\n2017-03-03 14:26:00,967 DEBUG     curator.validators.SchemaCheck               __init__:26   Schema: <Schema(<function f at 0x102bd9230>, extra=PREVENT_EXTRA, required=False) object at 0x102330d90>\n2017-03-03 14:26:00,967 DEBUG     curator.validators.SchemaCheck               __init__:27   \"filters\" config: [{u'source': u'creation_date', u'direction': u'older', u'filtertype': u'age', u'unit_count': 2, u'unit': u'minutes'}]\n2017-03-03 14:26:00,967 DEBUG     curator.validators.SchemaCheck               __init__:26   Schema: <Schema({'source': Any(['name', 'creation_date', 'field_stats']), 'direction': Any(['older', 'younger']), 'timestring': Any([<type 'str'>, <type 'unicode'>, None]), 'unit_count': Coerce(int, msg=None), 'field': Any([<type 'str'>, <type 'unicode'>]), 'exclude': <function Boolean at 0x102bd9050>, 'epoch': Any([Coerce(int, msg=None), None]), 'unit': Any(['seconds', 'minutes', 'hours', 'days', 'weeks', 'months', 'years']), 'stats_result': Any(['min_value', 'max_value']), 'filtertype': Any([In(['age', 'alias', 'allocated', 'closed', 'count', 'forcemerged', 'kibana', 'none', 'opened', 'pattern', 'space', 'state'])])}, extra=PREVENT_EXTRA, required=False) object at 0x102bd5350>\n2017-03-03 14:26:00,967 DEBUG     curator.validators.SchemaCheck               __init__:27   \"filter\" config: {u'source': u'creation_date', u'direction': u'older', u'unit_count': 2, u'filtertype': u'age', u'unit': u'minutes'}\n2017-03-03 14:26:00,968 DEBUG     curator.validators.filters                      f:69   Filter #0: {u'direction': u'older', 'stats_result': 'min_value', u'filtertype': u'age', u'source': u'creation_date', 'epoch': None, 'timestring': None, 'exclude': False, u'unit_count': 2, u'unit': u'minutes'}\n2017-03-03 14:26:00,968 DEBUG          curator.indexlist          __get_indices:65   Getting all indices\n2017-03-03 14:26:00,971 DEBUG              curator.utils            get_indices:380  Detected Elasticsearch version 2.3.2\n2017-03-03 14:26:00,971 DEBUG              curator.utils            get_indices:395  All indices: [u'movies']\n2017-03-03 14:26:00,971 DEBUG          curator.indexlist       empty_list_check:182  Checking for empty list\n2017-03-03 14:26:00,971 DEBUG          curator.indexlist     __build_index_info:80   Building preliminary index metadata for movies\n2017-03-03 14:26:00,980 DEBUG          curator.indexlist          _get_metadata:145  Getting index metadata\n2017-03-03 14:26:00,980 DEBUG          curator.indexlist       empty_list_check:182  Checking for empty list\n2017-03-03 14:26:00,982 DEBUG          curator.indexlist       _get_index_stats:113  Getting index stats\n2017-03-03 14:26:00,982 DEBUG          curator.indexlist       empty_list_check:182  Checking for empty list\n2017-03-03 14:26:00,982 DEBUG          curator.indexlist           working_list:193  Generating working list of indices\n2017-03-03 14:26:00,982 DEBUG          curator.indexlist           working_list:193  Generating working list of indices\n2017-03-03 14:26:00,984 DEBUG          curator.indexlist     iterate_over_stats:122  Index: movies  Size: 795.0B  Docs: 0\n2017-03-03 14:26:00,984 DEBUG         curator.singletons            _do_filters:86   Running filters and testing for empty list object\n2017-03-03 14:26:00,984 DEBUG          curator.indexlist        iterate_filters:819  Iterating over a list of filters\n2017-03-03 14:26:00,984 DEBUG          curator.indexlist        iterate_filters:825  All filters: [{u'direction': u'older', 'stats_result': 'min_value', u'filtertype': u'age', u'source': u'creation_date', 'epoch': None, 'timestring': None, 'exclude': False, u'unit_count': 2, u'unit': u'minutes'}]\n2017-03-03 14:26:00,984 DEBUG          curator.indexlist        iterate_filters:827  Top of the loop: [u'movies']\n2017-03-03 14:26:00,984 DEBUG          curator.indexlist        iterate_filters:828  Un-parsed filter args: {u'direction': u'older', 'stats_result': 'min_value', u'filtertype': u'age', u'source': u'creation_date', 'epoch': None, 'timestring': None, 'exclude': False, u'unit_count': 2, u'unit': u'minutes'}\n2017-03-03 14:26:00,985 DEBUG     curator.validators.SchemaCheck               __init__:26   Schema: <Schema({'state': Any([<type 'str'>, <type 'unicode'>]), 'use_age': <function Boolean at 0x102c0d1b8>, 'field': Any([<type 'str'>, <type 'unicode'>, None]), 'epoch': Any([Coerce(int, msg=None), None]), 'source': Any([<type 'str'>, <type 'unicode'>]), 'disk_space': <type 'float'>, 'aliases': Any([<type 'str'>, [<type 'str'>], <type 'unicode'>, [<type 'unicode'>]]), 'value': Any([<type 'int'>, <type 'float'>, <type 'str'>, <type 'unicode'>, <type 'bool'>]), 'direction': Any([<type 'str'>, <type 'unicode'>]), 'max_num_segments': Coerce(int, msg=None), 'stats_result': Any([<type 'str'>, <type 'unicode'>, None]), 'count': Coerce(int, msg=None), 'reverse': Any([<type 'int'>, <type 'str'>, <type 'unicode'>, <type 'bool'>, None]), 'unit': Any([<type 'str'>, <type 'unicode'>]), 'kind': Any([<type 'str'>, <type 'unicode'>]), 'filtertype': Any([In(['age', 'alias', 'allocated', 'closed', 'count', 'forcemerged', 'kibana', 'none', 'opened', 'pattern', 'space', 'state'])]), 'allocation_type': Any([<type 'str'>, <type 'unicode'>]), 'timestring': Any([<type 'str'>, <type 'unicode'>, None]), 'unit_count': Coerce(int, msg=None), 'key': Any([<type 'str'>, <type 'unicode'>]), 'exclude': Any([<type 'int'>, <type 'str'>, <type 'unicode'>, <type 'bool'>, None])}, extra=PREVENT_EXTRA, required=False) object at 0x102bdbf90>\n2017-03-03 14:26:00,985 DEBUG     curator.validators.SchemaCheck               __init__:27   \"filter\" config: {u'direction': u'older', 'stats_result': 'min_value', u'filtertype': u'age', u'source': u'creation_date', 'epoch': None, 'timestring': None, 'exclude': False, u'unit_count': 2, u'unit': u'minutes'}\n2017-03-03 14:26:00,986 DEBUG          curator.indexlist        iterate_filters:835  Parsed filter args: {u'direction': u'older', 'stats_result': 'min_value', u'filtertype': u'age', u'source': u'creation_date', 'epoch': None, 'timestring': None, 'exclude': False, u'unit_count': 2, u'unit': u'minutes'}\n2017-03-03 14:26:00,986 DEBUG              curator.utils        iterate_filters:844  Filter args: {u'direction': u'older', 'stats_result': 'min_value', u'source': u'creation_date', 'epoch': None, 'timestring': None, 'exclude': False, u'unit_count': 2, u'unit': u'minutes'}\n2017-03-03 14:26:00,986 DEBUG              curator.utils        iterate_filters:845  Pre-instance: [u'movies']\n2017-03-03 14:26:00,986 DEBUG          curator.indexlist          filter_by_age:420  Filtering indices by age\n2017-03-03 14:26:00,986 DEBUG          curator.indexlist           working_list:193  Generating working list of indices\n2017-03-03 14:26:00,986 DEBUG          curator.indexlist           __actionable:35   Index movies is actionable and remains in the list.\n2017-03-03 14:26:00,986 DEBUG          curator.indexlist            __excludify:58   Remains in actionable list: Index \"movies\" age (1488576014), direction: \"older\", point of reference, (1488576240)\n2017-03-03 14:26:00,986 DEBUG              curator.utils        iterate_filters:847  Post-instance: [u'movies']\n2017-03-03 14:26:00,986 DEBUG          curator.indexlist       empty_list_check:182  Checking for empty list\n2017-03-03 14:26:00,987 DEBUG     curator.actions.delete_indices               __init__:421  master_timeout value: 30s\n2017-03-03 14:26:00,987 DEBUG         curator.singletons            _actionator:63   Doing the singleton \"delete_indices\" action here.\n2017-03-03 14:26:00,987 DEBUG          curator.indexlist       empty_list_check:182  Checking for empty list\n2017-03-03 14:26:00,987 INFO      curator.actions.delete_indices              do_action:478  Deleting selected indices: [u'movies']\n2017-03-03 14:26:00,987 INFO      curator.actions.delete_indices           __chunk_loop:453  ---deleting index movies\n2017-03-03 14:26:01,009 DEBUG              curator.utils            get_indices:380  Detected Elasticsearch version 2.3.2\n2017-03-03 14:26:01,010 DEBUG              curator.utils            get_indices:395  All indices: []\n2017-03-03 14:26:01,010 DEBUG     curator.actions.delete_indices         _verify_result:440  Successfully deleted all indices on try #1\n2017-03-03 14:26:01,010 INFO          curator.singletons            _actionator:82   Singleton \"delete_indices\" action completed.\nAs you can see, it works just fine for me.\nThis leaves me with a few possibilities and guesses, but no definitive answers.\n\nYou are using a plugin that is affecting the results of API calls (security, or otherwise).\nYour cluster state is unable to be updated because of something, possibly related to 1\nYour cluster is corrupted somehow\n???\n\nI can't fully explain why you're having a 404 response.  Have you looked at the Elasticsearch server logs for the server at the IP you connected to?. I don't know that Docker itself would be a problem. If Curator is connecting, all it knows is that it's connected to Elasticsearch.\nindex_not_found_exception That sounds semi-reasonable, if your index is indeed empty.  That would mean there are zero segments in it.. Closing this as it seems a non-issue.. The fix is not in 4.2.6 because the patch came after 4.2.6 was released. \nYou probably omitted this line in your patch:\nversion = get_version(self.client). Hmm.  Not sure what might be going on for you, but it tested completely fine across all versions of Python (2.7, 3.4, 3.5) Elasticsearch (2.0, 2.1, 2.2, 2.3, 2.4, 5.0, 5.1). Subsequent builds also test this code against 5.2, and it all passes.\n\n. That includes integration tests, so I know it works against Elasticsearch.. No more response. Closing. This feature is as simple as adding the opened filtertype :\n- filtertype: opened\n  exclude: True\nThis will exclude all open indices.  You can even test this at the command line with the curator_cli command, like this:\n$ curator_cli show_indices --filter_list '{\"filtertype\":\"opened\", \"exclude\":true}'\nlogstash-2017.03.01\nmetricbeat-2017.03.01\nNote that this command only runs without extra flags if you have a properly configured curator.yml file in the executing user's home directory in a .curator subdirectory, i.e. $HOME/.curator/curator.yml.\nSince this feature is already available, I'm closing this issue.. You can't \"unset\" it, as there are no examples of that in the documentation, but you can use an arbitrary value:\n\nindex.routing.allocation.exclude.{attribute}\nAssign the index to a node whose {attribute} has none of the comma-separated values.\n\nSet the exclude to an arbitrary value that none of your Elasticsearch will recognize.  If you use an arbitrary value, none of the nodes should have that value, so the shard can be assigned there. . That's worth considering.  I'll look into it.. @psaiz I did some testing. This is only possible in the 5.x branch.  In the 2.x branch, it fails.  The API does not allow for this in 2.x.\n```\n\n\n\nclient = elasticsearch.Elasticsearch()\nclient.indices.create(index='alltest')\n{u'acknowledged': True}\nclient.indices.put_settings(index='alltest', body={'index.routing.allocation.exclude.foo': 'bar'})\n{u'acknowledged': True}\nclient.indices.put_settings(index='alltest', body={'index.routing.allocation.exclude.foo': None})\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/Users/buh/Library/Python/2.7/lib/python/site-packages/elasticsearch/client/utils.py\", line 71, in _wrapped\n    return func(args, params=params, *kwargs)\n  File \"/Users/buh/Library/Python/2.7/lib/python/site-packages/elasticsearch/client/indices.py\", line 564, in put_settings\n    '_settings'), params=params, body=body)\n  File \"/Users/buh/Library/Python/2.7/lib/python/site-packages/elasticsearch/transport.py\", line 318, in perform_request\n    status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)\n  File \"/Users/buh/Library/Python/2.7/lib/python/site-packages/elasticsearch/connection/http_urllib3.py\", line 127, in perform_request\n    self._raise_error(response.status, raw_data)\n  File \"/Users/buh/Library/Python/2.7/lib/python/site-packages/elasticsearch/connection/base.py\", line 122, in _raise_error\n    raise HTTP_EXCEPTIONS.get(status_code, TransportError)(status_code, error_message, additional_info)\nelasticsearch.exceptions.RequestError: TransportError(400, u'action_request_validation_exception', u'Validation Failed: 1: no settings to update;')\n```\n\n\n\nSo I tried sending an empty string:\n```\n\n\n\nclient.indices.put_settings(index='alltest', body={'index.routing.allocation.exclude.foo':''})\n{u'acknowledged': True}\nclient.indices.get_settings(index='alltest')\n{u'alltest': {u'settings': {u'index': {u'number_of_replicas': u'1', u'uuid': u'hT-yeFl6TAmIV7ILMofGTw', u'number_of_shards': u'5', u'creation_date': u'1489691896090', u'version': {u'created': u'2040299'}, u'routing': {u'allocation': {u'exclude': {u'foo': u''}}}}}}}\n```\nIt still doesn't do any better than the arbitrary value approach I mentioned.\n\n\n\nSo, this will make it into Curator 5, but not Curator 4, as it will not support the 2.x versions at all.. Unfortunately this is moot until AWS ES supports Curator natively.. It will not be merged.  That has been stated on multiple occasions.  I'm not going to add a one-off to resolve that issue.. The chief reason is that it would iterate over that call for every index, rather than as a single call.. This is firmly in the adopt me category.  I do not have an AWS ES cluster to test against.  This will require community contribution to be resolved.. This should be addressed in #1084 . This is a duplicate of #880. The bundled package most assuredly does have py-yaml:\n```\ncat /etc/os-release\nNAME=\"Ubuntu\"\nVERSION=\"16.04.1 LTS (Xenial Xerus)\"\nID=ubuntu\nID_LIKE=debian\nPRETTY_NAME=\"Ubuntu 16.04.1 LTS\"\nVERSION_ID=\"16.04\"\nHOME_URL=\"http://www.ubuntu.com/\"\nSUPPORT_URL=\"http://help.ubuntu.com/\"\nBUG_REPORT_URL=\"http://bugs.launchpad.net/ubuntu/\"\nVERSION_CODENAME=xenial\nUBUNTU_CODENAME=xenial\napt list | grep ^elasticsearch-curator\nelasticsearch-curator/stable,now 4.2.6 amd64 [installed]\n```\n```\nls /opt/elasticsearch-curator/lib/python3.5/_yaml.cpython-35m-x86_64-linux-gnu.so\n/opt/elasticsearch-curator/lib/python3.5/_yaml.cpython-35m-x86_64-linux-gnu.so\n```\nI have this running personally out of my own cron.\nDo you have some other Python 3.5 on your system which might be in conflict?  This would be the very first time I have heard of any issues with the frozen release (which is what you have). Please share your configuration, and os + patch level, I would love to spin up a VM and see if I can replicate what you are seeing.. Is it possible you had an older version of Curator which wasn't completely cleaned out? . The debian packaged elasticsearch-curator does not need any python installed, as it is frozen, and comes with one bundled with all necessary libraries except for ones it expects the system to provide.  If you're using pip, you don't need the package.  So, if you tried the package and it failed, that sounds as if there's some other development dependency or missing library in your Docker images.  \nIt may take some time to track down.  I don't know when I'll get to it, but will when I can.  If you've got a valid work-around using pip, then that's great too.. The bundled version of Python is 3.5, which you probably gathered.. It shouldn't, though.  It's like running Python in a virtualenv.  Nothing outside of it should be considered.  That's why I'm leaning toward the missing system library idea.... Installing pyyaml generally results in a compiled version of libyaml.  I wonder if the libraries this was compiled against are not present in your docker image.  It would be lovely if you could test that theory. No worries if you can't.. This has been months without an update, and I cannot duplicate it.  I'm closing it for now.  If you feel this has been done in error, please feel free to reopen it.. Sorry for the confusion.\nIf you're using 5.3, I would upgrade to Curator 5.0. Very strange.  It's clearly on PyPI, and I can download it from there, using the provided link.\nroot@ubuntu:~# pip install -U --user elasticsearch-curator==5.0.1\nCollecting elasticsearch-curator==5.0.1\n  Could not find a version that satisfies the requirement elasticsearch-curator==5.0.1 (from versions: 0.6.0, 0.6.1, 0.6.2, 1.0.0, 1.1.0, 1.1.2, 1.1.3, 1.2.0, 1.2.1, 1.2.2, 2.0.0, 2.0.1, 2.0.2, 2.1.0, 2.1.1, 2.1.2, 3.0.0, 3.0.1, 3.0.2, 3.0.3, 3.1.0, 3.2.0, 3.2.1, 3.2.2, 3.2.3, 3.3.0, 3.4.0, 3.4.1, 3.5.0, 3.5.1, 4.0.0, 4.0.1, 4.0.3, 4.0.4, 4.0.5, 4.0.6, 4.1.0, 4.1.1, 4.1.2, 4.2.0, 4.2.1, 4.2.3, 4.2.3.post1, 4.2.4, 4.2.5, 4.2.6)\nNo matching distribution found for elasticsearch-curator==5.0.1\nAnd yet...\nroot@ubuntu:~# pip search elasticsearch-curator | grep elasticsearch-curator\nelasticsearch-curator (5.0.1)                  - Tending your Elasticsearch indices\nAnd if I run it with -vvv, I see lots of lines like this (for everything up to, and including 4.2.6):\nFound link https://pypi.python.org/packages/07/87/847725b631011213cec22a800074ee0489f7c857527f46de96485cf3e118/elasticsearch-curator-4.0.5.tar.gz#md5=df508ecec57bb5ea28f7bb52c4b5852f (from https://pypi.python.org/simple/elasticsearch-curator/), version: 4.0.5\nAnd yet, when I visit https://pypi.python.org/simple/elasticsearch-curator/, I see 5.0.1 right there!\nI'm guessing it's something wrong with PyPI at this point.\nYou should be able to make it work by running:\npip install -U https://github.com/elastic/curator/archive/v5.0.1.tar.gz\nor, you could put the huge PyPI URL in there:\npip install -U https://pypi.python.org/packages/07/de/c3b95535b0695fb67847fe7ef7535446a9bdb593de2cbd4594b5ef75febf/elasticsearch-curator-5.0.1.tar.gz\nI'm not sure what's up with new things added to PyPI, or if there's some cache that's not updated on their end, but it should work itself out at some point.\n. For some strange reason I cannot understand, the regular pip command now works:\nroot@ubuntu:~# pip install -U --user elasticsearch-curator==5.0.1\nCollecting elasticsearch-curator==5.0.1\nRequirement already up-to-date: click>=6.7 in ./.local/lib/python2.7/site-packages (from elasticsearch-curator==5.0.1)\nRequirement already up-to-date: pyyaml>=3.10 in ./.local/lib/python2.7/site-packages (from elasticsearch-curator==5.0.1)\nRequirement already up-to-date: certifi>=2017.1.23 in ./.local/lib/python2.7/site-packages (from elasticsearch-curator==5.0.1)\nCollecting elasticsearch<6.0.0,>=5.3.0 (from elasticsearch-curator==5.0.1)\n  Using cached elasticsearch-5.3.0-py2.py3-none-any.whl\nRequirement already up-to-date: voluptuous>=0.9.3 in ./.local/lib/python2.7/site-packages (from elasticsearch-curator==5.0.1)\nRequirement already up-to-date: urllib3<2.0,>=1.8 in ./.local/lib/python2.7/site-packages (from elasticsearch<6.0.0,>=5.3.0->elasticsearch-curator==5.0.1)\nRequirement already up-to-date: setuptools>=0.6b1 in ./.local/lib/python2.7/site-packages (from voluptuous>=0.9.3->elasticsearch-curator==5.0.1)\nRequirement already up-to-date: appdirs>=1.4.0 in ./.local/lib/python2.7/site-packages (from setuptools>=0.6b1->voluptuous>=0.9.3->elasticsearch-curator==5.0.1)\nRequirement already up-to-date: packaging>=16.8 in ./.local/lib/python2.7/site-packages (from setuptools>=0.6b1->voluptuous>=0.9.3->elasticsearch-curator==5.0.1)\nRequirement already up-to-date: six>=1.6.0 in ./.local/lib/python2.7/site-packages (from setuptools>=0.6b1->voluptuous>=0.9.3->elasticsearch-curator==5.0.1)\nRequirement already up-to-date: pyparsing in ./.local/lib/python2.7/site-packages (from packaging>=16.8->setuptools>=0.6b1->voluptuous>=0.9.3->elasticsearch-curator==5.0.1)\nInstalling collected packages: elasticsearch, elasticsearch-curator\n  Found existing installation: elasticsearch 2.4.1\n    Uninstalling elasticsearch-2.4.1:\n      Successfully uninstalled elasticsearch-2.4.1\n  Found existing installation: elasticsearch-curator 4.3.0\n    Uninstalling elasticsearch-curator-4.3.0:\n      Successfully uninstalled elasticsearch-curator-4.3.0\nSuccessfully installed elasticsearch-5.3.0 elasticsearch-curator-5.0.1\nClosing.. Thanks for bringing it to my attention.\nWe had an AWS host where we did our package staging and repository management get deleted \ud83d\ude2e by accident.\nI had to rebuild from scratch, and though the S3 repository was intact, the scripts we used were gone.  I ended up using reprepro to rebuild the APT repo, and that's the behavior it has.  I'm going to investigate using another repo manager like aptly to see if it will preserve the older versions in the Packages file.. See if it works for you now.  I've rebuilt the repository using aptly instead.. Thanks for contributing!. This looks great!  Thanks for contributing to Curator.  This ought to work well with the new period filter!. Will you please squash these commits?  It makes it much easier to keep the master and 5.x branches in sync.\nOther than that, LGTM\u2122. @panda87 \n\nbut this is lack the capability of how to tell the \"dest\" conf file the current week number in an automated way.\n\nIt would be great if date math were supported for index names in dest, but it isn't.  I can make it possible to do strftime renaming, like with the name option for the create_index action, but it won't do any date-math.  \nWould that be acceptable?  It would only allow for the current week to be shown, so it would not necessarily look correct, as \"this\" week would contain \"last\" week's data.\n\nAlso, once the action is done, can we add a flag that will delete the old merged daily indices?\n\nThis is as simple as just making another action with the same filters, but with delete_indices being the action.  I would ensure that the reindex action has continue_if_exception: False, so that it can't proceed if there's an error.\n\nmaybe there is an option to add a basic validation in the end of the action to make sure all the results merged.\n\nThe reindex action already checks to make sure the dest index is present, but there is no conclusive way, in or out of elasticsearch, to validate the reindex.  This is mostly because a reindex might involve a query, and only reindex a subset of results. Or it might be merging all documents from multiple.  Or you might be reformatting the data through an ingest node. If you've specified one of these, how can Curator know what the resulting index should look like?\nA safer bet would be a \"temporary\" snapshot and perhaps a dedicated repository, too.  That way you can snapshot that week's worth of data, and then do some other validation on the merged index.  Delete the temporary snapshot when you're satisfied.. @joskfg This is great for people using Watcher, but that's a commercial product and not everyone has access to it.. > It would be great if date math were supported for index names in dest, but it isn't. I can make it possible to do strftime renaming, like with the name option for the create_index action, but it won't do any date-math.\nApparently, date math is supported in dest.  This was just addressed properly in #1020 . This error is limited to the allocation action, and only when used with curator_cli.. Manual testing confirmed bug.\nManual testing also confirms fix:\n```\n$ ./run_singleton.py --dry-run --config stdout.yml allocation --key node_type --allocation_type require --value \"\" --filter_list '[{\"filtertype\": \"pattern\",\"kind\": \"regex\",\"value\": \".*\"},{\"filtertype\": \"age\",\"source\":\"name\",\"direction\":\"older\",\"timestring\": \"%Y.%m.%d\",\"unit\":\"days\",\"unit_count\":7}]'\n... skipping a bit here, since it's debug logging ...\n2017-04-19 13:51:30,914 DEBUG         curator.singletons            _actionator:63   Doing the singleton \"allocation\" action here.\n2017-04-19 13:51:30,914 INFO               curator.utils           show_dry_run:795  DRY-RUN MODE.  No changes will be made.\n2017-04-19 13:51:30,914 INFO               curator.utils           show_dry_run:798  (CLOSED) indices may be shown that may not be acted on by action \"allocation\".\n2017-04-19 13:51:30,914 INFO               curator.utils           show_dry_run:805  DRY-RUN: allocation: logstash-2016.10.22 with arguments: {'body': {'index.routing.allocation.require.node_type': u''}}\n2017-04-19 13:51:30,914 INFO               curator.utils           show_dry_run:805  DRY-RUN: allocation: logstash-2017.02.21 with arguments: {'body': {'index.routing.allocation.require.node_type': u''}}\n2017-04-19 13:51:30,914 INFO               curator.utils           show_dry_run:805  DRY-RUN: allocation: logstash-2017.03.13 with arguments: {'body': {'index.routing.allocation.require.node_type': u''}}\n2017-04-19 13:51:30,914 INFO               curator.utils           show_dry_run:805  DRY-RUN: allocation: logstash-2017.04.05 (CLOSED) with arguments: {'body': {'index.routing.allocation.require.node_type': u''}}\n2017-04-19 13:51:30,914 INFO               curator.utils           show_dry_run:805  DRY-RUN: allocation: logstash-2017.04.06 with arguments: {'body': {'index.routing.allocation.require.node_type': u''}}\n2017-04-19 13:51:30,914 INFO               curator.utils           show_dry_run:805  DRY-RUN: allocation: logstash-2017.04.07 with arguments: {'body': {'index.routing.allocation.require.node_type': u''}}\n2017-04-19 13:51:30,914 INFO               curator.utils           show_dry_run:805  DRY-RUN: allocation: logstash-2017.04.08 with arguments: {'body': {'index.routing.allocation.require.node_type': u''}}\n2017-04-19 13:51:30,915 INFO               curator.utils           show_dry_run:805  DRY-RUN: allocation: logstash-2017.04.09 with arguments: {'body': {'index.routing.allocation.require.node_type': u''}}\n2017-04-19 13:51:30,915 INFO               curator.utils           show_dry_run:805  DRY-RUN: allocation: logstash-2017.04.10 with arguments: {'body': {'index.routing.allocation.require.node_type': u''}}\n2017-04-19 13:51:30,915 INFO               curator.utils           show_dry_run:805  DRY-RUN: allocation: logstash-2017.04.11 with arguments: {'body': {'index.routing.allocation.require.node_type': u''}}\n2017-04-19 13:51:30,915 INFO               curator.utils           show_dry_run:805  DRY-RUN: allocation: logstash-2017.04.12 with arguments: {'body': {'index.routing.allocation.require.node_type': u''}}\n2017-04-19 13:51:30,915 INFO               curator.utils           show_dry_run:805  DRY-RUN: allocation: metricbeat-2017.04.05 (CLOSED) with arguments: {'body': {'index.routing.allocation.require.node_type': u''}}\n2017-04-19 13:51:30,915 INFO               curator.utils           show_dry_run:805  DRY-RUN: allocation: metricbeat-2017.04.06 with arguments: {'body': {'index.routing.allocation.require.node_type': u''}}\n2017-04-19 13:51:30,915 INFO               curator.utils           show_dry_run:805  DRY-RUN: allocation: metricbeat-2017.04.07 with arguments: {'body': {'index.routing.allocation.require.node_type': u''}}\n2017-04-19 13:51:30,915 INFO               curator.utils           show_dry_run:805  DRY-RUN: allocation: metricbeat-2017.04.08 with arguments: {'body': {'index.routing.allocation.require.node_type': u''}}\n2017-04-19 13:51:30,915 INFO               curator.utils           show_dry_run:805  DRY-RUN: allocation: metricbeat-2017.04.09 with arguments: {'body': {'index.routing.allocation.require.node_type': u''}}\n2017-04-19 13:51:30,915 INFO               curator.utils           show_dry_run:805  DRY-RUN: allocation: metricbeat-2017.04.10 with arguments: {'body': {'index.routing.allocation.require.node_type': u''}}\n2017-04-19 13:51:30,915 INFO               curator.utils           show_dry_run:805  DRY-RUN: allocation: metricbeat-2017.04.11 with arguments: {'body': {'index.routing.allocation.require.node_type': u''}}\n2017-04-19 13:51:30,915 INFO               curator.utils           show_dry_run:805  DRY-RUN: allocation: metricbeat-2017.04.12 with arguments: {'body': {'index.routing.allocation.require.node_type': u''}}\n2017-04-19 13:51:30,915 INFO          curator.singletons            _actionator:82   Singleton \"allocation\" action completed.\n. Why don't you submit it as a pull request?. Will close this after I add some integration tests and documentation.. This is a known issue when using Elastic Cloud, or connecting through a proxy. The issue is not Curator, nor can Curator compensate for it. I've raised an issue with Elastic Cloud, but do not know when it may be addressed. . When trying to send:\nPUT /\nIt gets properly URL encoded by the python client (and/or urllib3):\n2017-04-13 10:09:53,559 DEBUG     urllib3.connectionpool          _make_request:395  https://cb75052bb2dca95f5461337874a1a154.us-west-2.aws.found.io:9243 \"PUT /%3Cdatemath-%7Bnow%2Fd%2B1d%7D%3E HTTP/1.1\" 400 203\n2017-04-13 10:09:53,559 WARNING            elasticsearch       log_request_fail:96   PUT https://cb75052bb2dca95f5461337874a1a154.us-west-2.aws.found.io:9243/%3Cdatemath-%7Bnow%2Fd%2B1d%7D%3E [status:400 request:0.053s]\n```\nThe Python client reports the error:\n2017-04-13 10:09:53,559 DEBUG              elasticsearch       log_request_fail:109  < {\"error\":{\"root_cause\":[{\"type\":\"parse_exception\",\"reason\":\"operator not supported for date math [/d 1d]\"}],\"type\":\"parse_exception\",\"reason\":\"operator not supported for date math [/d 1d]\"},\"status\":400}\nThis is missing the plus symbol, which is %2B, but the %2B is in the proper place in the url encoded string that was sent.  That's why this is a proxy issue.. You can also see this in action with an nginx proxy here.. Unfortunately, no.\nRead about the native python client call (note, no options for anything other than master node timeout.)\nRead the official API documentation (again, no options for timeout of any kind).\nThe call simply blocks until it's done.  That's all that can be said about it.\nIdeas on what to try include trying to catch an Elasticsearch  TransportError and then test to see if the snapshot exists any more.  It seems like it gives the 404, even if the underlying delete is incomplete.. By the way, the retry is a function of urllib3, not Curator or the Elasticsearch client module.. I think a test for the presence of an alias can be added to cover this.  It's an edge case I hadn't considered.. It is possible, but you have to use double-quotes to encapsulate the --filter_list, and then escape every other double-quote, like this:\n$ i=14; curator_cli show_indices --filter_list \"[{\\\"filtertype\\\":\\\"age\\\",\\\"source\\\":\\\"creation_date\\\",\\\"direction\\\":\\\"older\\\",\\\"unit\\\":\\\"days\\\",\\\"unit_count\\\": \\\"$i\\\" },{\\\"filtertype\\\":\\\"pattern\\\",\\\"kind\\\":\\\"prefix\\\",\\\"value\\\":\\\"logstash\\\"}]\"\nlogstash-2017.04.11. And this isn't something I plan on putting in the documentation.  It's standard shell behavior.. I don't publish it.  Someone else may, but I haven't kept track of it.  Duplicate of #528.. Wow, that's a nasty bug.  I'm so sorry you hit that.  A fix is in progress right now.  It will be in 5.0.2 and back-ported to 4.3.1.. So you know, this comes upstream from the voluptuous module: https://github.com/alecthomas/voluptuous/issues/286. Ah. I found it.  Simple fix.  Wait for the update.  Thanks for reporting this and making Curator better.. Why are you launching it as a module?  While there is a module, you should be launching this as a regular command-line binary because setup.py builds entry_points for both curator and curator_cli.   The entry_point will launch Curator appropriately.  \nI assume you installed via pip, since you're running this via python. As such, the entry_point should have installed to either /usr/bin/curator_cli or /usr/local/bin/curator_cli.\nIf those are in your PATH, and a correctly configured curator.yml file is in ~/.curator/curator.yml, then I expect you should be able to run (and see somewhat similar output to) the following:\n$ curator_cli show_indices --verbose --header\nIndex                             State     Size      Docs Pri Rep   Creation Timestamp\n.kibana                            open  653.7KB       418   1   1 2017-01-26T06:01:23Z\n.ml-anomalies-shared               open   37.1MB    184100   5   1 2017-05-04T20:16:22Z\n.ml-notifications                  open   37.9KB        44   1   1 2017-05-04T20:16:24Z\n.ml-state                          open  115.4KB         4   5   1 2017-05-04T20:16:27Z\n.monitoring-alerts-2               open   13.8KB         2   1   1 2017-05-04T19:13:00Z\n.monitoring-data-2                 open   61.5KB        24   1   1 2017-01-26T04:10:33Z\n.monitoring-es-2-2017.05.02        open    1.2GB   1597278   1   1 2017-05-01T23:59:59Z\n.monitoring-es-2-2017.05.03        open    1.2GB   1610394   1   1 2017-05-03T00:00:01Z\n.monitoring-es-2-2017.05.04        open    1.2GB   1653678   1   1 2017-05-04T00:00:03Z\n.monitoring-es-2-2017.05.05        open    1.1GB   1753512   1   1 2017-05-05T00:00:01Z\n... truncated output for brevity. If you've installed in a virtualenv, why not just call run_singleton.py in the base directory?. You almost certainly have a misconfiguration in ~/.curator/curator.yml, to see nothing more in the logs.  Will you please show what you have there?. Oh, my.  You had nothing, so everything was default.  Yeah, that wouldn't provide any http_auth.  The only way for Curator to know that (and provide the message you wanted to see) is to set blacklist: [] in the logging section.  The default blacklist omits some of those connection messages, as they can be quite verbose.. @ejoso, this ticket is closed.  Please start a thread at https://discuss.elastic.co/c/elasticsearch for usage questions like these.  The answers will be much more useful to the community there.. This is an unfortunate side effect of waiting for each minor version to come out before updating the compatibility\u2014a practice I gave up on after one too many complaint.\n4.x Curator is compatible with Elasticsearch 5.x, so long as you are running anything 4.2.6 or later (4.3.1 is the most recent of the 4.x branch).\nThere's also no reason to run Curator 4.x if you're running Elasticsearch 5.x, especially with the new features added in Curator 5.. There was a split for the 5.x branch, and 5.x now trails master, while 4.x is managed separately.\nYou should be looking at the 4.x branch Changelog, or at the official documentation, which shows all of the documentation changes for minor releases since 3.3 by way of a drop-down.  \nAgain, I apologize for the inconvenience, but I got in the habit of tracking the most recent version I could trust until I updated tests for Travis.  Please understand the difficulties of spanning major versions by using an older version of the python API (2.4.1).  I didn't want to have anything break.  I was nervous about future code-arounds biting users.  Instead, I had them unable to use new versions of ES until I had updated Curator.  However, once I had integration testing with all current releases of all minor branches of Elasticsearch, I felt safe to put a blanket support for all 5.x versions.  Before that, Curator 4 supported ES 5 insofar as you had the most recent version of Curator. \nThis is why, with Curator 5, I opted to forgo any sort of reverse compatibility, so I wouldn't have to do that again.\nSee 4.0.0, 4.1.0, 4.2.0, 4.2.3, 4.2.4, 4.2.6.\nAs stated, now I track all new versions in Travis: 4.2.6, 4.3.1, 5.0.2.\nI apologize again for the inconvenience, but there's nothing I will be changing, since the most recent version of Curator 4 works just fine with ES 5.x.\n. There are two kinds of exception. There's an empty list, which generates the NoIndices or NoSnapshots exceptions, and everything else. Try using ignore_empty_list to continue past the exception you hit. \nThe reason for two different kinds of exceptions is that an empty list might be a benign, expected condition for new clusters and indices. The other kind of exception is almost always bad. . I'm adding a blurb to the continue_if_exception documentation to help inform about this.. And I've merged and cherry-picked it into all doc branches, 4.2 and up.  Our doc builder should add the bits to these branches soon.. I need a bit more than that. Please encapsulate your console and log text between triple back-ticks like this \n```\nText here\n```\n\nI need more lines above and below. \nI may also need to see the client configuration file, and you may need to set blacklist: [] in the logging configuration along with loglevel: DEBUG to increase the output, as indicated in the exception. . Please add blacklist: [] to the client configuration file in the logging section and run this again. Something else is happening and it's not showing in the default configuration. . Never mind. I see you just added that. . What version of Elasticsearch are you using?\nWhat is the size of hoard_v0.2_2017-05-04_fa837b4253af4d48b21e9e0a83041cd1 on the source?\nWhat is the output of the following?\nGET /_snapshot/force-merge/force-merge\n. Thanks for providing the debug logs.  I think I know what's going on now.\nThe debug information suggests that the restore request was successful:\n2017-05-15 13:40:40,473 INFO               elasticsearch    log_request_success:82   POST http://30.0.0.7:9200/_snapshot/force-merge/force-merge/_restore?wait_for_completion=false [status:200 request:0.188s]\n2017-05-15 13:40:40,473 DEBUG              elasticsearch    log_request_success:84   > {\"ignore_unavailable\": false, \"partial\": false, \"include_aliases\": false, \"rename_replacement\": \"\", \"rename_pattern\": \"\", \"indices\": [\"hoard_v0.2_2017-05-04_fa837b4253af4d48b21e9e0a83041cd1\"], \"include_global_state\": false}\n2017-05-15 13:40:40,473 DEBUG              elasticsearch    log_request_success:85   < {\"accepted\":true}\nThe next step it takes is to check that the restore is still happening:\n2017-05-15 13:40:40,475 INFO               elasticsearch    log_request_success:82   GET http://30.0.0.7:9200/hoard_v0.2_2017-05-04_fa837b4253af4d48b21e9e0a83041cd1/_recovery?human=true [status:200 request:0.002s]\n2017-05-15 13:40:40,475 DEBUG              elasticsearch    log_request_success:84   > None\n2017-05-15 13:40:40,475 DEBUG              elasticsearch    log_request_success:85   < {}\n...but nothing came back from Elasticsearch (The {} entry).  That's a problem.  The API shows that something should come back.\nThis API call should have done what the documentation says it should do, but for some reason, it doesn't even take 3 milliseconds to report back a failure.\nMy guess is that those 3 milliseconds are too fast a response time on your cluster, and it hasn't been enough time for the cluster to actually initiate a _recovery entry in the cluster state.  Hypothetically, that would create a situation where an empty {} response would come back.  I don't know that I can duplicate this scenario in integration testing, but I can do it in unit testing for sure.  The code doesn't currently have a way to process an empty response., as I never anticipated an empty response coming in, and I think this is what you're experiencing.\nThanks for reporting this.  I will try to have a bug fix release out soon.. Confirmed with new unit test:\ndef test_empty_recovery(self):\n        client = Mock()\n        client.indices.recovery.return_value = {}\n        self.assertFalse(\n            curator.restore_check(client, testvars.named_indices)\n        )\nresult:\n```\n$ python test/run_tests.py test.unit.test_utils:TestRestoreCheck\n.E..\n======================================================================\nERROR: test_empty_recovery (test.unit.test_utils.TestRestoreCheck)\n\nTraceback (most recent call last):\n  File \"/Users/buh/Dropbox/Development/curator/test/unit/test_utils.py\", line 744, in test_empty_recovery\n    curator.restore_check(client, testvars.named_indices)\n  File \"/Users/buh/Dropbox/Development/curator/curator/utils.py\", line 1439, in restore_check\n    for shard in range(0, len(response[index]['shards'])):\nKeyError: 'index-2015.01.01'\n\nRan 4 tests in 0.002s\nFAILED (errors=1)\nShutting down....\n. Adding the following code to `utils.py` at line 1438:\n    # This should address #962, where perhaps the cluster state hasn't yet\n    # had a chance to add a _recovery state yet, so it comes back empty.\n    if response == {}:\n        return False\nfixes the problem:\n$ python test/run_tests.py test.unit.test_utils:TestRestoreCheck\n....\n\nRan 4 tests in 0.001s\nOK\nShutting down....\n```. @Strijd Curator 5.0.3 is out on PyPI (pip), RPM, DEB, and Windows MSI and ZIP\nThe documentation hasn't caught up with the release version 5.0.3 yet, but that will catch up soon enough (and no documentation changed, other than the version number).\nPlease test 5.0.3 and let me know if that fixes the problem for you.  I would love to see the same debug logs for that time period, to see if the next iteration after the wait_interval has lapsed picks up where it should.\nSorry again for the bad experience, and thanks for reporting it thoroughly and responding to my debug requests promptly and accurately.. Please provide the debug logs with the same blacklist settings as before.. Wow.  This is really weird (but confirmed, nonetheless).\nSomehow using is not works for strings.\nI've changed this to != and it corrects the behavior for unicode, which is what comes back from the REST API anyway.\nI'm really sorry for the inconvenience to you.  Another patched version will be released shortly.. 5.0.4 is released to all repositories, including PyPI.\nThe documentation will catch up shortly.. I tested using your pasted data to confirm the bug and the fix, so I anticipate success.. At issue, for posterity:\n```\n\n\n\n'DONE' is not u'DONE'\nTrue\n'DONE' != u'DONE'\nFalse\n```. I'm sorry you've had a bad experience.  You are, indeed, hitting something I never foresaw happening.\n\n\n\nIs the box running Curator a very small machine?  Is that why 1.5G is a constraining factor?  Curator isn't obliged to run on the same machine as Elasticsearch itself, or even in the same subnet.  It only requires access to the REST endpoint of one of the nodes in the cluster.  This means you could have a larger machine run Curator without it requiring a larger machine.\nI mention this, because while it is possible to alter the chunk size, I hesitate to do so.  Allow me to explain my reasoning.\nFirst off, mappings that large per index are discouraged.  Indeed, if you have multiple _type values per index, that will pose a problem for future upgrading as the concept of _type has been removed for Elasticsearch 6.0.  In the future, Elasticsearch will only support one \"type\" (without using that terminology), or mapping per index.  This simplification will yield a tremendous benefit for indexing and searching performance, as well as index compression and storage density.  Consequently, giant mappings will be much more rare.  What I'm trying to gently suggest here is that your use of large mappings\u2013and presumably multiple types\u2013is not ideal and is likely to be a contributing factor, and that Elasticsearch developers have been working for a long time to gradually constrain such things as they lead to undesirable outcomes and performance issues.\nThe chunk_index_list function's primary function is to prevent the initial URL from being too large for Elasticsearch.  By default, the initial URL cannot be more than 4000 characters long.  Since Elasticsearch allows for providing a comma-separated list of indices to act on, the chunking function is there to prevent that default limit from being hit.  It apparently can also be used to limit the number of indices in a call, as you pointed out, but this was not its original intent.\nReducing the chunk size will increase the number of round-trips necessary to complete actions.  It is a tremendous benefit to be able to have Elasticsearch finish a batch of deletes in fewer passes.  A blanket change to the chunk_index_list function would affect the outbound requests, not just the one reading in the metadata\u2013which, as I understand it, is where your problem lies.\nA change to the chunk_index_list function to allow it to be selectable at call time is technically a breaking API change, which are usually reserved for major releases.  While I feel like I could ethically change it in a minor point version, that isn't the difficult part.  The difficult part is deciding whether to make chunk size definable in a configuration file, or to arbitrarily set it to some low number at just this ingestion point, which helps you, but slows things down for other users and adds to the load of the cluster\u2014which is a valid point with large mappings, as the cluster still has to isolate all of the metadata for only the requested indices.\nAs an afterthought, this issue is somewhat related to #795, and may be addressed in part by any changes made in the system to address this.\nAt any rate, I am still considering this.  These are my reasons for being hesitant to change things.. > This results in a unique document_type for each API call so that I don't end up with mapping conflicts. Are you saying that this would no longer work in ES6?\nYes.\n\nHow will it handle multiple document schemas within the same index, when different documents might contain varying types for a given field?\n\nIt won't.  You will have to create multiple indices with one mapping in each.  Perhaps that will be one index-type per API, so that the fields match.  It will require rethinking your indices and retention approach.\n\nUnfortunately, this bug means that curator uses so much memory that it frequently ends up either hanging\n\nWhile I appreciate that the current behavior is not what you'd like, the constraints you've placed on your nodes makes this seem more like a self-imposed difficulty than a bug.  Any process will hang when a system is starved for memory, and Curator is no exception.\n\nor OOM killing the master node, since they're running in EC2 with no swap, and ElasticSearch is configured to use the majority of memory on the host \n\nBy your own admission here your nodes are not running with 1/2 the system memory dedicated to Elasticsearch (leaving the other half behind to act as filesystem caching) per Elastic's official recommendations. If 3G of sudden RAM consumption can bring down an Elasticsearch node, you haven't left any room for anything else.  While it is possible to alter Curator to make it possible for you to keep doing this, the configuration described for your cluster nodes is not recommended.  Curator has behaved well in clusters that meet those recommendations.\nWhile I am not ruling out making an adjustment to help you, the assumption that this behavior is unwarranted in all cases (which would be an actual bug) is incorrect.. I did some character counting.  The chunk size of 3072 breaks up the index list into comma-separated list of indices. Each index of the pattern logstash-YYYY.MM.dd is 21 characters (when you consider the addition of the comma).  \nWith a chunk size of 64, Curator would be forced to make a separate call for every four indices in the cluster, as the character length for 4 indices would be 84 characters (that's the first time in the loop it would be bigger than 64).\nWith your 92 index cluster (not including any of the other system indices), that results in 23 round-trip calls to the cluster to get the metadata.  This is not a no-op for the cluster.  It is taxing for Elasticsearch to read ALL of the cluster metadata, then parse out the metadata for only the indices specified in the call, and return that value.  It's taxing enough to make one giant call for as many indices as you can fit in a single call (which was the reason to change to this behavior in Curator 4).. After pondering on this for some time, I will not be changing this part of the code.  There may be some memory optimizations I can perform, but this is not going to be one of them.. What you're reporting is actually expected behavior.\n2017-05-17 13:02:46,662 INFO      POST http://x.x.x.7:9200/index_v0.2_2017-05-16_270b7c8edcc74744b39bbf304753b06e/_forcemerge?max_num_segments=1 [status:200 request:2056.259s]\nSo, it took 2056 seconds to finish the forceMerge.  That's not unexpected.  Curator automatically increases the default client timeout when performing snapshot, restore, and forceMerge actions.  It does not do so, however, when performing the other actions (except for that tiny bump for close actions, since it tries to do a synced_flush).\n2017-05-17 13:03:26,934 WARNING   DELETE http://x.x.x.7:9200/_snapshot/force-merge/force-merge [status:N/A request:30.031s]\nand\nReadTimeoutError: HTTPConnectionPool(host=u'x.x.x.7', port=9200): Read timed out. (read timeout=30)\nCurator's default client timeout is 30 seconds.  Even with \nretry_interval: 10\n      retry_count: 10\n...Curator will not ever retry if a client times out before it will attempt a retry after failure.\nYou'll have to either change the value of timeout in your client YAML file, or set a timeout_override in the action file in the options section for the delete_snapshots action.\nIt should be noted that the snapshot delete operation is still happening behind the scenes in the cluster.  Curator just doesn't get to see the end of the operation if the client times out before it's over.. Since this appears to be a non-issue, I'm closing this.  Feel free to re-open if you feel that more discussion is needed.. Please sign the CLA so I can merge your code, after review.. I'll have to add some integration tests before it's released (to test the feature properly against all of the current 5.x branches), but this looks good to me.. From Honza: \n\nis tests whether it is the same object - same space in memory essentially \nfor regular comparison use ==\n\nand \n\nis is typically only used in is None comparison (since None is a singleton essentially) or when explicitly looking for identity (making sure it is the reference to the same object)\n. fixed by #979. Hmm.  Curator is specifically for index management.  I'm not sure this falls under Curator's index management scope.. I can see the argument in favor of this, but clearing the cache is not exactly managing the indices, but rather Elasticsearch itself.  I'll have to keep thinking about this.  Clearing cache is not something we typically recommend, though there are obvious use cases for it (like yours).\n\nWhether I add this to Curator or not, it would be rather trivial to use the Curator API to write your own way to do this right now.. That's already possible.\nIt's remarkably simple to duplicate the filters that selected the source index (provided you did that) in the very next action and delete the index or indices you selected.\n```\nactions:\n  1: \n    action: reindex\n    options:\n      continue_if_exception: False\n      wait_interval: 9\n      max_wait: -1\n      request_body:\n        source:\n          index: REINDEX_SELECTION\n        dest:\n          index: logs-2017.04 \n    filters:\n    - filtertype: pattern\n      kind: prefix\n      value: logs-\n    - filtertype: period\n      source: creation_date\n      range_from: -1\n      range_to: -1\n      unit: months\n  2:\n    action: delete_indices\n    options:\n      continue_if_exception: False\n    filters:\n    - filtertype: pattern\n      kind: prefix\n      value: logs-\n    - filtertype: period\n      source: creation_date\n      range_from: -1\n      range_to: -1\n      unit: months\n```\nWith continue_if_exception set to False (which is the default), it will not continue and delete anything unless the reindex is successful.. Since this is already possible, and there has been no further comment, I'm going to close this.. Nope! That's just a straight-up editing foobar.  Thanks for catching and correcting it.\nHave you already signed the CLA? (yes, even for a documentation change). Any update on that CLA?. Any update on this?. Duplicate of #656. Thanks for this.. The automated document build should push these changes anywhere from the next 30 minutes to several hours, depending on the wait in the queue.. Strictly speaking, it shouldn't have been required, until #979.  What version of Python are you using?. @croccam I would love to hear your response on this, even though I am merging this (and closing it).. You shouldn't need mock unless you're doing testing.  This is confirmed by the binary package builds I have.  I had to revert these changes because cx_Freeze wouldn't build with Mock as a dependency.\nTraceback (most recent call last):\n  File \"/home/buh/.local/lib/python3.6/site-packages/cx_Freeze/initscripts/__startup__.py\", line 12, in <module>\n  File \"/home/buh/.local/lib/python3.6/site-packages/cx_Freeze/initscripts/Console.py\", line 24, in <module>\n  File \"run_curator.py\", line 5, in <module>\n  File \"/tmp/curator/curator-5.1.0/curator/__init__.py\", line 5, in <module>\n  File \"/tmp/curator/curator-5.1.0/curator/utils.py\", line 6, in <module>\n  File \"/home/buh/.local/lib/python3.6/site-packages/mock/__init__.py\", line 2, in <module>\n  File \"/home/buh/.local/lib/python3.6/site-packages/mock/mock.py\", line 71, in <module>\n  File \"/home/buh/.local/lib/python3.6/site-packages/pbr/version.py\", line 461, in semantic_version\n  File \"/home/buh/.local/lib/python3.6/site-packages/pbr/version.py\", line 438, in _get_version_from_pkg_resources\n  File \"/home/buh/.local/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 70, in <module>\n  File \"/home/buh/.local/lib/python3.6/site-packages/pkg_resources/extern/__init__.py\", line 61, in load_module\nImportError: The 'appdirs' package is required; normally this is bundled with this package so if you get this warning, consult the packager of your distribution.\nWhen I remove Mock, and go back to string-based comparisons, at least for the Mock instances, I get a working Curator.\nSee #986. I addressed this in the documentation in https://github.com/elastic/curator/pull/988. From what I understand, this has been corrected in AWS ES 5.5.  Closing for now.. This is a usage question, not a bug or an enhancement request.  \nPlease ask such questions at https://discuss.elastic.co in the Elasticsearch forum.. That error is a bit of a mistake.  Curator isn't reporting its own version in those parenthesis, but the elasticsearch version.  I've neglected, on multiple occasions, to correct this:\nraise CuratorException(\n            'Elasticsearch version {0} incompatible '\n            'with this version of Curator '\n            '({0})'.format(\".\".join(map(str,version_number)), __version__)\n        )\nto be this:\nraise CuratorException(\n            'Elasticsearch version {0} incompatible '\n            'with this version of Curator '\n            '({1})'.format(\".\".join(map(str,version_number)), __version__)\n        )\nNote the changing of ({0}) to ({1}).  This is why it's not reporting Curator's version.\nSo the question is, what version of Curator are you actually using?\n$ curator --version. You see, there isn't yet a Curator version 5.4.  The current version is 5.1.1.. @alexcurtin You've missed the message in this issue.  Curator 5.1.1 fully supports all 5.x versions of Elasticsearch: https://www.elastic.co/guide/en/elasticsearch/client/curator/current/version-compatibility.html\nThe issue here is not that Curator is incompatible, but that it's not accurately reporting the correct version of Curator.  You can find the actual version of Curator you're running by running:\ncurator --version. reprepro is to blame, as it doesn't seem to support the desired behavior (preserving older versions in Packages.  I switched to using aptly for the Curator 4 repository, but have not done so for Curator 5 (and may never do so for that branch).  It's a huge risk taking down one published repo to put up another, and hope it all goes smoothly, with all paths properly preserved.  This will probably be addressed in a more permanent fashion for Curator 6.\nIf you really need to pull down prior versions, you should be able to specify MAJOR.MINOR.PATCH for the URL:\nhttps://packages.elastic.co/curator/5/debian/pool/main/e/elasticsearch-curator/elasticsearch-curator_MAJOR.MINOR.PATCH_amd64.deb\nYou should almost never have to do this, though.  I strive to ensure reverse-compatibility and not break functionality in minor point releases.  Anything 5.1 will still do everything 5.0 will do, but with fewer bugs, and some new functionality.  You should be able to always feel comfortable upgrading.. It is a known issue. Thanks for reminding me to get stuff working.  I'll update the title and see what I can do.. Thank you for this.. Duplicate of #994. It's a fascinating idea.  I cannot think that this could hope to work without a regular expression with a captured group, though.  That's certainly not a deal breaker, but it would be daunting for those not familiar with regular expressions.\nThe larger question for me is centered around the rollover API and action.  Elasticsearch is headed towards recommending away from named indices in favor of using the rollover API.  While you clearly could name indices with a numeric value and operate in a similar way, it becomes less important to do so, as you end up rotating based on document count and/or age relative to creation_date, rather than index name.  Does this approach still offer value if you're doing that?  It could, I guess, but with such a rotation, it seems it would be more valuable to use the field_stats API (or the new API replacing it) to determine the age of the oldest and newest documents, so as to not delete anything not fully out of the retention period.\n. If you want to tackle this, I'm all for it!  The planned behavior seems sound, so let's be sure to test it with unit tests and actual integration tests.  I'm happy to help with those.. Closed by #997 . I won't be able to address this PR this week, unfortunately.  I'll be attending (and presenting at) OpenWest.  I'll get to this as soon as I otherwise can.. Sorry for the delay.  I started a review.  It looks good!  Only cosmetic issues, plus adding the API documentation.\nAll we need now are some tests, and we'll be good to go.. Please be sure to rebase on the master branch.  I just released 5.1.2, and there are some changes.. If integration tests are handling it all, unit tests are redundant.  I tend to try to test as much as I can in unit tests, and then do some quick end-to-end tests, both positive and negative, in integration tests.  If you've added them all to integration, that's fine too, though.\nI'll take another look and merge when done.. Got burned by this test:\ndef test_retention_from_name_illegal_regex_with_fallback(self):\n        # Test extraction of unit_count from index name when pattern contains an illegal regular expression\n        # Create indices for 10 months with retention time of 2 months in index name\n        # Expected: Fallback value of 3 is used and 3 most recent indices remain in place\n        self.args['prefix'] = 'logstash_2_'\n        self.args['time_unit'] = 'months'\n        self.create_indices(10)\n        self.write_config(\n            self.args['configfile'], testvars.client_config.format(host, port))\n        self.write_config(self.args['actionfile'],\n                          testvars.delete_pattern_proto.format(\n                              'age', 'name', 'older', '\\'%Y.%m\\'', 'months', 3, '_[0-9+_', ' ', ' ', ' '\n                          )\n                          )\n        test = clicktest.CliRunner()\n        result = test.invoke(\n            curator.cli,\n            [\n                '--config', self.args['configfile'],\n                self.args['actionfile']\n            ],\n        )\n        self.assertEquals(3, len(curator.get_indices(self.client)))\nWorks great, except that 3 months is not calculated as calendar months, but 3*30 days, which means that running this test today (at this time) will fail, as 90 days deletes the 3rd index (July), as UTC rollover is older than 6am on the 1st:\nDEBUG      curator.indexlist          filter_by_age:457  Unit_count_pattern is set, trying to match pattern to index \"logstash_2_2017.07\"\nDEBUG      curator.indexlist          filter_by_age:472  Unable to match pattern using fallback value of \"3\"\nDEBUG      curator.indexlist           __actionable:35   Index logstash_2_2017.07 is actionable and remains in the list.\nDEBUG      curator.indexlist            __excludify:58   Remains in actionable list: Index \"logstash_2_2017.07\" age (1498867200), direction: \"older\", point of reference, (1498932790)\nGoing to have to convert this test to a unit test with a fixed epoch as a result.. Or...\nJust not test months, but rather days, which don't suffer from that problem. \ud83d\ude04 . I think the other tests are okay. Thanks, though!. How would those conditions be detected?. What conditions lead to this? Is Curator deleting these and then, due to some slowness not able to delete a second batch?\nI ask, because it will be hard for me to replicate those conditions in order to see what the API might return. Without having an accurate sample of real API returns in that case, it will be hard to catch. It's kind of hard to catch an edge case when you don't know the size or shape it takes. . > I'm not sure why curator is not waiting for the first snapshot to finish\nCurator doesn't expect it to take seconds (or longer) to delete, is why.   It just issues a \"delete snapshot\" API call, and if it's successful, moves on to the next action.  It's not a case I thought would come up, nor was I aware of API calls where I could catch long-running deletes in progress.  \nI really wish the tasks API handled this where a delete command issues a _task id.  That would make things much simpler. Unfortunately, it doesn't yet.\nI'll try to add some generic polling of the tasks API to look for anything snapshot related going on, and pause there until it's done.  It's going to be hard, still, to get reliable, accurate data from sample/test clusters that respond so quickly, the API calls don't appear.  I'll probably have to create some crazy loops to spool that data into a file as fast as it can go while I run some API calls and hope to capture the associated data.. I won't be able to address this issue this week, unfortunately.  I'll be attending (and presenting at) OpenWest.  I'll get to this as soon as I otherwise can.. That flag does not exist with curator_cli as it only performs one action.  If you're at the cli, you determine whether you want to run another action or not.. And you're obliged/forced to use curator_cli rather than regular Curator?  Because, as you've discovered, there is no way to exit with a 0 exit code with curator_cli unless it exited cleanly.\nA potential work-around would be to use show_indices with the same --filter_list before you run the delete_indices command.  If no indices show up, then you can skip running that particular command, avoiding a non-zero exit code from the actual call you want to make.  Just don't test for the exit on the show_indices command, as it will also be non-zero if there are no indices:\nroot@esclient:~# curator_cli  show_indices --filter_list '{\"filtertype\":\"pattern\",\"kind\":\"prefix\",\"value\":\"ipmi\"}'\nipmi-000002\nroot@esclient:~# echo $?\n0\nroot@esclient:~# curator_cli  show_indices --filter_list '{\"filtertype\":\"pattern\",\"kind\":\"prefix\",\"value\":\"ugly\"}'\nroot@esclient:~# echo $?\n1. Yes, indeed!  Feel free to close this issue if that satisfies you.. It might be helpful to get the creation_date of the logstash-iis-msgapp-2017.07.13 indices.  I would not be surprised to find that the creation date is after the timestamp 2017-07-20 17:29:11,740.\nIf I am right...\ncurl localhost:9200/logstash-2017.08.01/_settings?pretty\n{\n  \"logstash-2017.08.01\" : {\n    \"settings\" : {\n      \"index\" : {\n        \"refresh_interval\" : \"5s\",\n        \"number_of_shards\" : \"5\",\n        \"provided_name\" : \"logstash-2017.08.01\",\n        \"creation_date\" : \"1501549004079\",\n        \"number_of_replicas\" : \"1\",\n        \"uuid\" : \"5N50CzJxQN6RQCnIDIg_HA\",\n        \"version\" : {\n          \"created\" : \"5040399\"\n        }\n      }\n    }\n  }\n}\nTake the value for creation_date, which is epoch + milliseconds, and feed it into https://www.epochconverter.com, which will auto-compensate for milliseconds.  The above value yields a timestamp of GMT: Tuesday, August 1, 2017 12:56:44.079 AM.\nIf I am correct, then what is going on is that Logstash is still receiving stray values for those older indices, and they are being recreated.  \nAnother easy way to tell is to use the _count API.\ncurl localhost:9200/logstash-2017.08.01/_count?pretty\n{\n  \"count\": 1319,\n  \"_shards\": {\n    \"total\": 5,\n    \"successful\": 5,\n    \"failed\": 0\n  }\n}\nIf the count does not look like a full day's worth of records, that's another indicator.\nIf I am wrong\nIf somehow the indices are not being deleted\u2014and I see no evidence in your log files to support that\u2014then what you would be experiencing is your cluster failing, or giving up on deleting after trying for 30 seconds, and not getting an update of the master state.  This is not observable in your log file. Your logs indicate that the cluster immediately responded in the affirmative (with a difference of barely over a second).\nThis is basic API call stuff.  If Curator were failing to delete, then regular API calls would too.  I am 99.99% sure of my initial response.\n. Thanks for sharing, and sorry for the delayed response.  I was on a camping vacation with my son.\nThat timestamp (1501710194333) confirms exactly what I suspected.  That means that even though the index is named logstash-iis-msgapp-2017.07.13, indicating data from the 13th of July, it was created on August 2.  Logstash is still sending data with timestamps from July 13th to Elasticsearch\u2014apparently, at least 34,681 lines worth.\nMy conclusion is that Curator is working just fine, and you have stale data that is being reread and/or resent by Logstash.  . That's a different question altogether, and not one related to Curator, unfortunately.\nI'd ask for help in the Logstash discussion forum at https://discuss.elastic.co.. Going to close this now, as it's not a Curator issue.. In the meanwhile,  in Curator, indices to be restored must be specified using YAML array notation, which could be:\noptions:\n  indices:\n    - index1\n    - index2\nor\noptions:\n  indices: ['index1', 'index2']\nImportantly, though, you can use multi-index syntax for each array element, for example: test* or *test or te*t or *test*, and the ability to \"exclude\" (-), for example: \noptions:\n  indices: [`test*`, `-test3`]\nfor your indices option to get the desired effect (to an extent).. The problem with the 'works but \"fails\"' is that Curator cannot extrapolate the correct destination index name from Elasticsearch date math.  That's why the one which is statically named succeeds and the others give a failure message.\nI will have to see if there's a way I can get a date-math name back through a noop API call to make that possible.. I have a workaround.  Putting a fix in now.. Closed by #1010 . Thanks for contributing to Curator!  I have a concern. Have you tried to rollover an index named crazy_test-%Y.%m.%d?  \nElasticsearch does not allow you to rollover an index not ending with a dash and a number, e.g. -1, even with named indices.  The test you provided may work, but the problem would arise when you tried to rollover the resulting index named crazy_test-%Y.%m.%d.\nSee here:\n\nIt can be useful to use date math to name the rollover index according to the date that the index rolled over, e.g. logstash-2016.02.03. The rollover API supports date math, but requires the index name to end with a dash followed by a number, e.g. logstash-2016.02.03-1 which is incremented every time the index is rolled over.\n\nThe emphasis should be on the phrase, but requires the index name to end with a dash followed by a number.  It may work if your date string uses hyphens/dashes, but the reality is that if you do use that date format, then testing for this condition quickly gets messy and ugly.\nThis patch can still be merged, but a check to see if the index name ends with a dash followed by a number would have to be added, along with unit tests (positive and negative), and hopefully integration tests also verifying the same behavior.. The Elasticsearch API is letting you do something that the documentation says it should not allow you to do.  I will be raising this with the Elasticsearch developers.  I dare not merge this functionality if this is a bug they plan on fixing, or a loophole they plan on closing.. Because of the line in the date math section, referred to above.\nI did get this clarified from the ES team, and so long as no users try to use date math in the new name, then you can name each index whatever you want.  If, however, they want to use date math, then they must end with a dash and a number.\nTo verify that this works as expected, I would like to see 2 tests added that use a plain-text date math example (you don't need to do the escaping, it's handled automatically): one that passes, and one that fails because of the absence of a dash and a number at the end of the date-math-index-name.\nBonus points for a screwy new index name that combines strings for parse_date_pattern and date math (just not nested, as I'm reasonably certain that will fail).. Please be sure to rebase on the master branch.  I just released 5.1.2, and there are some changes.. I may merge this, and add more tests myself later.  It's functional as it is.  I would like to add this to the next release.. I'm not convinced the approach of putting aws_flag in settings.py is going to work reliably.  It's always being assigned as False in that file.  Any other file that opens it is going to potentially reset it as False.  A more consistent and deterministic approach should be used.\nI presume that there are plugins installed, or other features, that identify an AWS ES node.  Start probing the results of client.nodes.info():\nIn [10]: client.nodes.info()['nodes']['JLS3tYelSKy2XnK1ZOc3WA']['plugins']\nOut[10]:\n[{u'classname': u'org.elasticsearch.xpack.XPackPlugin',\n  u'description': u'Elasticsearch Expanded Pack Plugin',\n  u'has_native_controller': True,\n  u'name': u'x-pack',\n  u'version': u'5.4.3'}]\nThe keys at the node level that may be of benefit are:\n[u'plugins',\n u'thread_pool',\n u'transport_address',\n u'http',\n u'name',\n u'roles',\n u'total_indexing_buffer',\n u'process',\n u'ip',\n u'settings',\n u'modules',\n u'ingest',\n u'host',\n u'version',\n u'jvm',\n u'build_hash',\n u'attributes',\n u'os',\n u'transport']\nIt is relatively easy to crawl through this list and find the relevant information.  Something here has to inform us that this is an AWS ES node.  \nFeel free to paste the output in different boxes here (not one massive one, unless it's as a zip file).  I can compare it to a non-AWS ES 5.3 instance and see where the differences may be.. It could be something in the os section, for example:\nclient.nodes.info()['nodes']['JLS3tYelSKy2XnK1ZOc3WA']['os']\nOut[17]:\n{u'allocated_processors': 1,\n u'arch': u'amd64',\n u'available_processors': 1,\n u'name': u'Linux',\n u'refresh_interval_in_millis': 1000,\n u'version': u'4.10.15-1-pve'}\nWhat version does an AWS ES instance show?. And perhaps, most importantly, the build_hash:\nclient.nodes.info()['nodes']['JLS3tYelSKy2XnK1ZOc3WA']['build_hash']\nOut[23]: u'eed30a8'\nBy combining this with the output of version we can categorically identify an AWS ES node as separate from a non-AWS ES node, because AWS ES modifies the code from the ES release default.  \nThe known AWS ES version/build_hash map could be kept in the settings.py file, as it will not change.  When future releases come out, this list could be updated.  Of course, this hinges on the belief that AWS ES only has 1 version of the 5.3 code in deployment, so there would only be one build_hash in the wild.  If there turned out to be more, some way of specifying the build_hash or providing an override file would be a potential solution.\n. If we're going that route, just use client.info():\nIn [25]: client.info()\nOut[25]:\n{u'cluster_name': u'untergeek',\n u'cluster_uuid': u'7OfiDNTsTJqiHTK4_ghIsA',\n u'name': u'esclient',\n u'tagline': u'You Know, for Search',\n u'version': {u'build_date': u'2017-06-22T00:34:03.743Z',\n  u'build_hash': u'eed30a8',\n  u'build_snapshot': False,\n  u'lucene_version': u'6.5.1',\n  u'number': u'5.4.3'}}\nversion is a hash/dict here that includes the build_date, the build_hash, and the release number.  Those together constitute enough of a map to differentiate between release ES and AWS ES.. With AWS ES being tied (currently) to v5.3.2 (are there other minor versions for AWS ES?), you can probably check for a combination of 5.3.whatever, a build_hash value of Unknown, and the presence of the discovery-ec2 plugin to safely assume an AWS ES instance.  We can include other versions when AWS ES adds them.  \nA variable in settings.py, as an array of version tuples, like aws_es_versions = [ (5,3,2) ] would probably be the best place to store this.  An example of version comparison can be seen in the get_version function in utils.py.. Please be sure to rebase on the master branch.  I just released 5.1.2, and there are some changes.. There are 4 tests that are not passing.  Each is a unit test, and each is failing due to the new line added to the snapshot_running function in utils.py.\nfound_version = client.info()['version']\nThis call needs to be properly mocked in these four tests for the tests to come back passing.\nThe tests are:\n\nERROR: test_do_action_snap_in_progress (test.unit.test_action_restore.TestActionRestore)\nERROR: test_do_action_success_no_wfc (test.unit.test_action_restore.TestActionRestore)\nERROR: test_false (test.unit.test_utils.TestSnapshotRunning)\nERROR: test_true (test.unit.test_utils.TestSnapshotRunning)\n\nYou should add a line to each of these tests like this:\nclient.into.return_value = testvars.es_release_version\nIn testvars.py, you should add an entry like this:\nes_release_version = { 'version': {u'build_date': u'2017-06-22T00:34:03.743Z', u'build_hash': u'eed30a8', u'build_snapshot': False, u'lucene_version': u'6.5.1', u'number': u'5.4.3'} }\nThis will at least get those tests to pass.\nSince there is no way to properly test against an AWS ES version, it would be good if you also made new tests that were \"clones\" of these same 4 tests, but that use an AWS ES version, so that they test the if is_aws functionality:\naws_release_version = { 'version': {'build_date': '2017-04-28T21:44:52.237Z','build_hash': 'Unknown','build_snapshot': False,'lucene_version': '6.4.2','number': '5.3.2'} }\nIt will be more work to properly mock those calls, but it is the best way to ensure this works across multiple python versions.. @kobuskc You probably need to do some rebasing before this can be committed.  Also, since AWS ES has a 5.5 version, you may need to include that in your hash/array.. Any update on this @kobuskc ?. @kobuskc It's not a bad idea to do that. Feel free to do so.. The RPM installed elasticsearch, the Java program.  It did not install elasticsearch the python module, which is what Curator depends on.  The elasticsearch python module is currently at version 5.4.0, and apparently your pip is unable to download that dependency.\nI recommend installing Curator from the YUM repository, rather than via pip.. And if that doesn't work, you can always download the RPMs directly.\nThe problem with a pip install is that it will attempt to download dependencies.  If you have no internet access from the device you're trying to install Curator on, you should definitely use the RPMs provided in the link above.  They are fully-bundled with all dependencies.  You don't even need to have python installed for them to work.. Since there's an explanation for what happened, as well as a path forward, I'm closing this issue.. PS C:\\[redacted]\\curator\\build\\exe.win-amd64-3.6> .\\curator.exe --config ..\\..\\win_client.yml ..\\.\n\\win_reindex.yml\n2017-08-08 13:07:24,459 INFO      Preparing Action ID: 1, \"reindex\"\n2017-08-08 13:07:24,589 INFO      Trying Action ID: 1, \"reindex\": No description given\n2017-08-08 13:07:24,615 INFO      Commencing reindex operation\n2017-08-08 13:07:24,682 INFO      Task \"reindex from [index1] to [index2]\" with task_id \"qtM_Qpv7SAa7d9bKOpQH3Q:2510\" h\ns been running for 0.043942405000000004 seconds\n2017-08-08 13:07:25,695 INFO      Task \"reindex from [index1] to [index2]\" with task_id \"qtM_Qpv7SAa7d9bKOpQH3Q:2510\" h\ns been running for 1.058892028 seconds\n2017-08-08 13:07:26,699 INFO      Task \"reindex from [index1] to [index2]\" with task_id \"qtM_Qpv7SAa7d9bKOpQH3Q:2510\" h\ns been running for 2.062923376 seconds\n2017-08-08 13:07:27,709 INFO      Task \"reindex from [index1] to [index2]\" completed at 2017-08-08T13:07:26Z.\n2017-08-08 13:07:27,723 INFO      Action ID: 1, \"reindex\" completed.\n2017-08-08 13:07:27,725 INFO      Job completed.\nPS C:\\[redacted]\\curator\\build\\exe.win-amd64-3.6> .\\curator.exe --version\ncurator.exe, version 5.1.2\nThis will be corrected in the upcoming 5.1.2 release.  I think the Windows build environment was shipping an old version of the Elasticsearch-py module, though how that worked, I cannot tell.. The code does populate the body, but I think you may need to do what you did with the curl notation:\nextra_settings:                                      \n  index_settings:                                       \n      number_of_replicas: 0\nI get that this means the extra_settings documentation here and here are incorrect.  I will use this PR as an indicator to fix that.. This is a known issue.  It will be a bit before I'm able to get a 6.0 beta out for Curator.. No.  I'm kind of dependent on at least a pre-release of the 6.0 python client before I can alter the current code to work with ES 6.. Actually, I've made Curator 5.4 completely compatible with 6.0.0-rc2, and it will be out shortly.. This is resolved now, with the release of Curator 5.4.0. https://www.elastic.co/blog/serverless-elasticsearch-curator-on-aws-lambda\nAs this is a usage question, please ask follow-up questions at https://discuss.elastic.co. I think you're missing the point of using Curator with the Lambda architecture.  You're not supposed to use the Click API, which is exclusively for a CLI interface, with Lambda architecture.  I think the blog post does a pretty good job of explaining how to do it the right way.  You're supposed to use the Curator API, rather than the CLI interface.  There are ways of doing so programmatically (that's how the cli function behaves).\nYou're effectively trying to run an interactive program in a non-interactive environment.  You were never meant to pass things to the cli function this way.  Since you've created your own work-around to this, you're welcome to continue doing so, but will likely have to go through the same hoops with every future release of Curator.. > Would it be possible to move some of the functionality that is offered by the CLI API into the curator API? i.e. so that someone could use curator from source or from CLI without needing to write code, just supply an action file and configuration file?\nThat's an interesting idea.  It could be as simple as renaming the cli function to something else, like process_configuration:\ndef process_configuration(config, action_file, dry_run=False):\nI'd re-order the arguments so that dry_run is a keyword arg with a default of False.\nOnce this was done, I'd rebuild the cli function to point to the process_configuration function:\n@click.command()\n@click.option('--config',\n    help=\"Path to configuration file. Default: ~/.curator/curator.yml\",\n    type=click.Path(exists=True), default=settings.config_file()\n)\n@click.option('--dry-run', is_flag=True, help='Do not perform any changes.')\n@click.argument('action_file', type=click.Path(exists=True), nargs=1)\n@click.version_option(version=__version__)\ndef cli(config, dry_run, action_file):\n    process_configuration(config, action_file, dry_run)\nYour wrapper function would then only have to call process_configuration (I'm not settled on the name yet) to get the same effect as you have now.\nWhat do you think?. It would result in a ton of refactoring how the integration tests are called, but it'd be cleaner.. Based on what you've shared, I think I can help:\n\nI am running elasticsearch version 5.5.0 on 2 data nodes, 1 master and 1 client node, curator 5.1.1 on amazon aws (ubuntu). our repository is in S3. our active shards is around 5200\n\nThis is your problem in a nutshell: You have heap exhaustion. \nYou have 2 data nodes, with 2600 shards per node.   I don't even have to look at any of the other statistics, or configuration.  Even with a 30G heap size, you shouldn't have more than 1000 shards per node.  If you look at the JVM stats, I am convinced you'll find that your heap is well over 70% full, all the time, and it's trying to do full GCs one right after the other.  This is why your requests are timing out.  The data nodes are out of usable memory, and they are fighting to do what you ask, but it's just taking too long.\nIf you want things to work properly again, your recourses are as follows:\n\nAdd more nodes to bring the shard count per node well below 1000 (especially if your data node heap size is < 30g)\nDelete a ton of indices to bring the shard count per node down to well below 1000.\nIncrease the heap (if even possible), though this won't help much if the shard count is still over 1000 per node.. As this is not a Curator bug, I'm going to close the issue.  If you have further questions about the issue at hand, I suggest asking them at https://discuss.elastic.co. This has been requested many, many times.  The problem is that disk space is unreliable as a metric for how much a cluster can handle.  Part of the reason is that shard count per node, if too high, can drastically affect a cluster's performance and well being long before you come close to exhausting the amount of disk space in your cluster.  Users typically get around this by setting the indices to close so that resources aren't consumed, but still consume disk space.\n\nThe second problem with using disk space is that shard size and routing and allocation differ from node to node.  They are not always equally balanced.  You might run out of space on one node while waiting for a cluster-wide space-based trigger to happen.\nHow would you use resource limits to trigger a change?  What resources were you considering?  Was it something besides disk space?. It is also apparent that your cluster was unable to delete logs in a rapid fashion.  I saw this when I dug deeper in the log entries you shared:\n\nReadTimeoutError(HTTPConnectionPool(host=u'127.0.0.1', port=9200): Read timed out. (read timeout=30))\nIf It takes longer than 30 seconds to delete indices, your cluster might already be suffering.  Regardless, increasing the timeout in your curator.yml file can help.  I suggest 300 seconds, as that's also as much as Curator will increment the master_timeout.\n. With no updates in 2 months, I'm closing this.  Feel free to re-open or open a new issue if you have more to add.. Thanks for contributing to the Curator docs.  Can you please sign the CLA?  Yes, this is required even for documentation changes.. Any update on the CLA?. So, @HonzaKral, if I understand it correctly, you want to keep the last count indices in each matching group, rather than have to specify each prefix at a time?  This is great territory for those who are careful with their regexes, but potentially catastrophic for people who suddenly add new indices that match the pattern, but don't pay attention.\n\nI think it's a worthy feature, but it'll result in multiple iterations, one for each matching group found (but no extra API calls).. These have been addressed by #1076, #1081, and #1085. If it proves difficult to make schema validation work with this, I will roll a new filter (absolute_period).. An unfortunate thing, indeed.  There are several other Elastic tools that lack official logos, too.  I'm not sure there's anything I can do about it.  It's not a huge priority for them.. This is mostly because of this.  Curator uses this class.  I'm sure I could parse the URL down and populate these.. @joshuar Sorry for the delay.  I've been busy moving from the Logstash team\nIf it's empty, and you've set it to use SSL, then it uses the certificates provided by the certifi Python module*, which is a lumped bundle of the certificates recognized by Mozilla in Python module form.  In such a case, it should guarantee that most commercially signed certificates will be recognized.  It does not use the OS certificate bundle.\ncertificate ostensibly should be able to recognize remote keys, where as client_key and client_cert imply the means by which a remote entity can verify Curator is trusted.\nssl_no_validate is really only useful for people who use self-signed certificates and can't be bothered to actually use the public key/certificate they've created as the certificate, so as to ensure that secure communications are validated.  It will do the PSK thing, but won't validate that the remote host is recognized by certificate.  So, you have encrypted but not validated communications.\n* If you've installed a DEB/RPM or Windows package, this is provided.  If you're installing via pip, you need to install it yourself.. There is (currently) no interpretation of strftime strings in the migration_prefix or migration_suffix.  At present\u2014even if this functionality were added\u2014this would only yield the current day/month/year.  This can be a feature request for now.. You don't need the MIGRATION feature for this.  Date math already works:\nactions:\n  1:\n    action: reindex\n    description: reindex .kibana to .kibana-Y-m-d\n    options:\n      ignore_empty_list: True\n      wait_interval: 9\n      max_wait: -1\n      request_body:\n        source:\n          index: '.kibana'\n        dest:\n          index: '<.kibana-{now-1d/d}>'\n    filters:\n    - filtertype: none. Curator 3.0 is no longer supported.\nWith that said, there is at least one thing you can try.  \nUpgrade to Curator 3.5.1, which is the last release in the 3.x series, and has all of the fixes and patches that might have addressed things.  There were several changes in the versions between that may or may not have addressed this.\nHowever, after all is said and done, I note this:\nes_repo_mgr --debug --host myhostname ...\nBut in the logs, it says this:\n2017-09-13 22:01:10,694 WARNING elasticsearch log_request_fail:82 GET http://localhost:9200/ [status:N/A request:0.000s]\nThe urllib3 error that says Connection refused is what Curator gets back from attempting to reach the host in question.  If it was unable to make a connection, it is because you have a firewall blocking access, or an incorrect host/port combination, or security credentials or a proxy in the way preventing access.. Thanks for the report.  This will be addressed in the next version.. In addition to needing the CLA signed, tests (preferably integration tests) need to be added for this.  Is that something you're comfortable with?. @monkey3199 OH SNAP! I can't believe I didn't do a git add on that file before I committed those changes and pushed.\nRegardless, if you rebase on master now, they're in there.. @monkey3199 I can't merge this until you've signed the CLA.... @monkey3199 I'm still waiting on that CLA getting signed.. @monkey3199 I don't know what to do here.  I will have to close this PR if I do not get the CLA signed.  It's our official policy.. @monkey3199 Please make that last adjustment I requested in the review, and we're good to go.  I want to merge this in time to make it into 5.3. Check out my review.  Line 6 of examples/curator.yml should be indented the way it was.  That's all that remains.. That sounds like a bug.  I'll see what I can do about that.  I thought there was a check in the part reading the YAML file, but I'll have to double-check that.. I can't replicate this.  If the file doesn't exist, it does give an error, as expected: \n```\nroot@esclient:~# curator --config nonexistent.yml --dry-run rollover.yml\nUsage: curator [OPTIONS] ACTION_FILE\nError: Invalid value for \"--config\": Path \"nonexistent.yml\" does not exist.\nHowever, if it's _empty,_ it uses all the defaults (localhost, port 9200, INFO logging, etc...):\nroot@esclient:~# curator --config empty.yml --dry-run rollover.yml\n2018-03-04 16:09:43,467 INFO      Preparing Action ID: 1, \"rollover\"\n\n```\nIs it possible the user had an empty config file, rather than a missing one?  Closing, as that isn't the same bug as the title.. Thanks for catching this and submitting a pull request.  Please make the requested change, and I will be happy to merge this.. > It should be used when getting indices in IndexList \nThe chunking utility is for preventing a URL-length overrun when making requests.  Elasticsearch can't handle it when a URL longer than 4K bytes is sent.  How exactly would I limit this on the request end?\nAs it stands, the flow is as follows:\n\nAt the end of Class instance initialization, it calls self.__get_indices().\nInstance method __get_indices calls utility function get_indices\nUtility function get_indices calls the get_settings API and extracts a list of indices.\nIf there are indices, it will loop through all discovered indices, and create an empty dictionary schema for index metadata.  No API calls are made in this step.\nIt populates the metadata by calling a function, which actually uses the chunking utility function.\n\nSo far as using chunking in the IndexList class is concerned, it sounds more appropriate to use Elasticsearch's wildcarding and/or regular expressions to pull in a limited number of matching indices first.  This is a feature I've long contemplated adding but have not yet gotten around to implementing.\n\nadditionally use a configurable number of indices as chunk size too\n\nThis cannot be done for the reasons stipulated already: Chunking is about URL line length, not a count of index names.  It could be possible to define the chunk size, but that is less effective, I think than changing the index count to start with, as already suggested.  I may still add it.. The aforementioned pre-loading feature will be coming in Curator v6.  Closing this ticket for now.. Closed by #1067 . Let me think about the name of the flag.  I like what you've done here, especially guaranteeing reverse-compatibility.  But this is a big shift in what the filter does.  I definitely want the name to be a good fit, and index_comparison doesn't quite seem the right fit.  I will think about this for a bit.  . While I still consider the naming for this, will you please rebase against the master branch?  This commit has conflicts now.. Thanks for rebasing.  \nI like the idea of preserving the current behavior with a named approach, threshold_behavior as the key, with acceptable values of less_than and greater_than.  Of course, greater_than would be the default behavior to preserve reverse compatibility.\nThoughts?. I'm not a fan of mixing disk_space and behavior because the disk space isn't behaving.  We have a threshold that is established by the value of disk_space, and then we need to do something with that value.  Here are some other possibilities:\n\nthreshold_calculation\nthreshold_direction\nthreshold_measurement\nthreshold_comparison. It is.  If you're using master-eligible nodes as data nodes, you need to use this flag.  I probably need to document it better, though.. Making the flag permit_master_eligible felt too long.  Most people think, \"I have 3 dedicated master nodes,\" when they build a cluster.  Even though only one of those is the elected master, they still consider it 3 master nodes.  All of these would be considered \"masters\" for the purposes of this flag.. Closed by #1069 . If you see that in the show_indices command, and you immediately run the delete_indices again, do those get deleted?  Or does it show indices, but still say they're not there?\n\nIn the event that they are deleted again, then this indicates that Logstash is receiving old data that is going into accordingly named indices.\nIf it shows up in show_indices, it should also show up in Curator's regular logs, as they share the same code. . Ah... Found it.  You have specified:\ntimestring: '%Y.%m.%d'\nin your age filter, but your index names have a pattern of:\ntimestring: '%Y-%m-%d'\n. This is a good idea.  Thanks for submitting this.. Though, if you've named your shrunk indices with a uniform suffix, e.g. -shrunk, then it's easy to filter those.. > it seemed like it would be a good feature to have.\nAgreed.  . It's not high on my priority list. I'd accept a PR that addresses it, however.. @cushind Are you not renaming indices by appending a suffix after shrinking?  That is a simple work-around as it allows the suffix to be excluded.. @cushind That is correct as of the 5.5.4 release. It does not automatically filter any indices which already have only 1 shard.  It must also be understood that not all shrink operations will necessarily go to 1 shard, so any check should be to see if the present number of shards is less than or equal to the desired number of shards. The current code block catches this information and then will log on a dry run or raise an exception.  I agree that it should auto-filter indices which do not meet those criteria, and that the place for the filter is in the index list code, but the place it's most likely to be used is in the shrink action.. > Should this filter type be exposed through the curator api or be implicit on every shrink action?\nThe answer is \"yes\" to both.  It should exist in the Curator API as a regular filter, but the Shrink action should also use it implicitly.  And yes, I think there may be some who only want to shrink \"upper bounds\" indices, too. For example, someone who started with the default 5+1 sharding, then changed, but wants to clean up the earlier indices, but not all indices.\nThe next iteration of Curator will do a lot of caching, including (I hope to make it so) the last index list, so the next action will be able to use the previous list with or without additional filtering.  This would permit automatic segueing into another action without expressly having to re-define the exact same filters.. Would love a PR.. I'm out of town on a business trip this week.  I'll look it over more closely when I get back.. I like the idea, but can you match the name to what's used in Curator?  It's called skip_repo_fs_check.  I realize that complicates your code, but it would be consistent with what's already in use.. This commit will need to be rebased on master, and the new feature added under the 5.3.0 part of the changelog.. You may need to rebase again, as I updated the Changelog recently.. > I have Curator version 5.2.0 installed via YUM\nThis is the problem.  The YUM package does not include the system python libraries/modules.  It is a standalone version for running only.\nYou need to install via pip in order to have the libraries accessible for API use.\npip list | grep curator\nYou should see what version is installed that way.\nYou can upgrade via:\nsudo pip install -U elasticsearch-curator\nor \npip install -U --user elasticsearch-curator\nto limit the installation to a local user (and not overwrite the system python libraries).  Just remember that only the user that ran this will have that version of the Curator module installed.\nPLEASE NOTE: You should not have both the pip version and the YUM package simultaneously installed, so as to avoid confusion when calling curator at the command-line.. Which version of Curator is installed?\npip list  | grep curator. And which version of Python?. Do me a favor, and run this:\n```\n$ python\nPython 2.7.13 (default, Mar 19 2017, 12:50:26)\n[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport curator\ncurator._version.version\n'5.2.0'\n``\nDo you see the same thing?. That would do it :). You might need to rebase this against master and make a few changes.  You've got a new feature under 5.1.2 in the Changelog, when this is something which won't make it until 5.3.  You may even need to add a new block to the head of Changelog.. Any update on this?  I can't merge this with the new feature under the changelog for 5.1.2.... Go ahead and do the one indocs/.  The other is a symlink.  I'll fix it if I have to.. Oh, you already did.  No worries, then.. That's a worthy addition.  The code has a subroutine that should actually already allow for it.. Oops, my mistake.  I see what's going on here.  No, there's no way to skip the deterministic node search.  You either name the target node, or data moves.  If you know which node has the shards already (because it was named in the logs from the prior run), you could re-run with that node's name in place ofDETERMINISTIC.. I will investigate makingDETERMINISTIC` even more so. . Oh, it'd be very nice to have it exactly the same, but that will be a lot more coding than I'm willing to add right now.  Here's why:  Date math support in index names (emphasis mine).\n\n\n\nElasticsearch does the work for this for the create_index action.  That's why it works there.  The problem with adding this for snapshot names is that it isn't supported by Elasticsearch, so I'd have to mimic the behavior exactly, or risk upsetting users who expect identical behavior.  I've had my eye on adding this behavior for a while, but it's very difficult to make it happen perfectly, as the libraries and such for Elasticsearch are in Java, and Curator is in Python.. Oh, you're going to love what's coming.... Closed by #1158 . @Alsheh \nAdding it to the Curator code for snapshot-time extrapolation wouldn't be hard, but what are you using to generate new repositories on the fly? Are you asking for this to be put into es_repo_mgr? . If you'd be so kind as to add documentation for this, how it works, etc.  If you're not comfortable adding it to the asciidoc files yourself, please just document it here in a comment and I'll see about a commit to your branch.. Any update on this?. Also, how will this work in conjunction with the changes going into https://github.com/elastic/curator/pull/1012 ?. If increasing the client timeout to something larger than 30 doesn't work, then you do, indeed, have a cluster problem.  It is true, that an overburdened cluster can take a lot of time to process metadata changes and propagate them to all nodes in the cluster.  This is, however, indicative of a cluster in need of more resources, rather than a fault in Curator.  Elasticsearch was made to handle bulk responses.  Curator used to do what you are requesting, and it was horribly underperforming. It hammered Elasticsearch with unnecessarily redundant API calls.\nMy recommendation, if you can't increase cluster resources (more nodes, more memory, faster disks, etc), would be to reduce the response time/overhead by trying smaller batches of indices, limiting the number of indices you're changing allocation on in a single run.. I recommend using a shell-scripted loop, then using curator_cli, rather than the full version of Curator that uses a YAML-based action file.\nThe approach would be to build a list of indices with one iteration that uses a --filter_list to select indices, just as you're doing now.  Then on each iteration of the loop, call \ncurator_cli allocation [OPTIONS] --filter_list \\{\\\"filtertype\\\":\\\"pattern\\\",\\\"kind\\\":\\\"regex\\\",\\\"value\\\":\\\"\\^$INDEXNAME\\$\\\"\\}\nwhere $INDEXNAME is the index from the loop.\nIt might look something like this:\nfor INDEX in $(curator_cli show_indices --filter_list \\{\\\"filtertype\\\":\\\"pattern\\\",\\\"kind\\\":\\\"regex\\\",\\\"value\\\":\\\"\\^netflow\\\"\\}); do\n  curator_cli allocation [OPTIONS] --filter_list \\{\\\"filtertype\\\":\\\"pattern\\\",\\\"kind\\\":\\\"regex\\\",\\\"value\\\":\\\"\\^$INDEX\\$\\\"\\}\nThis loop will iterate over all indices that start with netflow.  You can add more filters by making them a JSON array\u2014effectively translating what you have in the YAML to a single string.\nObviously, you'd have to replace [OPTIONS] with what you want for your allocation, but this will do a single action at a time, like you're asking.. Going to close this since there has been no further response.. False alarm.  Code not needed.. LGTM\u2122. Something is still not working with this code.  Client instances are not being created, causing every integration test to fail.. So, is this connecting using boto now on your end?\nIf that part is working, the next step will be to add the proper documentation to docs/asciidoc/configuration.asciidoc.  You may even need to edit existing information with regards to aws_key and the other related fields.  As these changes you are adding are not tested by the Travis CI testing, please label them as hand tested.  I do not have the capability to test them, so they are a community addition, and should be properly vetted and tested by end users with the --dry-run flag, etc. before using.. Thanks for your work on this.  Hopefully it results in some very happy AWS users.. I considered this, but there's a wrinkle that came up: What if someone doesn't want this set this way? Or worse, doesn't want it automatically set back to _all after the shrink has run?\nThe work-around is actually using the cluster_routing action before and after the shrink action, or yet more crazy flags and options.  My first option was to allow users to do it themselves.. It's dangerous to presume that it's wise or safe to worry about only specific relocations.  Most clusters have limits to how many relocations can happen at a time.  As much as I may agree that this would be a beneficial feature, it's also one that is terribly easy to get wrong.  For now, I'm going to continue to expect users to handle the rebalance settings on their own.. Disk I/O and/or network bandwidth limitations might be met.  That's why I would like the end user to explicitly choose to disable rebalancing during this.  If moving all shards for a given index happens, it's almost a guarantee that nodes will want to rebalance.  Maybe they want that to happen, maybe they don't.  It should be their choice, though, not mine.. > However, if the user does not disable rebalancing, and still calls the shrink operation, that is an explicit indication that the user wants the two to occur simultaneously.\nI'm not sure I follow the logic behind this assumption.  I understand that you presume that, but why would every user?. Is this fixed by the changes in #1129 ?. This is already the fixed version. Compare to https://github.com/elastic/curator/blob/v4.2.5/curator/utils.py#L442. It's under active development right now.  There's a major refactor being done, and some changes to how things are being handled, mostly due to API changes in 6.  It's not a trivial upgrade, unfortunately.  Many parts won't have to change, but a few will.\nI'm also trying to make 6.x less version dependent than 5.x, which is, I think, a necessary improvement.\nLong story short, it might not be on time for the 6.0 GA release.\nClosing this in favor of #1030 (which predates this issue).. Python 2.6 is not supported.  I immediately assume that you're using RHEL or CentOS 6 if you're stuck with Python 2.6.\nIf you need to use the Curator API, then you will need to upgrade or use a separate Python installation.\nIf you only need the Curator binary functionality, then by all means, install the RPM package for your distribution, as it is a stand-alone version with all required libraries bundled.  It will not affect the system Python in any way.. Thanks for adding this issue, @ruflin \nThis is an interesting use case, and I'd love to help make Beats more streamlined this way.  I have a few questions:\n\nIs there any easy way to associate these templates expressly?  For instance, if I am filtering templates by metadata, the index pattern (e.g. beats-something-*), then that's easier than having to figure out all possible wildcards.\nAre there associated metadata fields or versions in the templates?  That could further aid in guaranteeing that Curator won't accidentally delete something.\n\nMy biggest fear in adding a feature like this is accidentally deleting some index template someone has added, but not added an index for, and causing mayhem in their cluster as a result.  Any ways to strictly limit this by ages, metadata, index pattern, or template name, or some combination of the above would help me feel a lot more secure deploying this feature.. Parsing semantic versioning is not a problem.  Curator already has to do that between Python versions for a few API calls, so that's covered.  If you can get a list of potential older versions, then this will work, though it probably won't come out until Curator 6.0 (Curator 5.4.0 will support Elasticsearch 6.0, so I'm taking more time to make Curator 6.0 work well, API cleanup, etc.).. Need to verify that min and max will be accurate in large, multi-sharded indices.. The question is whether this will work due to the potential inaccuracy of terms when there are large numbers of shards.. Please ask usage questions at https://discuss.elastic.co in the Elasticsearch forum so the answers will benefit the entire community.  When usage questions get asked in GitHub issues, the answers tend to get buried.. May I draw your attention to the --http_auth flag?\n```\ncurator_cli --help\nUsage: curator_cli [OPTIONS] COMMAND [ARGS]...\nOptions:\n  --config PATH       Path to configuration file. Default:\n                      ~/.curator/curator.yml\n  --host TEXT         Elasticsearch host.\n  --url_prefix TEXT   Elasticsearch http url prefix.\n  --port TEXT         Elasticsearch port.\n  --use_ssl           Connect to Elasticsearch through SSL.\n  --certificate TEXT  Path to certificate to use for SSL validation.\n  --client-cert TEXT  Path to file containing SSL certificate for client auth.\n  --client-key TEXT   Path to file containing SSL key for client auth.\n  --ssl-no-validate   Do not validate SSL certificate\n  --http_auth TEXT    Use Basic Authentication ex: user:pass\n  --timeout INTEGER   Connection timeout in seconds.\n  --master-only       Only operate on elected master node.\n  --dry-run           Do not perform any changes.\n  --loglevel TEXT     Log level\n  --logfile TEXT      log file\n  --logformat TEXT    Log output format [default|logstash|json].\n  --version           Show the version and exit.\n  --help              Show this message and exit.\nCommands:\n  allocation        Shard Routing Allocation\n  close             Close indices\n  delete_indices    Delete indices\n  delete_snapshots  Delete snapshots\n  forcemerge        forceMerge index/shard segments\n  open              Open indices\n  replicas          Change replica count\n  restore           Restore a snapshot\n  show_indices      Show indices\n  show_snapshots    Show snapshots\n  snapshot          Snapshot indices\n```\nYou'd use curator_cli --http_auth 'user:pass' .... I'm sorry you had a hard time navigating the documentation, but the alias action is clearly at the top of the list of actions available in Curator.\nTo create an alias, you only need to name it, and then use the add filter block to select which indices you want associated with the name you selected.  You can even apply extra settings to your alias.. Thank you for checking out against 5.x.\nAside from signing the CLA (would you please be so kind), there are a few tests which are still not working:\n```\nFAIL: test_do_something_with_int_value (test.integration.test_envvars.TestEnvVars)\nTraceback (most recent call last):\n  File \"/home/travis/build/elastic/curator/test/integration/test_envvars.py\", line 75, in test_do_something_with_int_value\n    os.environ.get(evar)\nAssertionError: '${JMILLQK9}' != '1234'\n======================================================================\nFAIL: test_not_present (test.integration.test_envvars.TestEnvVars)\n\nTraceback (most recent call last):\n  File \"/home/travis/build/elastic/curator/test/integration/test_envvars.py\", line 49, in test_not_present\n    self.assertIsNone(cfg['client']['hosts'])\nAssertionError: '${CFRR6PTB}' is not None\n======================================================================\nFAIL: test_not_present_with_default (test.integration.test_envvars.TestEnvVars)\n\nTraceback (most recent call last):\n  File \"/home/travis/build/elastic/curator/test/integration/test_envvars.py\", line 61, in test_not_present_with_default\n    default\nAssertionError: '${ODS5TJRH:CRDYI2RJ}' != 'CRDYI2RJ'\n======================================================================\nFAIL: test_present (test.integration.test_envvars.TestEnvVars)\n\nTraceback (most recent call last):\n  File \"/home/travis/build/elastic/curator/test/integration/test_envvars.py\", line 38, in test_present\n    os.environ.get(evar)\nAssertionError: '${LCE4Q46B}' != '1234'\n```\nEach of the 4 tests calls:\ncfg = curator.get_yaml(self.args['configfile'])\nSo I imagine that there must be something related to the change.. Could it be that yaml.safe_load doesn't respect environment variables?. Thanks for this change.  Please make it merge it against the 5.x branch instead of master.  I've already got a huge code change against master locally, and this will not work there.  I'd rather let you include it into a working branch than have to redact it and have you have to start over.  This approach makes me do the work to re-incorporate your work into my new branch.. Unless this is changed to merge against 5.x as base, I cannot merge this.. Please change this to be committed against the 5.x branch, rather than master.  Huge changes have already been made to my local code branch for master, and this will actually conflict there.\nThanks!. Sorry to hear you're having a hard time.  Curator does not ever automatically assign a _name shard routing, except when issuing a shrink action, or if you expressly assign key: '_name'.\nDid this happen?. @allansene It does do what you describe, but not until after a successful shrink.  Why unset this if it is only a matter of trying again?  Why let the shards go back?  It's a tricky problem to try to predict what is necessary and automatically do the right thing.\n. This is a place where curator_cli does not normally check for an empty list, unfortunately.  What's happening is that you are processing your filters, and they are checking for an empty list condition, which is passing, as you have indices after your filters.  What's happening next, however, is that the action itself is checking for closed indices (because, why re-close an already closed index?).  And all of the indices you're attempting to close are already closed, so the action is self filtering the actionable list to remove indices already closed.  When it discovered that your list only had closed indices, the list was empty, and it throws the exception there.\nWhile I can (and probably will) add a special check for \"closed\" indices before running actions (for actions which automatically do this), to avoid this behavior, it will not be added soon.  A great workaround would be for you to add \"filtertype\": \"closed\" to your filter list.  That will perform the step the action is taking in the filter section, and will result in the 0 exit code you seek.. This will probably continue to be the workaround, so I'm closing this.. This only tends to happen when a few conditions are met. If the cluster state is huge, and the initial API isn't able to delete all of the indices in a single pass in a perfectly timely fashion, curator retries the delete, but it does so from a second IP (due to the round-robin behavior due to multiple connections).  When Curator tries from the second IP, sometimes the cluster state has caught up in that moment, and it discovers that the index isn't actually there, resulting in the 404 not found error.\nNoting that your indices are named as being from 2015 (even if they're recently reingested), I wonder how many indices are in your cluster, and how many you're trying to delete at a time.. And you're 100% certain both nodes are in the same cluster?  This message will also happen if that is not the case.  I very rarely hear about the condition you describe, and there are many, many people who use the multi-host feature with no problems.  The only times it happens is when either:\n\nYou have a huge cluster state, as described above, or \nYou are trying to use different hosts from different clusters.\n\nWhen I see 192.168.101.188 and 192.168.10.136, I see either 2 subnets, or one absolutely enormous subnet.  That's why I ask whether these two nodes are actually in the same cluster.  If these are two hosts in the same cluster, are they separated by data center?  If so, that is absolutely not a recommended practice, and the lag between the two would also explain why it takes time for the cluster state to update (as with reason 1 given above).\n\nFor other node I changed json file manually ,and change names to logstash-2015.06.18,logstash-2015.06.19 and logstash-2015.06.20. Otherwise both node have same data with different indices name.\n\nThis is not how it works.  This is the other reason why I ask whether these nodes are in the same cluster.  If they're truly clustered, you wouldn't be attempting to insert data to a given node.  The cluster handles the data distribution automatically.  This sounds more and more like you have 2 nodes in separate clusters.. How do you know they're connected and properly clustered?  I'm convinced they are not clustered properly.  Why?\nWhen you run Curator against 192.168.101.188, you see only indices which are named logstash-2015.06.*, and when you run curator against 192.168.10.136, you see only indices which are named logstash-2015.05.*.  If they were clustered properly, you would see all six indices when run against either or both IPs.  Because you tried to round-robin them, that is exactly why you are getting the 404 error.  It is behaving as mentioned in reason 2 in my last comment.\nThese nodes are not behaving as though they are in the same cluster, but individual nodes in separate clusters.. This is a usage issue and not a bug.  All future comments will go to the Discuss forum topic I just discovered you created.. Please commit against the 5.x branch, rather than 5.4, please.. @christianvozar What do you think of these changes?  I ask, since it was your build originally.. @christianvozar, if you have no objections, I will likely go ahead with these changes.. Please ask usage questions at https://discuss.elastic.co/c/elasticsearch\nIf it turns out that there's an actual bug, then raise an issue then.. Yes, you can ask questions there.  I answer there personally.. I'm sorry if I sound terse, or curt.  The Curator GitHub issues area is getting more and more crowded with usage questions, instead of bug reports and feature requests.  The issues section is not the correct place for usage questions, unless they are actually a bug.  While you may feel that you have a bug on your hands, it's much more useful for the eyes of the community (and myself) to have a centralized location where others with experience can respond, not just me.\nI've even gone so far as to add issue and pull request templates, to help direct usage questions to the correct location.  I'm sure there's a good explanation for what you're experiencing, but an issue here on GitHub will wind up closed, and ignored, where the discussion threads remain open and searchable by many users.. I prefer you open it there, because now that I edited your initial comment to encapsulate the plain text bits in triple back ticks, I can see exactly what happened.  It's not a bug.  It's a usage issue.. Hmmm.   With these conflicts, you might be better off rebasing, or kill this PR and do another against the 5.x branch (check out 5.x locally instead of master). Version 5.4.0 already does.. Thanks for merging against 5.x (I know that felt like an unnecessary hassle).. Thanks for adding this.. Rather than delete this line, perhaps set kwargs['verify_certs'] to the value of ssl_no_validate?. Also, please use 5.x as the base, not master, as there have been significant changes to master in my current branch, and these won't merge nicely anymore.. I got it wrong, though.  It should be more like here, as ssl_no_validate == True is the same thing as kwargs['verify_certs'] == False.. Also, have you tried the new boto authentication? See https://www.elastic.co/guide/en/elasticsearch/client/curator/5.4/configfile.html#aws_sign_request. This looks good now.  Thanks for adding it.  Will merge after Travis CI is green.. This is great.  Can you use 5.x as the branch instead of master?  This will make it into 5.4.1 if you can do that soon.. Thanks for improving Curator.  It will be several days before I can take a good look at this, as I'm on business travel in Asia.  . Please sign the CLA, and I will merge this.. Sure, @reswob10, but this shows the actual compatibility in a much more visible chart higher up on the page.. I will add this dependency to the RPM package, rather than merely document it.. The dependency has been addressed in the build scripts.. @Denagar @vanetix These errors are suggestive of misconfigured yaml files .\n@Denagar \nFor example, in what is pasted above, filters is inappropriately indented an extra space.\n@vanetix Can you double-check for properly indented yaml and try again? If that fails, please share your config.. Debian has a 5.4.1 build? You're using that instead of the Elastic-provided one?. Try setting blacklist: [] and run again.  You should see more information about the inability to connect, or what it is actually trying.. Thanks for the follow-up, @TeroPihlaja . Since these seem to be configuration issues, rather than bugs in Curator, I'm going to close this.. @sbellone see #1237 . Well, crap.  It seems that PyPI has changed its URL schema.  Will have to work at this a bit.. I can't find the https://codeload.github.com/elastic/elasticsearch-py/tar.gz/5.4.1 anywhere, or even any instance of codeload.  I do see:\n```\n=== elasticsearch (python module)\nDownload and install the https://github.com/elastic/elasticsearch-py[elasticsearch-py] dependency:\n. wget https://github.com/elastic/elasticsearch-py/archive/+pass:attributes[{es_py_version}].tar.gz -O elasticsearch-py.tar.gz+\n. pip install elasticsearch-py.tar.gz\nor uncompress and run python setup.py install\n```\nSo it appears to already be using the github archive location.\nI'll use the github pages for each of these.\n. This needs to be merged against the 5.x branch, rather than master. . And possibly rebased against it first. . Sorry for the delay.  I've been on vacation.\nPlease go ahead and make a 5.4.2 header and put this under it (for now).. This is necessary behavior, unfortunately.  It's not about indices without docs, it's about indices where field is not present.  This behavior is documented:\n\nThis field must be present in the indices being filtered or an exception will be raised, and execution will halt.. Heehee.  I just found a quick solution, since I was already collecting the document count in the metadata. \ud83d\ude04 . @deiwin The original issue shouldn't be an issue any longer, after Curator 5.5, which is slated for release this week.\n\nWith that said, catching a TypeError and assuming it is because of a missing field and not because of something else which could be causing a TypeError is a trickier proposition.  I'll take my chances with the new behavior which catches empty indices and removes them from consideration before going down the much deeper rabbit hole to try to catch on a TypeError which is specifically about a missing field.  If it continues to be an issue, I'll reconsider.. I think you may be expecting more than AWS Elasticsearch offers.  You cannot close indices in AWS Elasticsearch.  See the AWS Elasticsearch supported operations for more information.\nIf you want to be able to open and close indices, I highly recommend Elastic Cloud, which also permits you to do AWS Marketplace Billing, so it's tied to your same billing account as your other AWS services.. This sounds like an AWS Elasticsearch-specific problem here.  Curator specifically got away from making a bajillion redundant API calls because that hammers and traumatizes your cluster.  I'm really not open to this, but I'm willing to listen to why you'd change behavior that's been 100% fine for Curator users since 4.x to suit a narrow use-case that is AWS Elasticsearch-specific.. @rewiko You mention:\n\nspecially on AWS when you can't customize the payload parameter  - getting HTTP/1.1 413 FULL head\n\nOut of a vested interest in pursuing or denying this PR, how many indices do you have?  How many nodes in the cluster?  How many shards per index?. You've provided the node count and the index count.  However, I really want to know the number of shards per index.  The number of shards per node can have a dramatic effect on cluster performance, if it is too big.\nWhat constitutes \"not a big mapping definition?\"  A 100MB cluster state metadata size is quite large, and Curator only pulls the metadata, not even the rest of the cluster state.. Actually, I think it makes more sense to catch a failure and process in batches if that fails.. Please catch that 413 specifically for this retry in batches, and fail on any other failure.. The timeout_override of the client is sometimes not what's timing out.  Sometimes it's a proxy or a load balancer in front.  You'll be able to see for sure if you set blacklist: [] in the client config under the logging section, and set loglevel: debug.  The default hides the elasticsearch and urllib3 responses, and those settings will unhide them.  The timeout is so exactly 60 seconds that it leads me to believe it's a different issue killing it.  But it might not be a proxy or load balancer.  It could even be some code Amazon hacked into their version of Elasticsearch (have they shared the code somewhere? If so you could possibly chase that down).\nRegardless, the best troubleshooting you can do is to increase the logging with the provided steps, and see what error code the http request generates.  It will reveal what timed out.. You do have \"permit_masters\" set to true.\nAs for answers you requested:\n\nExpected Behavior\nCurator should successfully shrink the index. The debug logging should at least give a better understanding of why the moving was, or wasn't successful\n\nThe trouble here is that Elasticsearch has the answers in its logs, but the API call Curator uses only receives pass/fail type responses.  It cannot tell what happened in Elasticsearch, only that Elasticsearch says it worked, but then upon checking, the shards aren't there.\n\nActual Behavior\nCurator unsuccessfully shrinks an index. The error states that not all shards are on the designated node, but does not provide cause or steps for resolution.\n\nThe most Curator could do here is effectively to say, \"look in the Elasticsearch logs for the node(s) in question.\"  Curator makes API calls.  It's little more than an index-selection wrapper around the regular API calls.  If you were to attempt to allocate all shards to the same node manually, you might be able to see those in the logs in real time.. Of note to me is that Elasticsearch apparently didn't even spend a full second before returning and saying, \"we're done.\"  Please check the logs on the node(s) in question. It does try to catch them.  Elasticsearch doesn't always respond with intelligent errors, so Curator had nothing to catch.  This is also why Curator does a sanity check after allocation, to make sure it did what was expected.  . > /update Why does a minor release version difference actually matter?\nBecause the Lucene versions in Elasticsearch change between even point releases.. This is really not needed.  If you look in the documentation for 5.3 (from the drop-down), it doesn't show 6.x support.  It only shows that 5 supports 6 in the 5.4 documentation.  I deliberately did not make this over-complicated with a special 5.4 column for that reason.. Exhibit A: Curator 5.4 documentation\nExhibit B: Curator 5.3 documentation\nAnd as you can see, there is a disclaimer on the 5.3 page that says, \"this version is outdated.\". Sorry if my tone comes across as grumpy.  I'm grateful you took the time to submit a patch and to contribute to the community.. Going to close this as unnecessary, as explained already.. Are you 100% sure you got the 4.2.6 release of Curator installed?  Can you reinstall from the RPM just to be sure?  That error can happen if a binary was compiled against the wrong version of libc.  RedHat tends not to change libc versions in minor point releases.. Even double-checking by doing \nls -l /opt/elasticsearch-curator/bin/curator\nfile /opt/elasticsearch-curator/bin/curator\nmd5sum /opt/elasticsearch-curator/bin/curator \non both machines to fully compare would be a good idea.\nAlso, what is the e?\n<production>[root@elasticsearch-01 ~]# lsattr /opt/elasticsearch-curator/curator\n--------------- /opt/elasticsearch-curator/curator\nvs\n[root@elasticsearch-02 ~]# lsattr /opt/elasticsearch-curator/curator\n-------------e- /opt/elasticsearch-curator/curator\nSecond dash from the right, just before /opt/elasticsearch-curator?\n. So curator_cli is working but curator does not?  That seems odd.  Did I read that correctly?\nWhat happens if you execute /opt/elasticsearch-curator/curator directly, instead of via the symlink?. This is an anomaly which has been observed in RedHat/CentOS machines before.  It's something unusual about how they do things that nobody (not even at RedHat) has been able to explain satisfactorily.\nMy recommendation is to (as root or via sudo) remove the symlink at /usr/bin/curator and re-link it again to see if that does anything differently.. > I tried deleting the creating the symlink /usr/bin/curator. I also tried creating symlink on other paths on the instance, but none of it works.\nThis seems to me to indicate something amiss with RedHat, if a functioning binary cannot be effectively symlinked.\n\nIs it possible to compile the rpm for elasticsearch-curator-4.2.6 for Centos 6.9 ?\n\nYou can try to use the scripts that I used here.\n. Please change this to merge into elastic:5.x instead of master.. Hmmm.  Can you rebase this against 5.x?  That's a bunch of other merges that don't need to happen in there.. If it doesn't build cleanly, I can't merge it.  Can you rebase against 5.x please?. I just re-ran the CI tests, and the thing keeping this from being merged has been fixed, but you will need to rebase against the current 5.x branch for that to work.  \nHow will you proceed?. I'm releasing version 5.5 soon, and if you want this change included, please do the requested changes soon.. You are so right.  \ud83d\ude1e \nI apologize for the extra work.  I want you to get credit for the change.  Part of the problem was an unrelated error that prevent the CI tests from succeeding.  But your PR was still against the old code, so it still failed.  I probably should have merged it anyway, being a doc change only.\nThank you for persevering.. I just noticed that box yesterday as well.  However, I think that the Allow edits from maintainers option does not allow me to rebase your original PR, just make edits to the branch.  This merge required a full rebase, unfortunately.. That has to go in a lot more places than just the option defaults. It also has to have version detection for the versions of Elasticsearch that Curator supports, but that don't support this new feature. . What I mean is that you can't just add it there.  It also has to be documented.  And documented that this feature is only acceptable in certain versions of Elasticsearch.  For example, it's listed in the 6.1 documentation but not in the 6.0 documentation.  This means that if that is added and some user tries to use it in any version older than 6.1, they'll get an error.  This has to be properly caught.. Closed by #1148 . @LucaWintergerst can you change the merge base from master to 5.x please?. You may need to rebase this against 5.x to get it to be build cleanly.. Duplicate of #1140 . I will have to confirm.  I was initially informed it was an AND, but the Elasticsearch documentation does appear to be an OR.\nCurator only passes the arguments along, so it will be a matter of correcting the docs is all, should the OR be the correct interpretation.. Fixed in #1140 . fixed by #1147 . What you have shared for the steps to reproduce the problem is not what you are using.\nThe error identifies Action ID \"2\", and the dictionary value new_index does not appear anywhere in the supplied config.\nPlease share exactly what you have, so I can duplicate it, or share the error generated by the supplied configuration.. By way of comparison, my own CI (Travis) tests with Voluptuous 0.10.5 and that works across python versions 2.7, 3.4, 3.5, and 3.6.. Will look into it.  My current CI builds report 10 failures, and all of them are schema related.. This is fixed by way of #1156 . Yes. Should be within a week or two. I have a few more bugs to squash before I make a new release. . What command are you executing?  What does your config.yml file look like (don't share passwords).\ne.g.\n```\n\nclient:\n  hosts: [ \"127.0.0.1\" ]\n  port: 9200\nlogging:\n  loglevel: DEBUG\n  logfile: \"/var/log/curator/curator.log\"\n```\nYou don't technically need more than this.  You don't even need logfile.  Without it, Curator will log to stdout.  Is this file properly indented?\nOr, are you using curator_cli?  If so, what command?\ncurator_cli --host 127.0.0.1 --port 9200 show_indices --verbose --header\nThis should show all of your indices on the command-line.. What does your delete_indices.yml look like?. I will demonstrate in a minute with a live example.... Replace the age filter with one line:\n- filtertype: none\nUse --dry-run again.  That will be a completely unfiltered result.. And you're sure that hosts ['173.173.0.16'], 'port': 9201 is what you intended to connect to?  Because that's all the indices there are on this particular cluster.. There are no other indices, then.  That's all there is.. There is no .kibana index.  You'd see it if it were there:\ncurator_cli show_indices --verbose --header\nIndex                             State      Size      Docs Pri Rep   Creation Timestamp\n.kibana-6                          open     1.1MB       842   1   1 2017-12-23T21:50:13Z\n.logstash                          open    50.2KB         8   1   1 2017-12-24T06:00:05Z\n.ml-anomalies-shared               open    37.1MB    184100   5   1 2017-05-04T20:16:22Z\n.ml-notifications                  open    33.1KB        44   1   1 2017-05-04T20:16:24Z\n.ml-state                          open   116.6KB         4   5   1 2017-05-04T20:16:27Z\n.monitoring-alerts-6               open    92.0KB        14   1   1 2017-08-25T19:16:26Z\n.monitoring-beats-6-2018.02.13     open     1.4MB      2374   1   1 2018-02-13T22:21:24Z\n.monitoring-beats-6-2018.02.14     open   123.9MB    237734   1   1 2018-02-14T00:00:00Z\n.monitoring-beats-6-2018.02.15     open   100.7MB    188612   1   1 2018-02-15T00:00:00Z\n.monitoring-es-6-2018.02.09        open   604.8MB   1003192   1   1 2018-02-09T00:00:00Z\n.monitoring-es-6-2018.02.10        open   579.5MB    962520   1   1 2018-02-10T00:00:01Z\n.monitoring-es-6-2018.02.11        open   578.9MB    959128   1   1 2018-02-11T00:00:00Z\n.monitoring-es-6-2018.02.12        open   491.2MB    805890   1   1 2018-02-12T00:00:01Z\n.monitoring-es-6-2018.02.13        open   552.0MB    951226   1   1 2018-02-13T00:08:55Z\n.monitoring-es-6-2018.02.14        open   626.9MB   1024786   1   1 2018-02-14T00:00:00Z\n.monitoring-es-6-2018.02.15        open   575.3MB    804222   1   1 2018-02-15T00:00:02Z\n.monitoring-kibana-6-2018.02.09    open     4.0MB     17276   1   1 2018-02-09T00:00:05Z\n.monitoring-kibana-6-2018.02.10    open     4.0MB     17278   1   1 2018-02-10T00:00:01Z\n.monitoring-kibana-6-2018.02.11    open     3.9MB     17276   1   1 2018-02-11T00:00:06Z\n.monitoring-kibana-6-2018.02.12    open     3.4MB     14412   1   1 2018-02-12T00:00:01Z\n.monitoring-kibana-6-2018.02.13    open     4.4MB     16878   1   1 2018-02-13T00:16:20Z\n.monitoring-kibana-6-2018.02.14    open     4.4MB     17278   1   1 2018-02-14T00:00:02Z\n.monitoring-kibana-6-2018.02.15    open     3.1MB     13470   1   1 2018-02-15T00:00:03Z\n.monitoring-logstash-6-2018.02.09  open   203.2MB    967008   1   1 2018-02-09T00:00:00Z\n.monitoring-logstash-6-2018.02.10  open   204.5MB    966896   1   1 2018-02-10T00:00:05Z\n.monitoring-logstash-6-2018.02.11  open   202.8MB    965664   1   1 2018-02-11T00:00:00Z\n.monitoring-logstash-6-2018.02.12  open   169.6MB    806624   1   1 2018-02-12T00:00:06Z\n.monitoring-logstash-6-2018.02.13  open   189.2MB    936824   1   1 2018-02-13T00:25:03Z\n.monitoring-logstash-6-2018.02.14  open   198.5MB    967120   1   1 2018-02-14T00:00:05Z\n.monitoring-logstash-6-2018.02.15  open   155.3MB    753984   1   1 2018-02-15T00:00:04Z\n.security-6                        open    11.4KB         6   1   2 2017-12-23T21:50:22Z\n.tasks                             open   167.2KB        24   1   1 2017-03-16T20:53:39Z\n.triggered_watches-6               open     1.2MB         0   1   1 2017-12-23T21:50:31Z\n.watcher-history-7-2018.02.12      open    14.9MB     12104   1   1 2018-02-12T00:00:06Z\n.watcher-history-7-2018.02.13      open    24.8MB     15488   1   1 2018-02-13T00:16:40Z\n.watcher-history-7-2018.02.14      open    15.2MB      8882   1   1 2018-02-14T00:00:57Z\n.watcher-history-7-2018.02.15      open    11.8MB      6946   1   1 2018-02-15T00:00:57Z\n.watches-6                         open     2.1MB        12   1   1 2017-12-23T21:50:35Z\nfail2ban-000004                    open     3.0MB     13736   1   1 2018-01-04T21:45:11Z\nipmi-000004                        open    96.1MB    379288   1   1 2017-12-24T06:15:29Z\nmetricbeat-000097                  open     3.3GB  14270614   1   1 2018-01-23T09:29:48Z\nmetricbeat-000098                  open     3.5GB  14657628   1   1 2018-02-04T09:30:03Z\nmetricbeat-000099                  open     3.7GB  13868654   1   1 2018-02-09T09:30:01Z\nmetricbeat-000100                  open  1000.0MB   3067436   1   1 2018-02-14T09:30:02Z\nnxfilter-000003                    open    46.9MB    535826   1   1 2018-01-04T21:47:02Z\npacketbeat-000053                  open     2.1GB   8637212   1   1 2018-02-14T02:42:01Z\nrsyslog-000003                     open   265.8MB   1378992   1   1 2018-01-04T21:39:44Z\nwebserver-000001                   open   531.6MB   1746012   1   1 2018-01-04T22:31:47Z. Yes, you can.  But those indices that Curator showed were the only ones on the cluster at the time of execution.. You either created them in a Kibana connected to a different cluster, or they didn't get created.. Closing, as this does not appear to be a problem with Curator.. While long requested, the Elasticsearch team has pretty much always recommended that people not use index size as a determining characteristic.  In fact, while it's absolutely possible to have a 200G shard size, we typically recommend that users not let shard sizes grow past 50G, and even 20G-30G as a best-practices maximum.  These recommendations are primarily because we know too well how long it takes for data to migrate between nodes.\nI will contemplate adding this feature, but please know if I don't it's mostly because I worry that if Curator supports it, it gives the appearance that Elastic and the Elasticsearch developers put the stamp of approval on cluster practices it doesn't actually recommend.. There is a request for max_size to be added to Curator, but it's a bit of a pain as that has to have tests to only work in 6.1+ or 6.2+ as it isn't in any other versions.  While it's not impossible for it to be used in other cases, it was specifically targeted at users who use a single shard index and want to rollover when it hits a designated shard size.  Also, Rollover indices almost certainly will not be the product of specific shard routing type indexing, where a given shard will be bigger than another.  It is because of these changes that max_size was even considered.. max_size was recently merged.  If you are using 6.1 or higher, then that will be available to you.. I'm almost done adding this feature.  I need to write a test or two for it to ensure it works as expected.. Added in #1214 . Please rebase against the 5.x branch, and use that as the merge branch. . This cannot be merged until 2 changes are made:\n\nChange the merge base from elastic:master to elastic:5.x\nRebase your changes against the most recent 5.x branch.\n\nThese two changes will fix the build issues that are preventing it from working in Travis CI.. I'm getting ready to release 5.5 soon.  If you want this change to be included, please expedite the requested changes.. Excellent question.  It's already resolved in 5.4.  It's still called \"field_stats\" in Curator, but it is actually just queries/aggregations.\nSo, long story short: You're already covered.. And, yes, Curator 5.4 supports Elasticsearch 6. This would be a duplicate of #1124 \nYou need the OpenSSL 1.0.2 package for Curator to work.  I apologize for the inconvenience.. No guarantees for SuSE-based distros.  The RPMs I make are tested on CentOS 6 and 7.. Please understand that the error is not from Curator, but from Elasticsearch.  Curator is an index selection wrapper on plain old Elasticsearch API calls.  If you had made the API call directly (e.g. command-line, or the Console in Kibana), the result would have been the exact same.\nWith that said, the error may (or may not) indicate something else going on:\norg.elasticsearch.repositories.RepositoryException: [repo] concurrent modification of the index-N file, expected current generation [1290], actual current generation [1293] - possibly due to simultaneous snapshot deletion requests\nYou're trying to snapshot to a file that is being modified while you are snapshotting.  This should not be possible.  What kind of repository is it? Do you have multiple clusters accessing the same repository?  Curator will not allow a snapshot_delete to happen while a snapshot is IN_PROGRESS, as it tests to see if something is going on, but that state is only within a single cluster (if you had multiples accessing the same repository).\nI can't say for sure what's happening in your cluster, but I can say with much certainty that it isn't likely to be Curator that is causing the problem.\n. And that's what I get for not reading deeper:\n\nWent from single cluster single snapshot for all Daily indices (which had high likelihood of causing partial snapshots) to multi-cluster where each cluster had action file with 1 action per daily index (giving more granularity and visibility on snapshots that fail).\n\nIt seems my guess was not far from the mark.\nIf you're doing a multi-cluster snapshot to the same shared filesystem, at least be wise and have the filesystem paths for each cluster be different. Create different repositories, each using their own filesystem path.  It seems certain you ended up with these collisions because of this approach.. As this is not a Curator bug, and a workable solution to your problem presented, I am closing this.. Curator 3.4 only overrides the default value of 30 seconds if no default value is provided\nThe code that overrides that value is here.\nThe --request-timeout flag value is different, and doesn't work correctly or have an effect beyond the regular client --timeout value, which is why this setting is omitted in future versions of Curator.\nYou should know that Curator 3.5.1 is the last one in the Curator 3.x series, and was release 2016-03-21\u2014nearly 2 years ago.  The version you're using, 3.4.0, was released 2015-10-28, well over 2 years ago.  Neither version is supported any longer, as the versions of Elasticsearch they support are no longer supported either.  If you are compelled to use an older version of Curator, you should at least use version 3.5.1, with the latest settings.  If you're stuck with 3.4.0, you probably got it from an alternate repository, as that's the latest version I've seen bundled in some of those repositories.  You can use the official repositories for 3.5 to download a newer version of the 3.x series.  By way of comparison, Curator 4 supports Elasticsearch 2.x and 5.x installs.  Curator 5 supports Elasticsearch 5.x and 6.x installs.\nSince the version you're requesting a change for is very much no longer supported, this issue will be closed.. There is a feature coming in the 5.5 release which will help you to get an acceptable result, hopefully, using Elasticsearch date math.  See the pending documentation here.. Date math is an Elasticsearch concept.  Rather than extract the date from the index (which would be a regular expression operation), date math would allow you to do a relative match of the date.\nFor example, if I ran Curator at 02:00 UTC, today's index might be logstash-2018.03.19, but I want to associate yesterday's index with an alias.  I can use the index selection filters to select only yesterday's index, but would use date math to get the alias name to match:\nactions:\n  1:\n    action: alias\n    description: Associate my-alias-%Y.%m.%d with yesterday's logstash-%Y.%m.%d index with matching dates\n    options:\n      name: \"<my_alias-{now/d-1d}>\"\n    add:\n      filters:\n      - filtertype: pattern\n        kind: prefix \n        value: logstash-\n      - filtertype: period\n        source: name\n        range_from: -1\n        range_to: -1\n        timestring: '%Y.%m.%d'\n        unit: days\nThis example action file should only match yesterday's logstash-%Y.%m.%d index because of the pattern and period filters, and the date math in the name will create a my_alias-%Y.%m.%d alias associated with that index.. Since there's a potential work-around here, I'm closing this.. Is dynamic index creation disabled? If Elasticsearch is unable to create the index automatically, that would be the result.\nOtherwise, please enable debug logging and attach the relevant logs.  What you are describing does not happen in the Curator CI tests that perform both local and remote reindexing tests, and I'd like to better understand what's happening in your environment.. The source index will not be deleted automatically after a reindex operation, neither by Elasticsearch, nor Curator.. Can you add a blurb to the Changelog to reflect these changes, please?. Thanks for reporting this.  It seems that I neglected to update the schema completely for snapshot actions using the period filter.  I will see about fixing this for the pending 5.5 release.. The behavior you are seeing is exactly what is described in the Elasticsearch documentation for the Rollover API.  Curator has no control over this.\nThere is an undocumented new_index option for rollover that allows you to specify what the new index name should be.  I should document this feature for the 5.5 release.. Did you have any closed indices? I think I know what the problem was.  \nCurator already checks for an empty list condition before proceeding.  The problem is that it checks for closed indices after that.  I'm changing the order of these statement so an empty list cannot be passed here.. This appears to have been the only class that was doing it out of order, so I apologize for the inconvenience it caused you.\nYou barely got this bug submitted in time to be fixed for 5.5 (which is building/testing right now).. Bug fix is in for 5.5.  Thanks for reporting it!. Hmmm.  On a clean build of Python 3.6.4 I did last night on a 32bit ARM machine:\npip3.6 install elasticsearch-curator\nCollecting elasticsearch-curator\n  Downloading elasticsearch-curator-5.5.0.tar.gz (212kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 215kB 603kB/s\nCollecting voluptuous>=0.9.3 (from elasticsearch-curator)\n  Downloading voluptuous-0.11.1-py2.py3-none-any.whl\nCollecting urllib3>=1.20 (from elasticsearch-curator)\n  Downloading urllib3-1.22-py2.py3-none-any.whl (132kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133kB 721kB/s\nCollecting elasticsearch==5.5.2 (from elasticsearch-curator)\n  Downloading elasticsearch-5.5.2-py2.py3-none-any.whl (59kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 906kB/s\nCollecting click>=6.7 (from elasticsearch-curator)\n  Downloading click-6.7-py2.py3-none-any.whl (71kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 879kB/s\nCollecting pyyaml>=3.10 (from elasticsearch-curator)\n  Downloading PyYAML-3.12.tar.gz (253kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 256kB 564kB/s\nCollecting certifi>=2018.1.18 (from elasticsearch-curator)\n  Downloading certifi-2018.1.18-py2.py3-none-any.whl (151kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 153kB 762kB/s\nInstalling collected packages: voluptuous, urllib3, elasticsearch, click, pyyaml, certifi, elasticsearch-curator\n  Running setup.py install for pyyaml ... done\n  Running setup.py install for elasticsearch-curator ... done\nSuccessfully installed certifi-2018.1.18 click-6.7 elasticsearch-5.5.2 elasticsearch-curator-5.5.0 pyyaml-3.12 urllib3-1.22 voluptuous-0.11.1\nBut it fails with the system Python 2.7.10 on macOS 10.13:\n```\n$ /usr/bin/python -m pip install --user -U elasticsearch-curator\nCollecting elasticsearch-curator\n  Downloading elasticsearch-curator-5.5.0.tar.gz (212kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 215kB 1.2MB/s\nCollecting voluptuous>=0.9.3 (from elasticsearch-curator)\n  Using cached voluptuous-0.11.1-py2.py3-none-any.whl\nRequirement already up-to-date: urllib3>=1.20 in ./Library/Python/2.7/lib/python/site-packages (from elasticsearch-curator)\nCollecting elasticsearch==5.5.2 (from elasticsearch-curator)\n  Using cached elasticsearch-5.5.2-py2.py3-none-any.whl\nRequirement already up-to-date: click>=6.7 in ./Library/Python/2.7/lib/python/site-packages (from elasticsearch-curator)\nRequirement already up-to-date: pyyaml>=3.10 in ./Library/Python/2.7/lib/python/site-packages (from elasticsearch-curator)\nCollecting certifi>=2018.1.18 (from elasticsearch-curator)\n  Using cached certifi-2018.1.18-py2.py3-none-any.whl\nBuilding wheels for collected packages: elasticsearch-curator\n  Running setup.py bdist_wheel for elasticsearch-curator ... error\n  Complete output from command /usr/bin/python -u -c \"import setuptools, tokenize;file='/private/var/folders/ww/dzq_b7452vsddzqbn9thn28m0000gn/T/pip-build-T0JKse/elasticsearch-curator/setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" bdist_wheel -d /var/folders/ww/dzq_b7452vsddzqbn9thn28m0000gn/T/tmpc71XHRpip-wheel- --python-tag cp27:\n  running bdist_wheel\n  running build\n  running build_py\n  creating build\n  creating build/lib\n  creating build/lib/curator\n  copying curator/indexlist.py -> build/lib/curator\n  copying curator/_version.py -> build/lib/curator\n  copying curator/actions.py -> build/lib/curator\n  copying curator/init.py -> build/lib/curator\n  copying curator/curator_cli.py -> build/lib/curator\n  copying curator/cli.py -> build/lib/curator\n  copying curator/singletons.py -> build/lib/curator\n  copying curator/utils.py -> build/lib/curator\n  copying curator/logtools.py -> build/lib/curator\n  copying curator/exceptions.py -> build/lib/curator\n  copying curator/config_utils.py -> build/lib/curator\n  copying curator/main.py -> build/lib/curator\n  copying curator/repomgrcli.py -> build/lib/curator\n  copying curator/snapshotlist.py -> build/lib/curator\n  running egg_info\n  writing requirements to elasticsearch_curator.egg-info/requires.txt\n  writing elasticsearch_curator.egg-info/PKG-INFO\n  writing top-level names to elasticsearch_curator.egg-info/top_level.txt\n  writing dependency_links to elasticsearch_curator.egg-info/dependency_links.txt\n  writing entry points to elasticsearch_curator.egg-info/entry_points.txt\n  reading manifest file 'elasticsearch_curator.egg-info/SOURCES.txt'\n  reading manifest template 'MANIFEST.in'\n  warning: no files found matching 'Changelog.rst'\n  warning: no previously-included files matching 'pycache' found under directory ''\n  warning: no previously-included files matching '.py[co]' found under directory ''\n  warning: no previously-included files matching '.pyc' found under directory 'curator'\n  warning: no previously-included files matching '.pyo' found under directory 'curator'\n  warning: no previously-included files matching '.pyc' found under directory 'docs'\n  warning: no previously-included files matching '.pyo' found under directory 'docs'\n  warning: no previously-included files matching '.pyc' found under directory 'test'\n  warning: no previously-included files matching '*.pyo' found under directory 'test'\n  no previously-included directories found matching 'docs/_build'\n  no previously-included directories found matching 'docs/asciidoc/html_docs'\n  writing manifest file 'elasticsearch_curator.egg-info/SOURCES.txt'\n  creating build/lib/curator/defaults\n  copying curator/defaults/init.py -> build/lib/curator/defaults\n  copying curator/defaults/client_defaults.py -> build/lib/curator/defaults\n  copying curator/defaults/filter_elements.py -> build/lib/curator/defaults\n  copying curator/defaults/filtertypes.py -> build/lib/curator/defaults\n  copying curator/defaults/option_defaults.py -> build/lib/curator/defaults\n  copying curator/defaults/settings.py -> build/lib/curator/defaults\n  creating build/lib/curator/validators\n  copying curator/validators/init.py -> build/lib/curator/validators\n  copying curator/validators/actions.py -> build/lib/curator/validators\n  copying curator/validators/config_file.py -> build/lib/curator/validators\n  copying curator/validators/filters.py -> build/lib/curator/validators\n  copying curator/validators/options.py -> build/lib/curator/validators\n  copying curator/validators/schemacheck.py -> build/lib/curator/validators\n  running build_exe\n  error: [Errno 2] No such file or directory: 'run_curator.py'\n\nFailed building wheel for elasticsearch-curator\n  Running setup.py clean for elasticsearch-curator\nFailed to build elasticsearch-curator\nInstalling collected packages: voluptuous, elasticsearch, certifi, elasticsearch-curator\n  Found existing installation: voluptuous 0.10.5\n    Uninstalling voluptuous-0.10.5:\n      Successfully uninstalled voluptuous-0.10.5\n  Found existing installation: elasticsearch 5.4.0\n    Uninstalling elasticsearch-5.4.0:\n      Successfully uninstalled elasticsearch-5.4.0\n  Running setup.py install for elasticsearch-curator ... error\n    Complete output from command /usr/bin/python -u -c \"import setuptools, tokenize;file='/private/var/folders/ww/dzq_b7452vsddzqbn9thn28m0000gn/T/pip-build-T0JKse/elasticsearch-curator/setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" install --record /var/folders/ww/dzq_b7452vsddzqbn9thn28m0000gn/T/pip-uwDXT4-record/install-record.txt --single-version-externally-managed --compile --user --prefix=:\n    usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n       or: -c --help [cmd1 cmd2 ...]\n       or: -c --help-commands\n       or: -c cmd --help\nerror: option --single-version-externally-managed not recognized\n\n----------------------------------------\n\nCommand \"/usr/bin/python -u -c \"import setuptools, tokenize;file='/private/var/folders/ww/dzq_b7452vsddzqbn9thn28m0000gn/T/pip-build-T0JKse/elasticsearch-curator/setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" install --record /var/folders/ww/dzq_b7452vsddzqbn9thn28m0000gn/T/pip-uwDXT4-record/install-record.txt --single-version-externally-managed --compile --user --prefix=\" failed with error code 1 in /private/var/folders/ww/dzq_b7452vsddzqbn9thn28m0000gn/T/pip-build-T0JKse/elasticsearch-curator/\nYou are using pip version 9.0.1, however version 9.0.3 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\n```\nAnd passes with the brew installed 2.7.13:\n$ python -m pip install --user -U elasticsearch-curator\nCollecting elasticsearch-curator\n  Using cached elasticsearch-curator-5.5.0.tar.gz\nRequirement already up-to-date: voluptuous>=0.9.3 in ./Library/Python/2.7/lib/python/site-packages (from elasticsearch-curator)\nRequirement already up-to-date: urllib3>=1.20 in ./Library/Python/2.7/lib/python/site-packages (from elasticsearch-curator)\nRequirement already up-to-date: elasticsearch==5.5.2 in ./Library/Python/2.7/lib/python/site-packages (from elasticsearch-curator)\nRequirement already up-to-date: click>=6.7 in ./Library/Python/2.7/lib/python/site-packages (from elasticsearch-curator)\nRequirement already up-to-date: pyyaml>=3.10 in ./Library/Python/2.7/lib/python/site-packages (from elasticsearch-curator)\nRequirement already up-to-date: certifi>=2018.1.18 in ./Library/Python/2.7/lib/python/site-packages (from elasticsearch-curator)\nBuilding wheels for collected packages: elasticsearch-curator\n  Running setup.py bdist_wheel for elasticsearch-curator ... done\n  Stored in directory: /Users/buh/Library/Caches/pip/wheels/7b/c3/f5/d0674ae57c1201b3a31c903d5ee613e90e27c8713392241f8c\nSuccessfully built elasticsearch-curator\nInstalling collected packages: elasticsearch-curator\nSuccessfully installed elasticsearch-curator-5.5.0\nYou are using pip version 9.0.1, however version 9.0.3 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\n(That last one may be because I typically use the brew installed version, so there might be conflicts between the system and brew versions).\nIt also installed flawlessly on Windows with pip (again, Python 3.6.4).\nI'll continue testing on my end.  Any more information you can give me will be helpful as I try to track this down.. This is messed up.  I literally cannot replicate what you are seeing.  In a clean, newly spun up environment (Ubuntu 1604) I used the system Python 3.5.2, and installed python3-pip via apt.  I installed virtualenv, and you can see the flow below:\nbuh@ubuntu1604-pkg-test:~$ pip3 install --user -U virtualenv\nCollecting virtualenv\n  Downloading virtualenv-15.2.0-py2.py3-none-any.whl (2.6MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.6MB 609kB/s\nInstalling collected packages: virtualenv\nSuccessfully installed virtualenv\nYou are using pip version 8.1.1, however version 9.0.3 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\nbuh@ubuntu1604-pkg-test:~$ ~/.local/bin/virtualenv curator_venv\nUsing base prefix '/usr'\nNew python executable in /home/buh/curator_venv/bin/python3\nAlso creating executable in /home/buh/curator_venv/bin/python\nInstalling setuptools, pip, wheel...done.\nbuh@ubuntu1604-pkg-test:~$ source curator_venv/bin/activate\n(curator_venv) buh@ubuntu1604-pkg-test:~$ cd curator_venv/\n(curator_venv) buh@ubuntu1604-pkg-test:~/curator_venv$ pip install elasticsearch-curator\nCollecting elasticsearch-curator\n  Downloading elasticsearch-curator-5.5.0.tar.gz (212kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 215kB 1.3MB/s\nCollecting voluptuous>=0.9.3 (from elasticsearch-curator)\n  Downloading voluptuous-0.11.1-py2.py3-none-any.whl\nCollecting urllib3>=1.20 (from elasticsearch-curator)\n  Downloading urllib3-1.22-py2.py3-none-any.whl (132kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133kB 1.1MB/s\nCollecting elasticsearch==5.5.2 (from elasticsearch-curator)\n  Downloading elasticsearch-5.5.2-py2.py3-none-any.whl (59kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 1.4MB/s\nCollecting click>=6.7 (from elasticsearch-curator)\n  Downloading click-6.7-py2.py3-none-any.whl (71kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 1.4MB/s\nCollecting pyyaml>=3.10 (from elasticsearch-curator)\n  Downloading PyYAML-3.12.tar.gz (253kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 256kB 1.4MB/s\nCollecting certifi>=2018.1.18 (from elasticsearch-curator)\n  Downloading certifi-2018.1.18-py2.py3-none-any.whl (151kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 153kB 1.5MB/s\nBuilding wheels for collected packages: elasticsearch-curator, pyyaml\n  Running setup.py bdist_wheel for elasticsearch-curator ... done\n  Stored in directory: /home/buh/.cache/pip/wheels/7b/c3/f5/d0674ae57c1201b3a31c903d5ee613e90e27c8713392241f8c\n  Running setup.py bdist_wheel for pyyaml ... done\n  Stored in directory: /home/buh/.cache/pip/wheels/2c/f7/79/13f3a12cd723892437c0cfbde1230ab4d82947ff7b3839a4fc\nSuccessfully built elasticsearch-curator pyyaml\nInstalling collected packages: voluptuous, urllib3, elasticsearch, click, pyyaml, certifi, elasticsearch-curator\nSuccessfully installed certifi-2018.1.18 click-6.7 elasticsearch-5.5.2 elasticsearch-curator-5.5.0 pyyaml-3.12 urllib3-1.22 voluptuous-0.11.1\nNothing here was cached, it's all clean downloads.  Why can I not replicate what you are seeing?. Tried the same on a different Mac (a mac mini):\nadmin@mini-osxserver (08:42 AM) ~\n\uf8ff virtualenv curator_venv\nUsing base prefix '/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6'\nNew python executable in /Users/admin/curator_venv/bin/python3.6\nAlso creating executable in /Users/admin/curator_venv/bin/python\nInstalling setuptools, pip, wheel...done.\nadmin@mini-osxserver (08:42 AM) ~\n\uf8ff source curator_venv/bin/activate\n(curator_venv) admin@mini-osxserver (08:42 AM) ~\n\uf8ff pip install elasticsearch-curator\nCollecting elasticsearch-curator\n  Downloading elasticsearch-curator-5.5.0.tar.gz (212kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 215kB 1.2MB/s\nCollecting voluptuous>=0.9.3 (from elasticsearch-curator)\n  Downloading voluptuous-0.11.1-py2.py3-none-any.whl\nCollecting urllib3>=1.20 (from elasticsearch-curator)\n  Downloading urllib3-1.22-py2.py3-none-any.whl (132kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133kB 1.2MB/s\nCollecting elasticsearch==5.5.2 (from elasticsearch-curator)\n  Downloading elasticsearch-5.5.2-py2.py3-none-any.whl (59kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 1.2MB/s\nCollecting click>=6.7 (from elasticsearch-curator)\n  Downloading click-6.7-py2.py3-none-any.whl (71kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 1.3MB/s\nCollecting pyyaml>=3.10 (from elasticsearch-curator)\n  Downloading PyYAML-3.12.tar.gz (253kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 256kB 1.1MB/s\nCollecting certifi>=2018.1.18 (from elasticsearch-curator)\n  Downloading certifi-2018.1.18-py2.py3-none-any.whl (151kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 153kB 1.4MB/s\nBuilding wheels for collected packages: elasticsearch-curator, pyyaml\n  Running setup.py bdist_wheel for elasticsearch-curator ... done\n  Stored in directory: /Users/admin/Library/Caches/pip/wheels/7b/c3/f5/d0674ae57c1201b3a31c903d5ee613e90e27c8713392241f8c\n  Running setup.py bdist_wheel for pyyaml ... done\n  Stored in directory: /Users/admin/Library/Caches/pip/wheels/2c/f7/79/13f3a12cd723892437c0cfbde1230ab4d82947ff7b3839a4fc\nSuccessfully built elasticsearch-curator pyyaml\nInstalling collected packages: voluptuous, urllib3, elasticsearch, click, pyyaml, certifi, elasticsearch-curator\nSuccessfully installed certifi-2018.1.18 click-6.7 elasticsearch-5.5.2 elasticsearch-curator-5.5.0 pyyaml-3.12 urllib3-1.22 voluptuous-0.11.1\nIt works here with Python 3.6.1.. I have an even longer flow installing from python setup.py install from the tarball on github here.  Still can't get those errors.. I can confirm the docker behavior with the python:3.6.1-alpine image, but it works flawlessly with python:3.6.4-alpine:\nadmin@mini-osxserver (09:23 AM) ~\n\uf8ff docker run -ti --rm python:3.6.4-alpine /bin/sh\nUnable to find image 'python:3.6.4-alpine' locally\n3.6.4-alpine: Pulling from library/python\n81033e7c1d6a: Pull complete\n9b61101706a6: Pull complete\n35b21c1a8b97: Pull complete\n4856f5aeeec4: Pull complete\n5d8be0d67f9c: Pull complete\nDigest: sha256:20d014036dc80a73b64d35db756ce5d79abf6e319b545e54719e815d8c9660c5\nStatus: Downloaded newer image for python:3.6.4-alpine\n/ # pip install elasticsearch-curator\nCollecting elasticsearch-curator\n  Downloading elasticsearch-curator-5.5.0.tar.gz (212kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 215kB 1.2MB/s\nCollecting voluptuous>=0.9.3 (from elasticsearch-curator)\n  Downloading voluptuous-0.11.1-py2.py3-none-any.whl\nCollecting urllib3>=1.20 (from elasticsearch-curator)\n  Downloading urllib3-1.22-py2.py3-none-any.whl (132kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133kB 1.2MB/s\nCollecting elasticsearch==5.5.2 (from elasticsearch-curator)\n  Downloading elasticsearch-5.5.2-py2.py3-none-any.whl (59kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 1.4MB/s\nCollecting click>=6.7 (from elasticsearch-curator)\n  Downloading click-6.7-py2.py3-none-any.whl (71kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 1.4MB/s\nCollecting pyyaml>=3.10 (from elasticsearch-curator)\n  Downloading PyYAML-3.12.tar.gz (253kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 256kB 996kB/s\nCollecting certifi>=2018.1.18 (from elasticsearch-curator)\n  Downloading certifi-2018.1.18-py2.py3-none-any.whl (151kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 153kB 1.4MB/s\nBuilding wheels for collected packages: elasticsearch-curator, pyyaml\n  Running setup.py bdist_wheel for elasticsearch-curator ... done\n  Stored in directory: /root/.cache/pip/wheels/7b/c3/f5/d0674ae57c1201b3a31c903d5ee613e90e27c8713392241f8c\n  Running setup.py bdist_wheel for pyyaml ... done\n  Stored in directory: /root/.cache/pip/wheels/2c/f7/79/13f3a12cd723892437c0cfbde1230ab4d82947ff7b3839a4fc\nSuccessfully built elasticsearch-curator pyyaml\nInstalling collected packages: voluptuous, urllib3, elasticsearch, click, pyyaml, certifi, elasticsearch-curator\nSuccessfully installed certifi-2018.1.18 click-6.7 elasticsearch-5.5.2 elasticsearch-curator-5.5.0 pyyaml-3.12 urllib3-1.22 voluptuous-0.11.1\nYou are using pip version 9.0.2, however version 9.0.3 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.. And it fails in docker with python:2.7.13-alpine:\n```\nadmin@mini-osxserver (09:25 AM) ~\n\uf8ff docker run -ti --rm python:2.7.13-alpine /bin/sh\nUnable to find image 'python:2.7.13-alpine' locally\n2.7.13-alpine: Pulling from library/python\n90f4dba627d6: Already exists\na615e2cf13bb: Pull complete\n2e911db79bd7: Pull complete\n206337ed2b09: Pull complete\nDigest: sha256:91139f5fe39083d5fa1c37ba7b4213395104c146bc7f841880d136c5dbff3bfd\nStatus: Downloaded newer image for python:2.7.13-alpine\n/ # pip install elasticsearch-curator\nCollecting elasticsearch-curator\n  Downloading elasticsearch-curator-5.5.0.tar.gz (212kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 215kB 1.1MB/s\n    Complete output from command python setup.py egg_info:\n    Traceback (most recent call last):\n      File \"\", line 1, in \n      File \"/tmp/pip-build-sK3qRm/elasticsearch-curator/setup.py\", line 140, in \n        test_suite = \"test.run_tests.run_all\",\n      File \"/usr/local/lib/python2.7/distutils/core.py\", line 124, in setup\n        dist.parse_config_files()\n      File \"/usr/local/lib/python2.7/site-packages/setuptools/dist.py\", line 437, in parse_config_files\n        parse_configuration(self, self.command_options)\n      File \"/usr/local/lib/python2.7/site-packages/setuptools/config.py\", line 106, in parse_configuration\n        meta.parse()\n      File \"/usr/local/lib/python2.7/site-packages/setuptools/config.py\", line 382, in parse\n        section_parser_method(section_options)\n      File \"/usr/local/lib/python2.7/site-packages/setuptools/config.py\", line 355, in parse_section\n        self[name] = value\n      File \"/usr/local/lib/python2.7/site-packages/setuptools/config.py\", line 173, in setitem\n        value = parser(value)\n      File \"/usr/local/lib/python2.7/site-packages/setuptools/config.py\", line 428, in _parse_version\n        version = self._parse_attr(value)\n      File \"/usr/local/lib/python2.7/site-packages/setuptools/config.py\", line 305, in _parse_attr\n        module = import_module(module_name)\n      File \"/usr/local/lib/python2.7/importlib/init.py\", line 37, in import_module\n        import(name)\n      File \"/tmp/pip-build-sK3qRm/elasticsearch-curator/curator/init.py\", line 4, in \n        from .validators import \n      File \"/tmp/pip-build-sK3qRm/elasticsearch-curator/curator/validators/init.py\", line 1, in \n        from .schemacheck import SchemaCheck\n      File \"/tmp/pip-build-sK3qRm/elasticsearch-curator/curator/validators/schemacheck.py\", line 1, in \n        from voluptuous import \n    ImportError: No module named voluptuous\n----------------------------------------\n\nCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-sK3qRm/elasticsearch-curator/\nYou are using pip version 9.0.1, however version 9.0.3 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\n``. Same with 2.7.14, as it seems to have a hard time with the dependencies.  I think I need to roll back the changes introduced in #1122 as they are the lynchpin here.. It seems the issues are related to changes in thesetup.pythat were not supported by Python 3.6.1, and perhaps older versions.  These were addressed in continuous CI by adding [these lines](https://github.com/elastic/curator/blob/v5.5.0/.travis.yml#L28-L32) to the.travis.yml`.  Pre-installing the dependencies seems to have addressed it for those cases, but this hid the deeper problem.\nI'm rolling those changes back, and taking out those lines until it builds properly in CI.. The committed changes work.. Confirmed behavior fixed where it was previously broken:\nadmin@mini-osxserver (11:28 AM) ~\n\uf8ff docker run -ti --rm python:2.7.13-alpine /bin/sh\n/ # pip install elasticsearch-curator\nCollecting elasticsearch-curator\n  Downloading elasticsearch-curator-5.5.1.tar.gz (212kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 215kB 1.2MB/s\nCollecting elasticsearch==5.5.2 (from elasticsearch-curator)\n  Downloading elasticsearch-5.5.2-py2.py3-none-any.whl (59kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 1.4MB/s\nCollecting click>=6.7 (from elasticsearch-curator)\n  Downloading click-6.7-py2.py3-none-any.whl (71kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 1.3MB/s\nCollecting pyyaml>=3.10 (from elasticsearch-curator)\n  Downloading PyYAML-3.12.tar.gz (253kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 256kB 1.2MB/s\nCollecting voluptuous>=0.9.3 (from elasticsearch-curator)\n  Downloading voluptuous-0.11.1-py2.py3-none-any.whl\nCollecting certifi>=2018.1.18 (from elasticsearch-curator)\n  Downloading certifi-2018.1.18-py2.py3-none-any.whl (151kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 153kB 160kB/s\nCollecting urllib3<1.23,>=1.21.1 (from elasticsearch==5.5.2->elasticsearch-curator)\n  Downloading urllib3-1.22-py2.py3-none-any.whl (132kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133kB 1.2MB/s\nBuilding wheels for collected packages: elasticsearch-curator, pyyaml\n  Running setup.py bdist_wheel for elasticsearch-curator ... done\n  Stored in directory: /root/.cache/pip/wheels/c1/b9/58/addd16f5286686748b19791dfc6580291e04d7c04248f4616a\n  Running setup.py bdist_wheel for pyyaml ... done\n  Stored in directory: /root/.cache/pip/wheels/2c/f7/79/13f3a12cd723892437c0cfbde1230ab4d82947ff7b3839a4fc\nSuccessfully built elasticsearch-curator pyyaml\nInstalling collected packages: urllib3, elasticsearch, click, pyyaml, voluptuous, certifi, elasticsearch-curator\nSuccessfully installed certifi-2018.1.18 click-6.7 elasticsearch-5.5.2 elasticsearch-curator-5.5.1 pyyaml-3.12 urllib3-1.22 voluptuous-0.11.1\nYou are using pip version 9.0.1, however version 9.0.3 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.. That's an old bug, where the Curator version was incorrectly reported.  It's been fixed. See #1089 . Sorry.  There's not much I can do with old versions misreporting due to a bug I've since fixed.. @topkool this is a fixed bug. You are installing an older version of Curator.  Please update to version 5.5.4 of Curator, as compiled for Debian 9.. That requirement is for the Elasticsearch-py python module, not Elasticsearch itself.  As you can see in the compatibility matrix, Elasticsearch 6.x is supported in the current versions of Curator 5.x (support was added in Curator 5.4).. The elasticsearch==5.5.2 of the current release refers to the elasticsearch-py module.  The python module is called elasticsearch, not elasticsearch-py inside the python files.. That's because the tests do not cover SSL.  The errors for elasticsearch-py versions greater than 5.5.2 show up when you use SSL.\nThis will be addressed in a future version.  Until then, the requirement stays tied to 5.5.2.. The exception Curator is reporting doesn't correlate to a shard initialization timeout:\nException: name 'index' is not defined\nWhich appears to be in this line of code which should have the variable index as idx.\nIf you installed via pip, you could edit this yourself and find out.  If you installed via RPM or APT, then you won't be able to edit the file in question.\nMind you, this doesn't mean that everything will magically work if I fix that.  It may just return the correct error response.\nI'd be very much obliged if you could test this and report your findings.  If you need help to install Curator into a virtualenv so it doesn't conflict with anything, please let me know.. No, because the RPM and DEB packages are compiled.  I appreciate you testing this fix before I merge it (which I will be doing soon anyway, and possibly releasing as 5.5.2).. Now we can actually see what is happening under the hood.\nWhat do you have set for timeout in your client config file?  Because the default is 30 seconds, and you set wait_interval: 30.  If both are 30, then yes, it could trigger a timeout.  You should have timeout, or timeout_override set to 120 or some higher number.. The other thing to note is that it does, indeed seem that 30 seconds has elapsed:\n2018-03-30 07:51:32,370 INFO      curator.actions.shrink              do_action:2131...\n2018-03-30 07:52:02,403 ERROR     curator.actions.shrink              do_action:2143...\nSo, why does it take 30 seconds? It shouldn't take too terribly long to execute a hard link against all shards, but in your case it is.  You probably just need to increase the timeout to get this to work.\nThe second issue that could trigger this, is are you connecting through a load balancer or proxy or some other end point between you and the cluster? That can also cause issues on long running transactions.. The only way this happens is if the different hosts are not part of the same cluster. This happened in #1110 and was followed up in a discussion forum post.  Please ensure your nodes are actually clustered.  You can verify this by checking the cluster state of each host and comparing the results.. That is correct.. This represents a slight misunderstanding of what happens under the hood.  A snapshot completing with state FAILED can still represent successful execution of the Snapshot API\u2014which is exactly what happened, based on the log lines you provided.  In other words, continue_if_exception only catches code exceptions, and one did not happen here.\nSnapshot failure is never a good thing.  Do you know what's going on with this?  Have you checked the Elasticsearch logs to see why it's failing?  That might be a more pressing concern than altering this feature in Curator, as you're not even getting a PARTIAL snapshot state\u2014it's outright saying the snapshot has FAILED. (See more on snapshot status codes here).\nWith that said, it might be a good feature to add an option flag to force Curator to raise an exception if a snapshot doesn't complete with state SUCCESS, for example. . Yeah, it's not a failure if the API call returns successfully, regardless of what state the snapshot is in.  I do like the idea of setting a flag to raise an exception if the state is something other than SUCCESS.  I will change this to a feature request ticket.. A fair question.  In Curator 5.x (should I add it before releasing 6.x), it would default to not change the current behavior.  I could perhaps change it to raise an exception by default in 6.x.. As a reason, I would offer that you are the first person to whom this has happened, or at least reported it and considered it a fault of Curator.. This is fixed as of #1221 and fixed in release 5.5.3.. I wish I could help you here, but somehow, on your system, Elasticsearch itself is unable to read/write from the specified directory, even in /tmp (hence the 500 error from Elasticsearch, evident in TransportError: TransportError(500, u'exception', u'failed to create blob container')). \nSolve Elasticsearch's access problems, and the Curator test will work just fine.  Is Elasticsearch running as a different user than the user executing Curator?  That alone could explain the issue.. You've specified the path.repo option for your test cluster, right?\nIn my test Elasticsearch instances (both local, and Travis CI), I specify: \npath.repo: /\nin the elasticsearch.yml file.. I prefer to answer these at https://discuss.elastic.co/c/elasticsearch, as they are more searchable, and a better help to the community.\nThe way Curator does it is with a try/except block, catching NoIndices, as here.\nYour code might look like:\nilo.filter_by_regex(kind='prefix', value='logstash-')\nilo.filter_by_age(source='name', direction='older', timestring='%Y.%m.%d', unit='days', unit_count=30)\ndelete_indices = curator.Close(ilo)\ntry:\n    delete_indices.do_action()\nexcept curator.NoIndices:\n    # Process empty index list here.\n. No.  That's how it's actually handled in Curator's command-line code.  That setting is parsed from the option block of the action, and then used to catch errors before proceeding accordingly.  The code for this behavior is here.  ignore_empty_list is not handled in the action code itself, but between actions, based on how the previous action behaved.  This is why it is caught where it is.. Sorry.  This is rather arbitrary, and unnecessary, as an Elasticsearch repository can only be identified by its name in the API anyway.. Why stop there? Why not have it be Python 3.6?  Alpine Docker images ship with this, and are tiny.. Added in #1214 . Don't use Python 2.7 if you can, as its Unicode support is horrible, and will be EOL as of 1 Jan 2020.  The pre-compiled versions available as RPM and DEB packages are built with self-contained versions of Python 3.6, and should not suffer from this shortcoming.\nAlso, which release of Python 2.7 are you using?  Anything older than 2.7.9 may not be fully supported anyway.  I'm trying to get away from supporting 2.7 as much as I can with all of my Python projects, but there are just too many users of Linux distributions (which shall remain unnamed) which not only ship with but depend on ancient versions of Python.  These shipped versions of Python 2.7 in addition to being quite old and outdated also tend to have issues with outdated SSL libraries, which make it difficult if not impossible to fully support the range of capabilities Curator normally offers.\nYour best options are to use Python 3.5, 3.6, or if you have to, the most recent version of 2.7 compiled or installed into a special location, and then use a virtualenv with that version, specifically for Curator.. Closing for now.  If you believe this is in error, please feel free to re-open with a comment.. > In my case, the exception was caused by an index with no documents, \nThis was already addressed in #1167 and is in the latest version of Curator.  You're on 5.4.1, and 5.5.1 is the latest version.. Thanks for the patch, but I prefer the approach to this in #1203, so I'm going with that change instead.. Thanks for catching this.. Fixed in #1212 . > 2. Our X-Pack trial expiry\nThis is almost certainly what happened.  You have this much data in the logs, indicating that it started to process what was going on.  What happened is that Curator went to perform the next step which is likely a now-prohibited API call, and failed there.  You can see more by changing from command-line configuration to configuration file, and setting blacklist: [] in the logging section.  It will show the urllib3 and elasticsearch messages which are suppressed by default.. It won't until you've fully uninstalled the X-Pack plugin.  So long as that API call fails, Curator can't continue past that point.. \ud83d\udc4d \nFor your PR, please ensure that at least one test is added to test this functionality.  I don't care if it's a unit or integration test.. Also, please ensure the PR is against the 5.x branch.. Fixed by #1208 . \ud83d\udc4d \nThe more context and information you can give me about these pending changes, the better.  I'll be sure to add this functionality soon.. Closed by #1218. Thanks for the fix!. You seem to have missed this option for the current version of Curator which addresses the issue.. It will be simple enough to add a check for warn_if_no_indices in the spot you mentioned.  That will be the proper fix for this.. Except that ignore_empty_list is never passed to the Action, and warn_if_no_indices is.  That's part of the compromise.  ignore_empty_list exceptions are only processed after the action is complete.  In this case, we want to raise a NoIndices exception if there are no add or remove actions and warn_if_no_indices is true, otherwise raise the ActionError exception (like it currently does).  That will satisfy the problem you found, and allow ignore_empty_list: true to pass to the next action.. Fixed by #1213. It's looking like Monday, my time zone (UTC-6), probably in the afternoon.  I have a few other patches to apply today and perhaps tomorrow, and then building and testing Monday before release.. It could be pushed to Tuesday if I find something serious, or another bug shows up between now and then, though.. Any changes should be against the 5.x branch.  Please do not submit your changes to master.. You're still against the 5.5 branch, and not the 5.x branch.  \nPlease sign the CLA, or I cannot merge any code.. This PR is much more complex than it needs to be.  Please refer to https://github.com/elastic/curator/issues/1209#issuecomment-387889166 for more information.. This isn't going to be merged in its current state.  Please wait for me to make the needed updates without needing an API-breaking addition of arguments passed to every action.. Obviated by #1213. Curator does not support file globbing, and probably never will.  It is more likely that action storage in Elasticsearch will happen first.. Also, Curator v6 will allow you to define multiple actions to run against a single filter-set, removing the need for redundancy, saving some of that configuration length.. It's not a bug.  It's Elasticsearch telling you that you can't name the index the same as one already in existence.  Elasticsearch gives you the exact same error if you do this manually.  I'll show you what happens using console, as follows.\nI'll create this one, like what you have: \nPUT %3Ctest-%7Bnow%2FH%7BYYYY.MM.dd.HH%7D%7D-1%3E\n{\n  \"settings\": {\n    \"index\": {\n      \"number_of_shards\": 1,\n      \"number_of_replicas\": 0\n    }\n  },\n  \"aliases\": {\n    \"test\": {}\n  }\n}\n(You have to use URL-encoding for those characters in Console. It's exactly the same sequence you provided: <test-{now/H{YYYY.MM.dd.HH}}-1>)\nHere's the result:\n{\n  \"acknowledged\": true,\n  \"shards_acknowledged\": true,\n  \"index\": \"test-2018.05.19.21-1\"\n}\nNow, if I send a rollover request, with that same name in the same hour:\nPOST /test/_rollover/%3Ctest-%7Bnow%2FH%7BYYYY.MM.dd.HH%7D%7D-1%3E\n{\n  \"conditions\": {\n    \"max_age\":   \"1s\"\n  }\n}\nThis is the result (which is what you're getting in Curator):\n{\n  \"error\": {\n    \"root_cause\": [\n      {\n        \"type\": \"resource_already_exists_exception\",\n        \"reason\": \"index [test-2018.05.19.21-1/qdStmzQsRqCwJOj1VSZgJQ] already exists\",\n        \"index_uuid\": \"qdStmzQsRqCwJOj1VSZgJQ\",\n        \"index\": \"test-2018.05.19.21-1\"\n      }\n    ],\n    \"type\": \"resource_already_exists_exception\",\n    \"reason\": \"index [test-2018.05.19.21-1/qdStmzQsRqCwJOj1VSZgJQ] already exists\",\n    \"index_uuid\": \"qdStmzQsRqCwJOj1VSZgJQ\",\n    \"index\": \"test-2018.05.19.21-1\"\n  },\n  \"status\": 400\n}\nYou see the same 400 error with the accompanying resource_already_exists_exception in your Curator log.  Because you're doing a named-by-hour rollover, and since the hour hasn't rolled over, ending with -1 means a collision, as the number cannot increase.  \nProposed Solution\nI think what you want is to do is describe the name pattern just once.\nIf I change the index creation pattern to be per minute (I'm not going to wait an hour to demonstrate this):\n%3Ctest-%7Bnow%2Fm%7BYYYY.MM.dd.HH.mm%7D%7D-1%3E\n(which is <test-{now/m{YYYY.MM.dd.HH.mm}}-1>)\nI get an index named: test-2018.05.19.22.02-1\nWhen I rollover within the same minute and do not specify a new index name, I will see this output:\n{\n  \"old_index\": \"test-2018.05.19.22.02-1\",\n  \"new_index\": \"test-2018.05.19.22.02-000002\",\n  \"rolled_over\": true,\n  \"dry_run\": false,\n  \"acknowledged\": true,\n  \"shards_acknowledged\": true,\n  \"conditions\": {\n    \"[max_age: 1s]\": true\n  }\n}\nPlease note that you cannot prevent Elasticsearch from the zero-padding of the trailing number with this approach.  This is documented:\n\nIf the name of the existing index ends with - and a number\u2009\u2014\u2009e.g. logs-000001\u2009\u2014\u2009then the name of the new index will follow the same pattern, incrementing the number (logs-000002). The number is zero-padded with a length of 6, regardless of the old index name.\n\nSubsequently, if I rollover in a new minute, and do not specify a new index name, I will see this output:\n{\n  \"old_index\": \"test-2018.05.19.22.02-000002\",\n  \"new_index\": \"test-2018.05.19.22.04-000003\",\n  \"rolled_over\": true,\n  \"dry_run\": false,\n  \"acknowledged\": true,\n  \"shards_acknowledged\": true,\n  \"conditions\": {\n    \"[max_age: 1s]\": true\n  }\n}\nElasticsearch automatically preserves the naming pattern (test-2018.05.19.22.02 moves to test-2018.05.19.22.04) and just appends digits to the end.  No more naming magic required from you, which is needed if you will have more than 1 index per date period you have in your index date-math name.\nIn my mind, it's so much simpler to just have numbered rollovers, e.g. test-000001 and let Curator do the date math with the period and/or age filters using the field_stats calculated ages.  Then it just doesn't matter when it rolls over.  Even at 2 rollovers per hour, my rough calculation would be 5.7 years worth of digits in a 1 to 99999 count.\n. As this issue is not actually a Curator problem, I'm marking it closed.  Please feel free to comment here, if you have further questions.. Everything about Curator's Rollover API call is really just a simple pass-through to the Elasticsearch Rollover API.  Elasticsearch even does the dry-run for you, so Curator just passes the dry run flag directly to the Rollover API call and logs the actual Elasticsearch response.. Travis is having issues:\nE: Failed to fetch http://mirror.jmu.edu/pub/ubuntu/dists/trusty-updates/universe/i18n/Translation-en  Writing more data than expected (243993 > 243863)\nE: Some index files failed to download. They have been ignored, or old ones used instead.\nThe command \"sudo apt-get update && sudo apt-get install oracle-java8-installer\" exited with 100.\nA sample of the actual tests shows they're passing.. I'd like to know more about how this happened.  Curator shouldn't experience this as there is code that tries to prevent chunks bigger than roughly 3K from being sent, because of this 4K limit.  This code has been in place for a lot longer than Curator version 5.4.1 (the version you are using).\nHow large are your index names?  Is there more log data than what you've provided?. Let's start with these questions:\nHow large are your index names? Can you give me a sample of what they look like?\nIs there more log data than what you've provided?  You only included the error. Was there more?\nWhat is the full text of your action YAML file? I'd like to see how your indices were being selected and filtered.. I ask, because I can't replicate this error you encountered since I initially patched it in #296 way back in March, 2015.  I have tests that ensure the index list gets chunked that run for each pull request, and each release.  \nI just want help to find out what happened.  I don't care that you're not a developer.  I can do the leg work.. @breml This would be a problem. I did not anticipate anyone restoring such a large number of indices at once, I guess. \nAt issue is that the verification step is trying to ensure all indices have been restored, but if that index list is too long, it will not get a good response. I may instead have to check for all indices out of a _cat/indices call, and verify them that way, rather than pass each index in a CSV list. \n@breml, this should be a separate issue, as yours is specific to restore. Can you open a new issue, please?. Since you didn't encapsulate your file data with triple back ticks, e.g.\n```\nPASTE HERE\n```\n\nI can't tell, but that error only happens when your YAML file is improperly indented.. \nIs the index vmc-2018-05-26 empty?  Even if it is, it should pass.  You say it goes further, but the way you've written it seems to indicate something else may be wrong.  While it's true that if not stats: return should work, you're causing other issues with that because it should always return with stats for even an empty index with the _get_indices_stats call:\ndef _get_indices_stats(self, data):\n        return self.client.indices.stats(index=utils.to_csv(data), metric='store,docs')\nHere is the block which generated your exception in context:\n```\n            for l in index_lists:\n                stats_result = {}\n            try:\n                stats_result.update(self._get_indices_stats(l))\n            except TransportError as err:\n                if err.status_code == 413:\n                    self.loggit.debug('Huge Payload 413 Error - Trying to get information with multiple requests')\n                    stats_result = {}\n                    stats_result.update(self._bulk_queries(l, self._get_indices_stats))\n\n            iterate_over_stats(stats_result)\n\n``\nFrom what I can tell, you may have triggered atry/exceptexception by getting aTransportErrorother than a413, and that would preventiterate_over_statsfrom having a populated dictionary.  The way to determine if this is happening would be to setblacklist: []in yourloggingblock in your configuration yaml file (not your action yaml file).  The default behavior is to suppress the very verboseelasticsearchandurllib3` messages.  By passing an empty blacklist, you will prevent that from happening, and we should see what message is being passed.\nI would be happy to code an additional exception here for other response code errors, which seems a likely response here, but I'll need to know what you're seeing.  A catch-all exception after that might not be a bad idea too, but I'd still like to see what is happening with your case first.. Also, why are you building from github instead of just pip install elasticsearch-curator?. > Somehow I made strange decision that this should be 'curator' (not elasticsearch-curator).\nYeah, there was already a curator project on PyPi, so I made the name explicitly elasticsearch-oriented.. I will change the name of this to reflect the actual change I will make here, and catch other exceptions separate from the 413.. Not yet.  This is an extremely rare condition, only likely to happen under an overloaded cluster.   You may be compelled to do a lot of manual work to fix things when that happens anyway.\nDo you know what error code you're encountering?. You'll have to set your logging up to catch more information:\nlogging:\n  loglevel: DEBUG\n  # ...\n  blacklist: []\nSince you already have debug logging enabled, you'll have to set the blacklist to be an empty array.  This will show what HTTP errors are being generated.. @JavaCS3 \nThere isn't much I can do for you.  Your license expired, so the credentials are invalid.  You'll have to disable xpack security and restart your cluster, or update to a valid license.. These are already fixed in the 5.5.4 release.. Actually, they were fixed in the 5.5.3 release first.. I'm working on a huge change to the API for 6.0 and it's taken a long time to put it in.  I don't want anyone pushing to master until that's in.  I've just been instructing people to patch against the 5.x branch.. I just back ported everything from 5.x into master in https://github.com/elastic/curator/commit/736f6fde34cc4989b93ba553c930983848d90883. I would be happy to answer this question at https://discuss.elastic.co/c/elasticsearch, as this is definitely a usage question, and not a bug.  The reason I ask that it be asked at the discussion forums is that the answer will be available to a broader audience than a github issue, especially one which will be closed and hidden.. Make it 3.6.5 and I might merge the changes.  We're not shipping an official image at this time, however.. Also, master is behind the 5.x branch.  You should commit against that branch.. Mostly from a cleanliness standpoint.  It's a scripted rollover, something that will happen (or so we'd expect) repeatedly at intervals.  Why would you convert a mapping into YAML and embed that into a curator action file when you could just use an index template?  I understand why you might want to change a mapping as a one-off rollover, but that's not where Curator is likely to be used.  You really should be putting mappings into an index template instead of in here.\nCould I allow mappings? Sure.  If I do, I will not document the feature, because I don't want to have to support troubleshooting someone's JSON->YAML mapping and explain to them why it didn't work.  It's just not a good thing to be doing with Curator.. The same is mostly true for the create_index action, but it's not commonly used.. If rollover is not instantaneous, your cluster might be under a bit of strain.  There are, of course, a few moments\u2014usually milliseconds, but sometimes seconds in a busy cluster\u2014when a new index is provisioned in the cluster, and the shards are initialized. While that is happening, and the alias is pointing to the as-yet not-completely provisioned index/shards, Logstash will, of course, not be able to push documents yet.\n\nwhen a rollover occurs, the index is created but until a document is indexed, the index doesn't have it's mapping applied from logstash's output.\n\nThis is a misunderstanding of how Elasticsearch and Logstash work.\nWith index templates, the index is created with the required mapping.  There is no delay where documents are indexed improperly because a mapping template has not been applied yet.  To be clear: Logstash does not apply a mapping to indexes. Ever.  Logstash does not create an index, ever. Logstash is a streaming operator.  It sends bulk operations to Elasticsearch.  These operations are essentially \"Index event A into index named blah_blah\".  If the target index does not exist when one of these bulk requests goes in, Elasticsearch creates the index, not Logstash.  If no mapping is applied to the index, Elasticsearch will guess the mapping (dynamic mapping is on by default).    Logstash does upload a default index template which only applies to indices named logstash-*, but this is a very generic mapping. \n. You are likely getting hung up in the post check, which is performing:\nindex_exists = self.client.indices.exists(index=index_name)\nHowever, when I try this in my own environment, I can see that this call works as expected:\n```\n$ python3\nPython 3.6.5 (default, May  6 2018, 09:00:36)\n[GCC 4.2.1 Compatible Apple LLVM 9.1.0 (clang-902.0.39.1)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport elasticsearch\nindex_name = '<.watcher-history-7-{now/d}>'\nclient = elasticsearch.Elasticsearch(host='REDACTED', use_ssl=True, ca_certs='/path/to/ca.crt', http_auth=('REDACTED','REDACTED'))\nclient.indices.exists(index=index_name)\nTrue\n``\nIs there a chance that something else is going on?  If you set up alogger`, and set it to DEBUG, you should be able to see what else is going on behind the scenes.. The only other supposition I can come up with is that you're testing this first with an empty, or near-empty index, and it finishes so fast that the index really doesn't exist yet, because the cluster metadata hasn't propagated fully when the check happens.\n\n\n\nTypically this shouldn't happen, as the wait period should be at least the default 9 seconds, but it's the best I can imagine without more debug data.. You have a 10 day wait period defined.  This is quite large.  Do you really expect a reindex of your data to take up to 10 days?  If you are on Elasticsearch 5 or higher, you should be able to use the slices option to speed things up.. What version of Curator are you using this against?\n\nfor self.loggit.error, after it happens, who will take over it?\n\nThis just writes the error before it raises an exception which is caught here.\nWithout more debug logging\u2014specifically getting the elasticsearch and urllib3 logs, I won't be able to help you much more than this.\nThe use case you are describing is fully tested in the Curator tests, which is reindexing with date_math names.. I just (temporarily) added an exit code check to the above test and ran it again, just to be sure I was catching an exception properly and not just checking to see if the reindex result was correct.  I did not get an exception, and the tests succeed.. I will have to see if the Tasks API reports it as cancelled.  If so, this should be fairly straightforward.  If the Tasks API does not, then there may not be a lot I can do.  Cancelling the task appears to mark it as complete, which is what it's looking for in the background.  Curator takes the additional step of checking for the target index to exist.  It can't tell how many documents it should have, because what if you're reindexing a queried subset of an index?\nThoughts? Ideas?. I just spoke with the Elasticsearch developers. Once a task is cancelled, it's no longer in the Tasks API at all.  It's gone, and there's no audit trail.  There is no way\u2014using Tasks API calls\u2014to differentiate between a complete or cancelled Reindex.\nThis means that while I agree that the behavior you've described is undesirable, there's no way around it right now.. After further discussion with the Elasticsearch developers, I submitted a feature request to hopefully address this.. Should be able to get this: \n```\nGET /.tasks/task/dW_Lc0UcT92Z64bGwzU8jQ:565\n{\n  \"_index\" : \".tasks\",\n  \"_type\" : \"task\",\n  \"_id\" : \"dW_Lc0UcT92Z64bGwzU8jQ:565\",\n  \"_version\" : 1,\n  \"found\" : true,\n  \"_source\" : {\n    \"completed\" : true,\n    \"task\" : {\n      \"node\" : \"dW_Lc0UcT92Z64bGwzU8jQ\",\n      \"id\" : 565,\n      \"type\" : \"transport\",\n      \"action\" : \"indices:data/write/reindex\",\n      \"status\" : {\n        \"total\" : 87,\n        \"updated\" : 0,\n        \"created\" : 51,\n        ...\n      },\n      \"description\" : \"reindex from [docs] to [reindex-docs]\",\n      \"start_time_in_millis\" : 1529482732581,\n      \"running_time_in_nanos\" : 60113248976,\n      \"cancellable\" : true,\n      \"headers\" : { }\n    },\n    \"response\" : {\n      \"took\" : 50841,\n      \"timed_out\" : false,\n      \"total\" : 87,\n      \"updated\" : 0,\n      ...\n      \"canceled\" : \"by user request\",\n      \"throttled_until_millis\" : 0,\n      \"failures\" : [ ]\n    }\n  }\n}\nJust read for a reason in `canceled`.. Using the exact example above, the soon-to-be-updated code will yield:\n$ python run_curator.py --config badyaml.yml action.yml\nUnable to read/parse YAML file: badyaml.yml\nwhile scanning a simple key\n  in \"\", line 5, column 3:\n      certificate:'/etc/elasticsearch/ ...\n      ^\ncould not find expected ':'\n  in \"\", line 6, column 1:\n^\n\n``\nand immediately terminate with an exit code of 1. This should actually be easy now that I have created a hacky work-around for date math in other places.  This is a good idea.. Unfortunately,es_repo_mgr` is pretty far out of date with regards to some of the other bits.  I will try to rework this for the 6.0 release.. You must have installed the Debian Jessie package.  There is a Debian-stretch package:\ndeb [arch=amd64] https://packages.elastic.co/curator/5/debian9 stable main\n\nPlease see https://www.elastic.co/guide/en/elasticsearch/client/curator/current/apt-repository.html#_repository_configuration for more information.. Does it run if you run it from /opt/elasticsearch-curator/curator or /opt/elasticsearch-curator/curator_cli?\nI have the strangest issues with RHEL/CentOS 7 and this is not the first time this particular behavior has manifested.. And you're 100% certain you downloaded the v7 RPM and not the v6 one?. I can tell you that I cannot replicate this with my CentOS 7 test bed:\n```\n[buh@centos7-pkg-test ~]$ cat /etc/redhat-release\nCentOS Linux release 7.2.1511 (Core)\n[buh@centos7-pkg-test ~]$ uname -a\nLinux centos7-pkg-test.untergeek.net 4.15.17-3-pve #1 SMP PVE 4.15.17-12 (Fri, 08 Jun 2018 11:18:32 +0200) x86_64 x86_64 x86_64 GNU/Linux\n[buh@centos7-pkg-test ~]$ wget https://packages.elastic.co/curator/5/centos/7/Packages/elasticsearch-curator-5.5.4-1.x86_64.rpm\n--2018-07-10 15:54:18--  https://packages.elastic.co/curator/5/centos/7/Packages/elasticsearch-curator-5.5.4-1.x86_64.rpm\nResolving packages.elastic.co (packages.elastic.co)... 107.21.237.188, 54.235.82.130, 184.72.242.47, ...\nConnecting to packages.elastic.co (packages.elastic.co)|107.21.237.188|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 14959417 (14M) [application/x-redhat-package-manager]\nSaving to: \u2018elasticsearch-curator-5.5.4-1.x86_64.rpm\u2019\n100%[=====================================================================================================================================================================================================================>] 14,959,417  11.6MB/s   in 1.2s\n2018-07-10 15:54:19 (11.6 MB/s) - \u2018elasticsearch-curator-5.5.4-1.x86_64.rpm\u2019 saved [14959417/14959417]\n[buh@centos7-pkg-test ~]$ sudo yum install elasticsearch-curator-5.5.4-1.x86_64.rpm\nLoaded plugins: fastestmirror\nRepodata is over 2 weeks old. Install yum-cron? Or run: yum makecache fast\nExamining elasticsearch-curator-5.5.4-1.x86_64.rpm: elasticsearch-curator-5.5.4-1.x86_64\nMarking elasticsearch-curator-5.5.4-1.x86_64.rpm to be installed\nResolving Dependencies\n--> Running transaction check\n---> Package elasticsearch-curator.x86_64 0:5.5.4-1 will be installed\n--> Finished Dependency Resolution\nDependencies Resolved\n===============================================================================================================================================================================================================================================================\n Package                                                          Arch                                              Version                                             Repository                                                                        Size\n===============================================================================================================================================================================================================================================================\nInstalling:\n elasticsearch-curator                                            x86_64                                            5.5.4-1                                             /elasticsearch-curator-5.5.4-1.x86_64                                             54 M\nTransaction Summary\nInstall  1 Package\nTotal size: 54 M\nInstalled size: 54 M\nIs this ok [y/d/N]: y\nDownloading packages:\nRunning transaction check\nRunning transaction test\nTransaction test succeeded\nRunning transaction\n  Installing : elasticsearch-curator-5.5.4-1.x86_64                                                                                                                                                                                                        1/1\n  Verifying  : elasticsearch-curator-5.5.4-1.x86_64                                                                                                                                                                                                        1/1\nInstalled:\n  elasticsearch-curator.x86_64 0:5.5.4-1\nComplete!\n[buh@centos7-pkg-test ~]$ curator --version\ncurator, version 5.5.4\n``. Do you have anything else on the box? Is it in any way a non-standard installation? (security add-ons, selinux, etc?). Version and patch level information? i.e.cat /etc/redhat-releaseanduname -aoutput, please.. If you're going that route, @anthonyloukinas, you might as well use the [curatorAPI](http://curator.readthedocs.io/en/latest/) to achieve the same.. I have not yet been able to reproduce this on either CentOS or Debian (or Ubuntu).. And it also apparently hasn't been updated to reflect that it runs with any 5.x or 6.x Elasticsearch.  It's the underlying API that Curator uses, so if Curator does it, that's how.. @anthonyloukinas if you've installedelasticsearch-curatorviapip`, you have everything.  You actually don't need the RPM.\nTry it: curator --version or curator_cli --version.  The entry_points were installed somewhere when you installed via pip.. I highly recommend using the latest 3.6 branch of Python.  I haven't tested it with 3.7 yet. I'm happy to re-open it.  The easiest workaround right now is to install via pip, rather than RPM or DEB.  I will have to do some docker-level debugging to find out why it doesn't work, and I'm a bit busy to delve deep super soon.. I'm reasonably certain that a pip install will work without problems.  It will function just like an RPM installation, though the installed paths may be a bit different.  The entry_points that get installed will make it function just the same.  If this approach is taken, I do recommend using Python 3.6, rather than 2.7.  3.5 is also acceptable.. > I needed to export LC_ALL=C.UTF-8 and LANG=C.UTF-8\nThat is true, regardless.  The libraries expect UTC, as does Elasticsearch.. Thank you, @ceeeekay.  That may actually be the solution for several others.  I am unsure why no error is generated by docker images.  I normally see an error when the wrong LC is set.\nI will do some digging and see what else I can find.. @sferry Your issue seems to be unrelated to the parent issue.  Yes, you are not seeing output, but if you turn on debug logging, you are seeing a ConnectionRefusedError, which indicates that Curator can't connect to the target machine.\nYour system python version is irrelevant here, as the RPM version bundles its own (File \"/home/buh/.local/lib/python3.6/site-packages/urllib3/connection.py\").\nSo, while I appreciate that you're having a bad experience, I do not think it's related to this issue.  Since it's a usage question at this point, please ask for some help in the discussion forums at https://discuss.elastic.co and I'll be happy to answer there.. The fact that it stops at:\nget_client:880 Not using \"requests_aws4auth\" python module to connect.\nmeans that Curator is not able to create a client connection to Elasticsearch.  You will probably be able to see why by adding an empty blacklist to the logging block in the client configuration file (don't remove whatever else you have there, just add the blacklist, like this):\nlogging:\n  blacklist: []\nBy default, Curator does not display the log files from the elasticsearch-py and urllib3 python modules, as they are extremely chatty.  Setting the blacklist to an empty array will expose them again.. http_auth: 'user:pass' is what you add to the client config portion, then.. The --ignore_empty_list option for curator_cli doesn't prevent the error message.  It hasn't been fully implemented yet (in curator_cli), because its whole reason for being was to allow Curator to proceed to the next action if it was only an empty list condition that caused the error.  \nWith curator_cli, there is no next action, as they are singletons.  You are free to script another action after this without regard to the output of an error.\nWhen I eventually implement this fully, it will still not necessarily prevent the error message. It will simply guarantee that it exits with a zero exit code.. Closing as there is really nothing to be done here.. I'm thinking that you will need to establish AWS credentials in order to use curator or curator_cli with AWS ES.  Please see the documentation for more details around these options, specifically aws_sign_request.. I have no way of testing this. As such, it sits with the tags it has.. Indeed.  The YAML would parse that properly, so I was initially surprised by the issue.\nThere are no simple or reliable ways to pass lists from shell variables.  This is a known limitation.  If someone can find a clever way of hacking around this, I'm game to merge it, but it's not something I'm pressed to fix urgently.. Duplicate of #1237 . Can you please elaborate a bit more on what you're seeing?  I don't have as much time this week to dedicate to this, so a slightly more in-depth description will be helpful.. > Now I realize I should be able to work around that by first filtering with a pattern filter. That way, the count filter would not reject any index based on the regex, and the exclude parameter should not be overwritten.\nBingo! \nI was pretty sure this is what you meant, and it looks like you found the way to use the pattern filter to only catch the ones you want (which is exactly what I would have recommended).  \nI'm sorry it wasn't as simple as using an advanced regex to do both in one step\u2014which does make a lot of sense.  Unfortunately, Curator isn't quite smart enough to do that, since the indices don't come from the Elasticsearch API sorted or formatted.  I'm kind of forced to take a more na\u00efve approach, and not presuppose what the end user wants to do, hence the need for multiple filters.\nIf you're good to go, feel free to close this.  If you need anything else, please let me know.. Also, the pattern feature here was to fulfill a slightly different purpose.  See https://github.com/elastic/curator/issues/1046#issue-254002362 and #1044 for more information.  The idea was that you be able to do counts across several matching patterns and keep n indices for each of those patterns.. Curator manages indices at the index level.  What you are asking is not possible with Curator.  You should look into using Elasticsearch's delete_by_query API.. This is not a problem:\n2018-08-07 17:43:00,787 INFO      Preparing Action ID: 1, \"delete_indices\"\n   2018-08-07 17:43:00,807 INFO      Trying Action ID: 1, \"delete_indices\": Clean up ES by deleting \n   logstash indices\n   2018-08-07 17:43:01,009 INFO      Skipping action \"delete_indices\" due to empty list: <class \n   'curator.exceptions.NoIndices'>\nThis is telling you that there were no indices matching your patterns, resulting in an empty list, so nothing was deleted.\nThe second error says that action 2 (which was not included here) was not performed due to disable_action being set to True, which is also not a problem.\nIt sounds more like you have no indices, or something in your pattern matching is wrong.  What do you see if you enable loglevel: DEBUG in the logging block of the client yaml file?  You'll see all of the indices being evaluated with this set, which will be helpful for troubleshooting.. > \ntimestring: '%Y.%m.%d.%H'\nDoes not match\nlogstash-2018.08.07\nThis is why you get an empty list.\nTry dropping the .%H off, and change the units:\ntimestring: '%Y.%m.%d'\nunits: days\nThis will still not match logstash-2018.08.07, but it will match logstash-2018.08.06 and older.. It is AND.. Also, the space filter is the sum total of primaries and all replicas.. When the space filter calculates disk usage per index, it uses the sum of all primaries and replicas for that index.  It is the store.size, rather than just the primaries.. Effectively, yes.  It's easy to see this in a --dry-run.  It shows the size of the indices, and the sum, but won't delete.. You are welcome.. I'm happy to help troubleshoot this, but I'm fairly certain it's not Curator to blame.\nTroubleshooting Steps\nThe fastest way to find an anomaly is to look at the creation_date.  For the following examples, substitute one of your suspect index names for INDEXNAME.\nIn Kibana in the Dev Tools console, or using curl or similar:\nGET INDEXNAME/_settings\nThe result will show you:\n{\n  \"INDEXNAME\": {\n    \"settings\": {\n      \"index\": {\n        \"codec\": \"best_compression\",\n        \"mapping\": {\n          \"total_fields\": {\n            \"limit\": \"10000\"\n          }\n        },\n        \"refresh_interval\": \"5s\",\n        \"number_of_shards\": \"1\",\n        \"provided_name\": \"INDEXNAME\",\n        \"creation_date\": \"1533630602162\",\n        \"number_of_replicas\": \"1\",\n        \"uuid\": \"ZxdRcRRcRwqX4GXnucyNww\",\n        \"version\": {\n          \"created\": \"6030199\"\n        }\n      }\n    }\n  }\n}\nThe important line is \"creation_date\": \"1533630602162\"\nHead on over to https://www.epochconverter.com/ and paste just the numeric value into the appropriate field, then click on \"Timestamp to Human Date\", and voila! You see the actual timestamp that the index was created.  I'm guessing that it isn't the same day as the the index, which is what you would have expected.  There are probably also very few documents in that index, too.  You can see exactly how many with:\nGET _cat/indices/INDEXNAME?v\nThis yields a report like this:\nhealth status index             uuid                   pri rep docs.count docs.deleted store.size pri.store.size\ngreen  open   INDEXNAME         ZxdRcRRcRwqX4GXnucyNww   1   1    1346744            0      1.6gb            1gb\nAnd finally, you can see some of what's inside by running:\nPOST INDEXNAME/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  }\n}\nThis will show you 10 documents from the index in the hits portion of the output (there may or may not even be 10).  You should see whatever is coming from Logstash at this point.\nConclusion\nThe thing to remember is that Logstash is a stream processor.  It doesn't actually create indices in Elasticsearch.  It sends a bulk request that says, \"please index this document/logline into the index named logstash-YYYY.MM.dd\", and that date is derived from whatever the @timestamp field has for said document/logline.  Elasticsearch will automatically create the index if it does not exist.  Curator is, indeed, deleting these indices.  My guess is that you have some old files/data being re-read into Logstash, and the indices are being re-created.\nOf course, there's always the possibility that it's something else. This is my first best guess, as I've seen it on many occasions.\n. I'd be happy to merge this in if someone were to add it.  Unfortunately, I have no way to do this, or to test it, so it's unlikely to come from me.. In a Curator context, ignore_unavailable is just to ignore missing shards, or closed indices.  Why?\nBecause Curator shouldn't be able to detect an index that isn't there.  At action execution time, Curator queries the Elasticsearch API and pulls the current list of indices.  If Curator is detecting an index that is not there when the action is actually taking place\u2014which is a rare prospect, since it's a matter of moments between gathering the full index list, filtering it with the provided filters, and then performing the action\u2014do you have some other process deleting indices?   Or is your cluster heavily taxed, and it's taking the master nodes a long time to register the index has been deleted? I see that this is action ID 402.  What actions precede this.\n(Edited for clarity).. No actionable items in list: <class 'curator.exceptions.NoIndices'>\"\nPerhaps what you're looking for is the ignore_empty_list option? So that Curator will continue in the event that there are no matching indices?. Unfortunately, this cannot be easily coded around in any way I have been able to imagine.  The closest I have been able to come has been with the addition of the _tasks API in the most recent releases of Elasticsearch, but it amounts to a gamble and a guess.\nThe problem is that when you initiate a forceMerge in Elasticsearch, the base Lucene command offers no way to track the results, other than to count the number of segments per shard, which has proven an unreliable way to track status, especially if you accidentally have more data come in.  The only reliable way Elasticsearch allows you to tell if a forceMerge is done is to keep the client connection open until it has returned control to the client.  Should that client connection close while you are waiting\u2014like with your Lambda\u2014there is essentially no way to reconnect to that process without it being that guess and a gamble.  ForceMerges don\u2019t even allow API-level blocking responses.  If one has been initiated and is in progress, you can still initiate another, but it will be opaquely blocked and put on hold until the current one has completed\u2014and Elasticsearch won\u2019t tell you that this is the case.  \nThe forceMerge API is very ripe for changes that correct these issues, but I don\u2019t know if/when they will ever happen.  As stated, you can see that segments are being forceMerged in the _tasks API now, but there\u2019s no way to track the index they came from, which client process initiated them, or anything else that the _tasks API normally allows you to do.  I have been very hopeful that the _tasks API would allow me to do overcome these shortcomings, but in its current state, it\u2019s no more than a gamble that even using the _tasks API would work as desired, since it would require (at the very least):\n\nDumping all tasks from all nodes\nCombing through them to find forceMerges that may or may not have been initiated by me\nKeep doing 1 and 2 until there are no more forceMerges \n\nThe huge problem is that if I query the _tasks API and there has been a slight pause in between new tasks, such that I see no forceMerges, but a few (or several) seconds later, the next shard in the index starts a forceMerge (this is a very real possibility, looking at the monitoring data. The replica shards are sometimes the last to do the forceMerge, and can have delays in between), Curator may mistakenly think that the forceMerge is done when it is not.  It would be no problem, if a real parent task were created, complete with a task ID. But a task ID is not generated by Elasticsearch for a forceMerge, which makes this a guess and a gamble.\nSince you\u2019re already using a Lambda, and you will be using the API for this, rather than Curator directly, your best bet will be to code around this as best you can by combining the tasks API with the expected max_num_segments and hope that it\u2019s all done.  It is a lot of extra coding I wish you didn\u2019t have to do, but it\u2019s the only hope anyone currently has to overcome the API shortcomings I outlined, and those are still a guess and a gamble.  There are just no concrete solutions yet.. No.  There is no such thing.  As I tried to explain:\n\nThis call will block until the merge is complete. If the http connection is lost, the request will continue in the background, and any new requests will block until the previous force merge is complete.. Your best bet would be to spin up a second, parallel client with a relatively short timeout (before the default) and use that to execute the forceMerge, knowing that it will fail.  If you only have one index to forceMerge, this is your \"best effort.\"\n\nIt gets significantly more complicated, though, if you have several indices per day to forceMerge.  This is why I outlined the scenario above, and what you'd have to do to make sure it detected when an index had completed the forceMerge, so it could schedule the next.\nI emphatically do not recommend trying to forceMerge more than one index at a time.  Yes, the API allows you to pass multiple indices to a forceMerge command.  The I/O load on the cluster could easily become crippling to ongoing indexing, merging, and querying operations.. > Do you know if there's an issue that I or others can track for forcemerge improvements in Elasticsearch?\nI am not aware.  You should visit https://github.com/elastic/elasticsearch to check their issues for an existing one.  If one isn't there, you could add one.. > Is this a way to go, or do you have a better idea to solve the Proxy Problem?\nMaybe, but I won\u2019t be merging client connection code changes in this repository. I will be using \nhttps://github.com/untergeek/es_client here in the next major release, and that\u2019s where a proxy solution would have to go to get in.  \nIt\u2019s more involved than just changing the connection class to use Requests instead of urllib3, too, as that\u2019s still the safer/saner default, where possible (at least in my mind).  Perhaps use Requests instead of the default when a proxy is requested, rather than to wholly replace everything as it currently exists.  A considerable amount of testing would be have to be added to outright replace urllib3 with Requests, but perhaps it would work just fine.. So yes, to recap, I did completely spin out the client code into its own class and project at \nhttps://github.com/untergeek/es_client and plan on using that for Curator and other projects in the future.. The 6.0 release. of Curator.  As to when date-wise, I do not know, yet. My coding time is somewhat curtailed now, as a Consultant, instead of a full-time developer, like I was.. Thank you for the detailed steps to reproduce and output.  I agree this is an undesirable response.  It shouldn't be too hard to look for the existence of a \"failure\", and its status code, and raise an exception accordingly.. I just _barely_ got enough time to pay attention to Curator again.  I will see what I can do before the next release.. You have version 4.2.5, which is not compatible with Elasticsearch 6. You need Curator version 5.5.4. . Did you mean thatssl_no_validate: True` works as a workaround?\nurllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:833)\nThis is not Curator.  This is the upstream urllib3 library that can't verify the certificate.  I do not know why this is, but it's trying, and failing.. > I'm not sure if curator/urllib3 is seeing the certificate file I'm passing in, at least that's not what I can readily tell in the debug output. Is there anyway to tell?\nHave you tried moving it to another location in your file path (I know you said the user had access)? I'd double-check that, just in case.  Also, is this running in cron? Or have you tried running it as root or sudo or anything else? I ask because I've seen cron environments alter things.. What happens if you run:\ncurl --cacert /etc/ssl/certs/sub_ca.pem https://m1.example.com:9200\nAlso, noting the name says sub_ca, are there multiple CAs associated? i.e. is sub_ca an intermediate certificate signed by a top-level CA?. Sorry for the confusion.  It does seem to me that urllib3 expects all parts of a CA to be present to validate the chain.  I will mark this as closed.. Yes.  I just restarted the two failed builds.. Thanks for submitting a pull request.  I appreciate the thought that went into this.  However, the word thereafter is proper english in this use case and does not require correction.  So, I will not be merging this change.\nIf you find other issues with the code or documentation, please do not hesitate to submit changes for review.  Thanks again.. You are correct, that these big strings of dot-separated numbers are problematic.  However, these indices are always created by the *beat, which means they are created according to UTC time, which means that you will be far better served by not using the timestring at all, and use source: creation_date instead of source: name for your age filters.\nsource: name is throwback to before the era of *beats, when Logstash was the only game in town, and created logstash-YYYY.MM.dd indices.  This pattern still exists for when source: creation_date is insufficient.  On top of this, we encourage the use of rollover indices instead of time-based, which further removes you from indices with a timestamp in the name.  In these cases source: creation_date or better yet, source: field_stats for time range selection is a preferable choice.\nCurator also has the period filter, which would accommodate your month-range example.\nWhile I concur that using source: name can be problematic with complex index names, these other options are available to you which are superior, and obviate the need to add complicated regex anchoring to the Curator syntax.. If it takes hours for even a few thousand individual field_stats queries to come back, that's a problem.  It should not be taking that long.\nSee https://github.com/elastic/curator/blob/v5.5.4/curator/indexlist.py#L307-L329\nfor l in index_lists:\n            for index in l:\n                body = {\n                    'aggs' : {\n                        'min' : { 'min' : { 'field' : field } },\n                        'max' : { 'max' : { 'field' : field } }\n                    }\n                }\n                response = self.client.search(index=index, size=0, body=body)\n                self.loggit.debug('RESPONSE: {0}'.format(response))\n                if response:\n                    try:\n                        r = response['aggregations']\n                        self.loggit.debug('r: {0}'.format(r))\n                        s = self.index_info[index]['age']\n                        s['min_value'] = utils.fix_epoch(r['min']['value'])\n                        s['max_value'] = utils.fix_epoch(r['max']['value'])\n                        self.loggit.debug('s: {0}'.format(s))\n                    except KeyError:\n                        raise exceptions.ActionError(\n                            'Field \"{0}\" not found in index '\n                            '\"{1}\"'.format(field, index)\n                            )\nIt loops over all indices in the current working list and does a small aggregation query to find the min and max.  This should not take that long.  You're telling me that it's taking hours to execute this across thousands of indices? Why are you not pre-filtering these indices with pattern filters to reduce the overhead?\nI, personally, will not add this requested feature for the following reasons:\n This is a limited scope use case that few users will need, irrespective of how pressing your need may be\n Curator is about to be superseded by Index Lifecycle Management, though that will require at least a Basic (free) license.  As such, Curator is in more of a maintenance mode, with patches, and limited new functionality being added. Curator will continue to exist for edge cases that ILM does not or will not cover.\nIf someone else wants to fork the code and add this feature, and add tests, and it passes code review, I will likely merge it.. I can't comment on release dates, unfortunately.. > Everyone who is using timestring needs it. The current behaviour is not a problem until it will become ... This is the type of issue when everything is OK, everything is OK, ... WTF! why my indexes disappears, where are my data ....\nYou're the first user in my 5 years at Elastic who has tried to make this sound like every user will be affected.  A few have been affected, but all of them have been okay with switching to creation_date or field_stats.  You would be the first user who\u2014for reasons already described\u2014cannot.  So why not switch the date separator character so it's always uniform, but differ significantly from the other index name bits? Why not use multiple characters? %Y.%m.%d isn't mandatory. It could just as easily be %Y_._%m_._%d or any other permutation you can imagine.  There are many easier ways to solve this that don't require a huge refactoring of Curator's code.. It would only be able to anchor the beginning or end of the line, but I don't see why it wouldn't work.. I'm sorry, I just don't see this as a necessary addition to this page.  The breadcrumb at the top refers to that page, and the sidebar does too.. The exit codes refer to both the curator and curator_cli binaries.  It shouldn't need more than that, really.. This won't build the docs.  I can't merge it like this.. ```\nBuilding HTML from index.asciidoc\nError executing: a2x -v --icons -d book -f chunked -a showcomments=1 -a lang=en -a base_edit_url= -a root_dir=/path/to/curator/docs/asciidoc --xsl-file resources/website_chunked.xsl --asciidoc-opts -fresources/es-asciidoc.conf --destination-dir=/path/to/curator/docs/asciidoc/html_docs --xsltproc-opts --stringparam toc.max.depth '5' --xsltproc-opts --stringparam toc.section.depth '0' --xsltproc-opts --stringparam chunk.section.depth '0' --xsltproc-opts --stringparam local.book.version 'test build' --xsltproc-opts --stringparam local.book.multi_version '0' --xsltproc-opts --stringparam local.page.header '' --xsltproc-opts --stringparam local.book.section.title 'Learn/Docs/' --xsltproc-opts --stringparam local.book.subject '' --xsltproc-opts --stringparam local.noindex '' /path/to/curator/docs/asciidoc/index.asciidoc\n/path/to/curator/docs/asciidoc/html_docs/index.xml:3263: element xref: validity error : Syntax of value for attribute linkend of xref is not valid\nshots see <xref linkend=\"command-line.asciidoc#singleton-command-line-interface\"\n                                                                               ^\n/path/to/curator/docs/asciidoc/html_docs/index.xml:3263: element xref: validity error : IDREF attribute linkend references an unknown ID \"command-line.asciidoc#singleton-command-line-interface\"\n\na2x: ERROR: \"xmllint\" --nonet --noout --valid \"/path/to/curator/docs/asciidoc/html_docs/index.xml\" returned non-zero exit status 4\n```. I hate to be the bearer of bad news, but Curator can't be blamed for this.  Curator makes API calls.  It's really nothing more than an index selection wrapper around the standard Elasticsearch APIs.  If you made the same calls via curl, or the Console in Kibana, the results would be no different.\nIt sounds like you need some help with your cluster in a general sense.\nPlease also understand that snapshots can be very hard on an Elasticsearch cluster if they contain too many indices/segments in a single run.  Elasticsearch is obliged to keep those segments in a frozen, unchanged state for the duration of time it takes to copy said segments to the repository.  In some cases, this can trigger other cluster issues.  I have no idea whether or not this is happening in your case, but the amount of time you are seeing a snapshot IN_PROGRESS is not a good sign.\nI regret that this repository is not the place to get help for this.  I would turn to https://discuss.elastic.co for that.. So what you're saying is that this code block isn't working as expected?. Sorry for the long delay in response. It's been a very busy last quarter for me. I'm redoubling my efforts on Curator to catch up on my issues. \nTL;DR\nDon't put the rollover alias name in the index template.\nLonger, full explanation with examples\nYour error yields a nugget, I just had to scroll to the right a bunch to see it:\nfound duplicated alias [[request-id]] in index template [mysql-slow-session]\nWhat this means is that you are assigning request-id in the index template. This is not required with the Rollover API, and is the cause of the error message.\nI replicated this behavior by putting this in a template called test:\nPUT _template/test\n{\n    \"order\": 0,\n    \"index_patterns\": [\n      \"test-*\"\n    ],\n    \"settings\": {\n      \"index\": {\n        \"number_of_shards\": \"1\",\n        \"number_of_replicas\": \"0\"\n      }\n    },\n    \"mappings\": {},\n    \"aliases\": {\n        \"foobar\": {},\n        \"testalias\": {}\n    }\n}\nThen creating a new index that should be rollable, like index-000001, which would be assigned these two aliases:\n$ curl localhost:9200/_cat/aliases\nfoobar    test-000001 - - -\ntestalias test-000001 - - -\nAnd when you try to call a rollover and the new index has the rollover alias already defined in the template (as above)?\nPOST testalias/_rollover\n{\n    \"conditions\": {\n        \"max_age\": \"1s\"\n    }\n}\nGives this output:\n{\n    \"error\": {\n        \"root_cause\": [\n            {\n                \"type\": \"illegal_argument_exception\",\n                \"reason\": \"Rollover alias [testalias] can point to multiple indices, found duplicated alias [[testalias, foobar]] in index template [test]\"\n            }\n        ],\n        \"type\": \"illegal_argument_exception\",\n        \"reason\": \"Rollover alias [testalias] can point to multiple indices, found duplicated alias [[testalias, foobar]] in index template [test]\"\n    },\n    \"status\": 400\n}\nSo, the fix is to remove the rollover alias from the template, so it looks like this at the end:\n\"aliases\": {\n        \"foobar\": {}\n    }\nI reapply the template with this change, et voila,\n{\n    \"acknowledged\": true,\n    \"shards_acknowledged\": true,\n    \"old_index\": \"test-000001\",\n    \"new_index\": \"test-000002\",\n    \"rolled_over\": true,\n    \"dry_run\": false,\n    \"conditions\": {\n        \"[max_age: 1s]\": true\n    }\n}\nAnd here are my aliases:\n$ curl localhost:9200/_cat/aliases\nfoobar    test-000001 - - -\nfoobar    test-000002 - - -\ntestalias test-000002 - - -\nNote that foobar (my analog to your mysql-slow-session alias) still points to both indices, while testalias (our rollover alias name), points only to the rollover index.  If I run it again...\n$ curl localhost:9200/_cat/aliases\nfoobar    test-000001 - - -\nfoobar    test-000003 - - -\ntestalias test-000003 - - -\nfoobar    test-000002 - - -\nTo recap, the solution is to remove the rollover alias name from the template. If you had to do this over again, you would add the template first (with only the carry-over alias) and manually add the rollover alias at index creation time like:\nPUT index-000001\n{\n  \"aliases\": {\n    \"testalias\": {}\n  }\n}. It is redundant on purpose.  I don\u2019t expect all users to read through both sections (i.e. the top section and the use_age section), and I wanted that portion to be in each for that reason.. Adding this feature will fundamentally change current functionality in the API, so it can't be added until the next major version.. Not yet, sorry. I'm no longer a full time developer, so I don't get to dedicate as much time to this as I would like.. As explained earlier, the problem here is not the coding, which is relatively simple, but that the coding would require a breaking change to how the API works today. \nCurator follows semantic versioning, which means that API changes can only be introduced in new major versions. This isn't something I can merge until Curator 6.x. \nThe API for Curator 6.x is being worked on, and it's going to be somewhat different from the current API, so any changes you add might not be able to be merged anyway. It's a quandary, I know. But this is the other reason no progress has been made here.. The best I can offer is to use the Curator API in conjunction with the Elasticsearch API and script it yourself. There simply isn't a non-API breaking work-around available for this right now.. Will try to release a quick patch release for this as soon as I can.. The new version will be out within a week-ish.  In the meanwhile, just do the workaround:\npip install -U click==6.7. > Does curator work with elastic search 6.3 to delete the indices which are 30 days old without deleting the index.\nI presume you are asking if Curator will delete data from an index with this question. The answer is no. Curator works at the index level only. \nCurator\u2019s version compatibility is described here.. This is a duplicate of #1279 . Are you forced to use elasticsearch5? Because the v6 version works just fine with 5x versions of Elasticsearch (at least with all Curator functionality).. I understood that. I was referring to Elasticsearch-py. Elasticsearch-py version 6 works with Elasticsearch versions 5.x through 6.x for all Curator functions. I was asking why you can\u2019t use elasticsearch-py v6. I can. I\u2019d be happy if you submit the PR and I can just merge it. Otherwise it could be 2 weeks before I get to it, as I\u2019m on a business trip. . Due to #1279, the CI checks will not pass.  You can actually address both issues by changing click>=6.7 to click>=6.7,<7.0 in the same files.. Which I understand if you don't want to add.  It's a catch-22 of sorts: The PR should only address one change, but it can't pass until either someone else merges that change and you rebase your PR, or you add both changes.. Fantastic, thanks.. Question: What version of Elasticsearch are you using? Is it behind a proxy or load balancer of any kind?\nElasticsearch does have an out-of-the-box 4k URL size limit, but Curator knows how to compensate for this, and generally limits the URL size to around 3k bytes, preventing this from happening.  Every call Curator makes that might require this batched approach uses it.\nIt is possible\u2014but not very likely\u2014that you have found a way around this, but it would require a single index name that exceeds the difference between 4k and 3k bytes, which would be a rather long index name, indeed (but Elasticsearch doesn't allow more than 255 character index names). If you follow the above link and read the code, it adds indices and commas one by one until the URL crosses the 3k byte threshold. You might be able to grab the entire URL from the logs and do a byte count on the string and see just how long it actually is.\nIt is possible that this URL size limit has been decreased in the configuration, which would also defeat this limit.  Conversely, it is possible to increase the limit above the 4k limit in the Elasticsearch configuration.\nTo troubleshoot, it would be helpful to run Curator with DEBUG logging and the logging blacklist set to an empty array (I can't tell if you added both DEBUG logging and removed the blacklist):\nlogging:\n    loglevel: DEBUG\n    # ... any other settings here\n    blacklist: []\nSet this in your client configuration yaml file and run again.  There will be much more information, especially from the urllib3 library which actually is what captures the 400 error.  Hopefully there is much more information contained there to explain what is happening.. Unfortunately this is not a limitation of Curator, but of Elasticsearch:\n\nThis call will block until the merge is complete. If the http connection is lost, the request will continue in the background, and any new requests will block until the previous force merge is complete. . Now, Curator could be altered to permit parallel force merges, where 1 request submits multiple indices for force merge at once.  This is possible in the Elasticsearch API.  It's also extremely painful in terms of disk I/O for the nodes performing this task.  This is why Curator\u2014as a matter of protecting the users from hammering their cluster too much\u2014does the force merges serially.  All things considered, however, that approach is a can of worms.  You'd have to have dedicated \"force merge\" nodes in a shard-allocation routing setup, and ensure your shards are in there for that approach to work safely.  Most users aren't going to have that, so I'd be wary of exposing them to a flag that permitted it.. Yeah, that approach is not safe to add to Curator.  I don't even approve of that approach in general.. There are safety reasons why the developers chose what they did with force merge.  Just because a thing can be done doesn't mean it's best practices, or safe for general consumption.. And you're right, it doesn't \"block\" the call, but queues it.  Elasticsearch should not be spawning multiple parallel force merges simply because of the extreme amount of disk I/O it represents.  That's why it's clearly outlined that it will block subsequent force merges in the documentation until the running one completes.. In the vein of \u201cpics, or it didn\u2019t happen,\u201d have you actually observed concurrent forceMerge operations from successive calls without altering any settings from the defaults? At the index-level, not the shard level. And this should be one index per call, not a wildcard or csv indices. . If you make a forceMerge call with wildcards, the concurrent merge throttling might have to be tweaked to permit more than 2 per node. But did you actually see multiple forceMerge tasks in the task api?. The allocation action code, in spite of containing the index and allocation is about shard routing, specifically.  You should be able to use the index_settings to change other index settings, including this one.. Sorry for the delay.  Been on vacation.  Let me take a look at these changes and get back to you.. Just a question, how does this intersect with the pattern feature in the count filter? I ask because I see you using the count filter in your test, but the pattern feature seems to intersect with what you're trying to do.. Sorry for the delays.  I will hopefully get around to reviewing this further tonight.. Sorry for the delays.  I will hopefully get around to reviewing this further tonight.. I will look this all over in a few days.  I'm super busy with work right now.. Cross-reference #1299. I don't see a quick or easy path.  It will be even more abstracted in the future when the client functionality is separated into its own module (and boto3 will be in that). \n\nIt's much more plausible to build a curator-lambda module that removes the extra client dependencies and just provides the bones.  This is not a project I'm in a rush to tackle.  I'd keep using the workaround for now.. Probably not before Curator 6.0 is released.. Sorry you've had a bad experience with Curator.  The reason for the timestamp length being 10 or 13 is because sometimes epoch timestamps comes with milliseconds, sometimes without.  It's super hard for Curator to know which is which.  The date math would be off significantly if Curator expected the wrong one and ended up being off by 3 orders of magnitude.\nYou've hit an edge case I did not anticipate, nor do I have any quick solutions.  I will review your pull request and consider it.. Have you rebased recently?  Multiple tests are failing, not just ones you added.. All of the failed tests share this error message:\nValueError: Invalid epoch datatype received, expected an int\nI'm going to go out on a limb and suggest that some other code needs changing.. I will look this all over and merge it in a few days.  I'm super busy with work right now.. Thanks for these changes. I will look this all over in a few days.  I'm super busy with work right now.. This is a good first draft. I'll merge this now, and add some documentation later.. Have you followed #1291? This PR may be merged at some point, but it will have to be optional. If the author of that would change it to be optional, I\u2019d be able to merge it sooner.. Thanks for pointing this out.  It's astonishing to me that a timestamp field would be protected, but I'll have to catch this.. Duplicate.\nThis was originally reported in #1279 and fixed in #1284 . This is a duplicate of #1279 which was fixed in #1280.  The current workaround is to run\npip install -U click==6.7. This is extremely unusual. I wonder if it is Docker-specific. It should terminate. It should not loop. It has never looped in any other environment I\u2019ve run Curator in. In fact, my CI tests would have caught that. Do you have a wrapper script that is simply calling Curator repeatedly?\nThere is already a flag that should address the empty list issue, and that is ignore_empty_list. I considered making it the default behavior, but there are times when an empty list is not expected. It should be an error in such cases. The flag allows you to bypass it. . The RPM and DEB installers also do not provide default YAML configuration files.\nPlease see the official documentation for examples and client configuration file instructions.. That is only the client configuration file. You also need an action file.\nThe syntax is:\ncurator --config /path/to/curator.yml /path/to/action.yml. There are many other example action files.\nHere is one for the delete_indices action.. Glad you got it figured out. Closing this issue.. There's already a flag that covers this: --ignore_empty_list\n```\n$ curator_cli delete_indices --help\nUsage: curator_cli delete_indices [OPTIONS]\nDelete Indices\nOptions:\n  --ignore_empty_list             Do not raise exception if there are no actionable indices\n...\n```\nThis is the difference between running with and without --ignore_empty_list (yes, I know that this uses show_indices, but it works the same for all actions) :\n$ curator_cli show_indices --filter_list '{\"filtertype\":\"pattern\",\"kind\":\"prefix\",\"value\":\"netfloo\"}'\n$ echo $?\n1\n$ curator_cli show_indices --ignore_empty_list --filter_list '{\"filtertype\":\"pattern\",\"kind\":\"prefix\",\"value\":\"netfloo\"}'\n$ echo $?\n0. First and foremost, use DEBUG logging instead of INFO, which will give you much more information.\nSecond, you have multiple filters blocks within each action, which is not supposed to happen. It could be causing the issues you are seeing.\nNamely, the first filters block in each action contains the prefix filter. Instead, you are ending up with only an age filter. It's as though the first action has only:\nfilters:\n- filtertype: age\n  source: name\n  direction: older\n  timestring: '%Y.%m.%d'\n  unit: days\n  unit_count: 14\nAnd the second action's filter only has:\nfilters:\n- filtertype: age\n  source: name\n  direction: older\n  timestring: '%Y.%m.%d'\n  unit: days\n  unit_count: 7\nThis is why the second action is deleting Logstash prefixed indices. It's only looking for indices with a %Y.%m.%d timestring, and that older than only 7 days. logstash-2018.11.14 matches that, as it's older than 7 days. The first action's filters match nothing because there is nothing older than 14 days.\nYour action's filter blocks should look like:\nfilters:\n        - filtertype: pattern\n          kind: prefix\n          value: logstash-\n        - filtertype: age\n          source: name\n          direction: older\n          timestring: '%Y.%m.%d'\n          unit: days\n          unit_count: 14\nand\nfilters:\n        - filtertype: pattern\n          kind: prefix\n          value: audit-\n        - filtertype: age\n          source: name\n          direction: older\n          timestring: '%Y.%m.%d'\n          unit: days\n          unit_count: 7\nNote the omission of the extra filters line in each case.\nEnabling DEBUG logging will show you what your filters have been parsed and processed into. Elevated logging is your first line of defense, as is running in --dry-run mode to see the output before actually running. Using --dry-run with DEBUG will show all kinds of troubleshooting info worth digging into.. So, let's not use the word \"rollover\" by itself as it's loaded and can be interpreted to mean multiple things, all of which might be true.\nWhat you're talking about is index creation at UTC rollover, where a new index is created per day.\nI can imagine a feature or options added to the existing create_index action which would permit providing a list of prefixes, and a wait for the cluster and index states to reach \"green\" state after each new index is created. This action would still need to be executed before 0:00 UTC, but would meet this use case.\nBut will it even be necessary in the future? Beats and Logstash are headed into ILM management territory, where the indices created are Rollover API compliant (-000001 suffixes, etc). This kind of fix will be nice for those not using ILM, and those with older setups, and probably for the .monitoring- indices too.. > So if you just had a wait for green and created indices sequentially for existing create index functionality, this might mean that some events timestamped for the next period land in the previous period's index \nThis is impossible. The bulk requests have a document and state the named index that document is supposed to be in.\nThe idea is to run this particular Curator job anywhere from an hour or two before UTC rollover to even a few minutes before, so that the next day's indices are there in advance. Nothing else would matter here.\nA Rollover API rollover would, at least in theory, happen at odd hours of the day, so there would be no cluster of them all happening at UTC 0:00, but you cannot pre-create a rollover index. You will always have to wait for the cluster state to gel after one of these. . Pseudo-duplicate of #1294, which functionality covers this. . When you say that Curator works with a 5.5.2 cluster, is it this exact installation of Curator? Or a different one?. Curator 5.x releases are mostly platform agnostic for Elasticsearch versions 5.x and 6.x.  There have been a few updates for compatibility, but Curator 5.6.0 will work with all 5.x and 6.x versions of Elasticsearch.  The continuous integration tests cover all of the latest patch releases of Elasticsearch from 5.x - 6.x (5.0.x, 5.1.y, ... 6.0.z, 6.1.a, ... 6.5.b).\nThat said, since you've installed with pip, there may be some inconsistencies with some releases of the elasticsearch python module.  What is the output of pip list | grep elasticsearch?\nI tested Curator 5.6.0 from my MacBook Pro against my own Elasticsearch cluster running out of 9243:\n```\n\nclient:\n  hosts: [ \"REDACTED.ece.untergeek.net\" ]\n  port: 9243\n  use_ssl: true\n  http_auth: REDACTED\nlogging:\n  loglevel: INFO\nI created a virtualenv so there were no other system python modules conflicting:\n$ python3 --version\nPython 3.6.5 (This is the latest 3.6 version available through homebrew)\n$ python3 -m virtualenv curator_560\n$ source curator_560/bin/activate\nThen I installed:\n$ pip install elasticsearch-curator\n... (install stuff) ...\n$ pip list\nPackage               Version\n\nboto3                 1.9.55\nbotocore              1.12.55\ncertifi               2018.10.15\nchardet               3.0.4\nclick                 6.7\ndocutils              0.14\nelasticsearch         6.3.1\nelasticsearch-curator 5.6.0\nidna                  2.7\njmespath              0.9.3\npip                   18.1\npython-dateutil       2.7.5\nPyYAML                3.13\nrequests              2.20.1\nrequests-aws4auth     0.9\ns3transfer            0.1.13\nsetuptools            40.6.2\nsix                   1.11.0\nurllib3               1.24.1\nvoluptuous            0.11.5\nwheel                 0.32.3\nI ran curator:\n$ curator --config curator-netflow.yml --dry-run netflow_rollover.yml\n2018-11-29 09:22:53,017 INFO      Preparing Action ID: 100, \"rollover\"\n2018-11-29 09:22:53,044 INFO      Trying Action ID: 100, \"rollover\": Rollover Netflow indices when the sum of all primary shards reaches 5gb\n2018-11-29 09:22:53,051 INFO      DRY-RUN MODE.  No changes will be made.\n2018-11-29 09:22:53,056 INFO      DRY-RUN: rollover: netflow-rollover result: {'acknowledged': False, 'shards_acknowledged': False, 'old_index': 'netflow-000001', 'new_index': 'netflow-000002', 'rolled_over': False, 'dry_run': True, 'conditions': {'[max_size: 5gb]': False}}\n2018-11-29 09:22:53,056 INFO      Action ID: 100, \"rollover\" completed.\n2018-11-29 09:22:53,056 INFO      Preparing Action ID: 110, \"delete_indices\"\n2018-11-29 09:22:53,076 INFO      Trying Action ID: 110, \"delete_indices\": Delete Netflow indices older than 7 days\n2018-11-29 09:22:53,119 INFO      DRY-RUN MODE.  No changes will be made.\n2018-11-29 09:22:53,119 INFO      (CLOSED) indices may be shown that may not be acted on by action \"delete_indices\".\n2018-11-29 09:22:53,119 INFO      Action ID: 110, \"delete_indices\" completed.\n2018-11-29 09:22:53,119 INFO      Preparing Action ID: 200, \"delete_indices\"\n2018-11-29 09:22:53,136 INFO      Trying Action ID: 200, \"delete_indices\": Delete Monitoring indices older than 2 days\n2018-11-29 09:22:53,176 INFO      DRY-RUN MODE.  No changes will be made.\n2018-11-29 09:22:53,176 INFO      (CLOSED) indices may be shown that may not be acted on by action \"delete_indices\".\n2018-11-29 09:22:53,176 INFO      Action ID: 200, \"delete_indices\" completed.\n2018-11-29 09:22:53,176 INFO      Job completed.\n``\nIt works for me, from my Mac.. It should be noted that--dry-run` still requires a full client connection, so even though no action was taken, everything else was real.\nYou may want to take the approach of installing in a virtualenv too, to see if that helps.. That message implies you're hitting a Kibana endpoint and not an Elasticsearch one.. Thank you for this, but I have no intention of allowing click 7.0 before the next major release of Curator. I will revisit all flags then, as the major release version will allow me to make breaking changes. \nI will probably standardize on hyphens at that point, instead of underscores. . I am on my phone right now, but it looks like all CI tests failed. I will take a look at \u201cwhy\u201d later. . Curator 5.6.0 pins the click version to disallow 7.0, in case you didn\u2019t see that change. . Again, I appreciate the effort, but there is no compelling need to upgrade to click 7.0. There is no new feature in it that Curator requires. Quite the contrary, the default hyphen/underscore behavior is a reason to avoid it, even if a reasonable work-around can be made for it. 6.7 is just fine for what Curator needs.\nI do plan on making major changes for Curator 6.0, and will pin click at >=7.0 for that release, and make appropriate changes to the command-line flags as needed.. I can confirm what you're seeing, but it's not a Curator bug. Curator just passes the date math along to the Elasticsearch API.  +5w yields 2019-02.  It seems like an ISO week math problem, but that problem is in Elasticsearch.\nI submitted https://github.com/elastic/elasticsearch/issues/36265 to the Elasticsearch team.. Please see the update provided by the Elasticsearch developers at \nhttps://github.com/elastic/elasticsearch/issues/36265#issuecomment-470924219. This appears to be a pip based install. Have you tried the all-in-one RPM? You could also add the yum repo and go that route, if you like.\nThis all-in-one bundles all necessary Python libraries/modules, so it should hopefully not be bound by system dependencies the same way.. For the record, this RPM also has a dependency on a higher release version of OpenSSL than the out-of-the-box RHEL/CentOS, which may also address the SSL issue on its own.. Which yum repo? The Curator one?. I will try to set up a clean CentOS 7.5 install and see if I can replicate this.. It's Windows. I haven't documented it yet, because you're only the second person I've encountered to have hit this snag. There's something about how the JSON bits are parsed on a Windows command line (cmd or PowerShell) that precludes it from being used.  The exact same line works just fine from a Linux or Mac machine, but won't on a Windows machine.\nThere may be something I'm missing, like there's some way to escape the characters to make them work in Windows. I haven't had a chance to try to chase that down, however.\nPerhaps if there's someone who knows the Windows command-line, that knows how to escape characters they could shed some light on this subject for me.\n. This is just a reporting/logging error. It\u2019s correct if it instead says, \u201crun time in seconds\u201d instead of nanoseconds, which is very hard to visually correlate with seconds. The change will be to report seconds instead of nanoseconds.. This suggests your environment variables are not getting passed correctly.\nWhat happens if you call Curator in a wrapper script that is called by cron, and the variables are passed to that (or read in by it)?. Please feel free to close this, as it\u2019s a cron/env variable issue, then, and not Curator.. How will Curator, running in a docker container with its own (virtual) ethernet interface, be able to read from a different container's 127.0.0.1 without doing some port forwarding? The reason you are getting the connection refused message is that there is no Elasticsearch running on the Curator container's 127.0.0.1. The localhost/127.0.0.1 is not shared among all containers that way.. That Dockerfile doesn't seem to indicate that Elasticsearch is also running in there. The entrypoint only seems to indicate Curator is running in that container. I wish I could draw a simple network diagram, but this is what's going on, if I am understanding correctly:\nContainer Host:\n  - Elasticsearch on 127.0.0.1:9200\n  - Docker (usually 172.x.y.z)\n    - Curator Container\n      - Separate 127.0.0.1\n      - Can't reach parent 127.0.0.1:9200\nIn order for Curator to talk to Elasticsearch, you will have to:\n\nMake sure to make sure port 9200 is listening on the all IPs, not just localhost\n\nor\n\nConfigure the Curator container to read from the localhost:9200 \n\nAnd then:\n\nPoint your ENV ES_HOST and/or an ENV ES_PORT to that IP address/port combo.\n. Since this is no longer actually a Curator issue, please close it when you are done. I'm going to flag it as an issue that will not be fixed (because it's not something Curator can fix).. I'm closing this too, because if you follow the instructions in #1245 and run with the --ignore_empty_list flag, you will still see an empty list error, but it will exit with a 0, and not a 1. This has been addressed for quite some time.. $ curator_cli show_indices --filter_list '{\"filtertype\":\"pattern\",\"kind\":\"prefix\",\"value\":\"netfloo\"}'\n$ echo $?\n1\n$ curator_cli show_indices --ignore_empty_list --filter_list '{\"filtertype\":\"pattern\",\"kind\":\"prefix\",\"value\":\"netfloo\"}'\n$ echo $?\n0. What you are trying to do is also not permitted: https://www.elastic.co/guide/en/elasticsearch/client/curator/5.6/envvars.html\n\nYou are adding additional values via additional environment variables. This cannot be property interpreted. YAML permits one key and one string value, and the environment variables can provide a default value in case there is no value via that colon. The one value, if an env var, must be inside double quotes. The extra colon will always result in an inability to be properly evaluated.\nThe only acceptable way around this is to create a third environment variable built from the other two. You are correct in assuming this precludes using a colon delimitation for a port number. . Have you tried using double-quotes instead of single quotes? The interpreter will see items inside single quotes as literal, and perhaps not pass the wildcard correctly. . In other words, replace:\n['index_*_2018.12.31']\n\nWith:\n[\"index_*_2018.12.31\"]. That's a completely different issue. Please open another issue with debug logging, and preferably an empty `blacklist` configured (i.e. `blacklist: []`).\n\nSince the issue that was raised here is now resolved, I'm closing this issue.. Tests are failing. They must be either removed, or reworked:\n```\nERROR: test_multi_data_path_node (test.unit.test_action_shrink.TestActionShrink_qualify_single_node)\nTraceback (most recent call last):\n  File \"/home/travis/build/elastic/curator/test/unit/test_action_shrink.py\", line 107, in test_multi_data_path_node\n    self.assertRaises(curator.ActionError, shrink.qualify_single_node)\n  File \"/opt/python/2.7.14/lib/python2.7/unittest/case.py\", line 473, in assertRaises\n    callableObj(args, *kwargs)\n  File \"/home/travis/build/elastic/curator/curator/actions.py\", line 1949, in qualify_single_node\n    self.client.nodes.stats()['nodes'][node_id]['fs']['total']['available_in_bytes']\nKeyError: 'total'\n-------------------- >> begin captured logging << --------------------\nDEBUG      curator.indexlist          __get_indices:65   Getting all indices\nDEBUG          curator.utils            get_indices:643  Detected Elasticsearch version 5.0.0\nDEBUG          curator.utils            get_indices:645  All indices: ['index_name']\nDEBUG      curator.indexlist     __build_index_info:80   Building preliminary index metadata for index_name\nDEBUG      curator.indexlist          _get_metadata:177  Getting index metadata\nDEBUG      curator.indexlist       empty_list_check:226  Checking for empty list\nDEBUG      curator.indexlist       _get_index_stats:117  Getting index stats\nDEBUG      curator.indexlist       empty_list_check:226  Checking for empty list\nDEBUG      curator.indexlist           working_list:237  Generating working list of indices\nDEBUG      curator.indexlist           working_list:237  Generating working list of indices\nDEBUG      curator.indexlist     iterate_over_stats:126  Index: index_name  Size: 1.0GB  Docs: 6374962\nDEBUG          curator.utils        name_to_node_id:1827 Found node_id \"my_node\" for name \"node_name\".\nDEBUG          curator.utils        node_id_to_name:1845 Name associated with node_id \"my_node\": node_name\n--------------------- >> end captured logging << ---------------------\n======================================================================\nFAIL: test_multi_data_path_node (test.unit.test_action_shrink.TestActionShrink_most_available_node)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/home/travis/build/elastic/curator/test/unit/test_action_shrink.py\", line 135, in test_multi_data_path_node\n    self.assertIsNone(shrink.shrink_node_name)\nAssertionError: u'node_name' is not None\n-------------------- >> begin captured logging << --------------------\nDEBUG      curator.indexlist          __get_indices:65   Getting all indices\nDEBUG          curator.utils            get_indices:643  Detected Elasticsearch version 5.0.0\nDEBUG          curator.utils            get_indices:645  All indices: ['index_name']\nDEBUG      curator.indexlist     __build_index_info:80   Building preliminary index metadata for index_name\nDEBUG      curator.indexlist          _get_metadata:177  Getting index metadata\nDEBUG      curator.indexlist       empty_list_check:226  Checking for empty list\nDEBUG      curator.indexlist       _get_index_stats:117  Getting index stats\nDEBUG      curator.indexlist       empty_list_check:226  Checking for empty list\nDEBUG      curator.indexlist           working_list:237  Generating working list of indices\nDEBUG      curator.indexlist           working_list:237  Generating working list of indices\nDEBUG      curator.indexlist     iterate_over_stats:126  Index: index_name  Size: 1.0GB  Docs: 6374962\nDEBUG          curator.utils        node_id_to_name:1845 Name associated with node_id \"my_node\": node_name\n--------------------- >> end captured logging << ---------------------\n``.test.unit.test_action_shrink.TestActionShrink_qualify_single_node` is a hint, with kind of path to it:\n\nLook in file system path  test/unit/test_action_shrink.py\nFind class TestActionShrink_qualify_single_node\ntest_multi_data_path_node will be one of the methods in the above class.\n\n. Curator may not be the place to configure this. The problem is much more likely to be in urllib3, which is what the Elasticsearch python module uses. If no such option exists there, then Curator cannot help. Curator just piggy-backs onto the Elasticsearch python module, which uses urllib3. If the Elasticsearch python module can support a keep-alive, then I can probably get Curator to do something of that sort too.\nI am not in a place where I can dig at this, and cannot promise to do so any time soon. . > So it seems that only indexes with more than 10 segments get Forcemerged, is that a normal behavior ?\nCurator checks to see if there are more segments than what you've set to max_num_segments and will only proceed if that is the case.. That's the max_num_segments per shard, rather than per index. How many shards does your index have? If it's the default 5 primary + 1 replica that would be 10 total segments, explaining why you saw the behavior you did.. Please sign the CLA, and I can merge this after the CI tests come back.\nIt might be nice to add this same change to the Snapshot list class as well.. This is a known issue, and a duplicate of #1329. Going to ignore the three 6.6.0 tests for now. Need to do deeper testing now that the ILM feature has been released in beta.. Can you better describe the problem this fixes? I\u2019ve seen no issue submitted for it. . Oh, I see. This allows for templating.. Please encapsulate your pasted content inside triple back ticks, like this:\n```\nPASTE HERE\n```\n\nI can't see your YAML as it is formatted without this, and based on what I'm seeing in your log output, it could be a YAML formatting error (because it never gets past checking the yaml file).. Try updating the indent of the filters:\nfilters:\n- filtertype: pattern\n  kind: prefix\n  value: .kibana\n  exclude: False\nThe indent levels I see suggest that the later keys are not part of the array element filtertype: pattern. I'm sorry you had a bad experience. The kibana filtertype will go away for the next major release due to continued development and underlying changes. You will be required to manually exclude Kibana on your own, using a pattern filtertype.. It doesn't appear that it's substituting your host either. Can you elaborate on how you are calling Curator?. Also, Python 3.7 is not yet tested for Curator. It may or may not work properly. It has not been fully evaluated.. Bad Value: \"{'hosts': ['${ES_HOST}'], 'timeout': 30, 'use_ssl': False, 'master_only': False, 'port': '${ES_PORT:9200}', 'ssl_no_validate': False}\"\nSomething about how Docker is handling this is not passing the environment variables in a way that Curator can see/extract them. It is plainly visible in the log you shared.. There were reasons I had to do this before, but in this case those reasons don't matter.  I'll \"update\" this. \n. Brain fart.\n. Thanks!  It was too ugly to leave alone.\n. Thanks for catching this.  I never delete by space, but I know there are plenty who do.  They thank you too, I'm sure.\n. @HonzaKral I caught this today as I realized it wasn't counting replicas properly.\n. Let's make this another elif anyway, since there may be other changes (by month in rare cases, for example).\n. This is not going to work the way you want it to.\nAs presently constituted, it will return the value of the \"elected\" master, not any \"eligible\" master node.  If you have 3 master eligible nodes, and the one you're normally pointing at is no longer the elected master, this will always fail.  You'd have to always find the elected master, and reconfigure the client to connect to only it.  Even in a split-brain scenario, you would be hitting the \"elected\" master of one side of the split.\nHere's an example from my 2 node cluster (2 master nodes, 2 minimum masters so 2 are always required for quorum) and both nodes are up and the cluster is green:\n```\n\n\n\nmy_node_id = client.nodes.info('_local')['nodes'].keys()[0]\nmy_node_id\nu'LsfriHYWSTuP4xG00HpiKw'\nmaster_node_id = client.cluster.state(metric='master_node')['master_node']\nmaster_node_id\nu'Je-JVV_LRMCeM4I6dr0Suw'\nclient.cluster.state(metric='master_node')\n{u'cluster_name': u'untergeek', u'master_node': u'Je-JVV_LRMCeM4I6dr0Suw'}\n```\n\n\n\nAs you can see, the node ids differ, even though the cluster is completely valid to do operations from my_node_id\n. Thanks!  This looks much better.\n. I like the idea.  This fixes some use-case requests I've heard. \nPlease note that this is dangerous, though, especially with regards to potential split-brain scenarios.  If you're going to go this far, we should perhaps also put in protections to only operate on a cluster when it is not in a red state as well (without an override, anyway).\n. Is that what would be here?  NotFoundError?\n. Please remove this newline.\n. Please restore the newline after main()\n. No, because date objects and datetime objects are different.  I have to reference year and month separately here, and I also want each month to be calculated based on the 1st.\n. I had that idea too, though. I tried it that way first, and it didn't work.  I learned I had to convert :smile: \n. That's a good idea!\n. His pythonness, Honza, himself put that there.  I thought it was odd, but a really cool shortcut now that I know it can be done.\n. Ah, I see.  It's because timedelta uses kwargs.  This converts it into the format desired.\nExpanded out, it actually would say something like:\nstep = timedelta(days=1)\nSo, **{unit: 1} becomes days=1\n. Can you change the copyright to match https://github.com/elasticsearch/curator/blob/master/LICENSE.txt#L1 ?\n. Renaming snapshot_name to snap in this method to prevent a collision with kwarg['snapshot_name'].\n. Putting this here for consistency.\n. Condensing this.\n. This addresses the issue brought up in #185\n. Seriously don't know how I got bad indents in here.  Had to fix them.\n. I think this test should open the index after applying the rule also, and then test to ensure the setting is present after opening.\n. I think this test should open the index after changing the replica count and ensure the setting persists after re-opening.\n. This line is probably unnecessary, since we're feeding the logs to /dev/null\n. This should probably not be hard-coded to /tmp because it is not OS independent.  Perhaps a tmpdir provided by Python? \nimport tempfile\ntmpdir = tempfile.gettempdir()\n. host and port are defined by environment variables, so that's how that's taken care of.  Again, it's manually specified, and that's what we're looking for here.  It overrides the default, even if it overrides it with the same value.\n. That's an excellent point.  But I'm willing to let this one fail in instances where there's no matchup, just in case.  Most of the time the instance used in testing is on the same host.\n. Do document that inline, though.\n. Technically, this is not Logstash specific until you add --prefix logstash\n. Rather than make users feel like there's \"common issues,\" perhaps this would be better titled, Frequently Asked Questions\n. And you could link that entire header to http://www.elastic.co/guide/en/elasticsearch/client/curator/current/faq.html\n. And I like this here!  Go to the FAQ!\n. Why not do the same thing here as in line 35?\nbody='index.routing.allocation.{0}.{1}={2}'.format(allocation_type,key,value),\n. While this works, it's rather long.  Perhaps something more concise like this:\nif allocation_type not in ['require', 'include', 'exclude']:\n. I think an error log message here would be appropriate:\nlogger.error(\"{0} is an invalid allocation_type.  Must be one of 'require', 'include', 'exclude'.\".format(allocation_type))\n. Please add allocation_type to the docs here.\n. Please move this below rule, so they appear in the same order the arguments appear in the definition.\n. Please try to follow the descriptive patterns and capitalization.  This could be something like \"The allocation type to check for.  Must be one of 'require', 'include', or 'exclude'.\"\n. This doesn't quite fit the syntax of type.rule=value.  Perhaps \"Allocation rule {1} is already applied for type {2}.\"  \nThoughts?\n. As with the other, this could be more cleanly written as:\nif type not in ['require', 'include', 'exclude']:\n. Add be, e.g. '--type can only be one of'...\n. Why do all of these lines have an extra space at the end after \"foo=bar\"?\n. This was a clever approach!  \nI'd prefer to see append_closed as a keyword argument with a default, e.g. append_closed=False, rather than as a positional argument.  Also, I feel that return_closed is better semantically than append_closed, even if appending is what is happening.  We want the method to return the closed values.\n. You know what?  This is fine.  Disregard my previous comment.  We'll stick with this.\n. Please add a comment here to explain that this workaround is for AWS Elasticsearch, that it is using text/plain to respond.\n. I'll do this myself.  Tests are passing for all versions of Elasticsearch tested, from 1.0.3 -> 2.0.0.rc1\n. Yep. It's on purpose. This is rst documentation. \n. This didn't matter until the Coerce(int) was necessary.\n. How does this function get the aws_flag variable?  It won't keep state inside the utils.py file, as that's just a reference library.  That's why the tests are not passing:\nERROR            curator.cli                    cli:193  Failed to complete action: snapshot.  <class 'curator.exceptions.FailedExecution'>: Exception encountered.  Rerun with loglevel DEBUG and/or check Elasticsearch logs for more information. Exception: global name 'aws_flag' is not defined. Please add argument documentation for unit_count_pattern that follows the documentation pattern below:\n:arg unit_count_pattern: explanation here\nThis is needed for API documentation.. Rather than failing index, please keep the common terminology of \"removing index from actionable list.\"  Telling users that you're going to fail an index is not quite in keeping with saying, \"I'm not going to do anything with this index, even though it's otherwise healthy\". Please capitalize the first letter of the first word for these debug lines.\nAlso, please place a period after \"Unable to match pattern. Using fallback value of ...\". Maybe a loop here?\npython\nis_aws = False\nfound_version = client.info()['version']\nfor aws_version in settings.aws_versions:\n    if found_version == aws_version:\n        is_aws = True\nThat way it will set the flag true if even one of the AWS versions matches.  It won't matter if the others don't.. Then you'll have a valid boolean with is_aws. ## Ignore all this if AWS ES disallows access to Snapshot API calls as handled by the python client.\nI can't imagine that AWS ES would allow you to make these transport-level calls, but somehow block regular API calls, though.\nI think you can greatly simplify this by not doing a direct client transport request approach.  The elasticsearch python API has calls you can make that normalize this so you don't even have to have different code for Python 2 vs Python 3.\nIn place of the /_cat/repositories request, why not just run client.snapshot.get_repository():\n```\n\n\n\nclient.snapshot.get_repository()\n{u'Untergeek': {u'settings': {u'compress': u'true',\n   u'location': u'/bigdisk/repos/Untergeek',\n   u'max_restore_bytes_per_sec': u'20mb',\n   u'max_snapshot_bytes_per_sec': u'20mb'},\n  u'type': u'fs'}}\n```\nThe root keys will be  your repository names.\n\n\n\nAnd if that doesn't work, the cat API calls through the Python client are much nicer:\n```\n\n\n\nclient.cat.repositories(format='json')\n[{u'id': u'Untergeek', u'type': u'fs'}]\n```\n\n\n\nIn place of the GET /_snapshot/REPO/_current, you can run:\n```\n\n\n\nclient.snapshot.get(repository='Untergeek', snapshot='_current')\n{u'snapshots': []}\n```\nAs you can see, the result is the same.\n\n\n\nThis whole block could probably be replaced with:\nrepos = list[client.snapshot.get_repository().keys()]\nrepos.pop('cs-automated') # This removes that key from the list before we even iterate\nfor r in repos:\n    status = client.snapshot.get(repository=repo, snapshot='_current')['snapshots']\n    return False if status == [] else True\nAnd it should work for both Python 2 and 3.. I take it back (in part). I worry that the loop will yield a false positive because it will return False on even the first iteration of the loop if an empty status comes back.  I know it's the same return clause I have at the end, but that won't work right with a loop.  If the second iteration would have yielded a \"busy\" status, as it were, it will never get to that status.  This should be tested differently.  This sample block should cover it, I think.\nNOTE: I got the usage of pop wrong.  You have to do it on a dictionary to remove named keys.  This shows the fix for that oversight.\ntry:\n        snapshot_active = False\n        #\n        # Do test for is_aws here\n        #\n        if not is_aws:\n            # This will return False if empty and True if populated.\n            snapshot_active = (client.snapshot.status()['snapshots'] != []) \n        else: # is_aws\n            repos = client.snapshot.get_repository()\n            repos.pop('cs-automated') # This removes that key from the list before we even iterate\n            for r in list(repos.keys()]:\n                status = client.snapshot.get(repository=repo, snapshot='_current')['snapshots']\n                if status != []:\n                    snapshot_active = True\n    except Exception as e:\n        report_failure(e)\n    return snapshot_active. Why change this?  I don't understand.. Please put it on the last line before the closing parenthesis.  You don't need to use the + \\ syntax that way, and it should work just fine.. If not, I stand corrected.. This still needs to be indented.. Confirmed that it will work that way (in ipython):\nIn [2]: print(\n   ...:     'a-'\n   ...:     'b-{0}-'\n   ...:     'c'.format('FOO')\n   ...: )\na-b-FOO-c. Please put this under New Features, as it is an improvement.  Add the PR number, since you have that now, and put your github handle in parenthesis, following the pattern already there.. Question.  If a user is currently using aws_key and aws_secret_key, won't this result in a breaking change to them if they upgrade?  I'm concerned about breaking someone's existing functionality in a minor version change.\nThis would be an easy change for a major revision, but if there's no way to preserve existing functionality, that's going to be a problem.. This is going to cause a problem:\ndefault='larger'\nThis has to be greater_than or we'll have serious issues.. I'm not seeing it yet.  Where is it?  It should still allow for these args you've removed.  It has to be there, in case users are running from a non-AWS location (hey, it could happen).. I'm not seeing the reverse compatibility here yet.  Perhaps a try/except block that falls back to the old credentials?. This is going to need to be more complex.\nNot only does it have to try/except on the import module, it has to try/except on the session.Session() (because it will raise an exception if it is unable, and we should control how it acts if it does, and log good messages).  If it is unable to get the session, then it would then rely on the original aws_key and aws_secret_key, which should be set first.\nThe contents of the else block should precede the if kwargs['aws_sign_request'] statement as a pre-population thing, even if they end up being False because they were not specified.  The boto3 stuff will overwrite the necessary values if it exists, so it is completely okay to have kwargs['aws_key'] and kwargs['aws_secret_key'] set first.\nDoes this make sense?. Do you think it's wise to set the default to us-east-1, rather than just leave it kind of required to be set if you're using AWS credentials?. Don't do an exit here.  This should simply raise an Exception of some kind, either a ConfigurationError, indicating something is misconfigured, or a CuratorException, as there isn't an exception in exceptions.py that's more specific.\nexcept AttributeError:\n    raise CuratorException('Unable to locate AWS credentials, even though \"aws_sign_request\" was specified'). \ud83e\udd26\u200d\u2642\ufe0f Thanks for this.  Can't believe I didn't think of that.. Please add a comment to the Changelog on this in the bug fixes section.. NoCredentialsError is not a valid member of the base exceptions class.  Is that in the boto module?  If so, it would just be NoCredentialsError, rather than exceptions.NoCredentialsError.. Can you mention the PR # and your github handle in parenthesis, like the other lines?. I think know why it's like that now.  I probably cut and pasted what I was using in ipython to test (it wasn't in a loop, there).. You're missing the trailing ) here, and that's why the test is failing.  It was there before, so you probably removed it by accident.. This should probably read, \"If installing via pip, this setting will not work unless ...\".  I install requests-aws4auth in the RPM/DEB packages.  If you want to edit both of the files in unix_packages directory to add boto to be installed, that would be okay, too.. This should be False or True here, don't you think?. There should be a line somewhere indicating the default value (which I presume is False, right?).. Please re-indent this to where it was previously, and that's the last change.  Otherwise, it looks good to me.. Hmmm.  I'll let this slide, since we're raising the NoCredentialsError anyway, but it seems like this should be logger.error, rather than logger.debug.. This here, line 6 of examples/curator.yml needs to be indented the way it used to be.  Yes, I know it technically works the way you have it indented, but proper YAML has it indented.. You might want to also add boto3 and botocore if someone is going to be using AWS Elasticsearch.  See here. Why is this causing issues for you?. Nevermind.  That was one of the other commits from changing base.. You can probably just leave this line as it is, and then add the two new lines to override the kwargs['verify_certs'] = True if ssl_no_validate is True.  Otherwise, you'll need an else statement to assign it to True.. True.  But I like the code readability (for new users) of a default being set, and then overridden.\nThis will all get cleaned up for version 6 anyway, though.. Why did you remove these?  I mean, yeah, they're comments, but they were just there to show the names and types of the placeholders. . These two lines are superfluous with the addition of the previous lines. Feel free to remove them.. So, devil's advocate here.  What happens if someone does a reindex from query instead of a full index? Will these still be equal? What if someone reindexes from multiple indices to 1 index (e.g. 1 week's worth of daily indices into a single  weekly index)? Do these numbers match up? If the answer is not \"yes,\" then I can't use the code\u2014at least not as-is.\nThe draft code works just fine in the event of a 1:1 reindex. I could merge it if the test is only made made when a verify_doc_count flag is present (or something like it).  It would, of course, have to be vetted by conditional tests.  It wouldn't be valid, for example, if Curator was able to detect a many-to-one reindex (code which would have to be added).. Thanks for providing a concrete example, @rahst12 . This line is not properly terminated, so the CI tests will not even run.. This line is also missing a closing ). that is to say, it needs an additional ). ",
    "jordansissel": "Thanks! :)\n. :heart:\n. Tested days to keep + close; seems to work for me:\n```\nCreate indexes for today through 100 days ago\nseq 0 100 | xargs -P5 -n1 sh -c 'curl -XPUT http://localhost:9200/$(date -d \"$1 days ago\" +\"logstash-%Y.%m.%d\")' -\nCheck expected counts:\n% curl -s localhost:9200/_stats?pretty | grep logstash- | wc -l\n101\nporkrevenge(...lasticsearch/nodes/0/indices) % ls | wc -l\n101\nPurge + close\n% ./logstash_index_cleaner.py --keep-open-days 30 -d 60\nCheck existing indexes:\n% curl -s localhost:9200/_stats?pretty | grep logstash- | wc -l\n30\nCheck total indexes (including closed ones)\nporkrevenge(...lasticsearch/nodes/0/indices) % ls | wc -l\n60 \n```\n. :metal:\n. lgtm!\n. \nShip it!\n. @imperialwicket I'm not sure how restore would necessarily fit in. Curator feels like a program that acts as a maintenance worker for your Elasticsearch needs.  Daily maintenance, for example, doesn't feel like something you'd want to do \"restore\" process. In my mind, it certainly feels weird to propose doing a 'restore' via daily cronjob.\nMaybe a separate tool or workflow to help doing restores, that is, if the restore API is insufficient?\nOf course, I might be totally misunderstanding your request. Can you help me understand it more? :)\n. @kimchy ok cool, I removed the mapping point. Thanks!\n. I moved the 'alias management' point to a new ticket.\n. Agreed! I think it would be useful if we could expand in both directions:\n- outward: \"Daily indices\" to a 'weekly' or 'monthly'-style index alias pointing at multiple dailies\n- inward: \"Weekly indices\" strategy with a 'daily' alias that is filtered correctly\nOne requires more indexes to query, one requires one index + filters. +1 to both features.\n. @untergeek nice detective work!\n. We could reuse the same syntax from either logstash or from kibana:\n- logstash: some text here %{+TIME FORMAT HERE} ...\n- kibana: [some text here ]TIME FORMAT HERE \n. @divanikus You could make curator useful to you by patching it to support your use case, btw. :)\n. You can achieve 'es indices | ... ' with the _cat api and xargs and curl. Grep will never understand time, so it's not a fit for what curator focuses on today (time-oriented indices)\n% curl -s localhost:9200/_cat/indices | fex 2 | grep logstash- | xargs -n1 sh -c 'curl -XPOST localhost:9200/$1/_close' -\n{\"acknowledged\":true}\nOr really, your specific case, you don't need grep.\n% curl -s localhost:9200/*-05-*/_close\n. +1 on idea, <3 for tests. Any inline comments I made are minor questions.\n. LGTM.\n\n. @algesten it looks like your request to make it clear in the README is resolved. At this time (about 7 days ago, by the commit logs), I see this in the readme table:\n\n. This seems more readable as:\nd.update({ 'keepby':'time', 'unit':'hours', 'close':arguments.open_hours })\nOne line, more explicit ('update' it).\n. you can just do: if 'close' in d: here, since dict.contains tests for key presence:\n```\n\n\n\n{\"foo\":\"bar\"}.contains(\"foo\")\nTrue\n```\n. +1 on refactoring the verb/etc into a method.\n. Don't worry about this right now, but long term, probably worth refactoring some of these \"shared\" arguments into a single method?\n\n\n\nadd_common_arguments(parser_allocation)\npython\ndef add_common_arguments(parser):\n  parser.add_argument('-T'...)\n. For fun, I tried using range() here on now but you can't do a range() on a datetime. Oh well :P\n. Maybe this? for i in range(1, count):\n. yep I saw that a soon as I tried writing timedelta(unit=1) in python and it was like NO. ;P\nThanks much for clarifying.\n. ",
    "Yggdrasil": "I would very much like this as well, but don't have any Python skills to contribute.\nExample use case: Deleting apache logs from just the dev servers after two weeks, while keeping those for the production servers and any other types of events.\nShould be able to filter by values in one or more fields, such as @type:\"apache-access\" and @source_host:\"dev1\".\n. ",
    "benh57": "I'd also love this. Looks like ES has a simple delete by query API which could be used for this. I'm surprised this hasn't already been done.\n. I went ahead and implemented a delete by type + tag feature in this script. I will post it into my repo soon.\n. Thanks. Perhaps separate indexes is the right way. That may be trivial. But the documentation on doing this very obvious task  with logstash / elasticsearch / kibana setup may be out there but its hard to find. It's split across numerous different websites, forums,etc.   After some googling i see that it's common to do something like outputting to :  index => \"logstash-%{@type}-%{+YYYY.MM.dd}\" which would probably be the best thing for my purposes, then i can just pass --prefix=logstash-apache-error to this script.\n. ",
    "josegonzalez": "@untergeek then might a better way be to feed the non-matching records into a temp index, \"drop\" the old index, then rename the temp to the old index name?\n. My other comment is that, while your thoughts are quite valid, having an alternative actually written out somewhere is quite useful. The logstash cookbooks are good for this, and would push people to using logstash/es the right way.\n. Aliasing is what I meant ;)\n. ",
    "cmeisinger": "Thanks for the fast response!\n```\nPython 2.6.6 (r266:84292, Nov 22 2013, 12:16:22) \n[GCC 4.4.7 20120313 (Red Hat 4.4.7-4)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport logging\nlogging.version\n'0.5.0.5'\n```\n. \n\n\n",
    "electrical": "Would be nice to initiate a snapshot and remove it then indeed.\n. Looks good to me.\n@untergeek can you verify?\nCheers\n. Hi,\ncurator 0.6.2 is made for ES 0.90.x \nYou will need to use the Master branch at the moment until a new release will be done.\n. @xinity easiest way is to create aliases.\nSee http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-aliases.html for more info on that.\n. I had a same issue with cluster names being the same that caused weird errors.\n. LGTM\n. Hi,\nIn this case it requires the python elasticsearch dependency, not elasticsearch it self.\n. ",
    "DanielRedOak": "I would say that the functionality that would best align with Curator would be to allow for removal of old snapshots within whatever repository is used (configured as an outside process) at a specified threshold.  Since repository setup is a separate task, this functionality would work with all current and future repository providers. To accomplish this, you would need time based snapshot names, but combining this with the trimming of curator in one script would be awesome! Perhaps curator for local indices + snapshots for backup and/or archiving.\n. This would be awesome for us as we index by application using logstash.  Ie. app1-logstash-DATE and currently we have to list out the prefix w/o regex so it ends up to be a lot of manual updates.\n. ",
    "imperialwicket": "It would be great to support restore (maybe even directly from S3) as well.\n. @jordansissel fair point; the fact that curator manages snapshots, closing, and deleting seems to qualify it as a candidate for restore capability to me. I completely understand if the concept for curator is 'routine' index maintenance and not general index maintenance, and we can leave restoration for a separate project.\nHowever, I was under the impression that restore could fit into the 'custom tasks' mentioned in #44. That may circumvent this discussion entirely.\n. ",
    "victorcoder": "+1\n. @untergeek Great news!\n. ",
    "gelim": "It is at line 343 in the Elasticsearch() instantiation. \n. it has been done. Do I need to close this pull request and re-create a new one ?\nOh :-) Didn't see that the pull has been updated accordingly. Great.\n. ",
    "zothar": "Oops.  Been too long since I've used Github.\n. ",
    "arieb": "elasticsearch-curator is more expressive in my opinion. In any case I think the repository name should be the same as the package name.\nAny thoughts about the licensing?\n. I updated the setup.py to include the testing requirments and changed the name to curator-es\n. Done. \nso package name is now curator-es. \ni've made the curator.py to be an entry point so after someone installs the package he can just use \n\"curator\" to run it.\n. no particular reason. updated it to be elasticsearch-curator\n. damn typo :/ its supposed to be \"System Administrators\"\n. ",
    "mbbx6spp": "Any reason why you chose curator-es as the name of package instead of elasticsearch-curator?\n. This pleases me :) Thanks for your work! It needed to be done.\n. @untergeek Looks @arieb completed those tasks. Bump. I have been itching for this since I found curator the other day, can you tell? ;)\n. ",
    "timbutler": "My apologies for the slow reply, happy with the solution in #40.\n. ",
    "dharrigan": "Hi There!\nThank you, that makes sense. Yes, I reverted to using the CURL command. I\nsuppose it might be useful to have a \"-f\" for force if value is 0. It's\nuseful at times to completely remove the index, but meh, it's neither\nhere or there since I can use CURL if required.\nThank you again for a great tool! :-)\n-=david=-\nOn 10 February 2014 12:57, Aaron Mildenstein notifications@github.comwrote:\n\nIt does work. The problem is the 0. You cannot use -d 0. I suppose that\nshould be better documented.\nThis was an intentional design choice so that you could not accidentally\ndelete the current index. If you want to delete the current index you will\nhave to use a curl statement: curl -XDELETE\nlocalhost:9200/.marvel-2014.02.10?pretty\nPerhaps we could correct the script to allow for this in the future, but\nit was intentional.\nHere's what it looks like for me:\ncurator --host blackbox -c 6 -b 1 -d 7 -p .marvel-\n2014-02-10T06:55:27.821 INFO                        main:325  Job starting...\n2014-02-10T06:55:27.821 INFO                        main:345  Deleting indices older than 7 days...\n2014-02-10T06:55:27.822 INFO                   _new_conn:257  Starting new HTTP connection (1): blackbox\n2014-02-10T06:55:27.829 INFO         log_request_success:49   GET http://blackbox:9200/_settings [status:200 request:0.007s]\n2014-02-10T06:55:27.833 INFO                  index_loop:290  Attempting to delete index .marvel-2014.01.31 because it is 3 days, 0:00:00 older than cutoff.\n2014-02-10T06:55:28.040 INFO         log_request_success:49   DELETE http://blackbox:9200/.marvel-2014.01.31 [status:200 request:0.206s]\n2014-02-10T06:55:28.040 INFO                  index_loop:300  .marvel-2014.01.31: Successfully deleted.\n2014-02-10T06:55:28.040 INFO                  index_loop:290  Attempting to delete index .marvel-2014.02.01 because it is 2 days, 0:00:00 older than cutoff.\n2014-02-10T06:55:28.402 INFO         log_request_success:49   DELETE http://blackbox:9200/.marvel-2014.02.01 [status:200 request:0.362s]\n2014-02-10T06:55:28.402 INFO                  index_loop:300  .marvel-2014.02.01: Successfully deleted.\n2014-02-10T06:55:28.402 INFO                  index_loop:290  Attempting to delete index .marvel-2014.02.02 because it is 1 day, 0:00:00 older than cutoff.\n2014-02-10T06:55:29.118 INFO         log_request_success:49   DELETE http://blackbox:9200/.marvel-2014.02.02 [status:200 request:0.716s]\n2014-02-10T06:55:29.118 INFO                  index_loop:300  .marvel-2014.02.02: Successfully deleted.\n2014-02-10T06:55:29.119 INFO        find_expired_indices:188  .marvel-2014.02.03 is 0:00:00 above the cutoff.\n2014-02-10T06:55:29.119 INFO        find_expired_indices:188  .marvel-2014.02.04 is 1 day, 0:00:00 above the cutoff.\n2014-02-10T06:55:29.119 INFO        find_expired_indices:188  .marvel-2014.02.05 is 2 days, 0:00:00 above the cutoff.\n2014-02-10T06:55:29.119 INFO        find_expired_indices:188  .marvel-2014.02.06 is 3 days, 0:00:00 above the cutoff.\n2014-02-10T06:55:29.119 INFO        find_expired_indices:188  .marvel-2014.02.07 is 4 days, 0:00:00 above the cutoff.\n2014-02-10T06:55:29.119 INFO        find_expired_indices:188  .marvel-2014.02.08 is 5 days, 0:00:00 above the cutoff.\n2014-02-10T06:55:29.119 INFO        find_expired_indices:188  .marvel-2014.02.09 is 6 days, 0:00:00 above the cutoff.\n2014-02-10T06:55:29.119 INFO        find_expired_indices:188  .marvel-2014.02.10 is 7 days, 0:00:00 above the cutoff.\n2014-02-10T06:55:29.119 INFO                  index_loop:301  DELETE index operations completed.\n2014-02-10T06:55:29.120 INFO                        main:350  Closing indices older than 6 days...\n2014-02-10T06:55:29.122 INFO         log_request_success:49   GET http://blackbox:9200/_settings [status:200 request:0.002s]\n2014-02-10T06:55:29.122 INFO                  index_loop:290  Attempting to close index .marvel-2014.02.03 because it is 1 day, 0:00:00 older than cutoff.\n2014-02-10T06:55:29.130 INFO         log_request_success:49   GET http://blackbox:9200/_cluster/state?filter_nodes=true&filter_routing_table=true&filter_indices=.marvel-2014.02.03&filter_index_templates=true&filter_blocks=true [status:200 request:0.008s]\n2014-02-10T06:55:29.287 INFO         log_request_success:49   POST http://blackbox:9200/.marvel-2014.02.03/_close [status:200 request:0.153s]\n2014-02-10T06:55:29.287 INFO                  index_loop:300  .marvel-2014.02.03: Successfully closed.\n2014-02-10T06:55:29.288 INFO        find_expired_indices:188  .marvel-2014.02.04 is 0:00:00 above the cutoff.\n2014-02-10T06:55:29.288 INFO        find_expired_indices:188  .marvel-2014.02.05 is 1 day, 0:00:00 above the cutoff.\n2014-02-10T06:55:29.288 INFO        find_expired_indices:188  .marvel-2014.02.06 is 2 days, 0:00:00 above the cutoff.\n2014-02-10T06:55:29.288 INFO        find_expired_indices:188  .marvel-2014.02.07 is 3 days, 0:00:00 above the cutoff.\n2014-02-10T06:55:29.288 INFO        find_expired_indices:188  .marvel-2014.02.08 is 4 days, 0:00:00 above the cutoff.\n2014-02-10T06:55:29.288 INFO        find_expired_indices:188  .marvel-2014.02.09 is 5 days, 0:00:00 above the cutoff.\n2014-02-10T06:55:29.288 INFO        find_expired_indices:188  .marvel-2014.02.10 is 6 days, 0:00:00 above the cutoff.\n2014-02-10T06:55:29.288 INFO                  index_loop:301  CLOSE index operations completed.\n2014-02-10T06:55:29.289 INFO                        main:355  Disabling bloom filter on indices older than 1 days...\n2014-02-10T06:55:29.291 INFO         log_request_success:49   GET http://blackbox:9200/_settings [status:200 request:0.002s]\n2014-02-10T06:55:29.291 INFO                  index_loop:290  Attempting to disable bloom filter for index .marvel-2014.02.03 because it is 6 days, 0:00:00 older than cutoff.\n2014-02-10T06:55:29.299 INFO         log_request_success:49   GET http://blackbox:9200/_cluster/state?filter_nodes=true&filter_routing_table=true&filter_indices=.marvel-2014.02.03&filter_index_templates=true&filter_blocks=true [status:200 request:0.008s]\n2014-02-10T06:55:29.303 INFO                _bloom_index:267  Skipping index .marvel-2014.02.03: Already closed.\n2014-02-10T06:55:29.303 INFO                  index_loop:290  Attempting to disable bloom filter for index .marvel-2014.02.04 because it is 5 days, 0:00:00 older than cutoff.\n2014-02-10T06:55:29.312 INFO         log_request_success:49   GET http://blackbox:9200/_cluster/state?filter_nodes=true&filter_routing_table=true&filter_indices=.marvel-2014.02.04&filter_index_templates=true&filter_blocks=true [status:200 request:0.008s]\n2014-02-10T06:55:29.376 INFO         log_request_success:49   PUT http://blackbox:9200/.marvel-2014.02.04/_settings [status:200 request:0.059s]\n2014-02-10T06:55:29.376 INFO                  index_loop:300  .marvel-2014.02.04: Successfully bloom filter disabled.\n2014-02-10T06:55:29.376 INFO                  index_loop:290  Attempting to disable bloom filter for index .marvel-2014.02.05 because it is 4 days, 0:00:00 older than cutoff.\n2014-02-10T06:55:29.385 INFO         log_request_success:49   GET http://blackbox:9200/_cluster/state?filter_nodes=true&filter_routing_table=true&filter_indices=.marvel-2014.02.05&filter_index_templates=true&filter_blocks=true [status:200 request:0.008s]\n2014-02-10T06:55:29.446 INFO         log_request_success:49   PUT http://blackbox:9200/.marvel-2014.02.05/_settings [status:200 request:0.057s]\n2014-02-10T06:55:29.447 INFO                  index_loop:300  .marvel-2014.02.05: Successfully bloom filter disabled.\n2014-02-10T06:55:29.447 INFO                  index_loop:290  Attempting to disable bloom filter for index .marvel-2014.02.06 because it is 3 days, 0:00:00 older than cutoff.\n2014-02-10T06:55:29.455 INFO         log_request_success:49   GET http://blackbox:9200/_cluster/state?filter_nodes=true&filter_routing_table=true&filter_indices=.marvel-2014.02.06&filter_index_templates=true&filter_blocks=true [status:200 request:0.008s]\n2014-02-10T06:55:31.644 INFO         log_request_success:49   PUT http://blackbox:9200/.marvel-2014.02.06/_settings [status:200 request:2.185s]\n2014-02-10T06:55:31.645 INFO                  index_loop:300  .marvel-2014.02.06: Successfully bloom filter disabled.\n2014-02-10T06:55:31.645 INFO                  index_loop:290  Attempting to disable bloom filter for index .marvel-2014.02.07 because it is 2 days, 0:00:00 older than cutoff.\n2014-02-10T06:55:31.653 INFO         log_request_success:49   GET http://blackbox:9200/_cluster/state?filter_nodes=true&filter_routing_table=true&filter_indices=.marvel-2014.02.07&filter_index_templates=true&filter_blocks=true [status:200 request:0.008s]\n2014-02-10T06:55:35.807 INFO         log_request_success:49   PUT http://blackbox:9200/.marvel-2014.02.07/_settings [status:200 request:4.150s]\n2014-02-10T06:55:35.807 INFO                  index_loop:300  .marvel-2014.02.07: Successfully bloom filter disabled.\n2014-02-10T06:55:35.807 INFO                  index_loop:290  Attempting to disable bloom filter for index .marvel-2014.02.08 because it is 1 day, 0:00:00 older than cutoff.\n2014-02-10T06:55:35.816 INFO         log_request_success:49   GET http://blackbox:9200/_cluster/state?filter_nodes=true&filter_routing_table=true&filter_indices=.marvel-2014.02.08&filter_index_templates=true&filter_blocks=true [status:200 request:0.008s]\n2014-02-10T06:55:40.046 INFO         log_request_success:49   PUT http://blackbox:9200/.marvel-2014.02.08/_settings [status:200 request:4.226s]\n2014-02-10T06:55:40.047 INFO                  index_loop:300  .marvel-2014.02.08: Successfully bloom filter disabled.\n2014-02-10T06:55:40.047 INFO        find_expired_indices:188  .marvel-2014.02.09 is 0:00:00 above the cutoff.\n2014-02-10T06:55:40.047 INFO        find_expired_indices:188  .marvel-2014.02.10 is 1 day, 0:00:00 above the cutoff.\n2014-02-10T06:55:40.047 INFO                  index_loop:301  DISABLE BLOOM FILTER FOR index operations completed.\n2014-02-10T06:55:40.047 INFO                        main:365  Done in 0:00:12.228052.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/elasticsearch/curator/issues/37#issuecomment-34627923\n.\n\n\nI prefer encrypted and signed messages. KeyID: B20A22F9\nFingerprint: 110A F423 3647 54E2 880F ADAD 1C52 85BF B20A 22F9\n\"It is not usually until you've built and used a version of the program\nthat you understand the issues well enough to get the design right.\" - Rob\nPike, Brian Kernighan.\nNo trees were harmed in the sending of this message, however, a number of\nelectrons were inconvenienced.\n. May I suggest a slight grammar improvement,\nIf you have the sentence read \"Values for --delete, --close, --bloom or --optimize must be > 0\" then it's a bit easier to comprehend quickly :-)\n-=david=-\nOn 10 February 2014 13:02, Aaron Mildenstein notifications@github.comwrote:\n\nClosed #37 https://github.com/elasticsearch/curator/issues/37.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/elasticsearch/curator/issues/37\n.\n\n\nI prefer encrypted and signed messages. KeyID: B20A22F9\nFingerprint: 110A F423 3647 54E2 880F ADAD 1C52 85BF B20A 22F9\n\"It is not usually until you've built and used a version of the program\nthat you understand the issues well enough to get the design right.\" - Rob\nPike, Brian Kernighan.\nNo trees were harmed in the sending of this message, however, a number of\nelectrons were inconvenienced.\n. ",
    "ghost": "More info: I'm seeing the same error when trying to disable the bloom filter.\n. > Not presently, but that can be a feature request for the newer 4.x branch.\nThat would be very nice. \n. The entry points don't work when you install as a virtualenv and then move the installation. That's how we need to get this to prod machines. Is there anyway to call it from the python scripts?. run_ scripts are not a part of the installation. I'll copy then over and see what happens. run_ scripts did the job. Thanks very much!. @untergeek \ncreation_date\nhttps://search-sf-logs-ytamvk4k5ct6wsnvrrzpttyzhi.us-east-1.es.amazonaws.com/logstash-iis-msgapp-2017.07.13/_settings?pretty\n{\n  \"logstash-iis-msgapp-2017.07.13\" : {\n    \"settings\" : {\n      \"index\" : {\n        \"creation_date\" : \"1501710194333\",\n        \"number_of_shards\" : \"5\",\n        \"number_of_replicas\" : \"1\",\n        \"uuid\" : \"Nt3njSgrSOGa9FX-Sle70A\",\n        \"version\" : {\n          \"created\" : \"5030299\"\n        },\n        \"provided_name\" : \"logstash-iis-msgapp-2017.07.13\"\n      }\n    }\n  }\n}\nCount:\nhttps://search-sf-logs-ytamvk4k5ct6wsnvrrzpttyzhi.us-east-1.es.amazonaws.com/logstash-iis-msgapp-2017.07.13/_count?pretty\n{\n  \"count\" : 34681,\n  \"_shards\" : {\n    \"total\" : 5,\n    \"successful\" : 5,\n    \"failed\" : 0\n  }\n}. @untergeek \n\n. @untergeek \nThank you for the kind response.  Can you please tell me the steps to correct it?. ",
    "HonzaKral": "This is due to wrong version of elasticsearch-py - you have 1.0 installed which is only compatible with elasticsearch 1.0. You need to manually install older version by running:\npip install elasticsearch==0.4.5\nWe will soon release a new version of curator that will do this for you automatically, then you will just have to make sure to run 0.6.X version of curator with elasticsearch 0.90 or curator 1.0 with elasticsearch 1.0\n. You can actually do this by calling curator twice with different --prefix values and different values.\nWould that work for your use case?\n. Hi,\nthanks for the report. This is a weird error with python packaging schema, nothing actually to do with the version of elasticsearch. Curator is failing here to install it's dependency - elasticsearch-py in version >=1.0.0. The script curator then checks whether this is installed so it fails. I have no idea why this happens, I just tried it exactly as you described and it works for me.\nCould you please provide more information about your environment: python version, are you using virtualenv?, what OS you are using?\nThanks\n. hmm, even on 2.6 everything still works for me. Time to get hold of some amazon linux.\nAs a work-around you should be able to just call the curator.py file in the curator directory in the curator repo directly, that will bypass the version checking and should work for you.\n. So I installed CentOS, when I try to install curator to virtualenv everything works fine: pip install git+https://github.com/elasticsearch/curator or pip install  https://github.com/elasticsearch/curator/tarball/master both work.\nIf I install it into the system (using sudo, no virtualenv) everything gets installed correctly but I get the message saying elasticsearch isn't installed when running curator, the workaround helps (just use curator.py from the package).\nIf you see error with incorrect version of elasticsearch you need to upgrade it manually using pip install --upgrade elasticsearch==1.0.0 (@kevinava).\nI am still investigating why does RHEL hate python so much and I will keep updating this ticket with new information/fixes.\nThanks for the reports and for your patience.\n. Hi Poolski,\nthis is an intended feature because elasticsearch cannot give us the size of a closed index.\nUnfortunately there isn't anything we can easily do there.\n. Thanks nick, I will write some tests for this before we merge it in.\n. I am afraid this is an issue with elasticsearch not allowing such long urls. The fix would be to use wildcards if possible or issue several requests in curator to make sure each one fits in the default limit.\n. Pushed as https://github.com/elastic/elasticsearch-py/commit/aed7ec0d030b14eaadaaceb0ce6279e7cfd3a961 release comming later today, need to make sure all is in place.\n. The only guess I have is the expand_wildcards option where it wouldn't pick up the content_* indices for some reason. No idea how that could happen though...\n. https://pypi.python.org/pypi/elasticsearch/2.3.0\nFor your testing and releasing pleasure.\n. > keep the last count indices in each matching group, rather than have to specify each prefix at a time\nExactly - if I have logs- and metrics- I don't want to have 2 different curator configs that contain the same logic (and that I will inevitably forget to update when I add another group). Essentially having the filter act upon groups as defined by prefix via something like itertools.groupby (0).\nI absolutely agree that it can be potentially disastrous so there are several steps we can take to limit the reach like explicitly white-list prefixes. I do think, however, that a strict warning in the docs and a very safe example (since I assume most people are like me and copy-paste the examples in docs) ought to be enough...\n0 - https://docs.python.org/3.5/library/itertools.html#itertools.groupby. It was definitely an accident that lead to this discovery :) but it took as some time to discover the real culprit because of the error message.\nThank you!. catch specific exceptions (NotFoundError), and also include them in the log if they occur\n. from itertools import chain\nallindices = chain.from_iterable(s['indices'] for s in allsnaps if 'snapshot' in s and s['state'] == 'SUCCESS')\nreturn set(i for i in allindices if i.startswith(prefix))\nthough is checking for the 'snapshot' key necessary?\n. it seems better to extract the index (or snapshot) extracting logic as one function and then feed the results to the loop detecting the above/below cutoff\ndef find_expired_data(indices, time_unit, unit_count, separator='.', ...):\n    ...\n. again, please just catch NotFoundError\n. at least log the exception, preferably also identify what exceptions might occur and catch those\n. just declare it as a keyword argument, give it a default value of None and be done with it. Or, better, pass the individual params as keyword args and declare them with reasonable defaults\n. yes, if I read it correctly the only expected error here is 404 when the repository doesn't exist.\nso except elasticsearch.NotFoundError:\n. ",
    "kimchy": "the burst of new mappings has been heavily optimized in 1.0, I don't think this is curator job to fix.\n. @jordansissel obviously, if its still a problem in 1.0.x, then we should heavily look into this again. The problem with copying mapping is that its tricky to provide for change easily...\n. obviously, a reminder, Elastic official cloud offering, https://www.elastic.co/cloud, doesn't suffer from this limitation of AWS and curator works out of the box with it. This is being helped with the fact that both the devs behind ES and @untergeek work for Elastic and can make sure this happens. Obviously any fork of curator is not officially supported by Elastic.\n. ",
    "seang-es": "max_version for Elasticsearch.  It was throwing errors when I ran it against 1.0.1 until I increased max_version manually in curator.py.\n. I think the problem came because I was using the instructions here: http://untergeek.com/2014/02/18/curator-managing-your-logstash-and-other-time-series-indices-in-elasticsearch-beyond-delete-and-optimize/\nI didn't specify 0.6.2, but 0.6.2 is what I got when I used 'pip install elasticsearch-curator'.\n. Ok, let's close this out then.\n. It just seems odd that the ones being skipped are INFO but the ones being acted on are DEBUG.  You would think action would lead to a log event, even a single line saying 'Closing index X', where a skip may not.  Does it make sense for the skips to be DEBUG as well?\n. ",
    "hogbinj": "Also need to update the elasticsearch-py from the master as well.   This worked for me.\nsudo pip install --no-deps --ignore-installed https://github.com/elasticsearch/elasticsearch-py/tarball/master\nsudo pip install --no-deps --ignore-installed https://github.com/elasticsearch/curator/tarball/master\n. ",
    "grayaii": "wow. that was an insanely fast response. thanks!\n. I have exactly the same version of python, and I'm on a aws ec2 instance too.\nOne block that I have is my ElasticSearch cluster is less than 1.0, so I use curator 0.6:\nInstall it:\npip install git+https://github.com/elasticsearch/curator.git@0.6\nVerify Version:\npython /usr/lib/python2.6/site-packages/curator/curator.py --version\ncurator.py 0.6.2\nRun it:\npython /usr/lib/python2.6/site-packages/curator/curator.py --host localhost -c 20 -d 45\n...[clipped to show exception]\n2014-05-12T17:19:20.410 INFO                  index_loop:297  Attempting to close index logstash-2014.03.28 because it is 25 days, 0:00:00 older than cutoff.\nTraceback (most recent call last):\n  File \"/usr/lib/python2.6/site-packages/curator/curator.py\", line 382, in \n    main()\n  File \"/usr/lib/python2.6/site-packages/curator/curator.py\", line 365, in main\n    index_loop(client, 'close', expired_indices, arguments.dry_run)\n  File \"/usr/lib/python2.6/site-packages/curator/curator.py\", line 301, in index_loop\n    skipped = op(client, index_name, _kwargs)\n  File \"/usr/lib/python2.6/site-packages/curator/curator.py\", line 249, in _close_index\n    if index_closed(client, index_name):\n  File \"/usr/lib/python2.6/site-packages/curator/curator.py\", line 244, in index_closed\n    filter_routing_table=True,\n  File \"/usr/lib/python2.6/site-packages/elasticsearch/client/utils.py\", line 70, in _wrapped\n    return func(_args, params=params, *kwargs)\nTypeError: state() got an unexpected keyword argument 'filter_blocks'\nIs this a new issue, or should it part of this one?  Is there a workaround for this?\n. Thanks for the quick reply. I am in fact using the full path to the curator.py file:\npython /usr/lib/python2.6/site-packages/curator/curator.py --host localhost -c 20 -d 45\nUnfortunately I get the  unexpected keyword argument 'filter_blocks' exception. \nCould it be that 0.6.2 installing the wrong version of elasticsearch module?\nThanks,\nAlex\n. No love.\nI uninstalled everything. Clean machine.\nNote that this does not work:\npip install elasticsearch-curator==0.6.2\nYou must use:\npip install git+https://github.com/elasticsearch/curator.git@0.6\nI still get that exception from above of:\nstate() got an unexpected keyword argument 'filter_blocks'\nI wonder if 0.6.2 is pulling in the wrong version of elasticsearch package from pip.\nAgain, thanks for your time in looking into this.\n. Thanks for this info!\nIt looks like pip is only allowing installs from 1.0.0, and nothing else, which is whypip install elasticsearch-curator==0.6.2 is not working :\nhttps://pypi.python.org/pypi/elasticsearch-curator/1.0.0\nAnyway, here is the output:\n[root@sensu-184-72-143-71 ec2-user]# python /usr/lib/python2.6/site-packages/curator/curator.py --version\ncurator.py 0.6.2\n[root@sensu-184-72-143-71 ec2-user]# pip show elasticsearch\n```\nName: elasticsearch\nVersion: 1.0.0\nLocation: /usr/lib/python2.6/site-packages\nRequires: urllib3\n```\nIt looks like pip is offering 2 versions of elasticsearch:\n[root@sensu-184-72-143-71 python2.6]# yolk -V elasticsearch\nelasticsearch 1.0.0\nelasticsearch 0.4.5\n. You sir, are correct! This is what I did:\n```\nRemove pip \"cache\"\nrm -rf /tmp/pip*\nInstall elasticsearch 0.4.5:\npip install elasticsearch==0.4.5\nInstall curator 0.6.2:\npip install git+https://github.com/elasticsearch/curator.git@0.6\n```\nVoila! It worked!  Thanks untergeek!\n. Yes. This issue can be closed. Thanks!\n. ",
    "CullyB": "Absolutely, I don't know how I missed that before; thank you!  And thanks for such a helpful tool\n\nOn Feb 26, 2014, at 5:07 PM, Honza Kr\u00e1l notifications@github.com wrote:\nYou can actually do this by calling curator twice with different --prefix values and different values.\nWould that work for your use case?\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "analytically": "A release would be great. ES 1.0.1 is out, so elasticsearch-curator should follow?\n. ",
    "akkie": "Hi,\nthanks for you answer.\nThe Python version is 2.6.9 and it runs on Amazon Linux 3.4.82-69.112.amzn1.x86_64 which is based on RHEL. The package virtualenv isn't installed.\nIf I run pip packages then it lists me the installed package elasticsearch (1.0.0). Is this the package you mentioned? This package gets installed when I run pip install . in the curator directory.\nThanks\n. OK, thanks the workaround works.\n. ",
    "msolujic": "Exactly same environment here as @akkie reported, just ES is bit older: 1.0.0 \nI did installation directly from tarball\nsudo pip install --upgrade  https://github.com/elasticsearch/curator/tarball/master\nsame errors.\n@HonzaKral  Thanks for workaround\n. ",
    "kevinava": "Hi, \nI have the same issue on Centos 6.5 with Python 2.6.6 and Elasticsearch 1.0.1 installed. \nPrevious version 0.6.x+ worked fine with ES 0.90.x.\nWhen I install  latest curator from master branch it wants to install an old 0.4.4 elasticsearch lib even if its requesting version >=1.0.0. and this gives me a host of errors when running curator, even if I use workaround.\nelasticsearch>=1.0.0,<2.0.0 (from elasticsearch-curator==1.0.0-dev), but installing version 0.4.4\nIf I run with @HonzaKral workaround I get the following error:\n```\n./curator.py --host localhost -c 31 -d 90\n2014-03-13T13:42:32.826 INFO                  index_loop:309  DELETE index operations completed.\n2014-03-13T13:42:32.826 INFO                        main:364  Closing indices older than 31 days...\n2014-03-13T13:42:32.828 INFO         log_request_success:49   GET http://localhost:9200/logstash-*/_settings?expand_wildcards=closed [status:200 request:0.002s]\n2014-03-13T13:42:32.836 INFO                  index_loop:298  Attempting to close index logstash-2014.02.04 because it is 6 days, 0:00:00 older than cutoff.\nTraceback (most recent call last):\n  File \"./curator.py\", line 383, in \n    main()\n  File \"./curator.py\", line 366, in main\n    index_loop(client, 'close', expired_indices, arguments.dry_run)\n  File \"./curator.py\", line 302, in index_loop\n    skipped = op(client, index_name, kwargs)\n  File \"./curator.py\", line 250, in _close_index\n    if index_closed(client, index_name):\n  File \"./curator.py\", line 245, in index_closed\n    metric='metadata',\n  File \"/usr/lib/python2.6/site-packages/elasticsearch/client/utils.py\", line 70, in _wrapped\n    return func(*args, params=params, kwargs)\nTypeError: state() got an unexpected keyword argument 'index'\n```\nI've tried git clone install, pip install from http etc, it all results in the same.\nExample full output from installation:\npip install . --upgrade\nUnpacking /root/curatorupg/curator\n  Running setup.py egg_info for package from file:///root/curatorupg/curator\n    warning: no previously-included files matching '__pycache__' found under directory '*'\n    warning: no previously-included files matching '*.py[co]' found under directory '*'\nDownloading/unpacking elasticsearch>=1.0.0,<2.0.0 (from elasticsearch-curator==1.0.0-dev)\n  Running setup.py egg_info for package elasticsearch\n    no previously-included directories found matching 'docs/_build'\n    no previously-included directories found matching 'test_elasticsearch'\n    warning: no previously-included files matching '__pycache__' found under directory '*'\n    warning: no previously-included files matching '*.py[co]' found under directory '*'\n  Requested elasticsearch>=1.0.0,<2.0.0 (from elasticsearch-curator==1.0.0-dev), but installing version 0.4.4\nDownloading/unpacking urllib3>=1.5,<2.0 from https://pypi.python.org/packages/source/u/urllib3/urllib3-1.8.tar.gz#md5=d664781c08f98a83f28df087b6edfdca (from elasticsearch>=1.0.0,<2.0.0->elasticsearch-curator==1.0.0-dev)\n  Running setup.py egg_info for package urllib3\nInstalling collected packages: elasticsearch, elasticsearch-curator, urllib3\n  Running setup.py install for elasticsearch\n    no previously-included directories found matching 'docs/_build'\n    no previously-included directories found matching 'test_elasticsearch'\n    warning: no previously-included files matching '__pycache__' found under directory '*'\n    warning: no previously-included files matching '*.py[co]' found under directory '*'\n  Found existing installation: elasticsearch-curator 1.0.0-dev\n    Uninstalling elasticsearch-curator:\n      Successfully uninstalled elasticsearch-curator\n  Running setup.py install for elasticsearch-curator\n    warning: no previously-included files matching '__pycache__' found under directory '*'\n    warning: no previously-included files matching '*.py[co]' found under directory '*'\n    Installing curator script to /usr/bin\n  Found existing installation: urllib3 1.7.1\n    Uninstalling urllib3:\n      Successfully uninstalled urllib3\n  Running setup.py install for urllib3\nSuccessfully installed elasticsearch elasticsearch-curator urllib3\nCleaning up...\n. Hi, again and thanks for grate support :+1: @HonzaKral \nI've installed a new elasticsearch cluster from scratch with CentOS 6.5, Elasticsearch 1.0.1 and Curator (Master). And I can report that curator installed with pip install https://github.com/elasticsearch/curator/tarball/master works like a charm on a clean CentOS/ES/Logstash/Curator install/setup. \nI don't even need to use the workaround.\nHowever curator won't work on our existing ES cluster which had ES 0.90.x and Curator 0.6.x.\nExisting ES cluster is working perfectly after upgrade from 0.90.x to 1.0.1, but Curator for some reason can't be reinstalled, installed, upgraded or run without errors. \nOn our setup it refuses to install elasticsearch-1.0.0 , and only installs 0.4.4 no matter what I do...\nManual upgrade doesn't help either, eg. pip install --upgrade elasticsearch==1.0.0.\n`Requested elasticsearch>=1.0.0,<2.0.0 (from elasticsearch-curator==1.0.0-dev), but installing version 0.4.4\nI'll also investigate some more and see if I can figure it out. \nCheers\n. Hi, I have now found a solution to \"our\" problem.\nIt has to do with some old lingering Curator / PIP  files that prevent us from upgrading/reinstalling Curator to current version. \nSo this is what I did to resolve the issue on CentoOS 6.5.\nEnter directory where Python's site-packages are installed.\ncd /usr/lib/python2.6/site-packages/\nNow uninstall previously installed Curator packages using pip and remove lingering files.\npip uninstall urllib3 elasticsearch elasticsearch_curator\nrm -rf curator elasticsearch_curator*\nThen remove all other possible pip and Curator files on your system.\nrm -rf ~/.pip\nrm -rf ~/curator\nrm -rf /tmp/pip-build-root*\nFinally use find /* -name curator to check if there are any lingering files from previously installed Curator and remove them.\nNow we can install Curator and run it without the workarond.\npip install https://github.com/elasticsearch/curator/tarball/master\n. ",
    "nwood888": "I think our issue was similar to what @kevinava described.  But I didn't need to go to those lengths.  I just did:\npip uninstall elasticsearch\npip install elasticsearch\nAnd now everything works.\n. ",
    "cleesmith": "I see the following:\n```\ncurator --host some.com -d 500 -n\n2014-03-24T22:14:00.416 INFO                        main:332  Job starting...\n2014-03-24T22:14:00.417 INFO                   _new_conn:172  Starting new HTTP connection (1): some.com\n2014-03-24T22:14:02.584 INFO         log_request_success:49   GET http://some.com:9200/ [status:200 request:2.167s]\nExpected Elasticsearch version range > 0.19.4 < 1.0.0\nERROR: Incompatible with version 1.0.1 of Elasticsearch.  Exiting.\npython --version\nPython 2.7.3\npip --version\npip 1.0 from /usr/lib/python2.7/dist-packages (python 2.7)\nelasticsearch-curator     - Tending your time-series indices in Elasticsearch\n  INSTALLED: 0.6.2 (latest)\n```\n. Thanks, good job, it looks like a useful program ... even if it isn't written in Ruby :-)\n. Just to be clear, what is meant by:\npip uninstall elasticsearch\n... as I installed elasticsearch by downloading the \".zip\" file (or the \".deb\" file for Ubuntu).\nIs it uninstalling elasticsearch, if not, what does it mean ?\nPlease, pardon my lack of python/pip knowledge.\n. I see, the \"elasticsearch\" module is a dependency for \"elasticsearch-curator\".\nJust curious, but is there a time line for adding snapshot/restore to curator ... I know it's \nvery new.\nThanks again and I will update if everything works now.\n. It works:\ncurator --host some.com -d 500 -n\n2014-03-25T18:30:48.765 INFO                        main:333  Job starting...\n2014-03-25T18:30:48.766 INFO                   _new_conn:172  Starting new HTTP connection (1): some.com\n2014-03-25T18:30:50.878 INFO         log_request_success:49   GET http://some.com:9200/ [status:200 request:2.112s]\n2014-03-25T18:30:50.879 INFO                        main:359  Deleting indices older than 500 days...\n2014-03-25T18:30:50.884 INFO         log_request_success:49   GET http://some.com:9200/logstash-*/_settings?expand_wildcards=closed [status:200 request:0.004s]\n2014-03-25T18:30:50.885 INFO                  index_loop:309  DELETE index operations completed.\n2014-03-25T18:30:50.885 INFO                        main:379  Done in 0:00:02.123951.\nI would \"pitch in\" and help with curator, but I'm an old C programmer and mostly use Ruby now with zero python experience.\nThanks again.\n. ",
    "Akshaykapoor": "Hi @untergeek @HonzaKral ,\nI have been trying to instal curator on my MAC OSX 10.9 following the pip install method but to no luck, I still get the same message,\nFile \"/usr/local/bin/curator\", line 5, in \n    from pkg_resources import load_entry_point\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources.py\", line 2603, in \n    working_set.require(requires)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources.py\", line 666, in require\n    needed = self.resolve(parse_requirements(requirements))\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources.py\", line 565, in resolve\n    raise DistributionNotFound(req)  # XXX put more info here\npkg_resources.DistributionNotFound: elasticsearch>=1.0.0,<2.0.0\nCan you help me out with this. \nAny help would be appreciated!\n. Hey @untergeek \nI did try these steps before and just to be sure, did it again and to no surprise it failed. So, I guess running it from /Library/Python/2.7/site-packages/curator/curator.py is the only option in my case as well. \nAnyways, thanks for the prompt reply!\n. ",
    "cdenneen": "Same here had to run it from curator.py.\n. OS X 10.9.4, curator 0.6.2 for ES 0.99\n. @untergeek thanks... yeah this is my old ES cluster still running... new one is lastest GA\n. @untergeek even seeing on Linux:\n[root@ctclnxmgr01 ~]# pip install elasticsearch==0.4.5\nRequirement already satisfied (use --upgrade to upgrade): elasticsearch==0.4.5 in /usr/lib/python2.6/site-packages\nCleaning up...\n[root@ctclnxmgr01 ~]# curator\nTraceback (most recent call last):\n  File \"/usr/bin/curator\", line 5, in \n    from pkg_resources import load_entry_point\n  File \"/usr/lib/python2.6/site-packages/pkg_resources.py\", line 2655, in \n    working_set.require(requires)\n  File \"/usr/lib/python2.6/site-packages/pkg_resources.py\", line 648, in require\n    needed = self.resolve(parse_requirements(requirements))\n  File \"/usr/lib/python2.6/site-packages/pkg_resources.py\", line 546, in resolve\n    raise DistributionNotFound(req)\npkg_resources.DistributionNotFound: elasticsearch>=0.4.4,<1.0.0\n[root@ctclnxmgr01 ~]# pip install elasticsearch==0.4.5\nRequirement already satisfied (use --upgrade to upgrade): elasticsearch==0.4.5 in /usr/lib/python2.6/site-packages\nCleaning up...\n. @untergeek \nsame with latest versions too:\n[root@ctclnxmgr02 ~]# pip install elasticsearch-curator\nDownloading/unpacking elasticsearch-curator\n  Downloading elasticsearch-curator-2.0.0.tar.gz\n  Running setup.py (path:/tmp/pip_build_root/elasticsearch-curator/setup.py) egg_info for package elasticsearch-curator\n    warning: no files found matching 'CHANGELOG'\n    warning: no previously-included files matching 'pycache' found under directory ''\n    warning: no previously-included files matching '.py[co]' found under directory ''\nDownloading/unpacking elasticsearch>=1.0.0,<2.0.0 (from elasticsearch-curator)\n  Downloading elasticsearch-1.2.0-py2.py3-none-any.whl (51kB): 51kB downloaded\nDownloading/unpacking argparse>=1.1.0 (from elasticsearch-curator)\n  Downloading argparse-1.2.1.tar.gz (69kB): 69kB downloaded\n  Running setup.py (path:/tmp/pip_build_root/argparse/setup.py) egg_info for package argparse\n    warning: no previously-included files matching '.pyc' found anywhere in distribution\n    warning: no previously-included files matching '.pyo' found anywhere in distribution\n    warning: no previously-included files matching '.orig' found anywhere in distribution\n    warning: no previously-included files matching '.rej' found anywhere in distribution\n    no previously-included directories found matching 'doc/_build'\n    no previously-included directories found matching 'env24'\n    no previously-included directories found matching 'env25'\n    no previously-included directories found matching 'env26'\n    no previously-included directories found matching 'env27'\nDownloading/unpacking urllib3>=1.8,<2.0 (from elasticsearch>=1.0.0,<2.0.0->elasticsearch-curator)\n  Downloading urllib3-1.9.1.tar.gz (171kB): 171kB downloaded\n  Running setup.py (path:/tmp/pip_build_root/urllib3/setup.py) egg_info for package urllib3\n    warning: no previously-included files matching '' found under directory 'docs/build'\nInstalling collected packages: elasticsearch-curator, elasticsearch, argparse, urllib3\n  Running setup.py install for elasticsearch-curator\n    warning: no files found matching 'CHANGELOG'\n    warning: no previously-included files matching 'pycache' found under directory ''\n    warning: no previously-included files matching '.py[co]' found under directory ''\n    Installing curator script to /usr/bin\n    Installing es_repo_mgr script to /usr/bin\n  Running setup.py install for argparse\n    warning: no previously-included files matching '.pyc' found anywhere in distribution\n    warning: no previously-included files matching '.pyo' found anywhere in distribution\n    warning: no previously-included files matching '.orig' found anywhere in distribution\n    warning: no previously-included files matching '.rej' found anywhere in distribution\n    no previously-included directories found matching 'doc/build'\n    no previously-included directories found matching 'env24'\n    no previously-included directories found matching 'env25'\n    no previously-included directories found matching 'env26'\n    no previously-included directories found matching 'env27'\n  Running setup.py install for urllib3\n    warning: no previously-included files matching '' found under directory 'docs/_build'\nSuccessfully installed elasticsearch-curator elasticsearch argparse urllib3\nCleaning up...\n[root@ctclnxmgr02 ~]# curator\nTraceback (most recent call last):\n  File \"/usr/bin/curator\", line 5, in \n    from pkg_resources import load_entry_point\n  File \"/usr/lib/python2.6/site-packages/pkg_resources.py\", line 2655, in \n    working_set.require(requires)\n  File \"/usr/lib/python2.6/site-packages/pkg_resources.py\", line 648, in require\n    needed = self.resolve(parse_requirements(requirements))\n  File \"/usr/lib/python2.6/site-packages/pkg_resources.py\", line 546, in resolve\n    raise DistributionNotFound(req)\npkg_resources.DistributionNotFound: elasticsearch>=1.0.0,<2.0.0\n[root@ctclnxmgr02 ~]# pip install elasticsearch\nRequirement already satisfied (use --upgrade to upgrade): elasticsearch in /usr/lib/python2.6/site-packages\nCleaning up...\n. CentOS... yes... and correct there as well... if I run the curator.py or curator_script.py on those 2 linux nodes with different version it works... but the bin wrapper doesn't want to work regardless\n. @untergeek i can create new issue for this if you'd like:\n2014-09-26T12:23:35.223 INFO         log_request_success:57   DELETE http://eslogs-1.domain.loc:9200/logstash-2014.09.07 [status:200 request:10.032s]\n2014-09-26T12:23:35.223 INFO                  index_loop:307  logstash-2014.09.07: Successfully deleted.\n2014-09-26T12:23:35.224 INFO                  index_loop:297  Attempting to delete index logstash-2014.09.08 because it is 4 days, 0:00:00 older than cutoff.\n2014-09-26T12:23:45.259 INFO         log_request_success:57   DELETE http://eslogs-1.domain.loc:9200/logstash-2014.09.08 [status:200 request:10.036s]\n2014-09-26T12:23:45.260 INFO                  index_loop:307  logstash-2014.09.08: Successfully deleted.\n2014-09-26T12:23:45.260 INFO                  index_loop:297  Attempting to delete index logstash-2014.09.09 because it is 3 days, 0:00:00 older than cutoff.\n2014-09-26T12:23:55.296 INFO         log_request_success:57   DELETE http://eslogs-1.domain.loc:9200/logstash-2014.09.09 [status:200 request:10.036s]\n2014-09-26T12:23:55.297 INFO                  index_loop:307  logstash-2014.09.09: Successfully deleted.\n2014-09-26T12:23:55.297 INFO                  index_loop:297  Attempting to delete index logstash-2014.09.10 because it is 2 days, 0:00:00 older than cutoff.\n2014-09-26T12:24:05.332 INFO         log_request_success:57   DELETE http://eslogs-1.domain.loc:9200/logstash-2014.09.10 [status:200 request:10.035s]\n2014-09-26T12:24:05.332 INFO                  index_loop:307  logstash-2014.09.10: Successfully deleted.\n2014-09-26T12:24:05.333 INFO                  index_loop:297  Attempting to delete index logstash-2014.09.11 because it is 1 day, 0:00:00 older than cutoff.\n2014-09-26T12:24:15.365 INFO         log_request_success:57   DELETE http://eslogs-1.domain.loc:9200/logstash-2014.09.11 [status:200 request:10.032s]\n2014-09-26T12:24:15.366 INFO                  index_loop:307  logstash-2014.09.11: Successfully deleted.\n2014-09-26T12:24:15.366 INFO        find_expired_indices:203  logstash-2014.09.12 is 0:00:00 above the cutoff.\n2014-09-26T12:24:15.366 INFO        find_expired_indices:203  logstash-2014.09.13 is 1 day, 0:00:00 above the cutoff.\n2014-09-26T12:24:15.366 INFO        find_expired_indices:203  logstash-2014.09.14 is 2 days, 0:00:00 above the cutoff.\n2014-09-26T12:24:15.366 INFO        find_expired_indices:203  logstash-2014.09.15 is 3 days, 0:00:00 above the cutoff.\n2014-09-26T12:24:15.367 INFO        find_expired_indices:203  logstash-2014.09.16 is 4 days, 0:00:00 above the cutoff.\n2014-09-26T12:24:15.367 INFO        find_expired_indices:203  logstash-2014.09.17 is 5 days, 0:00:00 above the cutoff.\n2014-09-26T12:24:15.367 INFO        find_expired_indices:203  logstash-2014.09.18 is 6 days, 0:00:00 above the cutoff.\n2014-09-26T12:24:15.367 INFO        find_expired_indices:203  logstash-2014.09.19 is 7 days, 0:00:00 above the cutoff.\n2014-09-26T12:24:15.367 INFO        find_expired_indices:203  logstash-2014.09.20 is 8 days, 0:00:00 above the cutoff.\n2014-09-26T12:24:15.367 INFO        find_expired_indices:203  logstash-2014.09.21 is 9 days, 0:00:00 above the cutoff.\n2014-09-26T12:24:15.368 INFO        find_expired_indices:203  logstash-2014.09.22 is 10 days, 0:00:00 above the cutoff.\n2014-09-26T12:24:15.368 INFO        find_expired_indices:203  logstash-2014.09.23 is 11 days, 0:00:00 above the cutoff.\n2014-09-26T12:24:15.368 INFO        find_expired_indices:203  logstash-2014.09.24 is 12 days, 0:00:00 above the cutoff.\n2014-09-26T12:24:15.368 INFO        find_expired_indices:203  logstash-2014.09.25 is 13 days, 0:00:00 above the cutoff.\n2014-09-26T12:24:15.368 INFO        find_expired_indices:203  logstash-2014.09.26 is 14 days, 0:00:00 above the cutoff.\n2014-09-26T12:24:15.369 INFO                  index_loop:308  DELETE index operations completed.\n2014-09-26T12:24:15.369 INFO                        main:363  Closing indices older than 2 days...\n2014-09-26T12:24:15.395 INFO         log_request_success:57   GET http://eslogs-1.domain.loc:9200/_settings [status:200 request:0.026s]\n2014-09-26T12:24:15.396 INFO                  index_loop:297  Attempting to close index logstash-2014.09.12 because it is 12 days, 0:00:00 older than cutoff.\nTraceback (most recent call last):\n  File \"/Library/Python/2.7/site-packages/curator/curator.py\", line 382, in <module>\n    main()\n  File \"/Library/Python/2.7/site-packages/curator/curator.py\", line 365, in main\n    index_loop(client, 'close', expired_indices, arguments.dry_run)\n  File \"/Library/Python/2.7/site-packages/curator/curator.py\", line 301, in index_loop\n    skipped = op(client, index_name, **kwargs)\n  File \"/Library/Python/2.7/site-packages/curator/curator.py\", line 249, in _close_index\n    if index_closed(client, index_name):\n  File \"/Library/Python/2.7/site-packages/curator/curator.py\", line 244, in index_closed\n    filter_routing_table=True,\n  File \"/Library/Python/2.7/site-packages/elasticsearch/client/utils.py\", line 68, in _wrapped\n    return func(*args, params=params, **kwargs)\nTypeError: state() got an unexpected keyword argument 'filter_blocks'\n. mismatch version of elasticsearch pip installed, downgraded to 0.4.5 to work with curator 0.6.2 and close worked... closing\n. I have the same thing... a few different *-write aliases that get associated with the index and rollover seems to only move one of them and not the other 2 causing a manual of those 2 to be removed from previous and add to the now rolled over index\n@Tedderouni have you had any luck finding a solution?\nFor example:\nmyindex-000001\naliases are\napp1-write\napp2-write\napp3-write\ncurator would only move the app1-write to myindex-000002 and app2-write and app3-write will be left on myindex-000001. But if you add all 3 to curator action then it could potentially end up in a worse situation with:\nmyindex-000001 - no alias\nmyindex-000002 - app1-write\nmyindex-000003 - app2-write\nmyindex-000004 - app3-write\nwhen in fact it just errors with the following:\n2019-01-31 17:40:10,131 ERROR     Failed to complete action: rollover.  <class 'elasticsearch.exceptions.RequestError'>: TransportError(400, 'resource_already_exists_exception', 'index [myindex-000002/Bkb72QrZSLyCJf4HivBUZQ] already exists'). ",
    "RJ3000": "Tried everything on this thread and had no luck with getting Curator to work, posting incase it helps anyone else\nCentOS release 6.5\nPython 2.6.6\nDifferent Errors included:\npkg_resources.DistributionNotFound: elasticsearch>=1.0.0,<2.0.0\npkg_resources.DistributionNotFound: argparse>=1.1.0\npkg_resources.DistributionNotFound: click>=3.3\nThis fixed the issue for us:\npip install -U setuptools\ncurator -v\n  curator 2.1.2\n. ",
    "wjimenez5271": "Ditto @RJ3000, pip install -U setuptools worked for me also on Centos 6.6\n. @untergeek thats good. I wonder if perhaps it should be mentioned in the README as well? I'll be happy to submit a PR...\n. OK good thought. \n. OK see https://github.com/elastic/curator/pull/361\n. Hi Aaron,\nThanks for the quick response. Yes I realize there are other methods to open indexes that are pretty easy to use, and thats great for opening a few indexes individually. The use case I was adding this for is when you've closed a substantial number of indexes (for example with curator), and you decide you want to open those same indexes again, it is tedious to open them individually. Also in case you close some by mistake, its nice to be able to perform the inverse of the same operation. \nCertainly this doesn't have as much value in the scheduled job scenario you mentioned, its more of for interactive use of curator. \nThanks again,\nWilliam\n. @untergeek @stuart-warren Would you re-consider if I changed the operator to be --newer-than? This feature is pretty important to my use of curator as I need to be able to open a range of indexes which is tedious to do individually (even using plugins like kopf and eshead). I understand the argument of writing something using the python API (indices.open()) to do that, but isn't that what curator is? \nThanks in advance for considering!\n. @untergeek no worries, just signed it\n. @untergeek oh cool, thanks!\n. ",
    "gbutt": "Sorry for posting here, my issue is different and very easy to fix. I'll create a separate issue for mine.\n. Here's another one I got while doing a delete:\nTraceback (most recent call last):\n  File \"/usr/bin/curator\", line 9, in \n    load_entry_point('elasticsearch-curator==1.1.0', 'console_scripts', 'curator')()\n  File \"/usr/lib/python2.6/site-packages/curator/curator.py\", line 669, in main\n    arguments.func(client, **argdict)\n  File \"/usr/lib/python2.6/site-packages/curator/curator.py\", line 538, in command_loop\n    logging.info(\"Beginning {} operations...\".format(command.upper()))\nValueError: zero length field name in format\n. Thanks for being awesome!\n. ",
    "nathanlburns": "Cool. Thanks for fixing so quickly. \n. ",
    "KhaoticMind": "@untergeek  installed curator with pip, but I still getting this problem.\n```\ncurator -t 7200 -o 1 -p \"syslog-\"  --max_num_segments 1\n\n2014-06-09T11:11:27.075 INFO                  index_loop:298  Attempting to optimize index syslog-2014.06.07 because it is 1 day, 0:00:00 older than cutoff.\n2014-06-09T11:11:27.080 INFO         log_request_success:49   GET http://localhost:9200/_cluster/state/metadata/syslog-2014.06.07 [status:200 request:0.004s]\n2014-06-09T11:11:27.098 INFO         log_request_success:49   GET http://localhost:9200/syslog-2014.06.07/_segments [status:200 request:0.003s]\n2014-06-09T11:11:27.105 INFO             _optimize_index:267  Optimizing index syslog-2014.06.07 to 2 segments per shard.  Please wait...\n2014-06-09T11:11:27.203 INFO         log_request_success:49   POST http://localhost:9200/syslog-2014.06.07/_optimize?max_num_segments=2 [status:200 request:0.098s]\ncurator --version\ncurator 1.0.0\n```\n. Perfect! Thanks.\n. ",
    "xinity": "any \"simple way\" to \"rename\" my current indices ? \n. just tried , deleting an alias doesn't delete the index itself :)\nlucky me i'm not in production mode, but very soon, so changing the timestamp of my indices will do no harm.\nbtw, having an option to define the timestamp format would be awesome!!\n. ",
    "deepakas": "I am having the same problem. My Flume Log Stash Serializer   create the index in the format raw_json-2014-05-12 . Also I had the same issue when I tried to delete marvel indices which has the format YYYY.MM.dd .\n/usr/lib/python2.6/site-packages/curator/curator.py  -d 3 -p .marvel\n2014-05-14T18:07:44.691 INFO                        main:333  Job starting...\n2014-05-14T18:07:44.692 INFO                   new_conn:180  Starting new HTTP connection (1): localhost\n2014-05-14T18:07:44.694 INFO         log_request_success:49   GET http://localhost:9200/ [status:200 request:0.003s]\n2014-05-14T18:07:44.695 INFO                        main:359  Deleting indices older than 3 days...\n2014-05-14T18:07:44.698 INFO         log_request_success:49   GET http://localhost:9200/.marvel/settings?expand_wildcards=closed [status:200 request:0.002s]\n2014-05-14T18:07:44.722 ERROR       find_expired_indices:201  Could not find a valid timestamp from the index: .marvel-2014.05.07\n2014-05-14T18:07:44.722 ERROR       find_expired_indices:201  Could not find a valid timestamp from the index: .marvel-2014.05.08\n2014-05-14T18:07:44.722 ERROR       find_expired_indices:201  Could not find a valid timestamp from the index: .marvel-2014.05.09\n2014-05-14T18:07:44.722 ERROR       find_expired_indices:201  Could not find a valid timestamp from the index: .marvel-2014.05.10\n2014-05-14T18:07:44.722 ERROR       find_expired_indices:201  Could not find a valid timestamp from the index: .marvel-2014.05.11\n2014-05-14T18:07:44.723 ERROR       find_expired_indices:201  Could not find a valid timestamp from the index: .marvel-2014.05.12\n2014-05-14T18:07:44.723 ERROR       find_expired_indices:201  Could not find a valid timestamp from the index: .marvel-2014.05.13\n2014-05-14T18:07:44.723 ERROR       find_expired_indices:201  Could not find a valid timestamp from the index: .marvel-2014.05.14\n2014-05-14T18:07:44.723 INFO                  index_loop:309  DELETE index operations completed.\n2014-05-14T18:07:44.723 INFO                        main:379  Done in 0:00:00.037963.\n[root@node05 github]# ls -ltr /var/lib/elasticsearch/bigdatadev/nodes/0/indices/.marvel\n/var/lib/elasticsearch/bigdatadev/nodes/0/indices/.marvel-2014.05.10:\ntotal 8\ndrwxr-xr-x. 2 elasticsearch elasticsearch 4096 May 10 01:00 _state\ndrwxr-xr-x. 5 elasticsearch elasticsearch 4096 May 10 01:00 0\n/var/lib/elasticsearch/bigdatadev/nodes/0/indices/.marvel-2014.05.11:\n. ",
    "faxm0dem": "this was helpful: I also had the issue while forgetting the -\n. You mean, apart from closing the index :-)\nThanks!\n. Duplicate of #150 ?\n. PS: setting indices to RO seems completely broken in ES: it completely disallows even READ metadata operations\n. curator, version 3.2.3\n. sorry, had forgotten about this one.\nI just ran a debug session, where can I send you the file (275K gzipped)?\n. Tomorrow\nDone! Thanks!\n. Thanks for digging into this. It gives me more confidence in curator :+1:\n. I can confirm this issue. whatever logging config you use, even debug, it stays silent. ",
    "email2liyang": "Thank you, it works with full path\npython /usr/lib/python2.6/site-packages/curator/curator.py --host localhost -d 30 -c 14\nyes, it's python 2.6 in aws ec2 \n```\n\nuname -a\nLinux xxxx 3.10.35-43.137.amzn1.x86_64 #1 SMP Wed Apr 2 09:36:59 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux\npython --version\nPython 2.6.9\npython -c 'import sys;print sys.path'\n['', '/usr/lib64/python26.zip', '/usr/lib64/python2.6', '/usr/lib64/python2.6/plat-linux2', '/usr/lib64/python2.6/lib-tk', '/usr/lib64/python2.6/lib-old', '/usr/lib64/python2.6/lib-dynload', '/usr/lib64/python2.6/site-packages', '/usr/lib64/python2.6/site-packages/PIL', '/usr/lib/python2.6/site-packages', '/usr/lib/python2.6/site-packages/setuptools-0.6c11-py2.6.egg-info']\n```\n. \n",
    "alex-leonhardt": "had the same issue on latest version, but was running python from CentOS 6.4 adn 6.5 -- I needed python-pip installed in order to make the /usr/bin/curator working (no other changes, all packages were installed via yum, not pip) ... strange days.\n. ",
    "ekamil": "Sorry for not answering earlier. I've changed prefix into re. This seems flexible enough, don't you think?\n. ",
    "joningle": "Aaron,  we need to pre-create daily indices with a specific mapping.  From what we could tell, there's no way to have ES automatically create a new index with the necessary mapping.  Our intent is that the curator would run on a daily basis, creating needed indices, adjusting the aliases, deleting/closing old indices.  Let us know if ES solves this particular use case.\n. @untergeek Our mapping is necessary for two reasons:\n1. we need to insure that certain longs are interpreted as date type\n2. we need to insure that certain string fields are not analyzed.\nHere is a snippet that shows both types in our mapping:\n{ \n \"properties\" : {\n     \"disposition\":{\"type\" : \"string\", \"index\" : \"not_analyzed\"},\n     \"beginDttm\":{\"type\" : \"date\", \"ignore_conflicts\" :true},\n     ...\n   }\n}\n. ",
    "cschellenger": "@untergeek The index templates look like exactly what we need for this particular feature. Is that available on 1.0.x or do we need to get upgraded to 1.1.x?\nThe next part that is related to this is automatically managing an alias: adding new indices to it and removing old ones from it. I'd like your input on that as well.\n. @untergeek We can't add non-existent indices to an alias, so even if you ignore the mapping piece, We want to precreate them so we can manage the aliases in advance.\n. @untergeek The precreate is necessary to do alias management in advance. If you'd prefer them to both be discussed in a new issue, that's fine with me.\n. OK, I'm satisfied that the alias at index creation combined with the template will work for what we need once we upgrade to 1.1.x. I think we will still need to automatically trim indices from the back of the alias using something like curator.\n. This provides a few things that we'd like to do with Curator:\n1. Support weekly time units in addition to days and hours.\n2. Support removal from an alias as a primary action.\n3. Refuse to run if not connected to a master node. This is primarily useful so we can have the same cron job across all our nodes but not have to worry about running concurrently and getting some spurious error.\n. I'm not afraid of a little refactoring, but do you have an idea of when I should rebase/merge my changes? Should I just rebase to your pending branch?\n. Rebase was messy, so I just merged.\n. Closing: Please review #86 instead.\n. Thanks for pulling in the feature! I look forward to getting back on the mainline.\n. Sure, sounds good.\n. @untergeek The elected master is exactly what I'm looking for. The use case is this: I have the same cron job running on all nodes, but I only want it to succeed on the elected master.\n. ",
    "dav3860": "thank you for this answer @untergeek this makes sense.\n. ",
    "dotmilk": "elasticsearch (1.0.0)\nelasticsearch-curator (1.0.0)\nCommand was\ncurator --host 127.0.0.1 -d 2\nFull output of command as follows\n2014-05-27T00:08:00.858 INFO                        main:333  Job starting...\n2014-05-27T00:08:00.859 INFO                   _new_conn:236  Starting new HTTP connection (1): 127.0.0.1\n2014-05-27T00:08:00.862 INFO         log_request_success:49   GET http://127.0.0.1:9200/ [status:200 request:0.003s]\nTraceback (most recent call last):\n  File \"/usr/bin/curator\", line 9, in <module>\n    load_entry_point('elasticsearch-curator==1.0.0', 'console_scripts', 'curator')()\n  File \"/usr/lib/python2.6/site-packages/curator/curator.py\", line 345, in main\n    version_number = get_version(client)\n  File \"/usr/lib/python2.6/site-packages/curator/curator.py\", line 163, in get_version\n    version = client.info()['version']['number']\nTypeError: string indices must be integers\n. Also I went into the repl to duplicate the process happening in code\n```\n\n\n\nclient = elasticsearch.Elasticsearch(host='127.0.0.1', port=9200, url_prefix='', timeout=30, use_ssl=False)\nclient.info()\nu'{\"OK\":{}}'\n```\n. How would i tell if it is client only? i'm using opscode/chef cookbook https://github.com/foxycoder/cookbook-elasticsearch\n\n\n\nIt completely works, i'm funneling logstash output into it, it's on a different machine than the logstash, so it isn't the one that logstash fires up. \nAnd it is definitely elasticsearch running on 9200.\n. ```\n$ curl -v localhost:9200\n Rebuilt URL to: localhost:9200/\n Hostname was NOT found in DNS cache\n   Trying 127.0.0.1...\n Connected to localhost (127.0.0.1) port 9200 (#0)\n\nGET / HTTP/1.1\nUser-Agent: curl/7.36.0\nHost: localhost:9200\nAccept: /\n< HTTP/1.1 200 OK\n< Content-Type: text/plain; charset=UTF-8\n< Content-Length: 9\n<\n Connection #0 to host localhost left intact\n{\"OK\":{}}\n.\n$ netstat -na | grep 92\ntcp        0      0 0.0.0.0:9200                0.0.0.0:                   LISTEN\ntcp        0      0 10.170.230.60:9200          10.188.182.251:19607        ESTABLISHED\n```\n\nand \nnetstat -tlnp | grep 92\ntcp        0      0 0.0.0.0:9200                0.0.0.0:*                   LISTEN      1467/java\n. $ curl localhost:9200/_cluster/stats?pretty\n{\n  \"timestamp\" : 1401421705219,\n  \"cluster_name\" : \"logstash\",\n  \"status\" : \"green\",\n  \"indices\" : {\n    \"count\" : 2,\n    \"shards\" : {\n      \"total\" : 20,\n      \"primaries\" : 10,\n      \"replication\" : 1.0,\n      \"index\" : {\n        \"shards\" : {\n          \"min\" : 10,\n          \"max\" : 10,\n          \"avg\" : 10.0\n        },\n        \"primaries\" : {\n          \"min\" : 5,\n          \"max\" : 5,\n          \"avg\" : 5.0\n        },\n        \"replication\" : {\n          \"min\" : 1.0,\n          \"max\" : 1.0,\n          \"avg\" : 1.0\n        }\n      }\n    },\n    \"docs\" : {\n      \"count\" : 5390609,\n      \"deleted\" : 0\n    },\n    \"store\" : {\n      \"size_in_bytes\" : 6950537568,\n      \"throttle_time_in_millis\" : 4888040\n    },\n    \"fielddata\" : {\n      \"memory_size_in_bytes\" : 0,\n      \"evictions\" : 0\n    },\n    \"filter_cache\" : {\n      \"memory_size_in_bytes\" : 0,\n      \"evictions\" : 0\n    },\n    \"id_cache\" : {\n      \"memory_size_in_bytes\" : 0\n    },\n    \"completion\" : {\n      \"size_in_bytes\" : 0\n    },\n    \"segments\" : {\n      \"count\" : 290,\n      \"memory_in_bytes\" : 137519218\n    },\n    \"percolate\" : {\n      \"total\" : 0,\n      \"time_in_millis\" : 0,\n      \"current\" : 0,\n      \"memory_size_in_bytes\" : -1,\n      \"memory_size\" : \"-1b\",\n      \"queries\" : 0\n    }\n  },\n  \"nodes\" : {\n    \"count\" : {\n      \"total\" : 2,\n      \"master_only\" : 0,\n      \"data_only\" : 0,\n      \"master_data\" : 2,\n      \"client\" : 0\n    },\n    \"versions\" : [ \"1.1.1\" ],\n    \"os\" : {\n      \"available_processors\" : 2,\n      \"mem\" : {\n        \"total_in_bytes\" : 7870529536\n      },\n      \"cpu\" : [ {\n        \"vendor\" : \"Intel\",\n        \"model\" : \"Xeon\",\n        \"mhz\" : 1795,\n        \"total_cores\" : 1,\n        \"total_sockets\" : 1,\n        \"cores_per_socket\" : 32,\n        \"cache_size_in_bytes\" : 20480,\n        \"count\" : 2\n      } ]\n    },\n    \"process\" : {\n      \"cpu\" : {\n        \"percent\" : 14\n      },\n      \"open_file_descriptors\" : {\n        \"min\" : 144,\n        \"max\" : 154,\n        \"avg\" : 149\n      }\n    },\n    \"jvm\" : {\n      \"max_uptime_in_millis\" : 282328522,\n      \"versions\" : [ {\n        \"version\" : \"1.7.0_55\",\n        \"vm_name\" : \"OpenJDK 64-Bit Server VM\",\n        \"vm_version\" : \"24.51-b03\",\n        \"vm_vendor\" : \"Oracle Corporation\",\n        \"count\" : 2\n      } ],\n      \"mem\" : {\n        \"heap_used_in_bytes\" : 2495761992,\n        \"heap_max_in_bytes\" : 4705353728\n      },\n      \"threads\" : 59\n    },\n    \"fs\" : {\n      \"total_in_bytes\" : 214643507200,\n      \"free_in_bytes\" : 207506128896,\n      \"available_in_bytes\" : 207506128896,\n      \"disk_reads\" : 211949,\n      \"disk_writes\" : 7807571,\n      \"disk_io_op\" : 8019520,\n      \"disk_read_size_in_bytes\" : 7604076544,\n      \"disk_write_size_in_bytes\" : 245330228736,\n      \"disk_io_size_in_bytes\" : 252934305280\n    },\n    \"plugins\" : [ {\n      \"name\" : \"http-basic-server-plugin\",\n      \"version\" : \"NA\",\n      \"description\" : \"HTTP Basic Server Plugin\",\n      \"jvm\" : true,\n      \"site\" : false\n    }, {\n      \"name\" : \"cloud-aws\",\n      \"version\" : \"2.1.1\",\n      \"description\" : \"Cloud AWS Plugin\",\n      \"jvm\" : true,\n      \"site\" : false\n    }, {\n      \"name\" : \"head\",\n      \"version\" : \"NA\",\n      \"description\" : \"No description found.\",\n      \"url\" : \"/_plugin/head/\",\n      \"jvm\" : false,\n      \"site\" : true\n    } ]\n  }\n}\n. Yes, just {\"OK\":{}}\n. I'm literally as baffled as you are. There doesn't seem to be anything in the cookbook that owuld be making it do this.\n. I'm fairly certain localhost is whitelisted otherwise that other query wouldn't work curl localhost:9200/_cluster/stats?pretty\n. But yes great detective work\n. Oh wait now i see what you're saying \n. Awesome i'm gonna go ahead and close this. Thanks for your awesome help.\n. ",
    "amulyas": "@untergeek is there a way I can use both Curator and http-basic-server-plugin at the same time \n. thanks @untergeek so the only way to secure elasticsearch is via nginx proxy or something .. I wish I can make this work but it seems ..it not trivial . is there a way I can pass version information to curator .. so it not depend on health check to get version \n. ",
    "Ramlalit": "@untergeek  i am aslo getting the same problem:\n  File \"/home/ram/Asiaville/asiaville-project/lib/python3.5/site-packages/django/template/base.py\", line 1046, in render\n    return render_value_in_context(output, context)\n  File \"/home/ram/Asiaville/asiaville-project/lib/python3.5/site-packages/django/template/base.py\", line 1024, in render_value_in_context\n    value = force_text(value)\n  File \"/home/ram/Asiaville/asiaville-project/lib/python3.5/site-packages/django/utils/encoding.py\", line 76, in force_text\n    s = six.text_type(s)\n  File \"/home/ram/Asiaville/asiaville-project/lib/python3.5/site-packages/elasticsearch/exceptions.py\", line 58, in str\n    cause = ', %r' % self.info['error']['root_cause'][0]['reason']\nTypeError: string indices must be integers\nbut i have not added plugin can you help me or anyone. output of curl -v localhost:9200/_clus\nter/stats?pretty\n   Trying 127.0.0.1...\n Connected to localhost (127.0.0.1) port 9200 (#0)\n\nGET /_cluster/stats?pretty HTTP/1.1\nHost: localhost:9200\nUser-Agent: curl/7.47.0\nAccept: /\n< HTTP/1.1 200 OK\n< Content-Type: application/json; charset=UTF-8\n< Content-Length: 2618\n< \n{\n  \"timestamp\" : 1551254433222,\n  \"cluster_name\" : \"elasticsearch\",\n  \"status\" : \"green\",\n  \"indices\" : {\n    \"count\" : 0,\n    \"shards\" : { },\n    \"docs\" : {\n      \"count\" : 0,\n      \"deleted\" : 0\n    },\n    \"store\" : {\n      \"size_in_bytes\" : 0,\n      \"throttle_time_in_millis\" : 0\n    },\n    \"fielddata\" : {\n      \"memory_size_in_bytes\" : 0,\n      \"evictions\" : 0\n    },\n    \"filter_cache\" : {\n      \"memory_size_in_bytes\" : 0,\n      \"evictions\" : 0\n    },\n    \"id_cache\" : {\n      \"memory_size_in_bytes\" : 0\n    },\n    \"completion\" : {\n      \"size_in_bytes\" : 0\n    },\n    \"segments\" : {\n      \"count\" : 0,\n      \"memory_in_bytes\" : 0,\n      \"index_writer_memory_in_bytes\" : 0,\n      \"index_writer_max_memory_in_bytes\" : 0,\n      \"version_map_memory_in_bytes\" : 0,\n      \"fixed_bit_set_memory_in_bytes\" : 0\n    },\n    \"percolate\" : {\n      \"total\" : 0,\n      \"time_in_millis\" : 0,\n      \"current\" : 0,\n      \"memory_size_in_bytes\" : -1,\n      \"memory_size\" : \"-1b\",\n      \"queries\" : 0\n    }\n  },\n  \"nodes\" : {\n    \"count\" : {\n      \"total\" : 1,\n      \"master_only\" : 0,\n      \"data_only\" : 0,\n      \"master_data\" : 1,\n      \"client\" : 0\n    },\n    \"versions\" : [ \"1.7.2\" ],\n    \"os\" : {\n      \"available_processors\" : 4,\n      \"mem\" : {\n        \"total_in_bytes\" : 16686645248\n      },\n      \"cpu\" : [ {\n        \"vendor\" : \"Intel\",\n        \"model\" : \"Core(TM) i5-7200U CPU @ 2.50GHz\",\n        \"mhz\" : 3100,\n        \"total_cores\" : 4,\n        \"total_sockets\" : 4,\n        \"cores_per_socket\" : 16,\n        \"cache_size_in_bytes\" : 3072,\n        \"count\" : 1\n      } ]\n    },\n    \"process\" : {\n      \"cpu\" : {\n        \"percent\" : 0\n      },\n      \"open_file_descriptors\" : {\n        \"min\" : 154,\n        \"max\" : 154,\n        \"avg\" : 154\n      }\n    },\n    \"jvm\" : {\n      \"max_uptime_in_millis\" : 6591021,\n      \"versions\" : [ {\n        \"version\" : \"1.8.0_201\",\n        \"vm_name\" : \"Java HotSpot(TM) 64-Bit Server VM\",\n        \"vm_version\" : \"25.201-b09\",\n        \"vm_vendor\" : \"Oracle Corporation\",\n        \"count\" : 1\n       \"count\" : 1\n      } ],\n      \"mem\" : {\n        \"heap_used_in_bytes\" : 50934312,\n        \"heap_max_in_bytes\" : 1038876672\n      },\n      \"threads\" : 44\n    },\n    \"fs\" : {\n      \"total_in_bytes\" : 970137522176,\n      \"free_in_bytes\" : 914553806848,\n      \"available_in_bytes\" : 865250025472,\n      \"disk_reads\" : 128774,\n      \"disk_writes\" : 319590,\n      \"disk_io_op\" : 448364,\n      \"disk_read_size_in_bytes\" : 2629964800,\n      \"disk_write_size_in_bytes\" : 11662929920,\n      \"disk_io_size_in_bytes\" : 14292894720,\n      \"disk_queue\" : \"0\",\n      \"disk_service_time\" : \"5.5\"\n    },\n    \"plugins\" : [ ]\n  }\n}\n* Connection #0 to host localhost left intact\n. @untergeek thanks for the reply, ok I will try. \n",
    "jsmouret": "Sure, no problem.\nA Usage>Logstash section would do?\n. Done. Thanks :)\n. ",
    "divanikus": "It would be nice to have custom date format. I'm using an app creating indexes with dd-mm-YYYY format, which makes curator completely useless in my case.\n. @jordansissel Yeah, I know, and thats rather simple. But I prefer to have support of such things in \"upstream\" :)\n. It's simple to hack it for my case, but not that simple to make more generic option...\n. Actually, it would be nice to have a generic cli tool for es, to be able to interact with it with unix shell comfortably. I mean something like\nbash\nes indices | grep -05- | xargs es close\nCurrently there is no such tool, most of present ones are just for one thing only, like es2unix or curator.\n. You can always make --host & --port options...\n. Yes, but typing so much curl is pain.\n. ",
    "bbuchacher": "There ya go :) \n. ",
    "timrwilliams": "Any timeline on when this will be merged on to master? Thanks.\n. ",
    "davidloubere": ":up:\n. ",
    "jipeprojects": ":+1: \n. ",
    "guanxudong": "+1\n. ",
    "pierrre": "+1\n. ",
    "bradvido": "Yeah i already fixed it for myself. I just wanted to file the bug report so it gets tracked.\n. I'll be looking forward to this in 2.0. I have the same problem with kibana-int (and other non time-based indices)\n. ",
    "ppf2": "Ah got it, makes sense.  Thx!\n. @untergeek Are we also adding shrink api support as part of this work for Curator 5?. Thanks for adding!. ",
    "suyograo": "LGTM, minor comments\n. LGTM\n. LGTM\n. Cant this be init'd to\ntarget_date = utc_now\nIt is all relative to now right?\n. Why not have a map from time units to format? You can then re-use it in other parts of the code. Also, one place to maintain\n. ",
    "mark-casey": "Is there any particular benefit of having STDERR as the output? I can open a new issue if desired, but thread resurrection seemed a less cluttered option in this case.\n. Aww, and I'd pulled together research and everything!   :)\nIn case what I found is useful for someone upgrading and getting this change (or for any in the future who are doubtful about it) I still want to mention a few findings...\nI tested a cronjob on Ubuntu 15.04 Vivid and RHEL7 that ran the date command and got results counter to what @dominikmueller saw. As expected it just printed the date to stdout, but I still got the cron emails. So it is likely that Curator will continue to email after this change if the output is not being redirected away. Obviously this may not hold true on some older releases or other distros, but it is better than the idea that this change will silence most people's output by default (which would be rather scary indeed).\nThere is also Chuck Houpt's Cronic (http://habilis.net/cronic/) and my spiritual successor to it \"Foilhat\" (repo is on this account) which can help you only get mail when there is output to stderr or the return code is non-zero.\nFinally, for any still uncertain I wanted to share a trivia (and there are many) that rsync prints usage to stderr when there are no arguments but prints almost identical usage output to stdout when the help flag is used (i.e.: because you asked them for help, the usage output is no longer an error).\n. ",
    "parkerd": "Awesome, thanks.\n. Alright, I suspected the current target use case might be more narrow than I need.  I'm interested in a more general approach of multiple snapshots for a single index that is not date based and cleaning those up based on time.\nThanks for the detail.\n. ",
    "digital-wonderland": "Ah, I see, thanks. Being able to use -p \\* with show made me think wildcards are supported (was trying to operate on all indices with one call).\nStill the handling is a bit strange, cause I have to use -p .marvel- since neither -p . nor -p .marvel works although the later two are working just fine with show (updated ticket title).\nI think if the handling of --prefix gets unified over all commands wildcards would be nice without doing much harm since one prolly would use show first to see on what indices it operates.\n. Heh, I feel stupid now :D As you might have guessed I misunderstood the meaning of --prefix (\"index must start with prefix\" vs \"index must be prefix + date\"). I must have missed that when skimming the wiki and relied more on the output of --help.\nRegarding index matching & date extraction I was wondering if you considered regular expressions?\nThat way one could do a find_matching_indices method that returns all indices that match \"^\" + prefix + \".*$\". show would just print that list and others would iterate over that list and try to do their respective operation.\nThe date string could be extracted with something like \"^(\" + prefix + \".*)(\\d{4}\\S\\d{2}\\S\\d{2})$\" (the second part has to be dynamically generated as well to adapt to --timestring).\nThis way all commands would behave consistently (regarding --prefix) and wildcards would come for free. \nI didn't look too much into the code but that should mostly be doable by adapting get_indices to behave like the above described find_matching_indices and changing the date parsing in find_expired_data / get_index_time.\nOTOH I understand your point, that this might get confusing and, since there is --dry-run I don't really need show to make sure ops are running on the correct indices.\nRegardless of the above, may I suggest to adapt the --help text for --prefix to make it explicit that $prefix + $date must match the whole index name (not sure about the proper wording)? The current one didn't make that obvious to me until you quoted the wiki.\n. I like your suggestion, makes it clear now. Also thanks a lot for your effort on #136 - seems like it covers all usecases one could think of, much appreciated :)\n. ",
    "neilschelly": "Another +1 from me.  We are also using logstash, but indexing into things like:\nlogstash-2014.08.15_category\nWe have index aliases that make logstash-2014.08.15_* available under the name logstash-2014.08.15, and use this as a means of keeping different category indices around for different time retention policies.  Not really different from what DanielRedOak said above, but using suffixes instead.\nHaving a --suffix option to correspond to --prefix and default to an empty string is probably the easiest solution to this.  This keeps backward compatibility, but means you need 3 arguments to define your index naming pattern now.  A new option that support globbing conventions like --pattern \"logstash-YYYY.mm.dd*\" might be cool, but definitely more difficult to implement and keep backward compatibility.\nEDIT: looks like this code as a --suffix option has already been committed to master for a 2.x release. Yay.\n. ",
    "t5unamie": "+1 from me. I want to be able to delete index's on the whole system and only keep the last 30 days regardless of the prefix name.\nsince Kibana3 has no concept of access control to specific logs.\n. K sorry, can someone outline how I can run a delete command to remove all index's in the last 30 days? \n. @untergeek Thanks mate\n. ",
    "murnieza": "I am talking not about all indexes but about this case \"delete all but the 3 most recent indices of each prefix\". To be more precise \"delete all but the 3 most recent indices with provided prefix\"\nIt should be used with any command like --older-than or --disk-space the only difference that there always will be at most 3 nodes after running lets say delete with --more-than 3 option.\nIs this functionality going to be in Curator 2.0 or I misunderstood you?\n. OK. I see what is confusing you. My indexes are not being created periodically. Sorry for not stating it at first.\nFor example I have following indexes: \nprefix-2014.07.11\nprefix-2014.08.01\nprefix-2014.08.02\nprefix-2014.08.07\nprefix-2014.08.09\nI need to keep last three releases so after running delete --prefix \"prefix-\" --more-than 3 I wish to have these indexes:\nprefix-2014.08.02\nprefix-2014.08.07\nprefix-2014.08.09\n. I am searching a tool for deleting not time-series indices. And curator is a tool I could almost use for my needs :) Will curator ever work with other than time-series indices?\nAnd my indices are not daily, I used daily ones just for example. Sometimes I have few a day and sometimes one in few days.\n. OK, no problem :)\n. ",
    "nheid": "if it's any help, i just updated to curator 1.2.2 and installed the available argparse via yum \nyum info python-argparse\nName        : python-argparse\nArch          : noarch\nVersion     : 1.2.1\nRelease    : 2.el6\nworks fine.\n. Hello, I hope you can give me some clarification on that fix.\nI just tried to snapshot kibana-int with the new feature set (version 2.0.2). Or non time based index in general.\nAs I understand (or maybe what I think i need is):\nsnapshot index_1 to index_1-TIMESTAMP every day.\nI don't seem to be able to source from a non time based index, even if I specify an empty --timestring\nOr maybe there is a different approach to solving this situation alltogether?\nAny help or clarification would be appreactiated.\n. ",
    "cassianoleal": "Thanks for the quick merge! :smile: \n. ",
    "bodgit": "0 works for me.\n. ",
    "evertonberz": "It works! Thanks!\n. ",
    "dontrebootme": "I had this same issue. Thanks for the solution!\nBreaks our automation workflow, is there a way to get the patch into pip as a new version?\n. Fair enough.\nI'm not privy to the shortcomings, I simply set curator up yesterday and this was my first attempt at using it. I figured disk space was better than some attempt to judge number of days/weeks.\n. Thank you for your helpfulness and effort on this project! Have a great day\n. ",
    "exklamationmark": "seems to be a mistake on io redirection on my end. Changed them to be\n/usr/local/bin/curator bloom --older-than 2 >> /var/log/opt/curator.log 2>&1 and it works\n. ",
    "sjdirect": "I had the same issue with crontab. Turns out crontab only allows so many characters in the command. Otherwise it truncates the command and tries to run it anyway. My solution was to have the crontab call a bash file that runs the curator command. Then everything starting working.\n. ",
    "geerlingguy": "Another thing I ran into was unescaped percent signs. I was using the command:\n/usr/local/bin/curator delete indices --older-than 10 --time-unit days --timestring '%Y.%m.%d'\nBut I had to escape the percent signs, like so:\n/usr/local/bin/curator delete indices --older-than 10 --time-unit days --timestring '\\%Y.\\%m.\\%d'\nNow my curator cron jobs run swimmingly!\n. ",
    "davmrtl": "@geerlingguy Thank you, it solved my problem too!\n. @untergeek I'll wait for news around that case, thank you.\n. ",
    "doberloh": "@geerlingguy This just solved my issue as well.\n. ",
    "nickolaysus": "@geerlingguy helped. Thanks!\n. ",
    "yami12376": "@geerlingguy  thanks. ",
    "nvidia99": "So I did a\nwget https://github.com/elasticsearch/curator/archive/master.zip -O elasticsearch-curator.zip\npip install elasticsearch-curator.zip\nand got curator 2.0.0-dev, but have exactly the same results. Am I doing something wrong ?\nThanks\n. Also the sorting looks wrong to me... is starting with the newest entries .\n. Thanks for explaining the sorting. I have 59 indices left now, one per day. I don't think any are closed, I didn't do it (or is it done automatically?). Anyway, how can I check it ? And if are empty, where is all the data stored ?\nI've installed head plugin and what i see is this for all the indices: \nlogstash-2014.08.22\nsize: 495B (495B)\ndocs: 0 (0)\n. all the logstash-xx.xx.xx indices looks the same as above. \nI've installed logstash, elasticsearch and kibana. I guess logstash created the indices, kibana should only read from the database...\nAt some point my hdd was getting full, so I've stopped logstash, then after a couple of days I found curator, which gave me that output... It looks like the indices are empty, but no idea where the data is.. The hdd is still full and if I use curator to delete old entries, logstash-xx.xx.xx old indices gets deleted and hdd space freed up..\n. Yes, it seems so. Thanks for the help.\ncheers\n. ",
    "andreamaruccia": "We've exact same disk for each member of the cluster: The 3 hosts have a dedicated disc of 20giga each.\nEach index has a dedicated disc space assigned:\nindexA : 10 giga\nindexB : 8 giga\nindexC:  2 giga\nThis is important because if index A is taking too much resources it should not penalize index B.\nA time based curator could not resolve this use case.\nDo you still think I would have the problem if I respect the rule of 3 same discs and I delete indexes by size and by prefix every time?\nThanks ;)\n. We collect logging information or application errors shared by multiple teams.\nImmagine a scenario where a team does not care much about the logs so they will abuse of the resources. In that case limiting disk space will guarantee that the abuse affects only them. (their error and log history will be very short in that case)\nIf the disk space allocated is big enough, a big flow of errors will not affect the history much, It would only if they don't remove log noise (which they can do by handling errors and log levels)\nWe delete by time as well because we don't care of too much old logs. Let me explain why cleaning by time is not enough for us:\nLet's say you've 20 gigabytes allocated for each clusternode. \nWe've the server configured to clean the logs older than one week:\nmonday it stores 3 giga\ntuesday it stores 3 giga\nwednesday it stores 5 giga\nthursday is stores 5 giga\nFriday 3 giga\nsaturday 1 giga\nsunday 1 gia\nIn this case the disk would be full on Friday (3+3+5+5+3=21), causing a disk space failure on the logserver.\n. As a matter of fact I've upgraded today! It worked fine thank you.\n. Ok thank you looking forward to it!\n. ",
    "AnonymMan": "Thanks a lot!\n. ",
    "cite": "150 talks about setting the whole index to read only, thereby e.g. preventing creation of new aliases to said index. This would be more like \"prevent new data from being added, so the index will stay 'optimized'\" (note the different parameters). Unless, of course, I did not misunderstand untergeek and this setting really doesn't have any effect.\n. ",
    "cembree": "Following up from #360 marked as duplicate.  Sorry, my search for read only didn't find this as readonly. :-/ \nOur case is for ES in a non-logstash config.  We're dumping a couple billion documents per hour to highly optimized indexes.   We did some extensive performance testing last summer w/ ES 1.3.x and found that after a few hours ES performance would begin to degrade quickly.  After a consultation w/ ES support, we tweaked the blocks.write and codec.bloom.load, ES was able to keep up again.   I think we peaked out around 700k dps.\n. Hm.  I do not.  We've seen a few items in 1.4 and 1.5.x that make us want to re-test, but I haven't had a chance to spin it up.\n. ",
    "stonith": "Probably not, I realized after submitting this that the API call is pretty simple in itself. I'll close this off.\n. ",
    "jb11211": "I'm looking into this. It's too bad Python does this as the RFC for syslog explicitly doesn't include a size limit.\nfrom the RFC5424 (which obsoletes the earlier rfc3164):\nSyslog message size limits are dictated by the syslog transport\n   mapping in use.  There is no upper limit per se.  Each transport\n   mapping defines the minimum maximum required message length support,\n   and the minimum maximum MUST be at least 480 octets in length.\nThe 1k maximum size is a hangover from syslog historically being transported over single UDP packets.\nLike I said, I'm looking into it.\n. ",
    "magnusbaeck": "We'd obviously have to catch the exception and perhaps truncate the message before retrying (not sure how we'd do that though \u2013 subclass SysLogHandler?), but if that only happens for debug-level messages at least I'd be fine with that. If you need debug-level logging you're probably not interested in logging to syslog anyway \u2013 it's typically things you want in your terminal or in a temporary local logfile. IMHO it's a reasonable limitation if properly documented.\n. Don't know how I managed to miss that bug. I thought I searched for \"logstash\".\nWait, what? You prefer having to manually insert json.dumps() calls for log messages that you think can contain characters that screw up the output?\nMaybe I'm too tired, but I don't see how it's supposed to work \u2013 json.dumps() obviously returns a double quoted, but the formatter's format string also includes literal double quotes for the message, resulting in double double quotes in the final rendered message. Here's an example message from a stock Curator 2.1.1 with --loglevel DEBUG and --logformat logstash, and it's not valid JSON:\n{\"@timestamp\":\"2015-01-13T23:35:10.623Z\", \"loglevel\":\"DEBUG\", \"name\":\"curator\", \"function\":\"filter_by_timestamp\", \"linenum\":\"375\", \"message\":\"\"Unable to match logstash-2014.07.31_ with regular expression ^logstash-(\\\\d{4}\\\\.\\\\d{2}\\\\.\\\\d{2})$.  Error: 'NoneType' object has no attribute 'group'\"\"}\n. Before I can publish a PR I need to know whether Curator is covered by the Elasticsearch CLA or if I need to get approval from my employer. I can't find anything indicating that Curator is covered but it would be surprising if it wasn't when Logstash and Elasticsearch itself are covered.\nBut, basically it's a ~15-line class descending from logging.Formatter that overrides format(), plus the obvious adjustments of the logging configuration in main(). Alternatively, we could choose to use github.com/exoscale/python-logstash-formatter if we think it's worth introducing a library dependency to avoid those 15 lines.\n. The ReadTimeoutError exception was introduced in commit 2074577 of github.com/shazow/urllib3 which ended up in 1.7.1 and all subsequent releases, including 1.10 that you're getting. Could you verify that there actually is a class ReadTimeoutError in your urllib3/exceptions.py? And if so, is that the file actually read by Python when running Curator (check with strace)?\n. ",
    "bobrik": "Wow! I think you should link wiki to this comment to make it clear for new folks.\nI think I'll use the next approach:\n- snapshot sessions-2014-06-10 contains index sessions-2014-06-10 and older indices\n- snapshot sessions-2014-06-11 contains index sessions-2014-06-11 and older indices\nThis way I'll have consistent view of my data for each day even though old data doesn't change. Not sure if my current backup is doing what I wanted because of elasticsearch/elasticsearch#7859.\n```\ndocker run -i -t --rm digitalwonderland/docker-elasticsearch-curator --host web487 -n snapshot --older-than 1 --prefix analytics- --timestring %Y-%m-%d --repository ceph_s3 --snapshot-name 'analytics-'$(date -d 'day ago' +%Y-%m-%d)\n2014-09-27 17:16:03,023 INFO      Job starting...\n2014-09-27 17:16:03,024 INFO      DRY RUN MODE.  No changes will be made.\n2014-09-27 17:16:03,024 INFO      Default timeout of 30 seconds is too low for command SNAPSHOT.  Overriding to 21,600 seconds (6 hours).\n2014-09-27 17:16:03,027 INFO      DRY RUN: Capturing snapshots of specified indices...\n2014-09-27 17:16:03,082 INFO      analytics-2014-09-27 is within the threshold period (1 days).\n2014-09-27 17:16:03,083 INFO      DRY RUN: Snapshots captured for specified indices.\n2014-09-27 17:16:03,083 INFO      Done in 0:00:00.069132.\n```\nLooks like filter_by_timestamp prints excluded indices, but in context they look like included. Maybe that's just my bad english, though.\nThank you for great explanation!\n. Yeah, but messages come in order:\n2014-09-27 17:16:03,027 INFO      DRY RUN: Capturing snapshots of specified indices...\n2014-09-27 17:16:03,082 INFO      analytics-2014-09-27 is within the threshold period (1 days).\n2014-09-27 17:16:03,083 INFO      DRY RUN: Snapshots captured for specified indices.\nAnd only analytics-2014-09-27 is printed after 'specified indices'. For me it looks like the only index that is going to be snapshotted is analytics-2014-09-27. No other indices are specified (printed) anyway. Maybe it's better to print included indices rather then excluded.\n. Cool. Setting indices read-only could be added as well.\n. @untergeek nice! Wiki need updating, though.\n. If it's not complete success, it shouldn't be exit code 0. I can't see other adequate way to automate things. Maybe I'm missing something.\n. @untergeek desire to keep bc is understandable. How about adding a flag to enable future behavior in the next release? That should not change anything that works already and would enable stuff that will be in 3.0.0 by default. Something like --exit-code-on-failure.\n. I meant that flag could be introduced in 2.1.0, while making correct exit codes default behavior from 3.0.0.\n. Ok, that makes sense.\n. Maybe :ship: new release with it?\n. Okay then, but turns out it doesn't work with previous month either:\ndocker run --rm docker.core.tf/elasticsearch-curator:2.0.0 --host statistics03.es.pretender.local snapshot --older-than 0 --time-unit months --prefix statistics-super-fast- --timestring %Y%m --repository ceph_s3 --snapshot-name statistics-super-fast-$(TZ=UTC date -d yesterday +%Y-%m-%d)\n/usr/bin/curator snapshot: error: expect one of --all-indices, --older-than, --most-recent, or --delete-older-than\nIf I use --older-than 1 it works, but:\n2014-10-31 08:24:13,342 INFO      Snapshot will capture indices: statistics-super-fast-201310, statistics-super-fast-201311, statistics-super-fast-201312, statistics-super-fast-201401, statistics-super-fast-201402, statistics-super-fast-201403, statistics-super-fast-201404, statistics-super-fast-201405, statistics-super-fast-201406, statistics-super-fast-201407, statistics-super-fast-201408\nThere is no 201409, but it is cold index that needs to be captured.\nWeird thing is that optimize works just fine with zero arg:\n2014-10-31 08:27:07,752 INFO      Job starting...\n2014-10-31 08:27:07,752 INFO      Default timeout of 30 seconds is too low for command OPTIMIZE.  Overriding to 21,600 seconds (6 hours).\n2014-10-31 08:27:07,755 INFO      Optimizing indices...\n2014-10-31 08:27:07,793 INFO      Skipping index statistics-super-fast-201310: Already optimized.\n2014-10-31 08:27:07,798 INFO      Skipping index statistics-super-fast-201311: Already optimized.\n2014-10-31 08:27:07,802 INFO      Skipping index statistics-super-fast-201312: Already optimized.\n2014-10-31 08:27:07,807 INFO      Skipping index statistics-super-fast-201401: Already optimized.\n2014-10-31 08:27:07,812 INFO      Skipping index statistics-super-fast-201402: Already optimized.\n2014-10-31 08:27:07,816 INFO      Skipping index statistics-super-fast-201403: Already optimized.\n2014-10-31 08:27:07,821 INFO      Skipping index statistics-super-fast-201404: Already optimized.\n2014-10-31 08:27:07,825 INFO      Skipping index statistics-super-fast-201405: Already optimized.\n2014-10-31 08:27:07,829 INFO      Skipping index statistics-super-fast-201406: Already optimized.\n2014-10-31 08:27:07,833 INFO      Skipping index statistics-super-fast-201407: Already optimized.\n2014-10-31 08:27:07,838 INFO      Skipping index statistics-super-fast-201408: Already optimized.\n2014-10-31 08:27:07,843 INFO      Skipping index statistics-super-fast-201409: Already optimized.\n2014-10-31 08:27:07,843 INFO      statistics-super-fast-201410 is within the threshold period (0 months).\n2014-10-31 08:27:07,843 INFO      Optimized specified indices.\n2014-10-31 08:27:07,844 INFO      Done in 0:00:00.101711.\nAnd about optimizing current index: it is not active for more than 5 minutes per day. The index is cold for 23 hours and 55 minutes per day. IO load for optimize is just fine, actually. Moreover, it happens nightly when there is no load happening.\n. I still don't get why 0 in --older-than works with optimize, but does not work with snapshot.\n. https://github.com/elasticsearch/curator/blob/4d16dd67a462e7ed45cbd1923962436f306b0ced/curator/curator_script.py#L299-L301\n```\n\n\n\nif 0:\n...     print \"this is not happening\"\n... else:\n...     print \"this is happening\"\n...\nthis is happening\n```\n\n\n\nSuch check is not performed for optimize. Maybe is None check should be used.\n. Nope, I don't want to delete indices. I want to optimize and snapshot them.\nThis does not explain why I can optimize cold index but cannot snapshot it. Seriously, why is this not possible? I am probably missing something. This is november and I can only snapshot september. What is wrong with october?\nweb540 ~ # docker run --rm docker.core.tf/elasticsearch-curator:2.0.0 --host statistics03.es.pretender.local snapshot --older-than 1 --time-unit months --prefix statistics-super-fast- --timestring %Y%m --repository ceph_s3 --snapshot-name statistics-super-fast-$(TZ=UTC date -d yesterday +%Y-%m-%d)\n2014-11-04 13:51:40,860 INFO      Job starting...\n2014-11-04 13:51:40,860 INFO      Default timeout of 30 seconds is too low for command SNAPSHOT.  Overriding to 21,600 seconds (6 hours).\n2014-11-04 13:51:40,863 INFO      Capturing snapshots of specified indices...\n2014-11-04 13:51:40,887 INFO      statistics-super-fast-201410 is within the threshold period (1 months).\n2014-11-04 13:51:40,887 INFO      statistics-super-fast-201411 is within the threshold period (1 months).\nweb540 ~ # docker run --rm docker.core.tf/elasticsearch-curator:2.0.0 --host statistics03.es.pretender.local snapshot --older-than 0 --time-unit months --prefix statistics-super-fast- --timestring %Y%m --repository ceph_s3 --snapshot-name statistics-super-fast-$(TZ=UTC date -d yesterday +%Y-%m-%d)\n/usr/bin/curator snapshot: error: expect one of --all-indices, --older-than, --most-recent, or --delete-older-than\n. @untergeek see #211, hope I didn't break anything.\n. I checked curator 3.0.1 and it seems to differ from 2.1.1:\n--older-than 0 --time-unit months, 3.0.1:\n2015-03-22 09:26:45,057 INFO      Job starting...\n2015-03-22 09:26:45,086 INFO      DRY RUN MODE.  No changes will be made.\n2015-03-22 09:26:45,086 INFO      The following indices would have been altered:\nstatistics-super-fast-201310\nstatistics-super-fast-201311\nstatistics-super-fast-201312\nstatistics-super-fast-201401\nstatistics-super-fast-201402\nstatistics-super-fast-201403\nstatistics-super-fast-201404\nstatistics-super-fast-201405\nstatistics-super-fast-201406\nstatistics-super-fast-201407\nstatistics-super-fast-201408\nstatistics-super-fast-201409\nstatistics-super-fast-201410\nstatistics-super-fast-201411\nstatistics-super-fast-201412\nstatistics-super-fast-201501\nstatistics-super-fast-201502\nstatistics-super-fast-201503\n--older-than 1 --time-unit months, 3.0.1:\n2015-03-22 09:27:50,795 INFO      Job starting...\n2015-03-22 09:27:50,863 INFO      DRY RUN MODE.  No changes will be made.\n2015-03-22 09:27:50,863 INFO      The following indices would have been altered:\nstatistics-super-fast-201310\nstatistics-super-fast-201311\nstatistics-super-fast-201312\nstatistics-super-fast-201401\nstatistics-super-fast-201402\nstatistics-super-fast-201403\nstatistics-super-fast-201404\nstatistics-super-fast-201405\nstatistics-super-fast-201406\nstatistics-super-fast-201407\nstatistics-super-fast-201408\nstatistics-super-fast-201409\nstatistics-super-fast-201410\nstatistics-super-fast-201411\nstatistics-super-fast-201412\nstatistics-super-fast-201501\n--older-than 0 --time-unit months, 2.1.1:\n2015-03-22 09:28:16,258 INFO      Job starting...\n2015-03-22 09:28:16,259 INFO      DRY RUN MODE.  No changes will be made.\n2015-03-22 09:28:16,259 INFO      Default timeout of 30 seconds is too low for command OPTIMIZE.  Overriding to 21,600 seconds (6 hours).\n2015-03-22 09:28:16,262 INFO      DRY RUN: Optimizing indices...\n2015-03-22 09:28:16,292 INFO      DRY RUN: of optimize_index operation on statistics-super-fast-201310\n2015-03-22 09:28:16,292 INFO      DRY RUN: of optimize_index operation on statistics-super-fast-201311\n2015-03-22 09:28:16,292 INFO      DRY RUN: of optimize_index operation on statistics-super-fast-201312\n2015-03-22 09:28:16,292 INFO      DRY RUN: of optimize_index operation on statistics-super-fast-201401\n2015-03-22 09:28:16,293 INFO      DRY RUN: of optimize_index operation on statistics-super-fast-201402\n2015-03-22 09:28:16,293 INFO      DRY RUN: of optimize_index operation on statistics-super-fast-201403\n2015-03-22 09:28:16,293 INFO      DRY RUN: of optimize_index operation on statistics-super-fast-201404\n2015-03-22 09:28:16,293 INFO      DRY RUN: of optimize_index operation on statistics-super-fast-201405\n2015-03-22 09:28:16,293 INFO      DRY RUN: of optimize_index operation on statistics-super-fast-201406\n2015-03-22 09:28:16,293 INFO      DRY RUN: of optimize_index operation on statistics-super-fast-201407\n2015-03-22 09:28:16,293 INFO      DRY RUN: of optimize_index operation on statistics-super-fast-201408\n2015-03-22 09:28:16,293 INFO      DRY RUN: of optimize_index operation on statistics-super-fast-201409\n2015-03-22 09:28:16,293 INFO      DRY RUN: of optimize_index operation on statistics-super-fast-201410\n2015-03-22 09:28:16,294 INFO      DRY RUN: of optimize_index operation on statistics-super-fast-201411\n2015-03-22 09:28:16,294 INFO      DRY RUN: of optimize_index operation on statistics-super-fast-201412\n2015-03-22 09:28:16,294 INFO      DRY RUN: of optimize_index operation on statistics-super-fast-201501\n2015-03-22 09:28:16,294 INFO      DRY RUN: of optimize_index operation on statistics-super-fast-201502\n2015-03-22 09:28:16,294 INFO      statistics-super-fast-201503 is within the threshold period (0 months).\n2015-03-22 09:28:16,294 INFO      DRY RUN: Optimized specified indices.\n2015-03-22 09:28:16,294 INFO      Done in 0:00:00.042380.\n--older-than 1 --time-unit months, 2.1.1:\n2015-03-22 09:28:43,362 INFO      Job starting...\n2015-03-22 09:28:43,362 INFO      DRY RUN MODE.  No changes will be made.\n2015-03-22 09:28:43,362 INFO      Default timeout of 30 seconds is too low for command OPTIMIZE.  Overriding to 21,600 seconds (6 hours).\n2015-03-22 09:28:43,366 INFO      DRY RUN: Optimizing indices...\n2015-03-22 09:28:43,389 INFO      DRY RUN: of optimize_index operation on statistics-super-fast-201310\n2015-03-22 09:28:43,389 INFO      DRY RUN: of optimize_index operation on statistics-super-fast-201311\n2015-03-22 09:28:43,389 INFO      DRY RUN: of optimize_index operation on statistics-super-fast-201312\n2015-03-22 09:28:43,389 INFO      DRY RUN: of optimize_index operation on statistics-super-fast-201401\n2015-03-22 09:28:43,389 INFO      DRY RUN: of optimize_index operation on statistics-super-fast-201402\n2015-03-22 09:28:43,389 INFO      DRY RUN: of optimize_index operation on statistics-super-fast-201403\n2015-03-22 09:28:43,389 INFO      DRY RUN: of optimize_index operation on statistics-super-fast-201404\n2015-03-22 09:28:43,390 INFO      DRY RUN: of optimize_index operation on statistics-super-fast-201405\n2015-03-22 09:28:43,390 INFO      DRY RUN: of optimize_index operation on statistics-super-fast-201406\n2015-03-22 09:28:43,390 INFO      DRY RUN: of optimize_index operation on statistics-super-fast-201407\n2015-03-22 09:28:43,390 INFO      DRY RUN: of optimize_index operation on statistics-super-fast-201408\n2015-03-22 09:28:43,390 INFO      DRY RUN: of optimize_index operation on statistics-super-fast-201409\n2015-03-22 09:28:43,390 INFO      DRY RUN: of optimize_index operation on statistics-super-fast-201410\n2015-03-22 09:28:43,390 INFO      DRY RUN: of optimize_index operation on statistics-super-fast-201411\n2015-03-22 09:28:43,390 INFO      DRY RUN: of optimize_index operation on statistics-super-fast-201412\n2015-03-22 09:28:43,390 INFO      DRY RUN: of optimize_index operation on statistics-super-fast-201501\n2015-03-22 09:28:43,390 INFO      statistics-super-fast-201502 is within the threshold period (1 months).\n2015-03-22 09:28:43,390 INFO      statistics-super-fast-201503 is within the threshold period (1 months).\n2015-03-22 09:28:43,391 INFO      DRY RUN: Optimized specified indices.\n2015-03-22 09:28:43,391 INFO      Done in 0:00:00.037166.\n2.1.1 is totally fine, but 3.0.1 includes statistics-super-fast-201503 (current month) or excludes statistics-super-fast-201502 (previous month). Let me know if that deserves a separate issue.\n. :fireworks: \n. Log message updated.\n. @untergeek :+1: , any eta for the next release?\n. Awesome, looking forward to it!\n. Got it, thanks!\n. ",
    "trompx": "Hello,\nI got the way elasticsearch store data as segments on the repository but so I have one more question, similar to http://elasticsearch-users.115913.n3.nabble.com/Delete-index-after-backup-td4070371.html but the answer does not appears obvious.\nSay I make a snapshot of my cluster : mycluster_2015-08-27 and 10 days later I remove some data from my cluster that the snapshot point to.\nHow is the segments persisted in S3 with the incremental nature of the backup process ? In the doc at https://www.elastic.co/guide/en/elasticsearch/guide/current/retiring-data.html, the \"Achiving Old Indices\" is not really detailing what is going on.\nSo can I be sure that as long as a snapshot exists on the repository, all indexes deleted in the cluster would still be restorable from S3 ?\nThank you for the clarification.\n. @untergeek \nThank you very much for the fast answer and precisions.\n. Hi there,\nI encounter the same problem. I use :\n- elasticsearch : 1.7.1\n- curator : 3.4.0 (same result with 3.2.3)\n- aws plugin : 2.7.1 (latest and version adviced for es 1.7.1)\nHere are the full log I get with --debug almost similar to pkr1234 logs :\nNov  6 11:36:31 atest docker/286fde13c77e[911]: Check if repository exists\nNov  6 11:36:32 atest docker/286fde13c77e[911]: Logs backup indices newer than 10 days at 2015-11-06_11-36-31\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,905 DEBUG          curator.cli.utils        filter_callback:189  REGEX = ^logs-.*$\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,905 DEBUG          curator.cli.utils        filter_callback:192  Added filter: {'pattern': '^logs-.*$'}\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,905 DEBUG          curator.cli.utils        filter_callback:193  New list of filters: [{'pattern': '^logs-.*$'}]\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,905 DEBUG         curator.api.filter         get_date_regex:158  regex = \\d{4}\\.\\d{2}\\.\\d{2}\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,905 DEBUG          curator.cli.utils        filter_callback:189  REGEX = (?P<date>\\d{4}\\.\\d{2}\\.\\d{2})\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,906 DEBUG          curator.cli.utils        filter_callback:192  Added filter: {'pattern': '(?P<date>\\\\d{4}\\\\.\\\\d{2}\\\\.\\\\d{2})', 'value': 10, 'groupname': 'date', 'time_unit': 'days', 'timestring': '%Y.%m.%d', 'method': 'newer_than'}\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,906 DEBUG          curator.cli.utils        filter_callback:193  New list of filters: [{'pattern': '^logs-.*$'}, {'pattern': '(?P<date>\\\\d{4}\\\\.\\\\d{2}\\\\.\\\\d{2})', 'value': 10, 'groupname': 'date', 'time_unit': 'days', 'timestring': '%Y.%m.%d', 'method': 'newer_than'}]\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,906 INFO      curator.cli.index_selection                indices:57   Job starting: snapshot indices\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,906 DEBUG     curator.cli.index_selection                indices:60   Params: {'url_prefix': '', 'http_auth': None, 'dry_run': False, 'certificate': None, 'loglevel': 'INFO', 'logformat': 'default', 'quiet': False, 'host': '192.168.33.105', 'timeout': 21600, 'debug': True, 'use_ssl': False, 'logfile': '/elasticsearch_curator.log', 'master_only': False, 'port': 9200, 'ssl_no_validate': False}\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,906 DEBUG          curator.cli.utils             get_client:112  kwargs = {'url_prefix': '', 'http_auth': None, 'dry_run': False, 'certificate': None, 'loglevel': 'INFO', 'quiet': False, 'debug': True, 'logformat': 'default', 'timeout': 21600, 'host': '192.168.33.105', 'use_ssl': False, 'logfile': '/elasticsearch_curator.log', 'master_only': False, 'port': 9200, 'ssl_no_validate': False}\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,906 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,906 INFO      urllib3.connectionpool              _new_conn:207  Starting new HTTP connection (1): 192.168.33.105\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,914 DEBUG     urllib3.connectionpool          _make_request:387  \"GET / HTTP/1.1\" 200 343\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,915 INFO               elasticsearch    log_request_success:63   GET http://192.168.33.105:9200/ [status:200 request:0.009s]\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,915 DEBUG              elasticsearch    log_request_success:65   > None\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,915 DEBUG              elasticsearch    log_request_success:66   < {\nNov  6 11:36:32 atest docker/286fde13c77e[911]:   \"status\" : 200,\nNov  6 11:36:32 atest docker/286fde13c77e[911]:   \"name\" : \"atest.com\",\nNov  6 11:36:32 atest docker/286fde13c77e[911]:   \"cluster_name\" : \"elasticsearch\",\nNov  6 11:36:32 atest docker/286fde13c77e[911]:   \"version\" : {\nNov  6 11:36:32 atest docker/286fde13c77e[911]:     \"number\" : \"1.7.1\",\nNov  6 11:36:32 atest docker/286fde13c77e[911]:     \"build_hash\" : \"b88f43fc40b0bcd7f173a1f9ee2e97816de80b19\",\nNov  6 11:36:32 atest docker/286fde13c77e[911]:     \"build_timestamp\" : \"2015-07-29T09:54:16Z\",\nNov  6 11:36:32 atest docker/286fde13c77e[911]:     \"build_snapshot\" : false,\nNov  6 11:36:32 atest docker/286fde13c77e[911]:     \"lucene_version\" : \"4.10.4\"\nNov  6 11:36:32 atest docker/286fde13c77e[911]:   },\nNov  6 11:36:32 atest docker/286fde13c77e[911]:   \"tagline\" : \"You Know, for Search\"\nNov  6 11:36:32 atest docker/286fde13c77e[911]: }\nNov  6 11:36:32 atest docker/286fde13c77e[911]:\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,918 DEBUG          curator.cli.utils          check_version:90   Detected Elasticsearch version 1.7.1\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,918 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,920 DEBUG     urllib3.connectionpool          _make_request:387  \"GET /_all/_settings?expand_wildcards=open%2Cclosed HTTP/1.1\" 200 4651\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,920 INFO               elasticsearch    log_request_success:63   GET http://192.168.33.105:9200/_all/_settings?expand_wildcards=open%2Cclosed [status:200 request:0.002s]\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,920 DEBUG              elasticsearch    log_request_success:65   > None\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,920 DEBUG              elasticsearch    log_request_success:66   < {\"logs-nginx-access-2015.10.30\":{\"settings\":{\"index\":{\"creation_date\":\"1446209966103\",\"number_of_shards\":\"5\",\"uuid\":\"94KEjOorQT6v3qOxCneDXA\",\"version\":{\"created\":\"1070199\"},\"number_of_replicas\":\"1\"}}}}\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,922 DEBUG          curator.api.utils            get_indices:28   All indices: [u'logs-nginx-access-2015.10.30']\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,926 DEBUG     curator.cli.index_selection                indices:76   Full list of indices: [u'logs-nginx-access-2015.10.30']\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,926 DEBUG     curator.cli.index_selection                indices:98   All filters: [{'pattern': '^logs-.*$'}, {'pattern': '(?P<date>\\\\d{4}\\\\.\\\\d{2}\\\\.\\\\d{2})', 'value': 10, 'groupname': 'date', 'time_unit': 'days', 'timestring': '%Y.%m.%d', 'method': 'newer_than'}]\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,926 DEBUG     curator.cli.index_selection                indices:103  Filter: {'pattern': '^logs-.*$'}\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,926 DEBUG     curator.cli.index_selection                indices:103  Filter: {'pattern': '(?P<date>\\\\d{4}\\\\.\\\\d{2}\\\\.\\\\d{2})', 'value': 10, 'groupname': 'date', 'time_unit': 'days', 'timestring': '%Y.%m.%d', 'method': 'newer_than'}\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,929 DEBUG         curator.api.filter        timestamp_check:301  Timestamp \"2015.10.28\" is outside the cutoff period (newer than 10 days).\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,929 DEBUG         curator.api.filter        timestamp_check:301  Timestamp \"2015.10.27\" is outside the cutoff period (newer than 10 days).\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,934 DEBUG         curator.api.filter        timestamp_check:301  Timestamp \"2015.10.28\" is outside the cutoff period (newer than 10 days).\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,934 DEBUG         curator.api.filter        timestamp_check:301  Timestamp \"2015.10.28\" is outside the cutoff period (newer than 10 days).\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,934 INFO      curator.cli.index_selection                indices:144  Action snapshot will be performed on the following indices: [u'logs-nginx-access-2015.10.30']\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,934 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,938 DEBUG     urllib3.connectionpool          _make_request:387  \"GET / HTTP/1.1\" 200 343\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,938 INFO               elasticsearch    log_request_success:63   GET http://192.168.33.105:9200/ [status:200 request:0.004s]\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,938 DEBUG              elasticsearch    log_request_success:65   > None\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,938 DEBUG              elasticsearch    log_request_success:66   < {\nNov  6 11:36:32 atest docker/286fde13c77e[911]:   \"status\" : 200,\nNov  6 11:36:32 atest docker/286fde13c77e[911]:   \"name\" : \"atest.com\",\nNov  6 11:36:32 atest docker/286fde13c77e[911]:   \"cluster_name\" : \"elasticsearch\",\nNov  6 11:36:32 atest docker/286fde13c77e[911]:   \"version\" : {\nNov  6 11:36:32 atest docker/286fde13c77e[911]:     \"number\" : \"1.7.1\",\nNov  6 11:36:32 atest docker/286fde13c77e[911]:     \"build_hash\" : \"b88f43fc40b0bcd7f173a1f9ee2e97816de80b19\",\nNov  6 11:36:32 atest docker/286fde13c77e[911]:     \"build_timestamp\" : \"2015-07-29T09:54:16Z\",\nNov  6 11:36:32 atest docker/286fde13c77e[911]:     \"build_snapshot\" : false,\nNov  6 11:36:32 atest docker/286fde13c77e[911]:     \"lucene_version\" : \"4.10.4\"\nNov  6 11:36:32 atest docker/286fde13c77e[911]:   },\nNov  6 11:36:32 atest docker/286fde13c77e[911]:   \"tagline\" : \"You Know, for Search\"\nNov  6 11:36:32 atest docker/286fde13c77e[911]: }\nNov  6 11:36:32 atest docker/286fde13c77e[911]:\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,939 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,947 DEBUG     urllib3.connectionpool          _make_request:387  \"GET /_snapshot/_status HTTP/1.1\" 200 16\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,947 INFO               elasticsearch    log_request_success:63   GET http://192.168.33.105:9200/_snapshot/_status [status:200 request:0.008s]\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,947 DEBUG              elasticsearch    log_request_success:65   > None\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,947 DEBUG              elasticsearch    log_request_success:66   < {\"snapshots\":[]}\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,948 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,951 DEBUG     urllib3.connectionpool          _make_request:387  \"GET / HTTP/1.1\" 200 343\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,951 INFO               elasticsearch    log_request_success:63   GET http://192.168.33.105:9200/ [status:200 request:0.003s]\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,951 DEBUG              elasticsearch    log_request_success:65   > None\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,951 DEBUG              elasticsearch    log_request_success:66   < {\nNov  6 11:36:32 atest docker/286fde13c77e[911]:   \"status\" : 200,\nNov  6 11:36:32 atest docker/286fde13c77e[911]:   \"name\" : \"atest.com\",\nNov  6 11:36:32 atest docker/286fde13c77e[911]:   \"cluster_name\" : \"elasticsearch\",\nNov  6 11:36:32 atest docker/286fde13c77e[911]:   \"version\" : {\nNov  6 11:36:32 atest docker/286fde13c77e[911]:     \"number\" : \"1.7.1\",\nNov  6 11:36:32 atest docker/286fde13c77e[911]:     \"build_hash\" : \"b88f43fc40b0bcd7f173a1f9ee2e97816de80b19\",\nNov  6 11:36:32 atest docker/286fde13c77e[911]:     \"build_timestamp\" : \"2015-07-29T09:54:16Z\",\nNov  6 11:36:32 atest docker/286fde13c77e[911]:     \"build_snapshot\" : false,\nNov  6 11:36:32 atest docker/286fde13c77e[911]:     \"lucene_version\" : \"4.10.4\"\nNov  6 11:36:32 atest docker/286fde13c77e[911]:   },\nNov  6 11:36:32 atest docker/286fde13c77e[911]:   \"tagline\" : \"You Know, for Search\"\nNov  6 11:36:32 atest docker/286fde13c77e[911]: }\nNov  6 11:36:32 atest docker/286fde13c77e[911]:\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,952 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,973 DEBUG     urllib3.connectionpool          _make_request:387  \"GET /_cat/indices/logs-nginx-access-2015.10.30?h=status&format=json HTTP/1.1\" 200 19\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,973 INFO               elasticsearch    log_request_success:63   GET http://192.168.33.105:9200/_cat/indices/logs-nginx-access-2015.10.30?h=status&format=json [status:200 request:0.021s]\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,973 DEBUG              elasticsearch    log_request_success:65   > None\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,973 DEBUG              elasticsearch    log_request_success:66   < [{\"status\":\"open\"}]\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,973 DEBUG          curator.api.utils   prune_open_or_closed:328  Including indexlogs-nginx-access-2015.10.30: Opened.\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,973 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,975 DEBUG     urllib3.connectionpool          _make_request:387  \"GET / HTTP/1.1\" 200 343\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,975 INFO               elasticsearch    log_request_success:63   GET http://192.168.33.105:9200/ [status:200 request:0.002s]\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,975 DEBUG              elasticsearch    log_request_success:65   > None\nNov  6 11:36:32 atest docker/286fde13c77e[911]: 2015-11-06 11:36:31,975 DEBUG              elasticsearch    log_request_success:66   < {\nNov  6 11:36:32 atest docker/286fde13c77e[911]:   \"status\" : 200,\nNov  6 11:36:32 atest docker/286fde13c77e[911]:   \"name\" : \"atest.com\",\nNov  6 11:36:32 atest docker/286fde13c77e[911]:   \"cluster_name\" : \"elasticsearch\",\nNov  6 11:36:32 atest docker/286fde13c77e[911]:   \"version\" : {\nNov  6 11:36:32 atest docker/286fde13c77e[911]:     \"number\" : \"1.7.1\",\nNov  6 11:36:32 atest docker/286fde13c77e[911]:     \"build_hash\" : \"b88f43fc40b0bcd7f173a1f9ee2e97816de80b19\",\nNov  6 11:36:32 atest docker/286fde13c77e[911]:     \"build_timestamp\" : \"2015-07-29T09:54:16Z\",\nNov  6 11:36:32 atest docker/286fde13c77e[911]:     \"build_snapshot\" : false,\nNov  6 11:36:32 atest docker/286fde13c77e[911]:     \"lucene_version\" : \"4.10.4\"\nNov  6 11:36:32 atest docker/286fde13c77e[911]:   },\nNov  6 11:36:32 atest docker/286fde13c77e[911]:   \"tagline\" : \"You Know, for Search\"\nNov  6 11:36:32 atest docker/286fde13c77e[911]: }\nNov  6 11:36:32 atest docker/286fde13c77e[911]:\nNov  6 11:36:34 atest docker/286fde13c77e[911]: 2015-11-06 11:36:33,983 DEBUG     urllib3.connectionpool          _make_request:387  \"POST /_snapshot/atest_backups/_verify HTTP/1.1\" 200 58\nNov  6 11:36:34 atest docker/286fde13c77e[911]: 2015-11-06 11:36:33,984 INFO               elasticsearch    log_request_success:63   POST http://192.168.33.105:9200/_snapshot/atest_backups/_verify [status:200 request:0.962s]\nNov  6 11:36:34 atest docker/286fde13c77e[911]: 2015-11-06 11:36:33,984 DEBUG              elasticsearch    log_request_success:65   > None\nNov  6 11:36:34 atest docker/286fde13c77e[911]: 2015-11-06 11:36:33,984 DEBUG              elasticsearch    log_request_success:66   < {\"nodes\":{\"U87Xhq2TTvyhaR9fVvgLVQ\":{\"name\":\"atest.com\"}}}\nNov  6 11:36:34 atest docker/286fde13c77e[911]: 2015-11-06 11:36:33,984 DEBUG       curator.api.snapshot        create_snapshot:62   Nodes with verified repository access: {u'U87Xhq2TTvyhaR9fVvgLVQ': {u'name': u'atest.com'}}\nNov  6 11:36:34 atest docker/286fde13c77e[911]: 2015-11-06 11:36:33,984 INFO        curator.api.snapshot        create_snapshot:70   Snapshot name: logs-20151106113633\nNov  6 11:36:34 atest docker/286fde13c77e[911]: 2015-11-06 11:36:33,984 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\nNov  6 11:36:35 atest docker/286fde13c77e[911]: 2015-11-06 11:36:35,038 DEBUG     urllib3.connectionpool          _make_request:387  \"GET /_snapshot/atest_backups/_all HTTP/1.1\" 200 5031\nNov  6 11:36:35 atest docker/286fde13c77e[911]: 2015-11-06 11:36:35,038 INFO               elasticsearch    log_request_success:63   GET http://192.168.33.105:9200/_snapshot/atest_backups/_all [status:200 request:1.054s]\nNov  6 11:36:35 atest docker/286fde13c77e[911]: 2015-11-06 11:36:35,038 DEBUG              elasticsearch    log_request_success:65   > None\nNov  6 11:36:35 atest docker/286fde13c77e[911]: 2015-11-06 11:36:35,039 DEBUG              elasticsearch    log_request_success:66   < {\"snapshots\":[\n{{\"snapshot\":\"logs-20151105175858\",\"version_id\":1070199,\"version\":\"1.7.1\",\"indices\":[\"logs-nginx-access-2015.10.30\"],\"state\":\"SUCCESS\",\"start_time\":\"2015-11-05T17:58:59.609Z\",\"start_time_in_millis\":1446746339609,\"end_time\":\"2015-11-05T18:03:19.815Z\",\"end_time_in_millis\":1446746599815,\"duration_in_millis\":260206,\"failures\":[],\"shards\":{\"total\":5,\"failed\":0,\"successful\":5}}}]}\nNov  6 11:36:35 atest docker/286fde13c77e[911]: 2015-11-06 11:36:35,040 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\nNov  6 11:36:43 atest kernel: [  990.428805] docker0: port 12(vethf2357a6) entered forwarding state\nNov  6 11:37:37 atest docker/286fde13c77e[911]: 2015-11-06 11:37:35,043 WARNING            elasticsearch       log_request_fail:82   PUT http://192.168.33.105:9200/_snapshot/atest_backups/logs-20151106113633?wait_for_completion=true [status:N/A request:60.003s]\nNov  6 11:37:37 atest docker/286fde13c77e[911]: Traceback (most recent call last):\nNov  6 11:37:37 atest docker/286fde13c77e[911]:   File \"/usr/lib/python2.7/site-packages/elasticsearch/connection/http_urllib3.py\", line 78, in perform_request\nNov  6 11:37:37 atest docker/286fde13c77e[911]:     response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)\nNov  6 11:37:37 atest docker/286fde13c77e[911]:   File \"/usr/lib/python2.7/site-packages/urllib3/connectionpool.py\", line 609, in urlopen\nNov  6 11:37:37 atest docker/286fde13c77e[911]:     _stacktrace=sys.exc_info()[2])\nNov  6 11:37:37 atest docker/286fde13c77e[911]:   File \"/usr/lib/python2.7/site-packages/urllib3/util/retry.py\", line 222, in increment\nNov  6 11:37:37 atest docker/286fde13c77e[911]:     raise six.reraise(type(error), error, _stacktrace)\nNov  6 11:37:37 atest docker/286fde13c77e[911]:   File \"/usr/lib/python2.7/site-packages/urllib3/connectionpool.py\", line 559, in urlopen\nNov  6 11:37:37 atest docker/286fde13c77e[911]:     body=body, headers=headers)\nNov  6 11:37:37 atest docker/286fde13c77e[911]:   File \"/usr/lib/python2.7/site-packages/urllib3/connectionpool.py\", line 376, in _make_request\nNov  6 11:37:37 atest docker/286fde13c77e[911]:     httplib_response = conn.getresponse(buffering=True)\nNov  6 11:37:37 atest docker/286fde13c77e[911]:   File \"/usr/lib/python2.7/httplib.py\", line 1132, in getresponse\nNov  6 11:37:37 atest docker/286fde13c77e[911]:     response.begin()\nNov  6 11:37:37 atest docker/286fde13c77e[911]:   File \"/usr/lib/python2.7/httplib.py\", line 453, in begin\nNov  6 11:37:37 atest docker/286fde13c77e[911]:     version, status, reason = self._read_status()\nNov  6 11:37:37 atest docker/286fde13c77e[911]:   File \"/usr/lib/python2.7/httplib.py\", line 417, in _read_status\nNov  6 11:37:37 atest docker/286fde13c77e[911]:     raise BadStatusLine(line)\nNov  6 11:37:37 atest docker/286fde13c77e[911]: ProtocolError: ('Connection aborted.', BadStatusLine(\"''\",))\nNov  6 11:37:37 atest docker/286fde13c77e[911]: 2015-11-06 11:37:37,524 DEBUG              elasticsearch       log_request_fail:90   > {\"indices\": \"logs-nginx-access-2015.10.30\", \"partial\": false, \"ignore_unavailable\": false, \"include_global_state\": true}\nNov  6 11:37:37 atest docker/286fde13c77e[911]: 2015-11-06 11:37:37,524 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\nNov  6 11:37:37 atest docker/286fde13c77e[911]: 2015-11-06 11:37:37,524 INFO      urllib3.connectionpool              _new_conn:207  Starting new HTTP connection (2): 192.168.33.105\nNov  6 11:37:57 atest docker/61f057472da8[911]: [2015-11-06 11:37:57,168][INFO ][snapshots                ] [atest.com] snapshot [atest_backups:logs-20151106113633] is done\nNov  6 11:37:57 atest docker/61f057472da8[911]: [2015-11-06 11:37:57,171][WARN ][snapshots                ] [atest.com] [atest_backups][logs-20151106113633] failed to create snapshot\nNov  6 11:37:57 atest docker/61f057472da8[911]: org.elasticsearch.snapshots.ConcurrentSnapshotExecutionException: [atest_backups:logs-20151106113633] a snapshot is already running\nNov  6 11:37:57 atest docker/61f057472da8[911]: #011at org.elasticsearch.snapshots.SnapshotsService$1.execute(SnapshotsService.java:192)\nNov  6 11:37:57 atest docker/61f057472da8[911]: #011at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:374)\nNov  6 11:37:57 atest docker/61f057472da8[911]: #011at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:196)\nNov  6 11:37:57 atest docker/61f057472da8[911]: #011at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:162)\nNov  6 11:37:57 atest docker/61f057472da8[911]: #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nNov  6 11:37:57 atest docker/61f057472da8[911]: #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nNov  6 11:37:57 atest docker/61f057472da8[911]: #011at java.lang.Thread.run(Thread.java:745)\nNov  6 11:37:57 atest docker/61f057472da8[911]: [2015-11-06 11:37:57,693][WARN ][snapshots                ] [atest.com] failed to create snapshot [atest_backups:logs-20151106113633]\nNov  6 11:37:57 atest docker/61f057472da8[911]: org.elasticsearch.snapshots.InvalidSnapshotNameException: [atest_backups:logs-20151106113633] Invalid snapshot name [logs-20151106113633], snapshot with such name already exists\nNov  6 11:37:57 atest docker/61f057472da8[911]: #011at org.elasticsearch.repositories.blobstore.BlobStoreRepository.initializeSnapshot(BlobStoreRepository.java:233)\nNov  6 11:37:57 atest docker/61f057472da8[911]: #011at org.elasticsearch.snapshots.SnapshotsService.beginSnapshot(SnapshotsService.java:290)\nNov  6 11:37:57 atest docker/61f057472da8[911]: #011at org.elasticsearch.snapshots.SnapshotsService.access$600(SnapshotsService.java:92)\nNov  6 11:37:57 atest docker/61f057472da8[911]: #011at org.elasticsearch.snapshots.SnapshotsService$1$1.run(SnapshotsService.java:211)\nNov  6 11:37:57 atest docker/61f057472da8[911]: #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nNov  6 11:37:57 atest docker/61f057472da8[911]: #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nNov  6 11:37:57 atest docker/61f057472da8[911]: #011at java.lang.Thread.run(Thread.java:745)\nNov  6 11:37:57 atest docker/286fde13c77e[911]: 2015-11-06 11:37:57,175 DEBUG     urllib3.connectionpool          _make_request:387  \"PUT /_snapshot/atest_backups/logs-20151106113633?wait_for_completion=true HTTP/1.1\" 503 129\nNov  6 11:37:57 atest docker/286fde13c77e[911]: 2015-11-06 11:37:57,176 WARNING            elasticsearch       log_request_fail:82   PUT /_snapshot/atest_backups/logs-20151106113633?wait_for_completion=true [status:503 request:19.652s]\nNov  6 11:37:57 atest docker/286fde13c77e[911]: 2015-11-06 11:37:57,176 DEBUG              elasticsearch       log_request_fail:90   > {\"indices\": \"logs-nginx-access-2015.10.30\", \"partial\": false, \"ignore_unavailable\": false, \"include_global_state\": true}\nNov  6 11:37:57 atest docker/286fde13c77e[911]: 2015-11-06 11:37:57,176 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\nNov  6 11:37:57 atest docker/286fde13c77e[911]: 2015-11-06 11:37:57,701 DEBUG     urllib3.connectionpool          _make_request:387  \"PUT /_snapshot/atest_backups/logs-20151106113633?wait_for_completion=true HTTP/1.1\" 400 175\nNov  6 11:37:57 atest docker/286fde13c77e[911]: 2015-11-06 11:37:57,701 WARNING            elasticsearch       log_request_fail:82   PUT /_snapshot/atest_backups/logs-20151106113633?wait_for_completion=true [status:400 request:0.525s]\nNov  6 11:37:57 atest docker/286fde13c77e[911]: 2015-11-06 11:37:57,701 DEBUG              elasticsearch       log_request_fail:90   > {\"indices\": \"logs-nginx-access-2015.10.30\", \"partial\": false, \"ignore_unavailable\": false, \"include_global_state\": true}\nNov  6 11:37:57 atest docker/286fde13c77e[911]: 2015-11-06 11:37:57,701 ERROR       curator.api.snapshot        create_snapshot:91   Client raised aTransportError.\nNov  6 11:37:57 atest docker/286fde13c77e[911]: 2015-11-06 11:37:57,701 WARNING        curator.cli.utils               exit_msg:69   Job did not complete successfully.\nNov  6 11:37:57 atest docker/286fde13c77e[911]: Logs backup indices complete in : 86 seconds.\nWhen I list all backup, the snapshot shows as SUCCESS :\n{\"snapshot\":\"logs-20151106113633\",\"version_id\":1070199,\"version\":\"1.7.1\",\"indices\":[\"logs-nginx-access-2015.10.30\"],\"state\":\"SUCCESS\",\"start_time\":\"2015-11-06T11:36:35.044Z\",\"start_time_in_millis\":1446809795044,\"end_time\":\"2015-11-06T11:37:57.237Z\",\"end_time_in_millis\":1446809877237,\"duration_in_millis\":82193,\"failures\":[],\"shards\":{\"total\":5,\"failed\":0,\"successful\":5}}\nThe full backup process duration indicate 82 seconds but the backup size is barely 1mb so when I delete all files on my aws bucket and start the backup, I can see that the files are uploaded within a few seconds so I don't really get why it is waiting so long to complete.\nI disabled my firewall but it does not work either.\nMaybe it has something to do with the timeout as you mentionned as the first log request fail happens after 60 seconds :\nNov  6 11:37:37 atest docker/286fde13c77e[911]: 2015-11-06 11:37:35,043 WARNING            elasticsearch       log_request_fail:82   PUT http://192.168.33.105:9200/_snapshot/atest_backups/logs-20151106113633?wait_for_completion=true [status:N/A request:60.003s]\n@pkr1234 could you share how you solved your problem ? was caused by your aws load balancer ?\nI use haproxy as loadbalancer and keepalived in front of it, don't know if one of them could be the culprit.\nThanks for the help :)\n. Thanks for the fast answer @pkr1234.\nI just stumbled upon an issue and was just investigating haproxy timeout : https://github.com/elastic/curator/issues/457\nI think I will try to have a direct connection between curator and elasticsearch without going through haproxy as I don't feel like increasing the timeout for all connections.\n. Yes everything is working now :)\nThank you again for the great tool @untergeek  !\n. Hello @untergeek,\nI stumble upon this issue a bit late. I was using ES2.x but essentially for the completion suggester which had one major issue which got solved in ES5.x. So I had to upgrade. However, I had implemented multiple scripts to automate the backup/restore of all my indices with curator 3.x and just found out that curator 3.x does not support ES5.x so I am kind of stuck, either with a buggy ES2.x or with ES5.x but without any backup support. As making curator 4 works with AWS ES seems complicated for now, what would it take to make curator 3.x works with ES5.x?\nIt's a frustrating situation and I cannot really compromise on either of this issues, neither have the money or time right now to implement a new backup system (with ES cloud for instance like it was clumsily promoted in another post, yet I am very grateful of all the work you and all elastic coders that made all those great tools available, I am just hoping that I did not invest so much time to need to rebuild a different system).. Wow that was fast :) Thanks a bunch!\nBefore I dig deep into this, it means I can run command like I used to like:\ncurator $ES_OPTS snapshot --repository $S3_REPOSITORY --prefix logs- indices --prefix logs- --newer-than $LOGS_BACKUP_DAYS --time-unit days --timestring '%Y.%m.%d' ?\n. Thanks, I was already converting my scripts and managed to make my first backup with curator 4.2.3.post1\ncurator_cli $ES_OPTS snapshot --repository $S3_REPOSITORY --name my-nice-name-hourly-%Y%m%d%H%M%S --filter_list '{\"filtertype\":\"pattern\",\"kind\":\"prefix\",\"value\":\"myindice\"}'\nFirst with curator 3.x, the snapshot backup on the s3 repository had the same name specified in command line with --prefix. With new version I have names like meta-0UH9cMZsR1Df5v68IaDPEQ.dat. Is it possible to still get the nice formatted name as specified in --name my-nice-name-hourly-%Y%m%d%H%M%S ?\nSecond, doing the first backup, when I did a show_snapshots right after, I got the following output:\nmy-nice-name-hourly-20161125002650\nmy-nice-name-hourly-20161125002650\nIs it normal that it lists the snapshot twice?\nThen I indexed some more data, did another snapshot and show_snapshots again, I got:\nmy-nice-name-hourly-20161125010047\nmy-nice-name-hourly-20161125010047\nWhile a new .dat file has been added to the s3 repository, the show_snapshots show no trace of the previous backups...\nThe whole point (at least in my case) of the backup is to be able to restore it, however the restore singleton is not available. Will it be implemented anytime soon? Otherwise I will need to look for other solutions :(\nAnyway, thank you very much for making thoses singletons available!. I Will open new issues tomorrow (it's 3am here and I am on mobile..). But just so I know, what is the point of a backup if it is not possible to restore it? Once we have a curator backup on s3, can we restore it with other tools or it will be useless until restore singleton is available?\n. You lost me here, as stated here https://www.elastic.co/guide/en/elasticsearch/client/curator/current/faq_aws_iam.html and as you mentionned, in order to use Curator 4 with aws, the cluster state metadata endpoint has to be opened.\nI managed to backup to aws s3 thanks to the singleton which uses the 3.x way of doing things that don't use that cluster state metadata.\nNow that I have my backup on s3, you're saying that Curator 4 needs that endpoint only to backup but it is possible to restore any backup from aws s3 with the Curator 4 restore action, so we can set the aws keys in the YAML file and it works?\nI used to restore with (thanks to elasticsearch aws plugin):\ncurl -s -S -o /dev/stderr -w \"%{http_code}\" -XPOST \"${ESHOST}/_snapshot/${S3REPO}/${snapshot_to_restore}/_restore?pretty=true&wait_for_completion=true\". Thanks for the clarifications.. It seems I had network issues and the snapshot did not complete. I tried it again today (after an elasticsearch cluster restart) and everything works fine. I am not sure but checking the snapshots status, one were either in IN_PROGRESS or ABORTED state.\nSo it makes sense that the output were always different as the snapshot never completed.\nWhat is still weird is:\n- why when the snapshot is still running, the show_snapshots showed the snapshot twice.\nAs for the new .dat file in the s3 repo, I guess it is created when the snapshot starts and not cleared if the snapshots fails.\n. ",
    "syndy1989": "Hi,\nIs it possible to take snapshot of only certain data within indices using curator?. Thank you very much @untergeek . Nginx: How do i forward http request to another port?\nI have three ES nodes.\nNode 1(primary ):http://localhost:9200\nNode 2:http://localhost:9201\nNode 3:http://localhost:9202\nwhat i want to do is, if primary node fails ES will make node 2 as primary but i want the page should redirect automatically to localhost:9201 and how would i do that using nginx on windows or is there any other way to do that. Thanks in advance\n. Hi ,\nI'm new to elastic search curator, trying to take snapshot using curator but it comes up with an error 2017-02-09 04:45:41,779 INFO      Action ID: 1: \"delete_snapshots\" not performed\n because \"disable_action\" is set to True\n2017-02-09 04:45:41,779 INFO      Job completed.. Please find my action.yml and config.yml files below\nactions:\n  1:\n    action: snapshot\n    description: >-\n      Snapshot first- prefixed indices older than 1 day (based on index\n      creation_date) with the default snapshot name pattern of\n      'curator-%Y%m%d%H%M%S'.  Wait for the snapshot to complete.  Do not skip\n      the repository filesystem access check.  Use the other options to create\n      the snapshot.\n    options:\n      repository: backup\n      # Leaving name blank will result in the default 'curator-%Y%m%d%H%M%S'\n      name:\n      ignore_unavailable: False\n      include_global_state: True\n      partial: False\n      wait_for_completion: True\n      skip_repo_fs_check: False\n      timeout_override:\n      continue_if_exception: False\n      disable_action: False\n    filters:\n    - filtertype: pattern\n      kind: prefix\n      value: first-\n      exclude:\n    - filtertype: age\n      source: creation_date\n      direction: older\n      unit: days\n      unit_count: 1\n      exclude:\n\n\nRemember, leave a key empty if there is no value.  None will be a string,\nnot a Python \"NoneType\"\nclient:\n  hosts:\n    - 127.0.0.1\n  port: 9200\n  url_prefix:\n  use_ssl: False\n  certificate:\n  client_cert:\n  client_key:\n  aws_key:\n  aws_secret_key:\n  aws_region:\n  ssl_no_validate: False\n  http_auth:\n  timeout: 30\n  master_only: False\nlogging:\n  loglevel: INFO\n  logfile:\n  logformat: default\n  blacklist: ['elasticsearch', 'urllib3']\nAm i missing something. ",
    "priyaprabhakar777": "@untergeek does that mean 503 error happened because another curator service was already running . ",
    "Stono": "@untergeek I've trawled through threads and struggling to get my head around all of this, can you help?\nI have an index built on {+xxxx.ww}, which gives me (for this week) syslog-2015.34\nCan I use curator (3.2.3) to delete and indices older than 8 weeks?\nI thought curator --host 0.0.0.0 delete indices --older-than 8 --time-unit weeks --prefix syslog- --timestring '%Y.%w' would work, but that actually deletes all indices regardless of date!\n. ok, needed %W\n. ",
    "cygnusb": "We are currently still investigating an issue.\n. Done\n. ",
    "vaijab": "Any idea when this is going to be merged and released?\n. ",
    "jalateras": "Just to add when i execute the same command a subsequent time i still get the same error\n. not using self-signed certificate but a purchases certificate from rapidssl. When I use curl i certainly don't get this type of error.  Curious whether anyone else has experienced a similar issue\n. No worries. I think the path of leas resistance is to run it locally for the moment. \n. ",
    "bai": "Can't wait to get this and Beta1 support shipped :ship: \n. ",
    "rhealitycheck": "I don't know about Scott but I would say this also applies for me. I am typically responsible for the maintenance of Elasticsearch in our environment but if I am unavailable for whatever reason I need other members of my organization to be able to restore indices as needed. Since users need to know the index name and the snapshot name in order to restore an index it makes more sense to have the snapshot labels match the index names, especially since I'm taking full index backups on a nightly basis so I don't have any incremental backups to identify by a timestamp that includes more precision than the day. This is information I gathered from your response in #174 with regards to using the --older-than flag, so if you are only taking full index backups what is the point of having an arbitrary name for all the snapshots?\nSecondly, no one knows based on the snapshot name how many indices are included in the backup. If we ever change the policy of how many indices get backed up every night it becomes confusing to know which snapshot will contain the index you need to restore. \nIf we need to restore because of some disaster it doesn't really make sense to need to go looking to see which snapshot includes the data needed, it makes more logical sense to be able to match your snapshot to your index name.\n. I haven't heard of Kopf, I'll definitely check it out, that looks like a pretty easy to use interface for restores in particular.\nAnd yes, I've been using a combination of shell scripts that I've written myself and the older version of curator for my snapshots so far.\n. you can capture the kibana index if you use most-recent and set it to 1. seems like it's not expected behavior but it does work. (or at least it was working in 1.2.2 and when i initially tried 2.0.0, i haven't used 2.0.0 in a while\n. Ok, so I just ran one with the --request_timeout taken out and --debug on. and this is the error message that i get:\n2015-08-17 12:06:25,441 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-08-17 12:07:25,443 WARNING            elasticsearch       log_request_fail:82   PUT http://haproxystaging:9200/_snapshot/my_s3_repository/mdp-2015.08.04?wait_for_completion=true [status:N/A request:60.002s]\nTraceback (most recent call last):\n  File \"/Users/rhea/dev/venv/lib/python2.7/site-packages/elasticsearch/connection/http_urllib3.py\", line 74, in perform_request\n    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)\n  File \"/Users/rhea/dev/venv/lib/python2.7/site-packages/urllib3/connectionpool.py\", line 607, in urlopen\n    _stacktrace=sys.exc_info()[2])\n  File \"/Users/rhea/dev/venv/lib/python2.7/site-packages/urllib3/util/retry.py\", line 222, in increment\n    raise six.reraise(type(error), error, _stacktrace)\n  File \"/Users/rhea/dev/venv/lib/python2.7/site-packages/urllib3/connectionpool.py\", line 557, in urlopen\n    body=body, headers=headers)\n  File \"/Users/rhea/dev/venv/lib/python2.7/site-packages/urllib3/connectionpool.py\", line 374, in _make_request\n    httplib_response = conn.getresponse(buffering=True)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py\", line 1045, in getresponse\n    response.begin()\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py\", line 409, in begin\n    version, status, reason = self._read_status()\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py\", line 373, in _read_status\n    raise BadStatusLine(line)\n**ProtocolError: ('Connection aborted.', BadStatusLine(\"''\",))**\n`2015-08-17 12:07:25,450 DEBUG              elasticsearch       log_request_fail:90   > {\"indices\": \"mdp-2015.08.04\", \"partial\": false, \"ignore_unavailable\": false, \"include_global_state\": true}\n2015-08-17 12:07:25,450 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-08-17 12:07:25,451 INFO      urllib3.connectionpool              _new_conn:205  Starting new HTTP connection (2): haproxystaging\n2015-08-17 12:07:25,528 DEBUG     urllib3.connectionpool          _make_request:385  \"PUT /_snapshot/my_s3_repository/mdp-2015.08.04?wait_for_completion=true HTTP/1.1\" 503 233\n2015-08-17 12:07:25,528 WARNING            elasticsearch       log_request_fail:82   PUT /_snapshot/my_s3_repository/mdp-2015.08.04?wait_for_completion=true [status:503 request:0.077s]\n2015-08-17 12:07:25,528 DEBUG              elasticsearch       log_request_fail:90   > {\"indices\": \"mdp-2015.08.04\", \"partial\": false, \"ignore_unavailable\": false, \"include_global_state\": true}\n2015-08-17 12:07:25,528 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-08-17 12:07:25,536 DEBUG     urllib3.connectionpool          _make_request:385  \"PUT /_snapshot/my_s3_repository/mdp-2015.08.04?wait_for_completion=true HTTP/1.1\" 503 233\n2015-08-17 12:07:25,537 WARNING            elasticsearch       log_request_fail:82   PUT /_snapshot/my_s3_repository/mdp-2015.08.04?wait_for_completion=true [status:503 request:0.008s]\n2015-08-17 12:07:25,537 DEBUG              elasticsearch       log_request_fail:90   > {\"indices\": \"mdp-2015.08.04\", \"partial\": false, \"ignore_unavailable\": false, \"include_global_state\": true}\n2015-08-17 12:07:25,537 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-08-17 12:07:25,544 DEBUG     urllib3.connectionpool          _make_request:385  \"PUT /_snapshot/my_s3_repository/mdp-2015.08.04?wait_for_completion=true HTTP/1.1\" 503 233\n2015-08-17 12:07:25,544 WARNING            elasticsearch       log_request_fail:82   PUT /_snapshot/my_s3_repository/mdp-2015.08.04?wait_for_completion=true [status:503 request:0.007s]\n2015-08-17 12:07:25,544 DEBUG              elasticsearch       log_request_fail:90   > {\"indices\": \"mdp-2015.08.04\", \"partial\": false, \"ignore_unavailable\": false, \"include_global_state\": true}\n2015-08-17 12:07:25,544 ERROR       curator.api.snapshot        create_snapshot:91   Client raised a TransportError.\n2015-08-17 12:07:25,544 WARNING        curator.cli.utils               exit_msg:69   Job did not complete successfully.\nand I'm confused where the bad status line is coming from given that before that it returned a giant list of all the snapshots I currently have in my repo. Also just to note the most recent one that I tried took roughly 3.5 mins\n. Ah yes, I see where the connection is having issues with the proxy now but connecting directly to a client node seems to be working out as expected -- we just changed our setup to have client nodes in front of the data nodes, so I had initially been connecting to those but when we stopped allowing connections to the data nodes I defaulted to the proxy but this makes more sense. I've tried using the --wait_for_completion False and have had mixed results with that which is why I finally asked how I could fix this because we'd really like reliable snapshots.\n. Yeah, we knew that going in we just didn't have the hardware to support that initially but thankfully we do now and then transitioning to it changed things up a bit so we're still working through all of it. Thanks for the help!\n. nevermind, i had a versioning issue where somehow i managed to install curator 3.2.3 but not elasticsearch 1.6.0 and was still on elasticsearch 1.2.0 as the python module. not sure how i managed that but that's the issue.\n. So we run the script at 4am when basically nothing is happening. And yes I've looked through the master's logs and haven't found anything -- there's some noise in the logs from some Kibana graphs where facets have broken since our upgrade, but those seem to be consistent throughout the day and essentially have no effect on the cluster. I don't always know which client the API is connection to though because we have a load balancer in front of our cluster, I suppose I could turn up the curator logs to debug that may help. I'll see if I can find the most recent client that it was connecting to and see if there's anything in there. \nI guess what's most strange to me is that I literally don't see anything in the master's logs until I see it saying that it's lost connection to the cluster and then it revives itself and rejects the master that the cluster has elected, which is where we need to go in and manually restart elasticsearch so it can rejoin the cluster. \nWe have 3-5 shards per node and 9 shards per index.\n. Ah apologies on the shard question, I was confused. We have 120 indexes with somewhere between 3-5 shards per node at a total of 9 shards per index. So on a single node we'll have ~ 360 shards or more. But since we're only deleting 4 indexes of those 120 indexes I didn't think this would be an issue.\nAlso, I was mistaken I am not connection to the proxy but actually directly to a client node (I'd asked this before and changed it on your recommendation and forgotten that I made that change). And these are the logs I see from the client -- but I really don't see anything in this other than the master data node leaving. And as far as I can tell nothing is happening on the cluster other than one Kibana graph that's having trouble rendering:\n[2015-10-04 03:58:14,850][INFO ][discovery.ec2            ] [AWSDATANODE01] master_left [[AWSLXPROD005][KuYwb4ULT2GKcHqDQHZgkA][AWSLXPROD005][inet[/xxx.xxx.xxx.xxxx:9300]]{aws_availability_zone=us-east-1a}], reason [transport disconnected]\n [2015-10-04 03:58:14,850][WARN ][discovery.ec2            ] [AWSDATANODE01] master left (reason = transport disconnected), current nodes: {[AWSLXPROD006][X7eklovjRISRnQD6i7vDfg][AWSDATANODE02][inet[/xx.xxx.xxx.xxxx:9300]]{aws_availability_zone=us-east-1a},[AWSDATANODE03][XzdCC3FQTGaGh3Q2bqeR9A][AWSDATANODE05][inet[/xx.xxx.xxxx.xx:9300]]{aws_availability_zone=us-east-1d},[AWSDATANODE04][sO_0Fk4BQjyw8UhCs0WXVQ][AWSLXPROD008][inet[/xxx.xxx.xxx.xxxx:9300]]{aws_availability_zone=us-east-1b},[AWSDATANODE06][PB1wSO1DTgO7NYZkumEUtA][AWSDATANODE02][inet[/xx.xxx.xx.xxxx:9300]]{aws_availability_zone=us-east-1b},[AWSDATANODE01][cBU0J9LnQGKmgsF_0y0Ihw][AWSDATANODE01][inet[/xxx.xxx.xx.xxx:9300]]{aws_availability_zone=us-east-1a, data=false, master=false},[AWSDATANODE07][nGdqrIb9TlieBtkIrai_Zw][AWSDATANODE07][inet[/xx.xxx.xxx.xxx:9300]]{aws_availability_zone=us-east-1a},[AWSDATANODE09][d6RtyZhISiWexxsShS4mhw][AWSLXPROD015][inet[/xx.xx.xxxx.xxx:9300]]{aws_availability_zone=us-east-1d, data=false, master=false},[AWSDATANODE010][A9pKJWQETAWq_ZT6bzDYbQ][AWSDATANODE010][inet[/xx.xxx.xxx.xxx:9300]]{aws_availability_zone=us-east-1b, data=false, master=false},[AWSLXPROD010][fMPQz1_ySaqXHGvLfA0PeQ][AWSLXPROD010][inet[/xxx.xxx.xxx.xxx:9300]]{aws_availability_zone=us-east-1d},}\n [2015-10-04 03:58:14,851][INFO ][cluster.service          ] [AWSDATANODE01] removed {[AWSDATANODE02][KuYwb4ULT2GKcHqDQHZgkA][AWSLXPROD005][inet[/xxx.xxx.xxx.xxxx:9300]]{aws_availability_zone=us-east-1a},}, reason: zen-disco-master_failed ([AWSDATANODE02][KuYwb4ULT2GKcHqDQHZgkA][AWSLXPROD005][inet[/xxx.xx.xx.xxx:9300]]{aws_availability_zone=us-east-1a})\n [2015-10-04 03:58:14,853][DEBUG][action.admin.indices.delete] [AWSDATANODE01] connection exception while trying to forward request to master node [[AWSDATANODE02][KuYwb4ULT2GKcHqDQHZgkA][AWSDATANODE02][inet[/xx.xxxx.xxx.xxx:9300]]{aws_availability_zone=us-east-1a}], scheduling a retry. Error: [org.elasticsearch.transport.NodeDisconnectedException: [AWSDATANODE02][inet[/172.16.1.237:9300]][indices:admin/delete] disconnected]\n [2015-10-04 03:58:14,857][DEBUG][action.admin.indices.delete] [AWSDATANODE01] no known master node, scheduling a retry\n [2015-10-04 03:58:20,697][INFO ][cluster.service          ] [AWSDATANODE01] detected_master [AWSLXPROD009][PB1wSO1DTgO7NYZkumEUtA][AWSLXPROD009][inet[/xxx.x.xx.xxx:9300]]{aws_availability_zone=us-east-1b}, reason: zen-disco-receive(from master [[AWSDATANODE03][PB1wSO1DTgO7NYZkumEUtA][AWSDATANODE03][inet[/xx.xxx.xxx.xxx:9300]]{aws_availability_zone=us-east-1b}])\n [2015-10-04 03:58:34,385][DEBUG][action.search.type       ] [AWSDATANODE01] [logstash-2015.10.04][1], node[XzdCC3FQTGaGh3Q2bqeR9A], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@5dd64e] lastShard [true]\n. So the client node logs are a little tough to read because before that I primarily see some version of this error because of a misconfigured kibana graph -- I just went through with my team to get rid of this, it's been in there for a while but aside from this noise I really don't see anything referring to the master or a delete operation and I've searched the logs:\n[2015-10-04 03:58:05,224][DEBUG][action.search.type       ] [AWSDATANODE01] All shards failed for phase: [query]\norg.elasticsearch.transport.RemoteTransportException: [AWSDATANODE02][inet[/xxx.xxx.xxx.xxx:9300]][indices:data/read/search[phase/query]]\nCaused by: org.elasticsearch.search.SearchParseException: [logstash-2015.10.04][5]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"facets\":{\"2\":{\"date_histogram\":{\"key_field\":\"@timestamp\",\"value_field\":\"band\",\"interval\":\"1m\"},\"global\":true,\"facet_filter\":{\"fquery\":{\"query\":{\"filtered\":{\"query\":{\"query_string\":{\"query\":\"symbol.raw:WMB\"}},\"filter\":{\"bool\":{\"must\":[{\"range\":{\"@timestamp\":{\"from\":1443938402147,\"to\":1443949202147}}},{\"fquery\":{\"query\":{\"query_string\":{\"query\":\"type.raw:hegemon_Production\"}},\"_cache\":true}}]}}}}}}}},\"size\":0}]]\n        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:747)\n        at org.elasticsearch.search.SearchService.createContext(SearchService.java:572)\n        at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:544)\n        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:306)\n        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:776)\n        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:767)\n        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:279)\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n        at java.lang.Thread.run(Unknown Source)\nCaused by: org.elasticsearch.search.facet.FacetPhaseExecutionException: Facet [2]: (value) field [band] not found\n        at org.elasticsearch.search.facet.datehistogram.DateHistogramFacetParser.parse(DateHistogramFacetParser.java:198)\n        at org.elasticsearch.search.facet.FacetParseElement.parse(FacetParseElement.java:93)\n        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:731)\n        ... 10 more\nOn the elected master this is what I see from when it receives the delete index request to when it's initializing again:\n[2015-10-04 00:01:01,774][INFO ][snapshots                ] [AWSDATANODE01] snapshot      [my_s3_repository:dashboards-2015.10.03] is done\n[2015-10-04 01:03:00,556][INFO ][cluster.metadata         ] [AWSDATANODE01] [logstash-2015.10.04] update_mapping [syslog-Production] (dynamic)\n[2015-10-04 01:31:27,839][INFO ][snapshots                ] [AWSDATANODE01] snapshot [my_s3_repository:mdp-2015.08.24] is done\n[2015-10-04 01:46:53,752][INFO ][snapshots                ] [AWSDATANODE01] snapshot [my_s3_repository:indices-2015.08.24] is done\n[2015-10-04 02:50:12,154][INFO ][cluster.metadata         ] [AWSDATANODE01] [logstash-2015.10.04] update_mapping [index_Production] (dynamic)\n[2015-10-04 02:51:47,066][INFO ][cluster.metadata         ] [AWSDATANODE01] [logstash-2015.10.04] update_mapping [index_Production] (dynamic)\n[2015-10-04 03:01:32,323][INFO ][cluster.metadata         ] [AWSDATANODE01] [logstash-2015.10.04] update_mapping [salt_UAT] (dynamic)\n[2015-10-04 03:02:33,739][INFO ][cluster.metadata         ] [AWSDATANODE01] [logstash-2015.10.04] update_mapping [salt_UAT] (dynamic)\n[2015-10-04 03:05:21,298][INFO ][cluster.metadata         ] [AWSDATANODE01] [logstash-2015.10.04] update_mapping [index_Production] (dynamic)\n[2015-10-04 03:07:58,083][INFO ][cluster.metadata         ] [AWSDATANODE01] [logstash-2015.10.04] update_mapping [monitoring_Production] (dynamic)\n[2015-10-04 03:07:59,268][INFO ][cluster.metadata         ] [AWSDATANODE01] [logstash-2015.10.04] update_mapping [monitoring_Production] (dynamic)\n[2015-10-04 03:10:46,884][INFO ][cluster.metadata         ] [AWSDATANODE01] [logstash-2015.10.04] update_mapping [logstash_agent_UAT] (dynamic)\n[2015-10-04 04:00:59,757][INFO ][cluster.metadata         ] [AWSDATANODE01] [logstash-1970.01.02] deleting index\n[2015-10-04 04:01:00,905][INFO ][cluster.metadata         ] [AWSDATANODE01] [logstash-2015.08.24] deleting index\n[2015-10-04 16:46:15,985][INFO ][node                     ] [AWSDATANODE01] version[1.7.0], pid[11074], build[929b973/2015-07-16T14:31:07Z]\n[2015-10-04 16:46:15,986][INFO ][node                     ] [AWSDATANODE01] initializing ...\nAlso as a side note I don't mean to imply this is a curator issue, I just thought since we use curator to delete the indices and that curator facilitates the deletes this would be the most likely place that someone else may have seen a similar issue. Thanks for the help so far!\n. Yeah I saw that too. I'll give upgrading a try. It can't hurt.\nAlso I'm confused how I can have DEBUG on one node but INFO on another... I thought those were cluster-wide settings, right?\n. Ah, ok. I had set it via the API but assumed it was cluster-wide not per node. I'll just update the logging.yml instead and do a restart. And I'll re-open this thread if after the update I still see it pop up and and hopefully debug will give some more information on it.\n. ",
    "moshe": "already fixed in master https://github.com/elasticsearch/curator/commit/685016295be83f14d4cad6ab9577eb1461ad9621\n@untergeek did you going to release a pip version that fix this issue?\n. ",
    "lethalman": "Another, using git master curator and elasticsearch 1.4 beta1:\n$ curator --host myhost --url_prefix myindex-2014.10.17.18 show --show-indices\nTraceback (most recent call last):\n  File \"/usr/local/bin/curator\", line 9, in <module>\n    load_entry_point('elasticsearch-curator==2.1.0-dev', 'console_scripts', 'curator')()\n  File \"/usr/local/lib/python2.7/dist-packages/elasticsearch_curator-2.1.0_dev-py2.7.egg/curator/curator_script.py\", line 347, in main\n    check_version(client)\n  File \"/usr/local/lib/python2.7/dist-packages/elasticsearch_curator-2.1.0_dev-py2.7.egg/curator/curator_script.py\", line 227, in check_version\n    version_number = curator.get_version(client)\n  File \"/usr/local/lib/python2.7/dist-packages/elasticsearch_curator-2.1.0_dev-py2.7.egg/curator/curator.py\", line 248, in get_version\n    version = client.info()['version']['number']\nKeyError: 'version'\n. ",
    "evanwong": "Ops, my bad. Thanks!\n. ",
    "heffergm": "I'm writing a log to /var/log, hence sudo.\nIn any case, no rush on the example, I just wasn't clear on if there was a non-time series expectation or not. Cheers!\n. ",
    "bflad": "Personally, we have a use case where we have over 200 non-dated indices with common prefixes, such as foo_a, foo_b, bar_a, bar_b, etc. It'd be extremely helpful to kick off those snapshots via curator. Trying with --all-indices --exclude-pattern logstash didn't do the trick unfortunately as the logstash indices still wind up in the snapshot.\n. What's interesting is that at least against ES 1.1, --all-indices works fine even though it displays the same output _, a, l, l. Tested on multiple clusters.\n. ",
    "kamaradclimber": "I've already tried setting time-unit to hours and it works. This is not really satisfying though since --older-than won't behave as expected (will try to remove .23, then .22, .21 which has no sense).\nAbout the # of queries, we only care about latency for our end users. Currently kibana3 serializes requests to indices (most recent first) which gives a bad feeling of latency when requesting 24 indices in a row. btw making a request on a 6 hours index is nearly as fast as a request on a 1 hour index\n. What I understand is that my index 2014-10-18.02 will be thought to be the third hour of this saturday. It contains actually data from noon to 6pm. \nThen asking curator to delete indices older than 2 days, will delete that index much sooner than expected (around 3am on 10/20 instead of 7pm 10/20).\nThis will lead to non linear retention accross the day.\nTechnically it works if we ignore that issue.\nA proper solution would be to have other time-units such as 6hours (formally any divisor of 24 could be achieved in the same way)\nWhat do you think ? Is it a too exotic use case for curator ?\n. Thanks for your answer.\nI think I'll keep a fork of curator (or a sript using your api, don't know yet).\nOur goal is to store as much data as possible (not with a static retention but as much as our disk can handle).\nThanks again for your advices. \n. thanks for this precision.\nfor now we reach space limit very early (2 days max), so I don't fear the shard limit yet\n. fullmatch has been introduced in python 3.4, I'll fix that\n. ",
    "oryband": "This is very unclear according to the docs and should be stated more clearly. Which, by the way, is inconsistent with elasticsearch main website (old version) and this github repo.\n. ",
    "dadoonet": "Forget it. It already exists.\n. Reopening\n. ",
    "wingZero21": "Hey,\nYeah apologies I assumed it was looking for the actual version of elasticsearch but its just a client that it needs.\nCheers for the update!\n. ",
    "nodesocket": "@untergeek Perfect, exactly what we needed. Thanks.\n. ",
    "gheppner": "Sure - I noticed this bug when I was playing around with this tool, but have since moved onto something else.  I wanted someone to be aware of the issue, but thought I could at least go as far as putting a PR in.  Have never actually set up a test before.\nGH.\nOn Oct 27, 2014, at 2:22 PM, Aaron Mildenstein notifications@github.com wrote:\n\nDo you need me to add a test?\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "krzaczek": "@untergeek thx that worked .... \n. ",
    "stuart-warren": "I could imagine needing this, but not opening everything with an --older-than flag. That would probably take out my cluster.\nI'd prefer to use --newer-than to reopen a number of indexes I closed accidentally or somehow specify a date range to reopen some previous weeks logs to investigate an issue.\n. We write different log levels into 2 indexes using logstash\nOne for debug, trace and \"all\", anything that doesn't meet that get's written to the standard index for the day.\nThe logstash-debug-* index is kept for less timebefore being pruned by curator\noutput {\n  # Forward the output to elasticsearch\n  if [level] in [\"DEBUG\",\"TRACE\",\"ALL\"] {\n    elasticsearch_http {\n      host => \"localhost\"\n      port => 9200\n      flush_size => 500\n      idle_flush_time => 1\n      replication => \"sync\"\n      workers => 3\n      index => \"logstash-debug-%{+YYYY.MM.dd}\"\n      manage_template => false\n      codec => json { charset => \"UTF-8\" }\n    }\n  } else {\n    elasticsearch_http {\n      host => \"localhost\"\n      port => 9200\n      flush_size => 500\n      idle_flush_time => 1\n      replication => \"sync\"\n      workers => 3\n      index => \"logstash-%{+YYYY.MM.dd}\"\n      manage_template => false\n      codec => json { charset => \"UTF-8\" }\n    }\n  }\n}\n. ",
    "whyscream": "great, and thanks for creating curator:)\n. Thanks for the quick response. Backups are already back in place, so inconvenience was not that big :)\nI indeed found out too that leaving out the --time-unit didn't matter, but as I had only one index not matching the --timesting format (the kibana-int index), it never occurred to me that it was actually applying a filter. Maybe a confirmation dialog is the best solution for this kind of problem.\n. ",
    "kyleprager": "Sure thing, I just edited the All-in-one-documentation.md, FAQ.md and Command.md files to add documentation for the issue.  Thanks!\n. ",
    "tomcashman": "I was unaware curator's functionality was being added to Elasticsearch.\nIf that's the case, then there's no point adding more to maintain under curator. I'll create a repo with a small bash script that people can use to generate deb/rpm packages.\nThanks for reviewing the pull request :)\n. ",
    "blalor": "That's one solution, but it requires three separate cron jobs that have to be scheduled so they don't overlap.  Or a wrapper script.  --delete-older-than still causes a snapshot to be taken, so there could be substantial load on the server when taking all of these snapshots.  I'd prefer seeing this functionality wrapped up into curator.\n. ",
    "quasipedia": "Thank you @untergeek for your super-prompt reply.  I was precisely looking at delete by query documentation and wondered if I should have added a curl call to my cronjob instead... good that you gave me the heads-up on the downsides of this solution.\nSo, if I understand you correctly, your proposal would be to have a different index for each different log level, right?  This is a bit unfortunate though, as I am using fluentd as a log transport layer and I use a third party adapter to route the log events that - to the best of my knowledge - does not allow for such configuration tweaks out of the box...\nBack to the \"extremely costly\" operation, I wonder two things:\n- Would run the maintenance job more often alleviate the problems? I can imagine it would diminish the memory foot-print - having to work with less documents - but I am more concerned about the indexes health here: would they be left \"less messy\"?\n- Would running curator optimize fix the messy indexes, or is the optimization just a brutal merge that does not do any housekeeping tasks?\n- More in general: would there be any way to \"heal\" my indexes after a delete-by-query run?\nThank you in advance for your help! :)\n. @untergeek - Thank you for you answer Aaron.  I have been away for a few days, will look at this next week! :)\n. Thank you @stuart-warren for posting this, I understood the logic of the proposed solution and I find it also quite elegant.  However we use fluentd as a transport layer [we have redoundant ES servers with duplicated data and we couldn't find a reliable method to serve them all with logstash -- did we miss the obvious solution/elephant in the room?].\nI will look at the logstash adapter for fluentd, but AFAIK [and I may be wrong] it does not offer much configurability, one has to hack the code, which adds to the burden of maintaining packages up-to-date on our servers.\n. For those who may find this issue by googling: inspired by the advice of @untergeek and the example of @stuart-warren I have achieved the same result with fluentd using the rewrite_tag_filter (which works a bit like mod_rewrite in apache).\n<match debug.** >\n  type                      elasticsearch\n  logstash_format           true\n  logstash_prefix           logstash-debug\n</match>\n<match vanilla.** >\n  type                      elasticsearch\n  logstash_format           true\n  logstash_prefix           logstash-vanilla\n</match>\n<match ** >\n  type               rewrite_tag_filter\n  rewriterule1       level DEBUG debug.${tag}\n  rewriterule2       level [^DEBUG] vanilla.${tag}\n</match>\n. @untergeek - Hey! Thank you for the informative explanation.  I will work around this in my crontab then.  It may be useful to add a note to the documentation of the current version though, as not to deceive doc-readers! :)\nAs a side note: I just wonder if you know about docopt.  I started using it a few years back and I find my code greatly benefit from it in terms of readability (maybe you know about it and have good reasons not to use it, but I just wanted to give an heads-up in case you never stumbled on it)...\n. ",
    "neoice": "if I understand this, you have an index, logstash-YYYY.MM.dd that contains all log levels. thus, deleting just loglevel==DEBUG requires traversing the entire index and removing individual entries. correct?\ncould a solution be to create indexes like logstash-YYYY.MM.dd-LOGLEVEL? because that sounds really cool, although the implementation details could be difficult. if you do go that route, you should publish a doc on it :smile:\nI bet you could do it with logstash.jar and creative filtering/tagging/outputs.\n. ",
    "matthughes": "Yes it was just a logging issue.  I presumed it wasn't getting backed up when I saw that.  Will close as fixed.\n. Like this?\nbash-4.1# curator show -t 1 -T days --show-indices\nusage: curator [-h] [-v] [--host HOST] [--url_prefix URL_PREFIX] [--port PORT]\n               [--ssl] [--auth AUTH] [--timeout TIMEOUT] [--master-only] [-n]\n               [-D] [--loglevel LOG_LEVEL] [--logfile LOG_FILE]\n               [--logformat LOGFORMAT]\n               {show,allocation,alias,snapshot,close,bloom,optimize,delete}\n               ...\ncurator: error: unrecognized arguments: -t 1 -T days\nIt doesn't look like they should according to the usage docs.  Although, note that only --timeout is shown; for some reason --time-unit is not there.\n. This is for doing a snapshot with the default behavior of waiting for the snapshot to complete.  By default, I believe Curator makes this 6 hours so 8 doesn't seem that far fetched to me.  Am I missing something?\nEither way, 2.0.2 doesn't appear to accept --timeout before or after.\n. Odd that discriminates based on the command.  I would think any communication with ES server should just use the --timeout passed to it.\nAnyway, got --timeout working for snapshot, but it won't accept --time-unit.\nbash-4.1# curator --timeout=6 --time-unit=hours --master-only --host localhost --port 9200 snapshot --repository test --all-indices\nusage: curator [-h] [-v] [--host HOST] [--url_prefix URL_PREFIX] [--port PORT]\n               [--ssl] [--auth AUTH] [--timeout TIMEOUT] [--master-only] [-n]\n               [-D] [--loglevel LOG_LEVEL] [--logfile LOG_FILE]\n               [--logformat LOGFORMAT]\n               {show,allocation,alias,snapshot,close,bloom,optimize,delete}\n               ...\ncurator: error: unrecognized arguments: --time-unit=hours\nbash-4.1#\nIs README.md just wrong?  Considering I don't see time-unit in the usage printed out there, I'm guessing yes.\n. Ah, I see what my problem is.  I thought time-unit specified the time units for timeout but it appears they have nothing to do with one another.   time-unit describes how your indexes are created.  \nClosing.  Thanks.\n. Think the bug was mistaken.  I had older snapshots with a different --snapshot-prefix.  Appears to be working as expected.  Sorry for the false alarm.\n. Also would love this for Docker purposes.  I ship a container that contains curator + a set of pre-baked curator scripts.  Love to be able to tweak them via environment variables rather than editing conf files.\n. ",
    "drahtGitter": "thank you for the info - that solves it :-)\n. ",
    "xpillons": "Thank you for your reply,\nhere is the debug log\n2014-12-09T17:29:14.717 INFO                        main:647  Job starting...\n2014-12-09T17:29:14.717 DEBUG                   from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2014-12-09T17:29:14.717 INFO                   _new_conn:188  Starting new HTTP connection (1): es-cluster.cloudapp.net\n2014-12-09T17:29:14.748 DEBUG              _make_request:362  \"GET / HTTP/1.1\" 200 315\n2014-12-09T17:29:14.748 INFO         log_request_success:57   GET http://es-cluster.cloudapp.net:9200/ [status:200 request:0.031s]\n2014-12-09T17:29:14.748 DEBUG        log_request_success:59   > None\n2014-12-09T17:29:14.748 DEBUG        log_request_success:60   < {\n\"status\" : 200,\n\"name\" : \"James Proudstar\",\n\"version\" : {\n```\n\"number\" : \"1.2.1\",\n\"build_hash\" : \"6c95b759f9e7ef0f8e17f77d850da43ce8a4b364\",\n\"build_timestamp\" : \"2014-06-03T15:02:52Z\",\n\"build_snapshot\" : false,\n\"lucene_version\" : \"4.8\"\n```\n},\n\"tagline\" : \"You Know, for Search\"\n}\n2014-12-09T17:29:14.748 DEBUG              check_version:235  Detected Elasticsearch version 1.2.1\n2014-12-09T17:29:14.748 DEBUG                       main:671  argdict = {'host': 'es-cluster.cloudapp.net', 'port': 9200, 'log_level': 'INFO', 'auth': None, 'exclude_pattern': None, 'debug': True, 'separator': '.', 'older_than': 30, 'master_only': False, 'prefix': 'logstash', 'time_unit': 'days', 'dry_run': False, 'command': 'close', 'ssl': False, 'func': , 'timeout': 30, 'log_file': 'curator.log', 'url_prefix': ''}\n2014-12-09T17:29:14.748 INFO                command_loop:540  Beginning CLOSE operations...\n2014-12-09T17:29:14.748 DEBUG                   from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2014-12-09T17:29:14.748 DEBUG              _make_request:362  \"GET /logstash*/_settings?expand_wildcards=closed HTTP/1.1\" 200 18850\n2014-12-09T17:29:14.748 INFO         log_request_success:57   GET http://es-cluster.cloudapp.net:9200/logstash*/_settings?expand_wildcards=closed [status:200 request:0.000s]\n2014-12-09T17:29:14.748 DEBUG        log_request_success:59   > None\n2014-12-09T17:29:14.748 DEBUG        log_request_success:60   < {\"logstash-2014.12.08\":{\"settings\":{\"index\":{\"uuid\":\"qyglS3WsS6u2DjrotMC11g\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.11.01\":{\"settings\":{\"index\":{\"uuid\":\"SeHYSw86Q2ezmnsKUnwUOQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.11.27\":{\"settings\":{\"index\":{\"uuid\":\"T67P6JSkSkK6M3rfoFNpRw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.11.05\":{\"settings\":{\"index\":{\"uuid\":\"qlFb9NwwQnyKqtfIaTmADA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.11.22\":{\"settings\":{\"index\":{\"uuid\":\"QsU0OkhgSOiTYq7GTLLi_A\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.09.16\":{\"settings\":{\"index\":{\"uuid\":\"JLDEshR_R9i3S9rWM-hFvw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.09.18\":{\"settings\":{\"index\":{\"uuid\":\"B3DMALb1TGKdD6ZBQxIKwg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.09.02\":{\"settings\":{\"index\":{\"uuid\":\"gs1huJLpS2Oc5gxqUMZjnw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.10.28\":{\"settings\":{\"index\":{\"uuid\":\"rx-4e08ZRQqp0mrYvo-kTg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.09.14\":{\"settings\":{\"index\":{\"uuid\":\"nMTYI-l_SzKTVxVOzqApaw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.09.22\":{\"settings\":{\"index\":{\"uuid\":\"vd5KEeUGT_Gn5LplDTSMGg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.09.25\":{\"settings\":{\"index\":{\"uuid\":\"q_xQs5qMReiY4tIJ9rOI7Q\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.10.18\":{\"settings\":{\"index\":{\"uuid\":\"AFJK_z3ZQjSGO2HH07VAtg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.11.11\":{\"settings\":{\"index\":{\"uuid\":\"F-CgCbuzRUK_ps8EPlv0qQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.10.02\":{\"settings\":{\"index\":{\"uuid\":\"2qcqLZqlTXC1Shi4MSvufw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.09.12\":{\"settings\":{\"index\":{\"uuid\":\"eiRxngwxT0udTuR7nVSasw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.09.23\":{\"settings\":{\"index\":{\"uuid\":\"mK7to0poQvqffZKAHFswag\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.11.26\":{\"settings\":{\"index\":{\"uuid\":\"uTDYys__S8KIEdlMWCYyJg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.11.18\":{\"settings\":{\"index\":{\"uuid\":\"Jh_r-eDSSbqplrX_gtMGpA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.11.10\":{\"settings\":{\"index\":{\"uuid\":\"pDkgaP7CSrCteWm9pswJzA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.11.16\":{\"settings\":{\"index\":{\"uuid\":\"mH0MGE1cT-qmiG-DIj2jww\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.11.28\":{\"settings\":{\"index\":{\"uuid\":\"jGAhV7M0S6Gsz2fNPCgeYQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.09.20\":{\"settings\":{\"index\":{\"uuid\":\"iXPXxmafSpyr2EBpzT4WMQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.08.02\":{\"settings\":{\"index\":{\"uuid\":\"i1Qq5Zq6RrmBv4jUgigLQw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.09.15\":{\"settings\":{\"index\":{\"uuid\":\"Mhgfy3QHQzOtH58SpAUcXA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.09.29\":{\"settings\":{\"index\":{\"uuid\":\"OKh3Mz8LRiek5W2BI1MOxA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.11.30\":{\"settings\":{\"index\":{\"uuid\":\"sSt3S580TaCzoMWwnf4gsQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.10.07\":{\"settings\":{\"index\":{\"uuid\":\"a9HOwQvUQniZnTwiUhnw4g\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.10.19\":{\"settings\":{\"index\":{\"uuid\":\"nQEEbMo-QD2RdSYWY7TMVw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.12.04\":{\"settings\":{\"index\":{\"uuid\":\"L4WWx5ulTY-er12Y_1tagA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.10.01\":{\"settings\":{\"index\":{\"uuid\":\"94Y2mjwrTs-8Q_aU6J8Qfw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.10.06\":{\"settings\":{\"index\":{\"uuid\":\"961jltPOQsuzsfmb7APIrg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.12.07\":{\"settings\":{\"index\":{\"uuid\":\"MN9vT8sdT8So8bCcdYwv0Q\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.11.17\":{\"settings\":{\"index\":{\"uuid\":\"S6CoVreYQ4KoorWcsxfxWw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.11.21\":{\"settings\":{\"index\":{\"uuid\":\"8_ol7AOmToGFDpFrIZNPUA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.09.17\":{\"settings\":{\"index\":{\"uuid\":\"HgUkk5plSvuBDHIL5mkIlA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.10.27\":{\"settings\":{\"index\":{\"uuid\":\"sjxkBQbGS3m-CinWwOmvhA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.10.21\":{\"settings\":{\"index\":{\"uuid\":\"PBxWK27uRuaFGCKtyYwhDg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.10.11\":{\"settings\":{\"index\":{\"uuid\":\"nMCC5X9OQc-v3WjRyvU_JA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.10.31\":{\"settings\":{\"index\":{\"uuid\":\"YxjIzrhZQY6c4cXfby0DGQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.10.25\":{\"settings\":{\"index\":{\"uuid\":\"xeFi6F5YQ5-lnnVBZFYrow\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.12.02\":{\"settings\":{\"index\":{\"uuid\":\"7bCUCVCpSKGjceVe90heqw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.09.01\":{\"settings\":{\"index\":{\"uuid\":\"0L41NG33Q_afszX_dRcFZQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.10.03\":{\"settings\":{\"index\":{\"uuid\":\"sdmB8hcbRSCbsKIQNc2NvA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.11.06\":{\"settings\":{\"index\":{\"uuid\":\"xz1Hlal-QLWNNcfKR1GANA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.09.09\":{\"settings\":{\"index\":{\"uuid\":\"O_8cnughQzuRUNXqKiNo9Q\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.09.08\":{\"settings\":{\"index\":{\"uuid\":\"mYyaNU0SSZSquK6C-Xy4Yw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.12.09\":{\"settings\":{\"index\":{\"uuid\":\"vhZI69eTQISw6AZumjUL8Q\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.09.04\":{\"settings\":{\"index\":{\"uuid\":\"KbCEVKwIR-S_hGUOz20_Hw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.10.17\":{\"settings\":{\"index\":{\"uuid\":\"8Fgk-WUvSkWD_WgUWI4wzw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.11.23\":{\"settings\":{\"index\":{\"uuid\":\"IoaVSVZgSXiPYayEpInEXg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.12.03\":{\"settings\":{\"index\":{\"uuid\":\"FcugHEzmSdKX2QURJWhQrg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.12.05\":{\"settings\":{\"index\":{\"uuid\":\"1GTIg63mRgWIT944gjR67A\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.09.03\":{\"settings\":{\"index\":{\"uuid\":\"WEnOaIYvR7eJepKw2XHPgQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.08.01\":{\"settings\":{\"index\":{\"uuid\":\"LrpB2_uwQESWRk9y6aW5aA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.10.26\":{\"settings\":{\"index\":{\"uuid\":\"oeSO5CuuT72yJBk7gC92og\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.12.06\":{\"settings\":{\"index\":{\"uuid\":\"K2Mr6gNqRbOz8FuOJ8Xo1w\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.10.13\":{\"settings\":{\"index\":{\"uuid\":\"KcTNopS1Tiavo3PSP3ioaw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.11.20\":{\"settings\":{\"index\":{\"uuid\":\"FB3dDcgAQHG8FFShaFRJuQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.10.10\":{\"settings\":{\"index\":{\"uuid\":\"sX_wM5sXR0Gi_wggppbibw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.10.04\":{\"settings\":{\"index\":{\"uuid\":\"GbjZCNoYTEicwPGQoMq9AA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.09.24\":{\"settings\":{\"index\":{\"uuid\":\"c7FGSeBCTn2sUtwaH-tPaQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.10.16\":{\"settings\":{\"index\":{\"uuid\":\"NN-52JqhSoarXQeJ8eP0rA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.11.19\":{\"settings\":{\"index\":{\"uuid\":\"iK7SW_qtTTyNPDd5oMEBtA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.10.30\":{\"settings\":{\"index\":{\"uuid\":\"op3dWJFfTueXRA51MPzgYA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.11.09\":{\"settings\":{\"index\":{\"uuid\":\"YHQbTXbcTX2uKwRp5cBYgw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.10.09\":{\"settings\":{\"index\":{\"uuid\":\"wBn1OVtnQZiLCOOFc9RWWw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.11.13\":{\"settings\":{\"index\":{\"uuid\":\"t4O8l0zwRIqaoo66ZUkHcQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.09.10\":{\"settings\":{\"index\":{\"uuid\":\"u3KtuP57ShCv1y-bUZD_Fw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.10.23\":{\"settings\":{\"index\":{\"uuid\":\"_Y3TmhqpQC-xyC_tevdhQQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.07.19\":{\"settings\":{\"index\":{\"uuid\":\"qEi-LLsRQse3_xc-xlayiA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.10.08\":{\"settings\":{\"index\":{\"uuid\":\"4KwhB22qSQO9bWKMWebOaw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.12.01\":{\"settings\":{\"index\":{\"uuid\":\"oWUQRTmOTUijCm3-L3VBfw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.10.14\":{\"settings\":{\"index\":{\"uuid\":\"Y6nvT62NSkOUjE4MKbSjOQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.10.15\":{\"settings\":{\"index\":{\"uuid\":\"lA2HgstYSp-WGNFjx_iCMQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.09.05\":{\"settings\":{\"index\":{\"uuid\":\"0Gndl8A-QkWAJvt2AmZbog\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.09.28\":{\"settings\":{\"index\":{\"uuid\":\"l7ddf-lKQweyLc_WFO-kPQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.09.06\":{\"settings\":{\"index\":{\"uuid\":\"O9P9hPMnTWmhQOCrbDZ48w\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.10.29\":{\"settings\":{\"index\":{\"uuid\":\"4NKioipKR3yBlfrir9qevw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.11.29\":{\"settings\":{\"index\":{\"uuid\":\"NpVwfZgER7O4lLWfuYhobg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.11.08\":{\"settings\":{\"index\":{\"uuid\":\"FXGtSrJWSEWJW_eA_aRhLg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.09.07\":{\"settings\":{\"index\":{\"uuid\":\"Ifpxq0LsTI2BIP-7TSroMw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.11.07\":{\"settings\":{\"index\":{\"uuid\":\"x1AB1fAJTqOZOD13Wy0TPw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.11.12\":{\"settings\":{\"index\":{\"uuid\":\"16tMFcI-TK2gtdRh-N657g\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.09.11\":{\"settings\":{\"index\":{\"uuid\":\"qM4OKEYxSQqNkXrlQsatxw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.09.13\":{\"settings\":{\"index\":{\"uuid\":\"uXyiIJ0YSDWq6vIZd4ruzw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.10.24\":{\"settings\":{\"index\":{\"uuid\":\"sA9ZKjxLRVCXPaf4QNfNzg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.11.03\":{\"settings\":{\"index\":{\"uuid\":\"SIsF0GQZT9Cx9HLFaZKXqg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.11.24\":{\"settings\":{\"index\":{\"uuid\":\"ftAvk-ZOQ5qtpHfUKwmArw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.10.05\":{\"settings\":{\"index\":{\"uuid\":\"i7ZFLw7FSEid8ZS6xBiZoQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.09.19\":{\"settings\":{\"index\":{\"uuid\":\"roExsJVGRSGjY4VJY6KmRg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.11.15\":{\"settings\":{\"index\":{\"uuid\":\"R9veQ4AmQIaLk-htyYYsVA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.10.22\":{\"settings\":{\"index\":{\"uuid\":\"Cc2vEtbCQGqBdKLHyu46sA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.11.25\":{\"settings\":{\"index\":{\"uuid\":\"79P-CpmaQDeV8906Mtq3UQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.09.30\":{\"settings\":{\"index\":{\"uuid\":\"YXsVRSPVQxyEaA9krEijmA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.11.02\":{\"settings\":{\"index\":{\"uuid\":\"eIhxum99Sr6DmCUQRCfiSg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.09.26\":{\"settings\":{\"index\":{\"uuid\":\"6KH0UplpRFqM4-7D71UA0w\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.09.21\":{\"settings\":{\"index\":{\"uuid\":\"1gSsL637SMuY-CntF9Re5Q\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.09.27\":{\"settings\":{\"index\":{\"uuid\":\"MRzb0maPQcKqQXd8zpo39Q\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.11.14\":{\"settings\":{\"index\":{\"uuid\":\"3G2E0IN9QXqTTGHD_cRijA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.10.20\":{\"settings\":{\"index\":{\"uuid\":\"_PXYM7r8RVmE7dFY8R1M9g\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.10.12\":{\"settings\":{\"index\":{\"uuid\":\"BI0NKYeFSCe4yGY8vjmEhg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}},\"logstash-2014.11.04\":{\"settings\":{\"index\":{\"uuid\":\"8tclN0E5QC237ha9oM602g\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1020199\"}}}}}\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.07.19\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.08.01\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.08.02\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.09.01\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.09.02\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.09.03\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.09.04\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.09.05\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.09.06\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.09.07\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.09.08\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.09.09\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.09.10\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.09.11\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.09.12\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.09.13\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.09.14\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.09.15\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.09.16\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.09.17\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.09.18\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.09.19\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.09.20\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.09.21\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.09.22\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.09.23\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.09.24\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.09.25\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.09.26\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.09.27\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.09.28\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.09.29\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.09.30\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.10.01\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.10.02\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.10.03\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.10.04\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.10.05\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.10.06\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.10.07\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.10.08\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.10.09\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.10.10\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.10.11\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.10.12\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.10.13\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.10.14\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.10.15\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.10.16\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.10.17\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.10.18\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.10.19\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.10.20\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.10.21\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.10.22\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.10.23\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.10.24\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.10.25\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.10.26\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.10.27\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.10.28\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.10.29\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.10.30\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.10.31\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.11.01\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.11.02\n2014-12-09T17:29:14.780 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.11.03\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.11.04\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.11.05\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.11.06\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.11.07\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.11.08\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.11.09\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.11.10\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.11.11\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.11.12\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.11.13\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.11.14\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.11.15\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.11.16\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.11.17\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.11.18\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.11.19\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.11.20\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.11.21\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.11.22\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.11.23\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.11.24\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.11.25\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.11.26\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.11.27\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.11.28\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.11.29\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.11.30\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.12.01\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.12.02\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.12.03\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.12.04\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.12.05\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.12.06\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.12.07\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.12.08\n2014-12-09T17:29:14.795 ERROR          find_expired_data:300  Could not find a valid timestamp for logstash-2014.12.09\n2014-12-09T17:29:14.795 INFO                command_loop:582  CLOSE index operations completed.\n2014-12-09T17:29:14.795 INFO                        main:674  Done in 0:00:00.093750.\nFrom: Aaron Mildenstein [mailto:notifications@github.com] \nSent: mardi 9 d\u00e9cembre 2014 18:27\nTo: elasticsearch/curator\nCc: xpillons\nSubject: Re: [curator] Could not find a valid timestamp for logstash-xxxxx (#231)\nCan you paste an actual example?\n\u2014\nReply to this email directly or view it on GitHub https://github.com/elasticsearch/curator/issues/231#issuecomment-66331515 .  https://github.com/notifications/beacon/AHAkoprTDexhQzROouUGDu84O0Wp4qc6ks5nVzaAgaJpZM4DGN_e.gif \n. Here it is\nC:\\Python34\\Scripts>curator --debug --logfile curator.log --host es-cluster.cloudapp.net close --prefix logstash --older-than 30\nFrom: Aaron Mildenstein [mailto:notifications@github.com] \nSent: mardi 9 d\u00e9cembre 2014 21:31\nTo: elasticsearch/curator\nCc: xpillons\nSubject: Re: [curator] Could not find a valid timestamp for logstash-xxxxx (#231)\nThank you. Can you also send the command-line used to run this?\n\u2014\nReply to this email directly or view it on GitHub https://github.com/elasticsearch/curator/issues/231#issuecomment-66361424 .  https://github.com/notifications/beacon/AHAkotSPBMM1dekFMgnXQH9npZBS65hFks5nV2GngaJpZM4DGN_e.gif \n. Great it now works :) thanks a lot, but in fact not so trivial.\nFrom: Aaron Mildenstein [mailto:notifications@github.com] \nSent: mardi 9 d\u00e9cembre 2014 21:40\nTo: elasticsearch/curator\nCc: xpillons\nSubject: Re: [curator] Could not find a valid timestamp for logstash-xxxxx (#231)\nThe same thing will appear with Marvel indices, but the prefix will be .marvel-\n\u2014\nReply to this email directly or view it on GitHub https://github.com/elasticsearch/curator/issues/231#issuecomment-66362830 .  https://github.com/notifications/beacon/AHAkoraobeaCrisUuqz4M3MCO_xSU0POks5nV2OvgaJpZM4DGN_e.gif \n. Make sense. Thank you.\nFrom: Aaron Mildenstein [mailto:notifications@github.com] \nSent: mardi 9 d\u00e9cembre 2014 21:41\nTo: elasticsearch/curator\nCc: xpillons\nSubject: Re: [curator] Could not find a valid timestamp for logstash-xxxxx (#231)\nThe bottom line is that the prefix must be exact or the regular expressions used to split prefix/timestamp/suffix won't know where the division is.\n\u2014\nReply to this email directly or view it on GitHub https://github.com/elasticsearch/curator/issues/231#issuecomment-66363012 .  https://github.com/notifications/beacon/AHAkotqKs-SKRlA2ieJfb0BmL-DZpQMcks5nV2PugaJpZM4DGN_e.gif \n. ",
    "bhennigar": "I'm having the same issue. Trying to delete indices older than 3 days.\ncurator delete --older-than 3 --prefix bro-\n2014-12-10 15:22:47,317 INFO      Job starting...\n2014-12-10 15:22:47,320 INFO      Deleting indices...\n2014-12-10 15:22:47,383 INFO      Speficied indices deleted.\n2014-12-10 15:22:47,383 INFO      Done in 0:00:00.076753.\nNo indices were deleted.\n. bro-201412060900\nbro-2014122041000\nbro-201412041100\nI have indices starting Dec 4 up to today, Dec 10. After running the command, all of the data is still there.\n. Command ran but still nothing removed\n. > curator --debug delete --time-unit hours --timestring '%Y%m%d%H00' --older-than 3 --prefix bro-\n\n2014-12-11 08:33:53,852 DEBUG            curator.curator    filter_by_timestamp:368  Unable to match bro-201412041300 with regular expression ^bro-(\\d{4}\\d{2}\\d{2}\\d{2}\\0\\0)$.  Error: 'NoneType' object has no attribute 'group'\n. Progress!  New error now. Is there a preferred way to submit a long output?  Short version appears to be this:\nelasticsearch.exceptions.ConnectionError: ConnectionError(HTTPConnectionPool(host='localhost', port=9200): Max retries exceeded with url: /bro-201412040900) caused by: MaxRetryError(HTTPConnectionPool(host='localhost', port=9200): Max retries exceeded with url: /bro-201412040900)\n. ```\ncurator --debug delete --time-unit hours --timestring '%Y%m%d%H' --older-than 3 --prefix bro- --suffix 00\n2014-12-11 14:03:59,282 INFO                        root                   main:327  Job starting...\n2014-12-11 14:03:59,283 INFO      urllib3.connectionpool              _new_conn:191  Starting new HTTP connection (1): localhost\n2014-12-11 14:03:59,286 DEBUG     urllib3.connectionpool          _make_request:283  \"GET / HTTP/1.1\" 200 340\n2014-12-11 14:03:59,286 INFO               elasticsearch    log_request_success:57   GET http://localhost:9200/ [status:200 request:0.003s]\n2014-12-11 14:03:59,286 DEBUG              elasticsearch    log_request_success:59   > None\n2014-12-11 14:03:59,286 DEBUG              elasticsearch    log_request_success:60   < {\n  \"status\" : 200,\n  \"name\" : \"gsa-brownian\",\n  \"cluster_name\" : \"elasticsearch\",\n  \"version\" : {\n    \"number\" : \"1.4.1\",\n    \"build_hash\" : \"89d3241d670db65f994242c8e8383b169779e2d4\",\n    \"build_timestamp\" : \"2014-11-26T15:49:29Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"4.10.2\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n\n2014-12-11 14:03:59,287 DEBUG     curator.curator_script          check_version:221  Detected Elasticsearch version 1.4.1\n2014-12-11 14:03:59,287 DEBUG                       root                   main:352  Matching indices with pattern: bro-%Y%m%d%H\n2014-12-11 14:03:59,287 DEBUG                       root                   main:356  argdict = {'url_prefix': '', 'func': , 'prefix': 'bro-', 'log_level': 'INFO', 'timestring': '%Y%m%d%H', 'dry_run': \nFalse, 'exclude_pattern': None, 'logformat': 'Default', 'auth': None, 'ssl': False, 'host': 'localhost', 'command': 'delete', 'time_unit': 'hours', 'timeout': 30, 'debug': True, 'disk_space': None, 'log_file': None, 'master_only': \nFalse, 'port': 9200, 'older_than': 3, 'suffix': '00'}\n2014-12-11 14:03:59,287 INFO                        root                 delete:951  Deleting indices...\n2014-12-11 14:03:59,353 DEBUG     urllib3.connectionpool          _make_request:283  \"GET //_settings?expand_wildcards=open%2Cclosed HTTP/1.1\" 200 27630\n2014-12-11 14:03:59,354 INFO               elasticsearch    log_request_success:57   GET http://localhost:9200//_settings?expand_wildcards=open%2Cclosed [status:200 request:0.066s]\n2014-12-11 14:03:59,354 DEBUG              elasticsearch    log_request_success:59   > None\n2014-12-11 14:03:59,354 DEBUG              elasticsearch    log_request_success:60   < {\"bro-201412051900\":{\"settings\":{\"index\":{\"creation_date\":\"1417820400439\",\"uuid\":\"uhrh_6F8Q-\na_nwPBDg4k7w\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412070200\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417932001812\",\"uuid\":\"1434xd8OTyKy65tos36qbg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412070500\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417942800965\",\"uuid\":\"6zN67bqUSfek8naNOoQmQw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412060900\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417870803394\",\"uuid\":\"FD1AyhtPQ1iT44QYh-9wHA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412080100\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418014806525\",\"uuid\":\"a6fFwo8LTq6CbAembM2y7w\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412092100\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418173202935\",\"uuid\":\"Me_L7tQMRDqVuTWhmXUokg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412070700\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417950005372\",\"uuid\":\"kyZKUyHpQXa7xEfJEBFldA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412060700\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417863605560\",\"uuid\":\"HWFKPGonTxeIx4d4cIKwpA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412081100\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418050801366\",\"uuid\":\"F5ohDfeMQXWr794a_yrdHw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412070100\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417928405475\",\"uuid\":\"9muQXdIbTTWZ4F_XN7Iepg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412100500\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418202001947\",\"uuid\":\"im9xBLfnR0ioQgDCRXRIxg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412090500\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418115603819\",\"uuid\":\"qr9f4z4kQ_S8KVvyrbtFGg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412052200\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417831200747\",\"uuid\":\"K9DXYjg1SeSpoJhWjjpmPA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412070000\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417924800485\",\"uuid\":\"z69SRQx0TsisP7UwfjGmBQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412082300\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418094001449\",\"uuid\":\"1m3dZy46RPa-nD7X4jmOwg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412050900\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417784400952\",\"uuid\":\"GTzvId8bTtCOoGboyeEXvA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412081900\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418079601541\",\"uuid\":\"oxxZSJFLToO4kjn71E6l4A\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412052000\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417824000980\",\"uuid\":\"3eoPZX9UQZCRmOk96alV4g\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412051000\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417788000832\",\"uuid\":\"1cb9FSQ-RpGYcbxaG28qIw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412041200\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417708800682\",\"uuid\":\"cxQtQlbuRkyXyhtBXWHpSg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412090100\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418101204995\",\"uuid\":\"GOhFyZAxTliOBfTijl7maA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412090300\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418108404168\",\"uuid\":\"6TP6PmwIT7yvuf27oTtL7w\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412041400\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417716001425\",\"uuid\":\"HQE1XmR0SN65vRq7bhvLww\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412072300\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418007601920\",\"uuid\":\"ZmitsC5KQ1iGIOvw-JCw6w\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412072100\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418000401875\",\"uuid\":\"9viIoxhiRvyG6QL8Gto-SQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412061500\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417892401515\",\"uuid\":\"yYHNOYAkQPer6gly9eNl3g\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412091200\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418140801580\",\"uuid\":\"dHVwX9VhRuOxUvhnx0AJpQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412080300\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418022001325\",\"uuid\":\"YvWOSpZnTmqt0qc_5BtoOA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412060500\":{\"settings\":{\"index\":{\"creation_date\":\"1417856401636\",\"uuid\":\"R6J-\nkETdS3OQbBJi0tZdUQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412090800\":{\"settings\":{\"index\":{\"creation_date\":\"1418126401622\",\"uuid\":\"ZvR1R1nOQ6-\nfn5tLttpwKQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412090900\":{\"settings\":{\"index\":{\"creation_date\":\"1418130001812\",\"uuid\":\"PNHcJZ4iQD-\nWnwo41mZG4g\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412041000\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417702365831\",\"uuid\":\"OQ1mke_MSSOXm1A1o8OeXw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412091100\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418137200869\",\"uuid\":\"AgDvpwjPTpa_leqTavjXWg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412041600\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417723200859\",\"uuid\":\"HihtQ_wdTdG3CznzSCB4Fg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412080500\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418029200776\",\"uuid\":\"eCexYbVGQOaxIPususoD0w\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412091800\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418162401122\",\"uuid\":\"SSdntQbZQkSZYG7sWwdBXg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412091600\":{\"settings\":{\"index\":{\"creation_date\":\"1418155200834\",\"uuid\":\"wqRyT-\nQcR26GcoT0szkMjg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412091300\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418144401823\",\"uuid\":\"dcLMEV6yRUulriOCOx3RdQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412080700\":{\"settings\":{\"index\":{\"creation_date\":\"1418036402378\",\"uuid\":\"s-\nQuVJQZQt-OpGbbWKLf8g\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412061300\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417885201312\",\"uuid\":\"JIb1bGdNQ8qAu7dPvciIXw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412100100\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418187601384\",\"uuid\":\"M_TBbC1sRQSuuyQH1KZDzA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412091500\":{\"settings\":{\"index\":{\"creation_date\":\"1418151602566\",\"uuid\":\"nI-\nypFctTI2h20QqDQu_CQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412061100\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417878000815\",\"uuid\":\"M8f9VxI8TsKEi2nYUH9DZA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412091400\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418148000800\",\"uuid\":\"JHV9LCKORVKyck7Z1F83mA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412061700\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417899603541\",\"uuid\":\"RYPQUQB8SyW9ruamSP2LeA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412081800\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418076000782\",\"uuid\":\"REmgabWmQ7eRwiECvy36LA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412081500\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418065200826\",\"uuid\":\"DTXkT8KRRuKxK1av8AJSVA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412100000\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418184003713\",\"uuid\":\"PG8TXK_wRTqegIgYPhWHSg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412051500\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417806000673\",\"uuid\":\"F4sE1IZvTz2LQzB7hKQlMw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412062300\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417921206314\",\"uuid\":\"_u8KteRjT-CnKCLQggZIZA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412081600\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418068801982\",\"uuid\":\"udXTm7mKSWiV8CIVVWy52w\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412041900\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417734001621\",\"uuid\":\"5N3ZwwEdQ2iHkCmaSHdjyg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412061800\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417903207477\",\"uuid\":\"JAj9MiWfQBigMnnlXqEo5w\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412051300\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417798800752\",\"uuid\":\"KCO8h2sZSgak87I0Vf3dLw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412071000\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417960807792\",\"uuid\":\"WWdjMOYBQwGnrqQnSSxndA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412081700\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418072401803\",\"uuid\":\"gz1EcABDTKWXpyLKbRJaVg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412040900\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417698148587\",\"uuid\":\"HV9JWS-JTc2vri1vgONdhg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412071700\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417986006342\",\"uuid\":\"KOjeWQk0TiWvQzsnKFbcUg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412042200\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417744800767\",\"uuid\":\"QUNP9oH_Rlqo3LK3SN_BGQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412050700\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417777202918\",\"uuid\":\"VbVy2c-CQwam0oBJtxFpOQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412071400\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417975200845\",\"uuid\":\"XsqDZbA1QDak8JY6TZ5Cqw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412082200\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418090401572\",\"uuid\":\"kQKp8CSaSwyQXzes_yy4Ng\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412062100\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417914001977\",\"uuid\":\"_X03zOisSs6B4aWydxKNMw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412051700\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417813202386\",\"uuid\":\"zRsC1adDTNOGkFed6loN9g\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412091900\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418166003169\",\"uuid\":\"0evDNKd9QgaPYJksOx1hMA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412081400\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418061601756\",\"uuid\":\"yK2kplmfTvqmLVn94XVL4g\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412081300\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418058003978\",\"uuid\":\"ypR5M0M3SLK2wTvd-netDA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412071900\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417993206803\",\"uuid\":\"2dCXktBFRjGqs9QJX8Jzmg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412061900\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417906805409\",\"uuid\":\"74zkSfb9Qxut0XKVa3k5bQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412071800\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417989603285\",\"uuid\":\"xqQknxDeT_eju-V5WA6YXA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412100300\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418194801037\",\"uuid\":\"xqUgCyo4QrywLhdjxoinsA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412091700\":{\"settings\":{\"index\":{\"creation_date\":\"1418158801714\",\"uuid\":\"Y6P2-\nRRPSN-vryjjFHPrMQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412100800\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418212801952\",\"uuid\":\"zacRYdBoQl6NRgsmaNvXLw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412082000\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418083202939\",\"uuid\":\"Qf7RZ_YIScyCaLY0v9HqnA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412071200\":{\"settings\":{\"index\":{\"creation_date\":\"1417968006367\",\"uuid\":\"W-\nfWTRKnTBW0x7WDIz1xCA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412042000\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417737602082\",\"uuid\":\"sXIrAqe6ReGutWCQppQZPQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412100200\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418191202151\",\"uuid\":\"HfjAE28_Sgiju9BlCjokBw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412070400\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417939201328\",\"uuid\":\"vBoQlKnORieZr8Brd_ZLlQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412081000\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418047200433\",\"uuid\":\"PuUAlcSYR-KFCFLCdESiZg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412050400\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417766401701\",\"uuid\":\"bPbW6dHIRXK7WBCzMHWb0A\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412101000\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418220001182\",\"uuid\":\"k5rAlKiVQqi8jPJgR9o2-w\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412100400\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418198400530\",\"uuid\":\"UkCazlCXTTmRbCeBTdKM0A\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412050300\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417762802553\",\"uuid\":\"QCdP30nSR0u2slM7jhDc7Q\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412052300\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417834802487\",\"uuid\":\"usAdsab-QQufqZhiykkHZw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412060600\":{\"settings\":{\"index\":{\"creation_date\":\"1417860004560\",\"uuid\":\"-\nSLNKHx9QeGlcer6UTA8oQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412090600\":{\"settings\":{\"index\":{\"creation_date\":\"1418119202606\",\"uuid\":\"K9aH6tIARxKmjeVo8u-\nNeA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412050500\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417770000418\",\"uuid\":\"Ow98lR7iR6muFAG9Aq2Cyw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412081200\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418054400830\",\"uuid\":\"b97h0MGRTee2S2d7mgAMXA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412071600\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417982406975\",\"uuid\":\"0fb9VpK9QDWOI3szFX4nyg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412090400\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418112003775\",\"uuid\":\"mrSA6iAeQUWj7fVopipJdg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412060800\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417867203320\",\"uuid\":\"DMi2RSTnQaKbIsbB_8UaTA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412100600\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418205601723\",\"uuid\":\"Cja95WbDRC2p8K88jsaVrQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412041300\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417712409237\",\"uuid\":\"kUDRbCv_QU-s94hcRrvS3g\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412071500\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417978801722\",\"uuid\":\"_v0BCo26SmirepniAYwIgg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412050600\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417773603943\",\"uuid\":\"u22IMouVQX6MXNMorkK-0A\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412070600\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417946401726\",\"uuid\":\"3CG2jlhWTqGRcL-ez0NeAw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412050800\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417780800423\",\"uuid\":\"cqCvnvx5T5CJKhR3_wsZzQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412042300\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417748400977\",\"uuid\":\"Luok5cP-RFeAvFyUsRnYJA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412052100\":{\"settings\":{\"index\":{\"creation_date\":\"1417827604737\",\"uuid\":\"-\nE0y8HAWQpicHOlvyeR5xg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412050200\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417759201563\",\"uuid\":\"BTRpabBzQS63bV2XNCAlig\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412080800\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418040003976\",\"uuid\":\"nvzpqQQzRP2EZuj5TzCJyw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412090000\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418097602366\",\"uuid\":\"Ln_IZaBxQ1aQhcRUkXtP9g\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412041500\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417719602437\",\"uuid\":\"vUoqaAKkSWq1RR6UmpIJXg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412051100\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417791600970\",\"uuid\":\"5lKO6l-2SHShzS0PxtFAvw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412080400\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418025601903\",\"uuid\":\"6C2CsywNSnqR5YTtHM-g9g\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412041700\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417726801922\",\"uuid\":\"cYL9rFH5QwyPd3A2M5s7FQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412061600\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417896001038\",\"uuid\":\"jO1u8UB1QhajTgoxMRRtJw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412080900\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418043601435\",\"uuid\":\"5HKtZPISQ3-_ByTOTcsl3Q\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412050100\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417755602850\",\"uuid\":\"gwu_sBJzS1qhsuFWRvcLfQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412060000\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417838403357\",\"uuid\":\"dBHJSz2xTrqxgoLuUHMFCw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412061400\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417888803796\",\"uuid\":\"9Yr2jlvHTaebOcJ1UV2ehA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412070800\":{\"settings\":{\"index\":{\"creation_date\":\"1417953607330\",\"uuid\":\"M-\nvOWO7YSnK9_U2W8B1x0g\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412080200\":{\"settings\":{\"index\":{\"creation_date\":\"1418018402509\",\"uuid\":\"u6lv0rpzS-\nSSRLLoZgg3Sg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412041100\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417705216353\",\"uuid\":\"_GTYeDqHSR6dCHyt2YEtlQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412090200\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418104804383\",\"uuid\":\"vBs0r3aeRwipgyPKx-2TtA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412050000\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417752000940\",\"uuid\":\"KOzMa6FxT5GmmYfwsvWZiQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412072200\":{\"settings\":{\"index\":{\"creation_date\":\"1418004000850\",\"uuid\":\"Lcq-\nq0eFQSKQVNTZr7YQoA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412092000\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418169602775\",\"uuid\":\"Oc488V_aQOqcyvelVVDTFg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412060300\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417849201961\",\"uuid\":\"SQkaVeeuTcOc6G9hwzEMdQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412070900\":{\"settings\":{\"index\":{\"creation_date\":\"1417957201978\",\"uuid\":\"-\ncYdfMXBRm2JRep8Z1PGOg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412080600\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418032804015\",\"uuid\":\"RFK7u8FtROeTytGeQGETFg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412091000\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418133601784\",\"uuid\":\"9NJXkyilSFOrB6Dyb7Fz6A\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"@bro-meta\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417698147861\",\"uuid\":\"cxowweMxQ4KWm2HLlzhkTA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412071100\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417964405644\",\"uuid\":\"U2A1tiRhTpesB8nFtOM9ug\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412061000\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417874404730\",\"uuid\":\"aM_BsFmkSSyHyejUTq5Txg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412060400\":{\"settings\":{\"index\":{\"creation_date\":\"1417852801321\",\"uuid\":\"laxC-\n9QPT8eNBYzm4n4xyw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412051400\":{\"settings\":{\"index\":{\"creation_date\":\"1417802404011\",\"uuid\":\"daVbP-\nZ3TOm_9mQSzE1vhw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412072000\":{\"settings\":{\"index\":{\"creation_date\":\"1417996804249\",\"uuid\":\"PSH8BDeXRpubSca-\n2RiI5Q\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412061200\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417881603937\",\"uuid\":\"6IjpL5BUQkmz8Pcs5OC7_w\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412060200\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417845602396\",\"uuid\":\"_6pGk8lVR6y5ixp1mseWnQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412060100\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417842000946\",\"uuid\":\"t_iCmsqvRy-zQjBp1Blq8w\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412051200\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417795200689\",\"uuid\":\"oJ4z2ooFTx2Y3R5I3wShnw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412041800\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417730401629\",\"uuid\":\"kYHpIIqGR_GrdhWq9NF4lQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412100900\":{\"settings\":{\"index\":{\"creation_date\":\"1418216402277\",\"uuid\":\"b-\nZSqDMmQ6ip9QiThc-aiA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412042100\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417741201335\",\"uuid\":\"lDDMorG1RtSfOFrJx8BGOQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412062200\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417917606443\",\"uuid\":\"6B8zy1sDR1C4cQ3dhmbCFQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412100700\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418209201535\",\"uuid\":\"N5YinGVNQxaJdkIT1Lxs4g\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412051600\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417809600633\",\"uuid\":\"qW1EeqLIRmGM_KMftfDqEA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412082100\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418086801455\",\"uuid\":\"q8WCtaPISJ24wyWkxkVT2A\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412080000\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418011200641\",\"uuid\":\"mdzWvukCTy-b6ZA80ktUew\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412092200\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418176801769\",\"uuid\":\"q5rDwxcQS_2ITxbzYglFTQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412051800\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417816800983\",\"uuid\":\"2GHoQ9qZTk6C50BqUIRZWg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412070300\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417935603323\",\"uuid\":\"rztd_uh8RkiNNJsGxp7xoQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412090700\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418122801462\",\"uuid\":\"2HVZ_2soT3q9AeKpGiL0Rg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412092300\":{\"settings\":{\"index\":\n{\"creation_date\":\"1418180401752\",\"uuid\":\"C_gMisJsRcqN1scfzasi0g\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412071300\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417971602699\",\"uuid\":\"2TvbUMPjQGe03DJSit1K1A\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}},\"bro-201412062000\":{\"settings\":{\"index\":\n{\"creation_date\":\"1417910401902\",\"uuid\":\"i7JjeSBTTue6NP_sWBnWGw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"1040199\"}}}}}\n2014-12-11 14:04:29,427 WARNING            elasticsearch       log_request_fail:76   DELETE http://localhost:9200/bro-201412040900 [status:N/A request:30.035s]\nTraceback (most recent call last):\n  File \"/var/www/Brownian/local/lib/python2.7/site-packages/elasticsearch/connection/http_urllib3.py\", line 46, in perform_request\n    response = self.pool.urlopen(method, url, body, retries=False, headers=headers, kw)\n  File \"/var/www/Brownian/local/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py\", line 442, in urlopen\n    timeout)\nTimeoutError: HTTPConnectionPool(host='localhost', port=9200): Request timed out. (timeout=30)\n2014-12-11 14:04:29,477 INFO               elasticsearch       log_request_fail:84   > None\n2014-12-11 14:04:29,477 WARNING            elasticsearch              mark_dead:141  Connection  has failed for 1 times in a row, putting on 60 second timeout.\n2014-12-11 14:04:29,477 INFO               elasticsearch              resurrect:185  Resurrecting connection  (force=True).\n2014-12-11 14:04:29,478 WARNING   urllib3.connectionpool                urlopen:468  Retrying (0 attempts remain) after connection broken by 'CannotSendRequest()': /bro-201412040900\n2014-12-11 14:04:29,478 WARNING            elasticsearch       log_request_fail:76   DELETE http://localhost:9200/bro-201412040900 [status:N/A request:0.000s]\nTraceback (most recent call last):\n  File \"/var/www/Brownian/local/lib/python2.7/site-packages/elasticsearch/connection/http_urllib3.py\", line 46, in perform_request\n    response = self.pool.urlopen(method, url, body, retries=False, headers=headers, kw)\n  File \"/var/www/Brownian/local/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py\", line 472, in urlopen\n    release_conn=release_conn, response_kw)\n  File \"/var/www/Brownian/local/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py\", line 390, in urlopen\n    raise MaxRetryError(self, url)\nMaxRetryError: HTTPConnectionPool(host='localhost', port=9200): Max retries exceeded with url: /bro-201412040900\n2014-12-11 14:04:29,478 INFO               elasticsearch       log_request_fail:84   > None\n2014-12-11 14:04:29,478 WARNING            elasticsearch              mark_dead:141  Connection  has failed for 2 times in a row, putting on 120 second timeout.\n2014-12-11 14:04:29,478 INFO               elasticsearch              resurrect:185  Resurrecting connection  (force=True).\n2014-12-11 14:04:29,478 INFO      urllib3.connectionpool              _new_conn:191  Starting new HTTP connection (2): localhost\n2014-12-11 14:04:59,509 WARNING            elasticsearch       log_request_fail:76   DELETE http://localhost:9200/bro-201412040900 [status:N/A request:30.031s]\nTraceback (most recent call last):\n  File \"/var/www/Brownian/local/lib/python2.7/site-packages/elasticsearch/connection/http_urllib3.py\", line 46, in perform_request\n    response = self.pool.urlopen(method, url, body, retries=False, headers=headers, kw)\n  File \"/var/www/Brownian/local/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py\", line 442, in urlopen\n    timeout)\nTimeoutError: HTTPConnectionPool(host='localhost', port=9200): Request timed out. (timeout=30)\n2014-12-11 14:04:59,510 INFO               elasticsearch       log_request_fail:84   > None\n2014-12-11 14:04:59,510 WARNING            elasticsearch              mark_dead:141  Connection  has failed for 3 times in a row, putting on 240 second timeout.\n2014-12-11 14:04:59,510 INFO               elasticsearch              resurrect:185  Resurrecting connection  (force=True).\n2014-12-11 14:04:59,510 WARNING   urllib3.connectionpool                urlopen:468  Retrying (0 attempts remain) after connection broken by 'CannotSendRequest()': /bro-201412040900\n2014-12-11 14:04:59,510 WARNING            elasticsearch       log_request_fail:76   DELETE http://localhost:9200/bro-201412040900 [status:N/A request:0.000s]\nTraceback (most recent call last):\n  File \"/var/www/Brownian/local/lib/python2.7/site-packages/elasticsearch/connection/http_urllib3.py\", line 46, in perform_request\n    response = self.pool.urlopen(method, url, body, retries=False, headers=headers, kw)\n  File \"/var/www/Brownian/local/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py\", line 472, in urlopen\n    release_conn=release_conn, response_kw)\n  File \"/var/www/Brownian/local/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py\", line 390, in urlopen\n    raise MaxRetryError(self, url)\nMaxRetryError: HTTPConnectionPool(host='localhost', port=9200): Max retries exceeded with url: /bro-201412040900\n2014-12-11 14:04:59,510 INFO               elasticsearch       log_request_fail:84   > None\n2014-12-11 14:04:59,510 WARNING            elasticsearch              mark_dead:141  Connection  has failed for 4 times in a row, putting on 480 second timeout.\nTraceback (most recent call last):\n  File \"/var/www/Brownian/bin/curator\", line 9, in \n    load_entry_point('elasticsearch-curator==2.0.2', 'console_scripts', 'curator')()\n  File \"/var/www/Brownian/local/lib/python2.7/site-packages/curator/curator_script.py\", line 357, in main\n    arguments.func(client, argdict)\n  File \"/var/www/Brownian/local/lib/python2.7/site-packages/curator/curator.py\", line 959, in delete\n    _op_loop(client, matching_indices, op=delete_index, dry_run=dry_run, kwargs)\n  File \"/var/www/Brownian/local/lib/python2.7/site-packages/curator/curator.py\", line 704, in _op_loop\n    skipped = op(client, item, kwargs)\n  File \"/var/www/Brownian/local/lib/python2.7/site-packages/curator/curator.py\", line 552, in delete_index\n    client.indices.delete(index=index_name)\n  File \"/var/www/Brownian/local/lib/python2.7/site-packages/elasticsearch/client/utils.py\", line 68, in _wrapped\n    return func(*args, params=params, kwargs)\n  File \"/var/www/Brownian/local/lib/python2.7/site-packages/elasticsearch/client/indices.py\", line 157, in delete\n    params=params)\n  File \"/var/www/Brownian/local/lib/python2.7/site-packages/elasticsearch/transport.py\", line 284, in perform_request\n    status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)\n  File \"/var/www/Brownian/local/lib/python2.7/site-packages/elasticsearch/connection/http_urllib3.py\", line 51, in perform_request\n    raise ConnectionError('N/A', str(e), e)\nelasticsearch.exceptions.ConnectionError: ConnectionError(HTTPConnectionPool(host='localhost', port=9200): Max retries exceeded with url: /bro-201412040900) caused by: MaxRetryError(HTTPConnectionPool(host='localhost', port=9200): Max \nretries exceeded with url: /bro-201412040900)\n\n```\n. This worked! Thank you so much!\n. I have timeout set to 900. The error appears immediately after starting the command.\n. No luck... I reinstalled curator as well.  Not a huge deal to rebuild the server as it's mostly for testing. I just can't figure out what changed.  Thanks\n. Sorry to bug you again but I rebuilt my server using the same process as I always use (worked in the past) and I'm still getting the error.\nHere's the output from the curator install\n```\npip install elasticsearch-curator                                                                             Downloading/unpacking elasticsearch-curator\nDownloading elasticsearch-curator-2.1.1.tar.gz\n  Running setup.py (path:/var/www/Brownian/build/elasticsearch-curator/setup.py)                                                                              egg_info for package elasticsearch-curator\nwarning: no files found matching 'CHANGELOG'\nwarning: no previously-included files matching '__pycache__' found under dir                                                                             ectory '*'\nwarning: no previously-included files matching '*.py[co]' found under direct                                                                             ory '*'\n\nDownloading/unpacking elasticsearch>=1.0.0,<2.0.0 (from elasticsearch-curator)\n  Downloading elasticsearch-1.3.0-py2.py3-none-any.whl (55kB): 55kB downloaded\nDownloading/unpacking urllib3>=1.8,<2.0 (from elasticsearch>=1.0.0,<2.0.0->elast                                                                             icsearch-curator)\n  Downloading urllib3-1.10-py2-none-any.whl (75kB): 75kB downloaded\nInstalling collected packages: elasticsearch-curator, elasticsearch, urllib3\n  Running setup.py install for elasticsearch-curator\nwarning: no files found matching 'CHANGELOG'\nwarning: no previously-included files matching '__pycache__' found under dir                                                                             ectory '*'\nwarning: no previously-included files matching '*.py[co]' found under direct                                                                             ory '*'\nInstalling curator script to /var/www/Brownian/bin\nInstalling es_repo_mgr script to /var/www/Brownian/bin\n\nSuccessfully installed elasticsearch-curator elasticsearch urllib3\nCleaning up...\n``\n.class ReadTimeoutErrordoes exist in urllib3/exceptions.py.  Checking with strace, it's reading requests/packages/urllib3/exceptions.py which does not containclass ReadTimeoutError`\nEdit for extra info on my deployment:\n/var/www/Brownian/local/lib/python2.7/site-packages/urllib3/exceptions.py has class ReadTimeoutError\n/var/www/Brownian/local/lib/python2.7/site-packages/requests/packages/urllib3/exceptions.py does not have class ReadTimeoutError which is where curator is looking\n. I don't think it would matter as it was working a couple weeks ago but I'm using a python virtualenv. Curator was installed while in the virtualenv.\n. Solved the problem. As I was using python virtualenv, I had installed curator while I was in the virtualenv and therefore it was looking for files in that location. I uninstalled it from the virtualenv and installed to the whole system and it worked.  Hope that makes sense.. It was a problem with my deployment, not the product. \n. ",
    "Grauen": "Guys I'm sorry, I screwed up the prefix parameter. Everything works fine. This happened because the default-prefix is \"logstash-\" and the indices on the 3rd machine had a special suffix \n. ",
    "samiadvize": "Thanks\nI tried a workaround by creating an alias like my-index-2013.01 for my-index-2013 and then a command \ncurator close --timestring %Y.%m --prefix my-index- --time-unit months --older-than 24\nbut it doesn't works , we can't use aliases to perform operation ?\n. OK thanks for the quick answers\n. ",
    "ktf": "[root@cmsmesos01 ~]# curator -v\ncurator 2.0.2\nthis is installed via PIP. BTW, you might also want to refuse to delete \n/kibana-int. :-)\n. ",
    "linux01d": "There is another log with debug level.\nAfter that we decide to remove curator at all from anywhere in our anviroment and make our own bicycle, but with predicted behaviour.\n```\n2015-02-14 04:47:01,581 INFO                        root                   main:327  Job starting...\n2015-02-14 04:47:01,581 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-02-14 04:47:01,581 INFO      urllib3.connectionpool              _new_conn:188  Starting new HTTP connection (1): localhost\n2015-02-14 04:47:01,583 DEBUG     urllib3.connectionpool          _make_request:368  \"GET / HTTP/1.1\" 200 334\n2015-02-14 04:47:01,584 INFO               elasticsearch    log_request_success:57   GET http://localhost:9200/ [status:200 request:0.003s]\n2015-02-14 04:47:01,584 DEBUG              elasticsearch    log_request_success:59   > None\n2015-02-14 04:47:01,584 DEBUG              elasticsearch    log_request_success:60   < {\n  \"status\" : 200,\n  \"name\" : \"host04\",\n  \"cluster_name\" : \"logstore\",\n  \"version\" : {\n    \"number\" : \"1.4.2\",\n    \"build_hash\" : \"927caff6f05403e936c20bf4529f144f0c89fd8c\",\n    \"build_timestamp\" : \"2014-12-16T14:11:12Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"4.10.2\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n2015-02-14 04:47:01,585 DEBUG     elasticsearch_curator.curator_script          check_version:221  Detected Elasticsearch version 1.4.2\n2015-02-14 04:47:01,585 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-02-14 04:47:01,595 DEBUG     urllib3.connectionpool          make_request:368  \"GET /_nodes/_local HTTP/1.1\" 200 4920\n2015-02-14 04:47:01,595 INFO               elasticsearch    log_request_success:57   GET http://localhost:9200/_nodes/_local [status:200 request:0.011s]\n2015-02-14 04:47:01,596 DEBUG              elasticsearch    log_request_success:59   > None\n2015-02-14 04:47:01,596 DEBUG              elasticsearch    log_request_success:60   < {\"cluster_name\":\"logstore\",\"nodes\":{\"YU6Kl1IzTGeEPL4K875vFg\":{\"name\":\"host04\",\"transport_address\":\"inet[/#.#.#.#:9300]\",\"host\":\"host4\",\"ip\":\"#.#.#.#\",\"version\":\"1.4.2\",\"build\":\"927caff\",\"http_address\":\"inet[/#.#.#.#:9200]\",\"attributes\":{\"hardware_numa_nodes\":\"2\",\"datacenter_line\":\"v2-1-1-2\",\"hardware_cpu_cores\":\"16\",\"hardware_hdd_type\":\"SATA@7200rpm\",\"datacenter\":\"v2\",\"master\":\"true\"},\"settings\":{\"index\":{\"number_of_replicas\":\"2\",\"store\":{\"type\":\"mmapfs\"},\"number_of_shards\":\"14\",\"refresh_interval\":\"5s\",\"merge\":{\"policy\":\"log_byte_size\",\"policy.merge_factor\":\"25\",\"scheduler.max_thread_count\":\"1\",\"policy.min_merge_size\":\"3.2mb\",\"scheduler\":\"concurrentmergescheduler\"}},\"bootstrap\":{\"mlockall\":\"true\"},\"client\":{\"type\":\"node\"},\"gateway\":{\"recover_after_time\":\"1m\",\"expected_nodes\":\"44\",\"recover_after_nodes\":\"23\"},\"sonian\":{\"elasticsearch\":{\"zookeeper\":{\"client\":{\"host\":\"zkhost01:2181\"},\"discovery\":{\"state_publishing\":{\"enabled\":\"true\"}},\"settings\":{\"enabled\":\"true\"}}}},\"pidfile\":\"/var/run/elasticsearch.pid\",\"network\":{\"publish_host\":\"#.#.#.#\"},\"node\":{\"datacenter_line\":\"v2-1-1-2\",\"hardware_numa_nodes\":\"2\",\"hardware_cpu_cores\":\"16\",\"name\":\"host04\",\"hardware_hdd_type\":\"SATA@7200rpm\",\"data\":\"true\",\"datacenter\":\"v2\",\"master\":\"true\"},\"http\":{\"port\":\"9200\",\"enabled\":\"true\",\"cors\":{\"enabled\":\"true\"}},\"transport\":{\"publish_port\":\"9300\",\"tcp\":{\"port\":\"9300\",\"compress\":\"false\",\"connect_timeout\":\"30s\"}},\"name\":\"host04\",\"path\":{\"data\":\"/var/lib/elasticsearch\",\"work\":\"/tmp/elasticsearch\",\"home\":\"/usr/share/elasticsearch\",\"conf\":\"/etc/elasticsearch\",\"logs\":\"/var/log/elasticsearch\"},\"config\":\"/etc/elasticsearch/yalasticsearch.json\",\"indices\":{\"store\":{\"throttle\":{\"type\":\"merge\",\"max_bytes_per_sec\":\"10mb\"}},\"memory\":{\"index_buffer_size\":\"40%\",\"min_shard_index_buffer_size\":\"4mb\"},\"recovery\":{\"concurrent_streams\":\"8\",\"max_bytes_per_sec\":\"120mb\"}},\"cluster\":{\"routing\":{\"allocation\":{\"awareness\":{\"force\":{\"datacenter\":{\"values\":\"v1,v2,v3\"}},\"attributes\":\"datacenter,datacenter_line\"},\"node_initial_primaries_recoveries\":\"4\",\"disable_allocation\":\"false\",\"node_concurrent_recoveries\":\"8\"}},\"name\":\"logstore\"},\"discovery\":{\"type\":\"com.sonian.elasticsearch.zookeeper.discovery.ZooKeeperDiscoveryModule\"}},\"os\":{\"refresh_interval_in_millis\":1000,\"available_processors\":32,\"cpu\":{\"vendor\":\"Intel\",\"model\":\"Xeon\",\"mhz\":3000,\"total_cores\":32,\"total_sockets\":1,\"cores_per_socket\":32,\"cache_size_in_bytes\":20480},\"mem\":{\"total_in_bytes\":135179485184},\"swap\":{\"total_in_bytes\":0}},\"process\":{\"refresh_interval_in_millis\":1000,\"id\":5749,\"max_file_descriptors\":250000,\"mlockall\":true},\"jvm\":{\"pid\":5749,\"version\":\"1.7.0_72\",\"vm_name\":\"Java HotSpot(TM) 64-Bit Server VM\",\"vm_version\":\"24.72-b04\",\"vm_vendor\":\"Oracle Corporation\",\"start_time_in_millis\":1423035870163,\"mem\":{\"heap_init_in_bytes\":27035435008,\"heap_max_in_bytes\":25684803584,\"non_heap_init_in_bytes\":24313856,\"non_heap_max_in_bytes\":136314880,\"direct_max_in_bytes\":25684803584},\"gc_collectors\":[\"ParNew\",\"ConcurrentMarkSweep\"],\"memory_pools\":[\"Code Cache\",\"Par Eden Space\",\"Par Survivor Space\",\"CMS Old Gen\",\"CMS Perm Gen\"]},\"thread_pool\":{\"generic\":{\"type\":\"cached\",\"keep_alive\":\"30s\",\"queue_size\":-1},\"index\":{\"type\":\"fixed\",\"min\":32,\"max\":32,\"queue_size\":\"200\"},\"bench\":{\"type\":\"scaling\",\"min\":1,\"max\":5,\"keep_alive\":\"5m\",\"queue_size\":-1},\"get\":{\"type\":\"fixed\",\"min\":32,\"max\":32,\"queue_size\":\"1k\"},\"snapshot\":{\"type\":\"scaling\",\"min\":1,\"max\":5,\"keep_alive\":\"5m\",\"queue_size\":-1},\"merge\":{\"type\":\"scaling\",\"min\":1,\"max\":5,\"keep_alive\":\"5m\",\"queue_size\":-1},\"suggest\":{\"type\":\"fixed\",\"min\":32,\"max\":32,\"queue_size\":\"1k\"},\"bulk\":{\"type\":\"fixed\",\"min\":32,\"max\":32,\"queue_size\":\"50\"},\"optimize\":{\"type\":\"fixed\",\"min\":1,\"max\":1,\"queue_size\":-1},\"warmer\":{\"type\":\"scaling\",\"min\":1,\"max\":5,\"keep_alive\":\"5m\",\"queue_size\":-1},\"flush\":{\"type\":\"scaling\",\"min\":1,\"max\":5,\"keep_alive\":\"5m\",\"queue_size\":-1},\"search\":{\"type\":\"fixed\",\"min\":96,\"max\":96,\"queue_size\":\"1k\"},\"listener\":{\"type\":\"fixed\",\"min\":10,\"max\":10,\"queue_size\":-1},\"percolate\":{\"type\":\"fixed\",\"\nmin\":32,\"max\":32,\"queue_size\":\"1k\"},\"management\":{\"type\":\"scaling\",\"min\":1,\"max\":5,\"keep_alive\":\"5m\",\"queue_size\":-1},\"refresh\":{\"type\":\"scaling\",\"min\":1,\"max\":10,\"keep_alive\":\"5m\",\"queue_size\":-1}},\"network\":{\"refresh_interval_in_millis\":5000,\"primary_interface\":{\"address\":\"#.#.#.#\",\"name\":\"eth0\",\"mac_address\":\"00:25:90:92:49:D4\"}},\"transport\":{\"bound_address\":\"inet[/0:0:0:0:0:0:0:0:9300]\",\"publish_address\":\"inet[/#.#.#.#:9300]\"},\"http\":{\"bound_address\":\"inet[/0:0:0:0:0:0:0:0:9200]\",\"publish_address\":\"inet[/#.#.#.#:9200]\",\"max_content_length_in_bytes\":104857600},\"plugins\":[{\"name\":\"zookeeper\",\"version\":\"NA\",\"description\":\"ZooKeeper Plugin Version: 0.0.0 (2015-02-04T07:44:44)\",\"jvm\":true,\"site\":false},{\"name\":\"zookeeper\",\"version\":\"NA\",\"description\":\"No description found.\",\"url\":\"/_plugin/zookeeper/\",\"jvm\":false,\"site\":true}]}}}\n2015-02-14 04:47:01,598 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-02-14 04:47:01,599 DEBUG     urllib3.connectionpool          _make_request:368  \"GET /_cluster/state/master_node HTTP/1.1\" 200 67\n2015-02-14 04:47:01,599 INFO               elasticsearch    log_request_success:57   GET http://localhost:9200/_cluster/state/master_node [status:200 request:0.001s]\n2015-02-14 04:47:01,599 DEBUG              elasticsearch    log_request_success:59   > None\n2015-02-14 04:47:01,599 DEBUG              elasticsearch    log_request_success:60   < {\"cluster_name\":\"logstore\",\"master_node\":\"YU6Kl1IzTGeEPL4K875vFg\"}\n2015-02-14 04:47:01,600 DEBUG                       root                   main:352  Matching indices with pattern: logs-%Y%m_%d\n2015-02-14 04:47:01,600 DEBUG                       root                   main:356  argdict = {'url_prefix': '', 'func': , 'prefix': 'logs-', 'log_level': 'INFO', 'timestring': '%Y_%m_%d', 'dry_run': False, 'exclude_pattern': None, 'logformat': 'Default', 'auth': None, 'ssl': False, 'host': 'localhost', 'command': 'delete', 'time_unit': 'days', 'timeout': 30, 'debug': True, 'disk_space': 2000.0, 'log_file': '/var/log/elasticsearch/elasticsearch-curator.log', 'master_only': True, 'port': 9200, 'older_than': None, 'suffix': ''}\n2015-02-14 04:47:01,600 INFO                        root                 delete:951  Deleting indices...\n2015-02-14 04:47:01,600 INFO      elasticsearch_curator.elasticsearch_curator                 delete:954  Deleting by space rather than time.\n2015-02-14 04:47:01,600 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-02-14 04:47:01,601 DEBUG     urllib3.connectionpool          make_request:368  \"GET //_settings?expand_wildcards=open%2Cclosed HTTP/1.1\" 200 2274\n2015-02-14 04:47:01,602 INFO               elasticsearch    log_request_success:57   GET http://localhost:9200//_settings?expand_wildcards=open%2Cclosed [status:200 request:0.002s]\n2015-02-14 04:47:01,602 DEBUG              elasticsearch    log_request_success:59   > None\n2015-02-14 04:47:01,602 DEBUG              elasticsearch    log_request_success:60   < {\"intape-rsyslog-2015-02-13\":{\"settings\":{\"index\":{\"creation_date\":\"1423788056867\",\"uuid\":\"9P_FHUBlTIaH0UwLCH2uqA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"14\",\"version\":{\"created\":\"1040299\"}}}},\"intape-rsyslog-2015-02-14\":{\"settings\":{\"index\":{\"creation_date\":\"1423872000271\",\"uuid\":\"jUza5uUmTG-hKSiH1sdDaQ\",\"number_of_replicas\":\"2\",\"number_of_shards\":\"14\",\"version\":{\"created\":\"1040299\"}}}},\"intape-rsyslog-2015-02-11\":{\"settings\":{\"index\":{\"creation_date\":\"1423612800271\",\"codec\":{\"bloom\":{\"load\":\"false\"}},\"uuid\":\"LfWYMePTTwW6vCjFzLC0Kg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"14\",\"version\":{\"created\":\"1040299\"}}}},\"intape-rsyslog-2015-02-12\":{\"settings\":{\"index\":{\"creation_date\":\"1423699200433\",\"codec\":{\"bloom\":{\"load\":\"false\"}},\"uuid\":\"ZMUsMnKpSCSZM9mVzEX5IQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"14\",\"version\":{\"created\":\"1040299\"}}}},\"rsyslog-2015-02-14\":{\"settings\":{\"index\":{\"creation_date\":\"1423861200017\",\"uuid\":\"r1IswtPAThaUXUoBBfNF5g\",\"number_of_replicas\":\"2\",\"number_of_shards\":\"14\",\"version\":{\"created\":\"1040299\"}}}},\"rsyslog-2015-02-13\":{\"settings\":{\"index\":{\"creation_date\":\"1423788056370\",\"uuid\":\"Uvu995csTNSqjdJTEK4vqw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"14\",\"version\":{\"created\":\"1040299\"}}}},\"kibana-int\":{\"settings\":{\"index\":{\"creation_date\":\"1419924505761\",\"uuid\":\"uKDh0wJORh-pFL2QI9fO3w\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"14\",\"version\":{\"created\":\"1040199\"}}}},\"rsyslog-2015-02-12\":{\"settings\":{\"index\":{\"creation_date\":\"1423688400026\",\"codec\":{\"bloom\":{\"load\":\"false\"}},\"uuid\":\"LKMxLWFrQIGj2jR1asOvsg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"14\",\"version\":{\"created\":\"1040299\"}}}},\"rsyslog-2015-02-11\":{\"settings\":{\"index\":{\"creation_date\":\"1423602000025\",\"codec\":{\"bloom\":{\"load\":\"false\"}},\"uuid\":\"EmK0_NjHT8is8HBrEr7tRQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"14\",\"version\":{\"created\":\"1040299\"}}}},\"rsyslog-2015-02-10\":{\"settings\":{\"index\":{\"creation_date\":\"1423515602987\",\"codec\":{\"bloom\":{\"load\":\"false\"}},\"uuid\":\"lSBVwtExTom3S_X4gzHnog\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"14\",\"version\":{\"created\":\"1040299\"}}}},\"video\":{\"settings\":{\"index\":{\"routing\":{\"allocation\":{\"disable_allocation\":\"false\"}},\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"version\":{\"created\":\"900299\"}}}}}\n2015-02-14 04:47:01,603 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-02-14 04:47:01,683 DEBUG     urllib3.connectionpool          _make_request:368  \"GET /_status HTTP/1.1\" 200 169288\n2015-02-14 04:47:01,685 INFO               elasticsearch    log_request_success:57   GET http://localhost:9200/_status [status:200 request:0.080s]\n2015-02-14 04:47:01,685 DEBUG              elasticsearch    log_request_success:59   > None\n2015-02-14 04:47:01,685 DEBUG              elasticsearch    log_request_success:60   < {\"_shards\":{\"total\":318,\"successful\":287,\"failed\":0},\"indices\":{\"intape-rsyslog-2015-02-13\":{\"index\":{\"primary_size_in_bytes\":310236024,\"size_in_bytes\":620502641},\"translog\":{\"operations\":0},\"docs\":{\"num_docs\":1231999,\"max_doc\":1231999,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":37516,\"total_time_in_millis\":70881014,\"total_docs\":1226947027,\"total_size_in_bytes\":309657133559},\"refresh\":{\"total\":334144,\"total_time_in_millis\":2521431},\"flush\":{\"total\":1099,\"total_time_in_millis\":119779},\"shards\":{\"0\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"RK84sm3-T-mO6xUfE5mq_Q\",\"relocating_node\":null,\"shard\":0,\"index\":\"intape-rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":22364362},\"translog\":{\"id\":1423790995503,\"operations\":0},\"docs\":{\"num_docs\":88634,\"max_doc\":88634,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1614,\"total_time_in_millis\":2432158,\"total_docs\":51167044,\"total_size_in_bytes\":12958449436},\"refresh\":{\"total\":14317,\"total_time_in_millis\":81919},\"flush\":{\"total\":45,\"total_time_in_millis\":3415}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"MimlEBKgQH2_oc3JxqxpXA\",\"relocating_node\":null,\"shard\":0,\"index\":\"intape-rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":22393974},\"translog\":{\"id\":1423790995503,\"operations\":0},\"docs\":{\"num_docs\":88634,\"max_doc\":88634,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":157,\"total_time_in_millis\":21427,\"total_docs\":621380,\"total_size_in_bytes\":172185250},\"refresh\":{\"total\":1392,\"total_time_in_millis\":6380},\"flush\":{\"total\":5,\"total_time_in_millis\":157}}],\"1\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"IW9Mab9kTNuH16VCZaOBIg\",\"relocating_node\":null,\"shard\":1,\"index\":\"intape-rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23107252},\"translog\":{\"id\":1423788114385,\"operations\":0},\"docs\":{\"num_docs\":91602,\"max_doc\":91602,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":181,\"total_time_in_millis\":49377,\"total_docs\":836761,\"total_size_in_bytes\":228762957},\"refresh\":{\"total\":1589,\"total_time_in_millis\":41594},\"flush\":{\"total\":6,\"total_time_in_millis\":711}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"YU6Kl1IzTGeEPL4K875vFg\",\"relocating_node\":null,\"shard\":1,\"index\":\"intape-rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23061167},\"translog\":{\"id\":1423788114385,\"operations\":0},\"docs\":{\"num_docs\":91602,\"max_doc\":91602,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1611,\"total_time_in_millis\":2492328,\"total_docs\":52055193,\"total_size_in_bytes\":13124164211},\"refresh\":{\"total\":14249,\"total_time_in_millis\":96790},\"flush\":{\"total\":47,\"total_time_in_millis\":4022}}],\"2\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"RJwWye67QkWKKjl_b3NZ-A\",\"relocating_node\":null,\"shard\":2,\"index\":\"intape-rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":16884235},\"translog\":{\"id\":1423792501473,\"operations\":0},\"docs\":{\"num_docs\":66869,\"max_doc\":66869,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1323,\"total_time_in_millis\":3203120,\"total_docs\":43088062,\"total_size_in_bytes\":10954898967},\"refresh\":{\"total\":12045,\"total_time_in_millis\":189949},\"flush\":{\"total\":45,\"total_time_in_millis\":10951}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"qXwalJKMSk6W0JaibIpTQg\",\"relocating_node\":null,\"shard\":2,\"index\":\"intape-rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":16880713},\"translog\":{\"id\":1423792501473,\"operations\":0},\"docs\":{\"num_docs\":66869,\"max_doc\":66869,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":390,\"total_time_in_millis\":871757,\"total_docs\":21442889,\"total_size_in_bytes\":5436553013},\"refresh\":{\"total\":3470,\"total_time_in_millis\":18054},\"flush\":{\"total\":12,\"total_time_in_millis\":1685}}],\"3\"\n:[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"te0M1g2yQhOAkWg9UHDMrw\",\"relocating_node\":null,\"shard\":3,\"index\":\"intape-rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":22809644},\"translog\":{\"id\":1423788330760,\"operations\":0},\"docs\":{\"num_docs\":90521,\"max_doc\":90521,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1620,\"total_time_in_millis\":3743081,\"total_docs\":52143306,\"total_size_in_bytes\":13149278892},\"refresh\":{\"total\":14410,\"total_time_in_millis\":117945},\"flush\":{\"total\":47,\"total_time_in_millis\":5818}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"ix5R9EwPTEWLzWL2ZyiUOA\",\"relocating_node\":null,\"shard\":3,\"index\":\"intape-rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":22812452},\"translog\":{\"id\":1423788330760,\"operations\":0},\"docs\":{\"num_docs\":90521,\"max_doc\":90521,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1414,\"total_time_in_millis\":2489310,\"total_docs\":51161808,\"total_size_in_bytes\":12883830427},\"refresh\":{\"total\":12650,\"total_time_in_millis\":70770},\"flush\":{\"total\":40,\"total_time_in_millis\":4577}}],\"4\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"ob27c5P3TbWWv-2xF1KTWQ\",\"relocating_node\":null,\"shard\":4,\"index\":\"intape-rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":22453236},\"translog\":{\"id\":1423789002784,\"operations\":0},\"docs\":{\"num_docs\":89083,\"max_doc\":89083,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1602,\"total_time_in_millis\":4020406,\"total_docs\":52063708,\"total_size_in_bytes\":13163842089},\"refresh\":{\"total\":14379,\"total_time_in_millis\":132531},\"flush\":{\"total\":47,\"total_time_in_millis\":6983}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"-5VMHj1DQYqKR6vi018o5w\",\"relocating_node\":null,\"shard\":4,\"index\":\"intape-rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":22492741},\"translog\":{\"id\":1423789002784,\"operations\":0},\"docs\":{\"num_docs\":89083,\"max_doc\":89083,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":395,\"total_time_in_millis\":279821,\"total_docs\":10604447,\"total_size_in_bytes\":2694676240},\"refresh\":{\"total\":3508,\"total_time_in_millis\":37649},\"flush\":{\"total\":12,\"total_time_in_millis\":360}}],\"5\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"Mg5HpexgTuCOtCs218D1GA\",\"relocating_node\":null,\"shard\":5,\"index\":\"intape-rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":22849174},\"translog\":{\"id\":1423788057432,\"operations\":0},\"docs\":{\"num_docs\":90901,\"max_doc\":90901,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1624,\"total_time_in_millis\":2502662,\"total_docs\":52189715,\"total_size_in_bytes\":13155522506},\"refresh\":{\"total\":14415,\"total_time_in_millis\":96384},\"flush\":{\"total\":47,\"total_time_in_millis\":3451}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"hsLYPGy2TyC-Z6gsJ_YMqQ\",\"relocating_node\":null,\"shard\":5,\"index\":\"intape-rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":22857450},\"translog\":{\"id\":1423788057432,\"operations\":0},\"docs\":{\"num_docs\":90901,\"max_doc\":90901,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1620,\"total_time_in_millis\":2501198,\"total_docs\":51920254,\"total_size_in_bytes\":13080063509},\"refresh\":{\"total\":14465,\"total_time_in_millis\":80607},\"flush\":{\"total\":47,\"total_time_in_millis\":2889}}],\"6\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"2tyDIGR0Ts2AQLMCBnqS1g\",\"relocating_node\":null,\"shard\":6,\"index\":\"intape-rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23304176},\"translog\":{\"id\":1423788057438,\"operations\":0},\"docs\":{\"num_docs\":92647,\"max_doc\":92647,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":161,\"total_time_in_millis\":32810,\"total_docs\":766298,\"total_size_in_bytes\":208981427},\"refresh\":{\"total\":1416,\"total_time_in_millis\":8487},\"flush\":{\"total\":5,\"total_time_in_millis\":262}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"\nnode\":\"gNLthbzQSW-h_v2oGvM4Mw\",\"relocating_node\":null,\"shard\":6,\"index\":\"intape-rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23302387},\"translog\":{\"id\":1423788057438,\"operations\":0},\"docs\":{\"num_docs\":92647,\"max_doc\":92647,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1628,\"total_time_in_millis\":4037001,\"total_docs\":52046989,\"total_size_in_bytes\":13124572630},\"refresh\":{\"total\":14453,\"total_time_in_millis\":104032},\"flush\":{\"total\":47,\"total_time_in_millis\":3426}}],\"7\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"cTWT_a2hRvaHHdtugjXxMw\",\"relocating_node\":null,\"shard\":7,\"index\":\"intape-rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":22683207},\"translog\":{\"id\":1423788410080,\"operations\":0},\"docs\":{\"num_docs\":89999,\"max_doc\":89999,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1614,\"total_time_in_millis\":2353257,\"total_docs\":52359215,\"total_size_in_bytes\":13221862105},\"refresh\":{\"total\":14391,\"total_time_in_millis\":72111},\"flush\":{\"total\":47,\"total_time_in_millis\":3516}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"4pLsTb6kQCur_Nw5jwDm2A\",\"relocating_node\":null,\"shard\":7,\"index\":\"intape-rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":22632094},\"translog\":{\"id\":1423788410080,\"operations\":0},\"docs\":{\"num_docs\":89999,\"max_doc\":89999,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1621,\"total_time_in_millis\":2248596,\"total_docs\":52450175,\"total_size_in_bytes\":13238008387},\"refresh\":{\"total\":14368,\"total_time_in_millis\":107754},\"flush\":{\"total\":47,\"total_time_in_millis\":4596}}],\"8\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"u0UGAe5cR8iqrfQobCI5_w\",\"relocating_node\":null,\"shard\":8,\"index\":\"intape-rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":19391058},\"translog\":{\"id\":1423788497558,\"operations\":0},\"docs\":{\"num_docs\":77044,\"max_doc\":77044,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1407,\"total_time_in_millis\":3644095,\"total_docs\":51931686,\"total_size_in_bytes\":13110848013},\"refresh\":{\"total\":12531,\"total_time_in_millis\":86274},\"flush\":{\"total\":43,\"total_time_in_millis\":4083}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"c5cY7a7EQrK1rPMFWheBtg\",\"relocating_node\":null,\"shard\":8,\"index\":\"intape-rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":19378792},\"translog\":{\"id\":1423788497558,\"operations\":0},\"docs\":{\"num_docs\":77044,\"max_doc\":77044,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1398,\"total_time_in_millis\":2229933,\"total_docs\":52164216,\"total_size_in_bytes\":13174970575},\"refresh\":{\"total\":12437,\"total_time_in_millis\":62319},\"flush\":{\"total\":43,\"total_time_in_millis\":3882}}],\"9\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"Pp9D83T_SDq73IfCI93JFA\",\"relocating_node\":null,\"shard\":9,\"index\":\"intape-rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23073576},\"translog\":{\"id\":1423788057446,\"operations\":0},\"docs\":{\"num_docs\":91565,\"max_doc\":91565,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1626,\"total_time_in_millis\":4042264,\"total_docs\":52482491,\"total_size_in_bytes\":13242356415},\"refresh\":{\"total\":14474,\"total_time_in_millis\":89651},\"flush\":{\"total\":47,\"total_time_in_millis\":4493}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"LGUsqbQXRei4YLyIjf7aow\",\"relocating_node\":null,\"shard\":9,\"index\":\"intape-rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23043343},\"translog\":{\"id\":1423788057446,\"operations\":0},\"docs\":{\"num_docs\":91565,\"max_doc\":91565,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1617,\"total_time_in_millis\":2390118,\"total_docs\":52375735,\"total_size_in_bytes\":13226006684},\"refresh\":{\"total\":14374,\"total_time_in_millis\":104360},\"flush\":{\"total\":47,\"total_time_in_millis\":9110}}],\"10\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"RJwWye67QkWKKjl\nb3NZ-A\",\"relocating_node\":null,\"shard\":10,\"index\":\"intape-rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":22400196},\"translog\":{\"id\":1423788057432,\"operations\":0},\"docs\":{\"num_docs\":88772,\"max_doc\":88772,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1597,\"total_time_in_millis\":4218997,\"total_docs\":51237735,\"total_size_in_bytes\":12972757489},\"refresh\":{\"total\":14175,\"total_time_in_millis\":248433},\"flush\":{\"total\":45,\"total_time_in_millis\":10068}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"aTLkCoPHScqMWT3zoEGTnw\",\"relocating_node\":null,\"shard\":10,\"index\":\"intape-rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":22382607},\"translog\":{\"id\":1423788057432,\"operations\":0},\"docs\":{\"num_docs\":88772,\"max_doc\":88772,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1608,\"total_time_in_millis\":2449571,\"total_docs\":51986831,\"total_size_in_bytes\":13164453884},\"refresh\":{\"total\":14358,\"total_time_in_millis\":67017},\"flush\":{\"total\":46,\"total_time_in_millis\":3158}}],\"11\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"4hA88zpfSX2WRoc_-wWxQ\",\"relocating_node\":null,\"shard\":11,\"index\":\"intape-rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":22690976},\"translog\":{\"id\":1423788057445,\"operations\":0},\"docs\":{\"num_docs\":90258,\"max_doc\":90258,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1599,\"total_time_in_millis\":2693411,\"total_docs\":52976877,\"total_size_in_bytes\":13338867648},\"refresh\":{\"total\":14238,\"total_time_in_millis\":93712},\"flush\":{\"total\":47,\"total_time_in_millis\":6116}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"yVMmqz7RR8mr4GSwkkX-4A\",\"relocating_node\":null,\"shard\":11,\"index\":\"intape-rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":22724364},\"translog\":{\"id\":1423788057445,\"operations\":0},\"docs\":{\"num_docs\":90258,\"max_doc\":90258,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1594,\"total_time_in_millis\":3665589,\"total_docs\":52899317,\"total_size_in_bytes\":13323423186},\"refresh\":{\"total\":14267,\"total_time_in_millis\":76964},\"flush\":{\"total\":47,\"total_time_in_millis\":5471}}],\"12\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"zcjnLJwYTI2nBn90i470OA\",\"relocating_node\":null,\"shard\":12,\"index\":\"intape-rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23300336},\"translog\":{\"id\":1423788057448,\"operations\":0},\"docs\":{\"num_docs\":92859,\"max_doc\":92859,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1629,\"total_time_in_millis\":3755486,\"total_docs\":52627775,\"total_size_in_bytes\":13223892358},\"refresh\":{\"total\":14465,\"total_time_in_millis\":194124},\"flush\":{\"total\":47,\"total_time_in_millis\":4710}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"qsymmz4LTeKkq_7AuKqG4g\",\"relocating_node\":null,\"shard\":12,\"index\":\"intape-rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23339570},\"translog\":{\"id\":1423788057448,\"operations\":0},\"docs\":{\"num_docs\":92859,\"max_doc\":92859,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1627,\"total_time_in_millis\":2421359,\"total_docs\":52628558,\"total_size_in_bytes\":13228109828},\"refresh\":{\"total\":14472,\"total_time_in_millis\":73514},\"flush\":{\"total\":47,\"total_time_in_millis\":3648}}],\"13\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"cTWT_a2hRvaHHdtugjXxMw\",\"relocating_node\":null,\"shard\":13,\"index\":\"intape-rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":22952973},\"translog\":{\"id\":1423788057464,\"operations\":0},\"docs\":{\"num_docs\":91245,\"max_doc\":91245,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1623,\"total_time_in_millis\":2433846,\"total_docs\":53368742,\"total_size_in_bytes\":13425566520},\"refresh\":{\"total\":14425,\"total_time_in_millis\":71523},\"flush\":{\"total\":47,\"total_time_in_millis\":2851}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"UT-z9BwoRYGHDO65Xe77lA\",\"relocating\nnode\":null,\"shard\":13,\"index\":\"intape-rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":22936586},\"translog\":{\"id\":1423788057464,\"operations\":0},\"docs\":{\"num_docs\":91245,\"max_doc\":91245,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1616,\"total_time_in_millis\":3658036,\"total_docs\":53349820,\"total_size_in_bytes\":13430228913},\"refresh\":{\"total\":14411,\"total_time_in_millis\":90584},\"flush\":{\"total\":47,\"total_time_in_millis\":5370}}]}},\"intape-rsyslog-2015-02-14\":{\"index\":{\"primary_size_in_bytes\":45149642,\"size_in_bytes\":118267700},\"translog\":{\"operations\":65450},\"docs\":{\"num_docs\":92490,\"max_doc\":92490,\"deleted_docs\":0},\"merges\":{\"current\":1,\"current_docs\":7041,\"current_size_in_bytes\":1826171,\"total\":4048,\"total_time_in_millis\":530032,\"total_docs\":12881017,\"total_size_in_bytes\":3580790831},\"refresh\":{\"total\":35827,\"total_time_in_millis\":196842},\"flush\":{\"total\":103,\"total_time_in_millis\":6143},\"shards\":{\"0\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"RJwWye67QkWKKjl_b3NZ-A\",\"relocating_node\":null,\"shard\":0,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":1194539},\"translog\":{\"id\":1423872001553,\"operations\":409},\"docs\":{\"num_docs\":2325,\"max_doc\":2325,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":5,\"total_time_in_millis\":418,\"total_docs\":10926,\"total_size_in_bytes\":3258082},\"refresh\":{\"total\":49,\"total_time_in_millis\":220},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"4hA88zpfSX2WRoc_-wWxQ\",\"relocating_node\":null,\"shard\":0,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":664944},\"translog\":{\"id\":1423872001554,\"operations\":0},\"docs\":{\"num_docs\":2325,\"max_doc\":2325,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":5,\"total_time_in_millis\":270,\"total_docs\":8861,\"total_size_in_bytes\":2708864},\"refresh\":{\"total\":49,\"total_time_in_millis\":207},\"flush\":{\"total\":1,\"total_time_in_millis\":44}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"LGUsqbQXRei4YLyIjf7aow\",\"relocating_node\":null,\"shard\":0,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":634661},\"translog\":{\"id\":1423872001554,\"operations\":0},\"docs\":{\"num_docs\":2325,\"max_doc\":2325,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":43,\"total_time_in_millis\":1636,\"total_docs\":48595,\"total_size_in_bytes\":16152192},\"refresh\":{\"total\":387,\"total_time_in_millis\":1760},\"flush\":{\"total\":2,\"total_time_in_millis\":65}}],\"1\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"2tyDIGR0Ts2AQLMCBnqS1g\",\"relocating_node\":null,\"shard\":1,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":3096643},\"translog\":{\"id\":1423872001555,\"operations\":1789},\"docs\":{\"num_docs\":6853,\"max_doc\":6853,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":81,\"total_time_in_millis\":11591,\"total_docs\":325074,\"total_size_in_bytes\":88595900},\"refresh\":{\"total\":710,\"total_time_in_millis\":3559},\"flush\":{\"total\":2,\"total_time_in_millis\":114}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"MimlEBKgQH2_oc3JxqxpXA\",\"relocating_node\":null,\"shard\":1,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":3081419},\"translog\":{\"id\":1423872001555,\"operations\":1789},\"docs\":{\"num_docs\":6853,\"max_doc\":6853,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":81,\"total_time_in_millis\":10179,\"total_docs\":315651,\"total_size_in_bytes\":86232605},\"refresh\":{\"total\":704,\"total_time_in_millis\":3528},\"flush\":{\"total\":2,\"total_time_in_millis\":74}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"yVMmqz7RR8mr4GSwkkX-4A\",\"relocating_node\":null,\"shard\":1,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":3456006},\"translog\":{\"id\":1423872001555,\"operations\":446},\"docs\":{\"num_docs\":6849,\"max_doc\":6849,\"deleted_docs\":0},\"merges\":{\"current\":0,\n\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":71,\"total_time_in_millis\":24321,\"total_docs\":302629,\"total_size_in_bytes\":82458467},\"refresh\":{\"total\":625,\"total_time_in_millis\":4617},\"flush\":{\"total\":3,\"total_time_in_millis\":1409}}],\"2\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"zcjnLJwYTI2nBn90i470OA\",\"relocating_node\":null,\"shard\":2,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":3285170},\"translog\":{\"id\":1423872001555,\"operations\":1818},\"docs\":{\"num_docs\":7041,\"max_doc\":7041,\"deleted_docs\":0},\"merges\":{\"current\":1,\"current_docs\":7041,\"current_size_in_bytes\":1826171,\"total\":82,\"total_time_in_millis\":11251,\"total_docs\":336098,\"total_size_in_bytes\":90696710},\"refresh\":{\"total\":727,\"total_time_in_millis\":3054},\"flush\":{\"total\":2,\"total_time_in_millis\":81}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"qXwalJKMSk6W0JaibIpTQg\",\"relocating_node\":null,\"shard\":2,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":3118853},\"translog\":{\"id\":1423872001555,\"operations\":1902},\"docs\":{\"num_docs\":7034,\"max_doc\":7034,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":119,\"total_time_in_millis\":11935,\"total_docs\":372158,\"total_size_in_bytes\":103052599},\"refresh\":{\"total\":1056,\"total_time_in_millis\":4289},\"flush\":{\"total\":3,\"total_time_in_millis\":95}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"Pp9D83T_SDq73IfCI93JFA\",\"relocating_node\":null,\"shard\":2,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":4980057},\"translog\":{\"id\":1423872001555,\"operations\":1808},\"docs\":{\"num_docs\":7034,\"max_doc\":7034,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":82,\"total_time_in_millis\":31218,\"total_docs\":345270,\"total_size_in_bytes\":93100044},\"refresh\":{\"total\":726,\"total_time_in_millis\":3960},\"flush\":{\"total\":2,\"total_time_in_millis\":169}}],\"3\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"2tyDIGR0Ts2AQLMCBnqS1g\",\"relocating_node\":null,\"shard\":3,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":3114281},\"translog\":{\"id\":1423872000803,\"operations\":1911},\"docs\":{\"num_docs\":6939,\"max_doc\":6939,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":121,\"total_time_in_millis\":13044,\"total_docs\":370619,\"total_size_in_bytes\":103303526},\"refresh\":{\"total\":1068,\"total_time_in_millis\":5044},\"flush\":{\"total\":3,\"total_time_in_millis\":96}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"_virthvxT4idC9C3NIMwTg\",\"relocating_node\":null,\"shard\":3,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":3139195},\"translog\":{\"id\":1423872000803,\"operations\":1911},\"docs\":{\"num_docs\":6939,\"max_doc\":6939,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":121,\"total_time_in_millis\":16369,\"total_docs\":379789,\"total_size_in_bytes\":105629817},\"refresh\":{\"total\":1074,\"total_time_in_millis\":5358},\"flush\":{\"total\":3,\"total_time_in_millis\":183}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"9yna4MXDQZCBN_XpvTKd3Q\",\"relocating_node\":null,\"shard\":3,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":3051990},\"translog\":{\"id\":1423872000803,\"operations\":1910},\"docs\":{\"num_docs\":6939,\"max_doc\":6939,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":122,\"total_time_in_millis\":28367,\"total_docs\":381193,\"total_size_in_bytes\":105953516},\"refresh\":{\"total\":1074,\"total_time_in_millis\":4356},\"flush\":{\"total\":3,\"total_time_in_millis\":421}}],\"4\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"cTWT_a2hRvaHHdtugjXxMw\",\"relocating_node\":null,\"shard\":4,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":3169449},\"translog\":{\"id\":1423872001559,\"operations\":1844},\"docs\":{\"num_docs\":6976,\"max_doc\":6976,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":84,\"total_time_in_millis\":11824,\"total_docs\":343712,\"total\nsize_in_bytes\":93402367},\"refresh\":{\"total\":736,\"total_time_in_millis\":9070},\"flush\":{\"total\":2,\"total_time_in_millis\":153}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"Mg5HpexgTuCOtCs218D1GA\",\"relocating_node\":null,\"shard\":4,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":3173465},\"translog\":{\"id\":1423872001559,\"operations\":1841},\"docs\":{\"num_docs\":6976,\"max_doc\":6976,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":84,\"total_time_in_millis\":10925,\"total_docs\":350786,\"total_size_in_bytes\":95211290},\"refresh\":{\"total\":732,\"total_time_in_millis\":3758},\"flush\":{\"total\":2,\"total_time_in_millis\":49}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"aTLkCoPHScqMWT3zoEGTnw\",\"relocating_node\":null,\"shard\":4,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":3129971},\"translog\":{\"id\":1423872001559,\"operations\":1928},\"docs\":{\"num_docs\":6976,\"max_doc\":6976,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":122,\"total_time_in_millis\":11517,\"total_docs\":365260,\"total_size_in_bytes\":102100630},\"refresh\":{\"total\":1073,\"total_time_in_millis\":4722},\"flush\":{\"total\":3,\"total_time_in_millis\":98}}],\"5\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"virthvxT4idC9C3NIMwTg\",\"relocating_node\":null,\"shard\":5,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":3071812},\"translog\":{\"id\":1423872000845,\"operations\":1887},\"docs\":{\"num_docs\":6913,\"max_doc\":6913,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":121,\"total_time_in_millis\":14788,\"total_docs\":380362,\"total_size_in_bytes\":106057212},\"refresh\":{\"total\":1082,\"total_time_in_millis\":4483},\"flush\":{\"total\":3,\"total_time_in_millis\":73}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"YU6Kl1IzTGeEPL4K875vFg\",\"relocating_node\":null,\"shard\":5,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":3125823},\"translog\":{\"id\":1423872000845,\"operations\":1887},\"docs\":{\"num_docs\":6910,\"max_doc\":6910,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":122,\"total_time_in_millis\":11629,\"total_docs\":357406,\"total_size_in_bytes\":100254114},\"refresh\":{\"total\":1076,\"total_time_in_millis\":5527},\"flush\":{\"total\":3,\"total_time_in_millis\":140}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"cTWT_a2hRvaHHdtugjXxMw\",\"relocating_node\":null,\"shard\":5,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":3106992},\"translog\":{\"id\":1423872000845,\"operations\":1890},\"docs\":{\"num_docs\":6910,\"max_doc\":6910,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":120,\"total_time_in_millis\":12898,\"total_docs\":368418,\"total_size_in_bytes\":103150381},\"refresh\":{\"total\":1063,\"total_time_in_millis\":15446},\"flush\":{\"total\":3,\"total_time_in_millis\":112}}],\"6\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"YU6Kl1IzTGeEPL4K875vFg\",\"relocating_node\":null,\"shard\":6,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":3091060},\"translog\":{\"id\":1423872001640,\"operations\":1914},\"docs\":{\"num_docs\":6937,\"max_doc\":6937,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":120,\"total_time_in_millis\":11625,\"total_docs\":367925,\"total_size_in_bytes\":102563065},\"refresh\":{\"total\":1063,\"total_time_in_millis\":5397},\"flush\":{\"total\":3,\"total_time_in_millis\":91}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"qsymmz4LTeKkq_7AuKqG4g\",\"relocating_node\":null,\"shard\":6,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":3084601},\"translog\":{\"id\":1423872001640,\"operations\":1912},\"docs\":{\"num_docs\":6933,\"max_doc\":6933,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":120,\"total_time_in_millis\":11000,\"total_docs\":381343,\"total_size_in_bytes\":106228083},\"refresh\":{\"total\":1070,\"total_time_in_millis\":4947},\"flush\":{\"total\":3,\"total_time_in_millis\":172}\n}],\"7\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"_virthvxT4idC9C3NIMwTg\",\"relocating_node\":null,\"shard\":7,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":3085112},\"translog\":{\"id\":1423872000830,\"operations\":1940},\"docs\":{\"num_docs\":6963,\"max_doc\":6963,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":120,\"total_time_in_millis\":15617,\"total_docs\":373253,\"total_size_in_bytes\":104431736},\"refresh\":{\"total\":1077,\"total_time_in_millis\":5465},\"flush\":{\"total\":3,\"total_time_in_millis\":209}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"4hA88zpfSX2WRoc-wWxQ\",\"relocating_node\":null,\"shard\":7,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":3127614},\"translog\":{\"id\":1423872000830,\"operations\":1940},\"docs\":{\"num_docs\":6954,\"max_doc\":6954,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":121,\"total_time_in_millis\":11565,\"total_docs\":375827,\"total_size_in_bytes\":104937941},\"refresh\":{\"total\":1081,\"total_time_in_millis\":5728},\"flush\":{\"total\":3,\"total_time_in_millis\":105}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"Pp9D83T_SDq73IfCI93JFA\",\"relocating_node\":null,\"shard\":7,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":3109766},\"translog\":{\"id\":1423872000830,\"operations\":1940},\"docs\":{\"num_docs\":6954,\"max_doc\":6954,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":122,\"total_time_in_millis\":34866,\"total_docs\":375185,\"total_size_in_bytes\":104875604},\"refresh\":{\"total\":1082,\"total_time_in_millis\":5620},\"flush\":{\"total\":3,\"total_time_in_millis\":109}}],\"8\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"9yna4MXDQZCBN_XpvTKd3Q\",\"relocating_node\":null,\"shard\":8,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":3069073},\"translog\":{\"id\":1423872001214,\"operations\":1885},\"docs\":{\"num_docs\":6815,\"max_doc\":6815,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":120,\"total_time_in_millis\":26840,\"total_docs\":359013,\"total_size_in_bytes\":100432555},\"refresh\":{\"total\":1068,\"total_time_in_millis\":5256},\"flush\":{\"total\":3,\"total_time_in_millis\":212}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"UT-z9BwoRYGHDO65Xe77lA\",\"relocating_node\":null,\"shard\":8,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":3031055},\"translog\":{\"id\":1423872001214,\"operations\":1886},\"docs\":{\"num_docs\":6822,\"max_doc\":6822,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":120,\"total_time_in_millis\":11032,\"total_docs\":360759,\"total_size_in_bytes\":101037150},\"refresh\":{\"total\":1071,\"total_time_in_millis\":4632},\"flush\":{\"total\":3,\"total_time_in_millis\":81}}],\"9\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"RJwWye67QkWKKjl_b3NZ-A\",\"relocating_node\":null,\"shard\":9,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":1207587},\"translog\":{\"id\":1423872000743,\"operations\":416},\"docs\":{\"num_docs\":2350,\"max_doc\":2350,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":44,\"total_time_in_millis\":1975,\"total_docs\":49962,\"total_size_in_bytes\":16540079},\"refresh\":{\"total\":394,\"total_time_in_millis\":1846},\"flush\":{\"total\":1,\"total_time_in_millis\":68}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"IW9Mab9kTNuH16VCZaOBIg\",\"relocating_node\":null,\"shard\":9,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":3103932},\"translog\":{\"id\":1423872000745,\"operations\":1890},\"docs\":{\"num_docs\":6905,\"max_doc\":6905,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":119,\"total_time_in_millis\":12155,\"total_docs\":376346,\"total_size_in_bytes\":104673016},\"refresh\":{\"total\":1057,\"total_time_in_millis\":3883},\"flush\":{\"total\":3,\"total_time_in_millis\":76}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"MimlEBKgQH2_oc3JxqxpXA\",\"relocating_node\":null,\"shard\":9,\"index\":\n\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":3051730},\"translog\":{\"id\":1423872000745,\"operations\":1886},\"docs\":{\"num_docs\":6911,\"max_doc\":6911,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":120,\"total_time_in_millis\":11927,\"total_docs\":367204,\"total_size_in_bytes\":102480267},\"refresh\":{\"total\":1065,\"total_time_in_millis\":5152},\"flush\":{\"total\":3,\"total_time_in_millis\":119}}],\"10\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"_virthvxT4idC9C3NIMwTg\",\"relocating_node\":null,\"shard\":10,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":3151354},\"translog\":{\"id\":1423872000813,\"operations\":1886},\"docs\":{\"num_docs\":7026,\"max_doc\":7026,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":124,\"total_time_in_millis\":16396,\"total_docs\":384801,\"total_size_in_bytes\":107276854},\"refresh\":{\"total\":1109,\"total_time_in_millis\":5641},\"flush\":{\"total\":3,\"total_time_in_millis\":212}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"zcjnLJwYTI2nBn90i470OA\",\"relocating_node\":null,\"shard\":10,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":3120439},\"translog\":{\"id\":1423872000813,\"operations\":1886},\"docs\":{\"num_docs\":7037,\"max_doc\":7037,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":125,\"total_time_in_millis\":13916,\"total_docs\":396313,\"total_size_in_bytes\":110146436},\"refresh\":{\"total\":1100,\"total_time_in_millis\":4670},\"flush\":{\"total\":3,\"total_time_in_millis\":116}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"u0UGAe5cR8iqrfQobCI5_w\",\"relocating_node\":null,\"shard\":10,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":3097042},\"translog\":{\"id\":1423872000813,\"operations\":1886},\"docs\":{\"num_docs\":7031,\"max_doc\":7031,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":125,\"total_time_in_millis\":12084,\"total_docs\":378325,\"total_size_in_bytes\":105498209},\"refresh\":{\"total\":1097,\"total_time_in_millis\":6280},\"flush\":{\"total\":3,\"total_time_in_millis\":101}}],\"11\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"-5VMHj1DQYqKR6vi018o5w\",\"relocating_node\":null,\"shard\":11,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":4881998},\"translog\":{\"id\":1423872000751,\"operations\":1900},\"docs\":{\"num_docs\":6974,\"max_doc\":6974,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":123,\"total_time_in_millis\":11301,\"total_docs\":389448,\"total_size_in_bytes\":108030506},\"refresh\":{\"total\":1067,\"total_time_in_millis\":6143},\"flush\":{\"total\":3,\"total_time_in_millis\":209}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"Mg5HpexgTuCOtCs218D1GA\",\"relocating_node\":null,\"shard\":11,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":3111273},\"translog\":{\"id\":1423872000751,\"operations\":1899},\"docs\":{\"num_docs\":6974,\"max_doc\":6974,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":124,\"total_time_in_millis\":12808,\"total_docs\":394198,\"total_size_in_bytes\":109172471},\"refresh\":{\"total\":1084,\"total_time_in_millis\":5561},\"flush\":{\"total\":3,\"total_time_in_millis\":105}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"ix5R9EwPTEWLzWL2ZyiUOA\",\"relocating_node\":null,\"shard\":11,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":3136074},\"translog\":{\"id\":1423872000751,\"operations\":1899},\"docs\":{\"num_docs\":6974,\"max_doc\":6974,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":122,\"total_time_in_millis\":11574,\"total_docs\":369118,\"total_size_in_bytes\":102686248},\"refresh\":{\"total\":1080,\"total_time_in_millis\":5287},\"flush\":{\"total\":3,\"total_time_in_millis\":151}}],\"12\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"ob27c5P3TbWWv-2xF1KTWQ\",\"relocating_node\":null,\"shard\":12,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":4884838},\"translog\":{\"id\":\n1423872000739,\"operations\":1908},\"docs\":{\"num_docs\":6928,\"max_doc\":6928,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":118,\"total_time_in_millis\":17884,\"total_docs\":361298,\"total_size_in_bytes\":100976533},\"refresh\":{\"total\":1046,\"total_time_in_millis\":10567},\"flush\":{\"total\":3,\"total_time_in_millis\":168}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"hsLYPGy2TyC-Z6gsJ_YMqQ\",\"relocating_node\":null,\"shard\":12,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":3114462},\"translog\":{\"id\":1423872000739,\"operations\":1908},\"docs\":{\"num_docs\":6930,\"max_doc\":6930,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":118,\"total_time_in_millis\":10510,\"total_docs\":370808,\"total_size_in_bytes\":103564437},\"refresh\":{\"total\":1046,\"total_time_in_millis\":3994},\"flush\":{\"total\":3,\"total_time_in_millis\":77}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"ix5R9EwPTEWLzWL2ZyiUOA\",\"relocating_node\":null,\"shard\":12,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":3089001},\"translog\":{\"id\":1423872000739,\"operations\":1908},\"docs\":{\"num_docs\":6934,\"max_doc\":6934,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":118,\"total_time_in_millis\":11681,\"total_docs\":364480,\"total_size_in_bytes\":101806327},\"refresh\":{\"total\":1049,\"total_time_in_millis\":4962},\"flush\":{\"total\":3,\"total_time_in_millis\":160}}],\"13\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"ob27c5P3TbWWv-2xF1KTWQ\",\"relocating_node\":null,\"shard\":13,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":3078986},\"translog\":{\"id\":1423872000741,\"operations\":1896},\"docs\":{\"num_docs\":6897,\"max_doc\":6897,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":119,\"total_time_in_millis\":18571,\"total_docs\":360498,\"total_size_in_bytes\":100780642},\"refresh\":{\"total\":1056,\"total_time_in_millis\":8187},\"flush\":{\"total\":3,\"total_time_in_millis\":145}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"c5cY7a7EQrK1rPMFWheBtg\",\"relocating_node\":null,\"shard\":13,\"index\":\"intape-rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":3045433},\"translog\":{\"id\":1423872000741,\"operations\":1895},\"docs\":{\"num_docs\":6897,\"max_doc\":6897,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":120,\"total_time_in_millis\":10525,\"total_docs\":362106,\"total_size_in_bytes\":101334356},\"refresh\":{\"total\":1054,\"total_time_in_millis\":4666},\"flush\":{\"total\":3,\"total_time_in_millis\":81}}]}},\"intape-rsyslog-2015-02-11\":{\"index\":{\"primary_size_in_bytes\":333336403,\"size_in_bytes\":666673166},\"translog\":{\"operations\":0},\"docs\":{\"num_docs\":1325838,\"max_doc\":1325838,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":40350,\"total_time_in_millis\":68925184,\"total_docs\":1208763587,\"total_size_in_bytes\":305718360665},\"refresh\":{\"total\":355238,\"total_time_in_millis\":2150844},\"flush\":{\"total\":1101,\"total_time_in_millis\":105378},\"shards\":{\"0\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"YU6Kl1IzTGeEPL4K875vFg\",\"relocating_node\":null,\"shard\":0,\"index\":\"intape-rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23887526},\"translog\":{\"id\":1423612800813,\"operations\":0},\"docs\":{\"num_docs\":94764,\"max_doc\":94764,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"hsLYPGy2TyC-Z6gsJ_YMqQ\",\"relocating_node\":null,\"shard\":0,\"index\":\"intape-rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23887526},\"translog\":{\"id\":1423612800813,\"operations\":0},\"docs\":{\"num_docs\":94764,\"max_doc\":94764,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1753,\"total_time_in_millis\":2601040,\"total_docs\":52021686,\"\ntotal_size_in_bytes\":13185261297},\"refresh\":{\"total\":15459,\"total_time_in_millis\":118376},\"flush\":{\"total\":48,\"total_time_in_millis\":7669}}],\"1\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"-5VMHj1DQYqKR6vi018o5w\",\"relocating_node\":null,\"shard\":1,\"index\":\"intape-rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23856843},\"translog\":{\"id\":1423612800804,\"operations\":0},\"docs\":{\"num_docs\":94934,\"max_doc\":94934,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1653,\"total_time_in_millis\":2382854,\"total_docs\":52429257,\"total_size_in_bytes\":13259078200},\"refresh\":{\"total\":14548,\"total_time_in_millis\":64987},\"flush\":{\"total\":45,\"total_time_in_millis\":3827}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"cTWT_a2hRvaHHdtugjXxMw\",\"relocating_node\":null,\"shard\":1,\"index\":\"intape-rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23856389},\"translog\":{\"id\":1423612800805,\"operations\":0},\"docs\":{\"num_docs\":94934,\"max_doc\":94934,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1760,\"total_time_in_millis\":3752888,\"total_docs\":52457693,\"total_size_in_bytes\":13269828570},\"refresh\":{\"total\":15479,\"total_time_in_millis\":76354},\"flush\":{\"total\":48,\"total_time_in_millis\":5814}}],\"2\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"zcjnLJwYTI2nBn90i470OA\",\"relocating_node\":null,\"shard\":2,\"index\":\"intape-rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23850074},\"translog\":{\"id\":1423612800839,\"operations\":0},\"docs\":{\"num_docs\":94740,\"max_doc\":94740,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1751,\"total_time_in_millis\":3817558,\"total_docs\":52516920,\"total_size_in_bytes\":13293411511},\"refresh\":{\"total\":15392,\"total_time_in_millis\":104341},\"flush\":{\"total\":48,\"total_time_in_millis\":4493}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"4pLsTb6kQCur_Nw5jwDm2A\",\"relocating_node\":null,\"shard\":2,\"index\":\"intape-rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23855214},\"translog\":{\"id\":1423612800839,\"operations\":0},\"docs\":{\"num_docs\":94740,\"max_doc\":94740,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1778,\"total_time_in_millis\":1329671,\"total_docs\":52352015,\"total_size_in_bytes\":13253388118},\"refresh\":{\"total\":15437,\"total_time_in_millis\":86065},\"flush\":{\"total\":48,\"total_time_in_millis\":1695}}],\"3\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"2tyDIGR0Ts2AQLMCBnqS1g\",\"relocating_node\":null,\"shard\":3,\"index\":\"intape-rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23826444},\"translog\":{\"id\":1423612800808,\"operations\":0},\"docs\":{\"num_docs\":94792,\"max_doc\":94792,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"UT-z9BwoRYGHDO65Xe77lA\",\"relocating_node\":null,\"shard\":3,\"index\":\"intape-rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23826444},\"translog\":{\"id\":1423612800808,\"operations\":0},\"docs\":{\"num_docs\":94792,\"max_doc\":94792,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1762,\"total_time_in_millis\":2399072,\"total_docs\":52728779,\"total_size_in_bytes\":13331328740},\"refresh\":{\"total\":15536,\"total_time_in_millis\":109568},\"flush\":{\"total\":48,\"total_time_in_millis\":2430}}],\"4\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"gNLthbzQSW-h_v2oGvM4Mw\",\"relocating_node\":null,\"shard\":4,\"index\":\"intape-rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23691454},\"translog\":{\"id\":1423612800807,\"operations\":0},\"docs\":{\"num_docs\":94285,\"max_doc\":94285,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1758,\"total_time_in_millis\":2678900,\"total_docs\":52859981,\"total_size_in_bytes\":13361523489},\"refresh\":{\"total\":15437,\"\ntotal_time_in_millis\":199895},\"flush\":{\"total\":48,\"total_time_in_millis\":16085}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"Pp9D83T_SDq73IfCI93JFA\",\"relocating_node\":null,\"shard\":4,\"index\":\"intape-rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23694289},\"translog\":{\"id\":1423612800807,\"operations\":0},\"docs\":{\"num_docs\":94285,\"max_doc\":94285,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1759,\"total_time_in_millis\":3862704,\"total_docs\":52344174,\"total_size_in_bytes\":13221933905},\"refresh\":{\"total\":15452,\"total_time_in_millis\":85482},\"flush\":{\"total\":48,\"total_time_in_millis\":4167}}],\"5\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"4hA88zpfSX2WRoc-wWxQ\",\"relocating_node\":null,\"shard\":5,\"index\":\"intape-rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23853126},\"translog\":{\"id\":1423612800805,\"operations\":0},\"docs\":{\"num_docs\":94812,\"max_doc\":94812,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1749,\"total_time_in_millis\":3785462,\"total_docs\":51773389,\"total_size_in_bytes\":13103080542},\"refresh\":{\"total\":15402,\"total_time_in_millis\":99529},\"flush\":{\"total\":48,\"total_time_in_millis\":3828}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"yVMmqz7RR8mr4GSwkkX-4A\",\"relocating_node\":null,\"shard\":5,\"index\":\"intape-rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23854326},\"translog\":{\"id\":1423612800805,\"operations\":0},\"docs\":{\"num_docs\":94812,\"max_doc\":94812,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1748,\"total_time_in_millis\":2399794,\"total_docs\":52866266,\"total_size_in_bytes\":13375647552},\"refresh\":{\"total\":15443,\"total_time_in_millis\":71332},\"flush\":{\"total\":48,\"total_time_in_millis\":3075}}],\"6\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"IW9Mab9kTNuH16VCZaOBIg\",\"relocating_node\":null,\"shard\":6,\"index\":\"intape-rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23769857},\"translog\":{\"id\":1423612800807,\"operations\":0},\"docs\":{\"num_docs\":94738,\"max_doc\":94738,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"u0UGAe5cR8iqrfQobCI5_w\",\"relocating_node\":null,\"shard\":6,\"index\":\"intape-rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23769857},\"translog\":{\"id\":1423612800807,\"operations\":0},\"docs\":{\"num_docs\":94738,\"max_doc\":94738,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1773,\"total_time_in_millis\":3923954,\"total_docs\":53332808,\"total_size_in_bytes\":13473656090},\"refresh\":{\"total\":15596,\"total_time_in_millis\":93154},\"flush\":{\"total\":48,\"total_time_in_millis\":5422}}],\"7\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"2tyDIGR0Ts2AQLMCBnqS1g\",\"relocating_node\":null,\"shard\":7,\"index\":\"intape-rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23827845},\"translog\":{\"id\":1423612800807,\"operations\":0},\"docs\":{\"num_docs\":94773,\"max_doc\":94773,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"te0M1g2yQhOAkWg9UHDMrw\",\"relocating_node\":null,\"shard\":7,\"index\":\"intape-rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23827845},\"translog\":{\"id\":1423612800807,\"operations\":0},\"docs\":{\"num_docs\":94773,\"max_doc\":94773,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1763,\"total_time_in_millis\":2396254,\"total_docs\":52731891,\"total_size_in_bytes\":13335376548},\"refresh\":{\"total\":15560,\"total_time_in_millis\":82806},\"flush\":{\"total\":48,\"total_time_in_millis\":2888}}],\"8\":[{\"routing\":{\"state\":\"\nSTARTED\",\"primary\":true,\"node\":\"qXwalJKMSk6W0JaibIpTQg\",\"relocating_node\":null,\"shard\":8,\"index\":\"intape-rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23821341},\"translog\":{\"id\":1423612800827,\"operations\":0},\"docs\":{\"num_docs\":94850,\"max_doc\":94850,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1758,\"total_time_in_millis\":2384217,\"total_docs\":52410494,\"total_size_in_bytes\":13254609554},\"refresh\":{\"total\":15520,\"total_time_in_millis\":71426},\"flush\":{\"total\":48,\"total_time_in_millis\":3545}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"ix5R9EwPTEWLzWL2ZyiUOA\",\"relocating_node\":null,\"shard\":8,\"index\":\"intape-rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23849034},\"translog\":{\"id\":1423612800827,\"operations\":0},\"docs\":{\"num_docs\":94850,\"max_doc\":94850,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1757,\"total_time_in_millis\":3881467,\"total_docs\":52527922,\"total_size_in_bytes\":13282311719},\"refresh\":{\"total\":15450,\"total_time_in_millis\":96936},\"flush\":{\"total\":48,\"total_time_in_millis\":4531}}],\"9\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"9yna4MXDQZCBN_XpvTKd3Q\",\"relocating_node\":null,\"shard\":9,\"index\":\"intape-rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23860155},\"translog\":{\"id\":1423612800826,\"operations\":0},\"docs\":{\"num_docs\":95059,\"max_doc\":95059,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1761,\"total_time_in_millis\":3855700,\"total_docs\":52652055,\"total_size_in_bytes\":13301814727},\"refresh\":{\"total\":15508,\"total_time_in_millis\":79005},\"flush\":{\"total\":48,\"total_time_in_millis\":5114}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"aTLkCoPHScqMWT3zoEGTnw\",\"relocating_node\":null,\"shard\":9,\"index\":\"intape-rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23854538},\"translog\":{\"id\":1423612800826,\"operations\":0},\"docs\":{\"num_docs\":95059,\"max_doc\":95059,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1760,\"total_time_in_millis\":2401217,\"total_docs\":53039827,\"total_size_in_bytes\":13402486627},\"refresh\":{\"total\":15513,\"total_time_in_millis\":70334},\"flush\":{\"total\":48,\"total_time_in_millis\":4004}}],\"10\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"RK84sm3-T-mO6xUfE5mq_Q\",\"relocating_node\":null,\"shard\":10,\"index\":\"intape-rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23789244},\"translog\":{\"id\":1423612800812,\"operations\":0},\"docs\":{\"num_docs\":94436,\"max_doc\":94436,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1755,\"total_time_in_millis\":2389044,\"total_docs\":52484096,\"total_size_in_bytes\":13282632991},\"refresh\":{\"total\":15513,\"total_time_in_millis\":91291},\"flush\":{\"total\":48,\"total_time_in_millis\":3583}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"Mg5HpexgTuCOtCs218D1GA\",\"relocating_node\":null,\"shard\":10,\"index\":\"intape-rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23765695},\"translog\":{\"id\":1423612800812,\"operations\":0},\"docs\":{\"num_docs\":94436,\"max_doc\":94436,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1762,\"total_time_in_millis\":3860527,\"total_docs\":52427206,\"total_size_in_bytes\":13266993293},\"refresh\":{\"total\":15521,\"total_time_in_millis\":76508},\"flush\":{\"total\":48,\"total_time_in_millis\":3413}}],\"11\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"RJwWye67QkWKKjl_b3NZ-A\",\"relocating_node\":null,\"shard\":11,\"index\":\"intape-rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23757634},\"translog\":{\"id\":1423612800808,\"operations\":0},\"docs\":{\"num_docs\":94653,\"max_doc\":94653,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"c5cY7a7EQrK1rPMFWheBtg\",\"relocating\nnode\":null,\"shard\":11,\"index\":\"intape-rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23757634},\"translog\":{\"id\":1423612800808,\"operations\":0},\"docs\":{\"num_docs\":94653,\"max_doc\":94653,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1760,\"total_time_in_millis\":2410216,\"total_docs\":53002650,\"total_size_in_bytes\":13376734743},\"refresh\":{\"total\":15512,\"total_time_in_millis\":93650},\"flush\":{\"total\":48,\"total_time_in_millis\":5033}}],\"12\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"MimlEBKgQH2_oc3JxqxpXA\",\"relocating_node\":null,\"shard\":12,\"index\":\"intape-rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23788498},\"translog\":{\"id\":1423612800827,\"operations\":0},\"docs\":{\"num_docs\":94473,\"max_doc\":94473,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1766,\"total_time_in_millis\":3845308,\"total_docs\":52668997,\"total_size_in_bytes\":13329313639},\"refresh\":{\"total\":15509,\"total_time_in_millis\":85683},\"flush\":{\"total\":48,\"total_time_in_millis\":3390}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"qsymmz4LTeKkq_7AuKqG4g\",\"relocating_node\":null,\"shard\":12,\"index\":\"intape-rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23793503},\"translog\":{\"id\":1423612800827,\"operations\":0},\"docs\":{\"num_docs\":94473,\"max_doc\":94473,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1753,\"total_time_in_millis\":2381101,\"total_docs\":52298178,\"total_size_in_bytes\":13231409655},\"refresh\":{\"total\":15458,\"total_time_in_millis\":98296},\"flush\":{\"total\":48,\"total_time_in_millis\":3999}}],\"13\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"ob27c5P3TbWWv-2xF1KTWQ\",\"relocating_node\":null,\"shard\":13,\"index\":\"intape-rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23750197},\"translog\":{\"id\":1423612800826,\"operations\":0},\"docs\":{\"num_docs\":94529,\"max_doc\":94529,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1766,\"total_time_in_millis\":3822129,\"total_docs\":52757723,\"total_size_in_bytes\":13352869357},\"refresh\":{\"total\":15514,\"total_time_in_millis\":113163},\"flush\":{\"total\":48,\"total_time_in_millis\":4030}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"LGUsqbQXRei4YLyIjf7aow\",\"relocating_node\":null,\"shard\":13,\"index\":\"intape-rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23750634},\"translog\":{\"id\":1423612800826,\"operations\":0},\"docs\":{\"num_docs\":94529,\"max_doc\":94529,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1745,\"total_time_in_millis\":2364107,\"total_docs\":52079580,\"total_size_in_bytes\":13173669798},\"refresh\":{\"total\":15479,\"total_time_in_millis\":82663},\"flush\":{\"total\":48,\"total_time_in_millis\":3343}}]}},\"intape-rsyslog-2015-02-12\":{\"index\":{\"primary_size_in_bytes\":330786209,\"size_in_bytes\":661551278},\"translog\":{\"operations\":0},\"docs\":{\"num_docs\":1317387,\"max_doc\":1317387,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":33796,\"total_time_in_millis\":62125982,\"total_docs\":1030385376,\"total_size_in_bytes\":260076755944},\"refresh\":{\"total\":299892,\"total_time_in_millis\":1799428},\"flush\":{\"total\":929,\"total_time_in_millis\":80739},\"shards\":{\"0\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"u0UGAe5cR8iqrfQobCI5_w\",\"relocating_node\":null,\"shard\":0,\"index\":\"intape-rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23605536},\"translog\":{\"id\":1423699201019,\"operations\":0},\"docs\":{\"num_docs\":93876,\"max_doc\":93876,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"hsLYPGy2TyC-Z6gsJ_YMqQ\",\"relocating_node\":null,\"shard\":0,\"index\":\"intape-rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23605536},\"translog\":{\"id\":1423699201019,\"operations\":0},\"\ndocs\":{\"num_docs\":93876,\"max_doc\":93876,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1782,\"total_time_in_millis\":2711109,\"total_docs\":54713351,\"total_size_in_bytes\":13812468648},\"refresh\":{\"total\":15817,\"total_time_in_millis\":138744},\"flush\":{\"total\":49,\"total_time_in_millis\":4128}}],\"1\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"4pLsTb6kQCur_Nw5jwDm2A\",\"relocating_node\":null,\"shard\":1,\"index\":\"intape-rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23613487},\"translog\":{\"id\":1423699201035,\"operations\":0},\"docs\":{\"num_docs\":94062,\"max_doc\":94062,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1774,\"total_time_in_millis\":2447075,\"total_docs\":54407100,\"total_size_in_bytes\":13732560478},\"refresh\":{\"total\":15808,\"total_time_in_millis\":72092},\"flush\":{\"total\":49,\"total_time_in_millis\":2805}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"Pp9D83T_SDq73IfCI93JFA\",\"relocating_node\":null,\"shard\":1,\"index\":\"intape-rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23607123},\"translog\":{\"id\":1423699201035,\"operations\":0},\"docs\":{\"num_docs\":94062,\"max_doc\":94062,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1781,\"total_time_in_millis\":4577090,\"total_docs\":53636969,\"total_size_in_bytes\":13538375121},\"refresh\":{\"total\":15797,\"total_time_in_millis\":99375},\"flush\":{\"total\":49,\"total_time_in_millis\":4818}}],\"2\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"cTWT_a2hRvaHHdtugjXxMw\",\"relocating_node\":null,\"shard\":2,\"index\":\"intape-rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23570337},\"translog\":{\"id\":1423699201035,\"operations\":0},\"docs\":{\"num_docs\":94067,\"max_doc\":94067,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"UT-z9BwoRYGHDO65Xe77lA\",\"relocating_node\":null,\"shard\":2,\"index\":\"intape-rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23570337},\"translog\":{\"id\":1423699201035,\"operations\":0},\"docs\":{\"num_docs\":94067,\"max_doc\":94067,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1784,\"total_time_in_millis\":2474504,\"total_docs\":54739469,\"total_size_in_bytes\":13789187846},\"refresh\":{\"total\":15801,\"total_time_in_millis\":88792},\"flush\":{\"total\":49,\"total_time_in_millis\":3153}}],\"3\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"zcjnLJwYTI2nBn90i470OA\",\"relocating_node\":null,\"shard\":3,\"index\":\"intape-rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23714623},\"translog\":{\"id\":1423699200997,\"operations\":0},\"docs\":{\"num_docs\":94381,\"max_doc\":94381,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1788,\"total_time_in_millis\":4785531,\"total_docs\":53911618,\"total_size_in_bytes\":13621744883},\"refresh\":{\"total\":15875,\"total_time_in_millis\":89455},\"flush\":{\"total\":49,\"total_time_in_millis\":5980}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"yVMmqz7RR8mr4GSwkkX-4A\",\"relocating_node\":null,\"shard\":3,\"index\":\"intape-rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23698263},\"translog\":{\"id\":1423699200997,\"operations\":0},\"docs\":{\"num_docs\":94381,\"max_doc\":94381,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1778,\"total_time_in_millis\":2435434,\"total_docs\":54060662,\"total_size_in_bytes\":13654763086},\"refresh\":{\"total\":15837,\"total_time_in_millis\":70369},\"flush\":{\"total\":49,\"total_time_in_millis\":4012}}],\"4\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"2tyDIGR0Ts2AQLMCBnqS1g\",\"relocating_node\":null,\"shard\":4,\"index\":\"intape-rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23705421},\"translog\":{\"id\":1423699200996,\"operations\":0},\"docs\":{\"num_docs\":94443,\"max_doc\":94443,\"deleted_docs\":0},\"merges\"\n:{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"LGUsqbQXRei4YLyIjf7aow\",\"relocating_node\":null,\"shard\":4,\"index\":\"intape-rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23705421},\"translog\":{\"id\":1423699200996,\"operations\":0},\"docs\":{\"num_docs\":94443,\"max_doc\":94443,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1785,\"total_time_in_millis\":2435736,\"total_docs\":54187082,\"total_size_in_bytes\":13669847554},\"refresh\":{\"total\":15872,\"total_time_in_millis\":78288},\"flush\":{\"total\":49,\"total_time_in_millis\":3111}}],\"5\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"gNLthbzQSW-h_v2oGvM4Mw\",\"relocating_node\":null,\"shard\":5,\"index\":\"intape-rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23688242},\"translog\":{\"id\":1423699200988,\"operations\":0},\"docs\":{\"num_docs\":94244,\"max_doc\":94244,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"4hA88zpfSX2WRoc_-wWxQ\",\"relocating_node\":null,\"shard\":5,\"index\":\"intape-rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23688242},\"translog\":{\"id\":1423699200988,\"operations\":0},\"docs\":{\"num_docs\":94244,\"max_doc\":94244,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1790,\"total_time_in_millis\":4782327,\"total_docs\":54033631,\"total_size_in_bytes\":13661197404},\"refresh\":{\"total\":15862,\"total_time_in_millis\":146678},\"flush\":{\"total\":49,\"total_time_in_millis\":7059}}],\"6\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"RJwWye67QkWKKjl_b3NZ-A\",\"relocating_node\":null,\"shard\":6,\"index\":\"intape-rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23571017},\"translog\":{\"id\":1423699200993,\"operations\":0},\"docs\":{\"num_docs\":93835,\"max_doc\":93835,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"aTLkCoPHScqMWT3zoEGTnw\",\"relocating_node\":null,\"shard\":6,\"index\":\"intape-rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23571017},\"translog\":{\"id\":1423699200993,\"operations\":0},\"docs\":{\"num_docs\":93835,\"max_doc\":93835,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1777,\"total_time_in_millis\":2446693,\"total_docs\":54218034,\"total_size_in_bytes\":13683149255},\"refresh\":{\"total\":15789,\"total_time_in_millis\":71661},\"flush\":{\"total\":49,\"total_time_in_millis\":2890}}],\"7\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"IW9Mab9kTNuH16VCZaOBIg\",\"relocating_node\":null,\"shard\":7,\"index\":\"intape-rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23649609},\"translog\":{\"id\":1423699200994,\"operations\":0},\"docs\":{\"num_docs\":94145,\"max_doc\":94145,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"YU6Kl1IzTGeEPL4K875vFg\",\"relocating_node\":null,\"shard\":7,\"index\":\"intape-rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23649609},\"translog\":{\"id\":1423699200994,\"operations\":0},\"docs\":{\"num_docs\":94145,\"max_doc\":94145,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1783,\"total_time_in_millis\":4786572,\"total_docs\":53611638,\"total_size_in_bytes\":13530963011},\"refresh\":{\"total\":15765,\"\ntotal_time_in_millis\":138567},\"flush\":{\"total\":49,\"total_time_in_millis\":6110}}],\"8\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"ob27c5P3TbWWv-2xF1KTWQ\",\"relocating_node\":null,\"shard\":8,\"index\":\"intape-rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23587167},\"translog\":{\"id\":1423699201009,\"operations\":0},\"docs\":{\"num_docs\":93925,\"max_doc\":93925,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"te0M1g2yQhOAkWg9UHDMrw\",\"relocating_node\":null,\"shard\":8,\"index\":\"intape-rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23587167},\"translog\":{\"id\":1423699201009,\"operations\":0},\"docs\":{\"num_docs\":93925,\"max_doc\":93925,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1774,\"total_time_in_millis\":1361220,\"total_docs\":53538957,\"total_size_in_bytes\":13515566415},\"refresh\":{\"total\":15695,\"total_time_in_millis\":77870},\"flush\":{\"total\":49,\"total_time_in_millis\":3267}}],\"9\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"-5VMHj1DQYqKR6vi018o5w\",\"relocating_node\":null,\"shard\":9,\"index\":\"intape-rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23495412},\"translog\":{\"id\":1423699200991,\"operations\":0},\"docs\":{\"num_docs\":93723,\"max_doc\":93723,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1705,\"total_time_in_millis\":2476676,\"total_docs\":54715385,\"total_size_in_bytes\":13792225266},\"refresh\":{\"total\":15137,\"total_time_in_millis\":72886},\"flush\":{\"total\":47,\"total_time_in_millis\":3620}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"ix5R9EwPTEWLzWL2ZyiUOA\",\"relocating_node\":null,\"shard\":9,\"index\":\"intape-rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23491788},\"translog\":{\"id\":1423699200991,\"operations\":0},\"docs\":{\"num_docs\":93723,\"max_doc\":93723,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1790,\"total_time_in_millis\":4908188,\"total_docs\":54526593,\"total_size_in_bytes\":13746219924},\"refresh\":{\"total\":15824,\"total_time_in_millis\":113271},\"flush\":{\"total\":49,\"total_time_in_millis\":7237}}],\"10\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"4hA88zpfSX2WRoc-wWxQ\",\"relocating_node\":null,\"shard\":10,\"index\":\"intape-rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23702520},\"translog\":{\"id\":1423699200999,\"operations\":0},\"docs\":{\"num_docs\":94341,\"max_doc\":94341,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"qXwalJKMSk6W0JaibIpTQg\",\"relocating_node\":null,\"shard\":10,\"index\":\"intape-rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23702520},\"translog\":{\"id\":1423699200999,\"operations\":0},\"docs\":{\"num_docs\":94341,\"max_doc\":94341,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1790,\"total_time_in_millis\":2441801,\"total_docs\":54293890,\"total_size_in_bytes\":13707920488},\"refresh\":{\"total\":15892,\"total_time_in_millis\":85187},\"flush\":{\"total\":49,\"total_time_in_millis\":3388}}],\"11\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"Mg5HpexgTuCOtCs218D1GA\",\"relocating_node\":null,\"shard\":11,\"index\":\"intape-rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23593933},\"translog\":{\"id\":1423699201042,\"operations\":0},\"docs\":{\"num_docs\":94118,\"max_doc\":94118,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1789,\"total_time_in_millis\":4789168,\"total_docs\":54394860,\"total_size_in_bytes\":13732031487},\"refresh\":{\"total\":15856,\"total_time_in_millis\":90393},\"flush\":{\"total\":49,\"total_time_in_millis\":4186}},{\"routing\":{\"state\"\n:\"STARTED\",\"primary\":true,\"node\":\"qsymmz4LTeKkq_7AuKqG4g\",\"relocating_node\":null,\"shard\":11,\"index\":\"intape-rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23618676},\"translog\":{\"id\":1423699201042,\"operations\":0},\"docs\":{\"num_docs\":94118,\"max_doc\":94118,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1779,\"total_time_in_millis\":2467415,\"total_docs\":54563578,\"total_size_in_bytes\":13771209782},\"refresh\":{\"total\":15807,\"total_time_in_millis\":84623},\"flush\":{\"total\":49,\"total_time_in_millis\":3614}}],\"12\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"9yna4MXDQZCBN_XpvTKd3Q\",\"relocating_node\":null,\"shard\":12,\"index\":\"intape-rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23670511},\"translog\":{\"id\":1423699200991,\"operations\":0},\"docs\":{\"num_docs\":94224,\"max_doc\":94224,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"RK84sm3-T-mO6xUfE5mq_Q\",\"relocating_node\":null,\"shard\":12,\"index\":\"intape-rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23670511},\"translog\":{\"id\":1423699200991,\"operations\":0},\"docs\":{\"num_docs\":94224,\"max_doc\":94224,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1771,\"total_time_in_millis\":2429303,\"total_docs\":53707753,\"total_size_in_bytes\":13564992378},\"refresh\":{\"total\":15757,\"total_time_in_millis\":94255},\"flush\":{\"total\":49,\"total_time_in_millis\":3242}}],\"13\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"MimlEBKgQH2_oc3JxqxpXA\",\"relocating_node\":null,\"shard\":13,\"index\":\"intape-rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23607242},\"translog\":{\"id\":1423699200998,\"operations\":0},\"docs\":{\"num_docs\":94003,\"max_doc\":94003,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1797,\"total_time_in_millis\":4900041,\"total_docs\":54471083,\"total_size_in_bytes\":13751609708},\"refresh\":{\"total\":15899,\"total_time_in_millis\":95620},\"flush\":{\"total\":49,\"total_time_in_millis\":4412}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"c5cY7a7EQrK1rPMFWheBtg\",\"relocating_node\":null,\"shard\":13,\"index\":\"intape-rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":23610011},\"translog\":{\"id\":1423699200998,\"operations\":0},\"docs\":{\"num_docs\":94003,\"max_doc\":94003,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1779,\"total_time_in_millis\":2470099,\"total_docs\":54653723,\"total_size_in_bytes\":13800723210},\"refresh\":{\"total\":15802,\"total_time_in_millis\":91302},\"flush\":{\"total\":49,\"total_time_in_millis\":3707}}]}},\"rsyslog-2015-02-14\":{\"index\":{\"primary_size_in_bytes\":108073017046,\"size_in_bytes\":326287114951},\"translog\":{\"operations\":19140942},\"docs\":{\"num_docs\":203830479,\"max_doc\":203830479,\"deleted_docs\":0},\"merges\":{\"current\":4,\"current_docs\":1058179,\"current_size_in_bytes\":561447723,\"total\":80632,\"total_time_in_millis\":231182016,\"total_docs\":2075096334,\"total_size_in_bytes\":1275435458365},\"refresh\":{\"total\":125033,\"total_time_in_millis\":12577728},\"flush\":{\"total\":1648,\"total_time_in_millis\":1125026},\"shards\":{\"0\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"te0M1g2yQhOAkWg9UHDMrw\",\"relocating_node\":null,\"shard\":0,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":7830774489},\"translog\":{\"id\":1423862054426,\"operations\":317666},\"docs\":{\"num_docs\":14959007,\"max_doc\":14959007,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":2187,\"total_time_in_millis\":6268169,\"total_docs\":52196895,\"total_size_in_bytes\":32066404546},\"refresh\":{\"total\":3202,\"total_time_in_millis\":244746},\"flush\":{\"total\":42,\"total_time_in_millis\":23883}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"MimlEBKgQH2_oc3JxqxpXA\",\"relocating_node\":null,\"shard\":0,\"index\":\"rsyslog-2015-02-14\"},\"\nstate\":\"STARTED\",\"index\":{\"size_in_bytes\":7889595809},\"translog\":{\"id\":1423862054426,\"operations\":278506},\"docs\":{\"num_docs\":14958649,\"max_doc\":14958649,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1948,\"total_time_in_millis\":3176881,\"total_docs\":51058958,\"total_size_in_bytes\":31450156752},\"refresh\":{\"total\":3191,\"total_time_in_millis\":281479},\"flush\":{\"total\":42,\"total_time_in_millis\":19693}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"qsymmz4LTeKkq_7AuKqG4g\",\"relocating_node\":null,\"shard\":0,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":7835444396},\"translog\":{\"id\":1423862054426,\"operations\":276141},\"docs\":{\"num_docs\":14960395,\"max_doc\":14960395,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1994,\"total_time_in_millis\":3116852,\"total_docs\":51574646,\"total_size_in_bytes\":31690713621},\"refresh\":{\"total\":3202,\"total_time_in_millis\":241187},\"flush\":{\"total\":42,\"total_time_in_millis\":29251}}],\"1\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"zcjnLJwYTI2nBn90i470OA\",\"relocating_node\":null,\"shard\":1,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":8168799331},\"translog\":{\"id\":1423862054415,\"operations\":227527},\"docs\":{\"num_docs\":14940474,\"max_doc\":14940474,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":2160,\"total_time_in_millis\":4611327,\"total_docs\":51311149,\"total_size_in_bytes\":31719403233},\"refresh\":{\"total\":3180,\"total_time_in_millis\":298531},\"flush\":{\"total\":42,\"total_time_in_millis\":39400}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"c5cY7a7EQrK1rPMFWheBtg\",\"relocating_node\":null,\"shard\":1,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":8364955123},\"translog\":{\"id\":1423862054415,\"operations\":298758},\"docs\":{\"num_docs\":14941860,\"max_doc\":14941860,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1915,\"total_time_in_millis\":3197261,\"total_docs\":52288550,\"total_size_in_bytes\":32019673413},\"refresh\":{\"total\":3199,\"total_time_in_millis\":244470},\"flush\":{\"total\":42,\"total_time_in_millis\":21405}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"ix5R9EwPTEWLzWL2ZyiUOA\",\"relocating_node\":null,\"shard\":1,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":8233931989},\"translog\":{\"id\":1423862054415,\"operations\":328294},\"docs\":{\"num_docs\":14940758,\"max_doc\":14940758,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":2125,\"total_time_in_millis\":3228238,\"total_docs\":51813895,\"total_size_in_bytes\":31936451185},\"refresh\":{\"total\":3193,\"total_time_in_millis\":275274},\"flush\":{\"total\":42,\"total_time_in_millis\":30012}}],\"2\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"Mg5HpexgTuCOtCs218D1GA\",\"relocating_node\":null,\"shard\":2,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":7895401081},\"translog\":{\"id\":1423862054471,\"operations\":276401},\"docs\":{\"num_docs\":14946355,\"max_doc\":14946355,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":2024,\"total_time_in_millis\":3280976,\"total_docs\":51016207,\"total_size_in_bytes\":31455556958},\"refresh\":{\"total\":3182,\"total_time_in_millis\":328946},\"flush\":{\"total\":42,\"total_time_in_millis\":27973}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"gNLthbzQSW-h_v2oGvM4Mw\",\"relocating_node\":null,\"shard\":2,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":7773573079},\"translog\":{\"id\":1423862054472,\"operations\":10270},\"docs\":{\"num_docs\":14946007,\"max_doc\":14946007,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1915,\"total_time_in_millis\":4683715,\"total_docs\":51024960,\"total_size_in_bytes\":31321126210},\"refresh\":{\"total\":2917,\"total_time_in_millis\":373481},\"flush\":{\"total\":41,\"total_time_in_millis\":27492}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"LGUsqbQXRei4YLyIjf7aow\",\"relocating_node\":null,\"shard\":2,\"index\":\"rsyslog-2015-\n02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":8347495783},\"translog\":{\"id\":1423862054471,\"operations\":284823},\"docs\":{\"num_docs\":14946356,\"max_doc\":14946356,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1993,\"total_time_in_millis\":3236484,\"total_docs\":53576692,\"total_size_in_bytes\":32773770964},\"refresh\":{\"total\":3197,\"total_time_in_millis\":254412},\"flush\":{\"total\":42,\"total_time_in_millis\":22937}}],\"3\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"UT-z9BwoRYGHDO65Xe77lA\",\"relocating_node\":null,\"shard\":3,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":7846239332},\"translog\":{\"id\":1423862054415,\"operations\":232410},\"docs\":{\"num_docs\":14896169,\"max_doc\":14896169,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1898,\"total_time_in_millis\":3128901,\"total_docs\":51893953,\"total_size_in_bytes\":31798282379},\"refresh\":{\"total\":3197,\"total_time_in_millis\":250239},\"flush\":{\"total\":42,\"total_time_in_millis\":21993}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"yVMmqz7RR8mr4GSwkkX-4A\",\"relocating_node\":null,\"shard\":3,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":7774756065},\"translog\":{\"id\":1423862054411,\"operations\":302831},\"docs\":{\"num_docs\":14896458,\"max_doc\":14896458,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1933,\"total_time_in_millis\":11281088,\"total_docs\":50875533,\"total_size_in_bytes\":31067524815},\"refresh\":{\"total\":2722,\"total_time_in_millis\":215929},\"flush\":{\"total\":38,\"total_time_in_millis\":34764}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"Pp9D83T_SDq73IfCI93JFA\",\"relocating_node\":null,\"shard\":3,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":7879242818},\"translog\":{\"id\":1423862054415,\"operations\":252955},\"docs\":{\"num_docs\":14895659,\"max_doc\":14895659,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1787,\"total_time_in_millis\":16089140,\"total_docs\":49690704,\"total_size_in_bytes\":30495224872},\"refresh\":{\"total\":3197,\"total_time_in_millis\":267783},\"flush\":{\"total\":42,\"total_time_in_millis\":25610}}],\"4\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"2tyDIGR0Ts2AQLMCBnqS1g\",\"relocating_node\":null,\"shard\":4,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":7944565149},\"translog\":{\"id\":1423862987204,\"operations\":288854},\"docs\":{\"num_docs\":14172338,\"max_doc\":14172338,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1965,\"total_time_in_millis\":4038186,\"total_docs\":49899628,\"total_size_in_bytes\":30735112253},\"refresh\":{\"total\":2990,\"total_time_in_millis\":455891},\"flush\":{\"total\":40,\"total_time_in_millis\":24004}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"9yna4MXDQZCBN_XpvTKd3Q\",\"relocating_node\":null,\"shard\":4,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":7509042084},\"translog\":{\"id\":1423862987204,\"operations\":283759},\"docs\":{\"num_docs\":14171650,\"max_doc\":14171650,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":2218,\"total_time_in_millis\":10053279,\"total_docs\":49380904,\"total_size_in_bytes\":30501634076},\"refresh\":{\"total\":3032,\"total_time_in_millis\":247396},\"flush\":{\"total\":40,\"total_time_in_millis\":27491}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"IW9Mab9kTNuH16VCZaOBIg\",\"relocating_node\":null,\"shard\":4,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":7420827345},\"translog\":{\"id\":1423862987204,\"operations\":320406},\"docs\":{\"num_docs\":14173418,\"max_doc\":14173418,\"deleted_docs\":0},\"merges\":{\"current\":1,\"current_docs\":13192,\"current_size_in_bytes\":7931588,\"total\":1869,\"total_time_in_millis\":5740326,\"total_docs\":49928102,\"total_size_in_bytes\":30635976468},\"refresh\":{\"total\":3023,\"total_time_in_millis\":292175},\"flush\":{\"total\":40,\"total_time_in_millis\":33089}}],\"5\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"_virthvxT4idC9C3NIMwTg\",\"relocating_node\":null,\"\nshard\":5,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":7585991632},\"translog\":{\"id\":1423862090196,\"operations\":83526},\"docs\":{\"num_docs\":14324296,\"max_doc\":14324296,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1864,\"total_time_in_millis\":6495081,\"total_docs\":47104181,\"total_size_in_bytes\":29207488346},\"refresh\":{\"total\":2855,\"total_time_in_millis\":629061},\"flush\":{\"total\":40,\"total_time_in_millis\":32650}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"IW9Mab9kTNuH16VCZaOBIg\",\"relocating_node\":null,\"shard\":5,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":7539307193},\"translog\":{\"id\":1423862090196,\"operations\":101190},\"docs\":{\"num_docs\":14324095,\"max_doc\":14324095,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1993,\"total_time_in_millis\":5858629,\"total_docs\":49671711,\"total_size_in_bytes\":30649069459},\"refresh\":{\"total\":3023,\"total_time_in_millis\":296539},\"flush\":{\"total\":41,\"total_time_in_millis\":25204}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"zcjnLJwYTI2nBn90i470OA\",\"relocating_node\":null,\"shard\":5,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":7582926664},\"translog\":{\"id\":1423862090196,\"operations\":125748},\"docs\":{\"num_docs\":14324301,\"max_doc\":14324301,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1968,\"total_time_in_millis\":4584384,\"total_docs\":49412519,\"total_size_in_bytes\":30446229704},\"refresh\":{\"total\":3027,\"total_time_in_millis\":292966},\"flush\":{\"total\":41,\"total_time_in_millis\":26962}}],\"6\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"2tyDIGR0Ts2AQLMCBnqS1g\",\"relocating_node\":null,\"shard\":6,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":7635028426},\"translog\":{\"id\":1423862666686,\"operations\":280737},\"docs\":{\"num_docs\":14535810,\"max_doc\":14535810,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1924,\"total_time_in_millis\":4177806,\"total_docs\":51378028,\"total_size_in_bytes\":31568153321},\"refresh\":{\"total\":3058,\"total_time_in_millis\":459910},\"flush\":{\"total\":41,\"total_time_in_millis\":30256}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"_virthvxT4idC9C3NIMwTg\",\"relocating_node\":null,\"shard\":6,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":7625014328},\"translog\":{\"id\":1423862666686,\"operations\":303273},\"docs\":{\"num_docs\":14537238,\"max_doc\":14537238,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1859,\"total_time_in_millis\":6938284,\"total_docs\":49783296,\"total_size_in_bytes\":30686678523},\"refresh\":{\"total\":2848,\"total_time_in_millis\":626190},\"flush\":{\"total\":40,\"total_time_in_millis\":29137}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"4pLsTb6kQCur_Nw5jwDm2A\",\"relocating_node\":null,\"shard\":6,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":7585277969},\"translog\":{\"id\":1423862666686,\"operations\":293323},\"docs\":{\"num_docs\":14537187,\"max_doc\":14537187,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":2092,\"total_time_in_millis\":8698742,\"total_docs\":52747133,\"total_size_in_bytes\":32300161894},\"refresh\":{\"total\":3103,\"total_time_in_millis\":228792},\"flush\":{\"total\":41,\"total_time_in_millis\":32178}}],\"7\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"u0UGAe5cR8iqrfQobCI5_w\",\"relocating_node\":null,\"shard\":7,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":7759733889},\"translog\":{\"id\":1423862054426,\"operations\":225320},\"docs\":{\"num_docs\":14903854,\"max_doc\":14903854,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1915,\"total_time_in_millis\":6258489,\"total_docs\":49284932,\"total_size_in_bytes\":30136321799},\"refresh\":{\"total\":2777,\"total_time_in_millis\":245360},\"flush\":{\"total\":38,\"total_time_in_millis\":28294}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"aTLkCoPHScqMWT3zoEGTnw\",\"relocating\nnode\":null,\"shard\":7,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":8232627944},\"translog\":{\"id\":1423862054426,\"operations\":234416},\"docs\":{\"num_docs\":14902419,\"max_doc\":14902419,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1890,\"total_time_in_millis\":4323082,\"total_docs\":52583907,\"total_size_in_bytes\":32149319665},\"refresh\":{\"total\":3192,\"total_time_in_millis\":235889},\"flush\":{\"total\":42,\"total_time_in_millis\":21840}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"Pp9D83T_SDq73IfCI93JFA\",\"relocating_node\":null,\"shard\":7,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":7888521987},\"translog\":{\"id\":1423862054426,\"operations\":70902},\"docs\":{\"num_docs\":14902423,\"max_doc\":14902423,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1648,\"total_time_in_millis\":15075805,\"total_docs\":46438046,\"total_size_in_bytes\":28409646733},\"refresh\":{\"total\":2800,\"total_time_in_millis\":249334},\"flush\":{\"total\":38,\"total_time_in_millis\":19391}}],\"8\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"ob27c5P3TbWWv-2xF1KTWQ\",\"relocating_node\":null,\"shard\":8,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":7468375568},\"translog\":{\"id\":1423862987215,\"operations\":312024},\"docs\":{\"num_docs\":14147639,\"max_doc\":14147639,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":2013,\"total_time_in_millis\":8377475,\"total_docs\":48622030,\"total_size_in_bytes\":29925962404},\"refresh\":{\"total\":2860,\"total_time_in_millis\":237364},\"flush\":{\"total\":39,\"total_time_in_millis\":33472}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"IW9Mab9kTNuH16VCZaOBIg\",\"relocating_node\":null,\"shard\":8,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":7427629921},\"translog\":{\"id\":1423862987215,\"operations\":210364},\"docs\":{\"num_docs\":14149275,\"max_doc\":14149275,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1879,\"total_time_in_millis\":5347465,\"total_docs\":48572036,\"total_size_in_bytes\":29909731808},\"refresh\":{\"total\":3020,\"total_time_in_millis\":316221},\"flush\":{\"total\":40,\"total_time_in_millis\":35281}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"cTWT_a2hRvaHHdtugjXxMw\",\"relocating_node\":null,\"shard\":8,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":7529103544},\"translog\":{\"id\":1423862987215,\"operations\":271966},\"docs\":{\"num_docs\":14147828,\"max_doc\":14147828,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":2058,\"total_time_in_millis\":4100560,\"total_docs\":49631716,\"total_size_in_bytes\":30592505473},\"refresh\":{\"total\":3024,\"total_time_in_millis\":279231},\"flush\":{\"total\":40,\"total_time_in_millis\":21233}}],\"9\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"cTWT_a2hRvaHHdtugjXxMw\",\"relocating_node\":null,\"shard\":9,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":7999622111},\"translog\":{\"id\":1423861204479,\"operations\":72726},\"docs\":{\"num_docs\":15335773,\"max_doc\":15335773,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":2070,\"total_time_in_millis\":4362916,\"total_docs\":53733375,\"total_size_in_bytes\":32917089233},\"refresh\":{\"total\":3360,\"total_time_in_millis\":307328},\"flush\":{\"total\":44,\"total_time_in_millis\":27132}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"qXwalJKMSk6W0JaibIpTQg\",\"relocating_node\":null,\"shard\":9,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":7969661137},\"translog\":{\"id\":1423861204479,\"operations\":80354},\"docs\":{\"num_docs\":15335764,\"max_doc\":15335764,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1983,\"total_time_in_millis\":4123248,\"total_docs\":53758320,\"total_size_in_bytes\":32833015855},\"refresh\":{\"total\":3370,\"total_time_in_millis\":248480},\"flush\":{\"total\":44,\"total_time_in_millis\":29459}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"4pLsTb6kQCur_Nw5jwDm2A\",\"\nrelocating_node\":null,\"shard\":9,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":7870960603},\"translog\":{\"id\":1423861204479,\"operations\":44888},\"docs\":{\"num_docs\":15337205,\"max_doc\":15337205,\"deleted_docs\":0},\"merges\":{\"current\":1,\"current_docs\":10437,\"current_size_in_bytes\":6329967,\"total\":2179,\"total_time_in_millis\":9171880,\"total_docs\":59300683,\"total_size_in_bytes\":35881122050},\"refresh\":{\"total\":3368,\"total_time_in_millis\":260812},\"flush\":{\"total\":44,\"total_time_in_millis\":31569}}],\"10\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"ob27c5P3TbWWv-2xF1KTWQ\",\"relocating_node\":null,\"shard\":10,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":7333782123},\"translog\":{\"id\":1423863784935,\"operations\":22182},\"docs\":{\"num_docs\":13880307,\"max_doc\":13880307,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1939,\"total_time_in_millis\":8093557,\"total_docs\":47982731,\"total_size_in_bytes\":29564600087},\"refresh\":{\"total\":2878,\"total_time_in_millis\":237537},\"flush\":{\"total\":40,\"total_time_in_millis\":30007}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"virthvxT4idC9C3NIMwTg\",\"relocating_node\":null,\"shard\":10,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":7348557385},\"translog\":{\"id\":1423863784935,\"operations\":33710},\"docs\":{\"num_docs\":13881568,\"max_doc\":13881568,\"deleted_docs\":0},\"merges\":{\"current\":1,\"current_docs\":116699,\"current_size_in_bytes\":65691811,\"total\":1947,\"total_time_in_millis\":6724823,\"total_docs\":47831049,\"total_size_in_bytes\":29617434967},\"refresh\":{\"total\":2799,\"total_time_in_millis\":631150},\"flush\":{\"total\":40,\"total_time_in_millis\":49737}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"qXwalJKMSk6W0JaibIpTQg\",\"relocating_node\":null,\"shard\":10,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":7859155598},\"translog\":{\"id\":1423863784934,\"operations\":347861},\"docs\":{\"num_docs\":13880319,\"max_doc\":13880319,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1986,\"total_time_in_millis\":3871597,\"total_docs\":49142125,\"total_size_in_bytes\":30217129538},\"refresh\":{\"total\":2883,\"total_time_in_millis\":209338},\"flush\":{\"total\":39,\"total_time_in_millis\":29850}}],\"11\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"YU6Kl1IzTGeEPL4K875vFg\",\"relocating_node\":null,\"shard\":11,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":8082240131},\"translog\":{\"id\":1423861204428,\"operations\":46538},\"docs\":{\"num_docs\":15375609,\"max_doc\":15375609,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":2049,\"total_time_in_millis\":3348737,\"total_docs\":53424343,\"total_size_in_bytes\":32790383595},\"refresh\":{\"total\":3363,\"total_time_in_millis\":297865},\"flush\":{\"total\":44,\"total_time_in_millis\":29926}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"gNLthbzQSW-h_v2oGvM4Mw\",\"relocating_node\":null,\"shard\":11,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":8057795987},\"translog\":{\"id\":1423861204428,\"operations\":28312},\"docs\":{\"num_docs\":15375290,\"max_doc\":15375290,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1864,\"total_time_in_millis\":4515870,\"total_docs\":50369837,\"total_size_in_bytes\":30904057726},\"refresh\":{\"total\":2915,\"total_time_in_millis\":371754},\"flush\":{\"total\":41,\"total_time_in_millis\":29728}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"aTLkCoPHScqMWT3zoEGTnw\",\"relocating_node\":null,\"shard\":11,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":8087594782},\"translog\":{\"id\":1423861204428,\"operations\":66323},\"docs\":{\"num_docs\":15375605,\"max_doc\":15375605,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1959,\"total_time_in_millis\":4267643,\"total_docs\":53288515,\"total_size_in_bytes\":32598985258},\"refresh\":{\"total\":3368,\"total_time_in_millis\":252600},\"flush\":{\"total\":44,\"total_time_in_millis\":25183}}],\"12\":[{\"routing\":{\"state\":\"STARTED\",\"primary\"\n:false,\"node\":\"RK84sm3-T-mO6xUfE5mq_Q\",\"relocating_node\":null,\"shard\":12,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":8130167510},\"translog\":{\"id\":1423861204500,\"operations\":85681},\"docs\":{\"num_docs\":15383593,\"max_doc\":15383593,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":2063,\"total_time_in_millis\":4003400,\"total_docs\":51993355,\"total_size_in_bytes\":31985630060},\"refresh\":{\"total\":3368,\"total_time_in_millis\":256050},\"flush\":{\"total\":44,\"total_time_in_millis\":22552}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"4hA88zpfSX2WRoc-wWxQ\",\"relocating_node\":null,\"shard\":12,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":8097039819},\"translog\":{\"id\":1423861204500,\"operations\":94103},\"docs\":{\"num_docs\":15383595,\"max_doc\":15383595,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":2031,\"total_time_in_millis\":3272256,\"total_docs\":52664390,\"total_size_in_bytes\":32357590576},\"refresh\":{\"total\":3360,\"total_time_in_millis\":299496},\"flush\":{\"total\":44,\"total_time_in_millis\":31839}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"hsLYPGy2TyC-Z6gsJ_YMqQ\",\"relocating_node\":null,\"shard\":12,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":8077287945},\"translog\":{\"id\":1423861204500,\"operations\":41521},\"docs\":{\"num_docs\":15383915,\"max_doc\":15383915,\"deleted_docs\":0},\"merges\":{\"current\":1,\"current_docs\":917851,\"current_size_in_bytes\":481494357,\"total\":1933,\"total_time_in_millis\":3218546,\"total_docs\":53127763,\"total_size_in_bytes\":32515506823},\"refresh\":{\"total\":3368,\"total_time_in_millis\":288360},\"flush\":{\"total\":44,\"total_time_in_millis\":22788}}],\"13\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"RJwWye67QkWKKjl_b3NZ-A\",\"relocating_node\":null,\"shard\":13,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":6576203688},\"translog\":{\"id\":1423862861957,\"operations\":11684353},\"docs\":{\"num_docs\":12027445,\"max_doc\":12027445,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1785,\"total_time_in_millis\":3320045,\"total_docs\":43008757,\"total_size_in_bytes\":26883278966},\"refresh\":{\"total\":2187,\"total_time_in_millis\":383496},\"flush\":{\"total\":1,\"total_time_in_millis\":2294}},{\"routing\":{\"state\":\"INITIALIZING\",\"primary\":false,\"node\":\"_virthvxT4idC9C3NIMwTg\",\"relocating_node\":null,\"shard\":13,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"RECOVERING\",\"index\":{\"size_in_bytes\":7801927984}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"RK84sm3-T-mO6xUfE5mq_Q\",\"relocating_node\":null,\"shard\":13,\"index\":\"rsyslog-2015-02-14\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":6420935210},\"translog\":{\"id\":1423862861991,\"operations\":0},\"docs\":{\"num_docs\":12027453,\"max_doc\":12027453,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":1810,\"total_time_in_millis\":3520873,\"total_docs\":42710780,\"total_size_in_bytes\":26721352757},\"refresh\":{\"total\":2235,\"total_time_in_millis\":164666},\"flush\":{\"total\":35,\"total_time_in_millis\":18067}}]}},\"rsyslog-2015-02-13\":{\"index\":{\"primary_size_in_bytes\":1920082217272,\"size_in_bytes\":3856793970848},\"translog\":{\"operations\":0},\"docs\":{\"num_docs\":3435418196,\"max_doc\":3435418196,\"deleted_docs\":0},\"merges\":{\"current\":19,\"current_docs\":170768890,\"current_size_in_bytes\":89872053841,\"total\":262802,\"total_time_in_millis\":4460335096,\"total_docs\":20355136394,\"total_size_in_bytes\":11784435203735},\"refresh\":{\"total\":325353,\"total_time_in_millis\":153468974},\"flush\":{\"total\":16844,\"total_time_in_millis\":26471375},\"shards\":{\"0\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"MimlEBKgQH2_oc3JxqxpXA\",\"relocating_node\":null,\"shard\":0,\"index\":\"rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":134238649825},\"translog\":{\"id\":1423790996087,\"operations\":0},\"docs\":{\"num_docs\":262474827,\"max_doc\":262474827,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":16174,\"total_time_in_millis\":112225571,\"total_docs\":959456170,\"total_size_in\nbytes\":562424043746},\"refresh\":{\"total\":11792,\"total_time_in_millis\":2949721},\"flush\":{\"total\":701,\"total_time_in_millis\":1133202}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"qsymmz4LTeKkq_7AuKqG4g\",\"relocating_node\":null,\"shard\":0,\"index\":\"rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":134287482780},\"translog\":{\"id\":1423790996087,\"operations\":0},\"docs\":{\"num_docs\":262474827,\"max_doc\":262474827,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":17335,\"total_time_in_millis\":111911258,\"total_docs\":975724056,\"total_size_in_bytes\":571227145391},\"refresh\":{\"total\":13450,\"total_time_in_millis\":2815955},\"flush\":{\"total\":720,\"total_time_in_millis\":1177729}}],\"1\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"qXwalJKMSk6W0JaibIpTQg\",\"relocating_node\":null,\"shard\":1,\"index\":\"rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":134966628026},\"translog\":{\"id\":1423788115118,\"operations\":0},\"docs\":{\"num_docs\":264368587,\"max_doc\":264368587,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":17169,\"total_time_in_millis\":106522318,\"total_docs\":998160359,\"total_size_in_bytes\":584451926357},\"refresh\":{\"total\":14022,\"total_time_in_millis\":2625604},\"flush\":{\"total\":725,\"total_time_in_millis\":1038419}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"Pp9D83T_SDq73IfCI93JFA\",\"relocating_node\":null,\"shard\":1,\"index\":\"rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":183227004529},\"translog\":{\"id\":1423788115119,\"operations\":0},\"docs\":{\"num_docs\":264368587,\"max_doc\":264368587,\"deleted_docs\":0},\"merges\":{\"current\":4,\"current_docs\":29713536,\"current_size_in_bytes\":16233500680,\"total\":3944,\"total_time_in_millis\":199816479,\"total_docs\":671274286,\"total_size_in_bytes\":382334293439},\"refresh\":{\"total\":13208,\"total_time_in_millis\":7500481},\"flush\":{\"total\":726,\"total_time_in_millis\":1192976}}],\"2\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"cTWT_a2hRvaHHdtugjXxMw\",\"relocating_node\":null,\"shard\":2,\"index\":\"rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":139821382847},\"translog\":{\"id\":1423792502038,\"operations\":0},\"docs\":{\"num_docs\":262096453,\"max_doc\":262096453,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"yVMmqz7RR8mr4GSwkkX-4A\",\"relocating_node\":null,\"shard\":2,\"index\":\"rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":186649475741},\"translog\":{\"id\":1423792502038,\"operations\":0},\"docs\":{\"num_docs\":262096453,\"max_doc\":262096453,\"deleted_docs\":0},\"merges\":{\"current\":2,\"current_docs\":19742965,\"current_size_in_bytes\":10158760183,\"total\":5430,\"total_time_in_millis\":215847817,\"total_docs\":718795124,\"total_size_in_bytes\":411287096225},\"refresh\":{\"total\":12463,\"total_time_in_millis\":16815921},\"flush\":{\"total\":720,\"total_time_in_millis\":1042506}}],\"3\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"Pp9D83T_SDq73IfCI93JFA\",\"relocating_node\":null,\"shard\":3,\"index\":\"rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":172701165116},\"translog\":{\"id\":1423789003351,\"operations\":0},\"docs\":{\"num_docs\":255362235,\"max_doc\":255362235,\"deleted_docs\":0},\"merges\":{\"current\":3,\"current_docs\":27593399,\"current_size_in_bytes\":14690281056,\"total\":1289,\"total_time_in_millis\":229688268,\"total_docs\":630040327,\"total_size_in_bytes\":357846496609},\"refresh\":{\"total\":10268,\"total_time_in_millis\":7018293},\"flush\":{\"total\":663,\"total_time_in_millis\":1128870}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"LGUsqbQXRei4YLyIjf7aow\",\"relocating_node\":null,\"shard\":3,\"index\":\"rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":130392518103},\"translog\":{\"id\":1423789003352,\"operations\":0},\"docs\":{\"num_docs\":255362235,\"max_doc\":255362235,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":\n15325,\"total_time_in_millis\":101025329,\"total_docs\":969277865,\"total_size_in_bytes\":565025778883},\"refresh\":{\"total\":13838,\"total_time_in_millis\":2563183},\"flush\":{\"total\":703,\"total_time_in_millis\":1111980}}],\"4\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"9yna4MXDQZCBN_XpvTKd3Q\",\"relocating_node\":null,\"shard\":4,\"index\":\"rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":187754750204},\"translog\":{\"id\":1423788331270,\"operations\":0},\"docs\":{\"num_docs\":253856779,\"max_doc\":253856779,\"deleted_docs\":0},\"merges\":{\"current\":2,\"current_docs\":19688139,\"current_size_in_bytes\":10358983593,\"total\":5745,\"total_time_in_millis\":211769703,\"total_docs\":727747933,\"total_size_in_bytes\":414740860064},\"refresh\":{\"total\":13100,\"total_time_in_millis\":7444660},\"flush\":{\"total\":698,\"total_time_in_millis\":1115636}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"hsLYPGy2TyC-Z6gsJ_YMqQ\",\"relocating_node\":null,\"shard\":4,\"index\":\"rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":129824898780},\"translog\":{\"id\":1423788331270,\"operations\":0},\"docs\":{\"num_docs\":253856779,\"max_doc\":253856779,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":13917,\"total_time_in_millis\":106160188,\"total_docs\":947922666,\"total_size_in_bytes\":553283142163},\"refresh\":{\"total\":13522,\"total_time_in_millis\":4685901},\"flush\":{\"total\":698,\"total_time_in_millis\":1061940}}],\"5\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"2tyDIGR0Ts2AQLMCBnqS1g\",\"relocating_node\":null,\"shard\":5,\"index\":\"rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":99353553353},\"translog\":{\"id\":1423790995912,\"operations\":0},\"docs\":{\"num_docs\":189084807,\"max_doc\":189084807,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":6669,\"total_time_in_millis\":196346586,\"total_docs\":566261635,\"total_size_in_bytes\":328338133937},\"refresh\":{\"total\":12145,\"total_time_in_millis\":8807143},\"flush\":{\"total\":526,\"total_time_in_millis\":861400}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"UT-z9BwoRYGHDO65Xe77lA\",\"relocating_node\":null,\"shard\":5,\"index\":\"rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":98772205871},\"translog\":{\"id\":1423790995911,\"operations\":0},\"docs\":{\"num_docs\":189084807,\"max_doc\":189084807,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":8054,\"total_time_in_millis\":174539163,\"total_docs\":608275845,\"total_size_in_bytes\":353234858629},\"refresh\":{\"total\":12998,\"total_time_in_millis\":3778680},\"flush\":{\"total\":525,\"total_time_in_millis\":643357}}],\"6\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"c5cY7a7EQrK1rPMFWheBtg\",\"relocating_node\":null,\"shard\":6,\"index\":\"rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":135050216564},\"translog\":{\"id\":1423788115105,\"operations\":0},\"docs\":{\"num_docs\":264406506,\"max_doc\":264406506,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":17037,\"total_time_in_millis\":106539281,\"total_docs\":993301332,\"total_size_in_bytes\":581092845300},\"refresh\":{\"total\":13988,\"total_time_in_millis\":2741944},\"flush\":{\"total\":726,\"total_time_in_millis\":1036607}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"ix5R9EwPTEWLzWL2ZyiUOA\",\"relocating_node\":null,\"shard\":6,\"index\":\"rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":135188301305},\"translog\":{\"id\":1423788115107,\"operations\":0},\"docs\":{\"num_docs\":264406506,\"max_doc\":264406506,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":18363,\"total_time_in_millis\":131729033,\"total_docs\":992181619,\"total_size_in_bytes\":581958222885},\"refresh\":{\"total\":13950,\"total_time_in_millis\":3003595},\"flush\":{\"total\":728,\"total_time_in_millis\":1255001}}],\"7\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"YU6Kl1IzTGeEPL4K875vFg\",\"relocating_node\":null,\"shard\":7,\"index\":\"rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":135249623204},\"translog\":{\"id\":1423788057675,\"operations\":0},\"docs\":{\"num_docs\":264476623,\"max_doc\":\n264476623,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":19508,\"total_time_in_millis\":122040244,\"total_docs\":985054545,\"total_size_in_bytes\":579115442259},\"refresh\":{\"total\":13889,\"total_time_in_millis\":3295243},\"flush\":{\"total\":725,\"total_time_in_millis\":1503729}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"aTLkCoPHScqMWT3zoEGTnw\",\"relocating_node\":null,\"shard\":7,\"index\":\"rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":135011111031},\"translog\":{\"id\":1423788057676,\"operations\":0},\"docs\":{\"num_docs\":264476623,\"max_doc\":264476623,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":16804,\"total_time_in_millis\":111398222,\"total_docs\":989845343,\"total_size_in_bytes\":578908084493},\"refresh\":{\"total\":13981,\"total_time_in_millis\":2836404},\"flush\":{\"total\":726,\"total_time_in_millis\":1080348}}],\"8\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"RJwWye67QkWKKjl_b3NZ-A\",\"relocating_node\":null,\"shard\":8,\"index\":\"rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":92632391184},\"translog\":{\"id\":1423792501819,\"operations\":0},\"docs\":{\"num_docs\":176250563,\"max_doc\":176250563,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":8110,\"total_time_in_millis\":185739336,\"total_docs\":559108929,\"total_size_in_bytes\":325927955061},\"refresh\":{\"total\":11936,\"total_time_in_millis\":8105320},\"flush\":{\"total\":491,\"total_time_in_millis\":927048}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"te0M1g2yQhOAkWg9UHDMrw\",\"relocating_node\":null,\"shard\":8,\"index\":\"rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":92061477322},\"translog\":{\"id\":1423792501819,\"operations\":0},\"docs\":{\"num_docs\":176250563,\"max_doc\":176250563,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":8347,\"total_time_in_millis\":164892764,\"total_docs\":587033890,\"total_size_in_bytes\":342650755641},\"refresh\":{\"total\":12705,\"total_time_in_millis\":3376822},\"flush\":{\"total\":491,\"total_time_in_millis\":627039}}],\"9\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"ob27c5P3TbWWv-2xF1KTWQ\",\"relocating_node\":null,\"shard\":9,\"index\":\"rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":131935481628},\"translog\":{\"id\":1423789003358,\"operations\":0},\"docs\":{\"num_docs\":254239333,\"max_doc\":254239333,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":3509,\"total_time_in_millis\":295369478,\"total_docs\":737044723,\"total_size_in_bytes\":417178669358},\"refresh\":{\"total\":12993,\"total_time_in_millis\":7709118},\"flush\":{\"total\":700,\"total_time_in_millis\":1134056}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"yVMmqz7RR8mr4GSwkkX-4A\",\"relocating_node\":null,\"shard\":9,\"index\":\"rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":192497370432},\"translog\":{\"id\":1423789003357,\"operations\":0},\"docs\":{\"num_docs\":254239333,\"max_doc\":254239333,\"deleted_docs\":0},\"merges\":{\"current\":4,\"current_docs\":36553408,\"current_size_in_bytes\":19607764764,\"total\":4834,\"total_time_in_millis\":269735911,\"total_docs\":721991779,\"total_size_in_bytes\":411398947728},\"refresh\":{\"total\":13229,\"total_time_in_millis\":6412227},\"flush\":{\"total\":699,\"total_time_in_millis\":987718}}],\"10\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"u0UGAe5cR8iqrfQobCI5_w\",\"relocating_node\":null,\"shard\":10,\"index\":\"rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":120530273235},\"translog\":{\"id\":1423788057568,\"operations\":0},\"docs\":{\"num_docs\":233269761,\"max_doc\":233269761,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":5312,\"total_time_in_millis\":235638388,\"total_docs\":652598089,\"total_size_in_bytes\":370189806837},\"refresh\":{\"total\":11729,\"total_time_in_millis\":13221188},\"flush\":{\"total\":638,\"total_time_in_millis\":962220}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"4pLsTb6kQCur_Nw5jwDm2A\",\"relocating_node\":null,\"shard\":10,\"index\":\"rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":120252109328},\n\"translog\":{\"id\":1423788057563,\"operations\":0},\"docs\":{\"num_docs\":231241060,\"max_doc\":231241060,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":114,\"total_time_in_millis\":37640291,\"total_docs\":112742526,\"total_size_in_bytes\":63898359689},\"refresh\":{\"total\":103,\"total_time_in_millis\":4107},\"flush\":{\"total\":2,\"total_time_in_millis\":2755}}],\"11\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"-5VMHj1DQYqKR6vi018o5w\",\"relocating_node\":null,\"shard\":11,\"index\":\"rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":120860927845},\"translog\":{\"id\":1423788057595,\"operations\":0},\"docs\":{\"num_docs\":232572654,\"max_doc\":232572654,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":131,\"total_time_in_millis\":28772607,\"total_docs\":142263917,\"total_size_in_bytes\":79864379208},\"refresh\":{\"total\":116,\"total_time_in_millis\":5443},\"flush\":{\"total\":7,\"total_time_in_millis\":7142}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"u0UGAe5cR8iqrfQobCI5_w\",\"relocating_node\":null,\"shard\":11,\"index\":\"rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":119951332092},\"translog\":{\"id\":1423788057589,\"operations\":0},\"docs\":{\"num_docs\":232572654,\"max_doc\":232572654,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":5324,\"total_time_in_millis\":244203931,\"total_docs\":648217383,\"total_size_in_bytes\":368101366438},\"refresh\":{\"total\":11725,\"total_time_in_millis\":15205263},\"flush\":{\"total\":631,\"total_time_in_millis\":959013}}],\"12\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"Mg5HpexgTuCOtCs218D1GA\",\"relocating_node\":null,\"shard\":12,\"index\":\"rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":132400293240},\"translog\":{\"id\":1423788057617,\"operations\":0},\"docs\":{\"num_docs\":258997139,\"max_doc\":258997139,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":15198,\"total_time_in_millis\":114557615,\"total_docs\":964137384,\"total_size_in_bytes\":561773529482},\"refresh\":{\"total\":13819,\"total_time_in_millis\":3190740},\"flush\":{\"total\":713,\"total_time_in_millis\":1015968}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"te0M1g2yQhOAkWg9UHDMrw\",\"relocating_node\":null,\"shard\":12,\"index\":\"rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":134263783759},\"translog\":{\"id\":1423788057616,\"operations\":0},\"docs\":{\"num_docs\":258997139,\"max_doc\":258997139,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":5739,\"total_time_in_millis\":262361337,\"total_docs\":775380659,\"total_size_in_bytes\":440427375426},\"refresh\":{\"total\":13324,\"total_time_in_millis\":6554593},\"flush\":{\"total\":712,\"total_time_in_millis\":1149762}}],\"13\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"9yna4MXDQZCBN_XpvTKd3Q\",\"relocating_node\":null,\"shard\":13,\"index\":\"rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":191876157990},\"translog\":{\"id\":1423788057669,\"operations\":0},\"docs\":{\"num_docs\":263961929,\"max_doc\":263961929,\"deleted_docs\":0},\"merges\":{\"current\":4,\"current_docs\":37477443,\"current_size_in_bytes\":18822763565,\"total\":5651,\"total_time_in_millis\":271660839,\"total_docs\":739449149,\"total_size_in_bytes\":422750244706},\"refresh\":{\"total\":13114,\"total_time_in_millis\":7883023},\"flush\":{\"total\":725,\"total_time_in_millis\":1161198}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"RK84sm3-T-mO6xUfE5mq_Q\",\"relocating_node\":null,\"shard\":13,\"index\":\"rsyslog-2015-02-13\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":135043405514},\"translog\":{\"id\":1423788057669,\"operations\":0},\"docs\":{\"num_docs\":263961929,\"max_doc\":263961929,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":17770,\"total_time_in_millis\":112203139,\"total_docs\":981848861,\"total_size_in_bytes\":575005443781},\"refresh\":{\"total\":13946,\"total_time_in_millis\":2918402},\"flush\":{\"total\":725,\"total_time_in_millis\":1153756}}]}},\"kibana-int\":{\"index\":{\"primary_size_in_bytes\":245234,\"size_in_bytes\":490468},\"translog\":{\"operations\":0},\"docs\":{\"num_\ndocs\":22,\"max_doc\":22,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":10,\"total_time_in_millis\":235},\"flush\":{\"total\":11,\"total_time_in_millis\":652},\"shards\":{\"0\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"YU6Kl1IzTGeEPL4K875vFg\",\"relocating_node\":null,\"shard\":0,\"index\":\"kibana-int\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":10855},\"translog\":{\"id\":1419924505987,\"operations\":0},\"docs\":{\"num_docs\":1,\"max_doc\":1,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"4pLsTb6kQCur_Nw5jwDm2A\",\"relocating_node\":null,\"shard\":0,\"index\":\"kibana-int\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":10855},\"translog\":{\"id\":1419924505987,\"operations\":0},\"docs\":{\"num_docs\":1,\"max_doc\":1,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}}],\"1\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"IW9Mab9kTNuH16VCZaOBIg\",\"relocating_node\":null,\"shard\":1,\"index\":\"kibana-int\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":24745},\"translog\":{\"id\":1419924505994,\"operations\":0},\"docs\":{\"num_docs\":2,\"max_doc\":2,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"zcjnLJwYTI2nBn90i470OA\",\"relocating_node\":null,\"shard\":1,\"index\":\"kibana-int\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":24745},\"translog\":{\"id\":1419924505994,\"operations\":0},\"docs\":{\"num_docs\":2,\"max_doc\":2,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":5,\"total_time_in_millis\":23},\"flush\":{\"total\":2,\"total_time_in_millis\":127}}],\"2\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"2tyDIGR0Ts2AQLMCBnqS1g\",\"relocating_node\":null,\"shard\":2,\"index\":\"kibana-int\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":32463},\"translog\":{\"id\":1419924505978,\"operations\":0},\"docs\":{\"num_docs\":3,\"max_doc\":3,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"RK84sm3-T-mO6xUfE5mq_Q\",\"relocating_node\":null,\"shard\":2,\"index\":\"kibana-int\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":32463},\"translog\":{\"id\":1419924505978,\"operations\":0},\"docs\":{\"num_docs\":3,\"max_doc\":3,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}}],\"3\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"cTWT_a2hRvaHHdtugjXxMw\",\"relocating_node\":null,\"shard\":3,\"index\":\"kibana-int\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":24963},\"translog\":{\"id\":1419924505993,\"operations\":0},\"docs\":{\"num_docs\":2,\"max_doc\":2,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":2,\"total_time_in_millis\":197},\"flush\":{\"total\":3,\"total_time_in_millis\":270}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"te0M1g2yQhOAkWg9UHDMrw\",\"relocating_node\":null,\"shard\":3,\"index\":\"kibana-int\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":24963},\"\ntranslog\":{\"id\":1419924505993,\"operations\":0},\"docs\":{\"num_docs\":2,\"max_doc\":2,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":2,\"total_time_in_millis\":10},\"flush\":{\"total\":2,\"total_time_in_millis\":33}}],\"4\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"-5VMHj1DQYqKR6vi018o5w\",\"relocating_node\":null,\"shard\":4,\"index\":\"kibana-int\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":21451},\"translog\":{\"id\":1419924505990,\"operations\":0},\"docs\":{\"num_docs\":2,\"max_doc\":2,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"u0UGAe5cR8iqrfQobCI5_w\",\"relocating_node\":null,\"shard\":4,\"index\":\"kibana-int\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":21451},\"translog\":{\"id\":1419924505990,\"operations\":0},\"docs\":{\"num_docs\":2,\"max_doc\":2,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}}],\"5\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"Pp9D83T_SDq73IfCI93JFA\",\"relocating_node\":null,\"shard\":5,\"index\":\"kibana-int\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":24756},\"translog\":{\"id\":1419924505985,\"operations\":0},\"docs\":{\"num_docs\":2,\"max_doc\":2,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":1,\"total_time_in_millis\":51}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"LGUsqbQXRei4YLyIjf7aow\",\"relocating_node\":null,\"shard\":5,\"index\":\"kibana-int\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":24756},\"translog\":{\"id\":1419924505985,\"operations\":0},\"docs\":{\"num_docs\":2,\"max_doc\":2,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":1,\"total_time_in_millis\":73}}],\"6\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"9yna4MXDQZCBN_XpvTKd3Q\",\"relocating_node\":null,\"shard\":6,\"index\":\"kibana-int\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":13743},\"translog\":{\"id\":1419924505981,\"operations\":0},\"docs\":{\"num_docs\":2,\"max_doc\":2,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"yVMmqz7RR8mr4GSwkkX-4A\",\"relocating_node\":null,\"shard\":6,\"index\":\"kibana-int\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":13743},\"translog\":{\"id\":1419924505981,\"operations\":0},\"docs\":{\"num_docs\":2,\"max_doc\":2,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}}],\"7\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"Mg5HpexgTuCOtCs218D1GA\",\"relocating_node\":null,\"shard\":7,\"index\":\"kibana-int\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":10989},\"translog\":{\"id\":1419924505992,\"operations\":0},\"docs\":{\"num_docs\":1,\"max_doc\":1,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"qXwalJKMSk6W0JaibIpTQg\",\"relocating_node\":null,\"shard\":7,\"index\":\"kibana-int\"},\"state\":\"\nSTARTED\",\"index\":{\"size_in_bytes\":10989},\"translog\":{\"id\":1419924505992,\"operations\":0},\"docs\":{\"num_docs\":1,\"max_doc\":1,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}}],\"8\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"YU6Kl1IzTGeEPL4K875vFg\",\"relocating_node\":null,\"shard\":8,\"index\":\"kibana-int\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":12204},\"translog\":{\"id\":1419924505981,\"operations\":0},\"docs\":{\"num_docs\":1,\"max_doc\":1,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"gNLthbzQSW-h_v2oGvM4Mw\",\"relocating_node\":null,\"shard\":8,\"index\":\"kibana-int\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":12204},\"translog\":{\"id\":1419924505981,\"operations\":0},\"docs\":{\"num_docs\":1,\"max_doc\":1,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}}],\"9\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"RJwWye67QkWKKjl_b3NZ-A\",\"relocating_node\":null,\"shard\":9,\"index\":\"kibana-int\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":10524},\"translog\":{\"id\":1419924505980,\"operations\":0},\"docs\":{\"num_docs\":1,\"max_doc\":1,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"hsLYPGy2TyC-Z6gsJ_YMqQ\",\"relocating_node\":null,\"shard\":9,\"index\":\"kibana-int\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":10524},\"translog\":{\"id\":1419924505980,\"operations\":0},\"docs\":{\"num_docs\":1,\"max_doc\":1,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}}],\"10\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"UT-z9BwoRYGHDO65Xe77lA\",\"relocating_node\":null,\"shard\":10,\"index\":\"kibana-int\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":21365},\"translog\":{\"id\":1419924505995,\"operations\":0},\"docs\":{\"num_docs\":2,\"max_doc\":2,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"ix5R9EwPTEWLzWL2ZyiUOA\",\"relocating_node\":null,\"shard\":10,\"index\":\"kibana-int\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":21365},\"translog\":{\"id\":1419924505995,\"operations\":0},\"docs\":{\"num_docs\":2,\"max_doc\":2,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}}],\"11\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"MimlEBKgQH2_oc3JxqxpXA\",\"relocating_node\":null,\"shard\":11,\"index\":\"kibana-int\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":12177},\"translog\":{\"id\":1419924505986,\"operations\":0},\"docs\":{\"num_docs\":1,\"max_doc\":1,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"qsymmz4LTeKkq_7AuKqG4g\",\"relocating_node\":null,\n\"shard\":11,\"index\":\"kibana-int\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":12177},\"translog\":{\"id\":1419924505986,\"operations\":0},\"docs\":{\"num_docs\":1,\"max_doc\":1,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}}],\"12\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"4hA88zpfSX2WRoc_-wWxQ\",\"relocating_node\":null,\"shard\":12,\"index\":\"kibana-int\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":24884},\"translog\":{\"id\":1419924505984,\"operations\":0},\"docs\":{\"num_docs\":2,\"max_doc\":2,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":1,\"total_time_in_millis\":5},\"flush\":{\"total\":1,\"total_time_in_millis\":75}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"aTLkCoPHScqMWT3zoEGTnw\",\"relocating_node\":null,\"shard\":12,\"index\":\"kibana-int\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":24884},\"translog\":{\"id\":1419924505984,\"operations\":0},\"docs\":{\"num_docs\":2,\"max_doc\":2,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":1,\"total_time_in_millis\":23}}],\"13\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"ob27c5P3TbWWv-2xF1KTWQ\",\"relocating_node\":null,\"shard\":13,\"index\":\"kibana-int\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":115},\"translog\":{\"id\":1419924505981,\"operations\":0},\"docs\":{\"num_docs\":0,\"max_doc\":0,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"c5cY7a7EQrK1rPMFWheBtg\",\"relocating_node\":null,\"shard\":13,\"index\":\"kibana-int\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":115},\"translog\":{\"id\":1419924505981,\"operations\":0},\"docs\":{\"num_docs\":0,\"max_doc\":0,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}}]}},\"rsyslog-2015-02-12\":{\"index\":{\"primary_size_in_bytes\":2065200781627,\"size_in_bytes\":4154834378965},\"translog\":{\"operations\":0},\"docs\":{\"num_docs\":3943145481,\"max_doc\":3943145481,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":251189,\"total_time_in_millis\":5672487004,\"total_docs\":21522481472,\"total_size_in_bytes\":12704467536556},\"refresh\":{\"total\":369916,\"total_time_in_millis\":141736202},\"flush\":{\"total\":18091,\"total_time_in_millis\":30251664},\"shards\":{\"0\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"9yna4MXDQZCBN_XpvTKd3Q\",\"relocating_node\":null,\"shard\":0,\"index\":\"rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":148295287779},\"translog\":{\"id\":1423688401408,\"operations\":0},\"docs\":{\"num_docs\":281392817,\"max_doc\":281392817,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"hsLYPGy2TyC-Z6gsJ_YMqQ\",\"relocating_node\":null,\"shard\":0,\"index\":\"rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":148295287779},\"translog\":{\"id\":1423688401408,\"operations\":0},\"docs\":{\"num_docs\":281392817,\"max_doc\":281392817,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":13856,\"total_time_in_millis\":140543176,\"total_docs\":1001628114,\"total_size_in_bytes\":597475973415},\"refresh\":{\"total\":15953,\"total_time_in_millis\":6816622},\"flush\":{\"total\":\n789,\"total_time_in_millis\":1652162}}],\"1\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"zcjnLJwYTI2nBn90i470OA\",\"relocating_node\":null,\"shard\":1,\"index\":\"rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":149753121462},\"translog\":{\"id\":1423688401306,\"operations\":0},\"docs\":{\"num_docs\":281558572,\"max_doc\":281558572,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":5033,\"total_time_in_millis\":368963655,\"total_docs\":829394130,\"total_size_in_bytes\":482462326239},\"refresh\":{\"total\":15912,\"total_time_in_millis\":8420255},\"flush\":{\"total\":789,\"total_time_in_millis\":1227208}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"UT-z9BwoRYGHDO65Xe77lA\",\"relocating_node\":null,\"shard\":1,\"index\":\"rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":146930857039},\"translog\":{\"id\":1423688401306,\"operations\":0},\"docs\":{\"num_docs\":281558572,\"max_doc\":281558572,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":16465,\"total_time_in_millis\":121845938,\"total_docs\":1036198008,\"total_size_in_bytes\":618744143712},\"refresh\":{\"total\":16614,\"total_time_in_millis\":3518730},\"flush\":{\"total\":789,\"total_time_in_millis\":1244339}}],\"2\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"cTWT_a2hRvaHHdtugjXxMw\",\"relocating_node\":null,\"shard\":2,\"index\":\"rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":149738557124},\"translog\":{\"id\":1423688401320,\"operations\":0},\"docs\":{\"num_docs\":281555854,\"max_doc\":281555854,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":5012,\"total_time_in_millis\":358993842,\"total_docs\":838550340,\"total_size_in_bytes\":487293279198},\"refresh\":{\"total\":15941,\"total_time_in_millis\":8197923},\"flush\":{\"total\":789,\"total_time_in_millis\":1145791}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"yVMmqz7RR8mr4GSwkkX-4A\",\"relocating_node\":null,\"shard\":2,\"index\":\"rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":146841822234},\"translog\":{\"id\":1423688401318,\"operations\":0},\"docs\":{\"num_docs\":281555854,\"max_doc\":281555854,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":16482,\"total_time_in_millis\":121479757,\"total_docs\":1041930763,\"total_size_in_bytes\":621851332178},\"refresh\":{\"total\":16642,\"total_time_in_millis\":3391988},\"flush\":{\"total\":787,\"total_time_in_millis\":1141920}}],\"3\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"4hA88zpfSX2WRoc-wWxQ\",\"relocating_node\":null,\"shard\":3,\"index\":\"rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":149826932639},\"translog\":{\"id\":1423688401341,\"operations\":0},\"docs\":{\"num_docs\":281670905,\"max_doc\":281670905,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":5091,\"total_time_in_millis\":400540687,\"total_docs\":831434927,\"total_size_in_bytes\":483213788197},\"refresh\":{\"total\":15920,\"total_time_in_millis\":8430877},\"flush\":{\"total\":790,\"total_time_in_millis\":1496404}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"LGUsqbQXRei4YLyIjf7aow\",\"relocating_node\":null,\"shard\":3,\"index\":\"rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":146866256770},\"translog\":{\"id\":1423688401339,\"operations\":0},\"docs\":{\"num_docs\":281670905,\"max_doc\":281670905,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":16857,\"total_time_in_millis\":121823919,\"total_docs\":1038933277,\"total_size_in_bytes\":620665714513},\"refresh\":{\"total\":16638,\"total_time_in_millis\":3418300},\"flush\":{\"total\":788,\"total_time_in_millis\":1287761}}],\"4\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"te0M1g2yQhOAkWg9UHDMrw\",\"relocating_node\":null,\"shard\":4,\"index\":\"rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":149862811949},\"translog\":{\"id\":1423688401315,\"operations\":0},\"docs\":{\"num_docs\":281768043,\"max_doc\":281768043,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"\ntotal_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"4hA88zpfSX2WRoc-wWxQ\",\"relocating_node\":null,\"shard\":4,\"index\":\"rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":149862811949},\"translog\":{\"id\":1423688401315,\"operations\":0},\"docs\":{\"num_docs\":281768043,\"max_doc\":281768043,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":5118,\"total_time_in_millis\":402978236,\"total_docs\":833810927,\"total_size_in_bytes\":484506494674},\"refresh\":{\"total\":15909,\"total_time_in_millis\":8435282},\"flush\":{\"total\":789,\"total_time_in_millis\":1489863}}],\"5\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"YU6Kl1IzTGeEPL4K875vFg\",\"relocating_node\":null,\"shard\":5,\"index\":\"rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":150017604792},\"translog\":{\"id\":1423688401319,\"operations\":0},\"docs\":{\"num_docs\":281753858,\"max_doc\":281753858,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":5394,\"total_time_in_millis\":360908309,\"total_docs\":831103909,\"total_size_in_bytes\":483448901123},\"refresh\":{\"total\":15822,\"total_time_in_millis\":8898954},\"flush\":{\"total\":789,\"total_time_in_millis\":1560430}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"aTLkCoPHScqMWT3zoEGTnw\",\"relocating_node\":null,\"shard\":5,\"index\":\"rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":146917322390},\"translog\":{\"id\":1423688401319,\"operations\":0},\"docs\":{\"num_docs\":281753858,\"max_doc\":281753858,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":16647,\"total_time_in_millis\":120509060,\"total_docs\":1041489388,\"total_size_in_bytes\":622240808520},\"refresh\":{\"total\":16650,\"total_time_in_millis\":3356040},\"flush\":{\"total\":789,\"total_time_in_millis\":1119864}}],\"6\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"_virthvxT4idC9C3NIMwTg\",\"relocating_node\":null,\"shard\":6,\"index\":\"rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":149798130327},\"translog\":{\"id\":1423688401312,\"operations\":0},\"docs\":{\"num_docs\":281696792,\"max_doc\":281696792,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"u0UGAe5cR8iqrfQobCI5_w\",\"relocating_node\":null,\"shard\":6,\"index\":\"rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":149798130327},\"translog\":{\"id\":1423688401312,\"operations\":0},\"docs\":{\"num_docs\":281696792,\"max_doc\":281696792,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":5068,\"total_time_in_millis\":370490508,\"total_docs\":833925577,\"total_size_in_bytes\":484523879311},\"refresh\":{\"total\":15883,\"total_time_in_millis\":8477342},\"flush\":{\"total\":788,\"total_time_in_millis\":1281933}}],\"7\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"2tyDIGR0Ts2AQLMCBnqS1g\",\"relocating_node\":null,\"shard\":7,\"index\":\"rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":146967474838},\"translog\":{\"id\":1423688401311,\"operations\":0},\"docs\":{\"num_docs\":281793181,\"max_doc\":281793181,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"4pLsTb6kQCur_Nw5jwDm2A\",\"relocating_node\":null,\"shard\":7,\"index\":\"rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":146967474838},\"translog\":{\"id\":1423688401311,\"operations\":0},\"docs\":{\"num_docs\":281793181,\"max_doc\":281793181,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":15410,\"total_time_in_millis\":117227265,\"total_docs\":989030672,\"total_size_in_bytes\":588127308795},\"refresh\":{\"total\":13605,\"total_time_in_millis\":3247967},\"flush\"\n:{\"total\":743,\"total_time_in_millis\":1098807}}],\"8\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"gNLthbzQSW-h_v2oGvM4Mw\",\"relocating_node\":null,\"shard\":8,\"index\":\"rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":147763845489},\"translog\":{\"id\":1423688401320,\"operations\":0},\"docs\":{\"num_docs\":281448839,\"max_doc\":281448839,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":14546,\"total_time_in_millis\":134226215,\"total_docs\":1012401819,\"total_size_in_bytes\":604638062039},\"refresh\":{\"total\":15996,\"total_time_in_millis\":6607827},\"flush\":{\"total\":786,\"total_time_in_millis\":1375033}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"ix5R9EwPTEWLzWL2ZyiUOA\",\"relocating_node\":null,\"shard\":8,\"index\":\"rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":149666324999},\"translog\":{\"id\":1423688401323,\"operations\":0},\"docs\":{\"num_docs\":281448839,\"max_doc\":281448839,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":5271,\"total_time_in_millis\":396277950,\"total_docs\":829286956,\"total_size_in_bytes\":482795126170},\"refresh\":{\"total\":15955,\"total_time_in_millis\":8246267},\"flush\":{\"total\":789,\"total_time_in_millis\":1421730}}],\"9\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"Mg5HpexgTuCOtCs218D1GA\",\"relocating_node\":null,\"shard\":9,\"index\":\"rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":149715813352},\"translog\":{\"id\":1423688401349,\"operations\":0},\"docs\":{\"num_docs\":281683604,\"max_doc\":281683604,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":4957,\"total_time_in_millis\":361928240,\"total_docs\":832780357,\"total_size_in_bytes\":484614973758},\"refresh\":{\"total\":15819,\"total_time_in_millis\":8827280},\"flush\":{\"total\":790,\"total_time_in_millis\":1213149}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"qXwalJKMSk6W0JaibIpTQg\",\"relocating_node\":null,\"shard\":9,\"index\":\"rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":146826062213},\"translog\":{\"id\":1423688401347,\"operations\":0},\"docs\":{\"num_docs\":281683604,\"max_doc\":281683604,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":16858,\"total_time_in_millis\":120357639,\"total_docs\":1041595321,\"total_size_in_bytes\":622234960712},\"refresh\":{\"total\":16634,\"total_time_in_millis\":3415886},\"flush\":{\"total\":788,\"total_time_in_millis\":1254885}}],\"10\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"RJwWye67QkWKKjl_b3NZ-A\",\"relocating_node\":null,\"shard\":10,\"index\":\"rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":147083951553},\"translog\":{\"id\":1423688401333,\"operations\":0},\"docs\":{\"num_docs\":281630153,\"max_doc\":281630153,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"qsymmz4LTeKkq_7AuKqG4g\",\"relocating_node\":null,\"shard\":10,\"index\":\"rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":147083951553},\"translog\":{\"id\":1423688401333,\"operations\":0},\"docs\":{\"num_docs\":281630153,\"max_doc\":281630153,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":17342,\"total_time_in_millis\":123387295,\"total_docs\":1040300197,\"total_size_in_bytes\":621615279742},\"refresh\":{\"total\":16595,\"total_time_in_millis\":3619388},\"flush\":{\"total\":788,\"total_time_in_millis\":1387995}}],\"11\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"RK84sm3-T-mO6xUfE5mq_Q\",\"relocating_node\":null,\"shard\":11,\"index\":\"rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":147239997745},\"translog\":{\"id\":1423688401340,\"operations\":0},\"docs\":{\"num_docs\":281719036,\"max_doc\":281719036,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":16950,\"total_time_in_millis\":122677072,\"total_docs\":1027929783,\"total_size_in_bytes\":614089756602},\"refresh\":{\"total\":\n16592,\"total_time_in_millis\":3619242},\"flush\":{\"total\":789,\"total_time_in_millis\":1454702}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"MimlEBKgQH2_oc3JxqxpXA\",\"relocating_node\":null,\"shard\":11,\"index\":\"rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":149763696176},\"translog\":{\"id\":1423688401339,\"operations\":0},\"docs\":{\"num_docs\":281719036,\"max_doc\":281719036,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":5069,\"total_time_in_millis\":374024125,\"total_docs\":838767676,\"total_size_in_bytes\":487009328045},\"refresh\":{\"total\":15826,\"total_time_in_millis\":8797045},\"flush\":{\"total\":788,\"total_time_in_millis\":1243274}}],\"12\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"c5cY7a7EQrK1rPMFWheBtg\",\"relocating_node\":null,\"shard\":12,\"index\":\"rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":146732913984},\"translog\":{\"id\":1423688401302,\"operations\":0},\"docs\":{\"num_docs\":281722226,\"max_doc\":281722226,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":16589,\"total_time_in_millis\":121122940,\"total_docs\":1039919384,\"total_size_in_bytes\":620810664802},\"refresh\":{\"total\":16624,\"total_time_in_millis\":3465759},\"flush\":{\"total\":788,\"total_time_in_millis\":1330665}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"Pp9D83T_SDq73IfCI93JFA\",\"relocating_node\":null,\"shard\":12,\"index\":\"rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":149437115212},\"translog\":{\"id\":1423688401303,\"operations\":0},\"docs\":{\"num_docs\":281722226,\"max_doc\":281722226,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":5231,\"total_time_in_millis\":394742778,\"total_docs\":835069799,\"total_size_in_bytes\":485105191462},\"refresh\":{\"total\":15922,\"total_time_in_millis\":8214735},\"flush\":{\"total\":789,\"total_time_in_millis\":1342981}}],\"13\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"ob27c5P3TbWWv-2xF1KTWQ\",\"relocating_node\":null,\"shard\":13,\"index\":\"rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":149706775136},\"translog\":{\"id\":1423688401344,\"operations\":0},\"docs\":{\"num_docs\":281751601,\"max_doc\":281751601,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":5427,\"total_time_in_millis\":394282412,\"total_docs\":841407833,\"total_size_in_bytes\":488916747857},\"refresh\":{\"total\":15857,\"total_time_in_millis\":8746542},\"flush\":{\"total\":789,\"total_time_in_millis\":1336550}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"-5VMHj1DQYqKR6vi018o5w\",\"relocating_node\":null,\"shard\":13,\"index\":\"rsyslog-2015-02-12\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":147074047317},\"translog\":{\"id\":1423688401343,\"operations\":0},\"docs\":{\"num_docs\":281751601,\"max_doc\":281751601,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":16516,\"total_time_in_millis\":123155986,\"total_docs\":1035592315,\"total_size_in_bytes\":618083495492},\"refresh\":{\"total\":16607,\"total_time_in_millis\":3565951},\"flush\":{\"total\":788,\"total_time_in_millis\":1144218}}]}},\"rsyslog-2015-02-11\":{\"index\":{\"primary_size_in_bytes\":2183404634683,\"size_in_bytes\":4380267322779},\"translog\":{\"operations\":0},\"docs\":{\"num_docs\":3895607976,\"max_doc\":3895607976,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":237165,\"total_time_in_millis\":5185744957,\"total_docs\":19384299302,\"total_size_in_bytes\":12131092654821},\"refresh\":{\"total\":338355,\"total_time_in_millis\":134604424},\"flush\":{\"total\":17602,\"total_time_in_millis\":30530189},\"shards\":{\"0\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"cTWT_a2hRvaHHdtugjXxMw\",\"relocating_node\":null,\"shard\":0,\"index\":\"rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":157370751497},\"translog\":{\"id\":1423602001357,\"operations\":0},\"docs\":{\"num_docs\":278156827,\"max_doc\":278156827,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":6677,\"total_time_in_millis\":395753643,\"total_docs\":838568430,\"total_size_in_bytes\":518643030535},\"refresh\":{\"total\":15954,\"\ntotal_time_in_millis\":7889533},\"flush\":{\"total\":839,\"total_time_in_millis\":1235788}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"hsLYPGy2TyC-Z6gsJ_YMqQ\",\"relocating_node\":null,\"shard\":0,\"index\":\"rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":156764066216},\"translog\":{\"id\":1423602001355,\"operations\":0},\"docs\":{\"num_docs\":278156827,\"max_doc\":278156827,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":13136,\"total_time_in_millis\":150410761,\"total_docs\":961412700,\"total_size_in_bytes\":606928637236},\"refresh\":{\"total\":15848,\"total_time_in_millis\":7219471},\"flush\":{\"total\":837,\"total_time_in_millis\":1805710}}],\"1\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"RJwWye67QkWKKjl_b3NZ-A\",\"relocating_node\":null,\"shard\":1,\"index\":\"rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":158834125176},\"translog\":{\"id\":1423602001371,\"operations\":0},\"docs\":{\"num_docs\":278318599,\"max_doc\":278318599,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":6879,\"total_time_in_millis\":418605446,\"total_docs\":834988170,\"total_size_in_bytes\":518936657416},\"refresh\":{\"total\":14961,\"total_time_in_millis\":12884427},\"flush\":{\"total\":837,\"total_time_in_millis\":1984002}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"IW9Mab9kTNuH16VCZaOBIg\",\"relocating_node\":null,\"shard\":1,\"index\":\"rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":158834125176},\"translog\":{\"id\":1423602001371,\"operations\":0},\"docs\":{\"num_docs\":278318599,\"max_doc\":278318599,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}}],\"2\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"ob27c5P3TbWWv-2xF1KTWQ\",\"relocating_node\":null,\"shard\":2,\"index\":\"rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":155614622534},\"translog\":{\"id\":1423602001335,\"operations\":0},\"docs\":{\"num_docs\":278254858,\"max_doc\":278254858,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"UT-z9BwoRYGHDO65Xe77lA\",\"relocating_node\":null,\"shard\":2,\"index\":\"rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":155614622534},\"translog\":{\"id\":1423602001335,\"operations\":0},\"docs\":{\"num_docs\":278254858,\"max_doc\":278254858,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":14818,\"total_time_in_millis\":135799719,\"total_docs\":986374330,\"total_size_in_bytes\":622030990404},\"refresh\":{\"total\":16415,\"total_time_in_millis\":4378528},\"flush\":{\"total\":839,\"total_time_in_millis\":1525151}}],\"3\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"gNLthbzQSW-h_v2oGvM4Mw\",\"relocating_node\":null,\"shard\":3,\"index\":\"rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":156533149111},\"translog\":{\"id\":1423602001348,\"operations\":0},\"docs\":{\"num_docs\":278138190,\"max_doc\":278138190,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":13494,\"total_time_in_millis\":149959158,\"total_docs\":968816310,\"total_size_in_bytes\":611413897467},\"refresh\":{\"total\":15802,\"total_time_in_millis\":7458812},\"flush\":{\"total\":836,\"total_time_in_millis\":1490768}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"Pp9D83T_SDq73IfCI93JFA\",\"relocating_node\":null,\"shard\":3,\"index\":\"rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":157383342166},\"translog\":{\"id\":1423602001348,\"operations\":0},\"docs\":{\"num_docs\":278138190,\"max_doc\":278138190,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":6537,\"total_time_in_millis\":390248184,\"total_docs\":835208982,\"total_size_in_bytes\":516234071033},\"refresh\":{\"total\":15975,\"total_time_in_millis\":\n7795406},\"flush\":{\"total\":836,\"total_time_in_millis\":1299992}}],\"4\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"4hA88zpfSX2WRoc-_wWxQ\",\"relocating_node\":null,\"shard\":4,\"index\":\"rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":157549491845},\"translog\":{\"id\":1423602001363,\"operations\":0},\"docs\":{\"num_docs\":278381700,\"max_doc\":278381700,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":6753,\"total_time_in_millis\":389828191,\"total_docs\":840405453,\"total_size_in_bytes\":519608969859},\"refresh\":{\"total\":15931,\"total_time_in_millis\":7933657},\"flush\":{\"total\":838,\"total_time_in_millis\":1587832}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"yVMmqz7RR8mr4GSwkkX-4A\",\"relocating_node\":null,\"shard\":4,\"index\":\"rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":155052977484},\"translog\":{\"id\":1423602001364,\"operations\":0},\"docs\":{\"num_docs\":278381700,\"max_doc\":278381700,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":15228,\"total_time_in_millis\":130752052,\"total_docs\":988109236,\"total_size_in_bytes\":623550820848},\"refresh\":{\"total\":16486,\"total_time_in_millis\":4026176},\"flush\":{\"total\":839,\"total_time_in_millis\":1263888}}],\"5\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"9yna4MXDQZCBN_XpvTKd3Q\",\"relocating_node\":null,\"shard\":5,\"index\":\"rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":155545147558},\"translog\":{\"id\":1423602001354,\"operations\":0},\"docs\":{\"num_docs\":278158351,\"max_doc\":278158351,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"4pLsTb6kQCur_Nw5jwDm2A\",\"relocating_node\":null,\"shard\":5,\"index\":\"rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":155545147558},\"translog\":{\"id\":1423602001354,\"operations\":0},\"docs\":{\"num_docs\":278158351,\"max_doc\":278158351,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}}],\"6\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"2tyDIGR0Ts2AQLMCBnqS1g\",\"relocating_node\":null,\"shard\":6,\"index\":\"rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":155612953101},\"translog\":{\"id\":1423602001359,\"operations\":0},\"docs\":{\"num_docs\":278200150,\"max_doc\":278200150,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"te0M1g2yQhOAkWg9UHDMrw\",\"relocating_node\":null,\"shard\":6,\"index\":\"rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":155612953101},\"translog\":{\"id\":1423602001359,\"operations\":0},\"docs\":{\"num_docs\":278200150,\"max_doc\":278200150,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":15381,\"total_time_in_millis\":134300821,\"total_docs\":984410078,\"total_size_in_bytes\":621364681467},\"refresh\":{\"total\":16448,\"total_time_in_millis\":4223187},\"flush\":{\"total\":838,\"total_time_in_millis\":1383818}}],\"7\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"Mg5HpexgTuCOtCs218D1GA\",\"relocating_node\":null,\"shard\":7,\"index\":\"rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":155785818279},\"translog\":{\"id\":1423602001514,\"operations\":0},\"docs\":{\"num_docs\":278263200,\"max_doc\":278263200,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\"\n:\"qXwalJKMSk6W0JaibIpTQg\",\"relocating_node\":null,\"shard\":7,\"index\":\"rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":155785818279},\"translog\":{\"id\":1423602001514,\"operations\":0},\"docs\":{\"num_docs\":278263200,\"max_doc\":278263200,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":14528,\"total_time_in_millis\":137021665,\"total_docs\":995821760,\"total_size_in_bytes\":626508443453},\"refresh\":{\"total\":16423,\"total_time_in_millis\":4348660},\"flush\":{\"total\":838,\"total_time_in_millis\":1288616}}],\"8\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"zcjnLJwYTI2nBn90i470OA\",\"relocating_node\":null,\"shard\":8,\"index\":\"rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":155754690353},\"translog\":{\"id\":1423602001349,\"operations\":0},\"docs\":{\"num_docs\":278215468,\"max_doc\":278215468,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":0,\"total_time_in_millis\":0,\"total_docs\":0,\"total_size_in_bytes\":0},\"refresh\":{\"total\":0,\"total_time_in_millis\":0},\"flush\":{\"total\":0,\"total_time_in_millis\":0}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"aTLkCoPHScqMWT3zoEGTnw\",\"relocating_node\":null,\"shard\":8,\"index\":\"rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":155754690353},\"translog\":{\"id\":1423602001349,\"operations\":0},\"docs\":{\"num_docs\":278215468,\"max_doc\":278215468,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":14434,\"total_time_in_millis\":135977623,\"total_docs\":996262054,\"total_size_in_bytes\":626891508704},\"refresh\":{\"total\":16450,\"total_time_in_millis\":4214682},\"flush\":{\"total\":840,\"total_time_in_millis\":1334175}}],\"9\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"RK84sm3-T-mO6xUfE5mq_Q\",\"relocating_node\":null,\"shard\":9,\"index\":\"rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":155535301256},\"translog\":{\"id\":1423602001343,\"operations\":0},\"docs\":{\"num_docs\":278218004,\"max_doc\":278218004,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":15337,\"total_time_in_millis\":135404773,\"total_docs\":990044105,\"total_size_in_bytes\":624503776714},\"refresh\":{\"total\":16446,\"total_time_in_millis\":4228976},\"flush\":{\"total\":838,\"total_time_in_millis\":1511246}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"YU6Kl1IzTGeEPL4K875vFg\",\"relocating_node\":null,\"shard\":9,\"index\":\"rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":157371959712},\"translog\":{\"id\":1423602001342,\"operations\":0},\"docs\":{\"num_docs\":278218004,\"max_doc\":278218004,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":6835,\"total_time_in_millis\":397114209,\"total_docs\":844851559,\"total_size_in_bytes\":522401387681},\"refresh\":{\"total\":15863,\"total_time_in_millis\":8364559},\"flush\":{\"total\":837,\"total_time_in_millis\":1485382}}],\"10\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"u0UGAe5cR8iqrfQobCI5_w\",\"relocating_node\":null,\"shard\":10,\"index\":\"rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":157268445696},\"translog\":{\"id\":1423602001354,\"operations\":0},\"docs\":{\"num_docs\":278264227,\"max_doc\":278264227,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":6836,\"total_time_in_millis\":394430345,\"total_docs\":843844953,\"total_size_in_bytes\":521840311983},\"refresh\":{\"total\":15920,\"total_time_in_millis\":8015531},\"flush\":{\"total\":840,\"total_time_in_millis\":1451751}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"c5cY7a7EQrK1rPMFWheBtg\",\"relocating_node\":null,\"shard\":10,\"index\":\"rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":155540901050},\"translog\":{\"id\":1423602001352,\"operations\":0},\"docs\":{\"num_docs\":278264227,\"max_doc\":278264227,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":14551,\"total_time_in_millis\":135355579,\"total_docs\":990913246,\"total_size_in_bytes\":623750483478},\"refresh\":{\"total\":16425,\"total_time_in_millis\":4338553},\"flush\":{\"total\":838,\"total_time_in_millis\":1551572}}],\"11\":[{\"routing\":{\"\nstate\":\"STARTED\",\"primary\":false,\"node\":\"Mg5HpexgTuCOtCs218D1GA\",\"relocating_node\":null,\"shard\":11,\"index\":\"rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":157721125538},\"translog\":{\"id\":1423602001349,\"operations\":0},\"docs\":{\"num_docs\":278302241,\"max_doc\":278302241,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":6706,\"total_time_in_millis\":381861479,\"total_docs\":841454730,\"total_size_in_bytes\":520618065845},\"refresh\":{\"total\":15866,\"total_time_in_millis\":8285733},\"flush\":{\"total\":839,\"total_time_in_millis\":1310478}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"qsymmz4LTeKkq_7AuKqG4g\",\"relocating_node\":null,\"shard\":11,\"index\":\"rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":155300492382},\"translog\":{\"id\":1423602001349,\"operations\":0},\"docs\":{\"num_docs\":278302241,\"max_doc\":278302241,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":15531,\"total_time_in_millis\":133760111,\"total_docs\":983971922,\"total_size_in_bytes\":620933439826},\"refresh\":{\"total\":16419,\"total_time_in_millis\":4362603},\"flush\":{\"total\":839,\"total_time_in_millis\":1549265}}],\"12\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"ix5R9EwPTEWLzWL2ZyiUOA\",\"relocating_node\":null,\"shard\":12,\"index\":\"rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":157843735419},\"translog\":{\"id\":1423602001350,\"operations\":0},\"docs\":{\"num_docs\":278362432,\"max_doc\":278362432,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":6624,\"total_time_in_millis\":387376728,\"total_docs\":835860219,\"total_size_in_bytes\":516731675793},\"refresh\":{\"total\":15958,\"total_time_in_millis\":7896135},\"flush\":{\"total\":839,\"total_time_in_millis\":1446263}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"LGUsqbQXRei4YLyIjf7aow\",\"relocating_node\":null,\"shard\":12,\"index\":\"rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":155755703298},\"translog\":{\"id\":1423602001349,\"operations\":0},\"docs\":{\"num_docs\":278362432,\"max_doc\":278362432,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":15146,\"total_time_in_millis\":134441951,\"total_docs\":986156808,\"total_size_in_bytes\":621313310801},\"refresh\":{\"total\":16441,\"total_time_in_millis\":4257349},\"flush\":{\"total\":838,\"total_time_in_millis\":1347278}}],\"13\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"-5VMHj1DQYqKR6vi018o5w\",\"relocating_node\":null,\"shard\":13,\"index\":\"rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":155774686885},\"translog\":{\"id\":1423602001378,\"operations\":0},\"docs\":{\"num_docs\":278373729,\"max_doc\":278373729,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":14999,\"total_time_in_millis\":135994982,\"total_docs\":987903741,\"total_size_in_bytes\":622438042807},\"refresh\":{\"total\":16441,\"total_time_in_millis\":4253479},\"flush\":{\"total\":838,\"total_time_in_millis\":1275292}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"MimlEBKgQH2_oc3JxqxpXA\",\"relocating_node\":null,\"shard\":13,\"index\":\"rsyslog-2015-02-11\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":157206479222},\"translog\":{\"id\":1423602001379,\"operations\":0},\"docs\":{\"num_docs\":278373729,\"max_doc\":278373729,\"deleted_docs\":0},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":6735,\"total_time_in_millis\":381347537,\"total_docs\":848920516,\"total_size_in_bytes\":524450451471},\"refresh\":{\"total\":15883,\"total_time_in_millis\":8228967},\"flush\":{\"total\":839,\"total_time_in_millis\":1401922}}]}},\"video\":{\"index\":{\"primary_size_in_bytes\":33363220902,\"size_in_bytes\":66726658683},\"translog\":{\"operations\":133},\"docs\":{\"num_docs\":16077909,\"max_doc\":17385654,\"deleted_docs\":1307745},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":6007,\"total_time_in_millis\":1651831,\"total_docs\":2149912,\"total_size_in_bytes\":8319976905},\"refresh\":{\"total\":58383,\"total_time_in_millis\":511033},\"flush\":{\"total\":2501,\"total_time_in_millis\":169922},\"shards\":{\"0\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"\n9yna4MXDQZCBN_XpvTKd3Q\",\"relocating_node\":null,\"shard\":0,\"index\":\"video\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":6895804857},\"translog\":{\"id\":1376557600580,\"operations\":11},\"docs\":{\"num_docs\":3222009,\"max_doc\":3558061,\"deleted_docs\":336052},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":695,\"total_time_in_millis\":207095,\"total_docs\":219372,\"total_size_in_bytes\":801047747},\"refresh\":{\"total\":6778,\"total_time_in_millis\":50857},\"flush\":{\"total\":288,\"total_time_in_millis\":17374}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"gNLthbzQSW-h_v2oGvM4Mw\",\"relocating_node\":null,\"shard\":0,\"index\":\"video\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":6896534950},\"translog\":{\"id\":1376557600579,\"operations\":24},\"docs\":{\"num_docs\":3222009,\"max_doc\":3558060,\"deleted_docs\":336051},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":499,\"total_time_in_millis\":108746,\"total_docs\":150939,\"total_size_in_bytes\":572550737},\"refresh\":{\"total\":4842,\"total_time_in_millis\":59750},\"flush\":{\"total\":203,\"total_time_in_millis\":13204}}],\"1\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"MimlEBKgQH2_oc3JxqxpXA\",\"relocating_node\":null,\"shard\":1,\"index\":\"video\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":6573115356},\"translog\":{\"id\":1376557600546,\"operations\":14},\"docs\":{\"num_docs\":3211079,\"max_doc\":3400727,\"deleted_docs\":189648},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":689,\"total_time_in_millis\":201221,\"total_docs\":248829,\"total_size_in_bytes\":949828400},\"refresh\":{\"total\":6710,\"total_time_in_millis\":61497},\"flush\":{\"total\":291,\"total_time_in_millis\":35503}},{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"qsymmz4LTeKkq_7AuKqG4g\",\"relocating_node\":null,\"shard\":1,\"index\":\"video\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":6575549719},\"translog\":{\"id\":1376557600544,\"operations\":8},\"docs\":{\"num_docs\":3211079,\"max_doc\":3400730,\"deleted_docs\":189651},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":721,\"total_time_in_millis\":167836,\"total_docs\":260534,\"total_size_in_bytes\":998259967},\"refresh\":{\"total\":7016,\"total_time_in_millis\":52708},\"flush\":{\"total\":306,\"total_time_in_millis\":16158}}],\"2\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"-5VMHj1DQYqKR6vi018o5w\",\"relocating_node\":null,\"shard\":2,\"index\":\"video\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":6373967857},\"translog\":{\"id\":1376557600772,\"operations\":31},\"docs\":{\"num_docs\":3218571,\"max_doc\":3365127,\"deleted_docs\":146556},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":31,\"total_time_in_millis\":10629,\"total_docs\":15966,\"total_size_in_bytes\":68588571},\"refresh\":{\"total\":289,\"total_time_in_millis\":3592},\"flush\":{\"total\":13,\"total_time_in_millis\":944}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"u0UGAe5cR8iqrfQobCI5_w\",\"relocating_node\":null,\"shard\":2,\"index\":\"video\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":6373626470},\"translog\":{\"id\":1376557600770,\"operations\":4},\"docs\":{\"num_docs\":3218571,\"max_doc\":3365127,\"deleted_docs\":146556},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":684,\"total_time_in_millis\":239822,\"total_docs\":261285,\"total_size_in_bytes\":1075831488},\"refresh\":{\"total\":6624,\"total_time_in_millis\":49142},\"flush\":{\"total\":284,\"total_time_in_millis\":23245}}],\"3\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"UT-z9BwoRYGHDO65Xe77lA\",\"relocating_node\":null,\"shard\":3,\"index\":\"video\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":6662207987},\"translog\":{\"id\":1376557600706,\"operations\":7},\"docs\":{\"num_docs\":3216797,\"max_doc\":3530765,\"deleted_docs\":313968},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":514,\"total_time_in_millis\":116248,\"total_docs\":189819,\"total_size_in_bytes\":730561130},\"refresh\":{\"total\":4970,\"total_time_in_millis\":47237},\"flush\":{\"total\":209,\"total_time_in_millis\":12062}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"Mg5HpexgTuCOtCs218D1GA\",\"relocating_node\":null,\"shard\":3,\"index\":\"video\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":6663459437},\"\ntranslog\":{\"id\":1376557600706,\"operations\":17},\"docs\":{\"num_docs\":3216797,\"max_doc\":3530763,\"deleted_docs\":313966},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":860,\"total_time_in_millis\":234922,\"total_docs\":323204,\"total_size_in_bytes\":1208575373},\"refresh\":{\"total\":8352,\"total_time_in_millis\":82412},\"flush\":{\"total\":356,\"total_time_in_millis\":19118}}],\"4\":[{\"routing\":{\"state\":\"STARTED\",\"primary\":false,\"node\":\"ix5R9EwPTEWLzWL2ZyiUOA\",\"relocating_node\":null,\"shard\":4,\"index\":\"video\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":6855177268},\"translog\":{\"id\":1376557600420,\"operations\":5},\"docs\":{\"num_docs\":3209453,\"max_doc\":3530977,\"deleted_docs\":321524},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":479,\"total_time_in_millis\":183694,\"total_docs\":174172,\"total_size_in_bytes\":729768430},\"refresh\":{\"total\":4677,\"total_time_in_millis\":40594},\"flush\":{\"total\":200,\"total_time_in_millis\":14822}},{\"routing\":{\"state\":\"STARTED\",\"primary\":true,\"node\":\"LGUsqbQXRei4YLyIjf7aow\",\"relocating_node\":null,\"shard\":4,\"index\":\"video\"},\"state\":\"STARTED\",\"index\":{\"size_in_bytes\":6857214782},\"translog\":{\"id\":1376557600420,\"operations\":12},\"docs\":{\"num_docs\":3209453,\"max_doc\":3530976,\"deleted_docs\":321523},\"merges\":{\"current\":0,\"current_docs\":0,\"current_size_in_bytes\":0,\"total\":835,\"total_time_in_millis\":181618,\"total_docs\":305792,\"total_size_in_bytes\":1184965062},\"refresh\":{\"total\":8125,\"total_time_in_millis\":63244},\"flush\":{\"total\":351,\"total_time_in_millis\":17492}}]}}}}\n2015-02-14 04:47:01,774 INFO      elasticsearch_curator.elasticsearch_curator        filter_by_space:443  skipping video, summed disk usage is 31.072 GB and disk limit is 2000.000 GB.\n2015-02-14 04:47:01,774 INFO      elasticsearch_curator.elasticsearch_curator        filter_by_space:443  skipping rsyslog-2015-02-14, summed disk usage is 131.723 GB and disk limit is 2000.000 GB.\n2015-02-14 04:47:01,774 INFO      elasticsearch_curator.elasticsearch_curator        filter_by_space:443  skipping rsyslog-2015-02-13, summed disk usage is 1919.939 GB and disk limit is 2000.000 GB.\n2015-02-14 04:47:01,774 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-02-14 04:47:29,322 DEBUG     urllib3.connectionpool          _make_request:368  \"DELETE /rsyslog-2015-02-12 HTTP/1.1\" 200 21\n2015-02-14 04:47:29,322 INFO               elasticsearch    log_request_success:57   DELETE http://localhost:9200/rsyslog-2015-02-12 [status:200 request:27.548s]\n2015-02-14 04:47:29,323 DEBUG              elasticsearch    log_request_success:59   > None\n2015-02-14 04:47:29,323 DEBUG              elasticsearch    log_request_success:60   < {\"acknowledged\":true}\n2015-02-14 04:47:29,323 INFO      elasticsearch_curator.elasticsearch_curator               _op_loop:711  delete_index operation succeeded on rsyslog-2015-02-12\n2015-02-14 04:47:29,323 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-02-14 04:47:45,115 DEBUG     urllib3.connectionpool          _make_request:368  \"DELETE /rsyslog-2015-02-11 HTTP/1.1\" 200 21\n2015-02-14 04:47:45,115 INFO               elasticsearch    log_request_success:57   DELETE http://localhost:9200/rsyslog-2015-02-11 [status:200 request:15.792s]\n2015-02-14 04:47:45,115 DEBUG              elasticsearch    log_request_success:59   > None\n2015-02-14 04:47:45,115 DEBUG              elasticsearch    log_request_success:60   < {\"acknowledged\":true}\n2015-02-14 04:47:45,116 INFO      elasticsearch_curator.elasticsearch_curator               _op_loop:711  delete_index operation succeeded on rsyslog-2015-02-11\n2015-02-14 04:47:45,116 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-02-14 04:47:47,461 DEBUG     urllib3.connectionpool          _make_request:368  \"DELETE /kibana-int HTTP/1.1\" 200 21\n2015-02-14 04:47:47,462 INFO               elasticsearch    log_request_success:57   DELETE http://localhost:9200/kibana-int [status:200 request:2.346s]\n2015-02-14 04:47:47,462 DEBUG              elasticsearch    log_request_success:59   > None\n2015-02-14 04:47:47,462 DEBUG              elasticsearch    log_request_success:60   < {\"acknowledged\":true}\n2015-02-14 04:47:47,462 INFO      elasticsearch_curator.elasticsearch_curator               _op_loop:711  delete_index operation succeeded on kibana-int\n2015-02-14 04:47:47,462 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-02-14 04:47:47,905 DEBUG     urllib3.connectionpool          _make_request:368  \"DELETE /intape-rsyslog-2015-02-14 HTTP/1.1\" 200 21\n2015-02-14 04:47:47,905 INFO               elasticsearch    log_request_success:57   DELETE http://localhost:9200/intape-rsyslog-2015-02-14 [status:200 request:0.443s]\n2015-02-14 04:47:47,905 DEBUG              elasticsearch    log_request_success:59   > None\n2015-02-14 04:47:47,906 DEBUG              elasticsearch    log_request_success:60   < {\"acknowledged\":true}\n2015-02-14 04:47:47,906 INFO      elasticsearch_curator.elasticsearch_curator               _op_loop:711  delete_index operation succeeded on intape-rsyslog-2015-02-14\n2015-02-14 04:47:47,906 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-02-14 04:47:48,186 DEBUG     urllib3.connectionpool          _make_request:368  \"DELETE /intape-rsyslog-2015-02-13 HTTP/1.1\" 200 21\n2015-02-14 04:47:48,187 INFO               elasticsearch    log_request_success:57   DELETE http://localhost:9200/intape-rsyslog-2015-02-13 [status:200 request:0.280s]\n2015-02-14 04:47:48,187 DEBUG              elasticsearch    log_request_success:59   > None\n2015-02-14 04:47:48,187 DEBUG              elasticsearch    log_request_success:60   < {\"acknowledged\":true}\n2015-02-14 04:47:48,187 INFO      elasticsearch_curator.elasticsearch_curator               _op_loop:711  delete_index operation succeeded on intape-rsyslog-2015-02-13\n2015-02-14 04:47:48,187 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-02-14 04:47:50,840 DEBUG     urllib3.connectionpool          _make_request:368  \"DELETE /intape-rsyslog-2015-02-12 HTTP/1.1\" 200 21\n2015-02-14 04:47:50,841 INFO               elasticsearch    log_request_success:57   DELETE http://localhost:9200/intape-rsyslog-2015-02-12 [status:200 request:2.653s]\n2015-02-14 04:47:50,841 DEBUG              elasticsearch    log_request_success:59   > None\n2015-02-14 04:47:50,841 DEBUG              elasticsearch    log_request_success:60   < {\"acknowledged\":true}\n2015-02-14 04:47:50,841 INFO      elasticsearch_curator.elasticsearch_curator               _op_loop:711  delete_index operation succeeded on intape-rsyslog-2015-02-12\n2015-02-14 04:47:50,841 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-02-14 04:47:51,269 DEBUG     urllib3.connectionpool          _make_request:368  \"DELETE /intape-rsyslog-2015-02-11 HTTP/1.1\" 200 21\n2015-02-14 04:47:51,270 INFO               elasticsearch    log_request_success:57   DELETE http://localhost:9200/intape-rsyslog-2015-02-11 [status:200 request:0.428s]\n2015-02-14 04:47:51,270 DEBUG              elasticsearch    log_request_success:59   > None\n2015-02-14 04:47:51,270 DEBUG              elasticsearch    log_request_success:60   < {\"acknowledged\":true}\n2015-02-14 04:47:51,270 INFO      elasticsearch_curator.elasticsearch_curator               _op_loop:711  delete_index operation succeeded on intape-rsyslog-2015-02-11\n2015-02-14 04:47:51,272 INFO      elasticsearch_curator.elasticsearch_curator                 delete:960  Speficied indices deleted.\n2015-02-14 04:47:51,272 INFO      elasticsearch_curator.curator_script                   main:359  Done in 0:00:49.699180.\n```\n. ",
    "rmuir": "note, as the option currently works, space savings only apply to stored fields data (not postings lists or term dictionaries for indexed fields), but today this is still typically the largest portion of the index.\nSo overall, we can think of current space savings of something like ~20-25% on average. But for many use-cases this is still a good trade-off.\n. ",
    "Onalu": "Even if the snapshots are segment level, there is still problem for me. \nOn the  January 6th, logstash-2014.01.05 and logstash-2014.01.06 is backed up but logstash-2014.01.06 is still incomplete. So, It will be re-backed up  on the January 7th. \nI want to take snapshot of the last complete index. I think I can use Curator API that you mentioned to do this. \nThank you for your suggestion.\n. ",
    "poolski": "I see. I'll check that and report back!\nCheers!\n. Still happening, even with a timeout value set..\nbash\nme@host$ curator --dry-run --timeout 120 --host localhost --port 9200 delete --disk-space 1024\n2015-01-16 16:35:31,601 INFO      Job starting...\n2015-01-16 16:35:31,602 INFO      DRY RUN MODE.  No changes will be made.\n2015-01-16 16:35:31,881 INFO      DRY RUN: Deleting indices...\n2015-01-16 16:35:31,881 INFO      DRY RUN: Deleting by space rather than time.\nTraceback (most recent call last):\n  File \"/usr/local/bin/curator\", line 9, in <module>\n    load_entry_point('elasticsearch-curator==2.1.1', 'console_scripts', 'curator')()\n  File \"/usr/local/lib/python2.7/dist-packages/curator/curator_script.py\", line 364, in main\n    arguments.func(client, **argdict)\n  File \"/usr/local/lib/python2.7/dist-packages/curator/curator.py\", line 1004, in delete\n    _op_loop(client, matching_indices, op=delete_index, dry_run=dry_run, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/curator/curator.py\", line 742, in _op_loop\n    for item in object_list:\n  File \"/usr/local/lib/python2.7/dist-packages/curator/curator.py\", line 434, in filter_by_space\n    stats = client.indices.status(index=csv_indices)\n  File \"/usr/local/lib/python2.7/dist-packages/elasticsearch/client/utils.py\", line 68, in _wrapped\n    return func(*args, params=params, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/elasticsearch/client/indices.py\", line 705, in status\n    params=params)\n  File \"/usr/local/lib/python2.7/dist-packages/elasticsearch/transport.py\", line 306, in perform_request\n    status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)\n  File \"/usr/local/lib/python2.7/dist-packages/elasticsearch/connection/http_urllib3.py\", line 78, in perform_request\n    raise ConnectionError('N/A', str(e), e)\nelasticsearch.exceptions.ConnectionError: ConnectionError(('Connection aborted.', BadStatusLine(\"''\",))) caused by: ProtocolError(('Connection aborted.', BadStatusLine(\"''\",)))\n. I thought this as well, but it fails after ~10 seconds, which implies that the time out isn't being hit at all. \n. Yes, likewise... \nOdd. \n. #252 seems to be reporting something similar. \n. ",
    "charlesnadeau": "I have the exact same problem:\ncharles@bigzilla:/etc/elasticsearch$ /usr/local/bin/curator --timeout 1000 --host localhost --port 9200 delete --disk-space 100\n2015-01-18 18:24:27,972 INFO      Job starting...\n2015-01-18 18:24:27,974 INFO      Deleting indices...\n2015-01-18 18:24:27,974 INFO      Deleting by space rather than time.\nTraceback (most recent call last):\n  File \"/usr/local/bin/curator\", line 9, in <module>\n    load_entry_point('elasticsearch-curator==2.1.1', 'console_scripts', 'curator')()\n  File \"/usr/local/lib/python2.7/dist-packages/curator/curator_script.py\", line 364, in main\n    arguments.func(client, **argdict)\n  File \"/usr/local/lib/python2.7/dist-packages/curator/curator.py\", line 1025, in delete\n    _op_loop(client, matching_indices, op=delete_index, dry_run=dry_run, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/curator/curator.py\", line 763, in _op_loop\n    for item in object_list:\n  File \"/usr/local/lib/python2.7/dist-packages/curator/curator.py\", line 434, in filter_by_space\n    stats = client.indices.status(index=csv_indices)\n  File \"/usr/local/lib/python2.7/dist-packages/elasticsearch/client/utils.py\", line 68, in _wrapped\n    return func(*args, params=params, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/elasticsearch/client/indices.py\", line 705, in status\n    params=params)\n  File \"/usr/local/lib/python2.7/dist-packages/elasticsearch/transport.py\", line 301, in perform_request\n    status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)\n  File \"/usr/local/lib/python2.7/dist-packages/elasticsearch/connection/http_urllib3.py\", line 78, in perform_request\n    raise ConnectionError('N/A', str(e), e)\nelasticsearch.exceptions.ConnectionError: ConnectionError(('Connection aborted.', BadStatusLine(\"''\",))) caused by: ProtocolError(('Connection aborted.', BadStatusLine(\"''\",)))\nHowever it does work with a delete by time:\ncharles@bigzilla:/etc/elasticsearch$ /usr/local/bin/curator --dry-run --timeout 1000 --host localhost --port 9200 delete --older-than 3\n2015-01-18 18:27:40,670 INFO      Job starting...\n2015-01-18 18:27:40,670 INFO      DRY RUN MODE.  No changes will be made.\n2015-01-18 18:27:40,672 INFO      DRY RUN: Deleting indices...\n2015-01-18 18:27:48,666 INFO      DRY RUN: of delete_index operation on logstash-2014.12.21\n2015-01-18 18:27:48,666 INFO      DRY RUN: of delete_index operation on logstash-2014.12.22\n2015-01-18 18:27:48,666 INFO      DRY RUN: of delete_index operation on logstash-2014.12.23\n2015-01-18 18:27:48,666 INFO      DRY RUN: of delete_index operation on logstash-2014.12.24\n2015-01-18 18:27:48,666 INFO      DRY RUN: of delete_index operation on logstash-2014.12.25\n2015-01-18 18:27:48,666 INFO      DRY RUN: of delete_index operation on logstash-2014.12.26\n2015-01-18 18:27:48,666 INFO      DRY RUN: of delete_index operation on logstash-2014.12.27\n2015-01-18 18:27:48,666 INFO      DRY RUN: of delete_index operation on logstash-2014.12.28\n2015-01-18 18:27:48,667 INFO      DRY RUN: of delete_index operation on logstash-2014.12.29\n2015-01-18 18:27:48,667 INFO      DRY RUN: of delete_index operation on logstash-2014.12.30\n2015-01-18 18:27:48,667 INFO      DRY RUN: of delete_index operation on logstash-2014.12.31\n2015-01-18 18:27:48,667 INFO      DRY RUN: of delete_index operation on logstash-2015.01.01\n2015-01-18 18:27:48,667 INFO      DRY RUN: of delete_index operation on logstash-2015.01.02\n2015-01-18 18:27:48,667 INFO      DRY RUN: of delete_index operation on logstash-2015.01.03\n2015-01-18 18:27:48,667 INFO      DRY RUN: of delete_index operation on logstash-2015.01.04\n2015-01-18 18:27:48,668 INFO      DRY RUN: of delete_index operation on logstash-2015.01.05\n2015-01-18 18:27:48,668 INFO      DRY RUN: of delete_index operation on logstash-2015.01.06\n2015-01-18 18:27:48,668 INFO      DRY RUN: of delete_index operation on logstash-2015.01.07\n2015-01-18 18:27:48,669 INFO      DRY RUN: of delete_index operation on logstash-2015.01.08\n2015-01-18 18:27:48,669 INFO      DRY RUN: of delete_index operation on logstash-2015.01.09\n2015-01-18 18:27:48,670 INFO      DRY RUN: of delete_index operation on logstash-2015.01.10\n2015-01-18 18:27:48,670 INFO      DRY RUN: of delete_index operation on logstash-2015.01.11\n2015-01-18 18:27:48,670 INFO      DRY RUN: of delete_index operation on logstash-2015.01.12\n2015-01-18 18:27:48,671 INFO      DRY RUN: of delete_index operation on logstash-2015.01.13\n2015-01-18 18:27:48,671 INFO      DRY RUN: of delete_index operation on logstash-2015.01.14\n2015-01-18 18:27:48,671 INFO      DRY RUN: of delete_index operation on logstash-2015.01.15\n2015-01-18 18:27:48,672 INFO      logstash-2015.01.16 is within the threshold period (3 days).\n2015-01-18 18:27:48,673 INFO      logstash-2015.12.25 is within the threshold period (3 days).\n2015-01-18 18:27:48,673 INFO      logstash-2015.12.27 is within the threshold period (3 days).\n2015-01-18 18:27:48,673 INFO      logstash-2015.12.29 is within the threshold period (3 days).\n2015-01-18 18:27:48,673 INFO      logstash-2015.12.30 is within the threshold period (3 days).\n2015-01-18 18:27:48,673 INFO      logstash-2015.12.31 is within the threshold period (3 days).\n2015-01-18 18:27:48,673 INFO      DRY RUN: Specified indices deleted.\n2015-01-18 18:27:48,674 INFO      Done in 0:00:08.018128.\n. ",
    "mfyorp": "I have the same error messsage. --timeout has no effect, as the error appears almost instantly. Deleting by time does not give me that error. Anything other info I could provide or any other tests I could do?\n. Works with v3.\nIn the mean time, the problem seems to be solved for me with 2.1.2 as well. I might not have been on the latest stable version (2.1.2), sorry for that. Problem solved for me now.\n. ",
    "djs52": "I believe this is the same issue, so I'm reopening. Seeing this with both 3 and 2.1.2 -- with debug, I eventually see the failed requests in the log:\n2015-03-09 18:27:07,683 INFO      urllib3.connectionpool              _new_conn:203  Starting new HTTP connection (3): rockall-nfs\n2015-03-09 18:27:07,686 WARNING            elasticsearch       log_request_fail:81   GET http://rockall-nfs:9200/logstash-2013.01.01,logstash-2013.01.02,logstash-2013.01.03,logstash-2013.01.04,logstash-2013.01.05,logstash-2013.01.06,logstash-2013.01.07,logstash-2013.08.13,logstash-2013.08.14,logstash-2013.08.15,logstash-2013.08.16,logstash-2013.08.17,logstash-2013.08.18,logstash-2013.08.19,logstash-2013.08.20,logstash-2013.08.21,logstash-2013.08.22,...\nThe request turns out to be about 6kb, which exceeds the http.max_initial_line_length setting in Elasticsearch (see http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-http.html). Increasing this setting seems to allow curator to work.\n. ",
    "AlexeyKupershtokh": "I've seen this issue with curator 3.0.3.\nThe workaround with http.max_initial_line_length: 16kb fixed it.\n. So let's begin.\nThis is common system information:\n```\nwicked@f2p-logs:~$ cat /etc/issue\nUbuntu 12.04.3 LTS \\n \\l\nwicked@f2p-logs:~$ /usr/local/bin/curator --version\ncurator, version 3.0.3\nwicked@f2p-logs:~$ aptitude show elasticsearch\nPackage: elasticsearch                 \nNew: yes\nState: installed\nAutomatically installed: no\nVersion: 1.5.0\nPriority: optional\nSection: web\nMaintainer: Elasticsearch Team info@elasticsearch.com\nArchitecture: all\nUncompressed Size: 31,0 M\nDepends: libc6, adduser\nDescription: Open Source, Distributed, RESTful Search Engine\n...\nwicked@f2p-logs:~$ grep 3072 /usr/local/lib/python2.7/dist-packages/curator/cli/index_selection.py\n                if len(to_csv(working_list)) > 3072 and not ctx.parent.info_name == 'snapshot':\n```\nThe command I run using curator is:\n/usr/local/bin/curator delete --disk-space 7 indices --older-than 2 --time-unit days --timestring \"%Y.%m.%d\"\nAnd these are outputs:\nWorkaround enabled:\nhttps://gist.github.com/AlexeyKupershtokh/b8c74b395a16c115f2e0#file-with-the-workaround-enabled\nWorkaround disabled:\nhttps://gist.github.com/AlexeyKupershtokh/b8c74b395a16c115f2e0#file-without-the-workaround\nAnd the corresponding elasticsearch log: https://gist.github.com/AlexeyKupershtokh/b8c74b395a16c115f2e0#file-errors-from-the-elasticsearch-log\nThe elasticsearch log tells that the error is:\norg.elasticsearch.common.netty.handler.codec.frame.TooLongFrameException: An HTTP line is larger than 4096 bytes.\n. Thank you for the thorough explanation. I've been using this approach just to make sure my single node won't fill. I don't like situations when it stops writing, start losing newest data and require additional ops to recover from this state. So this is sensible choice: \"hey, elasticsearch. just make sure you always have some space\". The other caveats you listed seems acceptable in my case.\nBut I should probably change it to another approach that would delete the oldest indexes only:\nn = 100;\nwhile (free space < 1 gb and n > 0) {\n    curator delete indices --older-than $n --time-unit days --timesting \"%Y.%m.%d\"\n    n--;\n}\nThough I wonder what else approaches could be used to automatically free space in cluster environments with many index series (like stage and prod multiplied by different projects writing logs) ?\n. ",
    "grahamc": "From IRC today:\ngchristensen | Hi, does elasticsearch-curator work with python2? or python3? or both?\n    untergeek | currently it should only be used with python2\n gchristensen | ok, thank you untergeek\n    untergeek | because of some weirdness with the script changes and python3\n. ",
    "Dinoshauer": "Ah! Well, that's awesome!\nDo you have any ETA on the release of 3.0?\n. No worries, I am no stranger to prioritizing :) I'll keep a look out for 3.0 and keep using this branch until then! You can close off this PR if you wish.\n.  Just tried it out and it worked awesome :+1: - Can I subscribe to something somewhere to be alerted of when this goes into production?\n. ",
    "rtoma": "$ fgrep xpr-log4json /usr/local/bin/curator.sh\n/usr/bin/curator --loglevel WARNING --timeout 3600 --host x.x.x.x optimize --prefix logstash-xpr-log4json- --older-than 2\n/usr/bin/curator --loglevel WARNING --timeout 3600 --host x.x.x.x delete --prefix logstash-xpr-log4json- --older-than 14\n/usr/bin/curator --loglevel WARNING --timeout 3600 --host x.x.x.x bloom --prefix logstash-xpr-log4json- --older-than 1\nMaybe its a race condition between the delete and bloom command? Maybe the delete is async and still underway when the bloom starts?\nLast night, same cron, all went fine as did before last weekend. Supporting the race condition hypothesis.\n. Great answer! We'll implement this.\n. ",
    "rk295": "Hi, I'm not sure if this is the problem. I have a single ES node and I get the same problem. If I delete the index shown in the error and re-run curator, then another later index causes the same problem. But curiously it isn't every index that causes the error.\n%> pip list |grep elastic\nelasticsearch (1.2.0)\nelasticsearch-curator (2.1.2)\n%> python --version\nPython 2.7.5\n%> curator optimize --older-than 2\n2015-02-02 10:02:45,831 INFO      Job starting...\n2015-02-02 10:02:45,831 INFO      Default timeout of 30 seconds is too low for command OPTIMIZE.  Overriding to 21,600 seconds (6 hours).\n2015-02-02 10:02:45,834 INFO      Optimizing indices...\n2015-02-02 10:02:45,842 INFO      Skipping index logstash-2015.01.04: Already optimized.\n2015-02-02 10:02:45,845 INFO      Skipping index logstash-2015.01.05: Already optimized.\n2015-02-02 10:02:45,848 INFO      Skipping index logstash-2015.01.06: Already optimized.\n2015-02-02 10:02:45,850 INFO      Skipping index logstash-2015.01.08: Already optimized.\n2015-02-02 10:02:45,853 INFO      Skipping index logstash-2015.01.09: Already optimized.\n2015-02-02 10:02:45,856 INFO      Skipping index logstash-2015.01.10: Already optimized.\n2015-02-02 10:02:45,860 INFO      Skipping index logstash-2015.01.11: Already optimized.\n2015-02-02 10:02:45,864 INFO      Skipping index logstash-2015.01.12: Already optimized.\nTraceback (most recent call last):\n  File \"/usr/bin/curator\", line 9, in <module>\n    load_entry_point('elasticsearch-curator==2.1.2', 'console_scripts', 'curator')()\n  File \"/usr/lib/python2.7/site-packages/curator/curator_script.py\", line 383, in main\n    arguments.func(client, **argdict)\n  File \"/usr/lib/python2.7/site-packages/curator/curator.py\", line 1086, in optimize\n    _op_loop(client, matching_indices, op=optimize_index, dry_run=dry_run, **kwargs)\n  File \"/usr/lib/python2.7/site-packages/curator/curator.py\", line 779, in _op_loop\n    skipped = op(client, item, **kwargs)\n  File \"/usr/lib/python2.7/site-packages/curator/curator.py\", line 637, in optimize_index\n    shards, segmentcount = get_segmentcount(client, index_name)\n  File \"/usr/lib/python2.7/site-packages/curator/curator.py\", line 266, in get_segmentcount\n    shards = client.indices.segments(index=index_name)['indices'][index_name]['shards']\nKeyError: u'logstash-2015.01.13'\n%>\n. Hi, thanks for the response. I'm running ES version 1.4.0, I've bumped by ES python module to 1.3.0, thanks for mentioning that I hadn't noticed a new version was out. It didn't solve it though :(\nI've run the tests you suggested above, and the results are interesting,  as you can see for the 13th and 14th of Jan there are no indices, but for later days in the month, the 25th for example it all looks fine.\nI've checked into my notes, and the 13th/14th was about the time Logstash had died and wasn't sending any data, which probably accounts for my empty indexes.\nAm I right in thinking that checking if Indices is not empty would probably solve this?\n```\n%> python\nPython 2.7.5 (default, Jun 17 2014, 18:11:42)\n[GCC 4.8.2 20140120 (Red Hat 4.8.2-16)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport curator\nimport elasticsearch\nclient = elasticsearch.Elasticsearch(host=\"127.0.0.1\")\nindex_name = \"logstash-2015.01.13\"\nclient.indices.segments(index=index_name)\n{u'indices': {}, u'_shards': {u'successful': 0, u'failed': 0, u'total': 10}}\nindex_name = \"logstash-2015.01.14\"\nclient.indices.segments(index=index_name)\n{u'indices': {}, u'_shards': {u'successful': 0, u'failed': 0, u'total': 10}}\nindex_name = \"logstash-2015.01.25\"\nclient.indices.segments(index=index_name)\n{u'indices': {u'logstash-2015.01.25':...... much stuff removed....\n```\n. :+1:  Ha, I always manage to break things in creative ways! Thanks again, I'll keep an eye out for the commit.\n. Confirmed, this fixes the problem for me. Thanks.\n. \n\n\n",
    "SteveDevOps": "Yes on the single quotes. I also tried your last suggesion.. but I get in debug log:\n2015-01-27 21:27:16,247 DEBUG     Matching indices with pattern: api.transactions-%Y.%m.%d\n2015-01-27 21:27:16,247 DEBUG     argdict = {'url_prefix': '', 'prefix': 'api.transactions-', 'log_level': 'DEBUG', 'timestring': '%Y.%m.%d', 'dry_run': False, 'exclude_pattern': None, 'logformat': 'Default', 'auth': None, 'ssl': False, 'host': 'xxxxxxx', 'command': 'close', 'time_unit': 'days', 'timeout': 30, 'debug': False, 'func': , 'log_file': '/var/log/curator.log', 'master_only': True, 'port': 9200, 'older_than': 3, 'suffix': ''}\n2015-01-27 21:27:16,247 INFO      Closing indices...\n2015-01-27 21:27:16,259 DEBUG     Unable to match api.transactions-2015.1.22 with regular expression ^api.transactions-(\\d{4}.\\d{2}.\\d{2})$.  Error: 'NoneType' object has no attribute 'group'\n. and same on last suggestion but without single quotes:\n2015-01-27 21:33:41,859 DEBUG     Matching indices with pattern: api.transactions-%Y.%m.%d\n2015-01-27 21:33:41,859 DEBUG     argdict = {'url_prefix': '', 'prefix': 'api.transactions-', 'log_level': 'DEBUG', 'timestring': '%Y.%m.%d', 'dry_run': False, 'exclude_pattern': None, 'logformat': 'Default', 'auth': None, 'ssl': False, 'host': 'xxxxxxx', 'command': 'close', 'time_unit': 'days', 'timeout': 30, 'debug': False, 'func': , 'log_file': '/var/log/curator.log', 'master_only': True, 'port': 9200, 'older_than': 3, 'suffix': ''}\n2015-01-27 21:33:41,859 INFO      Closing indices...\n2015-01-27 21:33:41,870 DEBUG     Unable to match api.transactions-2015.1.22 with regular expression ^api.transactions-(\\d{4}.\\d{2}.\\d{2})$.  Error: 'NoneType' object has no attribute 'group'\n. my test case with versions .. note.. I xx'ed out the host\n[root@xxxx curator-scripts]# curator --host xxxxxx --port 9200 --master-only --logfile /var/log/curator.log --loglevel DEBUG close --older-than 1 --time-unit days --prefix api.transactions-\n[root@xxxx curator-scripts]# cat /var/log/curator.log\n2015-01-28 15:00:08,020 INFO      Job starting...\n2015-01-28 15:00:08,028 DEBUG     Detected Elasticsearch version 1.1.1\n2015-01-28 15:00:08,039 DEBUG     Setting default timestring for days to %Y.%m.%d\n2015-01-28 15:00:08,039 DEBUG     Matching indices with pattern: api.transactions-%Y.%m.%d\n2015-01-28 15:00:08,039 DEBUG     argdict = {'url_prefix': '', 'prefix': 'api.transactions-', 'log_level': 'DEBUG', 'timestring': '%Y.%m.%d', 'dry_run': False, 'exclude_pattern': None, 'logformat': 'Default', 'auth': None, 'ssl': False, 'host': 'xxxx', 'command': 'close', 'time_unit': 'days', 'timeout': 30, 'debug': False, 'func': , 'log_file': '/var/log/curator.log', 'master_only': True, 'port': 9200, 'older_than': 1, 'suffix': ''}\n2015-01-28 15:00:08,039 INFO      Closing indices...\n2015-01-28 15:00:08,047 DEBUG     Unable to match api.transactions-2015.1.26 with regular expression ^api.transactions-(\\d{4}.\\d{2}.\\d{2})$.  Error: 'NoneType' object has no attribute 'group'\n2015-01-28 15:00:08,047 DEBUG     Unable to match api.transactions-2015.1.27 with regular expression ^api.transactions-(\\d{4}.\\d{2}.\\d{2})$.  Error: 'NoneType' object has no attribute 'group'\n2015-01-28 15:00:08,047 DEBUG     Unable to match api.transactions-2015.1.28 with regular expression ^api.transactions-(\\d{4}.\\d{2}.\\d{2})$.  Error: 'NoneType' object has no attribute 'group'\n2015-01-28 15:00:08,047 INFO      Closed specified indices.\n2015-01-28 15:00:08,048 INFO      Done in 0:00:00.050726.\n[root@xxxx curator-scripts]# curator --version\ncurator 2.1.2\n[root@xxxx curator-scripts]# python --version\nPython 2.6.6\n[root@xxxx curator-scripts]# cat /etc/redhat-release \nRed Hat Enterprise Linux Server release 6.5 (Santiago)\n. what is also odd is show works.. \n[root@xxxx curator-scripts]# curator show --show-indices --prefix api.transactions-\napi.transactions-2015.1.26\napi.transactions-2015.1.27\napi.transactions-2015.1.28\n. same error on dry run. \ncurator -n --debug delete --older-than 1 --prefix api.transactions-\n2015-01-28 15:38:04,816 DEBUG            curator.curator    filter_by_timestamp:374  Unable to match api.transactions-2015.1.26 with regular expression ^api.transactions-(\\d{4}.\\d{2}.\\d{2})$.  Error: 'NoneType' object has no attribute 'group'\n2015-01-28 15:38:04,816 DEBUG            curator.curator    filter_by_timestamp:374  Unable to match api.transactions-2015.1.27 with regular expression ^api.transactions-(\\d{4}.\\d{2}.\\d{2})$.  Error: 'NoneType' object has no attribute 'group'\n2015-01-28 15:38:04,817 DEBUG            curator.curator    filter_by_timestamp:374  Unable to match api.transactions-2015.1.28 with regular expression ^api.transactions-(\\d{4}.\\d{2}.\\d{2})$.  Error: 'NoneType' object has no attribute 'group'\n2015-01-28 15:38:04,817 INFO             curator.curator                 delete:1038 DRY RUN: Specified indices deleted.\n2015-01-28 15:38:04,817 INFO      curator.curator_script                   main:385  Done in 0:00:00.018822.\nre:\n```\n[root@xxx curator-scripts]# python\nPython 2.6.6 (r266:84292, Nov 21 2013, 10:50:32) \n[GCC 4.4.7 20120313 (Red Hat 4.4.7-4)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport re\nre.version\n'2.2.1'\n```\n. thx much!\n. no matched:\n\n\n\n```\n[root@myhost ~]# /usr/bin/curator --dry-run --master-only --host myhost --port 9200 delete snapshots --prefix ol- --older-than 3 --time-unit days --repository es-dr-backups\n2016-01-26 14:46:27,681 INFO      Job starting: delete snapshots\n2016-01-26 14:46:27,682 INFO      DRY RUN MODE.  No changes will be made.\n2016-01-26 14:46:28,237 WARNING   No snapshots matched provided args.\nNo snapshots matched provided args.\n```\n. couple examples of snaps currently sitting my repo:\n```\n}, {\n    \"snapshot\" : \"ol-close-01132016-141833-139\",\n    \"indices\" : [ \"omnilogs-2015.09.15\" ],\n    \"state\" : \"SUCCESS\",\n    \"start_time\" : \"2016-01-13T19:18:34.197Z\",\n    \"start_time_in_millis\" : 1452712714197,\n    \"end_time\" : \"2016-01-13T19:18:34.529Z\",\n    \"end_time_in_millis\" : 1452712714529,\n    \"duration_in_millis\" : 332,\n    \"failures\" : [ ],\n    \"shards\" : {\n      \"total\" : 5,\n      \"failed\" : 0,\n      \"successful\" : 5\n    }\n  }, {\n    \"snapshot\" : \"ol-close-01132016-141835-140\",\n    \"indices\" : [ \"omnilogs-2015.10.14\" ],\n    \"state\" : \"SUCCESS\",\n    \"start_time\" : \"2016-01-13T19:18:36.324Z\",\n    \"start_time_in_millis\" : 1452712716324,\n    \"end_time\" : \"2016-01-13T19:18:37.137Z\",\n    \"end_time_in_millis\" : 1452712717137,\n    \"duration_in_millis\" : 813,\n    \"failures\" : [ ],\n    \"shards\" : {\n      \"total\" : 5,\n      \"failed\" : 0,\n      \"successful\" : 5\n    }\n```\n. thx guys.. totally missed that, thx for clarifying. I simplified my naming on creation cron job ;)\n. ",
    "rtkmhart": "Ok, since I have an urgent need for this I'll do something else, but leave this PR for you to cherry pick from.\n. Thanks for the quick response!\nWe're at around 1250 indices on 20 data nodes, the cluster is very busy. There are no errors in logs on the master or data nodes that I can find.\nWhat's weird is I set the --timeout to 3600, which I'm assuming is seconds... but it comes back in 60 seconds, consistently across a few runs. Is there another timeout somewhere that's not getting overridden by the 3600?\n. It's definitely a timeout issue somewhere. If I set --older-than to something much higher so it's only deleting 10 indices at a time, it comes back in around 30 seconds and has successfully deleted the indices.\n. No load balancer, I'm running curator directly on the master node talking to localhost:9200.\n. There is nothing in the Elasticsearch logs, I've double checked it now.\nThere is no proxy or gateway of firewall in the path. I'm running curator directly on the same host as the master node, connecting to localhost:9200, and all data nodes are running in the same subnet without any firewalls.\nWhat's even weirder is that now it's returning consistently in 31 seconds without actually doing anything. I haven't changed anything except to keep running the same command (posted in the initial comment) over and over again, pushing the definition of insanity. :)\n. We're having similar discussions about cluster state with our support rep at Elastic :) Our current cluster state is around 250Mb. While the cluster is big, it's still dealing with what we're throwing at it. \nWe have some design changes coming, but in the meantime we have to deal with it. For the record, 1250 indices and 7778 shards. Masters have 4GB heap, and according to Marvel are not under any sort of duress, GC drops heap usage to 25% and CPU usage < 25%. \nSo while that may be big by some measure, it still doesn't really explain why this isn't working.\nI did notice that at some point curator splits the total number of indices to delete into smaller chunks (if memory serves, been a long day). Is that configurable? That is, can we tell curator not to try to delete more than say 10 in one batch?\n. It turns out we have a mapping problem (bug in our code) that's causing our massive cluster state, we're in the process of cleaning it up. Curator seems happy again. Thanks for your help and advice!\n. ",
    "cfeio": "Is there any way to restrict to ^(prefix)(timestamp)(suffix)$ like it functioned in Curator v2? \nIf I have index patterns of logstash-2015.05.08 and logstash-anotherindex-2015.05.08 I'd like to be able to specify to only delete \"logstash-\" and not \"logstash-anotherindex\".\n. Thanks for the quick response! The regex flag accomplishes what I was looking for: --regex '^logstash-(\\d{4}.\\d{2}.\\d{2})'\n. ",
    "ferki": "@untergeek: thanks for the quick feedback! I signed the CCLA and I'm already in the list of contributors ;)\n. @untergeek: yes, I used ferki as github username, and also my work email address/details to sign the CCLA. Then I received the email verification email, and then the final signed PDF document.\nLet me know if there are any additional steps.\n. Ok, alias is ruled out by #214.\n. I could at least change the required allocation rules and the number of replicas of a closed index with elasticsearch-1.4.4.\n. Yes, it did not reopen the index by doing so, but reopening it later has a result identical as if I had applied the changes while they were open. On huge datasets it really helps to be able to decrease replica count for older indices, or making sure they will be recovered to the proper hosts - so upon a later reopen, there won't be unnecessary shard juggling.\nOf course all this can be achieved by normal curl or other API calls, but I think it would be nice to give an opt-in possibility for users who might make a good use of it (of it is to be made the default behaviour, an opt-out for those who for some reason don't want to use it). Personally I'm fine with making it default :)\nAnd yes, it's an interesting question if/how it would work with other versions of elasticsearch. Can you do such testing easily?\n. Yeah, after all it makes sense to apply changes even to closed indices if that's an allowed operation and does not have unexpected side effects (like manipulating bloom filter settings on a closed index also triggered a reopen, or adding a closed index to an alias caused querying errors).\nProbably you're more familiar with the new code layout and can implement this fater than me, but let me know if you would like me to send a PR! :)\n. Tests are fine, and if there's still some time left to do it before the final 3.0.0 release, then I would like to take a bash at it a bit later, so I can also learn from the review ^^\n. Thanks for the feedback! I pushed the changes you requested.\nAnd yes, with the one-step replica count manipulation it's not necessary to skip the indices that already has the requested replica count.\nps.: are there something wrong with our CCLA or does it just need some time before tests start passing?\n. The company name on the CCLA is adjust GmbH.\nTests are passing for my local single-node elasticsearch-1.4.4 with both python-2.7.9 and 3.3.5.\n. On the last page of the CCLA, I gave my name (Ferenc Erki), my email (ferki at adjust.com - I also use this for the commits I send), and my github username (ferki). I used the same details at the start of the document as contact info. If that helps investigating, I can also send you other data from the signed document I received via email (like transaction ID, etc.) - just let me know if I can help in any way with this.\n. I'm already a contributor through our company CLA ^^ Looks like the automatic tests still refuse to recognize this :(\n. Thanks! \\o/\n. I would also like to create a new emtpy index sometimes on a scheduled basis. I might be able to send a PR about it later, but if someone can beat me to it, go ahead ;)\n. > Have you tried running with a higher timeout value than the default of 30 seconds?\nNo, but I can reproduce it on pretty much every run by decreasing --timeout. So I guess increasing it is an easy solution, but I cannot test it as now my indices are sync flushed, it finishes quickly :)\nI think trying to make the error message a bit more friendly would be enough (e.g. if it's a timeout problem, I'd prefer a message about timing out, not TypeError :), but increasing the default timeout for seal operations might also be useful.\n. @untergeek: thank you!\n. ",
    "robbles": "I'll probably just upgrade to 3.0.0 right away then - the current docs make it sound a lot better :)\nNo worries about the mismatch, the --help info definitely made this problem easier to work around. One way you could avoid this in future might be to move the API docs to markdown files within the repo itself. That way it's simple to see the docs for any given release by just switching to the appropriate tag. Not sure if that's worth it if you normally update the docs alongside the new release anyways though.\n. ",
    "dbeckham": "The output is definitely broken. If I run the same command with 20 days, I see the following output:\n2015-03-10 10:26:06,208 INFO      Job starting...\n2015-03-10 10:26:06,208 INFO      DRY RUN MODE.  No changes will be made.\n2015-03-10 10:26:08,866 WARNING   DRY RUN: Will not perform delete action\ncurator-20150208230022\ncurator-20150209230022\ncurator-20150210230029\ncurator-20150211230025\ncurator-20150212230025\ncurator-20150213230026\ncurator-20150214230026\ncurator-20150215230030\ncurator-20150216230024\ncurator-20150217230024\ncurator-20150218230025\n. I think it's two things.  I actually ended up running the show later and saw that the last line with a datestamp indicated that what follows was a list of the snapshots found. The delete snapshots command output above doesn't use similar language.  Because of the sudden change in the log format being output for the command and that it didn't tell me it found these snapshots, it looked like debug or unintentional output. \nI realized after running show and running this command with different parameters, that you intended to not have a datestamp or log level for that output, so it's mostly just that the dry run output should be a bit clearer like the show command is.\nHonestly, this is a pretty minor problem. I was trying to work through the command line differences between 2.0 and 3.0 and I was confused by some of the output where I was expecting something different than what I got.\n. Np, just wanted to let you guys know.  Along the same lines, I also noticed that the delete snapshots --help and delete indices --help output isn't really documented in the wiki, or displayed as a text block like the other --help options are.  Specifically, the --repository option and a couple of the generic flags for some of those commands are missing. \n. ",
    "sephlietz": "The command used looks something like\ncurator --host localhost delete --time-unit weeks --timestring %Y-%W- --prefix some-prefix- --suffix '*' --older-than 7\n. Got it, thanks.\n. ",
    "SegFaultAX": "My assumption was (perhaps wrongly so) that anything in the cli namespace was not necessarily something you'd want to use as an API consumer (which I am in this case). Should the filtering logic in index_selection.py be lifted into the API so it's available to, and shared by, both use cases? How can I help with getting the API namespace up to the usability standards of 2.x? Perhaps for now we can include a subset of my REPL session above in the curator documentation so users trying to upgrade can get a feel for how they might fetch and filter the list of indices from their ES cluster?\nIn any case, I'm happy to contribute code/documentation/etc., but the current state of curator 3.x API is an upgrade blocker for me.\n. Good question! The honest answer is, I don't know what the full API call should look like. But to get the ball rolling, we could break it down into several chainable filters:\n``` python\nes = elasticsearch.Elasticsearch(\"...\")\nindices = curator.get_indices(es)\nlogstash_indices = curator.filter.with_prefix(indices, \"logstash-\")\nold_indices = curator.filter.older_than(logstash_indices, 3, \"days\")\n```\nBreak it down into many small, easily composed functions to filter different aspects of the index names, rather than one massive monolithic filter. You could even create a class that makes composing multiple filters together natural. Here's a prototype I whipped together of what such a class could look like:\nhttps://gist.github.com/SegFaultAX/cc6caa30136f213b5e12\nThoughts?\n. Well, the proposed changes are purely additive in nature (relying partially on the existing API functionality). As such I'm not sure there is too much risk of it breaking anything. Your proposal seems to work along a similar axis (interesting how close our solutions ended up being), although you seem to be favoring the larger monolithic function to apply filters + a \"filter builder\" function. I guess my main feedback about that would be it drastically increases the complexity of the API since it's all coalesced under one function. But it's purely a stylistic thing. :)\nI totally get your concern with not introducing breaking changes, but I think if we do it right, we can make the changes purely additive in nature. Whatever your decision for the implementation style, I'm excited to have some working solution in place for this. Thank you again for your time and effort!\n. ",
    "OuesFa": "Ok thank you :)\nFor now it says No indices matched provided args. it's weird cause it's the correct pattern i think to match this indice \"logstash-2015.03.16\"\nBut i will refer to the documentation.\n. Thank you for quick reply and sorry i have been on holidays.\nWhen i use show indices :\n.kibana\nlogstash-2015.03.18\nlogstash-2015.03.19\nlogstash-2015.03.20\nlogstash-2015.03.21\nlogstash-2015.03.22\nlogstash-2015.03.23\nlogstash-2015.03.24\nlogstash-2015.03.25\nlogstash-soap2015.03.18\nlogstash-soap2015.03.19\nlogstash-soap2015.03.20\nlogstash-soap2015.03.21\nlogstash-soap2015.03.22\nlogstash-soap2015.03.23\nlogstash-soap2015.03.24\nlogstash-soap2015.03.25\nFor now, I'm trying to delete the \"simple\" indices logstash-YYYY.MM.DD\n. Same message when using show indices :/\n2015-03-25 16:22:53,377 INFO      Job starting...\n2015-03-25 16:22:53,396 WARNING   No indices matched provided args.\nNo indices matched provided args.\n. My bad sorry.\nIt works fine,  forgot that i deleted manually old indices ..\nThank you so much for your time.\n. ",
    "peterskim12": "I like the concept. But I'm thinking that a simpler logo would be better? Just trying to think what would look good in a smaller (e.g. 200px square) form also...\n. Thanks for trying. :) Yeah better to leave this to the pros. Hopefully we'll have Curator stickers at some point!\n. This is great @untergeek! Thanks for working through all the issues to make this a reality. Will be super-helpful to the non-Python folks out there.\n. ",
    "pkr1234": "Correction:\nIndexes are closed but dry-run returns incorrect results. Also there is no output of close indexes like \"closing index ...\"\n. Hi,\nThe dry-run is showing the indexes that are already \"closed\". Please see my output. If I close the indexes (without the dry-run) and then dry-run again, it still shows the indexes it would close (they are already closed by the previous command)\n. Yes that would be great! Thanks.\n. Thanks for the detailed response. There are no cluster issues or duplicate snapshots or individual nodes connection problem etc. All instances have the same instance profile. I can see the metadata and snapshot-curator fine.\nI can also retrieve the JSON which shows that snapshot was taken fine directly from elasticsearch. Here's another type of the problem:\nUnable to take snapshots : transport error (whatever that means)\n```\n% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n11:36:49                                             Dload  Upload   Total   Spent    Left  Speed\n11:36:49          \n11:36:49              0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0179   179  179   179    0     0   3881      0 --:--:-- --:--:-- --:--:--  7782  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n11:36:49                                             Dload  Upload   Total   Spent    Left  Speed\n11:36:49          \n11:36:49            Now taking snapshots ..\n11:36:49              0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0179   179  179   179    0     0   3969      0 --:--:-- --:--:-- --:--:--  77822015-03-31 11:36:49,675 INFO      Job starting...\n11:36:49            2015-03-31 11:36:49,771 INFO      Matching all indices. Ignoring flags other than --exclude.\n11:36:54            2015-03-31 11:36:54,279 INFO      Snapshot name: curator-20150331103654\n11:40:56            2015-03-31 11:40:56,125 ERROR     Client raised a TransportError.\n11:40:56            Unable to take snapshots.\n11:40:56            Result: 1\n11:40:56            Failed: NonZeroResultCode: Result code was 1\n11:40:56            Execution failed: 43236: [Workflow step failures: {1=Dispatch failed on 1 nodes: [rndvmhsvr01: NonZeroResultCode: Result code was 1]}, Node failures: {xxxxxx01=[NonZeroResultCode: Result code was 1]}]\n```\nHowever the snapshots was taken fine. I can see snapshot-curator-20150331103654 and metadata-20150331103654.\nI also retrieved the state of the snapshot using curl:\n```\n curl -s -XGET \"http://myserver.lala.com:9200/_snapshot/my_repository/curator-20150331103654?pretty\"\n{\n  \"snapshots\" : [ {\n    \"snapshot\" : \"curator-20150331103654\",\n    \"indices\" : [ \"kibana-int\", \"logstash-2015.01.28\", \"logstash-2015.01.29\", \"logstash-2015.01.30\", \"logstash-2015.02.02\", \"logstash-2015.02.03\", \"logstash-2015.02.04\", \"logstash-2015.02.05\", \"logstash-2015.02.06\", \"logstash-2015.02.07\", \"logstash-2015.02.08\", \"logstash-2015.02.09\", \"logstash-2015.02.10\", \"logstash-2015.02.11\", \"logstash-2015.02.12\", \"logstash-2015.02.13\", \"logstash-2015.02.14\", \"logstash-2015.02.15\", \"logstash-2015.02.16\", \"logstash-2015.02.17\", \"logstash-2015.02.18\", \"logstash-2015.02.19\", \"logstash-2015.02.20\", \"logstash-2015.02.21\", \"logstash-2015.02.22\", \"logstash-2015.02.23\", \"logstash-2015.02.24\", \"logstash-2015.02.25\", \"logstash-2015.02.26\", \"logstash-2015.02.27\", \"logstash-2015.02.28\", \"logstash-2015.03.01\", \"logstash-2015.03.02\", \"logstash-2015.03.03\", \"logstash-2015.03.04\", \"logstash-2015.03.05\", \"logstash-2015.03.06\", \"logstash-2015.03.07\", \"logstash-2015.03.08\", \"logstash-2015.03.09\", \"logstash-2015.03.10\", \"logstash-2015.03.11\", \"logstash-2015.03.12\", \"logstash-2015.03.13\", \"logstash-2015.03.14\", \"logstash-2015.03.15\", \"logstash-2015.03.16\", \"logstash-2015.03.17\", \"logstash-2015.03.18\", \"logstash-2015.03.19\", \"logstash-2015.03.20\", \"logstash-2015.03.21\", \"logstash-2015.03.22\", \"logstash-2015.03.23\", \"logstash-2015.03.24\", \"logstash-2015.03.25\", \"logstash-2015.03.26\", \"logstash-2015.03.27\", \"logstash-2015.03.28\", \"logstash-2015.03.29\", \"logstash-2015.03.30\", \"logstash-2015.03.31\" ],\n    \"state\" : \"SUCCESS\",\n    \"start_time\" : \"2015-03-31T10:36:57.683Z\",\n    \"start_time_in_millis\" : 1427798217683,\n    \"end_time\" : \"2015-03-31T10:44:34.495Z\",\n    \"end_time_in_millis\" : 1427798674495,\n    \"duration_in_millis\" : 456812,\n    \"failures\" : [ ],\n    \"shards\" : {\n      \"total\" : 310,\n      \"failed\" : 0,\n      \"successful\" : 310\n    }\n  } ]\n}\n```\nBut curator is raising a transport error failing my script. \n. Also to answer your query:\n\n\n\nFirst, you mention that you're trying to delete metadata-curator-[timestamp] and snapshot-curator-[timestamp] and it deletes these. However, the name in the error is curator-20150213001500. Was this correct? If so, is that snapshot supposed to be deleted?\n\n\n\nI don't use any prefixes - just whatever defaults curator uses. All my snapshots have format like  snapshot-curator-yyyymmddhhmmss\nMetadata is like metadata-curator-yyyymmddhhmmss\n. Hi,\nThis is the log activity for snapshot creation. Looks like a bug as it seems curator is retrying for some reason and bombing out. Elasticsearch complains as a snapshot is already running with the same name.\n```\n[2015-04-01 00:16:07,494][WARN ][snapshots                ] [Lemuel Dorcas] [my_backup_s3_repository][curator-20150331231504] failed to create snapshot\norg.elasticsearch.snapshots.ConcurrentSnapshotExecutionException: [my_backup_s3_repository:curator-20150331231504] a snapshot is already running\n        at org.elasticsearch.snapshots.SnapshotsService$1.execute(SnapshotsService.java:183)\n        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:328)\n        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\n[2015-04-01 00:16:07,542][WARN ][snapshots                ] [Lemuel Dorcas] [my_backup_s3_repository][curator-20150331231504] failed to create snapshot\norg.elasticsearch.snapshots.ConcurrentSnapshotExecutionException: [my_backup_s3_repository:curator-20150331231504] a snapshot is already running\n        at org.elasticsearch.snapshots.SnapshotsService$1.execute(SnapshotsService.java:183)\n        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:328)\n        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\n[2015-04-01 00:16:07,568][WARN ][snapshots                ] [Lemuel Dorcas] [my_backup_s3_repository][curator-20150331231504] failed to create snapshot\norg.elasticsearch.snapshots.ConcurrentSnapshotExecutionException: [my_backup_s3_repository:curator-20150331231504] a snapshot is already running\n        at org.elasticsearch.snapshots.SnapshotsService$1.execute(SnapshotsService.java:183)\n        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:328)\n        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\n[2015-04-01 00:16:16,966][INFO ][snapshots                ] [Lemuel Dorcas] snapshot [my_backup_s3_repository:curator-20150331231504] is done\n[2015-04-01 01:00:01,711][INFO ][cluster.metadata         ] [Lemuel Dorcas] [logstash-2015.04.01] creating index, cause [auto(bulk api)], shards [5]/[1], mappings [default]\n```\nI also found this on google which is somewhat similar to the problem.\nhttp://grokbase.com/t/gg/elasticsearch/149p3bhs0x/1-2-2-snapshot-throws-error-but-still-completes\nI did a grep xception * on my logs and found many such occurrences.\n```\nLogCentral.ElasticSearch.log.2015-03-12:org.elasticsearch.snapshots.ConcurrentSnapshotExecutionException: [my_backup_s3_repository:curator-20150312084624] a snapshot is already running\nLogCentral.ElasticSearch.log.2015-03-12:org.elasticsearch.snapshots.InvalidSnapshotNameException: [my_backup_s3_repository:curator-20150312084624] Invalid snapshot name [curator-20150312084624], snapshot with such name already exists\nLogCentral.ElasticSearch.log.2015-03-13:org.elasticsearch.snapshots.ConcurrentSnapshotExecutionException: [my_backup_s3_repository:curator-20150313001500] a snapshot is already running\nLogCentral.ElasticSearch.log.2015-03-13:org.elasticsearch.snapshots.InvalidSnapshotNameException: [my_backup_s3_repository:curator-20150313001500] Invalid snapshot name [curator-20150313001500], snapshot with such name already exists\nLogCentral.ElasticSearch.log.2015-03-14:org.elasticsearch.snapshots.ConcurrentSnapshotExecutionException: [my_backup_s3_repository:curator-20150314001500] a snapshot is already running\nLogCentral.ElasticSearch.log.2015-03-14:org.elasticsearch.snapshots.InvalidSnapshotNameException: [my_backup_s3_repository:curator-20150314001500] Invalid snapshot name [curator-20150314001500], snapshot with such name already exists\nLogCentral.ElasticSearch.log.2015-03-16:org.elasticsearch.snapshots.ConcurrentSnapshotExecutionException: [my_backup_s3_repository:curator-20150316001500] a snapshot is already running\nLogCentral.ElasticSearch.log.2015-03-16:org.elasticsearch.snapshots.InvalidSnapshotNameException: [my_backup_s3_repository:curator-20150316001500] Invalid snapshot name [curator-20150316001500], snapshot with such name already exists\nLogCentral.ElasticSearch.log.2015-03-16:org.elasticsearch.snapshots.ConcurrentSnapshotExecutionException: [my_backup_s3_repository:curator-20150316084051] a snapshot is already running\nLogCentral.ElasticSearch.log.2015-03-16:org.elasticsearch.snapshots.InvalidSnapshotNameException: [my_backup_s3_repository:curator-20150316084051] Invalid snapshot name [curator-20150316084051], snapshot with such name already exists\nLogCentral.ElasticSearch.log.2015-03-17:org.elasticsearch.snapshots.ConcurrentSnapshotExecutionException: [my_backup_s3_repository:curator-20150317001500] a snapshot is already running\nLogCentral.ElasticSearch.log.2015-03-17:org.elasticsearch.snapshots.InvalidSnapshotNameException: [my_backup_s3_repository:curator-20150317001500] Invalid snapshot name [curator-20150317001500], snapshot with such name already exists\nLogCentral.ElasticSearch.log.2015-03-18:org.elasticsearch.snapshots.ConcurrentSnapshotExecutionException: [my_backup_s3_repository:curator-20150318001500] a snapshot is already running\nLogCentral.ElasticSearch.log.2015-03-18:org.elasticsearch.snapshots.InvalidSnapshotNameException: [my_backup_s3_repository:curator-20150318001500] Invalid snapshot name [curator-20150318001500], snapshot with such name already exists\nLogCentral.ElasticSearch.log.2015-03-19:org.elasticsearch.snapshots.ConcurrentSnapshotExecutionException: [my_backup_s3_repository:curator-20150319001500] a snapshot is already running\nLogCentral.ElasticSearch.log.2015-03-19:org.elasticsearch.snapshots.InvalidSnapshotNameException: [my_backup_s3_repository:curator-20150319001500] Invalid snapshot name [curator-20150319001500], snapshot with such name already exists\nLogCentral.ElasticSearch.log.2015-03-20:org.elasticsearch.snapshots.ConcurrentSnapshotExecutionException: [my_backup_s3_repository:curator-20150320001500] a snapshot is already running\nLogCentral.ElasticSearch.log.2015-03-20:org.elasticsearch.snapshots.InvalidSnapshotNameException: [my_backup_s3_repository:curator-20150320001500] Invalid snapshot name [curator-20150320001500], snapshot with such name already exists\nLogCentral.ElasticSearch.log.2015-03-21:org.elasticsearch.snapshots.ConcurrentSnapshotExecutionException: [my_backup_s3_repository:curator-20150321001500] a snapshot is already running\nLogCentral.ElasticSearch.log.2015-03-21:org.elasticsearch.snapshots.InvalidSnapshotNameException: [my_backup_s3_repository:curator-20150321001500] Invalid snapshot name [curator-20150321001500], snapshot with such name already exists\nLogCentral.ElasticSearch.log.2015-03-27:org.elasticsearch.snapshots.ConcurrentSnapshotExecutionException: [my_backup_s3_repository:curator-20150327001500] a snapshot is already running\nLogCentral.ElasticSearch.log.2015-03-27:org.elasticsearch.snapshots.InvalidSnapshotNameException: [my_backup_s3_repository:curator-20150327001500] Invalid snapshot name [curator-20150327001500], snapshot with such name already exists\nLogCentral.ElasticSearch.log.2015-03-27:org.elasticsearch.snapshots.ConcurrentSnapshotExecutionException: [my_backup_s3_repository:curator-20150327102947] a snapshot is already running\nLogCentral.ElasticSearch.log.2015-03-27:org.elasticsearch.snapshots.InvalidSnapshotNameException: [my_backup_s3_repository:curator-20150327102947] Invalid snapshot name [curator-20150327102947], snapshot with such name already exists\n```\nI don't see any errors related to AWS API call. Note the log entry my_backup_s3_repository:curator-20150331231504] is done. It completes fine but it seems curator is trying to resubmit the request whilst it is running and receiving an exception - my guess.\n. Regarding the delete, here are the logs for comparison:\nExecution log\n```\n0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0179   179  179   179    0     0   1282      0 --:--:-- --:--:-- --:--:--  7458  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n00:15:00                                             Dload  Upload   Total   Spent    Left  Speed\n00:15:00          \n00:15:00              0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0179   179  179   179    0     0   3920      0 --:--:-- --:--:-- --:--:--  74582015-03-27 00:15:00,864 INFO      Job starting...\n00:15:00            2015-03-27 00:15:00,913 INFO      Capturing snapshots of specified indices...\n00:15:00            2015-03-27 00:15:00,914 INFO      Snapshot will capture all indices\n00:15:00            2015-03-27 00:15:00,914 INFO      Snapshot name: curator-20150327001500\n00:16:35            2015-03-27 00:16:35,727 ERROR     Client raised a TransportError.  Error: TransportError(400, u'RemoteTransportException[[Lemuel Dorcas][inet[/10.11.102.53:9300]][cluster/snapshot/create]]; nested: InvalidSnapshotNameException[[my_backup_s3_repository:curator-20150327001500] Invalid snapshot name [curator-20150327001500], snapshot with such name already exists]; ')\n00:16:35            2015-03-27 00:16:35,728 INFO      Snapshots captured for specified indices.\n00:16:35            2015-03-27 00:16:35,729 INFO      Done in 0:01:34.890266.\n00:16:36            2015-03-27 00:16:36,080 INFO      Job starting...\n00:16:36            2015-03-27 00:16:36,127 INFO      Deleting specified snapshots...\n00:16:37            2015-03-27 00:16:37,993 INFO      curator-20150211001500 is within the threshold period (45 days).\n00:16:37            2015-03-27 00:16:37,994 INFO      curator-20150212001500 is within the threshold period (45 days).\n00:16:37            2015-03-27 00:16:37,995 INFO      curator-20150213001500 is within the threshold period (45 days).\n00:16:37            2015-03-27 00:16:37,996 INFO      curator-20150214001500 is within the threshold period (45 days).\n00:16:37            2015-03-27 00:16:37,997 INFO      curator-20150215001505 is within the threshold period (45 days).\n00:16:37            2015-03-27 00:16:37,997 INFO      curator-20150216001500 is within the threshold period (45 days).\n00:16:37            2015-03-27 00:16:37,998 INFO      curator-20150217001500 is within the threshold period (45 days).\n00:16:37            2015-03-27 00:16:37,999 INFO      curator-20150218001500 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,000 INFO      curator-20150219001500 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,000 INFO      curator-20150220001500 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,001 INFO      curator-20150221001500 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,002 INFO      curator-20150222001500 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,003 INFO      curator-20150223001500 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,003 INFO      curator-20150224001500 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,004 INFO      curator-20150225001500 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,005 INFO      curator-20150226001500 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,006 INFO      curator-20150227001502 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,006 INFO      curator-20150228001500 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,007 INFO      curator-20150301001500 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,007 INFO      curator-20150302001500 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,007 INFO      curator-20150303001500 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,008 INFO      curator-20150304001505 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,008 INFO      curator-20150305001500 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,008 INFO      curator-20150306001500 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,008 INFO      curator-20150307001504 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,008 INFO      curator-20150309001500 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,009 INFO      curator-20150310093704 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,009 INFO      curator-20150310123006 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,009 INFO      curator-20150311001501 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,009 INFO      curator-20150311001601 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,010 INFO      curator-20150311001900 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,010 INFO      curator-20150312001501 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,016 INFO      curator-20150312084624 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,016 INFO      curator-20150313001500 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,016 INFO      curator-20150314001500 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,016 INFO      curator-20150316001500 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,017 INFO      curator-20150316084051 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,017 INFO      curator-20150317001500 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,017 INFO      curator-20150318001500 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,017 INFO      curator-20150319001500 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,017 INFO      curator-20150320001500 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,018 INFO      curator-20150321001500 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,018 INFO      curator-20150322001501 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,018 INFO      curator-20150323001500 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,018 INFO      curator-20150324001509 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,019 INFO      curator-20150325001500 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,019 INFO      curator-20150325084815 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,019 INFO      curator-20150326001500 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,019 INFO      curator-20150326085926 is within the threshold period (45 days).\n00:16:38            2015-03-27 00:16:38,019 INFO      curator-20150327001500 is within the threshold period (45 days).\n00:17:45            Traceback (most recent call last):\n00:17:45              File \"/usr/bin/curator\", line 9, in \n00:17:45                load_entry_point('elasticsearch-curator==2.1.2', 'console_scripts', 'curator')()\n00:17:45              File \"/usr/lib/python2.6/site-packages/curator/curator_script.py\", line 383, in main\n00:17:45                arguments.func(client, argdict)\n00:17:45              File \"/usr/lib/python2.6/site-packages/curator/curator.py\", line 1216, in snapshot\n00:17:45                _op_loop(client, matching_snapshots, op=delete_snapshot, dry_run=dry_run, kwargs)\n00:17:45              File \"/usr/lib/python2.6/site-packages/curator/curator.py\", line 779, in _op_loop\n00:17:45                skipped = op(client, item, kwargs)\n00:17:45              File \"/usr/lib/python2.6/site-packages/curator/curator.py\", line 749, in delete_snapshot\n00:17:45                client.snapshot.delete(repository=repository, snapshot=snap)\n00:17:45              File \"/usr/lib/python2.6/site-packages/elasticsearch/client/utils.py\", line 68, in _wrapped\n00:17:45                return func(*args, params=params, kwargs)\n00:17:45              File \"/usr/lib/python2.6/site-packages/elasticsearch/client/snapshot.py\", line 40, in delete\n00:17:45                _make_path('_snapshot', repository, snapshot), params=params)\n00:17:45              File \"/usr/lib/python2.6/site-packages/elasticsearch/transport.py\", line 301, in perform_request\n00:17:45                status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)\n00:17:45              File \"/usr/lib/python2.6/site-packages/elasticsearch/connection/http_urllib3.py\", line 82, in perform_request\n00:17:45                self._raise_error(response.status, raw_data)\n00:17:45              File \"/usr/lib/python2.6/site-packages/elasticsearch/connection/base.py\", line 102, in _raise_error\n00:17:45                raise HTTP_EXCEPTIONS.get(status_code, TransportError)(status_code, error_message, additional_info)\n00:17:45            elasticsearch.exceptions.NotFoundError: TransportError(404, u'RemoteTransportException[[Lemuel Dorcas][inet[/10.11.102.53:9300]][cluster/snapshot/delete]]; nested: SnapshotMissingException[[my_backup_s3_repository:curator-20150210001500] is missing]; nested: FileNotFoundException[The specified key does not exist. (Service: Amazon S3; Status Code: 404; Error Code: NoSuchKey; Request ID: 9904EBCF27CFB8FF)]; ')\n00:17:45            Unable to purge snapshots.\n00:17:45            Result: 1\n00:17:45            Failed: NonZeroResultCode: Result code was 1\n```\nElasticsearch log\n[2015-03-27 00:16:37,460][WARN ][snapshots                ] [Lemuel Dorcas] [my_backup_s3_repository][curator-20150327001500] failed to create snapshot\norg.elasticsearch.snapshots.ConcurrentSnapshotExecutionException: [my_backup_s3_repository:curator-20150327001500] a snapshot is already running\n        at org.elasticsearch.snapshots.SnapshotsService$1.execute(SnapshotsService.java:183)\n        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:328)\n        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\n[2015-03-27 00:16:37,605][WARN ][snapshots                ] [Lemuel Dorcas] failed to create snapshot [my_backup_s3_repository:curator-20150327001500]\norg.elasticsearch.snapshots.InvalidSnapshotNameException: [my_backup_s3_repository:curator-20150327001500] Invalid snapshot name [curator-20150327001500], snapshot with such name already exists\n        at org.elasticsearch.repositories.blobstore.BlobStoreRepository.initializeSnapshot(BlobStoreRepository.java:225)\n        at org.elasticsearch.snapshots.SnapshotsService.beginSnapshot(SnapshotsService.java:276)\n        at org.elasticsearch.snapshots.SnapshotsService.access$600(SnapshotsService.java:88)\n        at org.elasticsearch.snapshots.SnapshotsService$1$1.run(SnapshotsService.java:202)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\n[2015-03-27 03:06:51,186][INFO ][cluster.metadata         ] [Lemuel Dorcas] [logstash-2015.03.27] update_mapping [cloudtraillog] (dynamic)\n[2015-03-27 03:06:51,241][INFO ][cluster.metadata         ] [Lemuel Dorcas] [logstash-2015.03.27] update_mapping [cloudtraillog] (dynamic)\nYou see there are no AWS specific exceptions. It is important to mention that I was running older version of curator on the 27th which was mainly complaining about \"delete\" intermittently - transport error\nWith the latest version of curator, the problem seems to have shifted to creation of snapshots - \"transport error\".\nIn both the cases, the error logged in ELasticsearch is the same - \"a snapshot is already running\". So it seems curator is retrying for some reason and bombing out.\n. 1.3.2 Elasticsearch\n2.3.0 AWS Plugin\nI've tried to take snapshot using curl on Elasticsearch which works fine. But wait_for_completion works differently. See below:\ncurl -m 3600 -XPUT \"myserver.lala.com:9200/_snapshot/my_backup_s3_repository/adhocsnapshot_2?wait_for_completion=true\"\nThis command completes after 40-50 sec and $? is 0. I then check the snapshot status.\ncurl -m 3600 -XGET \"myserver.lala.com:9200/_snapshot/my_backup_s3_repository/adhocsnapshot_2?pretty\"\nThis shows the snapshot status as \"STARTED\" and not SUCCESS. So wait_for_completion does not wait until the snapshot status is successful.\nI then quickly ran another snapshot\ncurl -m 3600 -XPUT \"myserver.lala.com:9200/_snapshot/my_backup_s3_repository/adhocsnapshot_3?wait_for_completion=true\"\nThis failed with \"a snapshot is already running\"\nI then waited a couple more minutes and ran the command again. It succeeded.\nI'm afraid I'll have to stop using curator and use curl commands to take snapshots instead which works. Something in curator seems to be retrying I believe. \n. I ran curator snapshot in debug mode.\n```\n2015-04-02 13:17:37,925 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-04-02 13:18:37,702 DEBUG     urllib3.connectionpool          _make_request:368  \"PUT /_snapshot/my_backup_s3_repository/curator-20150402121735?wait_for_completion=true HTTP/1.1\" 504 0\n2015-04-02 13:18:37,703 WARNING            elasticsearch       log_request_fail:81   PUT /_snapshot/my_backup_s3_repository/curator-20150402121735?wait_for_completion=true [status:504 request:59.777s]\n2015-04-02 13:18:37,703 DEBUG              elasticsearch       log_request_fail:89   > {\"indices\": \"kibana-int,logstash-2015.03.11,logstash-2015.03.12,logstash-2015.03.13,logstash-2015.03.14,logstash-2015.03.15,logstash-2015.03.16,logstash-2015.03.17,logstash-2015.03.18,logstash-2015.03.19,logstash-2015.03.20,logstash-2015.03.21,logstash-2015.03.22,logstash-2015.03.23,logstash-2015.03.24,logstash-2015.03.25,logstash-2015.03.26,logstash-2015.03.27,logstash-2015.03.28,logstash-2015.03.29,logstash-2015.03.30,logstash-2015.03.31,logstash-2015.04.01,logstash-2015.04.02\", \"partial\": false, \"ignore_unavailable\": false, \"include_global_state\": true}\n2015-04-02 13:18:37,704 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-04-02 13:18:37,704 INFO      urllib3.connectionpool              _get_conn:223  Resetting dropped connection: logcentral.test.costcutter.com\n2015-04-02 13:18:37,789 DEBUG     urllib3.connectionpool          _make_request:368  \"PUT /_snapshot/my_backup_s3_repository/curator-20150402121735?wait_for_completion=true HTTP/1.1\" 503 252\n2015-04-02 13:18:37,790 WARNING            elasticsearch       log_request_fail:81   PUT /_snapshot/my_backup_s3_repository/curator-20150402121735?wait_for_completion=true [status:503 request:0.086s]\n2015-04-02 13:18:37,790 DEBUG              elasticsearch       log_request_fail:89   > {\"indices\": \"kibana-int,logstash-2015.03.11,logstash-2015.03.12,logstash-2015.03.13,logstash-2015.03.14,logstash-2015.03.15,logstash-2015.03.16,logstash-2015.03.17,logstash-2015.03.18,logstash-2015.03.19,logstash-2015.03.20,logstash-2015.03.21,logstash-2015.03.22,logstash-2015.03.23,logstash-2015.03.24,logstash-2015.03.25,logstash-2015.03.26,logstash-2015.03.27,logstash-2015.03.28,logstash-2015.03.29,logstash-2015.03.30,logstash-2015.03.31,logstash-2015.04.01,logstash-2015.04.02\", \"partial\": false, \"ignore_unavailable\": false, \"include_global_state\": true}\n2015-04-02 13:18:37,791 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-04-02 13:18:37,832 DEBUG     urllib3.connectionpool          _make_request:368  \"PUT /_snapshot/my_backup_s3_repository/curator-20150402121735?wait_for_completion=true HTTP/1.1\" 503 252\n2015-04-02 13:18:37,833 WARNING            elasticsearch       log_request_fail:81   PUT /_snapshot/my_backup_s3_repository/curator-20150402121735?wait_for_completion=true [status:503 request:0.042s]\n2015-04-02 13:18:37,833 DEBUG              elasticsearch       log_request_fail:89   > {\"indices\": \"kibana-int,logstash-2015.03.11,logstash-2015.03.12,logstash-2015.03.13,logstash-2015.03.14,logstash-2015.03.15,logstash-2015.03.16,logstash-2015.03.17,logstash-2015.03.18,logstash-2015.03.19,logstash-2015.03.20,logstash-2015.03.21,logstash-2015.03.22,logstash-2015.03.23,logstash-2015.03.24,logstash-2015.03.25,logstash-2015.03.26,logstash-2015.03.27,logstash-2015.03.28,logstash-2015.03.29,logstash-2015.03.30,logstash-2015.03.31,logstash-2015.04.01,logstash-2015.04.02\", \"partial\": false, \"ignore_unavailable\": false, \"include_global_state\": true}\n2015-04-02 13:18:37,834 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-04-02 13:18:37,876 DEBUG     urllib3.connectionpool          _make_request:368  \"PUT /_snapshot/my_backup_s3_repository/curator-20150402121735?wait_for_completion=true HTTP/1.1\" 503 252\n2015-04-02 13:18:37,877 WARNING            elasticsearch       log_request_fail:81   PUT /_snapshot/my_backup_s3_repository/curator-20150402121735?wait_for_completion=true [status:503 request:0.043s]\n2015-04-02 13:18:37,877 DEBUG              elasticsearch       log_request_fail:89   > {\"indices\": \"kibana-int,logstash-2015.03.11,logstash-2015.03.12,logstash-2015.03.13,logstash-2015.03.14,logstash-2015.03.15,logstash-2015.03.16,logstash-2015.03.17,logstash-2015.03.18,logstash-2015.03.19,logstash-2015.03.20,logstash-2015.03.21,logstash-2015.03.22,logstash-2015.03.23,logstash-2015.03.24,logstash-2015.03.25,logstash-2015.03.26,logstash-2015.03.27,logstash-2015.03.28,logstash-2015.03.29,logstash-2015.03.30,logstash-2015.03.31,logstash-2015.04.01,logstash-2015.04.02\", \"partial\": false, \"ignore_unavailable\": false, \"include_global_state\": true}\n2015-04-02 13:18:37,878 ERROR       curator.api.snapshot        create_snapshot:66   Client raised a TransportError.\n```\nThere are two lines where it shows it is retrying the request.\n2015-04-02 13:18:37,833 WARNING            elasticsearch       log_request_fail:81   PUT /_snapshot/my_backup_s3_repository/curator-20150402121735?wait_for_completion=true [status:503 request:0.042s]\n...\n...\n2015-04-02 13:18:37,877 WARNING            elasticsearch       log_request_fail:81   PUT /_snapshot/my_backup_s3_repository/curator-20150402121735?wait_for_completion=true [status:503 request:0.043s]\nAny ideas why it is doing a PUT on the same snapshot again?\nIn contrast the curl command works fine.\ncurl -m 3600 -XPUT \"myserver.lala.com:9200/_snapshot/my_backup_s3_repository/adhocsnapshot_3?wait_for_completion=true\"\n echo $?\n0\n. I think you are right. Both curl and urllib are not waiting for the snapshot. Culprit could be AWS load balancer which sits in front of Elasticsearch non data nodes in my cluster. It has an idle timeout of 60 sec by default. I'll try to change it to a higher value and see if it solves the problem. \nWill try on tuesday as Easter holidays have started and let you know. Happy Easter!\nPS: curator version is the latest one. I did a pip install on the 27th.\n. Working fine now. Thanks.\n. Hi,\nAWS loadbalancer has a default idle timeout of 60 sec. It was just a case of editing load balancer setting and changing that to a higher value.\nMind you, this timeout was not affecting the actual snapshot. It is just that client connection was being terminated and there was no way of knowing whether the snapshot succeeded or not. \nPS: I have not looked at your detailed output. \n. ",
    "feltnerm": "That appears to have worked. Submitted a PR for your review.\n. Not my greatest code, but it's a start. \nI was/am a bit confused about how to test changing the host parameter (for the CLI args). It appears that the integration tests run against a locally running elasticsearch instance (is that true?). It'd be nice to mock this out somehow so we could see if using different hosts had effect.\n. Done! \nThanks for your assistance! You made it trivial to contribute my patch. A+++ would pull request again!\n. D'oh, that was for my eyes only.\n. I had to hard-code this as '/tmp' because I am running elasticsearch in a virtual machine and happen to have my ports forwarded (so localhost:9200 is hitting ES on my VM). The test utilities attempt to create a temporary folder, but that folder only exists on my host machine. '/tmp' seemed reasonable, for now.\n. Testing this against localhost does not really tell if the erroneous behavior I experienced is fixed. It'd be nice to be able to test against any host, even if the connection fails.\n. Good point,\nOne issue is that we need to know the tempdir of the target OS, not our own (target: where ES is running).\n. I'll add a comment, definitely. Since we already have self.args['location'] = tempfile.mkdtemp(suffix=dirname) in CuratorTestCase it seems to me that I can just use self.args['location'] in the actual test case.\n. ",
    "spuder": "+1 \nI'd like to see something like a .vimrc file or an environment variable so I don't have to type out my host every time. \ncurator --host somerediculiouslylonghostnameiregretmaking.example.com ...\nFor now, I'm using bash aliases. \n. Ah, forgot about localhost. I'll close in favor of 343\n. ",
    "corey-hammerton": "+1 for me too\nIt sure beats having to write a cron to get curator to:\n- delete old snapshots\n- delete old indices \n- snapshot the previous index (time-based data)\n- remove replicas for previous index\n- optimize previous index\n- reset replicas for optimized index\n. While this is a nice-to-have we can always write your own Python script that uses the Elasticsearch and Curator APIs to do what you want...\n. You need the --index argument BEFORE logstash-fortigate-2015.12.01\n. Especially when you have a scenario like retiring logstash data in a 30 day cycle then gets changed to a 21 day cycle. Using the --oldest flag won't help in that case.\n. The --repository flag must come BEFORE the indices keyword.\n. You will need to include the --timestring flag. Based on your last comment I would add something like the following option to your curator command:\n--timestring %d%m%Y-%H%M%s\n. What versions of Elasticsearch and Curator are you using? It's not completely clear from your description.\n. The issue is that Curator requires the elasticsearch python client and it is trying to find it on pypi. You can follow the guidelines listed in the following link to ensure all dependencies are installed.\nhttps://www.elastic.co/guide/en/elasticsearch/client/curator/current/installation.html#_installation_pip_installation_from_source\n. The next question that would probably come up is when to call self.client.cluster.health() when configured?\nShould it be after each allocation or replica change or after the entire loop is finished? I'm more inclined for after each change, the chunk list might be too long for the URL for large index lists.\n. Now I understand it, I misinterpreted the code in chunk_index_list.\nFor simplicity's sake let's keep the batch changes and pause after each batch. The code for that could be as follows:\nself.loggit.info('Updating index setting {0}'.format(self.body))\n        try:\n            index_lists = chunk_index_list(self.index_list.indices)\n            for l in index_lists:\n                self.client.indices.put_settings(\n                    index=to_csv(l), body=self.body\n                )\n                if self.wait_for_completion:\n                    self.client.cluster.health(\n                        index=to_csv(l),\n                        wait_for_relocation_shards=0,\n                        wait_for_status='green'\n                    )\n. The crontab treats '%' as a line terminator so they need to be escaped for the cronjob to work.\nCan you also provide a sample list of the index names you are trying to move?\n. Can you run curator show indices --time-unit days --older-than 5 --timestring '\\%Y.\\%m.\\%d'?\nCould also try putting double-quotes around the timestring value.\n. Try removing the quotes/apostrophes from your crontab.\n. You need to fill out the repository option in your logstash_snapshot.yml file\n. Curator starting from 4.2.0 adds a curator_cli command-line utility with much of the previous syntax available. The curator command works by relying on a configuration file and an action file, both in YAML format. \nYou should be able to run the curator_cli utility as follows:\n$ curator_cli --host XXXXXXXXXXXXX.aws.found.io --port 9200 --http_auth 'XXXXXXX:XXXXXXXX' show indices\nThe configuration file options can be viewed here.\nThe action file options can be viewed here.\n. Curator is only capable of index management, your request requires the Elasticsearch delete-by-query plugin or Bulk delete them. . How is the disk usage on your node(s)? Optimizing indices requires sufficient disk space to build the new, optimized segments, you may have to remove more indices to get below the watermark.. It depends on the size of your shards. Try running the following command on your cluster and note the output/logs (Replace esnode with the proper hostname/IP address):\ncurl -XPOST http://_esnode_:9200/logstash-2016.12.06/_optimize?max_num_segments=1. Try using the kibana filter instead of a pattern filter.. These filters are looking for indices whose names match, for example, docker-filebeat-2017.07.19 or docer-filebeat-2017.07.18\nYour age filter is looking for older than 24 hours but are only supplying a timestring format that's down to the day. Are your index names suffixed by the hour?. You need to set number_of_replicas setting instead of number of shards. ",
    "aliostad": "I must say - for whatever it's worth - I really do not like this and prefer the old.\n. ",
    "tarjei": "Hi, I have just signed the contributors agreement, but I would appreciate\nif you could just move the example yourself as I will probably spend quite\nsome time reformatting the command etc :)\nThanks for a nice tool.\n2015-04-12 19:18 GMT+02:00 Aaron Mildenstein notifications@github.com:\n\nAnd the Examples page is a sub-page under Getting Started\nhttp://www.elastic.co/guide/en/elasticsearch/client/curator/current/getting-started.html,\nif you were wondering.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/elastic/curator/pull/344#issuecomment-92089205.\n\n\n\nTarjei Huse\nMobil: 920 63 413\n. Ah, ok, odd I didn't find that :(\nThanks for helping!\n2015-04-12 19:41 GMT+02:00 Aaron Mildenstein notifications@github.com:\n\nThat would be a bit redundant, since there is little difference between\nwhat you've added and the first example on the Examples\nhttp://www.elastic.co/guide/en/elasticsearch/client/curator/current/examples.html\npage, which reads:\nDelete all indices with matching --timestring which are older than 30\ndays\ncurator --host 10.0.0.2 delete indices --older-than 30 --time-unit days --timestring '%Y.%m.%d'`\nIn previous versions of Curator, you would have needed as many runs of the\ncommand-line as you had prefixes, no matter the matching pattern. Not so\nany more! With this example, all indices with a pattern of %Y.%m.%d\ninside will be deleted. This is useful for managing both Logstash and\nMarvel indices with a single command, if you want the same retention period.\nThe only differences are that the existing example has a --host flag, and\nit's 30 days vs. 35.\nI'd be amenable to another example, adding logstash as a --prefix.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/elastic/curator/pull/344#issuecomment-92093721.\n\n\n\nTarjei Huse\nMobil: 920 63 413\n. ",
    "hronecviktor": "Alright, after some minor confusion with the subcommands in v3 we successfuly migrated. Thanks ^^\n. ",
    "rammaram06": "Thank you, upgrading setup tools fixed the issue. Thanks for your prompt reply :+1: \n. ",
    "karussell": "Okay, that is possible via index templates ...\n. Thanks for the response. I think this is critical for me and probably all who have relative fast feeding.\n. > How do you select the indices to remove, and add from the alias?\nMaybe you allow more actions per command or you implement state and add an additional 'commit' command (?)\n. ",
    "spider-network": "+1\n. ",
    "petterl": "Please make new version with this fix available for pip.\n. ",
    "wkoot": "Confirmed working, thanks!\n. Yes exactly. I know how Lucene and ES work, but having a few hundred snapshots severely slows down the snapshots endpoint (i.e. even ls on the repository folder becomes slow because of all the metadata-/snapshot- files).\nRunning named snapshots could be an option, but it would be a bit messy to administer. I guess I could add a suffix with the timespec though.\n. Oh, one part isn't covered by the config you suggested: this will not ensure at least X snapshots for a given timespan. What I mean is that curator will delete anything that is older than Y, although I would never want it to throw away every snapshot. Any idea?\n. The use case of the snapshots is \"something got fucked up, I want to roll back to before the fuckup\". But if the fuckup happened 1 hour ago, you'll restore a different snapshot than if it happened 5 days ago (i.e. most recent not-broken data). So you want to have a high density of snapshots that spans back for quite some time.\nBecause lots snapshots hardly give (disk) overhead (i.e. because of Lucene segments), the simple but stupid approach was to store shitloads of snapshots. That is however out of the question because it makes the entire ES cluster slow whenever a snapshot is started (which, being every 20 min, basically means the entire time).\nSo I need to find a way to keep generations of snapshots easily. I can't hardcode to exclude a certain snapshot because the relevancy is subject to a sliding window. That would require a wrapper that lists all snapshots and then finds out which ones to keep, ergo what to pass to --exclude.\nI understand your suggestion, but it seems more convenient to put this into a parameter instead of a wrapper around --exclude. Just like there is --disk-space which recursively \"keeps\" snapshots until a certain counter is out of bounds?\n. ",
    "teknogeek0": "Thanks untergeek, will give that try. Just curious, where might this metadata live? Is it in ElasticSearch now, or is it data that might be in my S3 bucket already? I'm happy to just wack both and re-snapshot as the cluster is new-ish and contains all its data still.\nThanks for the quick reply btw!\n- Chris\n. Great, thanks again!\n. About 10 days after I posted this I ended up rebuilding my cluster, and while I didn't completely tear down old indexes or data(more of an in-place host exchange), moving to new independent ElasticSearch masters seemed to, somehow, make this work again(or maybe its unrelated). Closing for now as I can't recreate at the moment.\n. ",
    "derjohn": "Hi,\nI had to wait until the error is back. Now it is :)\nThe is the (shortened) output of the curator with --debug. \n```\n2015-05-26 09:12:32,614 DEBUG     curator.cli.index_selection                indices:79   Filter: {'pattern': '^XXXXX.debug.$'}\n2015-05-26 09:12:32,615 INFO      curator.cli.index_selection                indices:83   Pruning Kibana-related indices to prevent accidental deletion.\n2015-05-26 09:12:32,615 DEBUG     curator.cli.index_selection                indices:110  ACTION: delete. INDICES: [u'XXXXX-cassandra-debug-2015.05.20', (many indexes).......\n2015-05-26 09:12:32,615 INFO          curator.api.delete         delete_indices:17   Deleting indices as a batch operation:\n2015-05-26 09:12:32,615 INFO          curator.api.delete         delete_indices:19   ---deleting index XXXXX-cassandra-debug-2015.05.20\n[...]\n2015-05-26 09:12:32,626 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-05-26 09:13:02,654 WARNING            elasticsearch       log_request_fail:82   DELETE http://localhost:9200/XXXXX-cassandra-debug-2015.05.20,............(many indexes)....[status:N/A request:30.028s]\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/elasticsearch/connection/http_urllib3.py\", line 74, in perform_request\n    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)\n  File \"/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.py\", line 597, in urlopen\n    _stacktrace=sys.exc_info()[2])\n  File \"/usr/local/lib/python2.7/dist-packages/urllib3/util/retry.py\", line 222, in increment\n    raise six.reraise(type(error), error, _stacktrace)\n  File \"/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.py\", line 544, in urlopen\n    body=body, headers=headers)\n  File \"/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.py\", line 376, in _make_request\n    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n  File \"/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.py\", line 304, in _raise_timeout\n    raise ReadTimeoutError(self, url, \"Read timed out. (read timeout=%s)\" % timeout_value)\nReadTimeoutError: HTTPConnectionPool(host=u'localhost', port=9200): Read timed out. (read timeout=30)\n2015-05-26 09:13:02,660 DEBUG              elasticsearch       log_request_fail:90   > None\n2015-05-26 09:13:02,660 ERROR         curator.api.delete         delete_indices:23   Error deleting one or more indices.  Check logs for more information.\n2015-05-26 09:13:02,660 WARNING        curator.cli.utils               exit_msg:69   Job did not complete successfully.\n```\n\"Read timed out\" ==> ? Might this be caused by an out of threads or such in ES ? I'll ty to  raise --timeout. \nBut: It would be nice, if curator tells by default the cause of the error, if the cause is an timeout.\nrgds,\nj\n. Hello,\nyes, I do have a lot of indexes, currently more than ~200 indexes:\n  elkelastic-1:~# curl -s 'localhost:9200/_cat/indices' | wc -l ==> 217\nI run the curator cleanup each night, that should delete ~ 7-14 Indexes each day.\nThis is not on single node, but a cluster consisting of 7 ES nodes with 30 GB RAM and SSD only, about 1.5TB indexes (3TB with replicas). The clusterstate is green, we have sometimes slow average query times, but not that slow, still expressable in low numbers of unit ms :)\nAnd again: The strange thing is, that I was able to delete the indexes one by one with e.g. the kopf plugin without any problems.\nI played with the timeout a littlebit and it looks like with more than 60 secs it works. Could that have to to with the fact that we use \"index.refresh_interval\": \"60s\" ? Might there we somekind of delay \"I give you an ack only after the refresh was done\" ?\nrgds,\nj\n. I'll wait for Recommend re-running with --debug on errors #390.\n. ",
    "LiangAllen": "Thank you, It has been working, it may be a configuration error\n. OK At first I think it is my version of the problem, but now this command has been executed successfully, I found my nfs configuration  problems, thank you very much\n. ",
    "petitout": "I have executed the command with the --debug option and it seems the \"%j\" is not well handled :\n...\n2015-06-08 12:14:06,973 DEBUG         curator.api.filter         get_date_regex:157  regex = \\d{4}\\-\\j\n2015-06-08 12:14:06,973 DEBUG          curator.cli.utils        filter_callback:170  REGEX = (?P<date>\\d{4}\\-\\j)\n...\n. Hi,\nAlright.\nyes it is always returning 3 digits.\nThanks\n. Alright it seems we can close it as it has been implemented. \nThanks\n. Yep just did it :-)\nThanks\nLe 9 juin 2015 15:57, \"Aaron Mildenstein\" notifications@github.com a\n\u00e9crit :\n\nHave you signed the CLA\nhttp://www.elasticsearch.org/contributor-agreement/? I can merge this\nafter you've signed.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/elastic/curator/pull/397#issuecomment-110527357.\n. I've signed the CLA but always mentioning I have not ...\nis there anything special to do ?\n. I updated the test case and ran \"run_tests.py\"\n. You're welcome !\nThanks for your work\n\nI've done all you wanted.\nI've signed the CLA after creating the pull request and before pushing the\nlatest commit.\nHowever, the CLA state didn't change\nThanks\nJulien\nOn Tue, Mar 15, 2016 at 11:23 AM, Aaron Mildenstein \nnotifications@github.com wrote:\n\nThanks for catching this! Don't know why I didn't think of it when I\nmerged the original PR, but clearly there aren't many people using this or\nI'd have heard about it sooner (or they just didn't check until today [image:\n:smile:] )\nCan you please:\n1. Sign the CLA\n2. Update the Contributors file\n3. Update the Changelog with a brief synopsis of what you've done\n   (under Bug Fixes)\nIf you sign the CLA before merging other commits, it should pick up the\nchange then. If not, I can manually verify.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/elastic/curator/pull/578#issuecomment-196960399\n. No I didn't .\nI signed it again today using :\nName : julien mancuso\nGithub login : petitout\nEmail : petitout@gmail.com\n\nOn Tuesday, March 15, 2016, Aaron Mildenstein notifications@github.com\nwrote:\n\nVery strange. I wonder why it doesn't recognize you. A manual CLA check\nsays you signed the CLA on 2015-06-09. Did you commit to a different\nElastic repository last year? Did you change the email address associated\nwith your Github account by chance?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/elastic/curator/pull/578#issuecomment-196971530\n. \n",
    "apatrida": "+1 ... seems odd curator cannot restore.  If you write your own restore elsewhere, you have to either use the API from Python (for selection) and add the simple snapshot restore command, or need to reproduce all the behaviour of index selection somewhere else + restore.\n. (or is there a philosophical reason snapshot restore is absent?)\n. Our work flow moves data to processing nodes using backup and restore so it is part of the automated process a few times per day or more.\n. ",
    "robin13": "+1 with same workflow as @jaysonminard \n. Nothing more useful:\n2015-06-19 08:33:47,162 DEBUG     urllib3.connectionpool          _make_request:383  \"PUT /_snapshot/es_support HTTP/1.1\" 500 420\n2015-06-19 08:33:47,162 WARNING            elasticsearch       log_request_fail:81   PUT /_snapshot/es_support [status:500 request:1.412s]\n2015-06-19 08:33:47,163 DEBUG              elasticsearch       log_request_fail:89   > {\"type\": \"s3\", \"settings\": {\"access_key\": \"****************************\", \"compress\": true, \"region\": \"eu-west-1\", \"bucket\": \"users.eu.elasticsearch.org\", \"max_snapshot_bytes_per_sec\": \"20mb\", \"base_path\": \"/my/path/to/marvel\", \"concurrent_streams\": 5, \"secret_key\": \"*****************************\", \"max_restore_bytes_per_sec\": \"20mb\"}}\n2015-06-19 08:33:47,163 ERROR     curator.cli.es_repo_mgr      create_repository:111  Unable to create repository es_support.  Exception   Check logs for more information.\n. @untergeek Yes - now a meaningful error is printed! \nThank you! :)\n. Thank you @untergeek ! :). This will only be useful when working with elasticsearch >= 6.1  For lower versions a \"pause\" action maybe useful: https://github.com/elastic/curator/issues/1327. ",
    "gmoskovicz": "+1 for this, while a restore should not be automated, it is great to have a simple command to restore a snapshot using curator.\n. @untergeek i faced the same issue with the --ssl-no-validate:\n'Connecting to %s using SSL with verify_certs=False is insecure.' % host) \n/opt/rh/python27/root/usr/lib/python2.7/site-packages/urllib3/util/ssl_.py:100: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning. \nInsecurePlatformWarning \n/opt/rh/python27/root/usr/lib/python2.7/site-packages/urllib3/connectionpool.py:789: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html \nInsecureRequestWarning) \n/opt/rh/python27/root/usr/lib/python2.7/site-packages/urllib3/connectionpool.py:789: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html \nInsecureRequestWarning)\nDoes this means that a certificate must be added?\n. @untergeek what do you think about this?\n. @untergeek What do you think?\n. @untergeek added compatibility matrix\n. @untergeek thanks for confirming this. The ERROR messages are benign anyway, so not a huge issue, but do you think that this could be addressed in Curator 3?\n. @untergeek \n\nIs there some reason a user is resistant to Curator 4?\n\nNot really. I was just curious. Thanks for confirming i think that we can close this issue \ud83d\ude3a \nThanks!\n. Oh yes @untergeek .. i was trying with %Y.%m.%d to show this issue. The real issue is shown when you have daily indices and monthly indices at the same time. One with an index pattern and another with another index pattern, and Curator tries to parse the timestamp from indices not matching the prefix which is odd. \n. Thanks for the detailed explanation @untergeek . This makes sense. I was curious around why the pattern wasn't being calculated after matching the prefix, but given how this is done it makes sense.\n. ++ For this now that we support a similar feature with the secure keystore in the products.\n@untergeek what is your take on this one?. ",
    "PhaedrusTheGreek": "+1 - this will be handy to automate DR cluster sync\n. @untergeek \nI am hitting this issue as well.  Being that most people work in daily indices and curate monthly, this unexpected behaviour will probably be encountered by most curator users.    Perhaps we could improve the documentation around how to calculate monthly curating?\nAlthough non-intuitive, it is agreed that this indeed ends up being the desired (and most correct) behaviour.\n. Confirmed - just bad syntax on my end.\n. @clintongormley might know.... Thanks @untergeek .   there may not be a great deal of demand for this functionality in Elasticsearch 2.x, especially since  will soon be moving to 5.x anyhow, under which the issue is fixed in Elasticsearch.     \nCurrently 2 workarounds exist:\n1.  use ES 5.x or \n2. use the Snapshot API directly . Upgrading to 2.4.2 does allow a workaround in that you can manually snapshot and restore the .security index,  But I just tested, and curator still doesn't see the .security index.. ",
    "Jmusa": "+1, for DR.\n. ",
    "pickypg": "@untergeek Before I trick everyone into using my terminology, this is really being called \"synced flush\" and not \"sync flush\". But huge +1 to get this in. Big win.\n. Also releasing a .exe variant would be super handy to hide all of the nuisances of installing Python in some Windows environments.\n. @untergeek That's what I meant. :+1: I remembered that you were making them :).\n. @untergeek Very satisfied. Thanks!\n. This paragraph mixes the use of double / single tickmarks and single quotes.\n. ",
    "tigryss": "+10\n. @untergeek: can i help somehow? kann ich mal dich unterst\u00fctzen irgendwie?\n. ",
    "marcotoma": "Hi untergeek,\nmany thanks for your quick reply. \nWhen i execute the command the following message is provided: \n2015-06-12 17:14:22,844 INFO      Job starting: show indices\n2015-06-12 17:14:22,844 WARNING   No indices matched provided args: {'regex': No\nne, 'newer_than': None, 'suffix': None, 'index': (), 'time_unit': 'days', 'older\n_than': 7, 'all_indices': False, 'timestring': 'm.d', 'exclude': (), 'prefix': '\n.marvel'}\nNo indices matched provided args.\nHere the output with --debug parameter:\nC:\\Python34\\Scripts>curator --debug show indices   --time-unit days --timestring\n m.d --prefix .marvel --older-than 7\n2015-06-12 17:16:28,692 DEBUG          curator.cli.utils        filter_callback:\n170  REGEX = ^.marvel.$\n2015-06-12 17:16:28,692 DEBUG          curator.cli.utils        filter_callback:\n173  Added filter: {'pattern': '^.marvel.$'}\n2015-06-12 17:16:28,692 DEBUG          curator.cli.utils        filter_callback:\n174  New list of filters: [{'pattern': '^.marvel.$'}]\n2015-06-12 17:16:28,692 DEBUG         curator.api.filter         get_date_regex:\n157  regex = \\m.\\d\n2015-06-12 17:16:28,692 DEBUG          curator.cli.utils        filter_callback:\n170  REGEX = (?P\\m.\\d)\n2015-06-12 17:16:28,692 DEBUG          curator.cli.utils        filter_callback:\n173  Added filter: {'method': 'older_than', 'time_unit': 'days', 'value': 7, 'gr\noupname': 'date', 'pattern': '(?P\\m.\\d)', 'timestring': 'm.d'}\n2015-06-12 17:16:28,692 DEBUG          curator.cli.utils        filter_callback:\n174  New list of filters: [{'pattern': '^.marvel.$'}, {'method': 'older_than',\n'time_unit': 'days', 'value': 7, 'groupname': 'date', 'pattern': '(?P\\m\\\n.\\d)', 'timestring': 'm.d'}]\n2015-06-12 17:16:28,692 INFO      curator.cli.index_selection                ind\nices:53   Job starting: show indices\n2015-06-12 17:16:28,692 DEBUG     curator.cli.index_selection                ind\nices:54   Params: {'timeout': 30, 'debug': True, 'use_ssl': False, 'loglevel': '\nINFO', 'host': 'localhost', 'logfile': None, 'http_auth': None, 'dry_run': False\n, 'port': 9200, 'logformat': 'default', 'url_prefix': '', 'master_only': False}\n2015-06-12 17:16:28,692 DEBUG          curator.cli.utils             get_client:\n109  kwargs = {'timeout': 30, 'debug': True, 'use_ssl': False, 'port': 9200, 'lo\nglevel': 'INFO', 'logfile': None, 'http_auth': None, 'dry_run': False, 'master_o\nnly': False, 'url_prefix': '', 'logformat': 'default', 'host': 'localhost'}\n2015-06-12 17:16:28,692 DEBUG         urllib3.util.retry               from_int:\n155  Converted retries value: False -> Retry(total=False, connect=None, read=Non\ne, redirect=0)\n2015-06-12 17:16:28,692 INFO      urllib3.connectionpool              _new_conn:\n203  Starting new HTTP connection (1): localhost\n2015-06-12 17:16:28,692 DEBUG     urllib3.connectionpool          _make_request:\n383  \"GET / HTTP/1.1\" 200 308\n2015-06-12 17:16:28,692 INFO               elasticsearch    log_request_success:\n63   GET http://localhost:9200/ [status:200 request:0.000s]\n2015-06-12 17:16:28,692 DEBUG              elasticsearch    log_request_success:\n65   > None\n2015-06-12 17:16:28,692 DEBUG              elasticsearch    log_request_success:\n66   < {\n  \"status\" : 200,\n  \"name\" : \"R3_Alpha\",\n  \"version\" : {\n    \"number\" : \"1.3.4\",\n    \"build_hash\" : \"a70f3ccb52200f8f2c87e9c370c6597448eb3e45\",\n    \"build_timestamp\" : \"2014-09-30T09:07:17Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"4.9\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n2015-06-12 17:16:28,708 DEBUG          curator.cli.utils          check_version:\n90   Detected Elasticsearch version 1.3.4\n2015-06-12 17:16:28,708 DEBUG         urllib3.util.retry               from_int:\n155  Converted retries value: False -> Retry(total=False, connect=None, read=Non\ne, redirect=0)\n2015-06-12 17:16:28,708 DEBUG     urllib3.connectionpool          make_request:\n383  \"GET //settings?expand_wildcards=open%2Cclosed HTTP/1.1\" 200 10527\n2015-06-12 17:16:28,708 INFO               elasticsearch    log_request_success:\n63   GET http://localhost:9200//settings?expand_wildcards=open%2Cclosed [statu\ns:200 request:0.000s]\n2015-06-12 17:16:28,708 DEBUG              elasticsearch    log_request_success:\n65   > None\n2015-06-12 17:16:28,708 DEBUG              elasticsearch    log_request_success:\n66   < {\".marvel-2015.05.14\":{\"settings\":{\"index\":{\"mapper\":{\"dynamic\":\"true\"},\"\nuuid\":\"vCMhfgutQrOADeA6LOttew\",\"marvel\":{\"index_format\":\"5\"},\"analysis\":{\"analyz\ner\":{\"default\":{\"type\":\"standard\",\"stopwords\":\"_none\"}}},\"number_of_replicas\":\"\n1\",\"number_of_shards\":\"1\",\"version\":{\"created\":\"1030499\"}}}},\".marvel-2015.05.27\n\":{\"settings\":{\"index\":{\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"sE_49w6lRO-NHHGk1a7P\nVQ\",\"marvel\":{\"index_format\":\"5\"},\"analysis\":{\"analyzer\":{\"default\":{\"type\":\"sta\nndard\",\"stopwords\":\"none\"}}},\"number_of_replicas\":\"1\",\"number_of_shards\":\"1\",\"\nversion\":{\"created\":\"1030499\"}}}},\"es_search_purchasing\":{\"settings\":{\"index\":{\"\nnumber_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"uuid\":\"6gZbcPgSTqikulj7cxFIJw\",\"\nversion\":{\"created\":\"1030499\"}}}},\"es_search_administration\":{\"settings\":{\"index\n\":{\"uuid\":\"AcLyQOOMS3CXLxrnl_PKXA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"\n5\",\"version\":{\"created\":\"1030499\"}}}},\".marvel-2015.06.05\":{\"settings\":{\"index\":\n{\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"F1L-1u_KQrWIJHyiKrCJaQ\",\"analysis\":{\"analyz\ner\":{\"default\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"number_of_replicas\":\"\n1\",\"marvel\":{\"index_format\":\"5\"},\"number_of_shards\":\"1\",\"version\":{\"created\":\"10\n30499\"}}}},\".marvel-2015.06.09\":{\"settings\":{\"index\":{\"mapper\":{\"dynamic\":\"true\"\n},\"uuid\":\"QxzG3mnDRWehkcbui7eV9g\",\"analysis\":{\"analyzer\":{\"default\":{\"type\":\"sta\nndard\",\"stopwords\":\"none\"}}},\"number_of_replicas\":\"1\",\"marvel\":{\"index_format\"\n:\"5\"},\"number_of_shards\":\"1\",\"version\":{\"created\":\"1030499\"}}}},\"es_search_order\n\":{\"settings\":{\"index\":{\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"uuid\":\"\nYHJ9ak33SSukHu1KjHw39A\",\"version\":{\"created\":\"1030499\"}}}},\".marvel-2015.05.26\":\n{\"settings\":{\"index\":{\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"30azx1MmTEqTojOk1kXPfA\n\",\"marvel\":{\"index_format\":\"5\"},\"analysis\":{\"analyzer\":{\"default\":{\"type\":\"stand\nard\",\"stopwords\":\"none\"}}},\"number_of_replicas\":\"1\",\"number_of_shards\":\"1\",\"ve\nrsion\":{\"created\":\"1030499\"}}}},\"es_search_customer\":{\"settings\":{\"index\":{\"uuid\n\":\"nE3h-KLGSIuFsbOu6mKZQw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"vers\nion\":{\"created\":\"1030499\"}}}},\".marvel-2015.06.03\":{\"settings\":{\"index\":{\"mapper\n\":{\"dynamic\":\"true\"},\"uuid\":\"vWg3k91rQ6uT4b-hdbO4sw\",\"marvel\":{\"index_format\":\"5\n\"},\"analysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\n\"number_of_replicas\":\"1\",\"number_of_shards\":\"1\",\"version\":{\"created\":\"1030499\"}}\n}},\".marvel-2015.05.20\":{\"settings\":{\"index\":{\"mapper\":{\"dynamic\":\"true\"},\"uuid\"\n:\"m_ANvmZpQ3uAIUlHJzrQXA\",\"marvel\":{\"index_format\":\"5\"},\"analysis\":{\"analyzer\":{\n\"default\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"number_of_replicas\":\"1\",\"n\number_of_shards\":\"1\",\"version\":{\"created\":\"1030499\"}}}},\".marvel-2015.05.21\":{\"s\nettings\":{\"index\":{\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"XhBfKdisTFOv4SoWHR7Ulw\",\"\nmarvel\":{\"index_format\":\"5\"},\"analysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\n\",\"stopwords\":\"none\"}}},\"number_of_replicas\":\"1\",\"number_of_shards\":\"1\",\"versi\non\":{\"created\":\"1030499\"}}}},\"es_search_campagne\":{\"settings\":{\"index\":{\"number_\nof_replicas\":\"1\",\"number_of_shards\":\"5\",\"uuid\":\"vOd_voTBTpWcPpLwNFE5GQ\",\"version\n\":{\"created\":\"1030499\"}}}},\".marvel-2015.06.01\":{\"settings\":{\"index\":{\"mapper\":{\n\"dynamic\":\"true\"},\"uuid\":\"YG5_VBANTcCLJXyM4OraYg\",\"marvel\":{\"index_format\":\"5\"},\n\"analysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"nu\nmber_of_replicas\":\"1\",\"number_of_shards\":\"1\",\"version\":{\"created\":\"1030499\"}}}},\n\"es_search_returnrefund\":{\"settings\":{\"index\":{\"number_of_replicas\":\"1\",\"number_\nof_shards\":\"5\",\"uuid\":\"go6xr41FTFOGq86ST2PQkA\",\"version\":{\"created\":\"1030499\"}}}\n},\".marvel-2015.05.25\":{\"settings\":{\"index\":{\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\n\"D9sCtA8xRqGtSCwZmzTLOw\",\"marvel\":{\"index_format\":\"5\"},\"analysis\":{\"analyzer\":{\"\ndefault\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"number_of_replicas\":\"1\",\"nu\nmber_of_shards\":\"1\",\"version\":{\"created\":\"1030499\"}}}},\"es_search_article\":{\"set\ntings\":{\"index\":{\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"uuid\":\"pYnaSbH\nnQvqaqMCwS05i6g\",\"version\":{\"created\":\"1030499\"}}}},\".marvel-2015.06.07\":{\"setti\nngs\":{\"index\":{\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"ufcJU3e_Qf6BlKjDPoYT5A\",\"anal\nysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"number_\nof_replicas\":\"1\",\"marvel\":{\"index_format\":\"5\"},\"number_of_shards\":\"1\",\"version\":\n{\"created\":\"1030499\"}}}},\".marvel-2015.06.10\":{\"settings\":{\"index\":{\"mapper\":{\"d\nynamic\":\"true\"},\"uuid\":\"SfcbAn-rR1C7WMVPeuGcfQ\",\"analysis\":{\"analyzer\":{\"default\n\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"number_of_replicas\":\"1\",\"marvel\":{\n\"index_format\":\"5\"},\"number_of_shards\":\"1\",\"version\":{\"created\":\"1030499\"}}}},\".\nmarvel-2015.05.18\":{\"settings\":{\"index\":{\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"wh9\nxnK-IR9WfnN03hn9KCw\",\"marvel\":{\"index_format\":\"5\"},\"analysis\":{\"analyzer\":{\"defa\nult\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"number_of_replicas\":\"1\",\"number\nof_shards\":\"1\",\"version\":{\"created\":\"1030499\"}}}},\".marvel-2015.06.11\":{\"settin\ngs\":{\"index\":{\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"59Hf-HZ_RWOTBFvZA4twJA\",\"analy\nsis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"stopwords\":\"_none\"}}},\"number_o\nf_replicas\":\"1\",\"marvel\":{\"index_format\":\"5\"},\"number_of_shards\":\"1\",\"version\":{\n\"created\":\"1030499\"}}}},\".marvel-2015.05.16\":{\"settings\":{\"index\":{\"mapper\":{\"dy\nnamic\":\"true\"},\"uuid\":\"skpG_ppNRwSs1hNC7f0dRA\",\"marvel\":{\"index_format\":\"5\"},\"an\nalysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"numbe\nr_of_replicas\":\"1\",\"number_of_shards\":\"1\",\"version\":{\"created\":\"1030499\"}}}},\".m\narvel-2015.05.17\":{\"settings\":{\"index\":{\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"BnqX\nHr5ERUCB_Q0Cm8wN8A\",\"marvel\":{\"index_format\":\"5\"},\"analysis\":{\"analyzer\":{\"defau\nlt\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"number_of_replicas\":\"1\",\"number_\nof_shards\":\"1\",\"version\":{\"created\":\"1030499\"}}}},\".marvel-2015.05.30\":{\"setting\ns\":{\"index\":{\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"n-2MxqK1Thi_751d0tFgIA\",\"marvel\n\":{\"index_format\":\"5\"},\"analysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"sto\npwords\":\"none\"}}},\"number_of_replicas\":\"1\",\"number_of_shards\":\"1\",\"version\":{\"\ncreated\":\"1030499\"}}}},\".marvel-2015.05.29\":{\"settings\":{\"index\":{\"mapper\":{\"dyn\namic\":\"true\"},\"uuid\":\"9GY4tlJlTh6SZ54IZP5daQ\",\"marvel\":{\"index_format\":\"5\"},\"ana\nlysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"number\nof_replicas\":\"1\",\"number_of_shards\":\"1\",\"version\":{\"created\":\"1030499\"}}}},\"es\nsearch_campagne_costs\":{\"settings\":{\"index\":{\"number_of_replicas\":\"1\",\"number_of\nshards\":\"5\",\"uuid\":\"_iG3lo7RSiK8p_P3IyNfcA\",\"version\":{\"created\":\"1030499\"}}}},\n\".marvel-2015.06.02\":{\"settings\":{\"index\":{\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"6\nX2RsKhXQVC0jYNu3wEr_A\",\"marvel\":{\"index_format\":\"5\"},\"analysis\":{\"analyzer\":{\"de\nfault\":{\"type\":\"standard\",\"stopwords\":\"_none\"}}},\"number_of_replicas\":\"1\",\"numb\ner_of_shards\":\"1\",\"version\":{\"created\":\"1030499\"}}}},\".marvel-2015.05.31\":{\"sett\nings\":{\"index\":{\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"UMCovfxfQpWpOieoDvSSRg\",\"mar\nvel\":{\"index_format\":\"5\"},\"analysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"\nstopwords\":\"none\"}}},\"number_of_replicas\":\"1\",\"number_of_shards\":\"1\",\"version\"\n:{\"created\":\"1030499\"}}}},\".marvel-2015.05.28\":{\"settings\":{\"index\":{\"mapper\":{\"\ndynamic\":\"true\"},\"uuid\":\"9pBfaYnaQpudZGVoxUv3mA\",\"marvel\":{\"index_format\":\"5\"},\"\nanalysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"num\nber_of_replicas\":\"1\",\"number_of_shards\":\"1\",\"version\":{\"created\":\"1030499\"}}}},\"\n.marvel-2015.05.24\":{\"settings\":{\"index\":{\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"Z\nAr0kBPQguqQyj2u5SfOA\",\"marvel\":{\"index_format\":\"5\"},\"analysis\":{\"analyzer\":{\"def\nault\":{\"type\":\"standard\",\"stopwords\":\"_none\"}}},\"number_of_replicas\":\"1\",\"numbe\nr_of_shards\":\"1\",\"version\":{\"created\":\"1030499\"}}}},\".marvel-kibana\":{\"settings\"\n:{\"index\":{\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"HEK6bNVcQRe4LWC-FopYIw\",\"marvel\":\n{\"index_format\":\"5\"},\"analysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"stopw\nords\":\"none\"}}},\"number_of_replicas\":\"1\",\"number_of_shards\":\"1\",\"version\":{\"cr\neated\":\"1030499\"}}}},\".marvel-2015.06.04\":{\"settings\":{\"index\":{\"mapper\":{\"dynam\nic\":\"true\"},\"uuid\":\"gpNWagzlR-icXj7jDAwSHg\",\"marvel\":{\"index_format\":\"5\"},\"analy\nsis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"number_o\nf_replicas\":\"1\",\"number_of_shards\":\"1\",\"version\":{\"created\":\"1030499\"}}}},\".marv\nel-2015.06.08\":{\"settings\":{\"index\":{\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"gV4nRtz\neRhGXOFuXgvJcxg\",\"analysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"stopwords\n\":\"none\"}}},\"number_of_replicas\":\"1\",\"marvel\":{\"index_format\":\"5\"},\"number_of_\nshards\":\"1\",\"version\":{\"created\":\"1030499\"}}}},\"es_search_warehouse_movements\":{\n\"settings\":{\"index\":{\"number_of_replicas\":\"1\",\"number_of_shards\":\"5\",\"uuid\":\"TGM\nxOr3pRcul84Z26ZAMdg\",\"version\":{\"created\":\"1030499\"}}}},\".marvel-2015.06.06\":{\"s\nettings\":{\"index\":{\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"qJJ9fknhTuGbwhYYhCnQYA\",\"\nanalysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"num\nber_of_replicas\":\"1\",\"marvel\":{\"index_format\":\"5\"},\"number_of_shards\":\"1\",\"versi\non\":{\"created\":\"1030499\"}}}},\".marvel-2015.06.12\":{\"settings\":{\"index\":{\"mapper\"\n:{\"dynamic\":\"true\"},\"uuid\":\"C85BhYEdQTCXT7OQlN1vMA\",\"analysis\":{\"analyzer\":{\"def\nault\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"number_of_replicas\":\"1\",\"marve\nl\":{\"index_format\":\"5\"},\"number_of_shards\":\"1\",\"version\":{\"created\":\"1030499\"}}}\n},\".marvel-2015.05.19\":{\"settings\":{\"index\":{\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\n\"uGUJvEkbTvWmuDj6e3C3pg\",\"marvel\":{\"index_format\":\"5\"},\"analysis\":{\"analyzer\":{\"\ndefault\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"number_of_replicas\":\"1\",\"nu\nmber_of_shards\":\"1\",\"version\":{\"created\":\"1030499\"}}}},\".marvel-2015.05.15\":{\"se\nttings\":{\"index\":{\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"Ucvu0uYGQW6Hqy3OAsMQOA\",\"m\narvel\":{\"index_format\":\"5\"},\"analysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\"\n,\"stopwords\":\"none\"}}},\"number_of_replicas\":\"1\",\"number_of_shards\":\"1\",\"versio\nn\":{\"created\":\"1030499\"}}}},\".marvel-2015.05.22\":{\"settings\":{\"index\":{\"mapper\":\n{\"dynamic\":\"true\"},\"uuid\":\"TDHb989PSCGiE2MlAEf-gw\",\"marvel\":{\"index_format\":\"5\"}\n,\"analysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"n\number_of_replicas\":\"1\",\"number_of_shards\":\"1\",\"version\":{\"created\":\"1030499\"}}}}\n,\".marvel-2015.05.23\":{\"settings\":{\"index\":{\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"\nTv-MdOMpSGqm7X0oaUc-UQ\",\"marvel\":{\"index_format\":\"5\"},\"analysis\":{\"analyzer\":{\"d\nefault\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"number_of_replicas\":\"1\",\"num\nber_of_shards\":\"1\",\"version\":{\"created\":\"1030499\"}}}}}\n2015-06-12 17:16:28,724 DEBUG          curator.api.utils            get_indices:\n27   All indices: ['.marvel-2015.06.09', '.marvel-2015.05.21', '.marvel-2015.06.\n07', '.marvel-2015.05.28', 'es_search_article', '.marvel-2015.06.05', '.marvel-2\n015.06.10', '.marvel-2015.06.02', '.marvel-2015.05.18', '.marvel-2015.06.04', '.\nmarvel-2015.05.16', '.marvel-2015.06.01', '.marvel-2015.06.08', '.marvel-kibana'\n, '.marvel-2015.05.19', '.marvel-2015.05.14', '.marvel-2015.05.22', '.marvel-201\n5.05.15', 'es_search_campagne_costs', 'es_search_campagne', '.marvel-2015.05.31'\n, 'es_search_customer', 'es_search_warehouse_movements', '.marvel-2015.05.30', '\n.marvel-2015.06.06', '.marvel-2015.05.25', 'es_search_purchasing', '.marvel-2015\n.05.17', '.marvel-2015.05.24', '.marvel-2015.05.26', 'es_search_order', '.marvel\n-2015.06.03', '.marvel-2015.06.12', '.marvel-2015.05.23', '.marvel-2015.06.11',\n'.marvel-2015.05.27', '.marvel-2015.05.20', 'es_search_returnrefund', 'es_search\nadministration', '.marvel-2015.05.29']\n2015-06-12 17:16:28,724 DEBUG     curator.cli.index_selection                ind\nices:60   Full list of indices: ['.marvel-2015.06.09', '.marvel-2015.05.21', '.m\narvel-2015.06.07', '.marvel-2015.05.28', 'es_search_article', '.marvel-2015.06.0\n5', '.marvel-2015.06.10', '.marvel-2015.06.02', '.marvel-2015.05.18', '.marvel-2\n015.06.04', '.marvel-2015.05.16', '.marvel-2015.06.01', '.marvel-2015.06.08', '.\nmarvel-kibana', '.marvel-2015.05.19', '.marvel-2015.05.14', '.marvel-2015.05.22'\n, '.marvel-2015.05.15', 'es_search_campagne_costs', 'es_search_campagne', '.marv\nel-2015.05.31', 'es_search_customer', 'es_search_warehouse_movements', '.marvel-\n2015.05.30', '.marvel-2015.06.06', '.marvel-2015.05.25', 'es_search_purchasing',\n '.marvel-2015.05.17', '.marvel-2015.05.24', '.marvel-2015.05.26', 'es_search_or\nder', '.marvel-2015.06.03', '.marvel-2015.06.12', '.marvel-2015.05.23', '.marvel\n-2015.06.11', '.marvel-2015.05.27', '.marvel-2015.05.20', 'es_search_returnrefun\nd', 'es_search_administration', '.marvel-2015.05.29']\n2015-06-12 17:16:28,724 DEBUG     curator.cli.index_selection                ind\nices:74   All filters: [{'pattern': '^.marvel.$'}, {'method': 'older_than', 'ti\nme_unit': 'days', 'value': 7, 'groupname': 'date', 'pattern': '(?P\\m.\\\nd)', 'timestring': 'm.d'}]\n2015-06-12 17:16:28,724 DEBUG     curator.cli.index_selection                ind\nices:79   Filter: {'pattern': '^.marvel._$'}\n2015-06-12 17:16:28,724 DEBUG     curator.cli.index_selection                ind\nices:79   Filter: {'method': 'older_than', 'time_unit': 'days', 'value': 7, 'gro\nupname': 'date', 'pattern': '(?P\\m.\\d)', 'timestring': 'm.d'}\n2015-06-12 17:16:28,724 WARNING   curator.cli.index_selection                ind\nices:136  No indices matched provided args: {'older_than': 7, 'time_unit': 'days\n', 'exclude': (), 'index': (), 'prefix': '.marvel', 'suffix': None, 'all_indices\n': False, 'regex': None, 'timestring': 'm.d', 'newer_than': None}\nNo indices matched provided args.\nC:\\Python34\\Scripts>pause\nPress any key to continue . . .\n. Ok, i tried to run it from the cmd and it works!! That's great..but... how can i schedule it now?\nseems that if i launch it from a bat file, the timestring characters are not read correctly\n. OK finally i figured it out!! \nEscape needed.  In the .bat file this line of code works like a charm:\ncurator show indices  --time-unit days --timestring \"%%Y.%%m.%%d\" --prefix .marvel --older-than 15\n. ",
    "lakomiec": "Some additional info:\n```\ncurator --version\ncurator, version 3.1.0\n. Maybe Elasticsearch version used causes any differences? I'm using 1.5.2, while in your test I see 1.6.0. Just a thought..\n.\npython\nPython 2.7.3 (default, Mar 13 2014, 11:03:55) \n[GCC 4.7.2] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n```\n```\n\n\n\nimport elasticsearch\nclient = elasticsearch.Elasticsearch(host='127.0.0.1', port='9200')\nclient.cat.indices().split('\\n')\n[u'green open content_2015_06_06 20 1 11863821  117 66.5gb 33.2gb ', u'green open content_2015_06_13 20 1 12302370  114 62.4gb 31.2gb ', u'green open content_2015_06_14 20 1 12348178  320   65gb 32.5gb ', u'green open content_2015_06_10 20 1 14069020  112 81.4gb 40.7gb ', u'green open foo                20 1        1    0 15.3kb  7.6kb ', u'green open foo1               20 1        1    0 13.7kb  6.8kb ', u'green open test                1 1        4    0   24kb   12kb ', u'green open content_2015_06_08 20 1 13259857  379 82.4gb 41.2gb ', u'green open content_2015_06_09 20 1 11537491  327   77gb 38.5gb ', u'green open content_2015_06_12 20 1 13315095 2235 79.8gb 39.9gb ', u'green open content_2015_06_16 20 1  9481933  146 54.4gb 26.8gb ', u'green open content_2015_06_15 20 1 14090736  403 78.7gb 39.3gb ', u'green open .kibana             1 1       17    1 85.3kb 42.6kb ', u'green open content_2015_06_11 20 1 12709713  253 73.2gb 36.6gb ', u'green open content_2015_06_07 20 1 11371618   76   62gb   31gb ', u'green open grafana-dash       20 1        1    0   29kb 14.5kb ', u'']\n```\n\n\n\n```\n\n\n\nlist(client.indices.get_settings(index='*', params={'expand_wildcards': 'open,closed'}))\n[u'test', u'.kibana', u'grafana-dash']\n.\n\n\n\ncurl -XGET http://localhost:9200/*/_settings?expand_wildcards=open%2Cclosed\n{\"test\":{\"settings\":{\"index\":{\"creation_date\":\"1419625453042\",\"uuid\":\"jNsHmax3S5GYwZ9ySxMLew\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"1\",\"version\":{\"created\":\"1040299\"}}}},\".kibana\":{\"settings\":{\"index\":{\"creation_date\":\"1417746278591\",\"uuid\":\"PrS7_zgnS5C4G1E-qNw8WQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"1\",\"version\":{\"created\":\"1040199\"}}}},\"grafana-dash\":{\"settings\":{\"index\":{\"creation_date\":\"1419297924758\",\"uuid\":\"hXYU0FQ2TFaA3dIJit1QAA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"20\",\"version\":{\"created\":\"1040299\"}}}}}\n```\n. Glad we're getting close to figuring this out. It's been pretty confusing.\n```\ncurl -XGET http://localhost:9200/*/_settings?pretty\n{\n  \"test\" : {\n    \"settings\" : {\n      \"index\" : {\n        \"creation_date\" : \"1419625453042\",\n        \"uuid\" : \"jNsHmax3S5GYwZ9ySxMLew\",\n        \"number_of_replicas\" : \"1\",\n        \"number_of_shards\" : \"1\",\n        \"version\" : {\n          \"created\" : \"1040299\"\n        }\n      }\n    }\n  },\n  \".kibana\" : {\n    \"settings\" : {\n      \"index\" : {\n        \"creation_date\" : \"1417746278591\",\n        \"uuid\" : \"PrS7_zgnS5C4G1E-qNw8WQ\",\n        \"number_of_replicas\" : \"1\",\n        \"number_of_shards\" : \"1\",\n        \"version\" : {\n          \"created\" : \"1040199\"\n        }\n      }\n    }\n  },\n  \"grafana-dash\" : {\n    \"settings\" : {\n      \"index\" : {\n        \"creation_date\" : \"1419297924758\",\n        \"uuid\" : \"hXYU0FQ2TFaA3dIJit1QAA\",\n        \"number_of_replicas\" : \"1\",\n        \"number_of_shards\" : \"20\",\n        \"version\" : {\n          \"created\" : \"1040299\"\n        }\n      }\n    }\n  }\n}\n```\n. Currently I have 7 nodes in ES cluster. I will try it on other nodes to make sure results are consistent.\nWhen I run above-mentioned curl call, I get the following response:\n```\ncurl -XGET http://localhost:9200/content_2015_06_08/_settings?pretty\n{\n  \"content_2015_06_08\" : {\n    \"settings\" : {\n      \"index\" : {\n        \"creation_date\" : \"1433721600104\",\n        \"uuid\" : \"4c4lNAJTSoeJI0VAu7r5tw\",\n        \"number_of_replicas\" : \"1\",\n        \"number_of_shards\" : \"20\",\n        \"version\" : {\n          \"created\" : \"1050299\"\n        }\n      }\n    }\n  }\n}\n. I just did the same tests on two other nodes and I'm getting exactly the same results.\n. It seems to work well for us and we are going to add more nodes to ES cluster soon. Anything wrong with that setup?\n.\ncurl -XGET http://localhost:9200/_all/_settings?pretty\n{\n  \"content_2015_06_06\" : {\n    \"settings\" : {\n      \"index\" : {\n        \"creation_date\" : \"1433548800017\",\n        \"uuid\" : \"4Q4XP4hHQHeIVrnYu6q4dg\",\n        \"number_of_replicas\" : \"1\",\n        \"number_of_shards\" : \"20\",\n        \"version\" : {\n          \"created\" : \"1050299\"\n        }\n      }\n    }\n  },\n  \"content_2015_06_13\" : {\n    \"settings\" : {\n      \"index\" : {\n        \"creation_date\" : \"1434153600041\",\n        \"uuid\" : \"hHD6cogdR4KIvkXa532UaA\",\n        \"number_of_replicas\" : \"1\",\n        \"number_of_shards\" : \"20\",\n        \"version\" : {\n          \"created\" : \"1050299\"\n        }\n      }\n    }\n  },\n  \"content_2015_06_14\" : {\n    \"settings\" : {\n      \"index\" : {\n        \"creation_date\" : \"1434240000017\",\n        \"uuid\" : \"qMBUi7dVRdqj9hU6oStqvw\",\n        \"number_of_replicas\" : \"1\",\n        \"number_of_shards\" : \"20\",\n        \"version\" : {\n          \"created\" : \"1050299\"\n        }\n      }\n    }\n  },\n  \"content_2015_06_10\" : {\n    \"settings\" : {\n      \"index\" : {\n        \"creation_date\" : \"1433894400009\",\n        \"uuid\" : \"f8QVe0duRAmlHqrOyB5U-g\",\n        \"number_of_replicas\" : \"1\",\n        \"number_of_shards\" : \"20\",\n        \"version\" : {\n          \"created\" : \"1050299\"\n        }\n      }\n    }\n  },\n  \"foo\" : {\n    \"settings\" : {\n      \"index\" : {\n        \"creation_date\" : \"1426823100564\",\n        \"uuid\" : \"JlSP1lxlQOKFOgLmCKFjow\",\n        \"number_of_replicas\" : \"1\",\n        \"number_of_shards\" : \"20\",\n        \"version\" : {\n          \"created\" : \"1040299\"\n        }\n      }\n    }\n  },\n  \"foo1\" : {\n    \"settings\" : {\n      \"index\" : {\n        \"creation_date\" : \"1426823386918\",\n        \"uuid\" : \"W_t0JwrzQyShmxG4tRetWw\",\n        \"number_of_replicas\" : \"1\",\n        \"number_of_shards\" : \"20\",\n        \"version\" : {\n          \"created\" : \"1040299\"\n        }\n      }\n    }\n  },\n  \"test\" : {\n    \"settings\" : {\n      \"index\" : {\n        \"creation_date\" : \"1419625453042\",\n        \"uuid\" : \"jNsHmax3S5GYwZ9ySxMLew\",\n        \"number_of_replicas\" : \"1\",\n        \"number_of_shards\" : \"1\",\n        \"version\" : {\n          \"created\" : \"1040299\"\n        }\n      }\n    }\n  },\n  \"content_2015_06_08\" : {\n    \"settings\" : {\n      \"index\" : {\n        \"creation_date\" : \"1433721600104\",\n        \"uuid\" : \"4c4lNAJTSoeJI0VAu7r5tw\",\n        \"number_of_replicas\" : \"1\",\n        \"number_of_shards\" : \"20\",\n        \"version\" : {\n          \"created\" : \"1050299\"\n        }\n      }\n    }\n  },\n  \"content_2015_06_09\" : {\n    \"settings\" : {\n      \"index\" : {\n        \"creation_date\" : \"1433808000032\",\n        \"uuid\" : \"A8OrZIX4Ta2Ii1FWfFmw0Q\",\n        \"number_of_replicas\" : \"1\",\n        \"number_of_shards\" : \"20\",\n        \"version\" : {\n          \"created\" : \"1050299\"\n        }\n      }\n    }\n  },\n  \"content_2015_06_12\" : {\n    \"settings\" : {\n      \"index\" : {\n        \"creation_date\" : \"1434067200111\",\n        \"uuid\" : \"VhyXVU6nSqWD2cffa-osPA\",\n        \"number_of_replicas\" : \"1\",\n        \"number_of_shards\" : \"20\",\n        \"version\" : {\n          \"created\" : \"1050299\"\n        }\n      }\n    }\n  },\n  \"content_2015_06_16\" : {\n    \"settings\" : {\n      \"index\" : {\n        \"creation_date\" : \"1434412800038\",\n        \"uuid\" : \"Yr7aDfglSfyiQyNSvIaMtg\",\n        \"number_of_replicas\" : \"1\",\n        \"number_of_shards\" : \"20\",\n        \"version\" : {\n          \"created\" : \"1050299\"\n        }\n      }\n    }\n  },\n  \"content_2015_06_15\" : {\n    \"settings\" : {\n      \"index\" : {\n        \"creation_date\" : \"1434326400037\",\n        \"uuid\" : \"emJoU2kCQ_SthmYJd6BESw\",\n        \"number_of_replicas\" : \"1\",\n        \"number_of_shards\" : \"20\",\n        \"version\" : {\n          \"created\" : \"1050299\"\n        }\n      }\n    }\n  },\n  \".kibana\" : {\n    \"settings\" : {\n      \"index\" : {\n        \"creation_date\" : \"1417746278591\",\n        \"uuid\" : \"PrS7_zgnS5C4G1E-qNw8WQ\",\n        \"number_of_replicas\" : \"1\",\n        \"number_of_shards\" : \"1\",\n        \"version\" : {\n          \"created\" : \"1040199\"\n        }\n      }\n    }\n  },\n  \"content_2015_06_11\" : {\n    \"settings\" : {\n      \"index\" : {\n        \"creation_date\" : \"1433980800017\",\n        \"uuid\" : \"Wbq3VDaTS1uFjsC5KBTA7A\",\n        \"number_of_replicas\" : \"1\",\n        \"number_of_shards\" : \"20\",\n        \"version\" : {\n          \"created\" : \"1050299\"\n        }\n      }\n    }\n  },\n  \"content_2015_06_07\" : {\n    \"settings\" : {\n      \"index\" : {\n        \"creation_date\" : \"1433635200024\",\n        \"uuid\" : \"JXxcavPDSbqdYKtXJ4H4Iw\",\n        \"number_of_replicas\" : \"1\",\n        \"number_of_shards\" : \"20\",\n        \"version\" : {\n          \"created\" : \"1050299\"\n        }\n      }\n    }\n  },\n  \"grafana-dash\" : {\n    \"settings\" : {\n      \"index\" : {\n        \"creation_date\" : \"1419297924758\",\n        \"uuid\" : \"hXYU0FQ2TFaA3dIJit1QAA\",\n        \"number_of_replicas\" : \"1\",\n        \"number_of_shards\" : \"20\",\n        \"version\" : {\n          \"created\" : \"1040299\"\n        }\n      }\n    }\n  }\n}\n.\ngrep -E '^[^#]' elasticsearch.yml\ncluster.name: sunshine\nnode.name: \"host0043.sunshine.com\"\nindex.number_of_shards: 20\nindex.number_of_replicas: 1\npath.data: /d0/elasticsearch/data\npath.work: /d0/elasticsearch/work\nhttp.cors.enabled: true\ndiscovery.zen.ping.multicast.enabled: false\ndiscovery.zen.ping.unicast.hosts: [ \"host0043.sunshine.com\", \"host0044.sunshine.com\", \"host0045.sunshine.com\" ]\nindex.search.slowlog.threshold.query.warn: 10s\nindex.search.slowlog.threshold.query.info: 5s\nindex.search.slowlog.threshold.query.debug: 2s\nindex.search.slowlog.threshold.query.trace: 1000ms\nindex.search.slowlog.threshold.fetch.warn: 4000ms\nindex.search.slowlog.threshold.fetch.info: 2000ms\nindex.search.slowlog.threshold.fetch.debug: 1000ms\nindex.search.slowlog.threshold.fetch.trace: 500ms\nindex.indexing.slowlog.threshold.index.warn: 10s\nindex.indexing.slowlog.threshold.index.info: 5s\nindex.indexing.slowlog.threshold.index.debug: 2s\nindex.indexing.slowlog.threshold.index.trace: 1000ms\nmonitor.jvm.gc.young.warn: 1000ms\nmonitor.jvm.gc.young.info: 700ms\nmonitor.jvm.gc.young.debug: 400ms\nmonitor.jvm.gc.old.warn: 10s\nmonitor.jvm.gc.old.info: 5s\nmonitor.jvm.gc.old.debug: 2s\n```\n. I appreciate your comments, suggestions and additional information you have provided regarding Elasticsearch shards configuration. I will initiate a brainstorm internally :)\n. I forgot - no plugins installed.\n. It works properly for me with that change - thanks @untergeek !\n. ",
    "burtonator": "\nGenerally speaking, the default 5+1 is more than enough for performance and scalability. I would never recommend going beyond 1 shard per node (ignoring replicas). The reason is that we recommend never having more than 300 shards per node (any combination of primaries & replicas to equal that number).\n\n...  I think the main reason is that ES doesn't support shard splitting.  If it did , we would just use 1 shard per node.  But we're expecting this install to grow to a fairly large size.  Probably a 50-100 node cluster over the next 12 months if everything goes well.\nWe don't have anything in place to manually split the shards. I think this is one of the main problems with ES right now - that it doesn't support shard splitting.\nWhich is ironic that it's not elastic.. because you have to configure it statically with the number of shards to scale.\nAlso, some of the community documentation is seemingly counter to your recommendation unfortunately... that ES can easily handle shards and that they are cheap so just use a lot of them.  This is the first I've heard about the memory limits. \nIn practice, I don't see this limit.  Our search overhead.. we're a bit tight at 5GB of heap right now but I think that's expected at the moment.  We're deploying more hardware at the moment to resolve it and should have more data by the end of the week.\n. Oh.. and if you have any pointers to documentation on this I'd really appreciate it.. \n. ",
    "chris-david-taylor": "Hi Untergeek,\nThanks for getting back to me so soon!  I'll spin up a free VM and post back how I get on.\nChris,\n. Apologies for the delay getting back to you. Okay, indeed that was the solution to the problem.\nCheers,\nChris.\n. ",
    "TinLe": "Sure.  But notice that older-than is not working.   I wanted to delete all indices older than 30 days.   It deletes everything.\nSo my question is:  Is this expected?   Eg \"--older-than\" does not work with \"--all-indices\" and I'd have to use regex to delete all indices older than 30 days for example.\n. OK.  That answers my question then.  Thank you.\nI'll have to play with the flags.  I wanted to delete all indices older than certain number of dates, but there maybe other conditions.   I guess I can use regex in combination with other flags.\n. Ooooh.... d'oh! :-)\nIt's a... little bit confusing.   Thank you for the explanation.   To be honest, I was surprised when it started deleting all my indices late last night.   Just glad I was on my test server.\n. Our retention policy is 31 days and we have plenty of disk space for the indices.  This particular cluster contain wrong indices settings and index mappings for the older (more than 7 days ago), so I was removing those in preparation for re-indexing.\nThe cluster is actually running on normal port 9200.   I am port forwarding to the cluster in production via a bastion host via port 9217 on my desktop (so, desktop:9217 --> production:9200).\nNone of the indices have replicas, but the number of shards for the indices are different depening on their  expected daily size (in order to keep shard size less than 45GB if possible).   So number of shards is between 3 to 40 per daily index (we have indices that hovers around 3+ TB daily).\nI didn't use the --debug flag, but I will do so next time.   I have other similar clusters in other datacenters that I need to do cleanup on, so I'll have more data for you.\nI did check my ES cluster log and see nothing extraordinary.  I saw the timeout errors while the cluster was in yellow state (it was initializing/creating new daily indices), but I expected I expected that.\nI have 30 nodes (30 x 64GB RAM in this cluster).    I have no issues with the clusters for months.  Rock solid.    I was just surprised when deleting and getting ok status, but upon checking, I find that the indices reported as deleted are still there :-)\n. I agree with too many shards is not good.   This is also part of the reason I am doing cleanup on this particular cluster.   I've migrated the large indices off and keeping the max number of shards to 8+0 for the larger indices, the rest are kept at 3+0.    This cluster has been stable and working fine for several months now.  Even with the larger indices on it.   I am moving them off to help distribute the load and increase performance.\nWith all that said...   Instead of reporting deletion OK, perhaps curator can say that deletion has been queued OK, but to check later to confirm.   That would seem to be more accurate.   All of my maintenance are done via scripts, but if my scripts can not trust the results curator is returning, it would be a problem.\nIt explains why some of my clusters are running out of disk space even with maintenance script running nightly.   I have now added in a loop to run script 10 times or until curator report there is no matching indices to ensure that the indices are deleted.\n. I get 900K size for cluster state.\n$ curl -s -XGET 'http://localhost:9200/_cluster/state?pretty' > /tmp/state.json\n$ ls -la /tmp/state.json\n-rw-r--r-- 1 tin user 953142 Jun 26 18:32 /tmp/state.json\nAh, without the pretty flag.  It's half the size.\n$ curl -s -XGET 'http://localhost:9200/_cluster/state' > /tmp/state.json\n$ ls -la /tmp/state.json\n-rw-r--r-- 1 tinle tinle 403267 Jun 26 21:04 /tmp/state.json\n. 30 nodes (64G RAM), maxheap is 24G.   My monitoring system tells me on average, they use around 9G, peaking to 16G before GC kicks in.\n. I have 16x1TB SAS drives.  14 are in RAID6 for data, 2 in RAID1 for boot, root, /home.\nI/O on RAID6 volume is:\nRead: max: 1.65MB, avg:160KB   (ops: max: 33.25/s, avg: 3.27/s)\nWrite: max: 8.8MB, avg: 6.44MB   (ops: max: 2.14K/s, avg: 1.57k/s)\nAverage is 16 shards per node, total of 479 shards across 30 nodes.\n. I tried https://github.com/elastic/curator/pull/423 and it does seem to work.   I'll do some more testing to see how it does on all my clusters.\nYes, RAID6 is partially for mitigating not using replica and also because we wanted storage size.\n. I've been out of office, just got back yesterday.   Still catching up on 1\nweek's worth of email :-(.   I'll update this thread asap.\nOn Tue, Jul 14, 2015 at 9:20 AM, Aaron Mildenstein <notifications@github.com\n\nwrote:\n@TinLe https://github.com/TinLe any news on this?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/elastic/curator/issues/420#issuecomment-121295432.\n. Confirmed that latest v3.2.3 works with the large clusters I have.   Just deleted a month's worth of indices.\n. \n",
    "jrmycanady": "I've added myself to the CONTRIBUTORS file and signed the contributor's agreement. Let me know if there is anything else I need to do! \n. ",
    "t-pascal": "My use case for this has gone away, so I won't be actively developing it. :)\n. ",
    "trevorjwilliams": "I may have some insight here, having just run into the issue myself, and it doesn't require a massively-overloaded server.\nWith Curator's switchover to using the bulk API instead of acting on individual indices, operations, such as the bulk deletion of a bunch of huge indices, will take place over a single long-lived request, which can timeout.  Bumping up the --timeout option fixes this on the client side... but not on the server side.  I executed a bulk deletion via Curator while tailing the master node's logs, and saw this:\n[2015-07-16 14:10:49,269][INFO ][cluster.metadata         ] [logstashsearch103-master-01] [common-2015.04.30] deleting index\n[2015-07-16 14:10:59,057][INFO ][cluster.metadata         ] [logstashsearch103-master-01] [common-2015.05.01] deleting index\n[2015-07-16 14:11:05,813][INFO ][cluster.metadata         ] [logstashsearch103-master-01] [common-2015.05.02] deleting index\n[2015-07-16 14:11:06,932][INFO ][cluster.metadata         ] [logstashsearch103-master-01] [common-2015.05.03] deleting index\n[2015-07-16 14:11:07,898][INFO ][cluster.metadata         ] [logstashsearch103-master-01] [common-2015.05.04] deleting index\n[2015-07-16 14:11:19,269][DEBUG][action.admin.indices.delete] [logstashsearch103-master-01] [common-2015.05.05] failed to delete index\norg.elasticsearch.cluster.metadata.ProcessClusterEventTimeoutException: failed to process cluster event (delete-index [common-2015.05.05]) within 30s\n        at org.elasticsearch.cluster.service.InternalClusterService$2$1.run(InternalClusterService.java:278)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n[2015-07-16 14:11:19,272][DEBUG][action.admin.indices.delete] [logstashsearch103-master-01] [common-2015.05.09] failed to delete index\norg.elasticsearch.cluster.metadata.ProcessClusterEventTimeoutException: failed to process cluster event (delete-index [common-2015.05.09]) within 30s\n        at org.elasticsearch.cluster.service.InternalClusterService$2$1.run(InternalClusterService.java:278)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nSo, --timeout extends the timeout for the client side, but the server side is unaffected.  You get a success message on the client side, presumably because you get a response from the server within the specified timeframe, but sadly the response is a timeout message.  It's likely, of course, that some of the indices you requested to be deleted were actually deleted, but probably not all of them.\nFortunately, I think there should be a trivial solution here: you can extend the server-side timeout for these operations with the master_timeout=(value) GET string argument.  If this were set and inherited the value passed from --timeout, I believe this would resolve this issue.\n. ",
    "dellis23": "For what it's worth, I ran into it also :-)  Thanks for the fix!\n. ",
    "joerayme": "Sorry for the delay in replying. We think it was just coincidence that we saw failures when enabling the VPC endpoint \u2013 we cleaned up our S3 bucket and it's all going fine now. Thanks for your help.\n. ",
    "Rumbles": "It's a single node cluster, my ES output uses the http protocol\nAlso I am using logstash-1.5.1-1 in case it's important!\nOn 10 July 2015 at 14:55, Aaron Mildenstein notifications@github.com\nwrote:\n\nThis is a bug. You've encountered an error I have not anticipated.\nThe flow here is that when you send a command to close indices, and you\nare using Elasticsearch 1.6+, it will try to seal (synced flush) open\nindices before closing them.\nWhat's happened is that, in trying to seal an index, Curator got an\nAttributeError exception, where I only ever anticipated TransportError\nexceptions. I cannot explain why you received an AttributeError exception,\nhowever. That sounds like something else is going on. Better error-type\ncapturing will allow me to display the results of the AttributeError.\nAre all nodes in your cluster running 1.6.0? Are your logstash\ninstances using the node protocol, or the http protocol? If node, it means\nthat not all of your cluster is running ES 1.6+ (as Logstash's client node\nis still probably ES 1.5-ish, depending on what version of Logstash you're\nusing. That's my current stab in the dark.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/elastic/curator/issues/429#issuecomment-120415676.\n. As discussed in IRC, I ran in to the following issue when running the update http://pastebin.centos.org/30266/\n. Hi,\n\nI have installed curator from the repo provided here:\nhttps://www.elastic.co/guide/en/elasticsearch/client/curator/current/installation.html#yum-repository\nI now have 3.2.2 installed and it appears to be working perfectly.\nIf I have any further troubles I will raise a fresh issue, thanks for the help!\n. I'm below the water mark... How much space is required? Does it depends on\nthe size of index?\nOn 28 Dec 2016 18:06, \"Corey Hammerton\" notifications@github.com wrote:\n\nHow is the disk usage on your node(s)? Optimizing indices requires\nsufficient disk space to build the new, optimized segments, you may have to\nremove more indices to get below the watermark.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/elastic/curator/issues/857#issuecomment-269515561,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AH2vUvrMqNYp8H8IpfdhvgZDfCZlkvEjks5rMqUogaJpZM4LWyed\n.\n. \n",
    "romale": "curator --version\ncurator, version 3.2.3\ncurator close indices --time-unit days --older-than 30 --timestring %Y.%m.%d --prefix logstash-netflow5-\n2015-07-17 15:10:53,069 INFO      Job starting: close indices\n2015-07-17 15:11:27,084 WARNING   Non-fatal error encountered.\nTraceback (most recent call last):\n  File \"/usr/bin/curator\", line 11, in \n    sys.exit(main())\n  File \"/usr/lib/python2.7/site-packages/curator/curator.py\", line 5, in main\n    cli( obj={ \"filters\": [] } )\n  File \"/usr/lib/python2.7/site-packages/click/core.py\", line 664, in call\n    return self.main(_args, _kwargs)\n  File \"/usr/lib/python2.7/site-packages/click/core.py\", line 644, in main\n    rv = self.invoke(ctx)\n  File \"/usr/lib/python2.7/site-packages/click/core.py\", line 991, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/usr/lib/python2.7/site-packages/click/core.py\", line 991, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/usr/lib/python2.7/site-packages/click/core.py\", line 837, in invoke\n    return ctx.invoke(self.callback, _ctx.params)\n  File \"/usr/lib/python2.7/site-packages/click/core.py\", line 464, in invoke\n    return callback(_args, **kwargs)\n  File \"/usr/lib/python2.7/site-packages/curator/cli/index_selection.py\", line 148, in indices\n    retval = do_command(client, ctx.parent.info_name, working_list, ctx.parent.params, master_timeout)\n  File \"/usr/lib/python2.7/site-packages/curator/cli/utils.py\", line 217, in do_command\n    return close(client, indices)\n  File \"/usr/lib/python2.7/site-packages/curator/api/close.py\", line 38, in close\n    return close_indices(client, indices)\n  File \"/usr/lib/python2.7/site-packages/curator/api/close.py\", line 19, in close_indices\n    seal_indices(client, indices)\n  File \"/usr/lib/python2.7/site-packages/curator/api/seal.py\", line 37, in seal_indices\n    fails = [ i for i in sorted(results) if results[i]['failed'] > 0 ]\nTypeError: sequence index must be integer, not 'str'\nWith 'show' it's Ok:\ncurator show indices --time-unit days --older-than 30 --timestring %Y.%m.%d --prefix logstash-netflow5-\n2015-07-17 15:13:22,586 INFO      Job starting: show indices\n2015-07-17 15:13:23,681 INFO      Matching indices:\nlogstash-netflow5-2015.05.19\nlogstash-netflow5-2015.05.20\nlogstash-netflow5-2015.05.21\nlogstash-netflow5-2015.05.22\nlogstash-netflow5-2015.05.23\nlogstash-netflow5-2015.05.24\nlogstash-netflow5-2015.05.25\nlogstash-netflow5-2015.05.26\nlogstash-netflow5-2015.05.27\nlogstash-netflow5-2015.05.28\nlogstash-netflow5-2015.05.29\nlogstash-netflow5-2015.05.30\nlogstash-netflow5-2015.05.31\nlogstash-netflow5-2015.06.01\nlogstash-netflow5-2015.06.02\nlogstash-netflow5-2015.06.03\nlogstash-netflow5-2015.06.04\nlogstash-netflow5-2015.06.06\nlogstash-netflow5-2015.06.07\nlogstash-netflow5-2015.06.08\nlogstash-netflow5-2015.06.09\nlogstash-netflow5-2015.06.10\nlogstash-netflow5-2015.06.11\nlogstash-netflow5-2015.06.12\nlogstash-netflow5-2015.06.13\nlogstash-netflow5-2015.06.14\nlogstash-netflow5-2015.06.15\nlogstash-netflow5-2015.06.16\nlogstash-netflow5-2015.06.17\nrpm -qi elasticsearch\nName        : elasticsearch\nVersion     : 1.6.0\nRelease     : 1\nArchitecture: noarch\nInstall Date: Thu 11 Jun 2015 03:42:33 PM MSK\nGroup       : Application/Internet\nSize        : 31223901\nLicense     : (c) 2009\nSignature   : RSA/SHA1, Tue 09 Jun 2015 04:41:31 PM MSK, Key ID d27d666cd88e42b4\nSource RPM  : elasticsearch-1.6.0-1.src.rpm\nBuild Date  : Tue 09 Jun 2015 04:41:28 PM MSK\nBuild Host  : ip-10-249-14-148.us-west-2.compute.internal\nRelocations : /usr \nPackager    : Elasticsearch\nSummary     : elasticsearch\nDescription :\nElasticsearch - Open Source, Distributed, RESTful Search Engine\n. i've two nodes, and yes both is 1.6 versions.\nI will try your command after sync finishing. Now, it's in progress)\n. p.s. With this commands it was worked:\ncurator close indices --time-unit days --older-than 30 --newer-than 43 --timestring %Y.%m.%d --prefix logstash-netflow5-\ncurator close indices --time-unit days --older-than 42 --newer-than 61 --timestring %Y.%m.%d --prefix logstash-netflow5-\n. Now it's seems Ok:\ncurator --debug --timeout 90 close indices --time-unit days --older-than 30 --timestring %Y.%m.%d --prefix logstash-netflow5-\n2015-07-17 17:40:45,354 DEBUG         curator.api.filter         get_date_regex:158  regex = \\d{4}.\\d{2}.\\d{2}\n2015-07-17 17:40:45,354 DEBUG          curator.cli.utils        filter_callback:179  REGEX = (?P\\d{4}.\\d{2}.\\d{2})\n2015-07-17 17:40:45,354 DEBUG          curator.cli.utils        filter_callback:182  Added filter: {'pattern': '(?P\\d{4}.\\d{2}.\\d{2})', 'value': 30, 'groupname': 'date', 'time_unit': 'days', 'timestring': u'%Y.%m.%d', 'method': 'older_than'}\n2015-07-17 17:40:45,354 DEBUG          curator.cli.utils        filter_callback:183  New list of filters: [{'pattern': '(?P\\d{4}.\\d{2}.\\d{2})', 'value': 30, 'groupname': 'date', 'time_unit': 'days', 'timestring': u'%Y.%m.%d', 'method': 'older_than'}]\n2015-07-17 17:40:45,354 DEBUG          curator.cli.utils        filter_callback:179  REGEX = ^logstash-netflow5-.$\n2015-07-17 17:40:45,354 DEBUG          curator.cli.utils        filter_callback:182  Added filter: {'pattern': '^logstash-netflow5-.$'}\n2015-07-17 17:40:45,355 DEBUG          curator.cli.utils        filter_callback:183  New list of filters: [{'pattern': '(?P\\d{4}.\\d{2}.\\d{2})', 'value': 30, 'groupname': 'date', 'time_unit': 'days', 'timestring': u'%Y.%m.%d', 'method': 'older_than'}, {'pattern': '^logstash-netflow5-.*$'}]\n2015-07-17 17:40:45,355 INFO      curator.cli.index_selection                indices:53   Job starting: close indices\n2015-07-17 17:40:45,355 DEBUG     curator.cli.index_selection                indices:56   Params: {'url_prefix': u'', 'http_auth': None, 'dry_run': False, 'loglevel': u'INFO', 'logformat': u'default', 'host': u'localhost', 'timeout': 90, 'debug': True, 'use_ssl': False, 'logfile': None, 'master_only': False, 'port': 9200}\n2015-07-17 17:40:45,355 DEBUG          curator.cli.utils             get_client:110  kwargs = {'url_prefix': u'', 'http_auth': None, 'dry_run': False, 'loglevel': u'INFO', 'host': u'localhost', 'logformat': u'default', 'timeout': 90, 'debug': True, 'use_ssl': False, 'logfile': None, 'master_only': False, 'port': 9200}\n2015-07-17 17:40:45,355 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:40:45,356 INFO      urllib3.connectionpool              _new_conn:203  Starting new HTTP connection (1): localhost\n2015-07-17 17:40:46,444 DEBUG     urllib3.connectionpool          _make_request:383  \"GET / HTTP/1.1\" 200 341\n2015-07-17 17:40:46,445 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:1.089s]\n2015-07-17 17:40:46,445 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:40:46,445 DEBUG              elasticsearch    log_request_success:66   < {\n  \"status\" : 200,\n  \"name\" : \"nettools-m1n1\",\n  \"cluster_name\" : \"elasticsearch\",\n  \"version\" : {\n    \"number\" : \"1.6.0\",\n    \"build_hash\" : \"cdd3ac4dde4f69524ec0a14de3828cb95bbb86d0\",\n    \"build_timestamp\" : \"2015-06-09T13:36:34Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"4.10.4\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n2015-07-17 17:40:46,446 DEBUG          curator.cli.utils          check_version:90   Detected Elasticsearch version 1.6.0\n2015-07-17 17:40:46,446 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:40:46,818 DEBUG     urllib3.connectionpool          make_request:383  \"GET /_all/_settings?expand_wildcards=open%2Cclosed HTTP/1.1\" 200 27871\n2015-07-17 17:40:46,819 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_all/_settings?expand_wildcards=open%2Cclosed [status:200 request:0.373s]\n2015-07-17 17:40:46,819 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:40:46,819 DEBUG              elasticsearch    log_request_success:66   < {\"logstash-2015.07.15\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1436918413122\",\"number_of_shards\":\"5\",\"uuid\":\"hrQiX_LGSDKSwlV8eG51wQ\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-2015.07.02\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435795210883\",\"number_of_shards\":\"5\",\"uuid\":\"3sN24EMVTYuQUOnWySngrQ\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-ot-syslog-2015.07.10\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1436486401951\",\"number_of_shards\":\"5\",\"uuid\":\"LC60OzllQ5iQyji73G96-w\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-ot-syslog-2015.07.06\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1436140801191\",\"number_of_shards\":\"5\",\"uuid\":\"Z7isBtgkS6O_R_tQpy13wg\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.07.11\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1436572781685\",\"number_of_shards\":\"5\",\"uuid\":\"EuS4hy4-RhiEv9Q-Ay3-QA\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-2015.06.21\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1434844804431\",\"number_of_shards\":\"5\",\"uuid\":\"FO6z_ZyJQwiVtkiISm2OjQ\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-ot-syslog-2015.07.01\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435708800360\",\"number_of_shards\":\"5\",\"uuid\":\"zPk28yFPQV-4Ya-X77NmWA\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-ot-syslog-2015.06.28\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435449600725\",\"number_of_shards\":\"5\",\"uuid\":\"rOSXuKgSRrKGkKzWhIs9UQ\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.05.20\":{\"settings\":{\"index\":{\"creation_date\":\"1432105194912\",\"number_of_shards\":\"5\",\"uuid\":\"NNRL23WqRiqDG8xXGhGGFw\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.07.01\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435708785404\",\"number_of_shards\":\"5\",\"uuid\":\"mlYhOQMIS2qeQ8CSAiyHPA\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.06.16\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1434412792992\",\"number_of_shards\":\"5\",\"uuid\":\"C7vp0kvCTNOhVCzDpbWtBQ\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-ot-syslog-2015.07.13\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1436745600798\",\"number_of_shards\":\"5\",\"uuid\":\"a01PmyTHSoe4C0En-cgBcg\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.05.26\":{\"settings\":{\"index\":{\"creation_date\":\"1432598401799\",\"number_of_shards\":\"5\",\"uuid\":\"2TLDtEjPRNmTjCqqZAFbAw\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}}},\"logstash-2015.07.13\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1436745610355\",\"number_of_shards\":\"5\",\"uuid\":\"DZ30oS4MRmePbZryQi2oNw\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-ot-syslog-2015.07.11\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1436572801231\",\"number_of_shards\":\"5\",\"uuid\":\"9wV4-bUSSoiPmKQo_3libA\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.06.20\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1434758393967\",\"number_of_shards\":\"5\",\"uuid\":\"Wdbp6nXtRzKv7XYOiq7Oxw\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.05.29\":{\"settings\":{\"index\":{\"creation_date\":\"1432887483995\",\"number_of_shards\":\"5\",\"uuid\":\"4O7dxtZ0QkK4zuq7r6EMPQ\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.06.12\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1434067197716\",\"number_of_shards\":\"5\",\"uuid\":\"LIRfVMp7S_CK6J4XysXPfg\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-ot-syslog-2015.06.26\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435276801181\",\"number_of_shards\":\"5\",\"uuid\":\"KxBf2ojGSW25Sazw-5OF_Q\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.07.06\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1436140782557\",\"number_of_shards\":\"5\",\"uuid\":\"uPBAimJYSZ2KrkpG07_uPw\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.07.12\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1436659184207\",\"number_of_shards\":\"5\",\"uuid\":\"cTHiOVH7Qbqy64Hkyzz3jA\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.06.13\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1434153594302\",\"number_of_shards\":\"5\",\"uuid\":\"UFwAc8iGQRupqHmR-QUj8g\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-ot-syslog-2015.07.07\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1436227203387\",\"number_of_shards\":\"5\",\"uuid\":\"VaLurmGCRC6WG5EHJBJYeA\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.07.02\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435795184959\",\"number_of_shards\":\"5\",\"uuid\":\"db_RneIcRQyHBPG2o2x9mw\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-2015.07.14\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1436832008282\",\"number_of_shards\":\"5\",\"uuid\":\"v1mpPqhcSf6_g1IPeldQ0g\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.06.02\":{\"settings\":{\"index\":{\"creation_date\":\"1433203199922\",\"number_of_shards\":\"5\",\"uuid\":\"Fl9otSpfRPixCPTLi3-urA\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}}},\"logstash-ot-syslog-2015.07.03\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435881601494\",\"number_of_shards\":\"5\",\"uuid\":\"pxsQm72oRBWvSy3DDHwGtQ\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.06.24\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435103990060\",\"number_of_shards\":\"5\",\"uuid\":\"JUdwClMPS1eFpGV41SX3OA\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.05.21\":{\"settings\":{\"index\":{\"creation_date\":\"1432217685501\",\"number_of_shards\":\"5\",\"uuid\":\"6vLPqB19QZetnEsVeaJkHg\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}}},\"logstash-ot-syslog-2015.06.25\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435190400926\",\"number_of_shards\":\"5\",\"uuid\":\"SbWU-hvOQ-2AlvEDREepiA\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-ot-syslog-2015.06.20\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1434758400272\",\"number_of_shards\":\"5\",\"uuid\":\"yamwEtp1SJqipKkOEOms5w\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.07.10\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1436486382387\",\"number_of_shards\":\"5\",\"uuid\":\"g9ZDYYqGSRWnldw4K0vCrA\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.06.05.orig\":{\"settings\":{\"index\":{\"creation_date\":\"1433462401945\",\"number_of_shards\":\"5\",\"uuid\":\"RRYwP8uVSQKesoy38MAvCw\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.07.13\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1436745581653\",\"number_of_shards\":\"5\",\"uuid\":\"PoNuq7oGRRGX3qGWEiuQng\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.07.04\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435967983854\",\"number_of_shards\":\"5\",\"uuid\":\"iJKU3-ecSNmcnZjlgJNooA\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-2015.07.12\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1436659213733\",\"number_of_shards\":\"5\",\"uuid\":\"bZANNFDfSkiQCpKm__je_g\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.06.11\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1433980796068\",\"number_of_shards\":\"5\",\"uuid\":\"wFRq1MW0R5e7hlAdU7vQpw\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}}},\"logstash-ot-syslog-2015.07.15\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1436918401653\",\"number_of_shards\":\"5\",\"uuid\":\"wxbVeJkTRKuPU3Yy92uIVg\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.06.25\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435190388769\",\"number_of_shards\":\"5\",\"uuid\":\"5YMIg0bDTFmAbBWe-TFf_g\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.06.27\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435363189258\",\"number_of_shards\":\"5\",\"uuid\":\"Q6afFB5IQreBPo1jWph8lg\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.06.22\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1434931189687\",\"number_of_shards\":\"5\",\"uuid\":\"0ccMxE8-QcGvlmctzqWkjA\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-2015.06.19\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1434672001347\",\"number_of_shards\":\"5\",\"uuid\":\"O4tEpDSwSl-ME64LgukmNQ\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.06.09\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1433807995794\",\"number_of_shards\":\"5\",\"uuid\":\"hQygCvHmTCOULb10hVOm1Q\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.06.17\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1434499193180\",\"number_of_shards\":\"5\",\"uuid\":\"QYI5NU_bR6qTuQyZwkA57w\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-2015.07.17\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1437091215791\",\"number_of_shards\":\"5\",\"uuid\":\"a1Utp6khRiGyowBawKZybg\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-2015.07.08\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1436313607950\",\"number_of_shards\":\"5\",\"uuid\":\"Dw9-ITHESiaHFAwXfCqGOw\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-ot-syslog-2015.06.24\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435104000973\",\"number_of_shards\":\"5\",\"uuid\":\"AfI20nP2RgqmwTgs5_8nGA\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-2015.06.18\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1434585600239\",\"number_of_shards\":\"5\",\"uuid\":\"X2MQDgG2SPSlyqcCu3ZB5Q\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.05.27\":{\"settings\":{\"index\":{\"creation_date\":\"1432684803412\",\"number_of_shards\":\"5\",\"uuid\":\"SU-PPJSTRv6BUfWb7lDWMg\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}}},\"logstash-ot-syslog-2015.07.04\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435968001049\",\"number_of_shards\":\"5\",\"uuid\":\"ppEpR1dISzu7OQ9tjtlyJA\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.06.14\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1434241954174\",\"number_of_shards\":\"5\",\"uuid\":\"k_3E2xtuRGGT27tamkvXDg\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.05.19\":{\"settings\":{\"index\":{\"creation_date\":\"1432073586724\",\"number_of_shards\":\"5\",\"uuid\":\"6OZICtf-QBOsC4YWvwmPkw\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.06.06\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1433548798251\",\"number_of_shards\":\"5\",\"uuid\":\"eJo6TxomQEmXLfntpfT9Iw\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.06.28\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435449586723\",\"number_of_shards\":\"5\",\"uuid\":\"8lh8L5RBTkqmujDb0vJpMw\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.07.14\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1436831978174\",\"number_of_shards\":\"5\",\"uuid\":\"Q31l1w9oT7KEfCBb0FU3Jg\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.06.21\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1434844791004\",\"number_of_shards\":\"5\",\"uuid\":\"B6ZEC-SWT3-mMUfphxfvdQ\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-ot-syslog-2015.07.12\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1436659201645\",\"number_of_shards\":\"5\",\"uuid\":\"h_5-TLHDTsS1nSeSNQvK0Q\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.07.03\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435881586564\",\"number_of_shards\":\"5\",\"uuid\":\"UwnfBxrIR3WVn5omH-dbmg\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"my_index_v1\":{\"settings\":{\"index\":{\"creation_date\":\"1433764327417\",\"number_of_shards\":\"5\",\"uuid\":\"cIo6C-eCQ9CrCMYq27qtsA\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}}},\"logstash-2015.06.23\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435017600995\",\"number_of_shards\":\"5\",\"uuid\":\"_dj2UeVzSS2qv37swZ4hDQ\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.07.09\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1436399981024\",\"number_of_shards\":\"5\",\"uuid\":\"7V-NkiLcQLiPZIQYhcT2BA\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-2015.06.26\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435276813644\",\"number_of_shards\":\"5\",\"uuid\":\"qn-Q2o-wRUSBmXUV4yOu6g\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.05.28\":{\"settings\":{\"index\":{\"creation_date\":\"1432771202947\",\"number_of_shards\":\"5\",\"uuid\":\"JEB76nchTfm0Jeepzk9SHg\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}}},\"logstash-2015.07.16\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1437004808182\",\"number_of_shards\":\"5\",\"uuid\":\"ZaRFUapoSf6TTyoHTqIyFQ\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.06.30\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435622386857\",\"number_of_shards\":\"5\",\"uuid\":\"dv18jkGrRau9VUPOJs5vow\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-2015.07.04\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435968001160\",\"number_of_shards\":\"5\",\"uuid\":\"w9FIim9ARGWVjSpT8c3S_g\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-2015.06.25\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435190407985\",\"number_of_shards\":\"5\",\"uuid\":\"Nw3NVwgNTgiX4kJZUOVQ6Q\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-ot-syslog-2015.07.09\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1436400003666\",\"number_of_shards\":\"5\",\"uuid\":\"Em3ZCY-YQ7ebvDGUwa6agA\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-2015.07.01\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435708803149\",\"number_of_shards\":\"5\",\"uuid\":\"mBuW2JP_QfaRhXzF90c2-w\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-ot-syslog-2015.06.18\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1434585601029\",\"number_of_shards\":\"5\",\"uuid\":\"fqNKWhiZQ-CZE6LeDq-oGg\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-2015.06.30\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435622412845\",\"number_of_shards\":\"5\",\"uuid\":\"PIjvczPKSmafFWoKNzezig\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.07.15\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1436918380373\",\"number_of_shards\":\"5\",\"uuid\":\"o3m9PLgZTYun0GriwBiTAg\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-2015.06.22\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1434931241194\",\"number_of_shards\":\"5\",\"uuid\":\"DkNYWpAEQzCsE3S3MILCxw\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.06.07\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1433635198724\",\"number_of_shards\":\"5\",\"uuid\":\"FEIW-L_uQ4iwoMuazTAsUQ\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}}},\"logstash-ot-syslog-2015.06.29\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435536001332\",\"number_of_shards\":\"5\",\"uuid\":\"c8WqtoyDSt2qowbW5f9RsA\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.06.29\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435535989291\",\"number_of_shards\":\"5\",\"uuid\":\"hOIHh_41Tqaaxtx9q5-DCA\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-2015.07.06\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1436140806254\",\"number_of_shards\":\"5\",\"uuid\":\"z9hrS9ynSzC4a0TTfnG3gA\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.07.17\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1437091179900\",\"number_of_shards\":\"5\",\"uuid\":\"cvdvNYEkRsuxRGJGH8Zl8Q\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-2015.06.20\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1434758400497\",\"number_of_shards\":\"5\",\"uuid\":\"tZlGMznjTS-TesVQoG4OKw\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-ot-syslog-2015.07.16\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1437004803264\",\"number_of_shards\":\"5\",\"uuid\":\"wWBaoj7CTa-v3I8aVMtLfA\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.06.10\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1433894396190\",\"number_of_shards\":\"5\",\"uuid\":\"W6uCXI9oRjqmSVBHrarPiQ\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}}},\"logstash-ot-syslog-2015.06.27\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435363200866\",\"number_of_shards\":\"5\",\"uuid\":\"K_kKL831SgK1KeWHo1xPNg\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.07.07\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1436227184806\",\"number_of_shards\":\"5\",\"uuid\":\"EanGtuyRR2K82pSN2vMGtg\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-2015.06.27\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435363208601\",\"number_of_shards\":\"5\",\"uuid\":\"fLuUvqX4RL-NKqhcFHMi8w\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-ot-syslog-2015.06.21\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1434844800180\",\"number_of_shards\":\"5\",\"uuid\":\"JOywHewxQuC86-3Xc1PdLA\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-2015.07.09\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1436400007990\",\"number_of_shards\":\"5\",\"uuid\":\"vGTIIZOKS82Z8S3l3UglIQ\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-ot-syslog-2015.07.05\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1436054401012\",\"number_of_shards\":\"5\",\"uuid\":\"4nHKoj7_QhaIJi_GFnkbsQ\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-ot-syslog-2015.07.14\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1436832001365\",\"number_of_shards\":\"5\",\"uuid\":\"q6lRrYq7Qg25MulKdW9iIQ\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-2015.06.29\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435536013454\",\"number_of_shards\":\"5\",\"uuid\":\"4yEVtJyJTsqgQqh_gSxMHg\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.06.15\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1434326393695\",\"number_of_shards\":\"5\",\"uuid\":\"HYhDEnJ9RZSA4U0oX6QXHw\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-ot-syslog-2015.06.22\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1434931237156\",\"number_of_shards\":\"5\",\"uuid\":\"mwYPGipoTPmUXdf_X2m83Q\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.05.31\":{\"settings\":{\"index\":{\"creation_date\":\"1433030402589\",\"number_of_shards\":\"5\",\"uuid\":\"Suwo3OE0R8CyESKjcOBhzw\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}}},\"logstash-ot-syslog-2015.06.19\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1434672000497\",\"number_of_shards\":\"5\",\"uuid\":\"eucP-wrcQuqm8kpEBM4bkw\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-ot-syslog-2015.06.23\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435017601376\",\"number_of_shards\":\"5\",\"uuid\":\"9h49W5piQPeFF7PrpVBBgg\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.05.24\":{\"settings\":{\"index\":{\"creation_date\":\"1432425603806\",\"number_of_shards\":\"5\",\"uuid\":\"UlEyDFlIT6aWQ99J2brdmw\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.06.08\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1433721598182\",\"number_of_shards\":\"5\",\"uuid\":\"YGPXODdfTkawzOtdUQJYgg\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.05.30\":{\"settings\":{\"index\":{\"creation_date\":\"1432944002711\",\"number_of_shards\":\"5\",\"uuid\":\"uBbxYdG1TTS1OeXIsYXOKg\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}}},\"logstash-2015.06.17\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1434499200912\",\"number_of_shards\":\"5\",\"uuid\":\"pdxQ7fDSTdiEV0LEvCp_Sg\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-ot-syslog-2015.06.11\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1433981339492\",\"number_of_shards\":\"5\",\"uuid\":\"zjmTAPCwTT-kAJE1pg1cDQ\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}}},\"logstash-2015.07.11\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1436572806717\",\"number_of_shards\":\"5\",\"uuid\":\"0xmN42ZqSBa6OU8pVvdKMA\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.06.03\":{\"settings\":{\"index\":{\"creation_date\":\"1433289600773\",\"number_of_shards\":\"5\",\"uuid\":\"qStFxkTQR3KQaQDFQCApHw\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.06.04\":{\"settings\":{\"index\":{\"creation_date\":\"1433375997917\",\"number_of_shards\":\"5\",\"uuid\":\"f1rCLjnOSjyVcdMToJwMCA\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}}},\"logstash-2015.07.07\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1436227211441\",\"number_of_shards\":\"5\",\"uuid\":\"OOecutq_QDqNodfuPTdfMQ\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-ot-syslog-2015.07.17\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1437091202478\",\"number_of_shards\":\"5\",\"uuid\":\"u_QSxB0hQ9W3uLsJR46ZAg\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-ot-syslog-2015.06.17\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1434499201028\",\"number_of_shards\":\"5\",\"uuid\":\"NvdUphf7TnKC__sujrEaPw\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-2015.07.03\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435881609889\",\"number_of_shards\":\"5\",\"uuid\":\"9pcKPwknSPuCPqLDlk7SHQ\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-2015.06.28\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435449601238\",\"number_of_shards\":\"5\",\"uuid\":\"ybI7KGCpQVqKI8Huvr8rNA\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-ot-syslog-2015.07.02\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435795200657\",\"number_of_shards\":\"5\",\"uuid\":\"SV8BVZC-S6KCHO4VXHIuOQ\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.06.01\":{\"settings\":{\"index\":{\"creation_date\":\"1433116801198\",\"number_of_shards\":\"5\",\"uuid\":\"RpoNJseySheEipEWOXurQg\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}}},\"logstash-ot-syslog-2015.06.30\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435622401309\",\"number_of_shards\":\"5\",\"uuid\":\"9Z2G30S6S_ijurRx8Nj-aw\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.07.05\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1436054384388\",\"number_of_shards\":\"5\",\"uuid\":\"ykSYsgXGS-ebG8gQiJzE-Q\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-ot-syslog-2015.06.16\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1434458856622\",\"number_of_shards\":\"5\",\"uuid\":\"uGocpKE8TMCpalQe9pvYdQ\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-ot-syslog-2015.07.08\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1436313602940\",\"number_of_shards\":\"5\",\"uuid\":\"ohdIy8JHQzyiwUeHwFwR1g\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-2015.07.10\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1436486409631\",\"number_of_shards\":\"5\",\"uuid\":\"nMvHriSvTzidTvXHvCYCpA\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.06.18\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1434585592875\",\"number_of_shards\":\"5\",\"uuid\":\"hCGdR2qGQf-mGmUvIsJxCw\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\".kibana\":{\"settings\":{\"index\":{\"creation_date\":\"1432073590966\",\"number_of_shards\":\"1\",\"uuid\":\"xsDw8VkSQAyKUJ7K_oMtDQ\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.07.08\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1436313582422\",\"number_of_shards\":\"5\",\"uuid\":\"C1bYzGWIRw24_by70QojpA\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-ot-syslog-2015.06.10\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1433928488205\",\"number_of_shards\":\"5\",\"uuid\":\"NCZ9NsmCTX-vFnnwYGPq6A\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}}},\"logstash-2015.07.05\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1436054410811\",\"number_of_shards\":\"5\",\"uuid\":\"JeAGSfCCQ_WmHZAiQhCJDQ\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.06.26\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435276787642\",\"number_of_shards\":\"5\",\"uuid\":\"tis4z3HKTUKygj5KBw85Ng\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-2015.06.24\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435104012683\",\"number_of_shards\":\"5\",\"uuid\":\"8DpvmcIVQgS4gHtgIRX4-g\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.07.16\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1437004779306\",\"number_of_shards\":\"5\",\"uuid\":\"b0KvYqJpQW6uNIJIjwNelw\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.06.19\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1434671994616\",\"number_of_shards\":\"5\",\"uuid\":\"3AIpmYw8Ta6_cOM-jfnLNA\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.05.25\":{\"settings\":{\"index\":{\"creation_date\":\"1432512002237\",\"number_of_shards\":\"5\",\"uuid\":\"blPCzOZcRLiQ2lWcelNN8w\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.05.23\":{\"settings\":{\"index\":{\"creation_date\":\"1432339202198\",\"number_of_shards\":\"5\",\"uuid\":\"eO4bFSzWQoOcBCZSq87cRw\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.06.23\":{\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1435017590228\",\"number_of_shards\":\"5\",\"uuid\":\"FXcy4aFYSMeA_njJ7F-XZw\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}}},\"logstash-netflow5-2015.05.22\":{\"settings\":{\"index\":{\"creation_date\":\"1432252803701\",\"number_of_shards\":\"5\",\"uuid\":\"SOX7n8mdTyKpP5-VJYm1Gw\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}}}}\n2015-07-17 17:40:46,875 DEBUG          curator.api.utils            get_indices:27   All indices: [u'logstash-netflow5-2015.06.20', u'logstash-netflow5-2015.05.20', u'logstash-netflow5-2015.06.22', u'logstash-netflow5-2015.06.23', u'logstash-netflow5-2015.06.24', u'logstash-netflow5-2015.06.25', u'logstash-netflow5-2015.05.27', u'logstash-netflow5-2015.05.26', u'logstash-netflow5-2015.05.29', u'logstash-netflow5-2015.05.28', u'my_index_v1', u'logstash-netflow5-2015.05.22', u'logstash-netflow5-2015.05.25', u'logstash-netflow5-2015.05.24', u'logstash-netflow5-2015.06.26', u'logstash-netflow5-2015.06.27', u'logstash-netflow5-2015.06.28', u'logstash-netflow5-2015.05.30', u'logstash-netflow5-2015.06.29', u'logstash-ot-syslog-2015.07.13', u'logstash-ot-syslog-2015.07.12', u'logstash-ot-syslog-2015.07.11', u'logstash-ot-syslog-2015.07.10', u'logstash-ot-syslog-2015.07.17', u'logstash-ot-syslog-2015.07.16', u'logstash-ot-syslog-2015.07.15', u'logstash-ot-syslog-2015.07.14', u'logstash-netflow5-2015.06.19', u'logstash-netflow5-2015.06.18', u'logstash-netflow5-2015.06.15', u'logstash-netflow5-2015.06.14', u'logstash-netflow5-2015.06.17', u'logstash-netflow5-2015.06.16', u'logstash-netflow5-2015.06.11', u'logstash-netflow5-2015.06.10', u'logstash-netflow5-2015.06.13', u'logstash-netflow5-2015.06.12', u'logstash-netflow5-2015.07.16', u'logstash-netflow5-2015.07.17', u'logstash-netflow5-2015.07.14', u'logstash-netflow5-2015.07.15', u'logstash-netflow5-2015.07.12', u'logstash-netflow5-2015.07.13', u'logstash-netflow5-2015.07.10', u'logstash-netflow5-2015.07.11', u'logstash-ot-syslog-2015.07.04', u'logstash-ot-syslog-2015.07.05', u'logstash-ot-syslog-2015.07.06', u'logstash-ot-syslog-2015.07.07', u'logstash-ot-syslog-2015.07.01', u'logstash-ot-syslog-2015.07.02', u'logstash-ot-syslog-2015.07.03', u'logstash-ot-syslog-2015.07.08', u'logstash-ot-syslog-2015.07.09', u'logstash-ot-syslog-2015.06.19', u'logstash-netflow5-2015.06.08', u'logstash-netflow5-2015.06.09', u'.kibana', u'logstash-netflow5-2015.06.02', u'logstash-netflow5-2015.06.03', u'logstash-netflow5-2015.05.31', u'logstash-netflow5-2015.06.01', u'logstash-netflow5-2015.06.06', u'logstash-netflow5-2015.06.07', u'logstash-netflow5-2015.06.04', u'logstash-ot-syslog-2015.06.10', u'logstash-ot-syslog-2015.06.11', u'logstash-netflow5-2015.06.05.orig', u'logstash-netflow5-2015.07.07', u'logstash-ot-syslog-2015.06.21', u'logstash-ot-syslog-2015.06.20', u'logstash-ot-syslog-2015.06.23', u'logstash-ot-syslog-2015.06.22', u'logstash-ot-syslog-2015.06.25', u'logstash-ot-syslog-2015.06.24', u'logstash-ot-syslog-2015.06.27', u'logstash-ot-syslog-2015.06.26', u'logstash-ot-syslog-2015.06.29', u'logstash-ot-syslog-2015.06.28', u'logstash-2015.06.18', u'logstash-2015.06.19', u'logstash-2015.06.17', u'logstash-netflow5-2015.05.21', u'logstash-ot-syslog-2015.06.30', u'logstash-2015.07.13', u'logstash-2015.07.12', u'logstash-2015.07.11', u'logstash-2015.07.10', u'logstash-2015.07.17', u'logstash-2015.07.16', u'logstash-2015.07.15', u'logstash-2015.07.14', u'logstash-2015.06.21', u'logstash-2015.06.20', u'logstash-2015.06.23', u'logstash-2015.06.22', u'logstash-2015.06.25', u'logstash-2015.06.24', u'logstash-2015.06.27', u'logstash-2015.06.26', u'logstash-2015.06.29', u'logstash-2015.06.28', u'logstash-netflow5-2015.05.23', u'logstash-ot-syslog-2015.06.18', u'logstash-2015.07.01', u'logstash-2015.07.02', u'logstash-netflow5-2015.06.30', u'logstash-2015.07.04', u'logstash-2015.07.05', u'logstash-2015.07.06', u'logstash-2015.07.07', u'logstash-2015.07.08', u'logstash-2015.07.09', u'logstash-netflow5-2015.05.19', u'logstash-ot-syslog-2015.06.16', u'logstash-ot-syslog-2015.06.17', u'logstash-netflow5-2015.07.01', u'logstash-netflow5-2015.07.03', u'logstash-netflow5-2015.07.02', u'logstash-netflow5-2015.07.05', u'logstash-netflow5-2015.07.04', u'logstash-2015.06.30', u'logstash-netflow5-2015.07.06', u'logstash-netflow5-2015.07.09', u'logstash-netflow5-2015.07.08', u'logstash-2015.07.03', u'logstash-netflow5-2015.06.21']\n2015-07-17 17:40:46,875 DEBUG     curator.cli.index_selection                indices:72   Full list of indices: [u'logstash-netflow5-2015.06.20', u'logstash-netflow5-2015.05.20', u'logstash-netflow5-2015.06.22', u'logstash-netflow5-2015.06.23', u'logstash-netflow5-2015.06.24', u'logstash-netflow5-2015.06.25', u'logstash-netflow5-2015.05.27', u'logstash-netflow5-2015.05.26', u'logstash-netflow5-2015.05.29', u'logstash-netflow5-2015.05.28', u'my_index_v1', u'logstash-netflow5-2015.05.22', u'logstash-netflow5-2015.05.25', u'logstash-netflow5-2015.05.24', u'logstash-netflow5-2015.06.26', u'logstash-netflow5-2015.06.27', u'logstash-netflow5-2015.06.28', u'logstash-netflow5-2015.05.30', u'logstash-netflow5-2015.06.29', u'logstash-ot-syslog-2015.07.13', u'logstash-ot-syslog-2015.07.12', u'logstash-ot-syslog-2015.07.11', u'logstash-ot-syslog-2015.07.10', u'logstash-ot-syslog-2015.07.17', u'logstash-ot-syslog-2015.07.16', u'logstash-ot-syslog-2015.07.15', u'logstash-ot-syslog-2015.07.14', u'logstash-netflow5-2015.06.19', u'logstash-netflow5-2015.06.18', u'logstash-netflow5-2015.06.15', u'logstash-netflow5-2015.06.14', u'logstash-netflow5-2015.06.17', u'logstash-netflow5-2015.06.16', u'logstash-netflow5-2015.06.11', u'logstash-netflow5-2015.06.10', u'logstash-netflow5-2015.06.13', u'logstash-netflow5-2015.06.12', u'logstash-netflow5-2015.07.16', u'logstash-netflow5-2015.07.17', u'logstash-netflow5-2015.07.14', u'logstash-netflow5-2015.07.15', u'logstash-netflow5-2015.07.12', u'logstash-netflow5-2015.07.13', u'logstash-netflow5-2015.07.10', u'logstash-netflow5-2015.07.11', u'logstash-ot-syslog-2015.07.04', u'logstash-ot-syslog-2015.07.05', u'logstash-ot-syslog-2015.07.06', u'logstash-ot-syslog-2015.07.07', u'logstash-ot-syslog-2015.07.01', u'logstash-ot-syslog-2015.07.02', u'logstash-ot-syslog-2015.07.03', u'logstash-ot-syslog-2015.07.08', u'logstash-ot-syslog-2015.07.09', u'logstash-ot-syslog-2015.06.19', u'logstash-netflow5-2015.06.08', u'logstash-netflow5-2015.06.09', u'.kibana', u'logstash-netflow5-2015.06.02', u'logstash-netflow5-2015.06.03', u'logstash-netflow5-2015.05.31', u'logstash-netflow5-2015.06.01', u'logstash-netflow5-2015.06.06', u'logstash-netflow5-2015.06.07', u'logstash-netflow5-2015.06.04', u'logstash-ot-syslog-2015.06.10', u'logstash-ot-syslog-2015.06.11', u'logstash-netflow5-2015.06.05.orig', u'logstash-netflow5-2015.07.07', u'logstash-ot-syslog-2015.06.21', u'logstash-ot-syslog-2015.06.20', u'logstash-ot-syslog-2015.06.23', u'logstash-ot-syslog-2015.06.22', u'logstash-ot-syslog-2015.06.25', u'logstash-ot-syslog-2015.06.24', u'logstash-ot-syslog-2015.06.27', u'logstash-ot-syslog-2015.06.26', u'logstash-ot-syslog-2015.06.29', u'logstash-ot-syslog-2015.06.28', u'logstash-2015.06.18', u'logstash-2015.06.19', u'logstash-2015.06.17', u'logstash-netflow5-2015.05.21', u'logstash-ot-syslog-2015.06.30', u'logstash-2015.07.13', u'logstash-2015.07.12', u'logstash-2015.07.11', u'logstash-2015.07.10', u'logstash-2015.07.17', u'logstash-2015.07.16', u'logstash-2015.07.15', u'logstash-2015.07.14', u'logstash-2015.06.21', u'logstash-2015.06.20', u'logstash-2015.06.23', u'logstash-2015.06.22', u'logstash-2015.06.25', u'logstash-2015.06.24', u'logstash-2015.06.27', u'logstash-2015.06.26', u'logstash-2015.06.29', u'logstash-2015.06.28', u'logstash-netflow5-2015.05.23', u'logstash-ot-syslog-2015.06.18', u'logstash-2015.07.01', u'logstash-2015.07.02', u'logstash-netflow5-2015.06.30', u'logstash-2015.07.04', u'logstash-2015.07.05', u'logstash-2015.07.06', u'logstash-2015.07.07', u'logstash-2015.07.08', u'logstash-2015.07.09', u'logstash-netflow5-2015.05.19', u'logstash-ot-syslog-2015.06.16', u'logstash-ot-syslog-2015.06.17', u'logstash-netflow5-2015.07.01', u'logstash-netflow5-2015.07.03', u'logstash-netflow5-2015.07.02', u'logstash-netflow5-2015.07.05', u'logstash-netflow5-2015.07.04', u'logstash-2015.06.30', u'logstash-netflow5-2015.07.06', u'logstash-netflow5-2015.07.09', u'logstash-netflow5-2015.07.08', u'logstash-2015.07.03', u'logstash-netflow5-2015.06.21']\n2015-07-17 17:40:46,875 DEBUG     curator.cli.index_selection                indices:89   All filters: [{'pattern': '(?P\\d{4}.\\d{2}.\\d{2})', 'value': 30, 'groupname': 'date', 'time_unit': 'days', 'timestring': u'%Y.%m.%d', 'method': 'older_than'}, {'pattern': '^logstash-netflow5-.$'}]\n2015-07-17 17:40:46,875 DEBUG     curator.cli.index_selection                indices:94   Filter: {'pattern': '(?P\\d{4}.\\d{2}.\\d{2})', 'value': 30, 'groupname': 'date', 'time_unit': 'days', 'timestring': u'%Y.%m.%d', 'method': 'older_than'}\n2015-07-17 17:40:46,879 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.20\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,879 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.22\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,879 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.23\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,879 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.24\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,879 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.25\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,880 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.26\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,880 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.27\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,880 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.28\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,880 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.29\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,881 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.13\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,881 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.12\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,881 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.11\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,881 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.10\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,881 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.17\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,881 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.16\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,882 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.15\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,882 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.14\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,882 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.19\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,882 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.18\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,883 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.16\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,883 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.17\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,883 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.14\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,883 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.15\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,883 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.12\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,884 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.13\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,884 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.10\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,884 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.11\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,884 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.04\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,884 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.05\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,884 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.06\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,885 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.07\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,885 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.01\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,885 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.02\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,885 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.03\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,885 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.08\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,886 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.09\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,886 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.19\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,887 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.07\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,887 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.21\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,887 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.20\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,887 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.23\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,887 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.22\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,888 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.25\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,888 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.24\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,888 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.27\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,888 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.26\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,888 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.29\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,889 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.28\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,889 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.18\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,889 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.19\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,889 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.30\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,889 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.13\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,890 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.12\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,890 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.11\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,890 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.10\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,890 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.17\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,890 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.16\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,891 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.15\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,891 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.14\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,891 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.21\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,891 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.20\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,891 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.23\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,891 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.22\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,892 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.25\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,892 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.24\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,892 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.27\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,892 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.26\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,892 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.29\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,892 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.28\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,893 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.18\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,893 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.01\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,893 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.02\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,893 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.30\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,893 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.04\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,893 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.05\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,894 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.06\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,894 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.07\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,894 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.08\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,894 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.09\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,894 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.01\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,895 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.03\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,895 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.02\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,895 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.05\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,895 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.04\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,895 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.30\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,895 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.06\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,896 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.09\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,896 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.08\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,896 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.03\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,896 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.06.21\" is outside the cutoff period (older than 30 days).\n2015-07-17 17:40:46,896 DEBUG     curator.cli.index_selection                indices:94   Filter: {'pattern': '^logstash-netflow5-._$'}\n2015-07-17 17:40:46,897 DEBUG     curator.cli.index_selection                indices:125  ACTION: close. INDICES: [u'logstash-netflow5-2015.05.19', u'logstash-netflow5-2015.05.20', u'logstash-netflow5-2015.05.21', u'logstash-netflow5-2015.05.22', u'logstash-netflow5-2015.05.23', u'logstash-netflow5-2015.05.24', u'logstash-netflow5-2015.05.25', u'logstash-netflow5-2015.05.26', u'logstash-netflow5-2015.05.27', u'logstash-netflow5-2015.05.28', u'logstash-netflow5-2015.05.29', u'logstash-netflow5-2015.05.30', u'logstash-netflow5-2015.05.31', u'logstash-netflow5-2015.06.01', u'logstash-netflow5-2015.06.02', u'logstash-netflow5-2015.06.03', u'logstash-netflow5-2015.06.04', u'logstash-netflow5-2015.06.05.orig', u'logstash-netflow5-2015.06.06', u'logstash-netflow5-2015.06.07', u'logstash-netflow5-2015.06.08', u'logstash-netflow5-2015.06.09', u'logstash-netflow5-2015.06.10', u'logstash-netflow5-2015.06.11', u'logstash-netflow5-2015.06.12', u'logstash-netflow5-2015.06.13', u'logstash-netflow5-2015.06.14', u'logstash-netflow5-2015.06.15', u'logstash-netflow5-2015.06.16', u'logstash-netflow5-2015.06.17']\n2015-07-17 17:40:46,897 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:40:46,898 DEBUG     urllib3.connectionpool          _make_request:383  \"GET / HTTP/1.1\" 200 341\n2015-07-17 17:40:46,899 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.002s]\n2015-07-17 17:40:46,899 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:40:46,899 DEBUG              elasticsearch    log_request_success:66   < {\n  \"status\" : 200,\n  \"name\" : \"nettools-m1n1\",\n  \"cluster_name\" : \"elasticsearch\",\n  \"version\" : {\n    \"number\" : \"1.6.0\",\n    \"build_hash\" : \"cdd3ac4dde4f69524ec0a14de3828cb95bbb86d0\",\n    \"build_timestamp\" : \"2015-06-09T13:36:34Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"4.10.4\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n2015-07-17 17:40:46,899 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:40:46,905 DEBUG     urllib3.connectionpool          make_request:383  \"GET /_cluster/state/metadata/logstash-netflow5-2015.05.19 HTTP/1.1\" 200 1277\n2015-07-17 17:40:46,905 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/logstash-netflow5-2015.05.19 [status:200 request:0.006s]\n2015-07-17 17:40:46,906 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:40:46,906 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"elasticsearch\",\"metadata\":{\"templates\":{},\"indices\":{\"logstash-netflow5-2015.05.19\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1432073586724\",\"number_of_shards\":\"5\",\"uuid\":\"6OZICtf-QBOsC4YWvwmPkw\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}},\"mappings\":{\"logs\":{\"properties\":{\"netflow\":{\"properties\":{\"dst_as\":{\"type\":\"long\"},\"in_pkts\":{\"type\":\"long\"},\"first_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"ipv4_next_hop\":{\"type\":\"string\"},\"l4_src_port\":{\"type\":\"long\"},\"sampling_algorithm\":{\"type\":\"long\"},\"in_bytes\":{\"type\":\"long\"},\"protocol\":{\"type\":\"long\"},\"tcp_flags\":{\"type\":\"long\"},\"l4_dst_port\":{\"type\":\"long\"},\"src_as\":{\"type\":\"long\"},\"output_snmp\":{\"type\":\"long\"},\"dst_mask\":{\"type\":\"long\"},\"ipv4_dst_addr\":{\"type\":\"string\"},\"src_tos\":{\"type\":\"long\"},\"src_mask\":{\"type\":\"long\"},\"version\":{\"type\":\"long\"},\"flow_records\":{\"type\":\"long\"},\"flow_seq_num\":{\"type\":\"long\"},\"ipv4_src_addr\":{\"type\":\"string\"},\"engine_type\":{\"type\":\"long\"},\"engine_id\":{\"type\":\"long\"},\"input_snmp\":{\"type\":\"long\"},\"last_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"sampling_interval\":{\"type\":\"long\"}}},\"@timestamp\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"@version\":{\"type\":\"string\"},\"host\":{\"type\":\"string\"}}}},\"aliases\":[]}}}}\n2015-07-17 17:40:46,907 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:40:46,909 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/logstash-netflow5-2015.05.20 HTTP/1.1\" 200 1277\n2015-07-17 17:40:46,909 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/logstash-netflow5-2015.05.20 [status:200 request:0.002s]\n2015-07-17 17:40:46,909 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:40:46,909 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"elasticsearch\",\"metadata\":{\"templates\":{},\"indices\":{\"logstash-netflow5-2015.05.20\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1432105194912\",\"number_of_shards\":\"5\",\"uuid\":\"NNRL23WqRiqDG8xXGhGGFw\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}},\"mappings\":{\"logs\":{\"properties\":{\"netflow\":{\"properties\":{\"dst_as\":{\"type\":\"long\"},\"in_pkts\":{\"type\":\"long\"},\"first_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"ipv4_next_hop\":{\"type\":\"string\"},\"l4_src_port\":{\"type\":\"long\"},\"sampling_algorithm\":{\"type\":\"long\"},\"in_bytes\":{\"type\":\"long\"},\"protocol\":{\"type\":\"long\"},\"tcp_flags\":{\"type\":\"long\"},\"l4_dst_port\":{\"type\":\"long\"},\"src_as\":{\"type\":\"long\"},\"output_snmp\":{\"type\":\"long\"},\"dst_mask\":{\"type\":\"long\"},\"ipv4_dst_addr\":{\"type\":\"string\"},\"src_tos\":{\"type\":\"long\"},\"src_mask\":{\"type\":\"long\"},\"version\":{\"type\":\"long\"},\"flow_records\":{\"type\":\"long\"},\"flow_seq_num\":{\"type\":\"long\"},\"ipv4_src_addr\":{\"type\":\"string\"},\"engine_type\":{\"type\":\"long\"},\"engine_id\":{\"type\":\"long\"},\"input_snmp\":{\"type\":\"long\"},\"last_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"sampling_interval\":{\"type\":\"long\"}}},\"@timestamp\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"@version\":{\"type\":\"string\"},\"host\":{\"type\":\"string\"}}}},\"aliases\":[]}}}}\n2015-07-17 17:40:46,911 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:40:46,914 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/logstash-netflow5-2015.05.21 HTTP/1.1\" 200 1277\n2015-07-17 17:40:46,914 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/logstash-netflow5-2015.05.21 [status:200 request:0.003s]\n2015-07-17 17:40:46,914 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:40:46,914 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"elasticsearch\",\"metadata\":{\"templates\":{},\"indices\":{\"logstash-netflow5-2015.05.21\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1432217685501\",\"number_of_shards\":\"5\",\"uuid\":\"6vLPqB19QZetnEsVeaJkHg\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}},\"mappings\":{\"logs\":{\"properties\":{\"netflow\":{\"properties\":{\"dst_as\":{\"type\":\"long\"},\"in_pkts\":{\"type\":\"long\"},\"first_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"ipv4_next_hop\":{\"type\":\"string\"},\"l4_src_port\":{\"type\":\"long\"},\"sampling_algorithm\":{\"type\":\"long\"},\"in_bytes\":{\"type\":\"long\"},\"protocol\":{\"type\":\"long\"},\"tcp_flags\":{\"type\":\"long\"},\"l4_dst_port\":{\"type\":\"long\"},\"src_as\":{\"type\":\"long\"},\"output_snmp\":{\"type\":\"long\"},\"dst_mask\":{\"type\":\"long\"},\"ipv4_dst_addr\":{\"type\":\"string\"},\"src_tos\":{\"type\":\"long\"},\"src_mask\":{\"type\":\"long\"},\"version\":{\"type\":\"long\"},\"flow_records\":{\"type\":\"long\"},\"flow_seq_num\":{\"type\":\"long\"},\"ipv4_src_addr\":{\"type\":\"string\"},\"engine_type\":{\"type\":\"long\"},\"engine_id\":{\"type\":\"long\"},\"input_snmp\":{\"type\":\"long\"},\"last_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"sampling_interval\":{\"type\":\"long\"}}},\"@timestamp\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"@version\":{\"type\":\"string\"},\"host\":{\"type\":\"string\"}}}},\"aliases\":[]}}}}\n2015-07-17 17:40:46,916 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:40:46,918 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/logstash-netflow5-2015.05.22 HTTP/1.1\" 200 1277\n2015-07-17 17:40:46,918 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/logstash-netflow5-2015.05.22 [status:200 request:0.003s]\n2015-07-17 17:40:46,918 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:40:46,918 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"elasticsearch\",\"metadata\":{\"templates\":{},\"indices\":{\"logstash-netflow5-2015.05.22\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1432252803701\",\"number_of_shards\":\"5\",\"uuid\":\"SOX7n8mdTyKpP5-VJYm1Gw\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}},\"mappings\":{\"logs\":{\"properties\":{\"netflow\":{\"properties\":{\"dst_as\":{\"type\":\"long\"},\"in_pkts\":{\"type\":\"long\"},\"first_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"ipv4_next_hop\":{\"type\":\"string\"},\"l4_src_port\":{\"type\":\"long\"},\"sampling_algorithm\":{\"type\":\"long\"},\"in_bytes\":{\"type\":\"long\"},\"protocol\":{\"type\":\"long\"},\"tcp_flags\":{\"type\":\"long\"},\"l4_dst_port\":{\"type\":\"long\"},\"src_as\":{\"type\":\"long\"},\"output_snmp\":{\"type\":\"long\"},\"dst_mask\":{\"type\":\"long\"},\"ipv4_dst_addr\":{\"type\":\"string\"},\"src_tos\":{\"type\":\"long\"},\"src_mask\":{\"type\":\"long\"},\"version\":{\"type\":\"long\"},\"flow_records\":{\"type\":\"long\"},\"flow_seq_num\":{\"type\":\"long\"},\"ipv4_src_addr\":{\"type\":\"string\"},\"engine_type\":{\"type\":\"long\"},\"engine_id\":{\"type\":\"long\"},\"input_snmp\":{\"type\":\"long\"},\"last_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"sampling_interval\":{\"type\":\"long\"}}},\"@timestamp\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"@version\":{\"type\":\"string\"},\"host\":{\"type\":\"string\"}}}},\"aliases\":[]}}}}\n2015-07-17 17:40:46,920 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:40:46,938 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/logstash-netflow5-2015.05.23 HTTP/1.1\" 200 1277\n2015-07-17 17:40:46,939 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/logstash-netflow5-2015.05.23 [status:200 request:0.019s]\n2015-07-17 17:40:46,939 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:40:46,939 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"elasticsearch\",\"metadata\":{\"templates\":{},\"indices\":{\"logstash-netflow5-2015.05.23\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1432339202198\",\"number_of_shards\":\"5\",\"uuid\":\"eO4bFSzWQoOcBCZSq87cRw\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}},\"mappings\":{\"logs\":{\"properties\":{\"netflow\":{\"properties\":{\"dst_as\":{\"type\":\"long\"},\"in_pkts\":{\"type\":\"long\"},\"first_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"ipv4_next_hop\":{\"type\":\"string\"},\"l4_src_port\":{\"type\":\"long\"},\"sampling_algorithm\":{\"type\":\"long\"},\"in_bytes\":{\"type\":\"long\"},\"protocol\":{\"type\":\"long\"},\"tcp_flags\":{\"type\":\"long\"},\"l4_dst_port\":{\"type\":\"long\"},\"src_as\":{\"type\":\"long\"},\"output_snmp\":{\"type\":\"long\"},\"dst_mask\":{\"type\":\"long\"},\"ipv4_dst_addr\":{\"type\":\"string\"},\"src_tos\":{\"type\":\"long\"},\"src_mask\":{\"type\":\"long\"},\"version\":{\"type\":\"long\"},\"flow_records\":{\"type\":\"long\"},\"flow_seq_num\":{\"type\":\"long\"},\"ipv4_src_addr\":{\"type\":\"string\"},\"engine_type\":{\"type\":\"long\"},\"engine_id\":{\"type\":\"long\"},\"input_snmp\":{\"type\":\"long\"},\"last_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"sampling_interval\":{\"type\":\"long\"}}},\"@timestamp\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"@version\":{\"type\":\"string\"},\"host\":{\"type\":\"string\"}}}},\"aliases\":[]}}}}\n2015-07-17 17:40:46,941 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:40:46,955 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/logstash-netflow5-2015.05.24 HTTP/1.1\" 200 1277\n2015-07-17 17:40:46,956 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/logstash-netflow5-2015.05.24 [status:200 request:0.015s]\n2015-07-17 17:40:46,956 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:40:46,956 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"elasticsearch\",\"metadata\":{\"templates\":{},\"indices\":{\"logstash-netflow5-2015.05.24\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1432425603806\",\"number_of_shards\":\"5\",\"uuid\":\"UlEyDFlIT6aWQ99J2brdmw\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}},\"mappings\":{\"logs\":{\"properties\":{\"netflow\":{\"properties\":{\"dst_as\":{\"type\":\"long\"},\"in_pkts\":{\"type\":\"long\"},\"first_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"ipv4_next_hop\":{\"type\":\"string\"},\"l4_src_port\":{\"type\":\"long\"},\"sampling_algorithm\":{\"type\":\"long\"},\"in_bytes\":{\"type\":\"long\"},\"protocol\":{\"type\":\"long\"},\"tcp_flags\":{\"type\":\"long\"},\"l4_dst_port\":{\"type\":\"long\"},\"src_as\":{\"type\":\"long\"},\"output_snmp\":{\"type\":\"long\"},\"dst_mask\":{\"type\":\"long\"},\"ipv4_dst_addr\":{\"type\":\"string\"},\"src_tos\":{\"type\":\"long\"},\"src_mask\":{\"type\":\"long\"},\"version\":{\"type\":\"long\"},\"flow_records\":{\"type\":\"long\"},\"flow_seq_num\":{\"type\":\"long\"},\"ipv4_src_addr\":{\"type\":\"string\"},\"engine_type\":{\"type\":\"long\"},\"engine_id\":{\"type\":\"long\"},\"input_snmp\":{\"type\":\"long\"},\"last_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"sampling_interval\":{\"type\":\"long\"}}},\"@timestamp\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"@version\":{\"type\":\"string\"},\"host\":{\"type\":\"string\"}}}},\"aliases\":[]}}}}\n2015-07-17 17:40:46,958 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:40:46,959 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/logstash-netflow5-2015.05.25 HTTP/1.1\" 200 1277\n2015-07-17 17:40:46,960 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/logstash-netflow5-2015.05.25 [status:200 request:0.002s]\n2015-07-17 17:40:46,960 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:40:46,960 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"elasticsearch\",\"metadata\":{\"templates\":{},\"indices\":{\"logstash-netflow5-2015.05.25\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1432512002237\",\"number_of_shards\":\"5\",\"uuid\":\"blPCzOZcRLiQ2lWcelNN8w\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}},\"mappings\":{\"logs\":{\"properties\":{\"netflow\":{\"properties\":{\"dst_as\":{\"type\":\"long\"},\"in_pkts\":{\"type\":\"long\"},\"first_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"ipv4_next_hop\":{\"type\":\"string\"},\"l4_src_port\":{\"type\":\"long\"},\"sampling_algorithm\":{\"type\":\"long\"},\"in_bytes\":{\"type\":\"long\"},\"protocol\":{\"type\":\"long\"},\"tcp_flags\":{\"type\":\"long\"},\"l4_dst_port\":{\"type\":\"long\"},\"src_as\":{\"type\":\"long\"},\"output_snmp\":{\"type\":\"long\"},\"dst_mask\":{\"type\":\"long\"},\"ipv4_dst_addr\":{\"type\":\"string\"},\"src_tos\":{\"type\":\"long\"},\"src_mask\":{\"type\":\"long\"},\"version\":{\"type\":\"long\"},\"flow_records\":{\"type\":\"long\"},\"flow_seq_num\":{\"type\":\"long\"},\"ipv4_src_addr\":{\"type\":\"string\"},\"engine_type\":{\"type\":\"long\"},\"engine_id\":{\"type\":\"long\"},\"input_snmp\":{\"type\":\"long\"},\"last_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"sampling_interval\":{\"type\":\"long\"}}},\"@timestamp\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"@version\":{\"type\":\"string\"},\"host\":{\"type\":\"string\"}}}},\"aliases\":[]}}}}\n2015-07-17 17:40:46,961 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:40:46,963 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/logstash-netflow5-2015.05.26 HTTP/1.1\" 200 1277\n2015-07-17 17:40:46,963 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/logstash-netflow5-2015.05.26 [status:200 request:0.002s]\n2015-07-17 17:40:46,963 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:40:46,963 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"elasticsearch\",\"metadata\":{\"templates\":{},\"indices\":{\"logstash-netflow5-2015.05.26\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1432598401799\",\"number_of_shards\":\"5\",\"uuid\":\"2TLDtEjPRNmTjCqqZAFbAw\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}},\"mappings\":{\"logs\":{\"properties\":{\"netflow\":{\"properties\":{\"dst_as\":{\"type\":\"long\"},\"in_pkts\":{\"type\":\"long\"},\"first_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"ipv4_next_hop\":{\"type\":\"string\"},\"l4_src_port\":{\"type\":\"long\"},\"sampling_algorithm\":{\"type\":\"long\"},\"in_bytes\":{\"type\":\"long\"},\"protocol\":{\"type\":\"long\"},\"tcp_flags\":{\"type\":\"long\"},\"l4_dst_port\":{\"type\":\"long\"},\"src_as\":{\"type\":\"long\"},\"output_snmp\":{\"type\":\"long\"},\"dst_mask\":{\"type\":\"long\"},\"ipv4_dst_addr\":{\"type\":\"string\"},\"src_tos\":{\"type\":\"long\"},\"src_mask\":{\"type\":\"long\"},\"version\":{\"type\":\"long\"},\"flow_records\":{\"type\":\"long\"},\"flow_seq_num\":{\"type\":\"long\"},\"ipv4_src_addr\":{\"type\":\"string\"},\"engine_type\":{\"type\":\"long\"},\"engine_id\":{\"type\":\"long\"},\"input_snmp\":{\"type\":\"long\"},\"last_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"sampling_interval\":{\"type\":\"long\"}}},\"@timestamp\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"@version\":{\"type\":\"string\"},\"host\":{\"type\":\"string\"}}}},\"aliases\":[]}}}}\n2015-07-17 17:40:46,965 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:40:46,966 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/logstash-netflow5-2015.05.27 HTTP/1.1\" 200 1277\n2015-07-17 17:40:46,966 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/logstash-netflow5-2015.05.27 [status:200 request:0.002s]\n2015-07-17 17:40:46,967 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:40:46,967 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"elasticsearch\",\"metadata\":{\"templates\":{},\"indices\":{\"logstash-netflow5-2015.05.27\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1432684803412\",\"number_of_shards\":\"5\",\"uuid\":\"SU-PPJSTRv6BUfWb7lDWMg\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}},\"mappings\":{\"logs\":{\"properties\":{\"netflow\":{\"properties\":{\"dst_as\":{\"type\":\"long\"},\"in_pkts\":{\"type\":\"long\"},\"first_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"ipv4_next_hop\":{\"type\":\"string\"},\"l4_src_port\":{\"type\":\"long\"},\"sampling_algorithm\":{\"type\":\"long\"},\"in_bytes\":{\"type\":\"long\"},\"protocol\":{\"type\":\"long\"},\"tcp_flags\":{\"type\":\"long\"},\"l4_dst_port\":{\"type\":\"long\"},\"src_as\":{\"type\":\"long\"},\"output_snmp\":{\"type\":\"long\"},\"dst_mask\":{\"type\":\"long\"},\"ipv4_dst_addr\":{\"type\":\"string\"},\"src_tos\":{\"type\":\"long\"},\"src_mask\":{\"type\":\"long\"},\"version\":{\"type\":\"long\"},\"flow_records\":{\"type\":\"long\"},\"flow_seq_num\":{\"type\":\"long\"},\"ipv4_src_addr\":{\"type\":\"string\"},\"engine_type\":{\"type\":\"long\"},\"engine_id\":{\"type\":\"long\"},\"input_snmp\":{\"type\":\"long\"},\"last_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"sampling_interval\":{\"type\":\"long\"}}},\"@timestamp\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"@version\":{\"type\":\"string\"},\"host\":{\"type\":\"string\"}}}},\"aliases\":[]}}}}\n2015-07-17 17:40:46,968 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:40:46,971 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/logstash-netflow5-2015.05.28 HTTP/1.1\" 200 1277\n2015-07-17 17:40:46,971 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/logstash-netflow5-2015.05.28 [status:200 request:0.003s]\n2015-07-17 17:40:46,971 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:40:46,972 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"elasticsearch\",\"metadata\":{\"templates\":{},\"indices\":{\"logstash-netflow5-2015.05.28\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1432771202947\",\"number_of_shards\":\"5\",\"uuid\":\"JEB76nchTfm0Jeepzk9SHg\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}},\"mappings\":{\"logs\":{\"properties\":{\"netflow\":{\"properties\":{\"dst_as\":{\"type\":\"long\"},\"in_pkts\":{\"type\":\"long\"},\"first_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"ipv4_next_hop\":{\"type\":\"string\"},\"l4_src_port\":{\"type\":\"long\"},\"sampling_algorithm\":{\"type\":\"long\"},\"in_bytes\":{\"type\":\"long\"},\"protocol\":{\"type\":\"long\"},\"tcp_flags\":{\"type\":\"long\"},\"l4_dst_port\":{\"type\":\"long\"},\"src_as\":{\"type\":\"long\"},\"output_snmp\":{\"type\":\"long\"},\"dst_mask\":{\"type\":\"long\"},\"ipv4_dst_addr\":{\"type\":\"string\"},\"src_tos\":{\"type\":\"long\"},\"src_mask\":{\"type\":\"long\"},\"version\":{\"type\":\"long\"},\"flow_records\":{\"type\":\"long\"},\"flow_seq_num\":{\"type\":\"long\"},\"ipv4_src_addr\":{\"type\":\"string\"},\"engine_type\":{\"type\":\"long\"},\"engine_id\":{\"type\":\"long\"},\"input_snmp\":{\"type\":\"long\"},\"last_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"sampling_interval\":{\"type\":\"long\"}}},\"@timestamp\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"@version\":{\"type\":\"string\"},\"host\":{\"type\":\"string\"}}}},\"aliases\":[]}}}}\n2015-07-17 17:40:46,973 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:40:46,988 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/logstash-netflow5-2015.05.29 HTTP/1.1\" 200 1277\n2015-07-17 17:40:46,988 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/logstash-netflow5-2015.05.29 [status:200 request:0.015s]\n2015-07-17 17:40:46,988 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:40:46,988 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"elasticsearch\",\"metadata\":{\"templates\":{},\"indices\":{\"logstash-netflow5-2015.05.29\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1432887483995\",\"number_of_shards\":\"5\",\"uuid\":\"4O7dxtZ0QkK4zuq7r6EMPQ\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}},\"mappings\":{\"logs\":{\"properties\":{\"netflow\":{\"properties\":{\"dst_as\":{\"type\":\"long\"},\"in_pkts\":{\"type\":\"long\"},\"first_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"ipv4_next_hop\":{\"type\":\"string\"},\"l4_src_port\":{\"type\":\"long\"},\"sampling_algorithm\":{\"type\":\"long\"},\"in_bytes\":{\"type\":\"long\"},\"protocol\":{\"type\":\"long\"},\"tcp_flags\":{\"type\":\"long\"},\"l4_dst_port\":{\"type\":\"long\"},\"src_as\":{\"type\":\"long\"},\"output_snmp\":{\"type\":\"long\"},\"dst_mask\":{\"type\":\"long\"},\"ipv4_dst_addr\":{\"type\":\"string\"},\"src_tos\":{\"type\":\"long\"},\"src_mask\":{\"type\":\"long\"},\"version\":{\"type\":\"long\"},\"flow_records\":{\"type\":\"long\"},\"flow_seq_num\":{\"type\":\"long\"},\"ipv4_src_addr\":{\"type\":\"string\"},\"engine_type\":{\"type\":\"long\"},\"engine_id\":{\"type\":\"long\"},\"input_snmp\":{\"type\":\"long\"},\"last_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"sampling_interval\":{\"type\":\"long\"}}},\"@timestamp\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"@version\":{\"type\":\"string\"},\"host\":{\"type\":\"string\"}}}},\"aliases\":[]}}}}\n2015-07-17 17:40:46,990 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:40:46,995 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/logstash-netflow5-2015.05.30 HTTP/1.1\" 200 1277\n2015-07-17 17:40:46,995 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/logstash-netflow5-2015.05.30 [status:200 request:0.005s]\n2015-07-17 17:40:46,995 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:40:46,995 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"elasticsearch\",\"metadata\":{\"templates\":{},\"indices\":{\"logstash-netflow5-2015.05.30\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1432944002711\",\"number_of_shards\":\"5\",\"uuid\":\"uBbxYdG1TTS1OeXIsYXOKg\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}},\"mappings\":{\"logs\":{\"properties\":{\"netflow\":{\"properties\":{\"dst_as\":{\"type\":\"long\"},\"in_pkts\":{\"type\":\"long\"},\"first_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"ipv4_next_hop\":{\"type\":\"string\"},\"l4_src_port\":{\"type\":\"long\"},\"sampling_algorithm\":{\"type\":\"long\"},\"in_bytes\":{\"type\":\"long\"},\"protocol\":{\"type\":\"long\"},\"tcp_flags\":{\"type\":\"long\"},\"l4_dst_port\":{\"type\":\"long\"},\"src_as\":{\"type\":\"long\"},\"output_snmp\":{\"type\":\"long\"},\"dst_mask\":{\"type\":\"long\"},\"ipv4_dst_addr\":{\"type\":\"string\"},\"src_tos\":{\"type\":\"long\"},\"src_mask\":{\"type\":\"long\"},\"version\":{\"type\":\"long\"},\"flow_records\":{\"type\":\"long\"},\"flow_seq_num\":{\"type\":\"long\"},\"ipv4_src_addr\":{\"type\":\"string\"},\"engine_type\":{\"type\":\"long\"},\"engine_id\":{\"type\":\"long\"},\"input_snmp\":{\"type\":\"long\"},\"last_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"sampling_interval\":{\"type\":\"long\"}}},\"@timestamp\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"@version\":{\"type\":\"string\"},\"host\":{\"type\":\"string\"}}}},\"aliases\":[]}}}}\n2015-07-17 17:40:46,997 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:40:47,003 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/logstash-netflow5-2015.05.31 HTTP/1.1\" 200 1277\n2015-07-17 17:40:47,004 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/logstash-netflow5-2015.05.31 [status:200 request:0.007s]\n2015-07-17 17:40:47,004 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:40:47,004 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"elasticsearch\",\"metadata\":{\"templates\":{},\"indices\":{\"logstash-netflow5-2015.05.31\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1433030402589\",\"number_of_shards\":\"5\",\"uuid\":\"Suwo3OE0R8CyESKjcOBhzw\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}},\"mappings\":{\"logs\":{\"properties\":{\"netflow\":{\"properties\":{\"dst_as\":{\"type\":\"long\"},\"in_pkts\":{\"type\":\"long\"},\"first_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"ipv4_next_hop\":{\"type\":\"string\"},\"l4_src_port\":{\"type\":\"long\"},\"sampling_algorithm\":{\"type\":\"long\"},\"in_bytes\":{\"type\":\"long\"},\"protocol\":{\"type\":\"long\"},\"tcp_flags\":{\"type\":\"long\"},\"l4_dst_port\":{\"type\":\"long\"},\"src_as\":{\"type\":\"long\"},\"output_snmp\":{\"type\":\"long\"},\"dst_mask\":{\"type\":\"long\"},\"ipv4_dst_addr\":{\"type\":\"string\"},\"src_tos\":{\"type\":\"long\"},\"src_mask\":{\"type\":\"long\"},\"version\":{\"type\":\"long\"},\"flow_records\":{\"type\":\"long\"},\"flow_seq_num\":{\"type\":\"long\"},\"ipv4_src_addr\":{\"type\":\"string\"},\"engine_type\":{\"type\":\"long\"},\"engine_id\":{\"type\":\"long\"},\"input_snmp\":{\"type\":\"long\"},\"last_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"sampling_interval\":{\"type\":\"long\"}}},\"@timestamp\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"@version\":{\"type\":\"string\"},\"host\":{\"type\":\"string\"}}}},\"aliases\":[]}}}}\n2015-07-17 17:40:47,005 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:40:47,017 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/logstash-netflow5-2015.06.01 HTTP/1.1\" 200 1277\n2015-07-17 17:40:47,018 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/logstash-netflow5-2015.06.01 [status:200 request:0.012s]\n2015-07-17 17:40:47,018 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:40:47,018 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"elasticsearch\",\"metadata\":{\"templates\":{},\"indices\":{\"logstash-netflow5-2015.06.01\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1433116801198\",\"number_of_shards\":\"5\",\"uuid\":\"RpoNJseySheEipEWOXurQg\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}},\"mappings\":{\"logs\":{\"properties\":{\"netflow\":{\"properties\":{\"dst_as\":{\"type\":\"long\"},\"in_pkts\":{\"type\":\"long\"},\"first_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"ipv4_next_hop\":{\"type\":\"string\"},\"l4_src_port\":{\"type\":\"long\"},\"sampling_algorithm\":{\"type\":\"long\"},\"in_bytes\":{\"type\":\"long\"},\"protocol\":{\"type\":\"long\"},\"tcp_flags\":{\"type\":\"long\"},\"l4_dst_port\":{\"type\":\"long\"},\"src_as\":{\"type\":\"long\"},\"output_snmp\":{\"type\":\"long\"},\"dst_mask\":{\"type\":\"long\"},\"ipv4_dst_addr\":{\"type\":\"string\"},\"src_tos\":{\"type\":\"long\"},\"src_mask\":{\"type\":\"long\"},\"version\":{\"type\":\"long\"},\"flow_records\":{\"type\":\"long\"},\"flow_seq_num\":{\"type\":\"long\"},\"ipv4_src_addr\":{\"type\":\"string\"},\"engine_type\":{\"type\":\"long\"},\"engine_id\":{\"type\":\"long\"},\"input_snmp\":{\"type\":\"long\"},\"last_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"sampling_interval\":{\"type\":\"long\"}}},\"@timestamp\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"@version\":{\"type\":\"string\"},\"host\":{\"type\":\"string\"}}}},\"aliases\":[]}}}}\n2015-07-17 17:40:47,019 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:40:47,048 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/logstash-netflow5-2015.06.02 HTTP/1.1\" 200 1277\n2015-07-17 17:40:47,048 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/logstash-netflow5-2015.06.02 [status:200 request:0.029s]\n2015-07-17 17:40:47,048 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:40:47,049 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"elasticsearch\",\"metadata\":{\"templates\":{},\"indices\":{\"logstash-netflow5-2015.06.02\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1433203199922\",\"number_of_shards\":\"5\",\"uuid\":\"Fl9otSpfRPixCPTLi3-urA\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}},\"mappings\":{\"logs\":{\"properties\":{\"netflow\":{\"properties\":{\"dst_as\":{\"type\":\"long\"},\"in_pkts\":{\"type\":\"long\"},\"first_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"ipv4_next_hop\":{\"type\":\"string\"},\"l4_src_port\":{\"type\":\"long\"},\"sampling_algorithm\":{\"type\":\"long\"},\"in_bytes\":{\"type\":\"long\"},\"protocol\":{\"type\":\"long\"},\"tcp_flags\":{\"type\":\"long\"},\"l4_dst_port\":{\"type\":\"long\"},\"src_as\":{\"type\":\"long\"},\"output_snmp\":{\"type\":\"long\"},\"dst_mask\":{\"type\":\"long\"},\"ipv4_dst_addr\":{\"type\":\"string\"},\"src_tos\":{\"type\":\"long\"},\"src_mask\":{\"type\":\"long\"},\"version\":{\"type\":\"long\"},\"flow_records\":{\"type\":\"long\"},\"flow_seq_num\":{\"type\":\"long\"},\"ipv4_src_addr\":{\"type\":\"string\"},\"engine_type\":{\"type\":\"long\"},\"engine_id\":{\"type\":\"long\"},\"input_snmp\":{\"type\":\"long\"},\"last_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"sampling_interval\":{\"type\":\"long\"}}},\"@timestamp\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"@version\":{\"type\":\"string\"},\"host\":{\"type\":\"string\"}}}},\"aliases\":[]}}}}\n2015-07-17 17:40:47,050 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:40:47,060 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/logstash-netflow5-2015.06.03 HTTP/1.1\" 200 1277\n2015-07-17 17:40:47,060 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/logstash-netflow5-2015.06.03 [status:200 request:0.010s]\n2015-07-17 17:40:47,061 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:40:47,061 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"elasticsearch\",\"metadata\":{\"templates\":{},\"indices\":{\"logstash-netflow5-2015.06.03\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1433289600773\",\"number_of_shards\":\"5\",\"uuid\":\"qStFxkTQR3KQaQDFQCApHw\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}},\"mappings\":{\"logs\":{\"properties\":{\"netflow\":{\"properties\":{\"dst_as\":{\"type\":\"long\"},\"in_pkts\":{\"type\":\"long\"},\"first_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"ipv4_next_hop\":{\"type\":\"string\"},\"l4_src_port\":{\"type\":\"long\"},\"sampling_algorithm\":{\"type\":\"long\"},\"in_bytes\":{\"type\":\"long\"},\"protocol\":{\"type\":\"long\"},\"tcp_flags\":{\"type\":\"long\"},\"l4_dst_port\":{\"type\":\"long\"},\"src_as\":{\"type\":\"long\"},\"output_snmp\":{\"type\":\"long\"},\"dst_mask\":{\"type\":\"long\"},\"ipv4_dst_addr\":{\"type\":\"string\"},\"src_tos\":{\"type\":\"long\"},\"src_mask\":{\"type\":\"long\"},\"version\":{\"type\":\"long\"},\"flow_records\":{\"type\":\"long\"},\"flow_seq_num\":{\"type\":\"long\"},\"ipv4_src_addr\":{\"type\":\"string\"},\"engine_type\":{\"type\":\"long\"},\"engine_id\":{\"type\":\"long\"},\"input_snmp\":{\"type\":\"long\"},\"last_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"sampling_interval\":{\"type\":\"long\"}}},\"@timestamp\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"@version\":{\"type\":\"string\"},\"host\":{\"type\":\"string\"}}}},\"aliases\":[]}}}}\n2015-07-17 17:40:47,062 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:40:50,455 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/logstash-netflow5-2015.06.04 HTTP/1.1\" 200 1277\n2015-07-17 17:40:50,455 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/logstash-netflow5-2015.06.04 [status:200 request:3.393s]\n2015-07-17 17:40:50,455 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:40:50,455 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"elasticsearch\",\"metadata\":{\"templates\":{},\"indices\":{\"logstash-netflow5-2015.06.04\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1433375997917\",\"number_of_shards\":\"5\",\"uuid\":\"f1rCLjnOSjyVcdMToJwMCA\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}},\"mappings\":{\"logs\":{\"properties\":{\"netflow\":{\"properties\":{\"dst_as\":{\"type\":\"long\"},\"in_pkts\":{\"type\":\"long\"},\"first_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"ipv4_next_hop\":{\"type\":\"string\"},\"l4_src_port\":{\"type\":\"long\"},\"sampling_algorithm\":{\"type\":\"long\"},\"in_bytes\":{\"type\":\"long\"},\"protocol\":{\"type\":\"long\"},\"tcp_flags\":{\"type\":\"long\"},\"l4_dst_port\":{\"type\":\"long\"},\"src_as\":{\"type\":\"long\"},\"output_snmp\":{\"type\":\"long\"},\"dst_mask\":{\"type\":\"long\"},\"ipv4_dst_addr\":{\"type\":\"string\"},\"src_tos\":{\"type\":\"long\"},\"src_mask\":{\"type\":\"long\"},\"version\":{\"type\":\"long\"},\"flow_records\":{\"type\":\"long\"},\"flow_seq_num\":{\"type\":\"long\"},\"ipv4_src_addr\":{\"type\":\"string\"},\"engine_type\":{\"type\":\"long\"},\"engine_id\":{\"type\":\"long\"},\"input_snmp\":{\"type\":\"long\"},\"last_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"sampling_interval\":{\"type\":\"long\"}}},\"@timestamp\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"@version\":{\"type\":\"string\"},\"host\":{\"type\":\"string\"}}}},\"aliases\":[]}}}}\n2015-07-17 17:40:50,457 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:40:50,458 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/logstash-netflow5-2015.06.05.orig HTTP/1.1\" 200 1892\n2015-07-17 17:40:50,458 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/logstash-netflow5-2015.06.05.orig [status:200 request:0.002s]\n2015-07-17 17:40:50,459 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:40:50,459 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"elasticsearch\",\"metadata\":{\"templates\":{},\"indices\":{\"logstash-netflow5-2015.06.05.orig\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1433462401945\",\"number_of_shards\":\"5\",\"uuid\":\"RRYwP8uVSQKesoy38MAvCw\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}},\"mappings\":{\"logs\":{\"properties\":{\"netflow\":{\"properties\":{\"dst_as\":{\"type\":\"long\"},\"in_pkts\":{\"type\":\"long\"},\"first_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"flowset_id\":{\"type\":\"long\"},\"ipv4_next_hop\":{\"type\":\"string\"},\"l4_src_port\":{\"type\":\"long\"},\"sampling_algorithm\":{\"type\":\"long\"},\"in_bytes\":{\"type\":\"long\"},\"protocol\":{\"type\":\"long\"},\"tcp_flags\":{\"type\":\"long\"},\"l4_dst_port\":{\"type\":\"long\"},\"direction\":{\"type\":\"long\"},\"src_as\":{\"type\":\"long\"},\"output_snmp\":{\"type\":\"long\"},\"dst_mask\":{\"type\":\"long\"},\"ipv4_dst_addr\":{\"type\":\"string\"},\"src_tos\":{\"type\":\"long\"},\"src_mask\":{\"type\":\"long\"},\"version\":{\"type\":\"long\"},\"flow_records\":{\"type\":\"long\"},\"flow_seq_num\":{\"type\":\"long\"},\"ipv4_src_addr\":{\"type\":\"string\"},\"engine_type\":{\"type\":\"long\"},\"engine_id\":{\"type\":\"long\"},\"input_snmp\":{\"type\":\"long\"},\"last_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"flow_sampler_id\":{\"type\":\"long\"},\"sampling_interval\":{\"type\":\"long\"}}},\"@timestamp\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"geoip\":{\"properties\":{\"timezone\":{\"type\":\"string\"},\"area_code\":{\"type\":\"long\"},\"ip\":{\"type\":\"string\"},\"latitude\":{\"type\":\"double\"},\"coordinates\":{\"type\":\"double\"},\"continent_code\":{\"type\":\"string\"},\"city_name\":{\"type\":\"string\"},\"country_code2\":{\"type\":\"string\"},\"country_name\":{\"type\":\"string\"},\"dma_code\":{\"type\":\"long\"},\"country_code3\":{\"type\":\"string\"},\"location\":{\"type\":\"double\"},\"region_name\":{\"type\":\"string\"},\"real_region_name\":{\"type\":\"string\"},\"postal_code\":{\"type\":\"string\"},\"longitude\":{\"type\":\"double\"}}},\"@version\":{\"type\":\"string\"},\"host\":{\"type\":\"string\"}}}},\"aliases\":[]}}}}\n2015-07-17 17:40:50,461 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:40:50,462 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/logstash-netflow5-2015.06.06 HTTP/1.1\" 200 4191\n2015-07-17 17:40:50,463 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/logstash-netflow5-2015.06.06 [status:200 request:0.002s]\n2015-07-17 17:40:50,463 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:40:50,463 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"elasticsearch\",\"metadata\":{\"templates\":{},\"indices\":{\"logstash-netflow5-2015.06.06\":{\"state\":\"open\",\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1433548798251\",\"number_of_shards\":\"5\",\"uuid\":\"eJo6TxomQEmXLfntpfT9Iw\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}},\"mappings\":{\"_default\":{\"dynamic_templates\":[{\"message_field\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\"},\"match_mapping_type\":\"string\",\"match\":\"message\"}},{\"string_fields\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"match_mapping_type\":\"string\",\"match\":\"\"}}],\"_all\":{\"omit_norms\":true,\"enabled\":true},\"properties\":{\"geoip\":{\"dynamic\":\"true\",\"properties\":{\"location\":{\"type\":\"geo_point\"}}},\"@version\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"logs\":{\"dynamic_templates\":[{\"message_field\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\"},\"match_mapping_type\":\"string\",\"match\":\"message\"}},{\"string_fields\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"match_mapping_type\":\"string\",\"match\":\"\"}}],\"all\":{\"omit_norms\":true,\"enabled\":true},\"properties\":{\"netflow\":{\"properties\":{\"output_snmp\":{\"type\":\"long\"},\"dst_as\":{\"type\":\"long\"},\"dst_mask\":{\"type\":\"long\"},\"in_pkts\":{\"type\":\"long\"},\"ipv4_dst_addr\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"src_tos\":{\"type\":\"long\"},\"first_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"flowset_id\":{\"type\":\"long\"},\"ipv4_next_hop\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"l4_src_port\":{\"type\":\"long\"},\"src_mask\":{\"type\":\"long\"},\"version\":{\"type\":\"long\"},\"flow_seq_num\":{\"type\":\"long\"},\"ipv4_src_addr\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"in_bytes\":{\"type\":\"long\"},\"protocol\":{\"type\":\"long\"},\"input_snmp\":{\"type\":\"long\"},\"last_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"tcp_flags\":{\"type\":\"long\"},\"flow_sampler_id\":{\"type\":\"long\"},\"l4_dst_port\":{\"type\":\"long\"},\"direction\":{\"type\":\"long\"},\"src_as\":{\"type\":\"long\"}}},\"@timestamp\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"geoip\":{\"dynamic\":\"true\",\"properties\":{\"timezone\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"area_code\":{\"type\":\"long\"},\"ip\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"latitude\":{\"type\":\"double\"},\"coordinates\":{\"type\":\"double\"},\"continent_code\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"city_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"country_code2\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"country_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"dma_code\":{\"type\":\"long\"},\"country_code3\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"location\":{\"type\":\"geo_point\"},\"region_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"real_region_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"postal_code\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"longitude\":{\"type\":\"double\"}}},\"@version\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"host\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}}}}},\"aliases\":[]}}}}\n2015-07-17 17:40:50,467 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:40:50,469 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/logstash-netflow5-2015.06.07 HTTP/1.1\" 200 4191\n2015-07-17 17:40:50,469 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/logstash-netflow5-2015.06.07 [status:200 request:0.002s]\n2015-07-17 17:40:50,469 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:40:50,469 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"elasticsearch\",\"metadata\":{\"templates\":{},\"indices\":{\"logstash-netflow5-2015.06.07\":{\"state\":\"open\",\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1433635198724\",\"number_of_shards\":\"5\",\"uuid\":\"FEIW-L_uQ4iwoMuazTAsUQ\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}},\"mappings\":{\"_default\":{\"dynamic_templates\":[{\"message_field\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\"},\"match_mapping_type\":\"string\",\"match\":\"message\"}},{\"string_fields\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"match_mapping_type\":\"string\",\"match\":\"\"}}],\"_all\":{\"omit_norms\":true,\"enabled\":true},\"properties\":{\"geoip\":{\"dynamic\":\"true\",\"properties\":{\"location\":{\"type\":\"geo_point\"}}},\"@version\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"logs\":{\"dynamic_templates\":[{\"message_field\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\"},\"match_mapping_type\":\"string\",\"match\":\"message\"}},{\"string_fields\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"match_mapping_type\":\"string\",\"match\":\"\"}}],\"all\":{\"omit_norms\":true,\"enabled\":true},\"properties\":{\"netflow\":{\"properties\":{\"output_snmp\":{\"type\":\"long\"},\"dst_as\":{\"type\":\"long\"},\"dst_mask\":{\"type\":\"long\"},\"in_pkts\":{\"type\":\"long\"},\"ipv4_dst_addr\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"src_tos\":{\"type\":\"long\"},\"first_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"flowset_id\":{\"type\":\"long\"},\"ipv4_next_hop\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"l4_src_port\":{\"type\":\"long\"},\"src_mask\":{\"type\":\"long\"},\"version\":{\"type\":\"long\"},\"flow_seq_num\":{\"type\":\"long\"},\"ipv4_src_addr\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"in_bytes\":{\"type\":\"long\"},\"protocol\":{\"type\":\"long\"},\"input_snmp\":{\"type\":\"long\"},\"last_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"tcp_flags\":{\"type\":\"long\"},\"flow_sampler_id\":{\"type\":\"long\"},\"l4_dst_port\":{\"type\":\"long\"},\"direction\":{\"type\":\"long\"},\"src_as\":{\"type\":\"long\"}}},\"@timestamp\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"geoip\":{\"dynamic\":\"true\",\"properties\":{\"timezone\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"area_code\":{\"type\":\"long\"},\"ip\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"latitude\":{\"type\":\"double\"},\"coordinates\":{\"type\":\"double\"},\"continent_code\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"city_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"country_code2\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"country_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"dma_code\":{\"type\":\"long\"},\"country_code3\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"location\":{\"type\":\"geo_point\"},\"region_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"real_region_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"postal_code\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"longitude\":{\"type\":\"double\"}}},\"@version\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"host\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}}}}},\"aliases\":[]}}}}\n2015-07-17 17:40:50,473 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:40:50,475 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/logstash-netflow5-2015.06.08 HTTP/1.1\" 200 4191\n2015-07-17 17:40:50,475 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/logstash-netflow5-2015.06.08 [status:200 request:0.002s]\n2015-07-17 17:40:50,476 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:40:50,476 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"elasticsearch\",\"metadata\":{\"templates\":{},\"indices\":{\"logstash-netflow5-2015.06.08\":{\"state\":\"open\",\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1433721598182\",\"number_of_shards\":\"5\",\"uuid\":\"YGPXODdfTkawzOtdUQJYgg\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}},\"mappings\":{\"_default\":{\"dynamic_templates\":[{\"message_field\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\"},\"match_mapping_type\":\"string\",\"match\":\"message\"}},{\"string_fields\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"match_mapping_type\":\"string\",\"match\":\"\"}}],\"_all\":{\"omit_norms\":true,\"enabled\":true},\"properties\":{\"geoip\":{\"dynamic\":\"true\",\"properties\":{\"location\":{\"type\":\"geo_point\"}}},\"@version\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"logs\":{\"dynamic_templates\":[{\"message_field\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\"},\"match_mapping_type\":\"string\",\"match\":\"message\"}},{\"string_fields\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"match_mapping_type\":\"string\",\"match\":\"\"}}],\"all\":{\"omit_norms\":true,\"enabled\":true},\"properties\":{\"netflow\":{\"properties\":{\"output_snmp\":{\"type\":\"long\"},\"dst_as\":{\"type\":\"long\"},\"dst_mask\":{\"type\":\"long\"},\"in_pkts\":{\"type\":\"long\"},\"ipv4_dst_addr\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"src_tos\":{\"type\":\"long\"},\"first_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"flowset_id\":{\"type\":\"long\"},\"ipv4_next_hop\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"l4_src_port\":{\"type\":\"long\"},\"src_mask\":{\"type\":\"long\"},\"version\":{\"type\":\"long\"},\"flow_seq_num\":{\"type\":\"long\"},\"ipv4_src_addr\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"in_bytes\":{\"type\":\"long\"},\"protocol\":{\"type\":\"long\"},\"input_snmp\":{\"type\":\"long\"},\"last_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"tcp_flags\":{\"type\":\"long\"},\"flow_sampler_id\":{\"type\":\"long\"},\"l4_dst_port\":{\"type\":\"long\"},\"direction\":{\"type\":\"long\"},\"src_as\":{\"type\":\"long\"}}},\"@timestamp\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"geoip\":{\"dynamic\":\"true\",\"properties\":{\"timezone\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"area_code\":{\"type\":\"long\"},\"ip\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"latitude\":{\"type\":\"double\"},\"coordinates\":{\"type\":\"double\"},\"continent_code\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"city_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"country_code2\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"country_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"dma_code\":{\"type\":\"long\"},\"country_code3\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"location\":{\"type\":\"geo_point\"},\"region_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"real_region_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"postal_code\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"longitude\":{\"type\":\"double\"}}},\"@version\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"host\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}}}}},\"aliases\":[]}}}}\n2015-07-17 17:40:50,480 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:40:50,482 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/logstash-netflow5-2015.06.09 HTTP/1.1\" 200 4453\n2015-07-17 17:40:50,482 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/logstash-netflow5-2015.06.09 [status:200 request:0.002s]\n2015-07-17 17:40:50,482 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:40:50,483 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"elasticsearch\",\"metadata\":{\"templates\":{},\"indices\":{\"logstash-netflow5-2015.06.09\":{\"state\":\"open\",\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1433807995794\",\"number_of_shards\":\"5\",\"uuid\":\"hQygCvHmTCOULb10hVOm1Q\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}},\"mappings\":{\"_default\":{\"dynamic_templates\":[{\"message_field\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\"},\"match_mapping_type\":\"string\",\"match\":\"message\"}},{\"string_fields\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"match_mapping_type\":\"string\",\"match\":\"\"}}],\"_all\":{\"omit_norms\":true,\"enabled\":true},\"properties\":{\"geoip\":{\"dynamic\":\"true\",\"properties\":{\"location\":{\"type\":\"geo_point\"}}},\"@version\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"logs\":{\"dynamic_templates\":[{\"message_field\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\"},\"match_mapping_type\":\"string\",\"match\":\"message\"}},{\"string_fields\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"match_mapping_type\":\"string\",\"match\":\"\"}}],\"all\":{\"omit_norms\":true,\"enabled\":true},\"properties\":{\"netflow\":{\"properties\":{\"dst_as\":{\"type\":\"long\"},\"in_pkts\":{\"type\":\"long\"},\"if_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"first_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"flowset_id\":{\"type\":\"long\"},\"ipv4_next_hop\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"l4_src_port\":{\"type\":\"long\"},\"in_bytes\":{\"type\":\"long\"},\"protocol\":{\"type\":\"long\"},\"if_desc\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"tcp_flags\":{\"type\":\"long\"},\"l4_dst_port\":{\"type\":\"long\"},\"direction\":{\"type\":\"long\"},\"src_as\":{\"type\":\"long\"},\"output_snmp\":{\"type\":\"long\"},\"dst_mask\":{\"type\":\"long\"},\"ipv4_dst_addr\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"src_tos\":{\"type\":\"long\"},\"src_mask\":{\"type\":\"long\"},\"version\":{\"type\":\"long\"},\"flow_seq_num\":{\"type\":\"long\"},\"ipv4_src_addr\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"input_snmp\":{\"type\":\"long\"},\"last_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"flow_sampler_id\":{\"type\":\"long\"}}},\"@timestamp\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"geoip\":{\"dynamic\":\"true\",\"properties\":{\"timezone\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"area_code\":{\"type\":\"long\"},\"ip\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"latitude\":{\"type\":\"double\"},\"coordinates\":{\"type\":\"double\"},\"continent_code\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"city_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"country_code2\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"country_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"dma_code\":{\"type\":\"long\"},\"country_code3\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"location\":{\"type\":\"geo_point\"},\"region_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"real_region_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"postal_code\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"longitude\":{\"type\":\"double\"}}},\"@version\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"host\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}}}}},\"aliases\":[]}}}}\n2015-07-17 17:40:50,487 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:40:50,496 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/logstash-netflow5-2015.06.10 HTTP/1.1\" 200 4453\n2015-07-17 17:40:50,496 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/logstash-netflow5-2015.06.10 [status:200 request:0.009s]\n2015-07-17 17:40:50,497 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:40:50,497 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"elasticsearch\",\"metadata\":{\"templates\":{},\"indices\":{\"logstash-netflow5-2015.06.10\":{\"state\":\"open\",\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1433894396190\",\"number_of_shards\":\"5\",\"uuid\":\"W6uCXI9oRjqmSVBHrarPiQ\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}},\"mappings\":{\"_default\":{\"dynamic_templates\":[{\"message_field\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\"},\"match_mapping_type\":\"string\",\"match\":\"message\"}},{\"string_fields\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"match_mapping_type\":\"string\",\"match\":\"\"}}],\"_all\":{\"omit_norms\":true,\"enabled\":true},\"properties\":{\"geoip\":{\"dynamic\":\"true\",\"properties\":{\"location\":{\"type\":\"geo_point\"}}},\"@version\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"logs\":{\"dynamic_templates\":[{\"message_field\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\"},\"match_mapping_type\":\"string\",\"match\":\"message\"}},{\"string_fields\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"match_mapping_type\":\"string\",\"match\":\"\"}}],\"all\":{\"omit_norms\":true,\"enabled\":true},\"properties\":{\"netflow\":{\"properties\":{\"dst_as\":{\"type\":\"long\"},\"in_pkts\":{\"type\":\"long\"},\"if_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"first_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"flowset_id\":{\"type\":\"long\"},\"ipv4_next_hop\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"l4_src_port\":{\"type\":\"long\"},\"in_bytes\":{\"type\":\"long\"},\"protocol\":{\"type\":\"long\"},\"if_desc\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"tcp_flags\":{\"type\":\"long\"},\"l4_dst_port\":{\"type\":\"long\"},\"direction\":{\"type\":\"long\"},\"src_as\":{\"type\":\"long\"},\"output_snmp\":{\"type\":\"long\"},\"dst_mask\":{\"type\":\"long\"},\"ipv4_dst_addr\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"src_tos\":{\"type\":\"long\"},\"src_mask\":{\"type\":\"long\"},\"version\":{\"type\":\"long\"},\"flow_seq_num\":{\"type\":\"long\"},\"ipv4_src_addr\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"input_snmp\":{\"type\":\"long\"},\"last_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"flow_sampler_id\":{\"type\":\"long\"}}},\"@timestamp\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"geoip\":{\"dynamic\":\"true\",\"properties\":{\"timezone\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"area_code\":{\"type\":\"long\"},\"ip\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"latitude\":{\"type\":\"double\"},\"coordinates\":{\"type\":\"double\"},\"continent_code\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"city_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"country_code2\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"country_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"dma_code\":{\"type\":\"long\"},\"country_code3\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"location\":{\"type\":\"geo_point\"},\"region_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"real_region_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"postal_code\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"longitude\":{\"type\":\"double\"}}},\"@version\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"host\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}}}}},\"aliases\":[]}}}}\n2015-07-17 17:40:50,501 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:40:50,504 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/logstash-netflow5-2015.06.11 HTTP/1.1\" 200 4453\n2015-07-17 17:40:50,504 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/logstash-netflow5-2015.06.11 [status:200 request:0.003s]\n2015-07-17 17:40:50,504 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:40:50,505 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"elasticsearch\",\"metadata\":{\"templates\":{},\"indices\":{\"logstash-netflow5-2015.06.11\":{\"state\":\"open\",\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1433980796068\",\"number_of_shards\":\"5\",\"uuid\":\"wFRq1MW0R5e7hlAdU7vQpw\",\"version\":{\"created\":\"1050299\"},\"number_of_replicas\":\"1\"}},\"mappings\":{\"_default\":{\"dynamic_templates\":[{\"message_field\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\"},\"match_mapping_type\":\"string\",\"match\":\"message\"}},{\"string_fields\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"match_mapping_type\":\"string\",\"match\":\"\"}}],\"_all\":{\"omit_norms\":true,\"enabled\":true},\"properties\":{\"geoip\":{\"dynamic\":\"true\",\"properties\":{\"location\":{\"type\":\"geo_point\"}}},\"@version\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"logs\":{\"dynamic_templates\":[{\"message_field\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\"},\"match_mapping_type\":\"string\",\"match\":\"message\"}},{\"string_fields\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"match_mapping_type\":\"string\",\"match\":\"\"}}],\"all\":{\"omit_norms\":true,\"enabled\":true},\"properties\":{\"netflow\":{\"properties\":{\"dst_as\":{\"type\":\"long\"},\"in_pkts\":{\"type\":\"long\"},\"if_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"first_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"flowset_id\":{\"type\":\"long\"},\"ipv4_next_hop\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"l4_src_port\":{\"type\":\"long\"},\"in_bytes\":{\"type\":\"long\"},\"protocol\":{\"type\":\"long\"},\"if_desc\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"tcp_flags\":{\"type\":\"long\"},\"l4_dst_port\":{\"type\":\"long\"},\"direction\":{\"type\":\"long\"},\"src_as\":{\"type\":\"long\"},\"output_snmp\":{\"type\":\"long\"},\"dst_mask\":{\"type\":\"long\"},\"ipv4_dst_addr\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"src_tos\":{\"type\":\"long\"},\"src_mask\":{\"type\":\"long\"},\"version\":{\"type\":\"long\"},\"flow_seq_num\":{\"type\":\"long\"},\"ipv4_src_addr\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"input_snmp\":{\"type\":\"long\"},\"last_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"flow_sampler_id\":{\"type\":\"long\"}}},\"@timestamp\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"geoip\":{\"dynamic\":\"true\",\"properties\":{\"timezone\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"area_code\":{\"type\":\"long\"},\"ip\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"latitude\":{\"type\":\"double\"},\"coordinates\":{\"type\":\"double\"},\"continent_code\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"city_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"country_code2\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"country_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"dma_code\":{\"type\":\"long\"},\"country_code3\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"location\":{\"type\":\"geo_point\"},\"region_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"real_region_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"postal_code\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"longitude\":{\"type\":\"double\"}}},\"@version\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"host\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}}}}},\"aliases\":[]}}}}\n2015-07-17 17:40:50,509 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:40:50,522 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/logstash-netflow5-2015.06.12 HTTP/1.1\" 200 4453\n2015-07-17 17:40:50,522 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/logstash-netflow5-2015.06.12 [status:200 request:0.013s]\n2015-07-17 17:40:50,522 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:40:50,522 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"elasticsearch\",\"metadata\":{\"templates\":{},\"indices\":{\"logstash-netflow5-2015.06.12\":{\"state\":\"open\",\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1434067197716\",\"number_of_shards\":\"5\",\"uuid\":\"LIRfVMp7S_CK6J4XysXPfg\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}},\"mappings\":{\"_default\":{\"dynamic_templates\":[{\"message_field\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\"},\"match_mapping_type\":\"string\",\"match\":\"message\"}},{\"string_fields\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"match_mapping_type\":\"string\",\"match\":\"\"}}],\"_all\":{\"omit_norms\":true,\"enabled\":true},\"properties\":{\"geoip\":{\"dynamic\":\"true\",\"properties\":{\"location\":{\"type\":\"geo_point\"}}},\"@version\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"logs\":{\"dynamic_templates\":[{\"message_field\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\"},\"match_mapping_type\":\"string\",\"match\":\"message\"}},{\"string_fields\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"match_mapping_type\":\"string\",\"match\":\"\"}}],\"all\":{\"omit_norms\":true,\"enabled\":true},\"properties\":{\"netflow\":{\"properties\":{\"dst_as\":{\"type\":\"long\"},\"in_pkts\":{\"type\":\"long\"},\"if_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"first_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"flowset_id\":{\"type\":\"long\"},\"ipv4_next_hop\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"l4_src_port\":{\"type\":\"long\"},\"in_bytes\":{\"type\":\"long\"},\"protocol\":{\"type\":\"long\"},\"if_desc\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"tcp_flags\":{\"type\":\"long\"},\"l4_dst_port\":{\"type\":\"long\"},\"direction\":{\"type\":\"long\"},\"src_as\":{\"type\":\"long\"},\"output_snmp\":{\"type\":\"long\"},\"dst_mask\":{\"type\":\"long\"},\"ipv4_dst_addr\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"src_tos\":{\"type\":\"long\"},\"src_mask\":{\"type\":\"long\"},\"version\":{\"type\":\"long\"},\"flow_seq_num\":{\"type\":\"long\"},\"ipv4_src_addr\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"input_snmp\":{\"type\":\"long\"},\"last_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"flow_sampler_id\":{\"type\":\"long\"}}},\"@timestamp\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"geoip\":{\"dynamic\":\"true\",\"properties\":{\"timezone\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"area_code\":{\"type\":\"long\"},\"ip\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"latitude\":{\"type\":\"double\"},\"coordinates\":{\"type\":\"double\"},\"continent_code\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"city_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"country_code2\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"country_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"dma_code\":{\"type\":\"long\"},\"country_code3\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"location\":{\"type\":\"geo_point\"},\"region_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"real_region_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"postal_code\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"longitude\":{\"type\":\"double\"}}},\"@version\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"host\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}}}}},\"aliases\":[]}}}}\n2015-07-17 17:40:50,526 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:40:50,529 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/logstash-netflow5-2015.06.13 HTTP/1.1\" 200 4453\n2015-07-17 17:40:50,529 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/logstash-netflow5-2015.06.13 [status:200 request:0.003s]\n2015-07-17 17:40:50,530 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:40:50,530 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"elasticsearch\",\"metadata\":{\"templates\":{},\"indices\":{\"logstash-netflow5-2015.06.13\":{\"state\":\"open\",\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1434153594302\",\"number_of_shards\":\"5\",\"uuid\":\"UFwAc8iGQRupqHmR-QUj8g\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}},\"mappings\":{\"_default\":{\"dynamic_templates\":[{\"message_field\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\"},\"match_mapping_type\":\"string\",\"match\":\"message\"}},{\"string_fields\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"match_mapping_type\":\"string\",\"match\":\"\"}}],\"_all\":{\"omit_norms\":true,\"enabled\":true},\"properties\":{\"geoip\":{\"dynamic\":\"true\",\"properties\":{\"location\":{\"type\":\"geo_point\"}}},\"@version\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"logs\":{\"dynamic_templates\":[{\"message_field\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\"},\"match_mapping_type\":\"string\",\"match\":\"message\"}},{\"string_fields\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"match_mapping_type\":\"string\",\"match\":\"\"}}],\"all\":{\"omit_norms\":true,\"enabled\":true},\"properties\":{\"netflow\":{\"properties\":{\"dst_as\":{\"type\":\"long\"},\"in_pkts\":{\"type\":\"long\"},\"if_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"first_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"flowset_id\":{\"type\":\"long\"},\"ipv4_next_hop\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"l4_src_port\":{\"type\":\"long\"},\"in_bytes\":{\"type\":\"long\"},\"protocol\":{\"type\":\"long\"},\"if_desc\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"tcp_flags\":{\"type\":\"long\"},\"l4_dst_port\":{\"type\":\"long\"},\"direction\":{\"type\":\"long\"},\"src_as\":{\"type\":\"long\"},\"output_snmp\":{\"type\":\"long\"},\"dst_mask\":{\"type\":\"long\"},\"ipv4_dst_addr\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"src_tos\":{\"type\":\"long\"},\"src_mask\":{\"type\":\"long\"},\"version\":{\"type\":\"long\"},\"flow_seq_num\":{\"type\":\"long\"},\"ipv4_src_addr\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"input_snmp\":{\"type\":\"long\"},\"last_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"flow_sampler_id\":{\"type\":\"long\"}}},\"@timestamp\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"geoip\":{\"dynamic\":\"true\",\"properties\":{\"timezone\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"area_code\":{\"type\":\"long\"},\"ip\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"latitude\":{\"type\":\"double\"},\"coordinates\":{\"type\":\"double\"},\"continent_code\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"city_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"country_code2\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"country_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"dma_code\":{\"type\":\"long\"},\"country_code3\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"location\":{\"type\":\"geo_point\"},\"region_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"real_region_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"postal_code\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"longitude\":{\"type\":\"double\"}}},\"@version\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"host\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}}}}},\"aliases\":[]}}}}\n2015-07-17 17:40:50,534 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:40:50,538 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/logstash-netflow5-2015.06.14 HTTP/1.1\" 200 4453\n2015-07-17 17:40:50,538 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/logstash-netflow5-2015.06.14 [status:200 request:0.004s]\n2015-07-17 17:40:50,538 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:40:50,538 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"elasticsearch\",\"metadata\":{\"templates\":{},\"indices\":{\"logstash-netflow5-2015.06.14\":{\"state\":\"open\",\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1434241954174\",\"number_of_shards\":\"5\",\"uuid\":\"k_3E2xtuRGGT27tamkvXDg\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}},\"mappings\":{\"_default\":{\"dynamic_templates\":[{\"message_field\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\"},\"match_mapping_type\":\"string\",\"match\":\"message\"}},{\"string_fields\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"match_mapping_type\":\"string\",\"match\":\"\"}}],\"_all\":{\"omit_norms\":true,\"enabled\":true},\"properties\":{\"geoip\":{\"dynamic\":\"true\",\"properties\":{\"location\":{\"type\":\"geo_point\"}}},\"@version\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"logs\":{\"dynamic_templates\":[{\"message_field\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\"},\"match_mapping_type\":\"string\",\"match\":\"message\"}},{\"string_fields\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"match_mapping_type\":\"string\",\"match\":\"\"}}],\"all\":{\"omit_norms\":true,\"enabled\":true},\"properties\":{\"netflow\":{\"properties\":{\"dst_as\":{\"type\":\"long\"},\"in_pkts\":{\"type\":\"long\"},\"if_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"first_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"flowset_id\":{\"type\":\"long\"},\"ipv4_next_hop\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"l4_src_port\":{\"type\":\"long\"},\"in_bytes\":{\"type\":\"long\"},\"protocol\":{\"type\":\"long\"},\"if_desc\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"tcp_flags\":{\"type\":\"long\"},\"l4_dst_port\":{\"type\":\"long\"},\"direction\":{\"type\":\"long\"},\"src_as\":{\"type\":\"long\"},\"output_snmp\":{\"type\":\"long\"},\"dst_mask\":{\"type\":\"long\"},\"ipv4_dst_addr\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"src_tos\":{\"type\":\"long\"},\"src_mask\":{\"type\":\"long\"},\"version\":{\"type\":\"long\"},\"flow_seq_num\":{\"type\":\"long\"},\"ipv4_src_addr\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"input_snmp\":{\"type\":\"long\"},\"last_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"flow_sampler_id\":{\"type\":\"long\"}}},\"@timestamp\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"geoip\":{\"dynamic\":\"true\",\"properties\":{\"timezone\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"area_code\":{\"type\":\"long\"},\"ip\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"latitude\":{\"type\":\"double\"},\"coordinates\":{\"type\":\"double\"},\"continent_code\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"city_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"country_code2\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"country_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"dma_code\":{\"type\":\"long\"},\"country_code3\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"location\":{\"type\":\"geo_point\"},\"region_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"real_region_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"postal_code\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"longitude\":{\"type\":\"double\"}}},\"@version\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"host\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}}}}},\"aliases\":[]}}}}\n2015-07-17 17:40:50,542 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:40:50,546 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/logstash-netflow5-2015.06.15 HTTP/1.1\" 200 4453\n2015-07-17 17:40:50,546 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/logstash-netflow5-2015.06.15 [status:200 request:0.004s]\n2015-07-17 17:40:50,547 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:40:50,547 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"elasticsearch\",\"metadata\":{\"templates\":{},\"indices\":{\"logstash-netflow5-2015.06.15\":{\"state\":\"open\",\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1434326393695\",\"number_of_shards\":\"5\",\"uuid\":\"HYhDEnJ9RZSA4U0oX6QXHw\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}},\"mappings\":{\"_default\":{\"dynamic_templates\":[{\"message_field\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\"},\"match_mapping_type\":\"string\",\"match\":\"message\"}},{\"string_fields\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"match_mapping_type\":\"string\",\"match\":\"\"}}],\"_all\":{\"omit_norms\":true,\"enabled\":true},\"properties\":{\"geoip\":{\"dynamic\":\"true\",\"properties\":{\"location\":{\"type\":\"geo_point\"}}},\"@version\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"logs\":{\"dynamic_templates\":[{\"message_field\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\"},\"match_mapping_type\":\"string\",\"match\":\"message\"}},{\"string_fields\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"match_mapping_type\":\"string\",\"match\":\"\"}}],\"all\":{\"omit_norms\":true,\"enabled\":true},\"properties\":{\"netflow\":{\"properties\":{\"dst_as\":{\"type\":\"long\"},\"in_pkts\":{\"type\":\"long\"},\"if_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"first_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"flowset_id\":{\"type\":\"long\"},\"ipv4_next_hop\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"l4_src_port\":{\"type\":\"long\"},\"in_bytes\":{\"type\":\"long\"},\"protocol\":{\"type\":\"long\"},\"if_desc\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"tcp_flags\":{\"type\":\"long\"},\"l4_dst_port\":{\"type\":\"long\"},\"direction\":{\"type\":\"long\"},\"src_as\":{\"type\":\"long\"},\"output_snmp\":{\"type\":\"long\"},\"dst_mask\":{\"type\":\"long\"},\"ipv4_dst_addr\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"src_tos\":{\"type\":\"long\"},\"src_mask\":{\"type\":\"long\"},\"version\":{\"type\":\"long\"},\"flow_seq_num\":{\"type\":\"long\"},\"ipv4_src_addr\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"input_snmp\":{\"type\":\"long\"},\"last_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"flow_sampler_id\":{\"type\":\"long\"}}},\"@timestamp\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"geoip\":{\"dynamic\":\"true\",\"properties\":{\"timezone\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"area_code\":{\"type\":\"long\"},\"ip\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"latitude\":{\"type\":\"double\"},\"coordinates\":{\"type\":\"double\"},\"continent_code\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"city_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"country_code2\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"country_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"dma_code\":{\"type\":\"long\"},\"country_code3\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"location\":{\"type\":\"geo_point\"},\"region_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"real_region_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"postal_code\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"longitude\":{\"type\":\"double\"}}},\"@version\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"host\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}}}}},\"aliases\":[]}}}}\n2015-07-17 17:40:50,551 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:40:50,559 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/logstash-netflow5-2015.06.16 HTTP/1.1\" 200 4453\n2015-07-17 17:40:50,559 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/logstash-netflow5-2015.06.16 [status:200 request:0.008s]\n2015-07-17 17:40:50,559 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:40:50,559 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"elasticsearch\",\"metadata\":{\"templates\":{},\"indices\":{\"logstash-netflow5-2015.06.16\":{\"state\":\"open\",\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1434412792992\",\"number_of_shards\":\"5\",\"uuid\":\"C7vp0kvCTNOhVCzDpbWtBQ\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}},\"mappings\":{\"_default\":{\"dynamic_templates\":[{\"message_field\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\"},\"match_mapping_type\":\"string\",\"match\":\"message\"}},{\"string_fields\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"match_mapping_type\":\"string\",\"match\":\"\"}}],\"_all\":{\"omit_norms\":true,\"enabled\":true},\"properties\":{\"geoip\":{\"dynamic\":\"true\",\"properties\":{\"location\":{\"type\":\"geo_point\"}}},\"@version\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"logs\":{\"dynamic_templates\":[{\"message_field\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\"},\"match_mapping_type\":\"string\",\"match\":\"message\"}},{\"string_fields\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"match_mapping_type\":\"string\",\"match\":\"\"}}],\"all\":{\"omit_norms\":true,\"enabled\":true},\"properties\":{\"netflow\":{\"properties\":{\"dst_as\":{\"type\":\"long\"},\"in_pkts\":{\"type\":\"long\"},\"if_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"first_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"flowset_id\":{\"type\":\"long\"},\"ipv4_next_hop\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"l4_src_port\":{\"type\":\"long\"},\"in_bytes\":{\"type\":\"long\"},\"protocol\":{\"type\":\"long\"},\"if_desc\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"tcp_flags\":{\"type\":\"long\"},\"l4_dst_port\":{\"type\":\"long\"},\"direction\":{\"type\":\"long\"},\"src_as\":{\"type\":\"long\"},\"output_snmp\":{\"type\":\"long\"},\"dst_mask\":{\"type\":\"long\"},\"ipv4_dst_addr\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"src_tos\":{\"type\":\"long\"},\"src_mask\":{\"type\":\"long\"},\"version\":{\"type\":\"long\"},\"flow_seq_num\":{\"type\":\"long\"},\"ipv4_src_addr\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"input_snmp\":{\"type\":\"long\"},\"last_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"flow_sampler_id\":{\"type\":\"long\"}}},\"@timestamp\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"geoip\":{\"dynamic\":\"true\",\"properties\":{\"timezone\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"area_code\":{\"type\":\"long\"},\"ip\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"latitude\":{\"type\":\"double\"},\"coordinates\":{\"type\":\"double\"},\"continent_code\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"city_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"country_code2\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"country_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"dma_code\":{\"type\":\"long\"},\"country_code3\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"location\":{\"type\":\"geo_point\"},\"region_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"real_region_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"postal_code\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"longitude\":{\"type\":\"double\"}}},\"@version\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"host\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}}}}},\"aliases\":[]}}}}\n2015-07-17 17:40:50,563 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:40:50,573 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_cluster/state/metadata/logstash-netflow5-2015.06.17 HTTP/1.1\" 200 4453\n2015-07-17 17:40:50,573 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cluster/state/metadata/logstash-netflow5-2015.06.17 [status:200 request:0.010s]\n2015-07-17 17:40:50,573 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:40:50,573 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"elasticsearch\",\"metadata\":{\"templates\":{},\"indices\":{\"logstash-netflow5-2015.06.17\":{\"state\":\"open\",\"settings\":{\"index\":{\"refresh_interval\":\"5s\",\"creation_date\":\"1434499193180\",\"number_of_shards\":\"5\",\"uuid\":\"QYI5NU_bR6qTuQyZwkA57w\",\"version\":{\"created\":\"1060099\"},\"number_of_replicas\":\"1\"}},\"mappings\":{\"_default\":{\"dynamic_templates\":[{\"message_field\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\"},\"match_mapping_type\":\"string\",\"match\":\"message\"}},{\"string_fields\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"match_mapping_type\":\"string\",\"match\":\"\"}}],\"_all\":{\"omit_norms\":true,\"enabled\":true},\"properties\":{\"geoip\":{\"dynamic\":\"true\",\"properties\":{\"location\":{\"type\":\"geo_point\"}}},\"@version\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"logs\":{\"dynamic_templates\":[{\"message_field\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\"},\"match_mapping_type\":\"string\",\"match\":\"message\"}},{\"string_fields\":{\"mapping\":{\"index\":\"analyzed\",\"omit_norms\":true,\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"match_mapping_type\":\"string\",\"match\":\"\"}}],\"_all\":{\"omit_norms\":true,\"enabled\":true},\"properties\":{\"netflow\":{\"properties\":{\"dst_as\":{\"type\":\"long\"},\"in_pkts\":{\"type\":\"long\"},\"if_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"first_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"flowset_id\":{\"type\":\"long\"},\"ipv4_next_hop\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"l4_src_port\":{\"type\":\"long\"},\"in_bytes\":{\"type\":\"long\"},\"protocol\":{\"type\":\"long\"},\"if_desc\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"tcp_flags\":{\"type\":\"long\"},\"l4_dst_port\":{\"type\":\"long\"},\"direction\":{\"type\":\"long\"},\"src_as\":{\"type\":\"long\"},\"output_snmp\":{\"type\":\"long\"},\"dst_mask\":{\"type\":\"long\"},\"ipv4_dst_addr\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"src_tos\":{\"type\":\"long\"},\"src_mask\":{\"type\":\"long\"},\"version\":{\"type\":\"long\"},\"flow_seq_num\":{\"type\":\"long\"},\"ipv4_src_addr\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"input_snmp\":{\"type\":\"long\"},\"last_switched\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"flow_sampler_id\":{\"type\":\"long\"}}},\"@timestamp\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"geoip\":{\"dynamic\":\"true\",\"properties\":{\"timezone\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"area_code\":{\"type\":\"long\"},\"ip\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"latitude\":{\"type\":\"double\"},\"coordinates\":{\"type\":\"double\"},\"continent_code\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"city_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"country_code2\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"country_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"dma_code\":{\"type\":\"long\"},\"country_code3\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"location\":{\"type\":\"geo_point\"},\"region_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"real_region_name\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"postal_code\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"longitude\":{\"type\":\"double\"}}},\"@version\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"host\":{\"norms\":{\"enabled\":false},\"type\":\"string\",\"fields\":{\"raw\":{\"ignore_above\":256,\"index\":\"not_analyzed\",\"type\":\"string\"}}}}}},\"aliases\":[]}}}}\n2015-07-17 17:40:50,579 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:40:50,580 DEBUG     urllib3.connectionpool          _make_request:383  \"GET / HTTP/1.1\" 200 341\n2015-07-17 17:40:50,581 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.001s]\n2015-07-17 17:40:50,581 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:40:50,581 DEBUG              elasticsearch    log_request_success:66   < {\n  \"status\" : 200,\n  \"name\" : \"nettools-m1n1\",\n  \"cluster_name\" : \"elasticsearch\",\n  \"version\" : {\n    \"number\" : \"1.6.0\",\n    \"build_hash\" : \"cdd3ac4dde4f69524ec0a14de3828cb95bbb86d0\",\n    \"build_timestamp\" : \"2015-06-09T13:36:34Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"4.10.4\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n2015-07-17 17:40:50,582 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:41:18,393 DEBUG     urllib3.connectionpool          _make_request:383  \"POST /logstash-netflow5-2015.05.19,logstash-netflow5-2015.05.20,logstash-netflow5-2015.05.21,logstash-netflow5-2015.05.22,logstash-netflow5-2015.05.23,logstash-netflow5-2015.05.24,logstash-netflow5-2015.05.25,logstash-netflow5-2015.05.26,logstash-netflow5-2015.05.27,logstash-netflow5-2015.05.28,logstash-netflow5-2015.05.29,logstash-netflow5-2015.05.30,logstash-netflow5-2015.05.31,logstash-netflow5-2015.06.01,logstash-netflow5-2015.06.02,logstash-netflow5-2015.06.03,logstash-netflow5-2015.06.04,logstash-netflow5-2015.06.05.orig,logstash-netflow5-2015.06.06,logstash-netflow5-2015.06.07,logstash-netflow5-2015.06.08,logstash-netflow5-2015.06.09,logstash-netflow5-2015.06.10,logstash-netflow5-2015.06.11,logstash-netflow5-2015.06.12,logstash-netflow5-2015.06.13,logstash-netflow5-2015.06.14,logstash-netflow5-2015.06.15,logstash-netflow5-2015.06.16,logstash-netflow5-2015.06.17/_flush/synced HTTP/1.1\" 200 2278\n2015-07-17 17:41:18,394 INFO               elasticsearch    log_request_success:63   POST http://localhost:9200/logstash-netflow5-2015.05.19,logstash-netflow5-2015.05.20,logstash-netflow5-2015.05.21,logstash-netflow5-2015.05.22,logstash-netflow5-2015.05.23,logstash-netflow5-2015.05.24,logstash-netflow5-2015.05.25,logstash-netflow5-2015.05.26,logstash-netflow5-2015.05.27,logstash-netflow5-2015.05.28,logstash-netflow5-2015.05.29,logstash-netflow5-2015.05.30,logstash-netflow5-2015.05.31,logstash-netflow5-2015.06.01,logstash-netflow5-2015.06.02,logstash-netflow5-2015.06.03,logstash-netflow5-2015.06.04,logstash-netflow5-2015.06.05.orig,logstash-netflow5-2015.06.06,logstash-netflow5-2015.06.07,logstash-netflow5-2015.06.08,logstash-netflow5-2015.06.09,logstash-netflow5-2015.06.10,logstash-netflow5-2015.06.11,logstash-netflow5-2015.06.12,logstash-netflow5-2015.06.13,logstash-netflow5-2015.06.14,logstash-netflow5-2015.06.15,logstash-netflow5-2015.06.16,logstash-netflow5-2015.06.17/_flush/synced [status:200 request:27.812s]\n2015-07-17 17:41:18,394 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:41:18,394 DEBUG              elasticsearch    log_request_success:66   < {\"_shards\":{\"total\":300,\"successful\":300,\"failed\":0},\"logstash-netflow5-2015.06.05.orig\":{\"total\":10,\"successful\":10,\"failed\":0},\"logstash-netflow5-2015.05.31\":{\"total\":10,\"successful\":10,\"failed\":0},\"logstash-netflow5-2015.05.30\":{\"total\":10,\"successful\":10,\"failed\":0},\"logstash-netflow5-2015.06.04\":{\"total\":10,\"successful\":10,\"failed\":0},\"logstash-netflow5-2015.06.03\":{\"total\":10,\"successful\":10,\"failed\":0},\"logstash-netflow5-2015.06.02\":{\"total\":10,\"successful\":10,\"failed\":0},\"logstash-netflow5-2015.06.01\":{\"total\":10,\"successful\":10,\"failed\":0},\"logstash-netflow5-2015.06.08\":{\"total\":10,\"successful\":10,\"failed\":0},\"logstash-netflow5-2015.06.07\":{\"total\":10,\"successful\":10,\"failed\":0},\"logstash-netflow5-2015.06.06\":{\"total\":10,\"successful\":10,\"failed\":0},\"logstash-netflow5-2015.05.19\":{\"total\":10,\"successful\":10,\"failed\":0},\"logstash-netflow5-2015.06.09\":{\"total\":10,\"successful\":10,\"failed\":0},\"logstash-netflow5-2015.05.20\":{\"total\":10,\"successful\":10,\"failed\":0},\"logstash-netflow5-2015.06.11\":{\"total\":10,\"successful\":10,\"failed\":0},\"logstash-netflow5-2015.05.21\":{\"total\":10,\"successful\":10,\"failed\":0},\"logstash-netflow5-2015.06.10\":{\"total\":10,\"successful\":10,\"failed\":0},\"logstash-netflow5-2015.05.24\":{\"total\":10,\"successful\":10,\"failed\":0},\"logstash-netflow5-2015.06.15\":{\"total\":10,\"successful\":10,\"failed\":0},\"logstash-netflow5-2015.05.25\":{\"total\":10,\"successful\":10,\"failed\":0},\"logstash-netflow5-2015.06.14\":{\"total\":10,\"successful\":10,\"failed\":0},\"logstash-netflow5-2015.05.22\":{\"total\":10,\"successful\":10,\"failed\":0},\"logstash-netflow5-2015.06.13\":{\"total\":10,\"successful\":10,\"failed\":0},\"logstash-netflow5-2015.05.23\":{\"total\":10,\"successful\":10,\"failed\":0},\"logstash-netflow5-2015.06.12\":{\"total\":10,\"successful\":10,\"failed\":0},\"logstash-netflow5-2015.05.28\":{\"total\":10,\"successful\":10,\"failed\":0},\"logstash-netflow5-2015.05.29\":{\"total\":10,\"successful\":10,\"failed\":0},\"logstash-netflow5-2015.05.26\":{\"total\":10,\"successful\":10,\"failed\":0},\"logstash-netflow5-2015.06.17\":{\"total\":10,\"successful\":10,\"failed\":0},\"logstash-netflow5-2015.05.27\":{\"total\":10,\"successful\":10,\"failed\":0},\"logstash-netflow5-2015.06.16\":{\"total\":10,\"successful\":10,\"failed\":0}}\n2015-07-17 17:41:18,397 INFO            curator.api.seal           seal_indices:49   Provided indices successfully sealed. (Shown with --debug flag enabled.)\n2015-07-17 17:41:18,397 DEBUG           curator.api.seal           seal_indices:51   Successfully sealed indices: [u'logstash-netflow5-2015.05.21', u'logstash-netflow5-2015.05.20', u'logstash-netflow5-2015.05.23', u'logstash-netflow5-2015.05.22', u'logstash-netflow5-2015.05.25', u'logstash-netflow5-2015.05.24', u'logstash-netflow5-2015.05.27', u'logstash-netflow5-2015.05.29', u'logstash-netflow5-2015.05.28', u'logstash-netflow5-2015.06.08', u'logstash-netflow5-2015.06.09', u'logstash-netflow5-2015.06.02', u'logstash-netflow5-2015.06.03', u'logstash-netflow5-2015.06.01', u'logstash-netflow5-2015.06.06', u'logstash-netflow5-2015.06.07', u'logstash-netflow5-2015.06.04', u'logstash-netflow5-2015.05.26', u'logstash-netflow5-2015.06.13', u'logstash-netflow5-2015.06.05.orig', u'logstash-netflow5-2015.06.12', u'logstash-netflow5-2015.05.19', u'logstash-netflow5-2015.06.15', u'logstash-netflow5-2015.06.14', u'logstash-netflow5-2015.06.17', u'logstash-netflow5-2015.06.16', u'logstash-netflow5-2015.06.11', u'logstash-netflow5-2015.06.10', u'logstash-netflow5-2015.05.30', u'logstash-netflow5-2015.05.31']\n2015-07-17 17:41:18,398 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-17 17:41:26,656 DEBUG     urllib3.connectionpool          _make_request:383  \"POST /logstash-netflow5-2015.05.19,logstash-netflow5-2015.05.20,logstash-netflow5-2015.05.21,logstash-netflow5-2015.05.22,logstash-netflow5-2015.05.23,logstash-netflow5-2015.05.24,logstash-netflow5-2015.05.25,logstash-netflow5-2015.05.26,logstash-netflow5-2015.05.27,logstash-netflow5-2015.05.28,logstash-netflow5-2015.05.29,logstash-netflow5-2015.05.30,logstash-netflow5-2015.05.31,logstash-netflow5-2015.06.01,logstash-netflow5-2015.06.02,logstash-netflow5-2015.06.03,logstash-netflow5-2015.06.04,logstash-netflow5-2015.06.05.orig,logstash-netflow5-2015.06.06,logstash-netflow5-2015.06.07,logstash-netflow5-2015.06.08,logstash-netflow5-2015.06.09,logstash-netflow5-2015.06.10,logstash-netflow5-2015.06.11,logstash-netflow5-2015.06.12,logstash-netflow5-2015.06.13,logstash-netflow5-2015.06.14,logstash-netflow5-2015.06.15,logstash-netflow5-2015.06.16,logstash-netflow5-2015.06.17/_close?ignore_unavailable=true HTTP/1.1\" 200 21\n2015-07-17 17:41:26,656 INFO               elasticsearch    log_request_success:63   POST http://localhost:9200/logstash-netflow5-2015.05.19,logstash-netflow5-2015.05.20,logstash-netflow5-2015.05.21,logstash-netflow5-2015.05.22,logstash-netflow5-2015.05.23,logstash-netflow5-2015.05.24,logstash-netflow5-2015.05.25,logstash-netflow5-2015.05.26,logstash-netflow5-2015.05.27,logstash-netflow5-2015.05.28,logstash-netflow5-2015.05.29,logstash-netflow5-2015.05.30,logstash-netflow5-2015.05.31,logstash-netflow5-2015.06.01,logstash-netflow5-2015.06.02,logstash-netflow5-2015.06.03,logstash-netflow5-2015.06.04,logstash-netflow5-2015.06.05.orig,logstash-netflow5-2015.06.06,logstash-netflow5-2015.06.07,logstash-netflow5-2015.06.08,logstash-netflow5-2015.06.09,logstash-netflow5-2015.06.10,logstash-netflow5-2015.06.11,logstash-netflow5-2015.06.12,logstash-netflow5-2015.06.13,logstash-netflow5-2015.06.14,logstash-netflow5-2015.06.15,logstash-netflow5-2015.06.16,logstash-netflow5-2015.06.17/_close?ignore_unavailable=true [status:200 request:8.258s]\n2015-07-17 17:41:26,656 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-17 17:41:26,656 DEBUG              elasticsearch    log_request_success:66   < {\"acknowledged\":true}\n2015-07-17 17:41:26,657 INFO           curator.cli.utils               exit_msg:67   Job completed successfully.\n. Is it normal, that after command above i've no output after this command?:\ncurator show indices --time-unit days --older-than 30 --timestring %Y.%m.%d --prefix logstash-netflow5-\nI think here should be indices list with (CLOSED) marker. Or not?\n. i'm sorry, it's my copy/paste mistake. I have copy and paste to shell terminal command with \"#\" at begining :)\ncurator show indices --time-unit days --older-than 35 --timestring %Y.%m.%d --prefix logstash-netflow5-*\n2015-07-17 17:51:35,773 INFO      Job starting: show indices\n2015-07-17 17:51:37,388 INFO      Matching indices:\nlogstash-netflow5-2015.05.19 (CLOSED)\nlogstash-netflow5-2015.05.20 (CLOSED)\nlogstash-netflow5-2015.05.21 (CLOSED)\nlogstash-netflow5-2015.05.22 (CLOSED)\nlogstash-netflow5-2015.05.23 (CLOSED)\nlogstash-netflow5-2015.05.24 (CLOSED)\nlogstash-netflow5-2015.05.25 (CLOSED)\nlogstash-netflow5-2015.05.26 (CLOSED)\nlogstash-netflow5-2015.05.27 (CLOSED)\nlogstash-netflow5-2015.05.28 (CLOSED)\nlogstash-netflow5-2015.05.29 (CLOSED)\nlogstash-netflow5-2015.05.30 (CLOSED)\nlogstash-netflow5-2015.05.31 (CLOSED)\nlogstash-netflow5-2015.06.01 (CLOSED)\nlogstash-netflow5-2015.06.02 (CLOSED)\nlogstash-netflow5-2015.06.03 (CLOSED)\nlogstash-netflow5-2015.06.04 (CLOSED)\nlogstash-netflow5-2015.06.05.orig (CLOSED)\nlogstash-netflow5-2015.06.06 (CLOSED)\nlogstash-netflow5-2015.06.07 (CLOSED)\nlogstash-netflow5-2015.06.08 (CLOSED)\nlogstash-netflow5-2015.06.09 (CLOSED)\nlogstash-netflow5-2015.06.10 (CLOSED)\nlogstash-netflow5-2015.06.11 (CLOSED)\nlogstash-netflow5-2015.06.12 (CLOSED)\n. ",
    "gzcf": "@untergeek I misunderstood --timestring. Thank you for help!\n. ",
    "nellicus": "Ok I guess not\nStarting new HTTPS connection (4): securelk\n. added rootCA cert to the host with sudo update-ca-certificates\nabonuccelli@w530 /usr/local/share/ca-certificates $ pwd\n/usr/local/share/ca-certificates\nabonuccelli@w530 /usr/local/share/ca-certificates $ ls elastictestca/ -alrth\ntotal 12K\ndrwxrwsr-x 3 root staff 4.0K Jul 14 13:54 ..\ndrwxr-sr-x 2 root staff 4.0K Jul 14 13:58 .\n-rw-r--r-- 1 root staff 1.4K Jul 14 13:58 cacert.pem\nabonuccelli@w530 /usr/local/share/ca-certificates $ awk -v cmd='openssl x509 -noout -subject' '\n    /BEGIN/{close(cmd)};{print | cmd}' < /etc/ssl/certs/ca-certificates.crt | tail -1\nsubject= /O=Elasticsearch Test Org/OU=Support/emailAddress=cacerttest@YOUR.COMPANY.TLD/L=Barcelona/ST=Spain/C=ES/CN=Elastic Test CA\nwith no luck\n. also tried \n$ sudo pip install requests[security] --upgrade\nRequirement already up-to-date: requests[security] in /usr/local/lib/python2.7/dist-packages\n\u2002 Installing extra requirements: 'security'\nCleaning up...\nfrom googling that \n/usr/local/lib/python2.7/dist-packages/urllib3/util/ssl_.py:90: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.\n  InsecurePlatformWarning\n. changed two lines of code to\n$ egrep verify_certs -A1 /usr/local/lib/python2.7/dist-packages/curator/cli/utils.py\n            kwargs['verify_certs'] = False \n            #kwargs['ca_certs'] = certifi.where()\nand it seems to run ok now\n```\nabonuccelli@w530 /opt/elk/PRODSEC $ curator --use_ssl --debug --host securelk --port 9200 --http_auth admin:password delete indices --time-unit days --older-than 10 --timestring %Y.%m.%d \n2015-07-14 14:25:16,049 DEBUG         curator.api.filter         get_date_regex:158  regex = \\d{4}.\\d{2}.\\d{2}\n2015-07-14 14:25:16,050 DEBUG          curator.cli.utils        filter_callback:179  REGEX = (?P\\d{4}.\\d{2}.\\d{2})\n2015-07-14 14:25:16,050 DEBUG          curator.cli.utils        filter_callback:182  Added filter: {'pattern': '(?P\\d{4}\\.\\d{2}\\.\\d{2})', 'value': 10, 'groupname': 'date', 'time_unit': 'days', 'timestring': u'%Y.%m.%d', 'method': 'older_than'}\n2015-07-14 14:25:16,050 DEBUG          curator.cli.utils        filter_callback:183  New list of filters: [{'pattern': '(?P\\d{4}\\.\\d{2}\\.\\d{2})', 'value': 10, 'groupname': 'date', 'time_unit': 'days', 'timestring': u'%Y.%m.%d', 'method': 'older_than'}]\n2015-07-14 14:25:16,050 INFO      curator.cli.index_selection                indices:53   Job starting: delete indices\n2015-07-14 14:25:16,050 DEBUG     curator.cli.index_selection                indices:54   Params: {'url_prefix': u'', 'http_auth': u'admin:password', 'dry_run': False, 'loglevel': u'INFO', 'logformat': u'default', 'host': u'securelk', 'timeout': 30, 'debug': True, 'use_ssl': True, 'logfile': None, 'master_only': False, 'port': 9200}\n2015-07-14 14:25:16,050 DEBUG          curator.cli.utils             get_client:110  kwargs = {'url_prefix': u'', 'http_auth': u'admin:password', 'dry_run': False, 'loglevel': u'INFO', 'host': u'securelk', 'logformat': u'default', 'timeout': 30, 'debug': True, 'use_ssl': True, 'logfile': None, 'master_only': False, 'port': 9200}\n2015-07-14 14:25:16,050 INFO           curator.cli.utils             get_client:114  Attempting to verify SSL certificate.\n/usr/local/lib/python2.7/dist-packages/elasticsearch/connection/http_urllib3.py:50: UserWarning: Connecting to securelk using SSL with verify_certs=False is insecure.\n  'Connecting to %s using SSL with verify_certs=False is insecure.' % host)\n2015-07-14 14:25:16,050 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-07-14 14:25:16,050 INFO      urllib3.connectionpool              new_conn:735  Starting new HTTPS connection (1): securelk\n/usr/local/lib/python2.7/dist-packages/urllib3/util/ssl.py:90: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.\n  InsecurePlatformWarning\n/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.py:768: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html\n  InsecureRequestWarning)\n2015-07-14 14:25:16,094 DEBUG     urllib3.connectionpool          _make_request:383  \"GET / HTTP/1.1\" 200 333\n2015-07-14 14:25:16,095 INFO               elasticsearch    log_request_success:63   GET http://securelk:9200/ [status:200 request:0.045s]\n2015-07-14 14:25:16,095 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-14 14:25:16,096 DEBUG              elasticsearch    log_request_success:66   < {\n  \"status\" : 200,\n  \"name\" : \"node1\",\n  \"cluster_name\" : \"tony_prod_sec\",\n  \"version\" : {\n    \"number\" : \"1.6.0\",\n    \"build_hash\" : \"cdd3ac4dde4f69524ec0a14de3828cb95bbb86d0\",\n    \"build_timestamp\" : \"2015-06-09T13:36:34Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"4.10.4\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n2015-07-14 14:25:16,096 DEBUG          curator.cli.utils          check_version:90   Detected Elasticsearch version 1.6.0\n2015-07-14 14:25:16,097 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.py:768: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html\n  InsecureRequestWarning)\n2015-07-14 14:25:16,107 DEBUG     urllib3.connectionpool          make_request:383  \"GET /_all/_settings?expand_wildcards=open%2Cclosed HTTP/1.1\" 200 17619\n2015-07-14 14:25:16,108 INFO               elasticsearch    log_request_success:63   GET http://securelk:9200/_all/_settings?expand_wildcards=open%2Cclosed [status:200 request:0.011s]\n2015-07-14 14:25:16,108 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-14 14:25:16,108 DEBUG              elasticsearch    log_request_success:66   < {\".triggered_watches\":{\"settings\":{\"index\":{\"creation_date\":\"1435755045430\",\"number_of_replicas\":\"1\",\"mapper\":{\"dynamic\":\"false\"},\"uuid\":\"pb6fxgh4RJy-kN40uiyT9A\",\"number_of_shards\":\"1\",\"refresh_interval\":\"-1\",\"version\":{\"created\":\"1060099\"}}}},\".marvel-2015.06.30\":{\"settings\":{\"index\":{\"creation_date\":\"1435668071428\",\"marvel\":{\"index_format\":\"6\"},\"number_of_replicas\":\"1\",\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"z9OJb_CISz2WG9-9vmDjtQ\",\"analysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"stopwords\":\"_none\"}}},\"number_of_shards\":\"1\",\"version\":{\"created\":\"1060099\"}}}},\".shield_audit_log-2015.07.08\":{\"settings\":{\"index\":{\"mapper\":{\"dynamic\":\"false\"},\"creation_date\":\"1436313602554\",\"uuid\":\"LMQFdT3BQQ-50smmEcaXAg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"version\":{\"created\":\"1060099\"}}}},\".marvel-2015.06.24\":{\"settings\":{\"index\":{\"creation_date\":\"1435104032841\",\"marvel\":{\"index_format\":\"6\"},\"number_of_replicas\":\"1\",\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"Er1-EIylSTmLK52yByXo-g\",\"analysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"number_of_shards\":\"1\",\"version\":{\"created\":\"1060099\"}}}},\".shield_audit_log-2015.07.09\":{\"settings\":{\"index\":{\"mapper\":{\"dynamic\":\"false\"},\"creation_date\":\"1436400000943\",\"uuid\":\"EpkdkWUvQfGHl8xwF9lzkw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"version\":{\"created\":\"1060099\"}}}},\".marvel-2015.07.07\":{\"settings\":{\"index\":{\"creation_date\":\"1436227200908\",\"number_of_replicas\":\"1\",\"marvel\":{\"index_format\":\"6\"},\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"OqX5t-HDQCKOWK1YHi0DDA\",\"analysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"number_of_shards\":\"1\",\"version\":{\"created\":\"1060099\"}}}},\".kibana\":{\"settings\":{\"index\":{\"creation_date\":\"1430136122221\",\"uuid\":\"wE1B3U5DRQK3gyo9p24c7A\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"1\",\"version\":{\"created\":\"1050199\"}}}},\"logstash-syslog-2015.07.09\":{\"settings\":{\"index\":{\"creation_date\":\"1436400002868\",\"uuid\":\"RYrn7a9dQAC4xifvwldxVA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1060099\"}}}},\".shield_audit_log-2015.07.14\":{\"settings\":{\"index\":{\"creation_date\":\"1436832002869\",\"mapper\":{\"dynamic\":\"false\"},\"uuid\":\"m8Wk5PpeTkS9tFZI5wVbMg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"version\":{\"created\":\"1060099\"}}}},\".marvel-2015.07.02\":{\"settings\":{\"index\":{\"creation_date\":\"1435829517478\",\"number_of_replicas\":\"1\",\"marvel\":{\"index_format\":\"6\"},\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"3f_V7ktpRrGNQ5zCUFWnvg\",\"analysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"number_of_shards\":\"1\",\"version\":{\"created\":\"1060099\"}}}},\".watches\":{\"settings\":{\"index\":{\"mapper\":{\"dynamic\":\"false\"},\"creation_date\":\"1436455091090\",\"uuid\":\"MzVXF83MRBueuERpdGA5tw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"1\",\"version\":{\"created\":\"1060099\"}}}},\".marvel-kibana\":{\"settings\":{\"index\":{\"creation_date\":\"1430128889683\",\"marvel\":{\"index_format\":\"6\"},\"number_of_replicas\":\"1\",\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"PzUNRTTJQBq2121WWvYl5Q\",\"analysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"number_of_shards\":\"1\",\"version\":{\"created\":\"1050199\"}}}},\".marvel-2015.06.25\":{\"settings\":{\"index\":{\"creation_date\":\"1435190410616\",\"marvel\":{\"index_format\":\"6\"},\"number_of_replicas\":\"1\",\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"nwBmr61eSJWM0Dthp0L0zQ\",\"analysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"number_of_shards\":\"1\",\"version\":{\"created\":\"1060099\"}}}},\".marvel-2015.07.08\":{\"settings\":{\"index\":{\"creation_date\":\"1436313603054\",\"number_of_replicas\":\"1\",\"marvel\":{\"index_format\":\"6\"},\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"YUDcUOzQRU2cWZ1DTlNZsg\",\"analysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"number_of_shards\":\"1\",\"version\":{\"created\":\"1060099\"}}}},\"logstash-other-2015.07.14\":{\"settings\":{\"index\":{\"creation_date\":\"1436861269028\",\"uuid\":\"Y69QQKdjT3yGLgG-o105BA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1060099\"}}}},\"packetbeat-2015.07.07\":{\"settings\":{\"index\":{\"creation_date\":\"1436279906906\",\"uuid\":\"SyrjpCOoTuisnckP9ab7AQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1060099\"}}}},\"logstash-syslog-2015.07.08\":{\"settings\":{\"index\":{\"creation_date\":\"1436313602224\",\"uuid\":\"TulV4VuRRRWNQuUaxCdPTg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1060099\"}}}},\"logs\":{\"settings\":{\"index\":{\"creation_date\":\"1434471027802\",\"uuid\":\"5xIMgcYWR4uceiUJLqeu6Q\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"version\":{\"created\":\"1060099\"}}}},\"logstash-syslog-2015.07.10\":{\"settings\":{\"index\":{\"creation_date\":\"1436486400996\",\"uuid\":\"cjR3trLAQ2Crvom5jwn5nQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1060099\"}}}},\"packetbeat-2015.07.13\":{\"settings\":{\"index\":{\"creation_date\":\"1436791272483\",\"uuid\":\"eoXf6xCUTKC-3Bd8D2nTjw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1060099\"}}}},\"indexb\":{\"settings\":{\"index\":{\"creation_date\":\"1434016895861\",\"uuid\":\"hRF93FBjTVmWXhICJrW3AQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"version\":{\"created\":\"1060099\"}}}},\".marvel-2015.07.09\":{\"settings\":{\"index\":{\"creation_date\":\"1436400001701\",\"number_of_replicas\":\"1\",\"marvel\":{\"index_format\":\"6\"},\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"ee5P3xE4TsqxACaqhXvZgQ\",\"analysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"number_of_shards\":\"1\",\"version\":{\"created\":\"1060099\"}}}},\".shield_audit_log-2015.07.13\":{\"settings\":{\"index\":{\"creation_date\":\"1436791239927\",\"mapper\":{\"dynamic\":\"false\"},\"uuid\":\"yQEqZp9TSNCIs714M9r-ow\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"version\":{\"created\":\"1060099\"}}}},\".shield-audit-log-2015-07-01\":{\"settings\":{\"index\":{\"creation_date\":\"1435753919513\",\"uuid\":\"VEYW31ZHSkW1SPdE42bR9w\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"version\":{\"created\":\"1060099\"}}}},\"users\":{\"settings\":{\"index\":{\"creation_date\":\"1431700289771\",\"uuid\":\"a2bTnirzRpm_AYe9e9puvA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"version\":{\"created\":\"1050299\"}}}},\"packetbeat-2015.07.09\":{\"settings\":{\"index\":{\"creation_date\":\"1436392918468\",\"uuid\":\"4XLp0tQnQLOoE5eHeoUr0A\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1060099\"}}}},\"test_index2\":{\"settings\":{\"index\":{\"creation_date\":\"1434026846321\",\"uuid\":\"1Q1bYJu-TEilVfs9nQRWOg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"version\":{\"created\":\"1060099\"}}}},\".watch_history-2015.07.14\":{\"settings\":{\"index\":{\"creation_date\":\"1436859076828\",\"mapper\":{\"dynamic\":\"false\"},\"uuid\":\"rdVYeFctTWuBwzsBnlwlyw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"1\",\"version\":{\"created\":\"1060099\"}}}},\".marvel-2015.06.26\":{\"settings\":{\"index\":{\"creation_date\":\"1435276830869\",\"number_of_replicas\":\"1\",\"marvel\":{\"index_format\":\"6\"},\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"CSNFM_SiTkuPlj7hJs_X0w\",\"analysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"number_of_shards\":\"1\",\"version\":{\"created\":\"1060099\"}}}},\".shield_audit_log-2015.07.06\":{\"settings\":{\"index\":{\"mapper\":{\"dynamic\":\"false\"},\"creation_date\":\"1436172714134\",\"uuid\":\"ul8bU-huQzWqwtqp0baAzQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"version\":{\"created\":\"1060099\"}}}},\"fruit\":{\"settings\":{\"index\":{\"creation_date\":\"1433838972384\",\"uuid\":\"IbIcNhstTSOP3crgdV2mcg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"version\":{\"created\":\"1050299\"}}}},\".shield_audit_log-2015.07.03\":{\"settings\":{\"index\":{\"mapper\":{\"dynamic\":\"false\"},\"creation_date\":\"1435914262487\",\"uuid\":\"nv_OQxLKT1eQDN0OmUzbJA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"version\":{\"created\":\"1060099\"}}}},\".shield_audit_log-2015.07.01\":{\"settings\":{\"index\":{\"mapper\":{\"dynamic\":\"false\"},\"creation_date\":\"1435758539159\",\"uuid\":\"9NxYj5X6RyGWQKPcS1OwFg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"version\":{\"created\":\"1060099\"}}}},\".marvel-2015.07.06\":{\"settings\":{\"index\":{\"creation_date\":\"1436172717413\",\"number_of_replicas\":\"1\",\"marvel\":{\"index_format\":\"6\"},\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"33QgQjh1S8GaBZkWct_glQ\",\"analysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"number_of_shards\":\"1\",\"version\":{\"created\":\"1060099\"}}}},\"attractions\":{\"settings\":{\"index\":{\"creation_date\":\"1433344654100\",\"uuid\":\"3KEJawgQRuObaGQ-pTVpJA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"version\":{\"created\":\"1050299\"}}}},\"test_index\":{\"settings\":{\"index\":{\"creation_date\":\"1434032542487\",\"uuid\":\"kio0qDfLSp6dFE31FmkXEA\",\"number_of_replicas\":\"1\",\"analysis\":{\"char_filter\":{\"my_char_filter1\":{\"type\":\"mapping\",\"mappings\":[\"\u222b=>\\u0020integral\\u0020\",\"\u00b5=>\\u0020micro\\u0020\"]}},\"analyzer\":{\"special_chars_analyzer\":{\"type\":\"custom\",\"char_filter\":\"my_char_filter1\",\"filter\":\"lowercase\",\"tokenizer\":\"whitespace\"}}},\"number_of_shards\":\"1\",\"version\":{\"created\":\"1060099\"}}}},\".marvel-2015.07.01\":{\"settings\":{\"index\":{\"creation_date\":\"1435708802510\",\"marvel\":{\"index_format\":\"6\"},\"number_of_replicas\":\"1\",\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"ZrDm6-3MRsqmXJSCLuXDPQ\",\"analysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"number_of_shards\":\"1\",\"version\":{\"created\":\"1060099\"}}}},\"index\":{\"settings\":{\"index\":{\"creation_date\":\"1434024363139\",\"uuid\":\"JG2QI-LYRrGI8z3RV-FLfg\",\"number_of_replicas\":\"1\",\"analysis\":{\"char_filter\":{\"my_mapping\":{\"type\":\"mapping\",\"mappings\":[\"\u00b5=>MICRO\",\"\u222b=>INTEGRAL\"]}},\"analyzer\":{\"custom_with_char_filter\":{\"char_filter\":[\"my_mapping\"],\"tokenizer\":\"standard\"}}},\"number_of_shards\":\"1\",\"version\":{\"created\":\"1060099\"}}}},\".marvel-2015.06.27\":{\"settings\":{\"index\":{\"creation_date\":\"1435363200018\",\"number_of_replicas\":\"1\",\"marvel\":{\"index_format\":\"6\"},\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"Sr2kWYmhT6OXeDGrRfWZaQ\",\"analysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"number_of_shards\":\"1\",\"version\":{\"created\":\"1060099\"}}}},\".marvel-2015.07.03\":{\"settings\":{\"index\":{\"creation_date\":\"1435914266395\",\"marvel\":{\"index_format\":\"6\"},\"number_of_replicas\":\"1\",\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"3CwZvOoYQKKtH0hM_1qSZw\",\"analysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"number_of_shards\":\"1\",\"version\":{\"created\":\"1060099\"}}}},\".shield_audit_log-2015.07.10\":{\"settings\":{\"index\":{\"mapper\":{\"dynamic\":\"false\"},\"creation_date\":\"1436486400710\",\"uuid\":\"AeReDo7mQVyKHMjOWs2CDg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"version\":{\"created\":\"1060099\"}}}},\".marvel-2015.06.21\":{\"settings\":{\"index\":{\"creation_date\":\"1434844805383\",\"marvel\":{\"index_format\":\"6\"},\"number_of_replicas\":\"1\",\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"C18o9FVsTHKj-6TfwciE8A\",\"analysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"number_of_shards\":\"1\",\"version\":{\"created\":\"1060099\"}}}},\"logstash-syslog-2015.07.07\":{\"settings\":{\"index\":{\"creation_date\":\"1436227200754\",\"uuid\":\"zTzg9MGWR4yFKbqFjBT40A\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1060099\"}}}},\".marvel-2015.06.28\":{\"settings\":{\"index\":{\"creation_date\":\"1435449600044\",\"number_of_replicas\":\"1\",\"marvel\":{\"index_format\":\"6\"},\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"2-YVbeCTT_GnbtF0ZQ5kyQ\",\"analysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"number_of_shards\":\"1\",\"version\":{\"created\":\"1060099\"}}}},\"test\":{\"settings\":{\"index\":{\"creation_date\":\"1436455088376\",\"uuid\":\"D38BrOFwSqSikRt_Q91H9Q\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"version\":{\"created\":\"1060099\"}}}},\"log-events\":{\"settings\":{\"index\":{\"creation_date\":\"1436453017205\",\"uuid\":\"t1bmEudkRmmPnPFz4-kAKQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"version\":{\"created\":\"1060099\"}}}},\"attractions2\":{\"settings\":{\"index\":{\"creation_date\":\"1433341273304\",\"uuid\":\"LlzK5-RbQBCY6uIEQe89kA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"version\":{\"created\":\"1050299\"}}}},\".shield_audit_log-2015.07.07\":{\"settings\":{\"index\":{\"mapper\":{\"dynamic\":\"false\"},\"creation_date\":\"1436227200031\",\"uuid\":\"oOQlteZ7QeKHkgjVM2AOKA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"version\":{\"created\":\"1060099\"}}}},\"10381\":{\"settings\":{\"index\":{\"creation_date\":\"1434097704836\",\"qmot-score-spring\":{\"mappings\":{\"qmot-score-spring\":{\"ttl\":{\"enabled\":\"true\",\"default\":\"2592000000\"},\"properties\":{\"con\":{\"type\":\"string\"},\"tags\":{\"type\":\"string\"},\"d\":{\"type\":\"long\"},\"alc\":{\"type\":\"integer\"},\"dla\":{\"type\":\"integer\"},\"ts\":{\"type\":\"date\",\"format\":\"dateOptionalTime\"},\"c\":{\"type\":\"string\"},\"dlc\":{\"type\":\"integer\"},\"dlb\":{\"type\":\"integer\"},\"ri\":{\"type\":\"string\"},\"n\":{\"type\":\"string\",\"index\":\"not_analyzed\"},\"ala\":{\"type\":\"integer\"},\"alb\":{\"type\":\"integer\"},\"m\":{\"type\":\"string\"},\"ext\":{\"properties\":{\"tx\":{\"properties\":{\"n\":{\"type\":\"string\"},\"txt0\":{\"type\":\"long\"},\"t\":{\"type\":\"string\"},\"txn\":{\"type\":\"string\"},\"txd\":{\"type\":\"long\"}}}}},\"si\":{\"type\":\"string\"},\"@timestamp\":{\"type\":\"date\",\"format\":\"dateOptionalTime\"},\"v\":{\"type\":\"string\"},\"r\":{\"type\":\"string\"},\"p\":{\"type\":\"string\"},\"pro\":{\"type\":\"string\"},\"es\":{\"type\":\"string\"}}}}},\"uuid\":\"_ZVBX3dXSdKMI0mhdeBNGw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"version\":{\"created\":\"1060099\"}}}},\"http-lookup\":{\"settings\":{\"index\":{\"creation_date\":\"1434466783299\",\"uuid\":\"DLzY-RYJTSmTdwyEeCXXhg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"version\":{\"created\":\"1060099\"}}}},\"packetbeat-2015.07.08\":{\"settings\":{\"index\":{\"creation_date\":\"1436342780189\",\"uuid\":\"w0GQUx97RZC7Zx-Ja91Blw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1060099\"}}}},\".marvel-2015.07.10\":{\"settings\":{\"index\":{\"creation_date\":\"1436486400574\",\"number_of_replicas\":\"1\",\"marvel\":{\"index_format\":\"6\"},\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"phAtxqChSKKIel10Oq6dFw\",\"analysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"stopwords\":\"_none\"}}},\"number_of_shards\":\"1\",\"version\":{\"created\":\"1060099\"}}}},\".marvel-2015.07.13\":{\"settings\":{\"index\":{\"creation_date\":\"1436791242948\",\"marvel\":{\"index_format\":\"6\"},\"number_of_replicas\":\"1\",\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"viYHxfn8Teyv27mDzGu_Ag\",\"analysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"number_of_shards\":\"1\",\"version\":{\"created\":\"1060099\"}}}},\"logstash-syslog-2015.07.14\":{\"settings\":{\"index\":{\"creation_date\":\"1436860007656\",\"uuid\":\"wlMefCRyRtuf0YI0st-htQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1060099\"}}}},\".marvel-2015.06.22\":{\"settings\":{\"index\":{\"creation_date\":\"1434931202896\",\"marvel\":{\"index_format\":\"6\"},\"number_of_replicas\":\"1\",\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"kLhf8wH2TC2WxHU20h5u_g\",\"analysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"number_of_shards\":\"1\",\"version\":{\"created\":\"1060099\"}}}},\".shield_audit_log-2015.07.02\":{\"settings\":{\"index\":{\"mapper\":{\"dynamic\":\"false\"},\"creation_date\":\"1435829515506\",\"uuid\":\"PsCNFa8ORWK8cB82K3OM0g\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"version\":{\"created\":\"1060099\"}}}},\"10228\":{\"settings\":{\"index\":{\"creation_date\":\"1433775305881\",\"uuid\":\"1AthROXARquD4ldy777BOA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"version\":{\"created\":\"1050299\"}}}},\"logstash-test\":{\"settings\":{\"index\":{\"creation_date\":\"1436453255844\",\"uuid\":\"rYBooYmNQ46hCJcBewGL6Q\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1060099\"}}}},\"kibana-int\":{\"settings\":{\"index\":{\"creation_date\":\"1436344185612\",\"uuid\":\"Aj-4A_M0Q2eQpTuBl4X3iQ\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"version\":{\"created\":\"1060099\"}}}},\".marvel-2015.06.23\":{\"settings\":{\"index\":{\"creation_date\":\"1435017608610\",\"marvel\":{\"index_format\":\"6\"},\"number_of_replicas\":\"1\",\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"XpcOv6haSrampbEbxMfRTw\",\"analysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"number_of_shards\":\"1\",\"version\":{\"created\":\"1060099\"}}}},\".packetbeat-topology\":{\"settings\":{\"index\":{\"creation_date\":\"1431507999699\",\"uuid\":\"d4yfgLHtRa6J8mjlduKcpg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"version\":{\"created\":\"1050299\"}}}},\"logstash-syslog-2015.07.06\":{\"settings\":{\"index\":{\"creation_date\":\"1436182796353\",\"uuid\":\"DiLXaG0oQhqfLvwDFHrJLw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1060099\"}}}},\"newyear_test\":{\"settings\":{\"index\":{\"creation_date\":\"1433864651574\",\"uuid\":\"BBK9nHl6R2ePgpA_qzKIFg\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"version\":{\"created\":\"1050299\"}}}},\".marvel-2015.07.14\":{\"settings\":{\"index\":{\"creation_date\":\"1436832003224\",\"marvel\":{\"index_format\":\"6\"},\"number_of_replicas\":\"1\",\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"1o5YaGtLRh2sBc8zL6VRfA\",\"analysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"number_of_shards\":\"1\",\"version\":{\"created\":\"1060099\"}}}},\".marvel-2015.06.29\":{\"settings\":{\"index\":{\"creation_date\":\"1435536003749\",\"number_of_replicas\":\"1\",\"marvel\":{\"index_format\":\"6\"},\"mapper\":{\"dynamic\":\"true\"},\"uuid\":\"e4KyVx1DQdegEgIMrwxlEw\",\"analysis\":{\"analyzer\":{\"default\":{\"type\":\"standard\",\"stopwords\":\"none\"}}},\"number_of_shards\":\"1\",\"version\":{\"created\":\"1060099\"}}}},\"packetbeat-2015.07.10\":{\"settings\":{\"index\":{\"creation_date\":\"1436479310050\",\"uuid\":\"sIn2_isKS1ajkL7cuTNr1Q\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1060099\"}}}},\"packetbeat-2015.07.14\":{\"settings\":{\"index\":{\"creation_date\":\"1436824821543\",\"uuid\":\"T8ZJYOpCQG-gYukLr-laaA\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"3\",\"refresh_interval\":\"5s\",\"version\":{\"created\":\"1060099\"}}}}}\n2015-07-14 14:25:16,119 DEBUG          curator.api.utils            get_indices:27   All indices: [u'logstash-syslog-2015.07.07', u'attractions2', u'logs', u'indexb', u'.shield-audit-log-2015-07-01', u'logstash-other-2015.07.14', u'logstash-syslog-2015.07.09', u'logstash-syslog-2015.07.08', u'.marvel-2015.07.03', u'.marvel-2015.07.02', u'.marvel-2015.07.01', u'logstash-syslog-2015.07.06', u'.marvel-2015.07.07', u'.kibana', u'10381', u'.watches', u'.marvel-2015.07.06', u'.marvel-2015.07.09', u'.marvel-2015.07.08', u'kibana-int', u'.watch_history-2015.07.14', u'index', u'.shield_audit_log-2015.07.02', u'.shield_audit_log-2015.07.03', u'.shield_audit_log-2015.07.01', u'.shield_audit_log-2015.07.08', u'.shield_audit_log-2015.07.09', u'packetbeat-2015.07.09', u'packetbeat-2015.07.08', u'log-events', u'attractions', u'test', u'packetbeat-2015.07.07', u'.shield_audit_log-2015.07.06', u'logstash-syslog-2015.07.14', u'10228', u'users', u'logstash-syslog-2015.07.10', u'.shield_audit_log-2015.07.07', u'.marvel-2015.07.14', u'test_index', u'.marvel-2015.07.10', u'test_index2', u'.marvel-2015.07.13', u'.marvel-2015.06.26', u'.marvel-2015.06.27', u'.marvel-2015.06.24', u'.marvel-2015.06.25', u'.marvel-2015.06.22', u'.marvel-2015.06.23', u'.marvel-kibana', u'.marvel-2015.06.21', u'newyear_test', u'.marvel-2015.06.28', u'.marvel-2015.06.29', u'.shield_audit_log-2015.07.10', u'.shield_audit_log-2015.07.13', u'.shield_audit_log-2015.07.14', u'.packetbeat-topology', u'http-lookup', u'fruit', u'logstash-test', u'packetbeat-2015.07.13', u'packetbeat-2015.07.10', u'.triggered_watches', u'packetbeat-2015.07.14', u'.marvel-2015.06.30']\n2015-07-14 14:25:16,119 DEBUG     curator.cli.index_selection                indices:60   Full list of indices: [u'logstash-syslog-2015.07.07', u'attractions2', u'logs', u'indexb', u'.shield-audit-log-2015-07-01', u'logstash-other-2015.07.14', u'logstash-syslog-2015.07.09', u'logstash-syslog-2015.07.08', u'.marvel-2015.07.03', u'.marvel-2015.07.02', u'.marvel-2015.07.01', u'logstash-syslog-2015.07.06', u'.marvel-2015.07.07', u'.kibana', u'10381', u'.watches', u'.marvel-2015.07.06', u'.marvel-2015.07.09', u'.marvel-2015.07.08', u'kibana-int', u'.watch_history-2015.07.14', u'index', u'.shield_audit_log-2015.07.02', u'.shield_audit_log-2015.07.03', u'.shield_audit_log-2015.07.01', u'.shield_audit_log-2015.07.08', u'.shield_audit_log-2015.07.09', u'packetbeat-2015.07.09', u'packetbeat-2015.07.08', u'log-events', u'attractions', u'test', u'packetbeat-2015.07.07', u'.shield_audit_log-2015.07.06', u'logstash-syslog-2015.07.14', u'10228', u'users', u'logstash-syslog-2015.07.10', u'.shield_audit_log-2015.07.07', u'.marvel-2015.07.14', u'test_index', u'.marvel-2015.07.10', u'test_index2', u'.marvel-2015.07.13', u'.marvel-2015.06.26', u'.marvel-2015.06.27', u'.marvel-2015.06.24', u'.marvel-2015.06.25', u'.marvel-2015.06.22', u'.marvel-2015.06.23', u'.marvel-kibana', u'.marvel-2015.06.21', u'newyear_test', u'.marvel-2015.06.28', u'.marvel-2015.06.29', u'.shield_audit_log-2015.07.10', u'.shield_audit_log-2015.07.13', u'.shield_audit_log-2015.07.14', u'.packetbeat-topology', u'http-lookup', u'fruit', u'logstash-test', u'packetbeat-2015.07.13', u'packetbeat-2015.07.10', u'.triggered_watches', u'packetbeat-2015.07.14', u'.marvel-2015.06.30']\n2015-07-14 14:25:16,119 DEBUG     curator.cli.index_selection                indices:74   All filters: [{'pattern': '(?P\\d{4}\\.\\d{2}\\.\\d{2})', 'value': 10, 'groupname': 'date', 'time_unit': 'days', 'timestring': u'%Y.%m.%d', 'method': 'older_than'}]\n2015-07-14 14:25:16,119 DEBUG     curator.cli.index_selection                indices:79   Filter: {'pattern': '(?P\\d{4}\\.\\d{2}\\.\\d{2})', 'value': 10, 'groupname': 'date', 'time_unit': 'days', 'timestring': u'%Y.%m.%d', 'method': 'older_than'}\n2015-07-14 14:25:16,130 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.07\" is outside the cutoff period (older than 10 days).\n2015-07-14 14:25:16,130 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.14\" is outside the cutoff period (older than 10 days).\n2015-07-14 14:25:16,131 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.09\" is outside the cutoff period (older than 10 days).\n2015-07-14 14:25:16,131 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.08\" is outside the cutoff period (older than 10 days).\n2015-07-14 14:25:16,131 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.06\" is outside the cutoff period (older than 10 days).\n2015-07-14 14:25:16,131 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.07\" is outside the cutoff period (older than 10 days).\n2015-07-14 14:25:16,132 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.06\" is outside the cutoff period (older than 10 days).\n2015-07-14 14:25:16,132 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.09\" is outside the cutoff period (older than 10 days).\n2015-07-14 14:25:16,132 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.08\" is outside the cutoff period (older than 10 days).\n2015-07-14 14:25:16,132 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.14\" is outside the cutoff period (older than 10 days).\n2015-07-14 14:25:16,133 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.08\" is outside the cutoff period (older than 10 days).\n2015-07-14 14:25:16,133 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.09\" is outside the cutoff period (older than 10 days).\n2015-07-14 14:25:16,133 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.09\" is outside the cutoff period (older than 10 days).\n2015-07-14 14:25:16,133 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.08\" is outside the cutoff period (older than 10 days).\n2015-07-14 14:25:16,133 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.07\" is outside the cutoff period (older than 10 days).\n2015-07-14 14:25:16,134 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.06\" is outside the cutoff period (older than 10 days).\n2015-07-14 14:25:16,134 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.14\" is outside the cutoff period (older than 10 days).\n2015-07-14 14:25:16,134 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.10\" is outside the cutoff period (older than 10 days).\n2015-07-14 14:25:16,134 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.07\" is outside the cutoff period (older than 10 days).\n2015-07-14 14:25:16,134 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.14\" is outside the cutoff period (older than 10 days).\n2015-07-14 14:25:16,135 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.10\" is outside the cutoff period (older than 10 days).\n2015-07-14 14:25:16,135 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.13\" is outside the cutoff period (older than 10 days).\n2015-07-14 14:25:16,135 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.10\" is outside the cutoff period (older than 10 days).\n2015-07-14 14:25:16,136 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.13\" is outside the cutoff period (older than 10 days).\n2015-07-14 14:25:16,136 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.14\" is outside the cutoff period (older than 10 days).\n2015-07-14 14:25:16,136 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.13\" is outside the cutoff period (older than 10 days).\n2015-07-14 14:25:16,136 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.10\" is outside the cutoff period (older than 10 days).\n2015-07-14 14:25:16,136 DEBUG         curator.api.filter        timestamp_check:297  Timestamp \"2015.07.14\" is outside the cutoff period (older than 10 days).\n2015-07-14 14:25:16,136 INFO      curator.cli.index_selection                indices:83   Pruning Kibana-related indices to prevent accidental deletion.\n2015-07-14 14:25:16,137 DEBUG     curator.cli.index_selection                indices:110  ACTION: delete. INDICES: [u'.marvel-2015.06.21', u'.marvel-2015.06.22', u'.marvel-2015.06.23', u'.marvel-2015.06.24', u'.marvel-2015.06.25', u'.marvel-2015.06.26', u'.marvel-2015.06.27', u'.marvel-2015.06.28', u'.marvel-2015.06.29', u'.marvel-2015.06.30', u'.marvel-2015.07.01', u'.marvel-2015.07.02', u'.marvel-2015.07.03', u'.shield_audit_log-2015.07.01', u'.shield_audit_log-2015.07.02', u'.shield_audit_log-2015.07.03']\n2015-07-14 14:25:16,137 INFO          curator.api.delete         delete_indices:17   Deleting indices as a batch operation:\n2015-07-14 14:25:16,137 INFO          curator.api.delete         delete_indices:19   ---deleting index .marvel-2015.06.21\n2015-07-14 14:25:16,137 INFO          curator.api.delete         delete_indices:19   ---deleting index .marvel-2015.06.22\n2015-07-14 14:25:16,137 INFO          curator.api.delete         delete_indices:19   ---deleting index .marvel-2015.06.23\n2015-07-14 14:25:16,137 INFO          curator.api.delete         delete_indices:19   ---deleting index .marvel-2015.06.24\n2015-07-14 14:25:16,137 INFO          curator.api.delete         delete_indices:19   ---deleting index .marvel-2015.06.25\n2015-07-14 14:25:16,137 INFO          curator.api.delete         delete_indices:19   ---deleting index .marvel-2015.06.26\n2015-07-14 14:25:16,137 INFO          curator.api.delete         delete_indices:19   ---deleting index .marvel-2015.06.27\n2015-07-14 14:25:16,137 INFO          curator.api.delete         delete_indices:19   ---deleting index .marvel-2015.06.28\n2015-07-14 14:25:16,137 INFO          curator.api.delete         delete_indices:19   ---deleting index .marvel-2015.06.29\n2015-07-14 14:25:16,137 INFO          curator.api.delete         delete_indices:19   ---deleting index .marvel-2015.06.30\n2015-07-14 14:25:16,137 INFO          curator.api.delete         delete_indices:19   ---deleting index .marvel-2015.07.01\n2015-07-14 14:25:16,138 INFO          curator.api.delete         delete_indices:19   ---deleting index .marvel-2015.07.02\n2015-07-14 14:25:16,138 INFO          curator.api.delete         delete_indices:19   ---deleting index .marvel-2015.07.03\n2015-07-14 14:25:16,138 INFO          curator.api.delete         delete_indices:19   ---deleting index .shield_audit_log-2015.07.01\n2015-07-14 14:25:16,138 INFO          curator.api.delete         delete_indices:19   ---deleting index .shield_audit_log-2015.07.02\n2015-07-14 14:25:16,138 INFO          curator.api.delete         delete_indices:19   ---deleting index .shield_audit_log-2015.07.03\n2015-07-14 14:25:16,138 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.py:768: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html\n  InsecureRequestWarning)\n2015-07-14 14:25:17,893 DEBUG     urllib3.connectionpool          _make_request:383  \"DELETE /.marvel-2015.06.21,.marvel-2015.06.22,.marvel-2015.06.23,.marvel-2015.06.24,.marvel-2015.06.25,.marvel-2015.06.26,.marvel-2015.06.27,.marvel-2015.06.28,.marvel-2015.06.29,.marvel-2015.06.30,.marvel-2015.07.01,.marvel-2015.07.02,.marvel-2015.07.03,.shield_audit_log-2015.07.01,.shield_audit_log-2015.07.02,.shield_audit_log-2015.07.03 HTTP/1.1\" 200 21\n2015-07-14 14:25:17,893 INFO               elasticsearch    log_request_success:63   DELETE http://securelk:9200/.marvel-2015.06.21,.marvel-2015.06.22,.marvel-2015.06.23,.marvel-2015.06.24,.marvel-2015.06.25,.marvel-2015.06.26,.marvel-2015.06.27,.marvel-2015.06.28,.marvel-2015.06.29,.marvel-2015.06.30,.marvel-2015.07.01,.marvel-2015.07.02,.marvel-2015.07.03,.shield_audit_log-2015.07.01,.shield_audit_log-2015.07.02,.shield_audit_log-2015.07.03 [status:200 request:1.755s]\n2015-07-14 14:25:17,893 DEBUG              elasticsearch    log_request_success:65   > None\n2015-07-14 14:25:17,893 DEBUG              elasticsearch    log_request_success:66   < {\"acknowledged\":true}\n2015-07-14 14:25:17,893 INFO           curator.cli.utils               exit_msg:67   Job completed successfully.\n```\nmost likely not the right thing to do.... \n. @untergeek any ideas?\n. @untergeek \nuser@host $ cat /etc/linuxmint/info \nRELEASE=18\nCODENAME=sarah\nEDITION=\"Cinnamon 64-bit\"\nDESCRIPTION=\"Linux Mint 18 Sarah\"\nDESKTOP=Gnome\nTOOLKIT=GTK\nNEW_FEATURES_URL=http://www.linuxmint.com/rel_sarah_cinnamon_whatsnew.php\nRELEASE_NOTES_URL=http://www.linuxmint.com/rel_sarah_cinnamon.php\nUSER_GUIDE_URL=help:linuxmint\nGRUB_TITLE=Linux Mint 18 Cinnamon 64-bit\nuser@host $ uname -a\nLinux w530 4.4.0-45-generic #66-Ubuntu SMP Wed Oct 19 14:12:37 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\npython was pointing at python2.7 then I've seen https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings and I've pointed to the newest binary I could find installled\n. thanks @untergeek ! \nno luck :(\n```\nuser@host /opt/elk/installers $ wget https://dl.dropboxusercontent.com/u/1085030/elasticsearch-curator_4.1.2_amd64.deb\n--2016-10-26 01:42:24--  https://dl.dropboxusercontent.com/u/1085030/elasticsearch-curator_4.1.2_amd64.deb\nResolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... 108.160.173.165\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|108.160.173.165|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 8192264 (7,8M) [application/x-debian-package]\nSaving to: \u2018elasticsearch-curator_4.1.2_amd64.deb\u2019\nelasticsearch-curator_4.1.2_amd64.deb                       100%[=========================================================================================================================================>]   7,81M  7,63MB/s    in 1,0s    \n2016-10-26 01:42:26 (7,63 MB/s) - \u2018elasticsearch-curator_4.1.2_amd64.deb\u2019 saved [8192264/8192264]\nuser@host /opt/elk/installers $ sudo dpkg -i elasticsearch-curator_4.1.2_amd64.deb \n(Reading database ... 238494 files and directories currently installed.)\nPreparing to unpack elasticsearch-curator_4.1.2_amd64.deb ...\nUnpacking elasticsearch-curator (4.1.2) over (4.1.2) ...\nrmdir: failed to remove '/opt/elasticsearch-curator': Directory not empty\ndpkg: warning: subprocess old post-removal script returned error exit status 1\ndpkg: trying script from the new package instead ...\nrmdir: failed to remove '/opt/elasticsearch-curator': Directory not empty\ndpkg: error processing archive elasticsearch-curator_4.1.2_amd64.deb (--install):\n subprocess new post-removal script returned error exit status 1\nErrors were encountered while processing:\n elasticsearch-curator_4.1.2_amd64.deb\nuser@host /opt/elk/installers $ ls -alrth /opt/\ntotal 530M\ndrwxr-xr-x  3 user user 4,0K Oct 25 09:41 google\ndrwxr-xr-x  8 user user 4,0K Oct 25 09:55 idea-IC-162.2228.15\ndrwxr-xr-x 23 root        root        4,0K Oct 25 10:15 ..\ndrwxr-xr-x  4 user user 4,0K Oct 26 01:42 elk\ndrwxr-xr-x  6 user user 4,0K Oct 26 01:42 .\n```\n. > Try doing apt purge elasticsearch-curator\nthat did the trick! \nuser@host /opt/elk/prod/common/curator $ curator --config prod.config delete_all_last_21d.action  --dry-run\n2016-10-26 02:50:55,257 INFO      Preparing Action ID: 1, \"delete_indices\"\n2016-10-26 02:50:55,277 INFO      Trying Action ID: 1, \"delete_indices\": Delete indices older than 45 days (based on index name), for logstash- prefixed indices. Ignore the error if the filter does not result in an actionable list of indices (ignore_empty_list) and exit cleanly.\n2016-10-26 02:50:55,308 INFO      Skipping action \"delete_indices\" due to empty list: <class 'curator.exceptions.NoIndices'>\n2016-10-26 02:50:55,308 INFO      Action ID: 1, \"delete_indices\" completed.\n2016-10-26 02:50:55,308 INFO      Job completed.\nI take this ^^ means the connection worked fine?\n. @untergeek this is from my test setup. SSL certs are done creating my own CA and signing my own certificates.\n. @untergeek this is from my test setup. SSL certs are done creating my own\nCA and signing my own certificates.\nOn Wed, Oct 26, 2016 at 6:05 AM, Aaron Mildenstein <notifications@github.com\n\nwrote:\n@nellicus https://github.com/nellicus is this for a customer? I plan to\nrelease 4.2.0 really soon, and the new binary packages will be based on\nthis build method.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/elastic/curator/issues/792#issuecomment-256244287,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AIXSMf4LFmAdFYZsMErSMfVSZVn26OI0ks5q3tFsgaJpZM4KgVel\n.\n\n\nAntonio Bonuccelli\nElastic Support\n. absolutely, thank you!\n. ",
    "delfuego": "That sounds great... but just to make sure I understand, the logstash-JSON-formatted output will still occur, yes? (I'm not logging to a file, I'm logging to stdout, and piping it to an nc connection to logstash's TCP input... so I still need stdout output, just not non-logstash-formatted output.)\n. ",
    "allenmchan": "My apologies, i didnt even realize there is a newer version. Let me test that right now.\n. Looks like the python error is now gone with the latest version. i still see curator can only close 4-5 indices per run. I guess i will close this and open a ticket on the elasticsearch side to troubleshoot.\n. You are awesome. Looks like adjusting timeout does indeed help. The longer the timeout the more i can close now. Thanks for the lightning fast response on the weekend.\n. Actually the weird thing is that even though the log says the optimize call failed, running \n'GET logstash-2015.10.01/_segments' in Marvel shows only one segment per shard. So it looks like the log is giving a false error.\n. Thank you for the explanation. I will close this as it seems to be my performance issue and not curator\n. ",
    "steffo": "Hi, \nAdded changes a per review. Integration test are underway. I don't understand why check is failing now. Working on it.\n. ",
    "ninjada": "timeunit change did the trick, thanks @untergeek \n. I guess just making sure the connection for backing up to s3 is encrypted and also if there are any possibilities to use S3's server side encryption.\nAgain, I'm only beginning to explore options but with S3 you can encrypt the data server side using the --sse flag and specific encryption keys with the aws cli.\nJust wondering if there's an equivalent option possible via Curator, specifying an encryption key for the data to be encrypted server side on S3?\n. ",
    "garyelephant": "My Indices optimization typically take < 20 minutes, so should I make client socket_timeout = 20 * 60 and leave request_timeout in optimize() default ?\nclient = Elasticsearch( hosts='es_in_production', socket_timeout=20 * 60 )\n. Sorry, I have no idea to write test code, although I read the code in test folder,\nCould you please tell me how to write test code in such condition:\nhttps://github.com/garyelephant/curator/blob/master/curator/api/utils.py#L253\n. ",
    "pricecarl": "Thanks untergeek, I actually installed via the yum option, is there a way I can fix this without pip?\n. Extract below:\n```\n[root@calhda05 ~]# python\nPython 2.6.6 (r266:84292, Jan 22 2014, 09:42:36)\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-4)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport elasticsearch\nelasticsearch.version\n(1, 0, 1)\n```\n. Hey,\n\n\n\nI've updated through pip and now I have the correct version:\n```\nCollecting elasticsearch\nDownloading elasticsearch-1.6.0-py2.py3-none-any.whl (58kB)\n100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 2.1MB/s\nCollecting urllib3<2.0,>=1.8 (from elasticsearch)\nDownloading urllib3-1.11-py2.py3-none-any.whl (84kB)\n100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86kB 4.5MB/s\nInstalling collected packages: urllib3, elasticsearch\n Found existing installation: urllib3 1.8.2\n Uninstalling urllib3-1.8.2:\n   Successfully uninstalled urllib3-1.8.2\nFound existing installation: elasticsearch 1.0.1\n Uninstalling elasticsearch-1.0.1:\n  Successfully uninstalled elasticsearch-1.0.1\nSuccessfully installed elasticsearch urllib3\n[root@calhda05 ~]# python\nPython 2.6.6 (r266:84292, Jan 22 2014, 09:42:36)\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-4)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport elasticsearch\nelasticsearch.version\n(1, 6, 0)\n```\n. Nope curator wasn't installed on this before I yum installed it so id have no need for pip.\n\n\n\nI just had to install pip in fact to update elasticsearch using your command\n. Okay so this wouldn't be a bug if this was a fresh install?\n. Ah I see, well I'm glad I've informed you.\n. Morning @untergeek \nI tried to run a close command on this cluster and received this error:\n2015-08-06 08:39:50,371 DEBUG          curator.cli.utils        filter_callback:179  REGEX = proxylog-*\n2015-08-06 08:39:50,371 DEBUG          curator.cli.utils        filter_callback:182  Added filter: {'pattern': 'proxylog-*'}\n2015-08-06 08:39:50,371 DEBUG          curator.cli.utils        filter_callback:183  New list of filters: [{'pattern': 'proxylog-*'}]\n2015-08-06 08:39:50,371 DEBUG         curator.api.filter         get_date_regex:158  regex = \\d{4}\\-\\d{2}\\-\\d{2}\n2015-08-06 08:39:50,371 DEBUG          curator.cli.utils        filter_callback:179  REGEX = (?P<date>\\d{4}\\-\\d{2}\\-\\d{2})\n2015-08-06 08:39:50,372 DEBUG          curator.cli.utils        filter_callback:182  Added filter: {'pattern': '(?P<date>\\\\d{4}\\\\-\\\\d{2}\\\\-\\\\d{2})', 'value': 7, 'groupname': 'date', 'time_unit': 'days', 'timestring': u'%Y-%m-%d', 'method': 'older_than'}\n2015-08-06 08:39:50,372 DEBUG          curator.cli.utils        filter_callback:183  New list of filters: [{'pattern': 'proxylog-*'}, {'pattern': '(?P<date>\\\\d{4}\\\\-\\\\d{2}\\\\-\\\\d{2})', 'value': 7, 'groupname': 'date', 'time_unit': 'days', 'timestring': u'%Y-%m-%d', 'method': 'older_than'}]\n2015-08-06 08:39:50,372 INFO      curator.cli.index_selection                indices:53   Job starting: close indices\n2015-08-06 08:39:50,372 DEBUG     curator.cli.index_selection                indices:56   Params: {'url_prefix': u'', 'http_auth': None, 'dry_run': False, 'loglevel': u'INFO', 'logformat': u'default', 'host': u'calhda05.cyber.lab', 'timeout': 30, 'debug': True, 'use_ssl': False, 'logfile': None, 'master_only': False, 'port': 9200}\n2015-08-06 08:39:50,372 DEBUG          curator.cli.utils             get_client:110  kwargs = {'url_prefix': u'', 'http_auth': None, 'dry_run': False, 'loglevel': u'INFO', 'host': u'calhda05.cyber.lab', 'logformat': u'default', 'timeout': 30, 'debug': True, 'use_ssl': False, 'logfile': None, 'master_only': False, 'port': 9200}\n2015-08-06 08:39:50,372 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-08-06 08:39:50,372 INFO      urllib3.connectionpool              _new_conn:205  Starting new HTTP connection (1): calhda05.cyber.lab\n2015-08-06 08:40:20,474 WARNING            elasticsearch       log_request_fail:82   GET http://calhda05.cyber.lab:9200/ [status:N/A request:30.102s]\nTraceback (most recent call last):\nFile \"/usr/lib/python2.6/site-packages/elasticsearch/connection/http_urllib3.py\", line 74, in perform_request\n    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)\nFile \"/usr/lib/python2.6/site-packages/urllib3/connectionpool.py\", line 607, in urlopen\n    _stacktrace=sys.exc_info()[2])\nFile \"/usr/lib/python2.6/site-packages/urllib3/util/retry.py\", line 222, in increment\n    raise six.reraise(type(error), error, _stacktrace)\nFile \"/usr/lib/python2.6/site-packages/urllib3/connectionpool.py\", line 557, in urlopen\n    body=body, headers=headers)\nFile \"/usr/lib/python2.6/site-packages/urllib3/connectionpool.py\", line 378, in _make_request\n    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\nFile \"/usr/lib/python2.6/site-packages/urllib3/connectionpool.py\", line 306, in _raise_timeout\nraise ReadTimeoutError(self, url, \"Read timed out. (read timeout=%s)\" % timeout_value)\nReadTimeoutError: HTTPConnectionPool(host=u'calhda05.cyber.lab', port=9200): Read timed out. (read timeout=30)\n2015-08-06 08:40:20,566 DEBUG              elasticsearch       log_request_fail:90   > None\nERROR: Connection failure.\nThen my whole cluster fell over (everynode). Nothing in the ES logs to indicate why...\n. Ignore that previous post I think this is an error from the cluster being down and me trying the command again.\nAfter getting the cluster back up I reran the command and got this:\n```\n2015-08-06 09:31:23,962 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-08-06 09:31:23,963 DEBUG     urllib3.connectionpool          _make_request:385  \"GET /_cluster/state/metadata/proxylog-2015-07-30 HTTP/1.1\" 200 3316\n2015-08-06 09:31:23,963 INFO               elasticsearch    log_request_success:63   GET http://calhda05.cyber.lab:9200/_cluster/state/metadata/proxylog-2015-07-30 [status:200 request:0.002s]\n2015-08-06 09:31:23,963 DEBUG              elasticsearch    log_request_success:65   > None\n2015-08-06 09:31:23,964 DEBUG              elasticsearch    log_request_success:66   < {\"cluster_name\":\"ucp-incubator\",\"metadata\":{\"templates\":{},\"indices\":{\"proxylog-2015-07-30\":{\"state\":\"open\",\"settings\":{\"index\":{\"creation_date\":\"1438243685613\",\"uuid\":\"2S-Ad4GXSBG2SuETgvXwaw\",\"number_of_replicas\":\"1\",\"number_of_shards\":\"8\",\"version\":{\"created\":\"1060099\"}}},\"mappings\":{\"event\":{\"properties\":{\"DeviceRequestURL\":{\"type\":\"string\",\"fields\":{\"raw\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"DeviceAction\":{\"type\":\"string\",\"fields\":{\"raw\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"SourceReferer\":{\"type\":\"string\",\"fields\":{\"raw\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"SourceCriticalHost\":{\"type\":\"boolean\"},\"SourceHostName\":{\"type\":\"string\",\"fields\":{\"raw\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"SourceUserAgent\":{\"type\":\"string\",\"fields\":{\"raw\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"SourceLoggedOnUserCriticalUser\":{\"type\":\"boolean\"},\"@ImportDateUTC\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"SourcePort\":{\"type\":\"long\"},\"DeviceAddressIPv4\":{\"type\":\"string\",\"fields\":{\"raw\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"SourceMethod\":{\"type\":\"string\",\"fields\":{\"raw\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"SourceURLQuery\":{\"type\":\"string\",\"fields\":{\"raw\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"FullURL\":{\"type\":\"string\",\"fields\":{\"raw\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"VirusId\":{\"type\":\"string\",\"fields\":{\"raw\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"DeviceContentType\":{\"type\":\"string\",\"fields\":{\"raw\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"SourceGroup\":{\"type\":\"string\",\"fields\":{\"raw\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"StartTime\":{\"type\":\"string\",\"fields\":{\"raw\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"SourceURLPath\":{\"type\":\"string\",\"fields\":{\"raw\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"BatchNumber\":{\"type\":\"string\",\"fields\":{\"raw\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"SourceRequestURL\":{\"type\":\"string\",\"fields\":{\"raw\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"ImportDate\":{\"type\":\"string\",\"fields\":{\"raw\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"TimeTaken\":{\"type\":\"long\"},\"BytesOut\":{\"type\":\"long\"},\"StartTimeUTC\":{\"type\":\"string\",\"fields\":{\"raw\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"DeviceFilter\":{\"type\":\"string\",\"fields\":{\"raw\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"ExceptionId\":{\"type\":\"string\",\"fields\":{\"raw\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"@ImportDate\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"Source\":{\"type\":\"string\",\"fields\":{\"raw\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"Category\":{\"type\":\"string\",\"fields\":{\"raw\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"SourceProtocol\":{\"type\":\"string\",\"fields\":{\"raw\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"FileName\":{\"type\":\"string\",\"fields\":{\"raw\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"@StartTime\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"ImportDateUTC\":{\"type\":\"string\",\"fields\":{\"raw\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"SourceUser\":{\"type\":\"string\",\"fields\":{\"raw\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"@StartTimeUTC\":{\"format\":\"dateOptionalTime\",\"type\":\"date\"},\"BytesIn\":{\"type\":\"long\"},\"DeviceStatus\":{\"type\":\"string\",\"fields\":{\"raw\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}},\"SourceAddressIPv4\":{\"type\":\"string\",\"fields\":{\"raw\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}}}}},\"aliases\":[]}}}}\n2015-08-06 09:31:23,971 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-08-06 09:31:23,971 DEBUG     urllib3.connectionpool          _make_request:385  \"GET / HTTP/1.1\" 200 336\n2015-08-06 09:31:23,971 INFO               elasticsearch    log_request_success:63   GET http://calhda05.cyber.lab:9200/ [status:200 request:0.001s]\n2015-08-06 09:31:23,972 DEBUG              elasticsearch    log_request_success:65   > None\n2015-08-06 09:31:23,972 DEBUG              elasticsearch    log_request_success:66   < {\n\"status\" : 200,\n\"name\" : \"calhda05\",\n\"cluster_name\" : \"ucp-incubator\",\n\"version\" : {\n    \"number\" : \"1.7.1\",\n    \"build_hash\" : \"b88f43fc40b0bcd7f173a1f9ee2e97816de80b19\",\n    \"build_timestamp\" : \"2015-07-29T09:54:16Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"4.10.4\"\n},\n\"tagline\" : \"You Know, for Search\"\n}\n2015-08-06 09:31:23,972 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-08-06 09:31:44,883 DEBUG     urllib3.connectionpool          _make_request:385  \"POST /proxylog-2015-07-29,proxylog-2015-07-30/_flush/synced HTTP/1.1\" 409 504\n2015-08-06 09:31:44,884 WARNING            elasticsearch       log_request_fail:82   POST /proxylog-2015-07-29,proxylog-2015-07-30/_flush/synced [status:409 request:20.911s]\n2015-08-06 09:31:44,884 DEBUG              elasticsearch       log_request_fail:90   > None\n2015-08-06 09:31:44,884 WARNING         curator.api.seal           seal_indices:30   Non-fatal error encountered.\n2015-08-06 09:31:44,885 DEBUG           curator.api.seal           seal_indices:32   Error: 409.  Message: {\"_shards\":{\"total\":48,\"successful\":38,\"failed\":10},\"proxylog-2015-07-30\":{\"total\":16,\"successful\":6,\"failed\":10,\"failures\":[{\"shard\":3,\"reason\":\"all shards failed to commit on pre-sync\"},{\"shard\":2,\"reason\":\"all shards failed to commit on pre-sync\"},{\"shard\":1,\"reason\":\"all shards failed to commit on pre-sync\"},{\"shard\":5,\"reason\":\"all shards failed to commit on pre-sync\"},{\"shard\":0,\"reason\":\"all shards failed to commit on pre-sync\"}]},\"proxylog-2015-07-29\":{\"total\":32,\"successful\":32,\"failed\":0}}\n2015-08-06 09:31:44,885 WARNING         curator.api.seal           seal_indices:39   1 indices failed to seal (synced flush):\n2015-08-06 09:31:44,885 ERROR           curator.api.seal           seal_indices:44   proxylog-2015-07-30: set([u'all shards failed to commit on pre-sync'])\n2015-08-06 09:31:44,885 INFO            curator.api.seal           seal_indices:47   All other indices provided have been successfully sealed. (Shown with --debug flag enabled.)\n2015-08-06 09:31:44,885 DEBUG           curator.api.seal           seal_indices:51   Successfully sealed indices: [u'proxylog-2015-07-29']\n2015-08-06 09:31:44,885 DEBUG         urllib3.util.retry               from_int:155  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2015-08-06 09:32:08,770 DEBUG     urllib3.connectionpool          _make_request:385  \"POST /proxylog-2015-07-29,proxylog-2015-07-30/_close?ignore_unavailable=true HTTP/1.1\" 200 21\n2015-08-06 09:32:08,771 INFO               elasticsearch    log_request_success:63   POST http://calhda05.cyber.lab:9200/proxylog-2015-07-29,proxylog-2015-07-30/_close?ignore_unavailable=true [status:200 request:23.885s]\n2015-08-06 09:32:08,771 DEBUG              elasticsearch    log_request_success:65   > None\n2015-08-06 09:32:08,771 DEBUG              elasticsearch    log_request_success:66   < {\"acknowledged\":true}\n2015-08-06 09:32:08,771 INFO           curator.cli.utils               exit_msg:67   Job completed successfully.\n```\nBut the job did complete and the indices are closed\n. ",
    "imreFitos": "I have an alternate solution for this: specify sorting on character position of index name: https://github.com/elastic/curator/pull/517\n. hey thank you for the prompt comment!\nCould you describe your scenario in more detail? Would your date be retrieved from the index names, or from somewhere else? What are your indexes named? How would you like the arguments to look like? \n. So where would you get the dates from? the index names or from some other source? And what would you like the new flag to be called?\n. ",
    "nik-petrov": "I think \"no match\" is kind of a soft error. For instances in case of curator delete it's usually fine to ignore it. Probably use specific exit code for \"no match\" errors.\n. ",
    "rstruber": "I did.\n. ",
    "sandwormusmc": "Why not add the dependency instead of rely on a workaround as a solution?\n. That makes sense.  I ran into this on a few CentOS 6.7 systems, so not terribly ancient in my case.\n. I see.  Good stuff, thanks for being so responsive.\n. ",
    "MichaelLiZhou": "Sorry not an issue. Was testing on wrong elasticsearch cluster ... my mistake its running well :) \n. ",
    "orrchen": "I think trying to cast is very welcome, but then we will need to throw the readable exception when we weren't able to do the casting right?\n. Yes - that sounds good :)\n. ",
    "abo-adapt": "Minor syntax error in previous patch. Use this instead: \n--- a/modules/curator/manifests/job.pp\n+++ b/modules/curator/manifests/job.pp\n@@ -81,7 +81,11 @@ define curator::job (\n$_regex = undef\n}\n- $_timestring = \"--timestring '${timestring}'\"\n- if ($command == \"snapshot\") {\n- $_timestring = undef\n- } else {\n- $_timestring = \"--timestring '${timestring}'\"\n- }\n. You are absolutely right. I meant to report this issue to evenup-curator, not directly to the curator project.\nAs to it just being a 10 second delay with the warning, I agree that it shouldn't prevent cron from running it, but at least on our servers, it did not run until I removed --timestring from the cronjob. Could it be because there is no interactive tty when cron is running it?\n. ",
    "GlenRSmith": "Oops. Sorry. I should have closed this.\n. I did a pip install into a new ve (2.7), and got 5.0.1 by default, with no errors, FWIW.. ",
    "rodriguezsergio": "Oh yeah... I should have mentioned that I created those snapshots just using the good ol' command line.\nFeel free to close this.\n. Looks like that did it. Thanks. :+1: \n```\n$ time curator --timeout 300 delete snapshots --older-than 57 --time-unit days --repository $S3bucket --timestring \"%Y_%m_%d\"\n2015-09-11 15:17:07,607 INFO      Job starting: delete snapshots\n2015-09-11 15:17:07,880 INFO      Deleting snapshot 2015_07_16\n2015-09-11 15:19:43,612 INFO      Job completed successfully.\nreal    2m36.367s\nuser    0m0.188s\nsys 0m0.088s\n```\n. ",
    "imreACTmd": "FYI: elasticsearch-py==1.7 is now out and Honza Kral says it's 2.0 compatible: https://github.com/elastic/elasticsearch-py/issues/271\n. Yaaay!\n. ",
    "markwalkom": "Also ran into this with a customer when trying to clean up some Marvel indices in a found cluster, replicated from my laptop on 3.0.3;\n$ curator --host CLUSTERID.found.io --port 9243 --use_ssl show indices --older-than 5 --time-unit days --timestring test-%Y.%m.%d\n2015-10-07 18:20:27,387 INFO      Job starting: show indices\n/Library/Python/2.7/site-packages/elasticsearch/connection/http_urllib3.py:50: UserWarning: Connecting to CLUSTERID.found.io using SSL with verify_certs=False is insecure.\n  'Connecting to %s using SSL with verify_certs=False is insecure.' % host)\n/Library/Python/2.7/site-packages/urllib3/connectionpool.py:768: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html\n  InsecureRequestWarning)\n/Library/Python/2.7/site-packages/urllib3/connectionpool.py:768: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html\n  InsecureRequestWarning)\n2015-10-07 18:20:28,196 WARNING   No indices matched provided args: {'regex': None, 'index': (), 'suffix': None, 'newer_than': None, 'prefix': None, 'time_unit': 'days', 'timestring': u'test-%Y.%m.%d', 'exclude': (), 'older_than': 5, 'all_indices': False}\nNo indices matched provided args.\nHowever if I check the HTTPS URL via browser or with openssl then it's fine.\n. https://gist.github.com/markwalkom/8a7201e3f6ea4354ae06 is another example of using LS for this\n. ",
    "xh3b4sd": "I had the same issue and fixed it with the following.\npip install urllib3[secure]\n. ",
    "mcuelenaere": "Ok, then I'll create a script to perform this task.\n. ",
    "Basster": "Sounds good. I've updated the code accordingly.\n. done and thank you for your feedback.\n. ",
    "univerio": "That's unfortunate. What do you think a workaround would look like? A command-line flag that turns on AWS ES-specific behavior? Or trying to automatically detect that the ES instance is from AWS?\nEDIT: It looks like there's a relatively simple way to detect that the cluster is an AWS ES domain. The 401 response for /_cluster/state/metadata contains the header x-amzn-requestid, which appears to be AWS-specific. Perhaps in the index_closed function we can look for this header, and, if it exists, assume False. This is a slightly fragile way of detecting AWS, but false positives are highly unlikely, and false negatives just falls back to existing behavior, so I think it should be okay. What do you think?\n. Great, I'll prepare a pull request.\nSide note, I couldn't find a way to make /_cat/indices/* output JSON, but using ?h=status works just as well.\n. ",
    "ulsa": "Great, guys. Looking forward to trying this when it appears.\n. ",
    "jgoodall": "Will both continue to work?\n. ",
    "cameronkerrnz": "After upgrading curator to version 3.4.0 yesterday and adding --quiet, I still get email from cron (although I don't get output when running the same script from the command-line).\nThis is the email that gets sent:\n```\n/etc/cron.hourly/elk-purge-old-logs:\nNo indices matched provided args: {'regex': '^nxlog', 'index': (), 'suffix': None, 'newer_than': None, 'closed_only': False, 'prefix': None, 'time_unit': 'days', 'timestring': '%Y.%m.%d', 'exclude': (), 'older_than': 8, 'all_indices': False}\n```\nThis is my script:\n```\ncat /etc/cron.hourly/elk-purge-old-logs\n!/bin/bash\ncurator=\"curator --quiet --loglevel ERROR\"\ndays=\"--time-unit days --timestring %Y.%m.%d\"\nbasename_X_replicated_Ndays_deleted_after_Mdays()\n{\n  $curator replicas --count 0 indices --regex \"^${1}\" --older-than \"${2}\" $days\n  $curator delete indices --regex \"^${1}\" --older-than \"${3}\" $days\n}\n$curator show indices --timestring %Y.%m.%d | sed -e 's/-..........//' | sort -u | while read index_basename\ndo\n  case \"${index_basename}\" in\n    active_directory)\n      basename_X_replicated_Ndays_deleted_after_Mdays \"${index_basename}\" 4 8\n      ;;\n    exchange)\n      basename_X_replicated_Ndays_deleted_after_Mdays \"${index_basename}\" 4 8\n      ;;\n    *)\n      basename_X_replicated_Ndays_deleted_after_Mdays \"${index_basename}\" 7 31\n      ;;\n  esac\ndone\nexit 0\n```\nand this is what happens when I run it from the command-line:\n```\n/etc/cron.hourly/elk-purge-old-logs\n(ie. no output)\n```\n. Thanks for the clarification, although I have some feedback about that.\nIf there is a reasonable use-case for making \"No indices matched provided args...\" an ERROR level message, then I think that behaviour should be configurable. As an idempotent operation (ie. we're promising to delete indices older than some criteria), the fact that there were no such indices found in this particular iteration isn't an error. Perhaps the log-level of that message ought to be an INFO-level message. Otherwise, I either generate useless logs, or have to filter out this particular message so it doesn't generate useless cron mail (which is worse because then it tends to get filtered and ignored).\nI still don't understand though, why the log output is generated when run under cron, but not when run manually. Running it with stdin being /dev/null doesn't reproduce that same behaviour.\nI have now updated my script to be:\n```\n!/bin/bash\ndays=\"--time-unit days --timestring %Y.%m.%d\"\ncurator=\"/usr/local/bin/curator --quiet --loglevel ERROR\"\ncurator_wrapper() {\n  $curator \"$@\" 2>&1 | grep -v -e '^No indices matched provided args'\n}\nbasename_X_replicated_Ndays_deleted_after_Mdays()\n{\n  curator_wrapper replicas --count 0 indices --regex \"^${1}\" --older-than \"${2}\" $days\n  curator_wrapper delete indices --regex \"^${1}\" --older-than \"${3}\" $days\n}\n$curator show indices --timestring %Y.%m.%d | sed -e 's/-..........//' | sort -u | while read index_basename\ndo\n  case \"${index_basename}\" in\n    active_directory)\n      basename_X_replicated_Ndays_deleted_after_Mdays \"${index_basename}\" 4 8\n      ;;\n    exchange)\n      basename_X_replicated_Ndays_deleted_after_Mdays \"${index_basename}\" 4 8\n      ;;\n    *)\n      basename_X_replicated_Ndays_deleted_after_Mdays \"${index_basename}\" 7 31\n      ;;\n  esac\ndone\nexit 0\n```\nThis seems to work as expected.\nThanks for your work on curator.\nCheers,\nCameron\n. ",
    "mizeng": "@untergeek Thanks a lot for so detailed explanation.\nI just checked, we're using \"elasticsearch-curator==2.0.2\" in \"elasticsearch-1.5.2.deb\". Seems need to upgrade elasticsearch-curator version.\n. ",
    "bgagnon": "+1\nHere is my (very) similar use case: I am a naive ELK user, so I start small and keep all defaults. This gives me one Elasticsearch index per day of logging data, which is reasonable. I get more and more data sources, which in turn yields more and more indices.\nThree months later, I realize I have hundreds of indices with very little data in any of them, though a reasonable volume if I look at the sum. I now want to change the logstash config and use an index per month instead of an index per day. With Curator I could back-merge existing indices into month-based aggregates, if only this feature existed.\nAs for the memory / storage requirements, I don't think it's a big issue: merge indices one at a time, deleting them as soon as they have been merged. At worst, it would be equivalent to storing one extra day of data.\n. ",
    "tuckerpm": "+1 for bgagnon's usecase\n. ",
    "referup-tarantegui": "I've tested @untergeek's solution. But the monthly index has less log entries (a lot less) than the sum of all daily indexes of the same month.. Hi @untergeek I've used Logstash 5.1.1 with Elasticsearch 2.4. @untergeek The merge process finishes ok, nothing on the logstash logs. At the end one big index with a lot less entries. From 1 billion entries (30 indexes) to 100 million entries (1 index). At the moment I didn't apply anything because the results are not acceptable in production.. I've applied your snippet:\n```\ninput {\n  elasticsearch {\n    # ... elasticsearch connection args here...\n    index => \"logstash-2015.11.*\"\n  }\n}\nfilter {}\noutput {\n  elasticsearch {\n    # ... elasticsearch connection args here...\n    # Make Logstash re-index them to monthly indices with this addition\n    index => \"logstash-%{+YYYY.MM}\"\n  }\n}\n```\nThe objective was to squash all month daily indexes into one monthly index. I've tested to to merge 7 daily indexes into a 1 weekly index, but there is data loss too.\nThe alias solution not applies because I want to reduce the number of old indexes.\nI didn't tested the reindex API yet.. The index solution doesn't fit for me now, because I want to know if the heap memory use is more efficient with one big index versus several indexes. That's the reason because I'm testing the merge index solution and I need to do this laboratory tests before production. . ",
    "temporafugiunt": "Little late to the party... But...\nWe needed to consolidate daily indexes into monthly indexes in an easy command line based way, and we are a .NET shop, so we created a command line application and a nuget package for the client library DLL for this purpose. The source is available here and can be compiled in any Visual Studio 2017 version:\nhttps://github.com/trackitsvalue/indexify\nCheers!. Little late to the party... But...\nWe needed to consolidate daily indexes into monthly indexes in an easy command line based way, and we are a .NET shop, so we created a command line application and a nuget package for the client library DLL for this purpose. The source is available here and can be compiled in any Visual Studio 2017 version:\nhttps://github.com/trackitsvalue/indexify\nWe love curator, but it was actually much more than what was necessary to perform this specific function so we created this specialized tool for this task. \nI realize this isn't exactly what you are asking for, but pull requests and additions are always welcome and I am sure with just a little bit more coding to the command line it could do exactly what you are asking for. In the future we may add it ourselves but for now going from daily to monthly was what we were looking for.\nCheers!. ",
    "bemeyert": "BTW: Currently I install with yum install --nogpgcheck -y python-elasticsearch-curator. That makes me quite uncomfortable.\n. @untergeek y It works. After a yum clean metadata the installation went smoothly.\nThanks a lot and closed ;)\n. ",
    "aochsner": "Thanks for the followup!  I'll likely just add a timeunit for the short term, but will try to dust off my Python skills and shoot you a PR.\n. ",
    "Geek2France": "I live in France (timezone GMT+1).\nI want use curator to create weekly index mondays at midnight (when the week begins).\nCurator fails because it considers the week is not yet finished and tells the index already exists.\nCurator is used with crontab that use local time. So, why do you use UTC time ?\nI would like understand. May be i misunderstood something.. ",
    "richm": "@nellicus What does curl -v ... say?\n. +1\n. @msimos I don't understand.  Isn't that what the proposed commit does?\n. We are recommending our users (of OpenShift aggregated logging) not to use months, but instead to use --time-unit days --older-than $((Ndays * 30)) which should suffice.\n. I have signed the CLA\n. the CI tests are failing - I don't know why - is this some sort of transient error?  If so, how can I trigger another run?  If not, what's wrong?\n. @untergeek yes, that's probably it - so how do I tell the test to use the version of elasticsearch-py from https://github.com/elastic/elasticsearch-py/issues/344 ?\n. @untergeek whoops, sorry about that.  should be good now.\n. ",
    "dudlo": "Sorry, I think this is a big hole in the feature set. I like Curator and planned on using it, rather than writing custom Bash script that shoots curl commands. I like alias action. However, the lack of support for time-based index creation defeats the purpose of using Curator.\nI do not use logstash but I want to follow the pattern of daily index, for example my_index-2017-03-22. I want to run cron-triggered curator script after midnight that will:\n- create new index\n- run alias to update my \"current\", \"last week\",... aliases\nAlias action does it nicely. But create_index cannot create date-based index name.\nPlease consider adding this feature.. ",
    "sundarv85": "We are currently having a lot of hacks (by write one day in the future and then running the curator for getting the alias in place), to handle this situation.\nIf curator can support index creation, it will be very useful. > Using the Elasticsearch stats_api to look at the youngest and oldest documents in an index, based on a timestamp field.\nThe stats_api method is only available from curator 4.0. Is that correct? Or will it work with curator 3.5.1 also.\n\nI hope to have a beta version of 4.0 out in the next few weeks\n\nDo you already have a date in mind when the first curator 4.0 will be out.\n. I think I found the problem, after enabling logging\n2017-01-02 16:37:29,498 DEBUG              curator.utils         get_date_regex:138  regex = \\d{4}_\\d{2}\nSo when my index is data_2017_01, then it works as the regex is looking for 2 digit week. 01 as opposed to 1. \nShould this be fixed from my side or in curator?\n. Got it. I will do so. . ",
    "joshuar": "Done.\n. ",
    "wespday": "Yes, I've tried on a few boxes including a fresh VM now with the same result.\n. LGTM.  Thanks a bunch @untergeek !\n. ",
    "hwwin": "Hi, I'm facing the same problem with Curator version 3.4.0. The patched build doesn't solve the problem. Tried it on different machines without any python installation present. The trace remains unchanged.\nThe Curator version 3.3.0. will start on the very same machines but stop due to incompatibility with elasticsearch 2.1.0\n. Tried it on Windows 7 Enterprise 64bit and Windows Server 2008 R2 Standard 64bit with SP1 installed. The error is identical in both instances.\n. LGTM. Thanks @untergeek !\n. ",
    "bwinterseg": "I'm having the same problem.  Windows server 2012 X64, no Python installed.\nC:\\temp\\curator-3.4.0>curator --host 127.0.0.1 delete indices --older-than 30 --time-unit days timestring '%Y.%m.%d'\nTraceback (most recent call last):\n  File \"C:\\temp\\curator-3.4.0\\run_curator.py\", line 5, in \n  File \"C:\\temp\\curator-3.4.0\\curator__init__.py\", line 2, in curator\n  File \"C:\\temp\\curator-3.4.0\\curator\\api__init__.py\", line 1, in api\n  File \"C:\\temp\\curator-3.4.0\\curator\\api\\utils.py\", line 2, in utils\n  File \"C:\\temp\\curator-3.4.0\\elasticsearch__init__.py\", line 17, in elasticsearch\n  File \"C:\\temp\\curator-3.4.0\\elasticsearch\\client__init__.py\", line 5, in client\n  File \"C:\\temp\\curator-3.4.0\\elasticsearch\\transport.py\", line 5, in transport\n  File \"C:\\temp\\curator-3.4.0\\elasticsearch\\connection__init__.py\", line 2, in connection\n  File \"C:\\temp\\curator-3.4.0\\elasticsearch\\connection\\http_requests.py\", line 11, in http_requests\n  File \"C:\\temp\\curator-3.4.0\\elasticsearch\\compat.py\", line 7, in compat\n  File \"C:\\Python27\\lib\\urllib.py\", line 26, in \n  File \"C:\\Python27\\lib\\socket.py\", line 47, in \nImportError: No module named _socket\n. ",
    "gschuager": "Same problem here on Win 2012 x64 without Python\n. LGTM. Thanks!\n. ",
    "samjudson": "Same here. Windows Server 2012 R2, no Python, Curator 3.4.0.\n. I tried downloading version 3.3.0 and it contained a selection of .pyd files, including _socket.pyd.\nI copied these PYD files to the 3.4.0 directory and I can now run curator. However I get a connection failure error when trying to run it, even when running on the same machine as elastic (default port etc).\nRunning the actual 3.3.0 files works (against 1.7.3, but fails against 2.1).\n. This now works against 1.7.3. and 2.1. I did notice however that the \"--quiet\" option doesn't seem to do anything?\n. @untergeek Ah thanks, missed that bit in the docs. A LGTM here too then.\n. ",
    "dparkar": "@untergeek There seems to be a small bug in the fixed version when you use --dry-run and if there is a timeout from elasticsearch :\n2016-01-11 18:27:40,105 INFO      DRY RUN: delete: some-index-name-2015.12.13\nTraceback (most recent call last):\n  File \"C:\\Python27\\lib\\site-packages\\cx_Freeze\\initscripts\\Console.py\", line 27, in \n  File \"run_curator.py\", line 8, in \n  File \"curator\\curator.py\", line 5, in main\n  File \"C:\\Python27\\lib\\site-packages\\click\\core.py\", line 716, in call\n  File \"C:\\Python27\\lib\\site-packages\\click\\core.py\", line 696, in main\n  File \"C:\\Python27\\lib\\site-packages\\click\\core.py\", line 1060, in invoke\n  File \"C:\\Python27\\lib\\site-packages\\click\\core.py\", line 1060, in invoke\n  File \"C:\\Python27\\lib\\site-packages\\click\\core.py\", line 889, in invoke\n  File \"C:\\Python27\\lib\\site-packages\\click\\core.py\", line 534, in invoke\n  File \"C:\\Python27\\lib\\site-packages\\click\\decorators.py\", line 17, in new_func\n  File \"curator\\cli\\index_selection.py\", line 152, in indices\n  File \"curator\\cli\\utils.py\", line 79, in show_dry_run\n  File \"curator\\api\\utils.py\", line 96, in index_closed\n  File \"C:\\Python27\\lib\\site-packages\\elasticsearch\\client\\utils.py\", line 69, in _wrapped\n  File \"C:\\Python27\\lib\\site-packages\\elasticsearch\\client\\cat.py\", line 119, in indices\n  File \"C:\\Python27\\lib\\site-packages\\elasticsearch\\transport.py\", line 307, in perform_request\n  File \"C:\\Python27\\lib\\site-packages\\elasticsearch\\connection\\http_urllib3.py\", line 86, in perform_request\nelasticsearch.exceptions.ConnectionTimeout: ConnectionTimeout caused by - ReadTimeoutError(HTTPConnectionPool(host=u'somehost', port=81): Read timed out. (read timeout=30))\nAlso, does dry-run behave slightly different than actual run ? I noticed when deleting a lot of indices, in actual run batching is done, where as in dry-run its showing as if its done one by one.\n. @untergeek Thanks for the confirmation. The cluster issue was expected as I was going through a deployment. I assumed that dry-run follows exactly the same steps as the real run except actually modifying the cluster.\n. ",
    "styfle": "I downloaded version 3.4.0 from the installation guide link today and got the same ImportError: No module named _socket as the OP.\nHowever the Dropbox link in issuecomment-168836720 seems to work for me. LGTM :+1: \n@untergeek Should the installation link on the elastic website be updated?\n. @untergeek Thanks!\nAny reason why the link was changed from win64 to win32?\n. ",
    "MonDeveloper": "Same problem here Win Srv 2012 R2 - No Pyton.\nDropbox link seems broken tonight from Italy\n. ",
    "ynux": "A feature to manage templates would help us, too. Keeping configuration like this in a datastore has implications for the way people manage it. Generally, we like our configuration versioned in git and deployed by puppet. This works great for files, but not for configuration kept in indices - like templates, kibana objects, watches. We have some ugly shell scripts as a work around, but curator would be much better. \nI'll start working on it, but I'm not experienced, so please be patient with me.\n. Thanks for the warning. I'll rewrite our clumsy scripts to something a bit nicer then and see if people like it. Template management is quite different from the other curator features.\n. Thanks for your answer, and you are right, I'm still using curator 3. I installed curator 4 and will test the https://www.elastic.co/guide/en/elasticsearch/client/curator/4.0/option_delete_aliases.html option tomorrow. I think it is what I'm looking for, though I wouldn't call it \"aliases will be deleted from indices\" but \"index will be removed from aliases\". \n. The delete_aliases option works perfectly for me. Thanks.\nI'm also happy that we now moved to curator 4. The curator 3 command lines were getting difficult to handle. Putting the central configuration into a CONFIG.YML removes a lot of duplication.\nA third praise: It's good that you consider the confusion of new users, and at the same time strive to keep the wording technically correct.\n. Yes, well, I admit that I was thinking more of a \"protect elasticsearch internal indices\"-filter. OK then, let people add the filter block, thanks for providing it. It was nice of Curator 3 to protect .kibana.\n. One small remark: The above filter block would also exclude the .watch_history-YYYY.MM.DD indices, which you may actually want to close and delete. To exclude \".watches\" and also \".triggered_watches\" (which is empty most of the time, I guess some kind of work index), we're using \nkind: suffix\nvalue: watches\n. ",
    "diogouchoas": "Thank you guys for your help.\nI guess I had some problem interpreting the documentation.\n. ",
    "jnjackins": "Sorry, the above code snippet is from the index_closed function in api/utils.py.\n. ",
    "msimos": "In def get_client(**kwargs): if you add kwargs['client_cert'] = \"/path/to/cert.pem\" then you can add in certificate based authentication. It seems fairly easy to add another parameter to add this in.\n. @richm sorry i didn't see that, thats great\n. ",
    "idsvandermolen": "This also increased required version for elastichsearch-py from 1.8.0 to 2.3.0 which might not be documented anywhere. So it also won't work with elasticsearch 1.x anymore.\n. ok, but I think I'm confused with the different major numbers. I thought elasticsearch-py 1.x was still required for ES 1.x and that elasticsearch-py 2.x didn't work for ES 1.x .\n. @untergeek Thanks for your explanation, this is clear now.\n. ",
    "vvanholl": "It seems there is a package dependency error during the CI check:\npkg_resources.VersionConflict: (mock 1.3.0 (/home/travis/virtualenv/python2.7.9/lib/python2.7/site-packages), Requirement.parse('mock==1.0.1'))\nCould you help me please ?\n. OK, this means that my pull request will be still examined ?\n. Hi,\nI think this feature could be useful for instance when you need to make some cleanup on your disk by deleting the oldest indices first. \nVincent\n. ",
    "rhoml": "Yes and no. \nWhat I want to accomplish here is to optimize all indexes older than 1 day but actually older-than 1 means 2 days older which is not the desired behaviour I think.\n. sorry I confused I meant +1 week so if my weekly indices is named logstash-2015.40 and logstash-2015.41 I would like to be able to optimize the first one.\n. I am afraid it doesn't respect the newer-than 1040 =/ it keeps optimizing all of my indexes.\n. in our case is logstash_environment_service-%Y.%W for example\n. Hey @untergeek, Actually no. \nThe delay seems not to be working properly on non-optimized indices. Here is the command that I am triggering and the result\n/usr/local/bin/curator optimize --delay 120 --max_num_segments 1 indices --older-than 1 --time-unit days --timestring \\%Y.\\%m.\\%d\n2016-01-05 22:47:20,960 INFO      Job starting: optimize indices\n2016-01-05 22:47:20,960 WARNING   Overriding default connection timeout.  New timeout: 21600\n2016-01-05 22:47:21,030 INFO      Action optimize will be performed on the following indices: [u'REDACTED-2015.12.17', u'REDACTED-2015.12.18', u'REDACTED-2015.12.19',................................................................................................................................................................]\n2016-01-05 22:47:21,030 WARNING   Very large list of indices.  Breaking it up into smaller chunks.\n2016-01-05 22:49:43,621 INFO      Optimizing index REDACTED-2016.01.04 to 1 segments per shard.  Please wait...\n2016-01-05 22:52:35,466 INFO      Optimizing index REDACTED-2016.01.04 to 1 segments per shard.  Please wait...\nThose where indices under optimization and the delay didn't seem to be working properly. \n~# curator --version\ncurator, version 3.4.0\nUnless I am missing something? :(\n. Can we reopen this issue? I think it is still a thing.\n. command\ncurator --debug optimize --delay 120 --max_num_segments 1 indices --older-than 1 --time-unit days --timestring \\%Y.\\%m.\\%d\nresult \n```\n2016-01-10 08:33:40,297 DEBUG          curator.api.utils              optimized:169  Index -2016.01.09 has 24 shards and 350 segments total.\n2016-01-10 08:33:40,297 DEBUG          curator.api.utils              optimized:171  Flagging index -2016.01.09 for optimization.\n2016-01-10 08:33:40,297 INFO        curator.api.optimize         optimize_index:22   Optimizing index -2016.01.09 to 1 segments per shard.  Please wait...\n2016-01-10 08:33:40,297 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-10 08:34:17,430 DEBUG     urllib3.connectionpool          _make_request:386  \"POST /-2016.01.09/_optimize?max_num_segments=1 HTTP/1.1\" 200 51\n2016-01-10 08:34:17,430 INFO               elasticsearch    log_request_success:63   POST http://localhost:9200/-2016.01.09/_optimize?max_num_segments=1 [status:200 request:37.133s]\n2016-01-10 08:34:17,430 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-10 08:34:17,430 DEBUG              elasticsearch    log_request_success:66   < {\"_shards\":{\"total\":24,\"successful\":24,\"failed\":0}}\n2016-01-10 08:36:17,521 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-10 08:36:17,522 DEBUG     urllib3.connectionpool          _make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-10 08:36:17,522 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.001s]\n2016-01-10 08:36:17,522 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-10 08:36:17,522 DEBUG              elasticsearch    log_request_success:66   < {\n  \"status\" : 200,\n  \"name\" : \"\",\n  \"cluster_name\" : \"\",\n  \"version\" : {\n    \"number\" : \"1.7.4\",\n    \"build_hash\" : \"0d3159b9fc8bc8e367c5c40c09c2a57c0032b32e\",\n    \"build_timestamp\" : \"2015-12-15T11:25:18Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"4.10.4\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n2016-01-10 08:36:17,523 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-10 08:36:17,953 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /_cat/indices/-2015.12.22?h=status&format=json HTTP/1.1\" 200 19\n2016-01-10 08:36:17,953 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cat/indices/-2015.12.22?h=status&format=json [status:200 request:0.431s]\n2016-01-10 08:36:17,954 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-10 08:36:17,954 DEBUG              elasticsearch    log_request_success:66   < [{\"status\":\"open\"}]\n2016-01-10 08:36:17,954 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-10 08:36:18,016 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /-2015.12.22/_segments HTTP/1.1\" 200 7740\n2016-01-10 08:36:18,016 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/-2015.12.22/_segments [status:200 request:0.062s]\n2016-01-10 08:36:18,016 DEBUG              elasticsearch    log_request_success:65   > None\n```\nthis is a part of our logs does this help? \n. 2016-01-11 01:07:12,648 DEBUG         curator.api.filter         get_date_regex:158  regex = \\d{4}\\.\\d{2}\\.\\d{2}\n2016-01-11 01:07:12,648 DEBUG          curator.cli.utils        filter_callback:189  REGEX = (?P<date>\\d{4}\\.\\d{2}\\.\\d{2})\n2016-01-11 01:07:12,648 DEBUG          curator.cli.utils        filter_callback:192  Added filter: {'pattern': '(?P<date>\\\\d{4}\\\\.\\\\d{2}\\\\.\\\\d{2})', 'value': 1, 'groupname': 'date',\n 'time_unit': 'days', 'timestring': u'%Y.%m.%d', 'method': 'older_than'}\n2016-01-11 01:07:12,648 DEBUG          curator.cli.utils        filter_callback:193  New list of filters: [{'pattern': '(?P<date>\\\\d{4}\\\\.\\\\d{2}\\\\.\\\\d{2})', 'value': 1, 'groupname':\n 'date', 'time_unit': 'days', 'timestring': u'%Y.%m.%d', 'method': 'older_than'}]\n2016-01-11 01:07:12,648 INFO      curator.cli.index_selection                indices:57   Job starting: optimize indices\n2016-01-11 01:07:12,648 DEBUG     curator.cli.index_selection                indices:60   Params: {'url_prefix': u'', 'http_auth': None, 'dry_run': False, 'certificate': None, 'logl\nevel': u'INFO', 'logformat': u'default', 'quiet': False, 'host': u'localhost', 'timeout': 30, 'debug': True, 'use_ssl': False, 'logfile': None, 'master_only': False, 'port': 9200, '\nssl_no_validate': False}\n2016-01-11 01:07:12,648 WARNING        curator.cli.utils       override_timeout:149  Overriding default connection timeout.  New timeout: 21600\n2016-01-11 01:07:12,648 DEBUG          curator.cli.utils             get_client:112  kwargs = {'url_prefix': u'', 'http_auth': None, 'dry_run': False, 'certificate': None, 'loglevel\n': u'INFO', 'quiet': False, 'debug': True, 'logformat': u'default', 'timeout': 21600, 'host': u'localhost', 'use_ssl': False, 'logfile': None, 'master_only': False, 'port': 9200, 's\nsl_no_validate': False}\n2016-01-11 01:07:12,649 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:12,649 INFO      urllib3.connectionpool              _new_conn:207  Starting new HTTP connection (1): localhost\n2016-01-11 01:07:12,654 DEBUG     urllib3.connectionpool          _make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-11 01:07:12,654 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.005s]\n2016-01-11 01:07:12,654 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:12,654 DEBUG          curator.cli.utils          check_version:90   Detected Elasticsearch version 1.7.4\n2016-01-11 01:07:12,654 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:12,690 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /_all/_settings?expand_wildcards=open%2Cclosed HTTP/1.1\" 200 269130\n2016-01-11 01:07:12,692 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_all/_settings?expand_wildcards=open%2Cclosed [status:200 request:0.03\n6s]\n2016-01-11 01:07:12,692 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:12,774 DEBUG     curator.cli.index_selection                indices:98   All filters: [{'pattern': '(?P<date>\\\\d{4}\\\\.\\\\d{2}\\\\.\\\\d{2})', 'value': 1, 'groupname': 'd\nate', 'time_unit': 'days', 'timestring': u'%Y.%m.%d', 'method': 'older_than'}]\n2016-01-11 01:07:12,774 DEBUG     curator.cli.index_selection                indices:103  Filter: {'pattern': '(?P<date>\\\\d{4}\\\\.\\\\d{2}\\\\.\\\\d{2})', 'value': 1, 'groupname': 'date', \n'time_unit': 'days', 'timestring': u'%Y.%m.%d', 'method': 'older_than'}\n2016-01-11 01:07:12,786 DEBUG         curator.api.filter        timestamp_check:301  Timestamp \"2016.01.11\" is outside the cutoff period (older than 1 days).\n2016-01-11 01:07:12,787 WARNING   curator.cli.index_selection                indices:157  Very large list of indices.  Breaking it up into smaller chunks.\n2016-01-11 01:07:12,787 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:12,788 DEBUG     urllib3.connectionpool          _make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-11 01:07:12,788 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.001s]\n2016-01-11 01:07:12,788 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:12,788 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:13,204 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /_cat/indices/logstash_env_<REDACTED INDEX>-2015.12.22?h=status&format=json HTTP/1.1\" 200 1\n9\n2016-01-11 01:07:13,204 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cat/indices/logstash_env_<REDACTED INDEX>-2015.12.22?h=status&format=\njson [status:200 request:0.416s]\n2016-01-11 01:07:13,204 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:13,204 DEBUG              elasticsearch    log_request_success:66   < [{\"status\":\"open\"}]\n2016-01-11 01:07:13,204 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:13,207 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /logstash_env_<REDACTED INDEX>-2015.12.22/_segments HTTP/1.1\" 200 7909\n2016-01-11 01:07:13,207 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/logstash_env_<REDACTED INDEX>-2015.12.22/_segments [status:200 request\n:0.003s]\n2016-01-11 01:07:13,207 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:13,210 DEBUG          curator.api.utils              optimized:169  Index logstash_env_<REDACTED INDEX>-2015.12.22 has 24 shards and 24 segments total.\n2016-01-11 01:07:13,210 DEBUG          curator.api.utils              optimized:174  Skipping index logstash_env_<REDACTED INDEX>-2015.12.22: Already optimized.\n2016-01-11 01:07:13,210 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:13,210 DEBUG     urllib3.connectionpool          _make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-11 01:07:13,210 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.000s]\n2016-01-11 01:07:13,211 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:13,211 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:13,838 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /_cat/indices/logstash_env_<REDACTED INDEX>-2015.12.23?h=status&format=json HTTP/1.1\" 200 1\n9\n2016-01-11 01:07:13,838 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cat/indices/logstash_env_<REDACTED INDEX>-2015.12.23?h=status&format=\njson [status:200 request:0.627s]\n2016-01-11 01:07:13,838 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:13,838 DEBUG              elasticsearch    log_request_success:66   < [{\"status\":\"open\"}]\n2016-01-11 01:07:13,838 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:13,850 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /logstash_env_<REDACTED INDEX>-2015.12.23/_segments HTTP/1.1\" 200 7909\n2016-01-11 01:07:13,851 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/logstash_env_<REDACTED INDEX>-2015.12.23/_segments [status:200 request\n:0.012s]\n2016-01-11 01:07:13,851 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:13,853 DEBUG          curator.api.utils              optimized:169  Index logstash_env_<REDACTED INDEX>-2015.12.23 has 24 shards and 24 segments total.\n2016-01-11 01:07:13,853 DEBUG          curator.api.utils              optimized:174  Skipping index logstash_env_<REDACTED INDEX>-2015.12.23: Already optimized.\n2016-01-11 01:07:13,853 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:13,854 DEBUG     urllib3.connectionpool          _make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-11 01:07:13,854 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.000s]\n2016-01-11 01:07:13,854 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:13,854 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:14,243 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /_cat/indices/logstash_env_<REDACTED INDEX>-2015.12.24?h=status&format=json HTTP/1.1\" 200 1\n9\n2016-01-11 01:07:14,243 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cat/indices/logstash_env_<REDACTED INDEX>-2015.12.24?h=status&format=\njson [status:200 request:0.389s]\n2016-01-11 01:07:14,243 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:14,243 DEBUG              elasticsearch    log_request_success:66   < [{\"status\":\"open\"}]\n2016-01-11 01:07:14,244 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:14,247 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /logstash_env_<REDACTED INDEX>-2015.12.24/_segments HTTP/1.1\" 200 7909\n2016-01-11 01:07:14,247 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/logstash_env_<REDACTED INDEX>-2015.12.24/_segments [status:200 request\n:0.003s]\n2016-01-11 01:07:14,247 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:14,249 DEBUG          curator.api.utils              optimized:169  Index logstash_env_<REDACTED INDEX>-2015.12.24 has 24 shards and 24 segments total.\n2016-01-11 01:07:14,249 DEBUG          curator.api.utils              optimized:174  Skipping index logstash_env_<REDACTED INDEX>-2015.12.24: Already optimized.\n2016-01-11 01:07:14,249 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:14,250 DEBUG     urllib3.connectionpool          _make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-11 01:07:14,250 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.000s]\n2016-01-11 01:07:14,250 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:14,250 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:14,632 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /_cat/indices/logstash_env_<REDACTED INDEX>-2015.12.25?h=status&format=json HTTP/1.1\" 200 1\n9\n2016-01-11 01:07:14,632 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cat/indices/logstash_env_<REDACTED INDEX>-2015.12.25?h=status&format=\njson [status:200 request:0.382s]\n2016-01-11 01:07:14,632 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:14,632 DEBUG              elasticsearch    log_request_success:66   < [{\"status\":\"open\"}]\n2016-01-11 01:07:14,632 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:14,665 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /logstash_env_<REDACTED INDEX>-2015.12.25/_segments HTTP/1.1\" 200 7909\n2016-01-11 01:07:14,665 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/logstash_env_<REDACTED INDEX>-2015.12.25/_segments [status:200 request\n:0.033s]\n2016-01-11 01:07:14,665 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:14,668 DEBUG          curator.api.utils              optimized:169  Index logstash_env_<REDACTED INDEX>-2015.12.25 has 24 shards and 24 segments total.\n2016-01-11 01:07:14,668 DEBUG          curator.api.utils              optimized:174  Skipping index logstash_env_<REDACTED INDEX>-2015.12.25: Already optimized.\n2016-01-11 01:07:14,668 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:14,668 DEBUG     urllib3.connectionpool          _make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-11 01:07:14,668 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.000s]\n2016-01-11 01:07:14,668 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:14,669 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:15,085 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /_cat/indices/logstash_env_<REDACTED INDEX>-2015.12.26?h=status&format=json HTTP/1.1\" 200 1\n9\n2016-01-11 01:07:15,085 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cat/indices/logstash_env_<REDACTED INDEX>-2015.12.26?h=status&format=\njson [status:200 request:0.417s]\n2016-01-11 01:07:15,085 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:15,086 DEBUG              elasticsearch    log_request_success:66   < [{\"status\":\"open\"}]\n2016-01-11 01:07:15,086 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:15,089 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /logstash_env_<REDACTED INDEX>-2015.12.26/_segments HTTP/1.1\" 200 7909\n2016-01-11 01:07:15,089 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/logstash_env_<REDACTED INDEX>-2015.12.26/_segments [status:200 request\n:0.003s]\n2016-01-11 01:07:15,089 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:15,092 DEBUG          curator.api.utils              optimized:169  Index logstash_env_<REDACTED INDEX>-2015.12.26 has 24 shards and 24 segments total.\n2016-01-11 01:07:15,092 DEBUG          curator.api.utils              optimized:174  Skipping index logstash_env_<REDACTED INDEX>-2015.12.26: Already optimized.\n2016-01-11 01:07:15,092 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:15,092 DEBUG     urllib3.connectionpool          _make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-11 01:07:15,092 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.001s]\n2016-01-11 01:07:15,092 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:15,093 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:15,474 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /_cat/indices/logstash_env_<REDACTED INDEX>-2015.12.27?h=status&format=json HTTP/1.1\" 200 1\n9\n2016-01-11 01:07:15,474 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cat/indices/logstash_env_<REDACTED INDEX>-2015.12.27?h=status&format=\njson [status:200 request:0.382s]\n2016-01-11 01:07:15,474 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:15,474 DEBUG              elasticsearch    log_request_success:66   < [{\"status\":\"open\"}]\n2016-01-11 01:07:15,475 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:15,494 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /logstash_env_<REDACTED INDEX>-2015.12.27/_segments HTTP/1.1\" 200 7909\n2016-01-11 01:07:15,494 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/logstash_env_<REDACTED INDEX>-2015.12.27/_segments [status:200 request\n:0.019s]\n2016-01-11 01:07:15,494 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:15,496 DEBUG          curator.api.utils              optimized:169  Index logstash_env_<REDACTED INDEX>-2015.12.27 has 24 shards and 24 segments total.\n2016-01-11 01:07:15,496 DEBUG          curator.api.utils              optimized:174  Skipping index logstash_env_<REDACTED INDEX>-2015.12.27: Already optimized.\n2016-01-11 01:07:15,496 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:15,497 DEBUG     urllib3.connectionpool          _make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-11 01:07:15,497 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.000s]\n2016-01-11 01:07:15,497 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:15,497 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:15,867 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /_cat/indices/logstash_env_<REDACTED INDEX>-2015.12.28?h=status&format=json HTTP/1.1\" 200 1\n9\n2016-01-11 01:07:15,867 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cat/indices/logstash_env_<REDACTED INDEX>-2015.12.28?h=status&format=\njson [status:200 request:0.370s]\n2016-01-11 01:07:15,868 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:15,868 DEBUG              elasticsearch    log_request_success:66   < [{\"status\":\"open\"}]\n2016-01-11 01:07:15,868 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:15,873 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /logstash_env_<REDACTED INDEX>-2015.12.28/_segments HTTP/1.1\" 200 7909\n2016-01-11 01:07:15,873 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/logstash_env_<REDACTED INDEX>-2015.12.28/_segments [status:200 request\n:0.006s]\n2016-01-11 01:07:15,873 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:15,876 DEBUG          curator.api.utils              optimized:169  Index logstash_env_<REDACTED INDEX>-2015.12.28 has 24 shards and 24 segments total.\n2016-01-11 01:07:15,876 DEBUG          curator.api.utils              optimized:174  Skipping index logstash_env_<REDACTED INDEX>-2015.12.28: Already optimized.\n2016-01-11 01:07:15,876 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:15,876 DEBUG     urllib3.connectionpool          _make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-11 01:07:15,877 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.000s]\n2016-01-11 01:07:15,877 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:15,877 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:16,322 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /_cat/indices/logstash_env_<REDACTED INDEX>-2015.12.29?h=status&format=json HTTP/1.1\" 200 1\n9\n2016-01-11 01:07:16,322 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cat/indices/logstash_env_<REDACTED INDEX>-2015.12.29?h=status&format=\njson [status:200 request:0.445s]\n2016-01-11 01:07:16,322 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:16,322 DEBUG              elasticsearch    log_request_success:66   < [{\"status\":\"open\"}]\n2016-01-11 01:07:16,323 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:16,326 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /logstash_env_<REDACTED INDEX>-2015.12.29/_segments HTTP/1.1\" 200 7885\n2016-01-11 01:07:16,327 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/logstash_env_<REDACTED INDEX>-2015.12.29/_segments [status:200 request\n:0.004s]\n2016-01-11 01:07:16,327 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:16,329 DEBUG          curator.api.utils              optimized:169  Index logstash_env_<REDACTED INDEX>-2015.12.29 has 24 shards and 24 segments total.\n2016-01-11 01:07:16,329 DEBUG          curator.api.utils              optimized:174  Skipping index logstash_env_<REDACTED INDEX>-2015.12.29: Already optimized.\n2016-01-11 01:07:16,329 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:16,329 DEBUG     urllib3.connectionpool          _make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-11 01:07:16,330 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.001s]\n2016-01-11 01:07:16,330 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:16,330 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:16,735 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /_cat/indices/logstash_env_<REDACTED INDEX>-2015.12.30?h=status&format=json HTTP/1.1\" 200 1\n9\n2016-01-11 01:07:16,735 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cat/indices/logstash_env_<REDACTED INDEX>-2015.12.30?h=status&format=\njson [status:200 request:0.405s]\n2016-01-11 01:07:16,735 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:16,735 DEBUG              elasticsearch    log_request_success:66   < [{\"status\":\"open\"}]\n2016-01-11 01:07:16,736 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:16,739 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /logstash_env_<REDACTED INDEX>-2015.12.30/_segments HTTP/1.1\" 200 7890\n2016-01-11 01:07:16,739 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/logstash_env_<REDACTED INDEX>-2015.12.30/_segments [status:200 request\n:0.003s]\n2016-01-11 01:07:16,739 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:16,741 DEBUG          curator.api.utils              optimized:169  Index logstash_env_<REDACTED INDEX>-2015.12.30 has 24 shards and 24 segments total.\n2016-01-11 01:07:16,741 DEBUG          curator.api.utils              optimized:174  Skipping index logstash_env_<REDACTED INDEX>-2015.12.30: Already optimized.\n2016-01-11 01:07:16,741 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:16,742 DEBUG     urllib3.connectionpool          _make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-11 01:07:16,742 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.000s]\n2016-01-11 01:07:16,742 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:16,742 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:17,126 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /_cat/indices/logstash_env_<REDACTED INDEX>-2015.12.31?h=status&format=json HTTP/1.1\" 200 1\n9\n2016-01-11 01:07:17,126 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cat/indices/logstash_env_<REDACTED INDEX>-2015.12.31?h=status&format=\njson [status:200 request:0.384s]\n2016-01-11 01:07:17,126 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:17,126 DEBUG              elasticsearch    log_request_success:66   < [{\"status\":\"open\"}]\n2016-01-11 01:07:17,127 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:17,130 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /logstash_env_<REDACTED INDEX>-2015.12.31/_segments HTTP/1.1\" 200 7885\n2016-01-11 01:07:17,130 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/logstash_env_<REDACTED INDEX>-2015.12.31/_segments [status:200 request\n:0.004s]\n2016-01-11 01:07:17,130 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:17,133 DEBUG          curator.api.utils              optimized:169  Index logstash_env_<REDACTED INDEX>-2015.12.31 has 24 shards and 24 segments total.\n2016-01-11 01:07:17,133 DEBUG          curator.api.utils              optimized:174  Skipping index logstash_env_<REDACTED INDEX>-2015.12.31: Already optimized.\n2016-01-11 01:07:17,133 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:17,133 DEBUG     urllib3.connectionpool          _make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-11 01:07:17,133 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.000s]\n2016-01-11 01:07:17,133 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:17,134 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:17,527 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /_cat/indices/logstash_env_<REDACTED INDEX>-2016.01.01?h=status&format=json HTTP/1.1\" 200 1\n9\n2016-01-11 01:07:17,527 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cat/indices/logstash_env_<REDACTED INDEX>-2016.01.01?h=status&format=\njson [status:200 request:0.393s]\n2016-01-11 01:07:17,527 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:17,527 DEBUG              elasticsearch    log_request_success:66   < [{\"status\":\"open\"}]\n2016-01-11 01:07:17,528 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-11 01:07:17,530 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /logstash_env_<REDACTED INDEX>-2016.01.01/_segments HTTP/1.1\" 200 7933\n2016-01-11 01:07:17,531 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/logstash_env_<REDACTED INDEX>-2016.01.01/_segments [status:200 request\n:0.003s]\n2016-01-11 01:07:17,531 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-11 01:07:17,533 DEBUG          curator.api.utils              optimized:169  Index logstash_env_<REDACTED INDEX>-2016.01.01 has 24 shards and 24 segments total.\n2016-01-11 01:07:17,533 DEBUG          curator.api.utils              optimized:174  Skipping index logstash_env_<REDACTED INDEX>-2016.01.01: Already optimized.\nWhat about this?\n. I might need to wait 'til tomorrow to run it again. \n. ```\n2016-01-13 02:02:05,764 DEBUG          curator.cli.utils        filter_callback:189  REGEX = ^logstash_-.$\n2016-01-13 02:02:05,764 DEBUG          curator.cli.utils        filter_callback:192  Added filter: {'pattern': '^logstash_-.$'}\n2016-01-13 02:02:05,764 DEBUG          curator.cli.utils        filter_callback:193  New list of filters: [{'pattern': '^logstash_-.$'}]\n2016-01-13 02:02:05,764 DEBUG         curator.api.filter         get_date_regex:158  regex = \\d{4}.\\d{2}.\\d{2}\n2016-01-13 02:02:05,764 DEBUG          curator.cli.utils        filter_callback:189  REGEX = (?P\\d{4}.\\d{2}.\\d{2})\n2016-01-13 02:02:05,764 DEBUG          curator.cli.utils        filter_callback:192  Added filter: {'pattern': '(?P\\d{4}\\.\\d{2}\\.\\d{2})', 'value': 1, 'groupname': 'date', 'time_unit': 'days', 'timestring': u'%Y.%m.%d', 'method': 'older_than'}\n2016-01-13 02:02:05,764 DEBUG          curator.cli.utils        filter_callback:193  New list of filters: [{'pattern': '^logstash_-.$'}, {'pattern': '(?P\\d{4}\\.\\d{2}\\.\\d{2})', 'value': 1, 'groupname': 'date', 'time_unit': 'days', 'timestring': u'%Y.%m.%d', 'method': 'older_than'}]\n2016-01-13 02:02:05,765 INFO      curator.cli.index_selection                indices:57   Job starting: optimize indices\n2016-01-13 02:02:05,765 DEBUG     curator.cli.index_selection                indices:60   Params: {'url_prefix': u'', 'http_auth': None, 'dry_run': False, 'certificate': None, 'loglevel': u'INFO', 'logformat': u'default', 'quiet': False, 'host': u'localhost', 'timeout': 30, 'debug': True, 'use_ssl': False, 'logfile': None, 'master_only': False, 'port': 9200, 'ssl_no_validate': False}\n2016-01-13 02:02:05,765 WARNING        curator.cli.utils       override_timeout:149  Overriding default connection timeout.  New timeout: 21600\n2016-01-13 02:02:05,765 DEBUG          curator.cli.utils             get_client:112  kwargs = {'url_prefix': u'', 'http_auth': None, 'dry_run': False, 'certificate': None, 'loglevel': u'INFO', 'quiet': False, 'debug': True, 'logformat': u'default', 'timeout': 21600, 'host': u'localhost', 'use_ssl': False, 'logfile': None, 'master_only': False, 'port': 9200, 'ssl_no_validate': False}\n2016-01-13 02:02:05,765 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:05,765 INFO      urllib3.connectionpool              new_conn:207  Starting new HTTP connection (1): localhost\n2016-01-13 02:02:05,779 DEBUG     urllib3.connectionpool          _make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-13 02:02:05,780 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.015s]\n2016-01-13 02:02:05,780 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:05,781 DEBUG          curator.cli.utils          check_version:90   Detected Elasticsearch version 1.7.4\n2016-01-13 02:02:05,782 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:05,829 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /_all/_settings?expand_wildcards=open%2Cclosed HTTP/1.1\" 200 262219\n2016-01-13 02:02:05,831 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_all/_settings?expand_wildcards=open%2Cclosed [status:200 request:0.048s]\n2016-01-13 02:02:05,831 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:05,913 DEBUG     curator.cli.index_selection                indices:98   All filters: [{'pattern': '^logstash-.$'}, {'pattern': '(?P\\d{4}\\.\\d{2}\\.\\d{2})', 'value': 1, 'groupname': 'date', 'time_unit': 'days', 'timestring': u'%Y.%m.%d', 'method': 'older_than'}]\n2016-01-13 02:02:05,913 DEBUG     curator.cli.index_selection                indices:103  Filter: {'pattern': '^logstash_-.$'}\n2016-01-13 02:02:05,914 DEBUG     curator.cli.index_selection                indices:103  Filter: {'pattern': '(?P\\d{4}\\.\\d{2}\\.\\d{2})', 'value': 1, 'groupname': 'date', 'time_unit': 'days', 'timestring': u'%Y.%m.%d', 'method': 'older_than'}\n2016-01-13 02:02:05,915 DEBUG         curator.api.filter        timestamp_check:301  Timestamp \"2016.01.13\" is outside the cutoff period (older than 1 days).\n2016-01-13 02:02:05,916 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:05,923 DEBUG     urllib3.connectionpool          make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-13 02:02:05,923 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.007s]\n2016-01-13 02:02:05,923 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:06,739 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cat/indices/logstash-2015.12.23?h=status&format=json [status:200 request:0.815s]\n2016-01-13 02:02:06,739 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:06,739 DEBUG              elasticsearch    log_request_success:66   < [{\"status\":\"open\"}]\n2016-01-13 02:02:06,740 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:06,747 DEBUG     urllib3.connectionpool          make_request:386  \"GET /logstash-2015.12.23/segments HTTP/1.1\" 200 7733\n2016-01-13 02:02:06,747 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/logstash-2015.12.23/segments [status:200 request:0.008s]\n2016-01-13 02:02:06,747 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:06,750 DEBUG          curator.api.utils              optimized:169  Index logstash-2015.12.23 has 24 shards and 24 segments total.\n2016-01-13 02:02:06,750 DEBUG          curator.api.utils              optimized:174  Skipping index logstash_-2015.12.23: Already optimized.\n2016-01-13 02:02:06,750 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:06,750 DEBUG     urllib3.connectionpool          make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-13 02:02:06,750 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.001s]\n2016-01-13 02:02:06,750 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:06,751 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:07,920 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /_cat/indices/logstash-2015.12.24?h=status&format=json HTTP/1.1\" 200 19\n2016-01-13 02:02:07,920 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/cat/indices/logstash-2015.12.24?h=status&format=json [status:200 request:1.169s]\n2016-01-13 02:02:07,920 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:07,920 DEBUG              elasticsearch    log_request_success:66   < [{\"status\":\"open\"}]\n2016-01-13 02:02:07,921 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:07,947 DEBUG     urllib3.connectionpool          make_request:386  \"GET /logstash-2015.12.24/segments HTTP/1.1\" 200 7717\n2016-01-13 02:02:07,947 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/logstash-2015.12.24/segments [status:200 request:0.026s]\n2016-01-13 02:02:07,947 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:07,950 DEBUG          curator.api.utils              optimized:169  Index logstash-2015.12.24 has 24 shards and 24 segments total.\n2016-01-13 02:02:07,950 DEBUG          curator.api.utils              optimized:174  Skipping index logstash_-2015.12.24: Already optimized.\n2016-01-13 02:02:07,950 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:07,950 DEBUG     urllib3.connectionpool          make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-13 02:02:07,950 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.001s]\n2016-01-13 02:02:07,951 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:07,951 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:08,888 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /_cat/indices/logstash-2015.12.25?h=status&format=json HTTP/1.1\" 200 19\n2016-01-13 02:02:08,889 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/cat/indices/logstash-2015.12.25?h=status&format=json [status:200 request:0.938s]\n2016-01-13 02:02:08,889 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:08,889 DEBUG              elasticsearch    log_request_success:66   < [{\"status\":\"open\"}]\n2016-01-13 02:02:08,889 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:08,959 DEBUG     urllib3.connectionpool          make_request:386  \"GET /logstash-2015.12.25/segments HTTP/1.1\" 200 7713\n2016-01-13 02:02:08,960 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/logstash-2015.12.25/segments [status:200 request:0.071s]\n2016-01-13 02:02:08,960 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:08,962 DEBUG          curator.api.utils              optimized:169  Index logstash-2015.12.25 has 24 shards and 24 segments total.\n2016-01-13 02:02:08,962 DEBUG          curator.api.utils              optimized:174  Skipping index logstash_-2015.12.25: Already optimized.\n2016-01-13 02:02:08,962 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:08,963 DEBUG     urllib3.connectionpool          make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-13 02:02:08,963 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.001s]\n2016-01-13 02:02:08,963 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:08,964 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:09,644 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /_cat/indices/logstash-2015.12.26?h=status&format=json HTTP/1.1\" 200 19\n2016-01-13 02:02:09,644 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/cat/indices/logstash-2015.12.26?h=status&format=json [status:200 request:0.681s]\n2016-01-13 02:02:09,645 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:09,645 DEBUG              elasticsearch    log_request_success:66   < [{\"status\":\"open\"}]\n2016-01-13 02:02:09,645 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:09,733 DEBUG     urllib3.connectionpool          make_request:386  \"GET /logstash-2015.12.26/segments HTTP/1.1\" 200 7665\n2016-01-13 02:02:09,734 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/logstash-2015.12.26/segments [status:200 request:0.089s]\n2016-01-13 02:02:09,734 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:09,737 DEBUG          curator.api.utils              optimized:169  Index logstash-2015.12.26 has 24 shards and 24 segments total.\n2016-01-13 02:02:09,737 DEBUG          curator.api.utils              optimized:174  Skipping index logstash_-2015.12.26: Already optimized.\n2016-01-13 02:02:09,738 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:09,738 DEBUG     urllib3.connectionpool          make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-13 02:02:09,738 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.001s]\n2016-01-13 02:02:09,738 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:09,739 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:10,580 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /_cat/indices/logstash-2015.12.27?h=status&format=json HTTP/1.1\" 200 19\n2016-01-13 02:02:10,581 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/cat/indices/logstash-2015.12.27?h=status&format=json [status:200 request:0.842s]\n2016-01-13 02:02:10,581 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:10,581 DEBUG              elasticsearch    log_request_success:66   < [{\"status\":\"open\"}]\n2016-01-13 02:02:10,581 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:10,589 DEBUG     urllib3.connectionpool          make_request:386  \"GET /logstash-2015.12.27/segments HTTP/1.1\" 200 7665\n2016-01-13 02:02:10,589 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/logstash-2015.12.27/segments [status:200 request:0.008s]\n2016-01-13 02:02:10,590 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:10,592 DEBUG          curator.api.utils              optimized:169  Index logstash-2015.12.27 has 24 shards and 24 segments total.\n2016-01-13 02:02:10,592 DEBUG          curator.api.utils              optimized:174  Skipping index logstash_-2015.12.27: Already optimized.\n2016-01-13 02:02:10,592 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:10,593 DEBUG     urllib3.connectionpool          make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-13 02:02:10,593 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.001s]\n2016-01-13 02:02:10,593 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:10,593 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:11,857 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /_cat/indices/logstash-2015.12.28?h=status&format=json HTTP/1.1\" 200 19\n2016-01-13 02:02:11,857 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/cat/indices/logstash-2015.12.28?h=status&format=json [status:200 request:1.264s]\n2016-01-13 02:02:11,857 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:11,857 DEBUG              elasticsearch    log_request_success:66   < [{\"status\":\"open\"}]\n2016-01-13 02:02:11,858 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:12,034 DEBUG     urllib3.connectionpool          make_request:386  \"GET /logstash-2015.12.28/segments HTTP/1.1\" 200 7665\n2016-01-13 02:02:12,034 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/logstash-2015.12.28/segments [status:200 request:0.176s]\n2016-01-13 02:02:12,034 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:12,036 DEBUG          curator.api.utils              optimized:169  Index logstash-2015.12.28 has 24 shards and 24 segments total.\n2016-01-13 02:02:12,037 DEBUG          curator.api.utils              optimized:174  Skipping index logstash_-2015.12.28: Already optimized.\n2016-01-13 02:02:12,037 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:12,037 DEBUG     urllib3.connectionpool          make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-13 02:02:12,037 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.001s]\n2016-01-13 02:02:12,037 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:12,038 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:13,273 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /_cat/indices/logstash-2015.12.29?h=status&format=json HTTP/1.1\" 200 19\n2016-01-13 02:02:13,273 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/cat/indices/logstash-2015.12.29?h=status&format=json [status:200 request:1.236s]\n2016-01-13 02:02:13,273 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:13,273 DEBUG              elasticsearch    log_request_success:66   < [{\"status\":\"open\"}]\n2016-01-13 02:02:13,274 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:13,307 DEBUG     urllib3.connectionpool          make_request:386  \"GET /logstash-2015.12.29/segments HTTP/1.1\" 200 7737\n2016-01-13 02:02:13,307 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/logstash-2015.12.29/segments [status:200 request:0.033s]\n2016-01-13 02:02:13,307 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:13,309 DEBUG          curator.api.utils              optimized:169  Index logstash-2015.12.29 has 24 shards and 24 segments total.\n2016-01-13 02:02:13,309 DEBUG          curator.api.utils              optimized:174  Skipping index logstash_-2015.12.29: Already optimized.\n2016-01-13 02:02:13,309 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:13,310 DEBUG     urllib3.connectionpool          make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-13 02:02:13,310 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.001s]\n2016-01-13 02:02:13,310 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:13,311 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:14,319 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /_cat/indices/logstash-2015.12.30?h=status&format=json HTTP/1.1\" 200 19\n2016-01-13 02:02:14,319 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/cat/indices/logstash-2015.12.30?h=status&format=json [status:200 request:1.008s]\n2016-01-13 02:02:14,319 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:14,319 DEBUG              elasticsearch    log_request_success:66   < [{\"status\":\"open\"}]\n2016-01-13 02:02:14,319 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:14,368 DEBUG     urllib3.connectionpool          make_request:386  \"GET /logstash-2015.12.30/segments HTTP/1.1\" 200 7737\n2016-01-13 02:02:14,368 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/logstash-2015.12.30/segments [status:200 request:0.049s]\n2016-01-13 02:02:14,369 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:14,371 DEBUG          curator.api.utils              optimized:169  Index logstash-2015.12.30 has 24 shards and 24 segments total.\n2016-01-13 02:02:14,371 DEBUG          curator.api.utils              optimized:174  Skipping index logstash_-2015.12.30: Already optimized.\n2016-01-13 02:02:14,371 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:14,372 DEBUG     urllib3.connectionpool          make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-13 02:02:14,372 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.001s]\n2016-01-13 02:02:14,372 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:15,005 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cat/indices/logstash-2015.12.31?h=status&format=json [status:200 request:0.633s]\n2016-01-13 02:02:15,005 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:15,005 DEBUG              elasticsearch    log_request_success:66   < [{\"status\":\"open\"}]\n2016-01-13 02:02:15,005 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:15,151 DEBUG     urllib3.connectionpool          make_request:386  \"GET /logstash-2015.12.31/segments HTTP/1.1\" 200 7647\n2016-01-13 02:02:15,152 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/logstash-2015.12.31/segments [status:200 request:0.146s]\n2016-01-13 02:02:15,152 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:15,165 DEBUG          curator.api.utils              optimized:169  Index logstash-2015.12.31 has 24 shards and 24 segments total.\n2016-01-13 02:02:15,165 DEBUG          curator.api.utils              optimized:174  Skipping index logstash_-2015.12.31: Already optimized.\n2016-01-13 02:02:15,165 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:15,166 DEBUG     urllib3.connectionpool          make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-13 02:02:15,166 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.001s]\n2016-01-13 02:02:15,167 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:15,769 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cat/indices/logstash-2016.01.01?h=status&format=json [status:200 request:0.602s]\n2016-01-13 02:02:15,769 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:15,769 DEBUG              elasticsearch    log_request_success:66   < [{\"status\":\"open\"}]\n2016-01-13 02:02:15,769 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:15,911 DEBUG     urllib3.connectionpool          make_request:386  \"GET /logstash-2016.01.01/segments HTTP/1.1\" 200 7645\n2016-01-13 02:02:15,911 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/logstash-2016.01.01/segments [status:200 request:0.142s]\n2016-01-13 02:02:15,911 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:15,913 DEBUG          curator.api.utils              optimized:169  Index logstash-2016.01.01 has 24 shards and 24 segments total.\n2016-01-13 02:02:15,913 DEBUG          curator.api.utils              optimized:174  Skipping index logstash_-2016.01.01: Already optimized.\n2016-01-13 02:02:15,914 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:15,915 DEBUG     urllib3.connectionpool          make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-13 02:02:15,915 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.002s]\n2016-01-13 02:02:15,915 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:16,654 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/_cat/indices/logstash-2016.01.02?h=status&format=json [status:200 request:0.739s]\n2016-01-13 02:02:16,655 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:16,655 DEBUG              elasticsearch    log_request_success:66   < [{\"status\":\"open\"}]\n2016-01-13 02:02:16,655 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:16,833 DEBUG     urllib3.connectionpool          make_request:386  \"GET /logstash-2016.01.02/segments HTTP/1.1\" 200 7652\n2016-01-13 02:02:16,833 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/logstash-2016.01.02/segments [status:200 request:0.179s]\n2016-01-13 02:02:16,834 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:16,836 DEBUG          curator.api.utils              optimized:169  Index logstash-2016.01.02 has 24 shards and 24 segments total.\n2016-01-13 02:02:16,836 DEBUG          curator.api.utils              optimized:174  Skipping index logstash_-2016.01.02: Already optimized.\n2016-01-13 02:02:16,836 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:16,837 DEBUG     urllib3.connectionpool          make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-13 02:02:16,837 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.001s]\n2016-01-13 02:02:16,837 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:16,838 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:17,536 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /_cat/indices/logstash-2016.01.03?h=status&format=json HTTP/1.1\" 200 19\n2016-01-13 02:02:17,537 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/cat/indices/logstash-2016.01.03?h=status&format=json [status:200 request:0.699s]\n2016-01-13 02:02:17,537 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:17,537 DEBUG              elasticsearch    log_request_success:66   < [{\"status\":\"open\"}]\n2016-01-13 02:02:17,537 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:17,598 DEBUG     urllib3.connectionpool          make_request:386  \"GET /logstash-2016.01.03/segments HTTP/1.1\" 200 7641\n2016-01-13 02:02:17,598 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/logstash-2016.01.03/segments [status:200 request:0.062s]\n2016-01-13 02:02:17,599 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:17,601 DEBUG          curator.api.utils              optimized:169  Index logstash-2016.01.03 has 24 shards and 24 segments total.\n2016-01-13 02:02:17,601 DEBUG          curator.api.utils              optimized:174  Skipping index logstash_-2016.01.03: Already optimized.\n2016-01-13 02:02:17,601 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:17,602 DEBUG     urllib3.connectionpool          make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-13 02:02:17,602 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.001s]\n2016-01-13 02:02:17,602 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:17,603 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:18,012 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /_cat/indices/logstash-2016.01.04?h=status&format=json HTTP/1.1\" 200 19\n2016-01-13 02:02:18,013 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/cat/indices/logstash-2016.01.04?h=status&format=json [status:200 request:0.410s]\n2016-01-13 02:02:18,013 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:18,013 DEBUG              elasticsearch    log_request_success:66   < [{\"status\":\"open\"}]\n2016-01-13 02:02:18,013 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:18,060 DEBUG     urllib3.connectionpool          make_request:386  \"GET /logstash-2016.01.04/segments HTTP/1.1\" 200 7737\n2016-01-13 02:02:18,061 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/logstash-2016.01.04/segments [status:200 request:0.048s]\n2016-01-13 02:02:18,061 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:18,063 DEBUG          curator.api.utils              optimized:169  Index logstash-2016.01.04 has 24 shards and 24 segments total.\n2016-01-13 02:02:18,063 DEBUG          curator.api.utils              optimized:174  Skipping index logstash_-2016.01.04: Already optimized.\n2016-01-13 02:02:18,063 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:18,064 DEBUG     urllib3.connectionpool          _make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-13 02:02:18,064 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.001s]\n2016-01-13 02:02:18,064 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:18,065 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:18,585 DEBUG     urllib3.connectionpool          make_request:386  \"GET /_cat/indices/logstash-2016.01.05?h=status&format=json HTTP/1.1\" 200 19\n2016-01-13 02:02:18,586 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/cat/indices/logstash-2016.01.05?h=status&format=json [status:200 request:0.521s]\n2016-01-13 02:02:18,586 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:18,586 DEBUG              elasticsearch    log_request_success:66   < [{\"status\":\"open\"}]\n2016-01-13 02:02:18,586 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:18,627 DEBUG     urllib3.connectionpool          make_request:386  \"GET /logstash-2016.01.05/segments HTTP/1.1\" 200 7665\n2016-01-13 02:02:18,627 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/logstash-2016.01.05/segments [status:200 request:0.041s]\n2016-01-13 02:02:18,627 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:18,630 DEBUG          curator.api.utils              optimized:169  Index logstash-2016.01.05 has 24 shards and 24 segments total.\n2016-01-13 02:02:18,630 DEBUG          curator.api.utils              optimized:174  Skipping index logstash_-2016.01.05: Already optimized.\n2016-01-13 02:02:18,630 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:18,631 DEBUG     urllib3.connectionpool          make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-13 02:02:18,631 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.001s]\n2016-01-13 02:02:18,631 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:18,631 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:19,162 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /_cat/indices/logstash-2016.01.06?h=status&format=json HTTP/1.1\" 200 19\n2016-01-13 02:02:19,163 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/cat/indices/logstash-2016.01.06?h=status&format=json [status:200 request:0.531s]\n2016-01-13 02:02:19,163 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:19,163 DEBUG              elasticsearch    log_request_success:66   < [{\"status\":\"open\"}]\n2016-01-13 02:02:19,163 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:19,216 DEBUG     urllib3.connectionpool          make_request:386  \"GET /logstash-2016.01.06/segments HTTP/1.1\" 200 7665\n2016-01-13 02:02:19,216 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/logstash-2016.01.06/segments [status:200 request:0.053s]\n2016-01-13 02:02:19,216 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:19,220 DEBUG          curator.api.utils              optimized:169  Index logstash-2016.01.06 has 24 shards and 24 segments total.\n2016-01-13 02:02:19,220 DEBUG          curator.api.utils              optimized:174  Skipping index logstash_-2016.01.06: Already optimized.\n2016-01-13 02:02:19,220 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:19,220 DEBUG     urllib3.connectionpool          make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-13 02:02:19,221 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.001s]\n2016-01-13 02:02:19,221 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:19,779 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /_cat/indices/logstash-2016.01.07?h=status&format=json HTTP/1.1\" 200 19\n2016-01-13 02:02:19,779 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/cat/indices/logstash-2016.01.07?h=status&format=json [status:200 request:0.558s]\n2016-01-13 02:02:19,779 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:19,780 DEBUG              elasticsearch    log_request_success:66   < [{\"status\":\"open\"}]\n2016-01-13 02:02:19,780 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:19,783 DEBUG     urllib3.connectionpool          make_request:386  \"GET /logstash-2016.01.07/segments HTTP/1.1\" 200 7761\n2016-01-13 02:02:19,783 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/logstash-2016.01.07/segments [status:200 request:0.003s]\n2016-01-13 02:02:19,783 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:19,785 DEBUG          curator.api.utils              optimized:169  Index logstash-2016.01.07 has 24 shards and 24 segments total.\n2016-01-13 02:02:19,785 DEBUG          curator.api.utils              optimized:174  Skipping index logstash_-2016.01.07: Already optimized.\n2016-01-13 02:02:19,785 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:19,786 DEBUG     urllib3.connectionpool          make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-13 02:02:19,786 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.001s]\n2016-01-13 02:02:19,786 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:19,787 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:20,258 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /_cat/indices/logstash-2016.01.08?h=status&format=json HTTP/1.1\" 200 19\n2016-01-13 02:02:20,258 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/cat/indices/logstash-2016.01.08?h=status&format=json [status:200 request:0.472s]\n2016-01-13 02:02:20,259 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:20,259 DEBUG              elasticsearch    log_request_success:66   < [{\"status\":\"open\"}]\n2016-01-13 02:02:20,259 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:20,289 DEBUG     urllib3.connectionpool          make_request:386  \"GET /logstash-2016.01.08/segments HTTP/1.1\" 200 7689\n2016-01-13 02:02:20,290 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/logstash-2016.01.08/segments [status:200 request:0.031s]\n2016-01-13 02:02:20,290 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:20,292 DEBUG          curator.api.utils              optimized:169  Index logstash-2016.01.08 has 24 shards and 24 segments total.\n2016-01-13 02:02:20,292 DEBUG          curator.api.utils              optimized:174  Skipping index logstash_-2016.01.08: Already optimized.\n2016-01-13 02:02:20,292 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:20,293 DEBUG     urllib3.connectionpool          make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-13 02:02:20,293 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.001s]\n2016-01-13 02:02:20,791 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /_cat/indices/logstash-2016.01.09?h=status&format=json HTTP/1.1\" 200 19\n2016-01-13 02:02:20,791 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/cat/indices/logstash-2016.01.09?h=status&format=json [status:200 request:0.497s]\n2016-01-13 02:02:20,791 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:20,792 DEBUG              elasticsearch    log_request_success:66   < [{\"status\":\"open\"}]\n2016-01-13 02:02:20,792 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:20,851 DEBUG     urllib3.connectionpool          make_request:386  \"GET /logstash-2016.01.09/segments HTTP/1.1\" 200 7671\n2016-01-13 02:02:20,851 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/logstash-2016.01.09/segments [status:200 request:0.059s]\n2016-01-13 02:02:20,851 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:20,853 DEBUG          curator.api.utils              optimized:169  Index logstash-2016.01.09 has 24 shards and 24 segments total.\n2016-01-13 02:02:20,854 DEBUG          curator.api.utils              optimized:174  Skipping index logstash_-2016.01.09: Already optimized.\n2016-01-13 02:02:20,854 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:20,854 DEBUG     urllib3.connectionpool          make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-13 02:02:20,854 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.001s]\n2016-01-13 02:02:20,854 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:20,855 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:21,329 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /_cat/indices/logstash-2016.01.10?h=status&format=json HTTP/1.1\" 200 19\n2016-01-13 02:02:21,329 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/cat/indices/logstash-2016.01.10?h=status&format=json [status:200 request:0.475s]\n2016-01-13 02:02:21,330 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:21,330 DEBUG              elasticsearch    log_request_success:66   < [{\"status\":\"open\"}]\n2016-01-13 02:02:21,330 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:21,352 DEBUG     urllib3.connectionpool          make_request:386  \"GET /logstash-2016.01.10/segments HTTP/1.1\" 200 7665\n2016-01-13 02:02:21,353 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/logstash-2016.01.10/segments [status:200 request:0.023s]\n2016-01-13 02:02:21,353 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:21,356 DEBUG          curator.api.utils              optimized:169  Index logstash-2016.01.10 has 24 shards and 24 segments total.\n2016-01-13 02:02:21,356 DEBUG          curator.api.utils              optimized:174  Skipping index logstash_-2016.01.10: Already optimized.\n2016-01-13 02:02:21,356 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:21,356 DEBUG     urllib3.connectionpool          make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-13 02:02:21,357 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.001s]\n2016-01-13 02:02:21,357 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:22,532 DEBUG     urllib3.connectionpool          _make_request:386  \"GET /_cat/indices/logstash-2016.01.11?h=status&format=json HTTP/1.1\" 200 19\n2016-01-13 02:02:22,532 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/cat/indices/logstash-2016.01.11?h=status&format=json [status:200 request:1.175s]\n2016-01-13 02:02:22,532 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:22,533 DEBUG              elasticsearch    log_request_success:66   < [{\"status\":\"open\"}]\n2016-01-13 02:02:22,533 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:22,548 DEBUG     urllib3.connectionpool          make_request:386  \"GET /logstash-2016.01.11/segments HTTP/1.1\" 200 34841\n2016-01-13 02:02:22,549 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/logstash-2016.01.11/segments [status:200 request:0.016s]\n2016-01-13 02:02:22,549 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:22,566 DEBUG          curator.api.utils              optimized:169  Index logstash-2016.01.11 has 36 shards and 176 segments total.\n2016-01-13 02:02:22,566 DEBUG          curator.api.utils              optimized:171  Flagging index logstash_-2016.01.11 for optimization.\n2016-01-13 02:02:22,566 INFO        curator.api.optimize         optimize_index:22   Optimizing index logstash_-2016.01.11 to 1 segments per shard.  Please wait...\n2016-01-13 02:02:22,566 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:02:23,117 DEBUG     urllib3.connectionpool          make_request:386  \"POST /logstash-2016.01.11/optimize?max_num_segments=1 HTTP/1.1\" 200 51\n2016-01-13 02:02:23,117 INFO               elasticsearch    log_request_success:63   POST http://localhost:9200/logstash-2016.01.11/_optimize?max_num_segments=1 [status:200 request:0.552s]\n2016-01-13 02:02:23,118 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:02:23,118 DEBUG              elasticsearch    log_request_success:66   < {\"_shards\":{\"total\":36,\"successful\":36,\"failed\":0}}\n2016-01-13 02:04:23,201 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:04:23,202 DEBUG     urllib3.connectionpool          _make_request:386  \"GET / HTTP/1.1\" 200 406\n2016-01-13 02:04:23,202 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/ [status:200 request:0.001s]\n2016-01-13 02:04:23,202 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:04:23,202 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:04:23,662 DEBUG     urllib3.connectionpool          make_request:386  \"GET /_cat/indices/logstash-2016.01.12?h=status&format=json HTTP/1.1\" 200 19\n2016-01-13 02:04:23,663 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/cat/indices/logstash-2016.01.12?h=status&format=json [status:200 request:0.461s]\n2016-01-13 02:04:23,663 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:04:23,663 DEBUG              elasticsearch    log_request_success:66   < [{\"status\":\"open\"}]\n2016-01-13 02:04:23,663 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:04:23,666 DEBUG     urllib3.connectionpool          make_request:386  \"GET /logstash-2016.01.12/segments HTTP/1.1\" 200 36122\n2016-01-13 02:04:23,667 INFO               elasticsearch    log_request_success:63   GET http://localhost:9200/logstash-2016.01.12/segments [status:200 request:0.004s]\n2016-01-13 02:04:23,667 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:04:23,677 DEBUG          curator.api.utils              optimized:169  Index logstash-2016.01.12 has 36 shards and 185 segments total.\n2016-01-13 02:04:23,677 DEBUG          curator.api.utils              optimized:171  Flagging index logstash_-2016.01.12 for optimization.\n2016-01-13 02:04:23,677 INFO        curator.api.optimize         optimize_index:22   Optimizing index logstash_-2016.01.12 to 1 segments per shard.  Please wait...\n2016-01-13 02:04:23,677 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-01-13 02:04:23,975 DEBUG     urllib3.connectionpool          make_request:386  \"POST /logstash-2016.01.12/optimize?max_num_segments=1 HTTP/1.1\" 200 51\n2016-01-13 02:04:23,975 INFO               elasticsearch    log_request_success:63   POST http://localhost:9200/logstash-2016.01.12/_optimize?max_num_segments=1 [status:200 request:0.298s]\n2016-01-13 02:04:23,975 DEBUG              elasticsearch    log_request_success:65   > None\n2016-01-13 02:04:23,975 DEBUG              elasticsearch    log_request_success:66   < {\"_shards\":{\"total\":36,\"successful\":36,\"failed\":0}}\n2016-01-13 02:06:24,010 INFO           curator.cli.utils               exit_msg:67   Job completed successfully.\n``\n. So, you are right the issue apparently exist if you optimize all your indices, I mean, if I remove the --prefix parameter of the curator command which I didn't do this time. =/ I think I will try to send you another debug log maybe on friday.\n. We are running a 17 node cluster 12,706 shards, 564 indices, ~10bn documents and 15TB of data.\n. Yes, 4 SSD volumes of 500GB on LVM\n. I will send you my debug of curator optimize on Friday since I think that the problem occurs when I want to optimize all indices older than 1 day in my cluster. \n. Yes, so I discovered that this happens when you have indices of with different patterns for example if you havelogstash_app_1-2016.12...,logstash_app_2-2016.12...,logstash_app_3-2016.12...and you do an optimize over all your indices curator doesn't respectdelaybetween the different patterns. Although it does per on aper day` basis on each indices type. I don't know if I am explaining myself correctly.\n. <3 awesome I'll be waiting. My biggest pain point is that I run optimizations everyday so there is always one index pending on each logstash pattern so if the delay is not respected it just goes index per index without any delay and the cluster suffers because it is optimizing the whole day ~5TB of data.\n. Sounds good I'll try to test that :) \n. Sweet! thanks :D \n. ",
    "aleksz": "any estimation when this awesome fix could make it's way to pip?\n. ",
    "xaka": "I've signed CLA and verified it by email, but check is still failing on me. Another failed check doesn't seem to be related to proposed changes so it's time for :beers:\n. ",
    "svenmueller": "+1\n. ",
    "jakommo": "Not a real answer but please note https://hub.docker.com/_/elasticsearch/ is not developed or provided by Elastic. It's just an \"official\" docker library image. \n. I hit this today with the following config:\nfilters:\n    - filtertype: age\n      source: name\n      direction: older\n      timestring: '%Y.%W'\n      unit: weeks\n      unit_count: 2\nand adding an exclude filter solved it.\nfilters:\n    - filtertype: pattern\n      kind: timestring\n      value: '%Y.%m.%d'\n      exclude: True\n    - filtertype: age\n      source: name\n      direction: older\n      timestring: '%Y.%W'\n      unit: weeks\n      unit_count: 2\nWhile there is a workaround, it sounds like a bug to me. '%Y.%W' should not match indices with '%Y.%m.%d'.\n@untergeek are you open for discussion on this one? \nI'm curious to understand why this happens. Is '%Y.%W' translated to a regexp without an $ at the end? \n. Thanks for the explanation, I didn't think about the \"date somewhere else then the end of the index name\" cases. . Thanks for looking into this @untergeek . I did test this again and the alias exits, but the same error is thrown, which makes me think the error is misleading. \n```\nroot@curator-20161229-095124trusty-64-jre-1:~# curl \"10.52.101.1:9200/_cat/aliases\"\ntoday logstash-2017.02.21 - - - \ntoday logstash-2017.02.24 - - - \nroot@curator-20161229-095124trusty-64-jre-1:~# curator test2.yml \n2017-02-24 17:09:46,781 DEBUG                curator.cli                    cli:117  Client and logging options validated.\n2017-02-24 17:09:46,781 DEBUG                curator.cli                    cli:121  default_timeout = 30\n2017-02-24 17:09:46,785 DEBUG     curator.validators.SchemaCheck               init:26   Schema: }, extra=PREVENT_EXTRA, required=False) object at 0x7f8fbbbe84a8>\n2017-02-24 17:09:46,785 DEBUG     curator.validators.SchemaCheck               init:27   \"Actions File\" config: {'actions': {'alias-today-remove': {'options': {'ignore_empty_list': True, 'name': 'today'}, 'action': 'alias', 'description': 'Remove indexes from today alias', 'remove': {'filters': [{'kind': 'prefix', 'filtertype': 'pattern', 'value': 'logstash-'}, {'unit': 'days', 'timestring': '%Y.%m.%d', 'unit_count': 1, 'direction': 'older', 'filtertype': 'age', 'source': 'name'}]}}}}\n2017-02-24 17:09:46,785 DEBUG     curator.validators.SchemaCheck               init:26   Schema: \n2017-02-24 17:09:46,786 DEBUG     curator.validators.SchemaCheck               init:27   \"action type\" config: {'options': {'ignore_empty_list': True, 'name': 'today'}, 'action': 'alias', 'description': 'Remove indexes from today alias', 'remove': {'filters': [{'kind': 'prefix', 'filtertype': 'pattern', 'value': 'logstash-'}, {'unit': 'days', 'timestring': '%Y.%m.%d', 'unit_count': 1, 'direction': 'older', 'filtertype': 'age', 'source': 'name'}]}}\n2017-02-24 17:09:46,786 DEBUG     curator.validators.SchemaCheck               init:26   Schema: , 'options': , 'description': Any([, ]), 'action': Any([In(['alias', 'allocation', 'close', 'cluster_routing', 'create_index', 'delete_indices', 'delete_snapshots', 'forcemerge', 'open', 'replicas', 'restore', 'snapshot'])]), 'remove': }, extra=PREVENT_EXTRA, required=False) object at 0x7f8fb6d61e10>\n2017-02-24 17:09:46,786 DEBUG     curator.validators.SchemaCheck               init:27   \"structure\" config: {'options': {'ignore_empty_list': True, 'name': 'today'}, 'action': 'alias', 'description': 'Remove indexes from today alias', 'remove': {'filters': [{'kind': 'prefix', 'filtertype': 'pattern', 'value': 'logstash-'}, {'unit': 'days', 'timestring': '%Y.%m.%d', 'unit_count': 1, 'direction': 'older', 'filtertype': 'age', 'source': 'name'}]}}\n2017-02-24 17:09:46,788 DEBUG     curator.validators.SchemaCheck               init:26   Schema: , ]), 'timeout_override': Any([Coerce(int, msg=None), None]), 'disable_action': , 'continue_if_exception': , 'ignore_empty_list': , 'extra_settings': }, extra=PREVENT_EXTRA, required=False) object at 0x7f8fb6d6e3c8>\n2017-02-24 17:09:46,788 DEBUG     curator.validators.SchemaCheck               init:27   \"options\" config: {'ignore_empty_list': True, 'name': 'today'}\n2017-02-24 17:09:46,788 DEBUG     curator.validators.SchemaCheck               init:26   Schema: .f at 0x7f8fb6d72ae8>, extra=PREVENT_EXTRA, required=False) object at 0x7f8fb6d6e198>\n2017-02-24 17:09:46,788 DEBUG     curator.validators.SchemaCheck               init:27   \"\"{0}\" filters\" config: [{'kind': 'prefix', 'filtertype': 'pattern', 'value': 'logstash-'}, {'unit': 'days', 'timestring': '%Y.%m.%d', 'unit_count': 1, 'direction': 'older', 'filtertype': 'age', 'source': 'name'}]\n2017-02-24 17:09:46,788 DEBUG     curator.validators.SchemaCheck               init:26   Schema: , 'filtertype': Any([In(['age', 'alias', 'allocated', 'closed', 'count', 'forcemerged', 'kibana', 'none', 'opened', 'pattern', 'space', 'state'])]), 'value': Any([, ])}, extra=PREVENT_EXTRA, required=False) object at 0x7f8fb6d6ec88>\n2017-02-24 17:09:46,789 DEBUG     curator.validators.SchemaCheck               init:27   \"filter\" config: {'kind': 'prefix', 'filtertype': 'pattern', 'value': 'logstash-'}\n2017-02-24 17:09:46,790 DEBUG     curator.validators.filters                      f:69   Filter #0: {'exclude': False, 'kind': 'prefix', 'filtertype': 'pattern', 'value': 'logstash-'}\n2017-02-24 17:09:46,794 DEBUG     curator.validators.SchemaCheck               init:26   Schema: , 'epoch': Any([Coerce(int, msg=None), None]), 'source': Any(['name', 'creation_date', 'field_stats']), 'timestring': Any([, ]), 'unit': Any(['seconds', 'minutes', 'hours', 'days', 'weeks', 'months', 'years']), 'unit_count': Coerce(int, msg=None), 'direction': Any(['older', 'younger'])}, extra=PREVENT_EXTRA, required=False) object at 0x7f8fb6dd1748>\n2017-02-24 17:09:46,795 DEBUG     curator.validators.SchemaCheck               init:27   \"filter\" config: {'unit': 'days', 'timestring': '%Y.%m.%d', 'unit_count': 1, 'direction': 'older', 'filtertype': 'age', 'source': 'name'}\n2017-02-24 17:09:46,797 DEBUG     curator.validators.filters                      f:69   Filter #1: {'unit': 'days', 'timestring': '%Y.%m.%d', 'stats_result': 'min_value', 'unit_count': 1, 'direction': 'older', 'exclude': False, 'filtertype': 'age', 'epoch': None, 'source': 'name'}\n2017-02-24 17:09:46,798 DEBUG     curator.validators.SchemaCheck               init:26   Schema: .f at 0x7f8fb6d86510>, extra=PREVENT_EXTRA, required=False) object at 0x7f8fb6de58d0>\n2017-02-24 17:09:46,798 DEBUG     curator.validators.SchemaCheck               init:27   \"filters\" config: [{'exclude': False, 'kind': 'prefix', 'filtertype': 'pattern', 'value': 'logstash-'}, {'unit': 'days', 'timestring': '%Y.%m.%d', 'stats_result': 'min_value', 'unit_count': 1, 'direction': 'older', 'exclude': False, 'filtertype': 'age', 'epoch': None, 'source': 'name'}]\n2017-02-24 17:09:46,798 DEBUG     curator.validators.SchemaCheck               init:26   Schema: , ]), 'kind': Any(['prefix', 'suffix', 'timestring', 'regex']), 'exclude': , 'filtertype': Any([In(['age', 'alias', 'allocated', 'closed', 'count', 'forcemerged', 'kibana', 'none', 'opened', 'pattern', 'space', 'state'])])}, extra=PREVENT_EXTRA, required=False) object at 0x7f8fba1f1d30>\n2017-02-24 17:09:46,799 DEBUG     curator.validators.SchemaCheck               init:27   \"filter\" config: {'exclude': False, 'kind': 'prefix', 'filtertype': 'pattern', 'value': 'logstash-'}\n2017-02-24 17:09:46,799 DEBUG     curator.validators.filters                      f:69   Filter #0: {'exclude': False, 'kind': 'prefix', 'filtertype': 'pattern', 'value': 'logstash-'}\n2017-02-24 17:09:46,801 DEBUG     curator.validators.SchemaCheck               init:26   Schema: , ]), 'filtertype': Any([In(['age', 'alias', 'allocated', 'closed', 'count', 'forcemerged', 'kibana', 'none', 'opened', 'pattern', 'space', 'state'])]), 'exclude': , 'direction': Any(['older', 'younger']), 'epoch': Any([Coerce(int, msg=None), None])}, extra=PREVENT_EXTRA, required=False) object at 0x7f8fb6d73eb8>\n2017-02-24 17:09:46,801 DEBUG     curator.validators.SchemaCheck               init:27   \"filter\" config: {'unit': 'days', 'unit_count': 1, 'stats_result': 'min_value', 'direction': 'older', 'exclude': False, 'filtertype': 'age', 'timestring': '%Y.%m.%d', 'source': 'name'}\n2017-02-24 17:09:46,802 DEBUG     curator.validators.filters                      f:69   Filter #1: {'unit': 'days', 'filtertype': 'age', 'stats_result': 'min_value', 'direction': 'older', 'exclude': False, 'timestring': '%Y.%m.%d', 'unit_count': 1, 'epoch': None, 'source': 'name'}\n2017-02-24 17:09:46,803 DEBUG                curator.cli                    cli:128  Full list of actions: {'alias-today-remove': {'options': {'extra_settings': {}, 'name': 'today', 'timeout_override': None, 'ignore_empty_list': True, 'disable_action': False, 'continue_if_exception': False}, 'action': 'alias', 'description': 'Remove indexes from today alias', 'remove': {'filters': [{'exclude': False, 'kind': 'prefix', 'filtertype': 'pattern', 'value': 'logstash-'}, {'unit': 'days', 'filtertype': 'age', 'stats_result': 'min_value', 'direction': 'older', 'exclude': False, 'timestring': '%Y.%m.%d', 'unit_count': 1, 'epoch': None, 'source': 'name'}]}}}\n2017-02-24 17:09:46,803 DEBUG                curator.cli                    cli:133  action_disabled = False\n2017-02-24 17:09:46,803 DEBUG                curator.cli                    cli:137  continue_if_exception = False\n2017-02-24 17:09:46,803 DEBUG                curator.cli                    cli:139  timeout_override = None\n2017-02-24 17:09:46,803 DEBUG                curator.cli                    cli:141  ignore_empty_list = True\n2017-02-24 17:09:46,803 INFO                 curator.cli                    cli:151  Preparing Action ID: alias-today-remove, \"alias\"\n2017-02-24 17:09:46,804 DEBUG              curator.utils             get_client:535  kwargs = {'port': 9200, 'client_cert': None, 'use_ssl': False, 'master_only': False, 'url_prefix': '', 'aws_secret_key': None, 'aws_region': None, 'certificate': None, 'timeout': 30, 'http_auth': None, 'client_key': None, 'ssl_no_validate': False, 'hosts': ['10.52.101.1'], 'aws_key': None}\n2017-02-24 17:09:46,826 DEBUG              curator.utils             get_client:583  \"requests_aws4auth\" module present, but not used.\n2017-02-24 17:09:46,840 DEBUG              curator.utils          check_version:438  Detected Elasticsearch version 2.4.2\n2017-02-24 17:09:46,840 DEBUG                curator.cli                    cli:167  client is \n2017-02-24 17:09:46,840 INFO                 curator.cli                    cli:173  Trying Action ID: alias-today-remove, \"alias\": Remove indexes from today alias\n2017-02-24 17:09:46,840 DEBUG                curator.cli         process_action:40   Configuration dictionary: {'options': {'extra_settings': {}, 'name': 'today'}, 'action': 'alias', 'description': 'Remove indexes from today alias', 'remove': {'filters': [{'exclude': False, 'kind': 'prefix', 'filtertype': 'pattern', 'value': 'logstash-'}, {'unit': 'days', 'filtertype': 'age', 'stats_result': 'min_value', 'direction': 'older', 'exclude': False, 'timestring': '%Y.%m.%d', 'unit_count': 1, 'epoch': None, 'source': 'name'}]}}\n2017-02-24 17:09:46,840 DEBUG                curator.cli         process_action:41   kwargs: {'timeout': 30, 'dry_run': False, 'master_timeout': 30}\n2017-02-24 17:09:46,840 DEBUG                curator.cli         process_action:46   opts: {'extra_settings': {}, 'name': 'today'}\n2017-02-24 17:09:46,840 DEBUG                curator.cli         process_action:62   Action kwargs: {'extra_settings': {}, 'name': 'today'}\n2017-02-24 17:09:46,840 DEBUG                curator.cli         process_action:67   Running \"ALIAS\" action\n2017-02-24 17:09:46,841 DEBUG              curator.utils     parse_date_pattern:1032 Partially rendered name: t\n2017-02-24 17:09:46,841 DEBUG              curator.utils     parse_date_pattern:1032 Partially rendered name: to\n2017-02-24 17:09:46,841 DEBUG              curator.utils     parse_date_pattern:1032 Partially rendered name: tod\n2017-02-24 17:09:46,841 DEBUG              curator.utils     parse_date_pattern:1032 Partially rendered name: toda\n2017-02-24 17:09:46,841 DEBUG              curator.utils     parse_date_pattern:1032 Partially rendered name: today\n2017-02-24 17:09:46,841 DEBUG              curator.utils     parse_date_pattern:1034 Fully rendered name: today\n2017-02-24 17:09:46,841 DEBUG                curator.cli         process_action:76   Removing indices from alias \"today\"\n2017-02-24 17:09:46,841 DEBUG          curator.indexlist          get_indices:65   Getting all indices\n2017-02-24 17:09:46,851 DEBUG              curator.utils            get_indices:380  Detected Elasticsearch version 2.4.2\n2017-02-24 17:09:46,851 DEBUG              curator.utils            get_indices:386  Using Elasticsearch >= 2.4.2 < 5.0.0\n2017-02-24 17:09:46,857 DEBUG              curator.utils            get_indices:395  All indices: ['logstash-2017.02.23', 'logstash-2017.02.21', 'logstash-2017.02.22', 'logstash-2017.02.24', 'logstash-2017.02.14']\n2017-02-24 17:09:46,857 DEBUG          curator.indexlist       empty_list_check:182  Checking for empty list\n2017-02-24 17:09:46,858 DEBUG          curator.indexlist     __build_index_info:80   Building preliminary index metadata for logstash-2017.02.23\n2017-02-24 17:09:46,859 DEBUG          curator.indexlist     __build_index_info:80   Building preliminary index metadata for logstash-2017.02.21\n2017-02-24 17:09:46,861 DEBUG          curator.indexlist     __build_index_info:80   Building preliminary index metadata for logstash-2017.02.22\n2017-02-24 17:09:46,861 DEBUG          curator.indexlist     __build_index_info:80   Building preliminary index metadata for logstash-2017.02.24\n2017-02-24 17:09:46,861 DEBUG          curator.indexlist     __build_index_info:80   Building preliminary index metadata for logstash-2017.02.14\n2017-02-24 17:09:46,861 DEBUG          curator.indexlist          _get_metadata:145  Getting index metadata\n2017-02-24 17:09:46,862 DEBUG          curator.indexlist       empty_list_check:182  Checking for empty list\n2017-02-24 17:09:46,887 DEBUG          curator.indexlist       _get_index_stats:113  Getting index stats\n2017-02-24 17:09:46,888 DEBUG          curator.indexlist       empty_list_check:182  Checking for empty list\n2017-02-24 17:09:46,888 DEBUG          curator.indexlist           working_list:193  Generating working list of indices\n2017-02-24 17:09:46,888 DEBUG          curator.indexlist           working_list:193  Generating working list of indices\n2017-02-24 17:09:46,899 DEBUG          curator.indexlist     iterate_over_stats:122  Index: logstash-2017.02.24  Size: 3.0KB  Docs: 1\n2017-02-24 17:09:46,902 DEBUG          curator.indexlist     iterate_over_stats:122  Index: logstash-2017.02.14  Size: 3.0KB  Docs: 1\n2017-02-24 17:09:46,903 DEBUG          curator.indexlist     iterate_over_stats:122  Index: logstash-2017.02.22  Size: 3.0KB  Docs: 1\n2017-02-24 17:09:46,904 DEBUG          curator.indexlist     iterate_over_stats:122  Index: logstash-2017.02.21  Size: 3.0KB  Docs: 1\n2017-02-24 17:09:46,905 DEBUG          curator.indexlist     iterate_over_stats:122  Index: logstash-2017.02.23  Size: 3.0KB  Docs: 1\n2017-02-24 17:09:46,905 DEBUG          curator.indexlist        iterate_filters:819  Iterating over a list of filters\n2017-02-24 17:09:46,905 DEBUG          curator.indexlist        iterate_filters:825  All filters: [{'exclude': False, 'kind': 'prefix', 'filtertype': 'pattern', 'value': 'logstash-'}, {'unit': 'days', 'filtertype': 'age', 'stats_result': 'min_value', 'direction': 'older', 'exclude': False, 'timestring': '%Y.%m.%d', 'unit_count': 1, 'epoch': None, 'source': 'name'}]\n2017-02-24 17:09:46,905 DEBUG          curator.indexlist        iterate_filters:827  Top of the loop: ['logstash-2017.02.23', 'logstash-2017.02.21', 'logstash-2017.02.22', 'logstash-2017.02.24', 'logstash-2017.02.14']\n2017-02-24 17:09:46,905 DEBUG          curator.indexlist        iterate_filters:828  Un-parsed filter args: {'exclude': False, 'kind': 'prefix', 'filtertype': 'pattern', 'value': 'logstash-'}\n2017-02-24 17:09:46,907 DEBUG     curator.validators.SchemaCheck               __init:26   Schema: , [], , []]), 'source': Any([, ]), 'field': Any([, , None]), 'allocation_type': Any([, ]), 'unit_count': Coerce(int, msg=None), 'state': Any([, ]), 'filtertype': Any([In(['age', 'alias', 'allocated', 'closed', 'count', 'forcemerged', 'kibana', 'none', 'opened', 'pattern', 'space', 'state'])]), 'timestring': Any([, , None]), 'epoch': Any([Coerce(int, msg=None), None]), 'stats_result': Any([, , None]), 'exclude': Any([, , , , None]), 'direction': Any([, ]), 'unit': Any([, ]), 'value': Any([, , , , ]), 'disk_space': , 'use_age': , 'reverse': Any([, , , , None]), 'key': Any([, ]), 'max_num_segments': Coerce(int, msg=None), 'kind': Any([, ])}, extra=PREVENT_EXTRA, required=False) object at 0x7f8fb6d34a58>\n2017-02-24 17:09:46,907 DEBUG     curator.validators.SchemaCheck               init:27   \"filter\" config: {'exclude': False, 'kind': 'prefix', 'filtertype': 'pattern', 'value': 'logstash-'}\n2017-02-24 17:09:46,909 DEBUG          curator.indexlist        iterate_filters:835  Parsed filter args: {'exclude': False, 'kind': 'prefix', 'filtertype': 'pattern', 'value': 'logstash-'}\n2017-02-24 17:09:46,909 DEBUG              curator.utils        iterate_filters:844  Filter args: {'exclude': False, 'kind': 'prefix', 'value': 'logstash-'}\n2017-02-24 17:09:46,910 DEBUG              curator.utils        iterate_filters:845  Pre-instance: ['logstash-2017.02.23', 'logstash-2017.02.21', 'logstash-2017.02.22', 'logstash-2017.02.24', 'logstash-2017.02.14']\n2017-02-24 17:09:46,910 DEBUG          curator.indexlist        filter_by_regex:362  Filtering indices by regex\n2017-02-24 17:09:46,910 DEBUG          curator.indexlist       empty_list_check:182  Checking for empty list\n2017-02-24 17:09:46,911 DEBUG          curator.indexlist           working_list:193  Generating working list of indices\n2017-02-24 17:09:46,911 DEBUG          curator.indexlist        filter_by_regex:383  Filter by regex: Index: logstash-2017.02.23\n2017-02-24 17:09:46,911 DEBUG          curator.indexlist           actionable:35   Index logstash-2017.02.23 is actionable and remains in the list.\n2017-02-24 17:09:46,911 DEBUG          curator.indexlist        filter_by_regex:383  Filter by regex: Index: logstash-2017.02.21\n2017-02-24 17:09:46,912 DEBUG          curator.indexlist           __actionable:35   Index logstash-2017.02.21 is actionable and remains in the list.\n2017-02-24 17:09:46,912 DEBUG          curator.indexlist        filter_by_regex:383  Filter by regex: Index: logstash-2017.02.22\n2017-02-24 17:09:46,912 DEBUG          curator.indexlist           __actionable:35   Index logstash-2017.02.22 is actionable and remains in the list.\n2017-02-24 17:09:46,912 DEBUG          curator.indexlist        filter_by_regex:383  Filter by regex: Index: logstash-2017.02.24\n2017-02-24 17:09:46,912 DEBUG          curator.indexlist           __actionable:35   Index logstash-2017.02.24 is actionable and remains in the list.\n2017-02-24 17:09:46,912 DEBUG          curator.indexlist        filter_by_regex:383  Filter by regex: Index: logstash-2017.02.14\n2017-02-24 17:09:46,912 DEBUG          curator.indexlist           __actionable:35   Index logstash-2017.02.14 is actionable and remains in the list.\n2017-02-24 17:09:46,913 DEBUG              curator.utils        iterate_filters:847  Post-instance: ['logstash-2017.02.23', 'logstash-2017.02.21', 'logstash-2017.02.22', 'logstash-2017.02.24', 'logstash-2017.02.14']\n2017-02-24 17:09:46,913 DEBUG          curator.indexlist        iterate_filters:827  Top of the loop: ['logstash-2017.02.23', 'logstash-2017.02.21', 'logstash-2017.02.22', 'logstash-2017.02.24', 'logstash-2017.02.14']\n2017-02-24 17:09:46,913 DEBUG          curator.indexlist        iterate_filters:828  Un-parsed filter args: {'unit': 'days', 'filtertype': 'age', 'stats_result': 'min_value', 'direction': 'older', 'exclude': False, 'timestring': '%Y.%m.%d', 'unit_count': 1, 'epoch': None, 'source': 'name'}\n2017-02-24 17:09:46,917 DEBUG     curator.validators.SchemaCheck               __init:26   Schema: , , , , None]), 'filtertype': Any([In(['age', 'alias', 'allocated', 'closed', 'count', 'forcemerged', 'kibana', 'none', 'opened', 'pattern', 'space', 'state'])]), 'timestring': Any([, , None]), 'max_num_segments': Coerce(int, msg=None), 'epoch': Any([Coerce(int, msg=None), None]), 'count': Coerce(int, msg=None), 'source': Any([, ]), 'allocation_type': Any([, ]), 'disk_space': , 'unit': Any([, ]), 'kind': Any([, ]), 'use_age': , 'unit_count': Coerce(int, msg=None), 'stats_result': Any([, , None]), 'direction': Any([, ]), 'aliases': Any([, [], , []]), 'key': Any([, ]), 'value': Any([, , , , ]), 'exclude': Any([, , , , None]), 'field': Any([, , None]), 'state': Any([, ])}, extra=PREVENT_EXTRA, required=False) object at 0x7f8fb6d3b4a8>\n2017-02-24 17:09:46,917 DEBUG     curator.validators.SchemaCheck               init:27   \"filter\" config: {'unit': 'days', 'filtertype': 'age', 'stats_result': 'min_value', 'direction': 'older', 'exclude': False, 'timestring': '%Y.%m.%d', 'unit_count': 1, 'epoch': None, 'source': 'name'}\n2017-02-24 17:09:46,922 DEBUG          curator.indexlist        iterate_filters:835  Parsed filter args: {'unit': 'days', 'timestring': '%Y.%m.%d', 'stats_result': 'min_value', 'unit_count': 1, 'direction': 'older', 'exclude': False, 'filtertype': 'age', 'epoch': None, 'source': 'name'}\n2017-02-24 17:09:46,922 DEBUG              curator.utils        iterate_filters:844  Filter args: {'unit': 'days', 'stats_result': 'min_value', 'direction': 'older', 'exclude': False, 'timestring': '%Y.%m.%d', 'unit_count': 1, 'epoch': None, 'source': 'name'}\n2017-02-24 17:09:46,923 DEBUG              curator.utils        iterate_filters:845  Pre-instance: ['logstash-2017.02.23', 'logstash-2017.02.21', 'logstash-2017.02.22', 'logstash-2017.02.24', 'logstash-2017.02.14']\n2017-02-24 17:09:46,924 DEBUG          curator.indexlist          filter_by_age:420  Filtering indices by age\n2017-02-24 17:09:46,925 DEBUG          curator.indexlist   _get_name_based_ages:227  Getting ages of indices by \"name\"\n2017-02-24 17:09:46,926 DEBUG          curator.indexlist       empty_list_check:182  Checking for empty list\n2017-02-24 17:09:46,927 DEBUG              curator.utils         get_date_regex:153  regex = \\d{4}.\\d{2}.\\d{2}\n2017-02-24 17:09:46,929 DEBUG          curator.indexlist           working_list:193  Generating working list of indices\n2017-02-24 17:09:46,934 DEBUG          curator.indexlist           working_list:193  Generating working list of indices\n2017-02-24 17:09:46,934 DEBUG          curator.indexlist           __actionable:35   Index logstash-2017.02.23 is actionable and remains in the list.\n2017-02-24 17:09:46,935 DEBUG          curator.indexlist            __excludify:58   Remains in actionable list: Index \"logstash-2017.02.23\" age (1487808000), direction: \"older\", point of reference, (1487869786)\n2017-02-24 17:09:46,935 DEBUG          curator.indexlist           __actionable:35   Index logstash-2017.02.21 is actionable and remains in the list.\n2017-02-24 17:09:46,935 DEBUG          curator.indexlist            __excludify:58   Remains in actionable list: Index \"logstash-2017.02.21\" age (1487635200), direction: \"older\", point of reference, (1487869786)\n2017-02-24 17:09:46,935 DEBUG          curator.indexlist           __actionable:35   Index logstash-2017.02.22 is actionable and remains in the list.\n2017-02-24 17:09:46,935 DEBUG          curator.indexlist            __excludify:58   Remains in actionable list: Index \"logstash-2017.02.22\" age (1487721600), direction: \"older\", point of reference, (1487869786)\n2017-02-24 17:09:46,935 DEBUG          curator.indexlist       __not_actionable:39   Index logstash-2017.02.24 is not actionable, removing from list.\n2017-02-24 17:09:46,935 DEBUG          curator.indexlist            __excludify:58   Removed from actionable list: Index \"logstash-2017.02.24\" age (1487894400), direction: \"older\", point of reference, (1487869786)\n2017-02-24 17:09:46,935 DEBUG          curator.indexlist           __actionable:35   Index logstash-2017.02.14 is actionable and remains in the list.\n2017-02-24 17:09:46,935 DEBUG          curator.indexlist            __excludify:58   Remains in actionable list: Index \"logstash-2017.02.14\" age (1487030400), direction: \"older\", point of reference, (1487869786)\n2017-02-24 17:09:46,936 DEBUG              curator.utils        iterate_filters:847  Post-instance: ['logstash-2017.02.23', 'logstash-2017.02.21', 'logstash-2017.02.22', 'logstash-2017.02.14']\n2017-02-24 17:09:46,936 DEBUG          curator.indexlist       empty_list_check:182  Checking for empty list\n2017-02-24 17:09:46,936 DEBUG          curator.indexlist           working_list:193  Generating working list of indices\n2017-02-24 17:09:46,936 DEBUG      curator.actions.alias                 remove:69   Removing index logstash-2017.02.23 from alias today\n2017-02-24 17:09:46,936 DEBUG      curator.actions.alias                 remove:69   Removing index logstash-2017.02.21 from alias today\n2017-02-24 17:09:46,936 DEBUG      curator.actions.alias                 remove:69   Removing index logstash-2017.02.22 from alias today\n2017-02-24 17:09:46,936 DEBUG      curator.actions.alias                 remove:69   Removing index logstash-2017.02.14 from alias today\n2017-02-24 17:09:46,936 DEBUG                curator.cli         process_action:98   Doing the action here.\n2017-02-24 17:09:46,937 INFO       curator.actions.alias              do_action:109  Updating aliases...\n2017-02-24 17:09:46,937 DEBUG      curator.actions.alias                   body:80   Alias actions: [{'remove': {'alias': 'today', 'index': 'logstash-2017.02.23'}}, {'remove': {'alias': 'today', 'index': 'logstash-2017.02.21'}}, {'remove': {'alias': 'today', 'index': 'logstash-2017.02.22'}}, {'remove': {'alias': 'today', 'index': 'logstash-2017.02.14'}}]\n2017-02-24 17:09:46,937 INFO       curator.actions.alias              do_action:110  Alias actions: {'actions': [{'remove': {'alias': 'today', 'index': 'logstash-2017.02.23'}}, {'remove': {'alias': 'today', 'index': 'logstash-2017.02.21'}}, {'remove': {'alias': 'today', 'index': 'logstash-2017.02.22'}}, {'remove': {'alias': 'today', 'index': 'logstash-2017.02.14'}}]}\n2017-02-24 17:09:46,937 DEBUG      curator.actions.alias                   body:80   Alias actions: [{'remove': {'alias': 'today', 'index': 'logstash-2017.02.23'}}, {'remove': {'alias': 'today', 'index': 'logstash-2017.02.21'}}, {'remove': {'alias': 'today', 'index': 'logstash-2017.02.22'}}, {'remove': {'alias': 'today', 'index': 'logstash-2017.02.14'}}]\n2017-02-24 17:09:46,965 INFO                 curator.cli                    cli:203  Action ID: alias-today-remove, \"alias\" completed.\n2017-02-24 17:09:46,965 INFO                 curator.cli                    cli:204  Job completed.\nroot@curator-20161229-095124trusty-64-jre-1:~# curl \"10.52.101.1:9200/_cat/aliases\"\ntoday logstash-2017.02.24 - - - \nroot@curator-20161229-095124trusty-64-jre-1:~# curator test2.yml \n2017-02-24 17:09:54,497 DEBUG                curator.cli                    cli:117  Client and logging options validated.\n2017-02-24 17:09:54,497 DEBUG                curator.cli                    cli:121  default_timeout = 30\n2017-02-24 17:09:54,509 DEBUG     curator.validators.SchemaCheck               init:26   Schema: }, extra=PREVENT_EXTRA, required=False) object at 0x7fdd70f3e4a8>\n2017-02-24 17:09:54,509 DEBUG     curator.validators.SchemaCheck               init:27   \"Actions File\" config: {'actions': {'alias-today-remove': {'description': 'Remove indexes from today alias', 'options': {'name': 'today', 'ignore_empty_list': True}, 'remove': {'filters': [{'filtertype': 'pattern', 'kind': 'prefix', 'value': 'logstash-'}, {'filtertype': 'age', 'source': 'name', 'timestring': '%Y.%m.%d', 'unit_count': 1, 'direction': 'older', 'unit': 'days'}]}, 'action': 'alias'}}}\n2017-02-24 17:09:54,510 DEBUG     curator.validators.SchemaCheck               init:26   Schema: \n2017-02-24 17:09:54,510 DEBUG     curator.validators.SchemaCheck               init:27   \"action type\" config: {'description': 'Remove indexes from today alias', 'options': {'name': 'today', 'ignore_empty_list': True}, 'remove': {'filters': [{'filtertype': 'pattern', 'kind': 'prefix', 'value': 'logstash-'}, {'filtertype': 'age', 'source': 'name', 'timestring': '%Y.%m.%d', 'unit_count': 1, 'direction': 'older', 'unit': 'days'}]}, 'action': 'alias'}\n2017-02-24 17:09:54,511 DEBUG     curator.validators.SchemaCheck               init:26   Schema: , 'action': Any([In(['alias', 'allocation', 'close', 'cluster_routing', 'create_index', 'delete_indices', 'delete_snapshots', 'forcemerge', 'open', 'replicas', 'restore', 'snapshot'])]), 'options': , 'remove': , 'description': Any([, ])}, extra=PREVENT_EXTRA, required=False) object at 0x7fdd6c127668>\n2017-02-24 17:09:54,511 DEBUG     curator.validators.SchemaCheck               init:27   \"structure\" config: {'description': 'Remove indexes from today alias', 'options': {'name': 'today', 'ignore_empty_list': True}, 'remove': {'filters': [{'filtertype': 'pattern', 'kind': 'prefix', 'value': 'logstash-'}, {'filtertype': 'age', 'source': 'name', 'timestring': '%Y.%m.%d', 'unit_count': 1, 'direction': 'older', 'unit': 'days'}]}, 'action': 'alias'}\n2017-02-24 17:09:54,514 DEBUG     curator.validators.SchemaCheck               init:26   Schema: , 'timeout_override': Any([Coerce(int, msg=None), None]), 'extra_settings': , 'name': Any([, ]), 'ignore_empty_list': , 'continue_if_exception': }, extra=PREVENT_EXTRA, required=False) object at 0x7fdd6c0c9fd0>\n2017-02-24 17:09:54,514 DEBUG     curator.validators.SchemaCheck               init:27   \"options\" config: {'name': 'today', 'ignore_empty_list': True}\n2017-02-24 17:09:54,514 DEBUG     curator.validators.SchemaCheck               init:26   Schema: .f at 0x7fdd6c0ca048>, extra=PREVENT_EXTRA, required=False) object at 0x7fdd6c0c98d0>\n2017-02-24 17:09:54,515 DEBUG     curator.validators.SchemaCheck               init:27   \"\"{0}\" filters\" config: [{'filtertype': 'pattern', 'kind': 'prefix', 'value': 'logstash-'}, {'filtertype': 'age', 'source': 'name', 'timestring': '%Y.%m.%d', 'unit_count': 1, 'direction': 'older', 'unit': 'days'}]\n2017-02-24 17:09:54,515 DEBUG     curator.validators.SchemaCheck               init:26   Schema: , 'kind': Any(['prefix', 'suffix', 'timestring', 'regex']), 'filtertype': Any([In(['age', 'alias', 'allocated', 'closed', 'count', 'forcemerged', 'kibana', 'none', 'opened', 'pattern', 'space', 'state'])]), 'value': Any([, ])}, extra=PREVENT_EXTRA, required=False) object at 0x7fdd6c127780>\n2017-02-24 17:09:54,515 DEBUG     curator.validators.SchemaCheck               init:27   \"filter\" config: {'filtertype': 'pattern', 'value': 'logstash-', 'kind': 'prefix'}\n2017-02-24 17:09:54,515 DEBUG     curator.validators.filters                      f:69   Filter #0: {'filtertype': 'pattern', 'exclude': False, 'kind': 'prefix', 'value': 'logstash-'}\n2017-02-24 17:09:54,516 DEBUG     curator.validators.SchemaCheck               init:26   Schema: , ]), 'unit': Any(['seconds', 'minutes', 'hours', 'days', 'weeks', 'months', 'years']), 'exclude': , 'filtertype': Any([In(['age', 'alias', 'allocated', 'closed', 'count', 'forcemerged', 'kibana', 'none', 'opened', 'pattern', 'space', 'state'])]), 'unit_count': Coerce(int, msg=None), 'direction': Any(['older', 'younger']), 'source': Any(['name', 'creation_date', 'field_stats']), 'stats_result': Any(['min_value', 'max_value']), 'epoch': Any([Coerce(int, msg=None), None])}, extra=PREVENT_EXTRA, required=False) object at 0x7fdd6c0c4cf8>\n2017-02-24 17:09:54,516 DEBUG     curator.validators.SchemaCheck               init:27   \"filter\" config: {'filtertype': 'age', 'source': 'name', 'timestring': '%Y.%m.%d', 'unit_count': 1, 'direction': 'older', 'unit': 'days'}\n2017-02-24 17:09:54,516 DEBUG     curator.validators.filters                      f:69   Filter #1: {'filtertype': 'age', 'source': 'name', 'stats_result': 'min_value', 'exclude': False, 'timestring': '%Y.%m.%d', 'epoch': None, 'unit_count': 1, 'direction': 'older', 'unit': 'days'}\n2017-02-24 17:09:54,517 DEBUG     curator.validators.SchemaCheck               init:26   Schema: .f at 0x7fdd6c0c16a8>, extra=PREVENT_EXTRA, required=False) object at 0x7fdd6c0c43c8>\n2017-02-24 17:09:54,517 DEBUG     curator.validators.SchemaCheck               init:27   \"filters\" config: [{'filtertype': 'pattern', 'exclude': False, 'kind': 'prefix', 'value': 'logstash-'}, {'filtertype': 'age', 'source': 'name', 'stats_result': 'min_value', 'exclude': False, 'timestring': '%Y.%m.%d', 'epoch': None, 'unit_count': 1, 'direction': 'older', 'unit': 'days'}]\n2017-02-24 17:09:54,517 DEBUG     curator.validators.SchemaCheck               init:26   Schema: , 'value': Any([, ]), 'filtertype': Any([In(['age', 'alias', 'allocated', 'closed', 'count', 'forcemerged', 'kibana', 'none', 'opened', 'pattern', 'space', 'state'])])}, extra=PREVENT_EXTRA, required=False) object at 0x7fdd6c0c4748>\n2017-02-24 17:09:54,517 DEBUG     curator.validators.SchemaCheck               init:27   \"filter\" config: {'filtertype': 'pattern', 'exclude': False, 'value': 'logstash-', 'kind': 'prefix'}\n2017-02-24 17:09:54,517 DEBUG     curator.validators.filters                      f:69   Filter #0: {'filtertype': 'pattern', 'exclude': False, 'kind': 'prefix', 'value': 'logstash-'}\n2017-02-24 17:09:54,519 DEBUG     curator.validators.SchemaCheck               init:26   Schema: , ]), 'exclude': , 'unit_count': Coerce(int, msg=None), 'epoch': Any([Coerce(int, msg=None), None]), 'filtertype': Any([In(['age', 'alias', 'allocated', 'closed', 'count', 'forcemerged', 'kibana', 'none', 'opened', 'pattern', 'space', 'state'])]), 'stats_result': Any(['min_value', 'max_value']), 'unit': Any(['seconds', 'minutes', 'hours', 'days', 'weeks', 'months', 'years']), 'direction': Any(['older', 'younger'])}, extra=PREVENT_EXTRA, required=False) object at 0x7fdd6c0c47f0>\n2017-02-24 17:09:54,520 DEBUG     curator.validators.SchemaCheck               init:27   \"filter\" config: {'filtertype': 'age', 'source': 'name', 'stats_result': 'min_value', 'unit_count': 1, 'timestring': '%Y.%m.%d', 'exclude': False, 'direction': 'older', 'unit': 'days'}\n2017-02-24 17:09:54,521 DEBUG     curator.validators.filters                      f:69   Filter #1: {'filtertype': 'age', 'source': 'name', 'stats_result': 'min_value', 'exclude': False, 'timestring': '%Y.%m.%d', 'epoch': None, 'unit_count': 1, 'direction': 'older', 'unit': 'days'}\n2017-02-24 17:09:54,521 DEBUG                curator.cli                    cli:128  Full list of actions: {'alias-today-remove': {'description': 'Remove indexes from today alias', 'options': {'ignore_empty_list': True, 'continue_if_exception': False, 'timeout_override': None, 'disable_action': False, 'name': 'today', 'extra_settings': {}}, 'remove': {'filters': [{'filtertype': 'pattern', 'exclude': False, 'kind': 'prefix', 'value': 'logstash-'}, {'filtertype': 'age', 'source': 'name', 'stats_result': 'min_value', 'exclude': False, 'timestring': '%Y.%m.%d', 'epoch': None, 'unit_count': 1, 'direction': 'older', 'unit': 'days'}]}, 'action': 'alias'}}\n2017-02-24 17:09:54,522 DEBUG                curator.cli                    cli:133  action_disabled = False\n2017-02-24 17:09:54,522 DEBUG                curator.cli                    cli:137  continue_if_exception = False\n2017-02-24 17:09:54,522 DEBUG                curator.cli                    cli:139  timeout_override = None\n2017-02-24 17:09:54,522 DEBUG                curator.cli                    cli:141  ignore_empty_list = True\n2017-02-24 17:09:54,522 INFO                 curator.cli                    cli:151  Preparing Action ID: alias-today-remove, \"alias\"\n2017-02-24 17:09:54,523 DEBUG              curator.utils             get_client:535  kwargs = {'client_cert': None, 'port': 9200, 'timeout': 30, 'aws_key': None, 'client_key': None, 'certificate': None, 'master_only': False, 'aws_region': None, 'aws_secret_key': None, 'use_ssl': False, 'http_auth': None, 'url_prefix': '', 'ssl_no_validate': False, 'hosts': ['10.52.101.1']}\n2017-02-24 17:09:54,535 DEBUG              curator.utils             get_client:583  \"requests_aws4auth\" module present, but not used.\n2017-02-24 17:09:54,542 DEBUG              curator.utils          check_version:438  Detected Elasticsearch version 2.4.2\n2017-02-24 17:09:54,542 DEBUG                curator.cli                    cli:167  client is \n2017-02-24 17:09:54,542 INFO                 curator.cli                    cli:173  Trying Action ID: alias-today-remove, \"alias\": Remove indexes from today alias\n2017-02-24 17:09:54,542 DEBUG                curator.cli         process_action:40   Configuration dictionary: {'description': 'Remove indexes from today alias', 'options': {'name': 'today', 'extra_settings': {}}, 'remove': {'filters': [{'filtertype': 'pattern', 'exclude': False, 'kind': 'prefix', 'value': 'logstash-'}, {'filtertype': 'age', 'source': 'name', 'stats_result': 'min_value', 'exclude': False, 'timestring': '%Y.%m.%d', 'epoch': None, 'unit_count': 1, 'direction': 'older', 'unit': 'days'}]}, 'action': 'alias'}\n2017-02-24 17:09:54,542 DEBUG                curator.cli         process_action:41   kwargs: {'master_timeout': 30, 'dry_run': False, 'timeout': 30}\n2017-02-24 17:09:54,543 DEBUG                curator.cli         process_action:46   opts: {'name': 'today', 'extra_settings': {}}\n2017-02-24 17:09:54,543 DEBUG                curator.cli         process_action:62   Action kwargs: {'name': 'today', 'extra_settings': {}}\n2017-02-24 17:09:54,544 DEBUG                curator.cli         process_action:67   Running \"ALIAS\" action\n2017-02-24 17:09:54,544 DEBUG              curator.utils     parse_date_pattern:1032 Partially rendered name: t\n2017-02-24 17:09:54,545 DEBUG              curator.utils     parse_date_pattern:1032 Partially rendered name: to\n2017-02-24 17:09:54,545 DEBUG              curator.utils     parse_date_pattern:1032 Partially rendered name: tod\n2017-02-24 17:09:54,545 DEBUG              curator.utils     parse_date_pattern:1032 Partially rendered name: toda\n2017-02-24 17:09:54,546 DEBUG              curator.utils     parse_date_pattern:1032 Partially rendered name: today\n2017-02-24 17:09:54,546 DEBUG              curator.utils     parse_date_pattern:1034 Fully rendered name: today\n2017-02-24 17:09:54,547 DEBUG                curator.cli         process_action:76   Removing indices from alias \"today\"\n2017-02-24 17:09:54,548 DEBUG          curator.indexlist          get_indices:65   Getting all indices\n2017-02-24 17:09:54,586 DEBUG              curator.utils            get_indices:380  Detected Elasticsearch version 2.4.2\n2017-02-24 17:09:54,587 DEBUG              curator.utils            get_indices:386  Using Elasticsearch >= 2.4.2 < 5.0.0\n2017-02-24 17:09:54,590 DEBUG              curator.utils            get_indices:395  All indices: ['logstash-2017.02.14', 'logstash-2017.02.24', 'logstash-2017.02.22', 'logstash-2017.02.23', 'logstash-2017.02.21']\n2017-02-24 17:09:54,591 DEBUG          curator.indexlist       empty_list_check:182  Checking for empty list\n2017-02-24 17:09:54,591 DEBUG          curator.indexlist     __build_index_info:80   Building preliminary index metadata for logstash-2017.02.14\n2017-02-24 17:09:54,591 DEBUG          curator.indexlist     __build_index_info:80   Building preliminary index metadata for logstash-2017.02.24\n2017-02-24 17:09:54,591 DEBUG          curator.indexlist     __build_index_info:80   Building preliminary index metadata for logstash-2017.02.22\n2017-02-24 17:09:54,591 DEBUG          curator.indexlist     __build_index_info:80   Building preliminary index metadata for logstash-2017.02.23\n2017-02-24 17:09:54,591 DEBUG          curator.indexlist     __build_index_info:80   Building preliminary index metadata for logstash-2017.02.21\n2017-02-24 17:09:54,591 DEBUG          curator.indexlist          _get_metadata:145  Getting index metadata\n2017-02-24 17:09:54,591 DEBUG          curator.indexlist       empty_list_check:182  Checking for empty list\n2017-02-24 17:09:54,615 DEBUG          curator.indexlist       _get_index_stats:113  Getting index stats\n2017-02-24 17:09:54,616 DEBUG          curator.indexlist       empty_list_check:182  Checking for empty list\n2017-02-24 17:09:54,617 DEBUG          curator.indexlist           working_list:193  Generating working list of indices\n2017-02-24 17:09:54,617 DEBUG          curator.indexlist           working_list:193  Generating working list of indices\n2017-02-24 17:09:54,624 DEBUG          curator.indexlist     iterate_over_stats:122  Index: logstash-2017.02.21  Size: 3.0KB  Docs: 1\n2017-02-24 17:09:54,624 DEBUG          curator.indexlist     iterate_over_stats:122  Index: logstash-2017.02.14  Size: 3.0KB  Docs: 1\n2017-02-24 17:09:54,624 DEBUG          curator.indexlist     iterate_over_stats:122  Index: logstash-2017.02.24  Size: 3.0KB  Docs: 1\n2017-02-24 17:09:54,624 DEBUG          curator.indexlist     iterate_over_stats:122  Index: logstash-2017.02.23  Size: 3.0KB  Docs: 1\n2017-02-24 17:09:54,625 DEBUG          curator.indexlist     iterate_over_stats:122  Index: logstash-2017.02.22  Size: 3.0KB  Docs: 1\n2017-02-24 17:09:54,625 DEBUG          curator.indexlist        iterate_filters:819  Iterating over a list of filters\n2017-02-24 17:09:54,625 DEBUG          curator.indexlist        iterate_filters:825  All filters: [{'filtertype': 'pattern', 'exclude': False, 'kind': 'prefix', 'value': 'logstash-'}, {'filtertype': 'age', 'source': 'name', 'stats_result': 'min_value', 'exclude': False, 'timestring': '%Y.%m.%d', 'epoch': None, 'unit_count': 1, 'direction': 'older', 'unit': 'days'}]\n2017-02-24 17:09:54,625 DEBUG          curator.indexlist        iterate_filters:827  Top of the loop: ['logstash-2017.02.14', 'logstash-2017.02.24', 'logstash-2017.02.22', 'logstash-2017.02.23', 'logstash-2017.02.21']\n2017-02-24 17:09:54,625 DEBUG          curator.indexlist        iterate_filters:828  Un-parsed filter args: {'filtertype': 'pattern', 'exclude': False, 'kind': 'prefix', 'value': 'logstash-'}\n2017-02-24 17:09:54,627 DEBUG     curator.validators.SchemaCheck               __init:26   Schema: , , None]), 'allocation_type': Any([, ]), 'reverse': Any([, , , , None]), 'value': Any([, , , , ]), 'unit': Any([, ]), 'count': Coerce(int, msg=None), 'source': Any([, ]), 'max_num_segments': Coerce(int, msg=None), 'use_age': , 'kind': Any([, ]), 'epoch': Any([Coerce(int, msg=None), None]), 'timestring': Any([, , None]), 'disk_space': , 'aliases': Any([, [], , []]), 'field': Any([, , None]), 'exclude': Any([, , , , None]), 'key': Any([, ]), 'state': Any([, ]), 'direction': Any([, ]), 'unit_count': Coerce(int, msg=None), 'filtertype': Any([In(['age', 'alias', 'allocated', 'closed', 'count', 'forcemerged', 'kibana', 'none', 'opened', 'pattern', 'space', 'state'])])}, extra=PREVENT_EXTRA, required=False) object at 0x7fdd6c08d748>\n2017-02-24 17:09:54,628 DEBUG     curator.validators.SchemaCheck               init:27   \"filter\" config: {'filtertype': 'pattern', 'exclude': False, 'kind': 'prefix', 'value': 'logstash-'}\n2017-02-24 17:09:54,629 DEBUG          curator.indexlist        iterate_filters:835  Parsed filter args: {'filtertype': 'pattern', 'exclude': False, 'value': 'logstash-', 'kind': 'prefix'}\n2017-02-24 17:09:54,629 DEBUG              curator.utils        iterate_filters:844  Filter args: {'exclude': False, 'kind': 'prefix', 'value': 'logstash-'}\n2017-02-24 17:09:54,629 DEBUG              curator.utils        iterate_filters:845  Pre-instance: ['logstash-2017.02.14', 'logstash-2017.02.24', 'logstash-2017.02.22', 'logstash-2017.02.23', 'logstash-2017.02.21']\n2017-02-24 17:09:54,630 DEBUG          curator.indexlist        filter_by_regex:362  Filtering indices by regex\n2017-02-24 17:09:54,630 DEBUG          curator.indexlist       empty_list_check:182  Checking for empty list\n2017-02-24 17:09:54,631 DEBUG          curator.indexlist           working_list:193  Generating working list of indices\n2017-02-24 17:09:54,632 DEBUG          curator.indexlist        filter_by_regex:383  Filter by regex: Index: logstash-2017.02.14\n2017-02-24 17:09:54,632 DEBUG          curator.indexlist           actionable:35   Index logstash-2017.02.14 is actionable and remains in the list.\n2017-02-24 17:09:54,632 DEBUG          curator.indexlist        filter_by_regex:383  Filter by regex: Index: logstash-2017.02.24\n2017-02-24 17:09:54,633 DEBUG          curator.indexlist           __actionable:35   Index logstash-2017.02.24 is actionable and remains in the list.\n2017-02-24 17:09:54,633 DEBUG          curator.indexlist        filter_by_regex:383  Filter by regex: Index: logstash-2017.02.22\n2017-02-24 17:09:54,634 DEBUG          curator.indexlist           __actionable:35   Index logstash-2017.02.22 is actionable and remains in the list.\n2017-02-24 17:09:54,634 DEBUG          curator.indexlist        filter_by_regex:383  Filter by regex: Index: logstash-2017.02.23\n2017-02-24 17:09:54,634 DEBUG          curator.indexlist           __actionable:35   Index logstash-2017.02.23 is actionable and remains in the list.\n2017-02-24 17:09:54,635 DEBUG          curator.indexlist        filter_by_regex:383  Filter by regex: Index: logstash-2017.02.21\n2017-02-24 17:09:54,636 DEBUG          curator.indexlist           __actionable:35   Index logstash-2017.02.21 is actionable and remains in the list.\n2017-02-24 17:09:54,637 DEBUG              curator.utils        iterate_filters:847  Post-instance: ['logstash-2017.02.14', 'logstash-2017.02.24', 'logstash-2017.02.22', 'logstash-2017.02.23', 'logstash-2017.02.21']\n2017-02-24 17:09:54,638 DEBUG          curator.indexlist        iterate_filters:827  Top of the loop: ['logstash-2017.02.14', 'logstash-2017.02.24', 'logstash-2017.02.22', 'logstash-2017.02.23', 'logstash-2017.02.21']\n2017-02-24 17:09:54,639 DEBUG          curator.indexlist        iterate_filters:828  Un-parsed filter args: {'filtertype': 'age', 'source': 'name', 'stats_result': 'min_value', 'exclude': False, 'timestring': '%Y.%m.%d', 'epoch': None, 'unit_count': 1, 'direction': 'older', 'unit': 'days'}\n2017-02-24 17:09:54,650 DEBUG     curator.validators.SchemaCheck               __init:26   Schema: , ]), 'epoch': Any([Coerce(int, msg=None), None]), 'count': Coerce(int, msg=None), 'unit_count': Coerce(int, msg=None), 'key': Any([, ]), 'max_num_segments': Coerce(int, msg=None), 'filtertype': Any([In(['age', 'alias', 'allocated', 'closed', 'count', 'forcemerged', 'kibana', 'none', 'opened', 'pattern', 'space', 'state'])]), 'field': Any([, , None]), 'unit': Any([, ]), 'stats_result': Any([, , None]), 'reverse': Any([, , , , None]), 'disk_space': , 'timestring': Any([, , None]), 'source': Any([, ]), 'kind': Any([, ]), 'direction': Any([, ]), 'value': Any([, , , , ]), 'state': Any([, ]), 'use_age': , 'aliases': Any([, [], , []]), 'exclude': Any([, , , , None])}, extra=PREVENT_EXTRA, required=False) object at 0x7fdd6c08cb70>\n2017-02-24 17:09:54,650 DEBUG     curator.validators.SchemaCheck               init:27   \"filter\" config: {'filtertype': 'age', 'source': 'name', 'stats_result': 'min_value', 'exclude': False, 'timestring': '%Y.%m.%d', 'epoch': None, 'unit_count': 1, 'direction': 'older', 'unit': 'days'}\n2017-02-24 17:09:54,652 DEBUG          curator.indexlist        iterate_filters:835  Parsed filter args: {'filtertype': 'age', 'unit': 'days', 'source': 'name', 'stats_result': 'min_value', 'unit_count': 1, 'timestring': '%Y.%m.%d', 'exclude': False, 'direction': 'older', 'epoch': None}\n2017-02-24 17:09:54,652 DEBUG              curator.utils        iterate_filters:844  Filter args: {'source': 'name', 'stats_result': 'min_value', 'exclude': False, 'timestring': '%Y.%m.%d', 'epoch': None, 'unit_count': 1, 'direction': 'older', 'unit': 'days'}\n2017-02-24 17:09:54,652 DEBUG              curator.utils        iterate_filters:845  Pre-instance: ['logstash-2017.02.14', 'logstash-2017.02.24', 'logstash-2017.02.22', 'logstash-2017.02.23', 'logstash-2017.02.21']\n2017-02-24 17:09:54,652 DEBUG          curator.indexlist          filter_by_age:420  Filtering indices by age\n2017-02-24 17:09:54,653 DEBUG          curator.indexlist   _get_name_based_ages:227  Getting ages of indices by \"name\"\n2017-02-24 17:09:54,653 DEBUG          curator.indexlist       empty_list_check:182  Checking for empty list\n2017-02-24 17:09:54,653 DEBUG              curator.utils         get_date_regex:153  regex = \\d{4}.\\d{2}.\\d{2}\n2017-02-24 17:09:54,654 DEBUG          curator.indexlist           working_list:193  Generating working list of indices\n2017-02-24 17:09:54,659 DEBUG          curator.indexlist           working_list:193  Generating working list of indices\n2017-02-24 17:09:54,660 DEBUG          curator.indexlist           __actionable:35   Index logstash-2017.02.14 is actionable and remains in the list.\n2017-02-24 17:09:54,660 DEBUG          curator.indexlist            __excludify:58   Remains in actionable list: Index \"logstash-2017.02.14\" age (1487030400), direction: \"older\", point of reference, (1487869794)\n2017-02-24 17:09:54,660 DEBUG          curator.indexlist       __not_actionable:39   Index logstash-2017.02.24 is not actionable, removing from list.\n2017-02-24 17:09:54,660 DEBUG          curator.indexlist            __excludify:58   Removed from actionable list: Index \"logstash-2017.02.24\" age (1487894400), direction: \"older\", point of reference, (1487869794)\n2017-02-24 17:09:54,660 DEBUG          curator.indexlist           __actionable:35   Index logstash-2017.02.22 is actionable and remains in the list.\n2017-02-24 17:09:54,661 DEBUG          curator.indexlist            __excludify:58   Remains in actionable list: Index \"logstash-2017.02.22\" age (1487721600), direction: \"older\", point of reference, (1487869794)\n2017-02-24 17:09:54,661 DEBUG          curator.indexlist           __actionable:35   Index logstash-2017.02.23 is actionable and remains in the list.\n2017-02-24 17:09:54,661 DEBUG          curator.indexlist            __excludify:58   Remains in actionable list: Index \"logstash-2017.02.23\" age (1487808000), direction: \"older\", point of reference, (1487869794)\n2017-02-24 17:09:54,661 DEBUG          curator.indexlist           __actionable:35   Index logstash-2017.02.21 is actionable and remains in the list.\n2017-02-24 17:09:54,661 DEBUG          curator.indexlist            __excludify:58   Remains in actionable list: Index \"logstash-2017.02.21\" age (1487635200), direction: \"older\", point of reference, (1487869794)\n2017-02-24 17:09:54,662 DEBUG              curator.utils        iterate_filters:847  Post-instance: ['logstash-2017.02.14', 'logstash-2017.02.22', 'logstash-2017.02.23', 'logstash-2017.02.21']\n2017-02-24 17:09:54,662 DEBUG          curator.indexlist       empty_list_check:182  Checking for empty list\n2017-02-24 17:09:54,662 DEBUG          curator.indexlist           working_list:193  Generating working list of indices\n2017-02-24 17:09:54,662 DEBUG      curator.actions.alias                 remove:69   Removing index logstash-2017.02.14 from alias today\n2017-02-24 17:09:54,662 DEBUG      curator.actions.alias                 remove:69   Removing index logstash-2017.02.22 from alias today\n2017-02-24 17:09:54,662 DEBUG      curator.actions.alias                 remove:69   Removing index logstash-2017.02.23 from alias today\n2017-02-24 17:09:54,663 DEBUG      curator.actions.alias                 remove:69   Removing index logstash-2017.02.21 from alias today\n2017-02-24 17:09:54,663 DEBUG                curator.cli         process_action:98   Doing the action here.\n2017-02-24 17:09:54,663 INFO       curator.actions.alias              do_action:109  Updating aliases...\n2017-02-24 17:09:54,663 DEBUG      curator.actions.alias                   body:80   Alias actions: [{'remove': {'index': 'logstash-2017.02.14', 'alias': 'today'}}, {'remove': {'index': 'logstash-2017.02.22', 'alias': 'today'}}, {'remove': {'index': 'logstash-2017.02.23', 'alias': 'today'}}, {'remove': {'index': 'logstash-2017.02.21', 'alias': 'today'}}]\n2017-02-24 17:09:54,663 INFO       curator.actions.alias              do_action:110  Alias actions: {'actions': [{'remove': {'index': 'logstash-2017.02.14', 'alias': 'today'}}, {'remove': {'index': 'logstash-2017.02.22', 'alias': 'today'}}, {'remove': {'index': 'logstash-2017.02.23', 'alias': 'today'}}, {'remove': {'index': 'logstash-2017.02.21', 'alias': 'today'}}]}\n2017-02-24 17:09:54,664 DEBUG      curator.actions.alias                   body:80   Alias actions: [{'remove': {'index': 'logstash-2017.02.14', 'alias': 'today'}}, {'remove': {'index': 'logstash-2017.02.22', 'alias': 'today'}}, {'remove': {'index': 'logstash-2017.02.23', 'alias': 'today'}}, {'remove': {'index': 'logstash-2017.02.21', 'alias': 'today'}}]\n2017-02-24 17:09:54,678 ERROR                curator.cli                    cli:193  Failed to complete action: alias.  : Exception encountered.  Rerun with loglevel DEBUG and/or check Elasticsearch logs for more information. Exception: TransportError(404, 'aliases_not_found_exception', 'aliases [today] missing')\nroot@curator-20161229-095124trusty-64-jre-1:~# curl \"10.52.101.1:9200/_cat/aliases\"\ntoday logstash-2017.02.24 - - - \nroot@curator-20161229-095124trusty-64-jre-1:~# \n```\nThe today alias contains logstash-2017.02.24 and logstash-2017.02.21, in the first run logstash-2017.02.21 is removed, which is expected. Second run, now the today alias just contains logstash-2017.02.24 and results in: \n2017-02-24 17:09:54,678 ERROR                curator.cli                    cli:193  Failed to complete action: alias.  <class 'curator.exceptions.FailedExecution'>: Exception encountered.  Rerun with loglevel DEBUG and/or check Elasticsearch logs for more information. Exception: TransportError(404, 'aliases_not_found_exception', 'aliases [today] missing')\nBut the alias today exists and holds logstash-2017.02.24. Looking at it in more detail, I think I know where the error is coming from. \nPOST /_aliases\n{\n    \"actions\" : [\n        { \"remove\" : { \"index\" : \"logstash-2017.02.21\", \"alias\" : \"today\" } }\n    ]\n}\nResults in:\n{\n  \"error\": {\n    \"root_cause\": [\n      {\n        \"type\": \"aliases_not_found_exception\",\n        \"reason\": \"aliases [today] missing\",\n        \"resource.type\": \"aliases\",\n        \"resource.id\": \"today\"\n      }\n    ],\n    \"type\": \"aliases_not_found_exception\",\n    \"reason\": \"aliases [today] missing\",\n    \"resource.type\": \"aliases\",\n    \"resource.id\": \"today\"\n  },\n  \"status\": 404\n}\nWhile (Note still only logstash-2017.02.24 is part of the today alias )\nPOST /_aliases\n{\n    \"actions\" : [\n        { \"remove\" : { \"index\" : \"logstash-2017.02.21\", \"alias\" : \"today\" } },\n        { \"remove\" : { \"index\" : \"logstash-2017.02.24\", \"alias\" : \"today\" } }\n    ]\n}\nResults in:\n{\n  \"acknowledged\": true\n}\nSo when I issue a alias remove command against ES, and that alias is not part of that index, the aliases_not_found_exception is returned. I guess this is related to the index being the \"top level\" and linking to the alias and not the alias linking to indices. \nIn the second run, the remove looks like:\n2017-02-24 17:09:54,664 DEBUG      curator.actions.alias                   body:80   Alias actions: [{'remove': {'index': 'logstash-2017.02.14', 'alias': 'today'}}, {'remove': {'index': 'logstash-2017.02.22', 'alias': 'today'}}, {'remove': {'index': 'logstash-2017.02.23', 'alias': 'today'}}, {'remove': {'index': 'logstash-2017.02.21', 'alias': 'today'}}]\nand as all indices in this list are not linked to the alias, it results in this exception.\nWould it be possible to check which indices from the list, are actually part of the alias and then only remove these? . Awesome, thanks @untergeek . ",
    "czerasz": "Published an image with curator 4.0.1 if someone needs it.\nAlready tested it and it works nice :-)\n. ",
    "davidkarlsen": "@untergeek it's about time now - since elastic publishes official images of kibana and elastic. Also the one in the library is currently lacking support for 5.1.1.. Great - it would be even more great if elastic inc would curate this image along the rest of ELK. ",
    "praseodym": "I've also created an automated Curator Docker image build based on the official Python 3-alpine image.\nIt is available from both Docker Hub and Quay:\n https://hub.docker.com/r/praseodym/elasticsearch-curator/tags/\n https://quay.io/repository/praseodym/elasticsearch-curator?tab=tags. ",
    "geekofalltrades": "@untergeek , any motion on this? I'm about to start deploying ELK stacks in Kubernetes, and I'd love to see curator up alongside the others on docker.elastic.co.. ",
    "frittentheke": "@untergeek Since your already have a Dockerfile yourself, wouldn't it make sense to have a somewhat offical image published somewhere? Otherwise everyone and their cat is simply forking your code at some point to create their own images with likely no real maintenance or updates.. @untergeek please also accept my offer to assist with setting things up. This really only takes a few minutes when using Travis CI ... https://ops.tips/blog/travis-ci-push-docker-image/. ",
    "MaikJaek": "@untergeek \nHi Aaron, \ndo you have any news regarding the suggested idea of publishing images on docker.elastic.co ? We'd love to use an official image for curator, as well. . ",
    "pete-woods": "Given you now publish official Docker images to https://www.docker.elastic.co/, is it the right time to start publishing this, too?. It's about a 5 minute job to set up an automated Docker Hub image, in case you haven't done it before. I'd be happy to take it from there instead of the Elastic repo.. You don't even need Jenkins, TravisCI, etc if you use Docker Hub, it has its own builders. At the minute I'm having to use a GitHub mirror of your repo to set up automated builds to here.\nhttps://hub.docker.com/r/surevinecom/elastic-curator/. ",
    "jippi": "@untergeek any plans for a 2019 fix for this? :) would be highly appreciated . ",
    "foresightyj": "Indeed it works.\nI patiently read the documentations and realized that the positions do matter. \nThanks!\n. ",
    "zzugg": "@untergeek I am sorry for late response. Thank you for merging it.\n. ",
    "radap": "Got it.\nIt is too bad that there is no support for this option.\n. ",
    "gfrankliu": "Can curator add support of non zero padded number? A new formatting function can be added in curator: http://stackoverflow.com/questions/9525944/python-datetime-formatting-without-zero-padding\n. @untergeek 4.0 filters look definitely better. Look forward to it.\n. field_stats with @timestamp seems like a solution.\nIf I want to set a filter to delete all indices whose last timestamp is more than 30 days (hasn't been updated for 30 days), shall I do:\n- filtertype: age\n      source: field_stats\n      field: @timestamp\n      stats_result: max_value\n      direction: older\n      unit: days\n      unit_count: 30\n      exclude:\n. Since my indices are having time series log data, the last entry should always have the latest timestamp. \nWhen you say \"young values\", are you referring to old or new logs? If there are new log entries being added to the index, the max_value should increase, right? My goal is only to delete the indices if they are \"idle\" for 30 days. \n. ",
    "djnelson9715": "It is returning No indices matched the provided args.\nSame as before.\nOn Tue, Jan 5, 2016 at 11:02 AM, Aaron Mildenstein <notifications@github.com\n\nwrote:\nWhat happens if you run:\ncurator show indices --prefix 'gbs.'\nI'll add a test to the suite to see what happens with the long index name.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/elastic/curator/issues/533#issuecomment-169062033.\n\n\nDoug Nelson\n. I noticed you are doing this in 3.4 curator and I am running 3.2.3 I wonder\nif that could be the issue.  Looks like it is time to upgrade\nOn Tue, Jan 5, 2016 at 12:57 PM, Aaron Mildenstein <notifications@github.com\n\nwrote:\nHmm. I put the index with:\ncurl -XPUT localhost:9200/gbs.evenlogger.logstash-2016.01.04\nAnd then when I ran curator (3.4.0) I saw this:\ncurator show indices --all-indices\n2016-01-05 11:53:40,041 INFO      Job starting: show indices\n2016-01-05 11:53:40,054 INFO      Matching all indices. Ignoring flags other than --exclude.\n2016-01-05 11:53:40,054 INFO      Action show will be performed on the following indices: [u'.kibana', u'.marvel-es-2015.12.22', u'.marvel-es-2015.12.29', u'.marvel-es-2015.12.30', u'.marvel-es-2015.12.31', u'.marvel-es-2016.01.01', u'.marvel-es-2016.01.02', u'.marvel-es-2016.01.03', u'.marvel-es-2016.01.04', u'.marvel-es-2016.01.05', u'.marvel-es-data', u'.marvel-kibana', u'gbs.evenlogger.logstash-2016.01.04', u'logstash-2015.12.22', u'logstash-2015.12.29', u'logstash-2015.12.30', u'logstash-2015.12.31', u'logstash-2016.01.01', u'logstash-2016.01.02', u'logstash-2016.01.03', u'logstash-2016.01.04', u'logstash-2016.01.05', u'test_batch']\n2016-01-05 11:53:40,054 INFO      Matching indices:\n.kibana\n.marvel-es-2015.12.22 (CLOSED)\n.marvel-es-2015.12.29\n.marvel-es-2015.12.30\n.marvel-es-2015.12.31\n.marvel-es-2016.01.01\n.marvel-es-2016.01.02\n.marvel-es-2016.01.03\n.marvel-es-2016.01.04\n.marvel-es-2016.01.05\n.marvel-es-data\n.marvel-kibana\ngbs.evenlogger.logstash-2016.01.04\nlogstash-2015.12.22 (CLOSED)\nlogstash-2015.12.29\nlogstash-2015.12.30\nlogstash-2015.12.31\nlogstash-2016.01.01\nlogstash-2016.01.02\nlogstash-2016.01.03\nlogstash-2016.01.04\nlogstash-2016.01.05\ntest_batch\nAnd when I run with --prefix gbs I see this:\ncurator show indices --prefix gbs\n2016-01-05 11:55:45,999 INFO      Job starting: show indices\n2016-01-05 11:55:46,013 INFO      Action show will be performed on the following indices: [u'gbs.evenlogger.logstash-2016.01.04']\n2016-01-05 11:55:46,013 INFO      Matching indices:\ngbs.evenlogger.logstash-2016.01.04\nI'm not sure what the problem is for you.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/elastic/curator/issues/533#issuecomment-169097116.\n\n\nDoug Nelson\n. Figured it out.  We have shield installed and I was not using the correct\naccount which has access to these indices.\nThanks for all the help.  Should have thought to check that first.\nOn Tue, Jan 5, 2016 at 1:12 PM, Aaron Mildenstein notifications@github.com\nwrote:\n\nTo test with 3.2.3, I had to install an older Elasticsearch instance:\n\u00bb curator --version\ncurator, version 3.2.3\n\u00bb curator show indices --all-indices\n2016-01-05 12:10:12,583 INFO      Job starting: show indices\n2016-01-05 12:10:12,592 INFO      Matching all indices. Ignoring flags other than --exclude.\n2016-01-05 12:10:12,592 INFO      Matching indices:\ngbs.evenlogger.logstash-2016.01.04\n\u00bb curator show indices --prefix gbs\n2016-01-05 12:10:18,670 INFO      Job starting: show indices\n2016-01-05 12:10:18,681 INFO      Matching indices:\ngbs.evenlogger.logstash-2016.01.04\nIt seems to be working with 3.2.3 and Elasticsearch 1.7 also.\nWhat does your environment look like? OS and version? Python version?\nElasticsearch version?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/elastic/curator/issues/533#issuecomment-169100888.\n\n\nDoug Nelson\n. ",
    "bhellsun": "hello @djnelson9715 \nCan you tell me how you solved the problem. Even I have shield installed.\nAnd when I give a command like\n\"curator show indices --all-indices\"\nit throws an error\n\"\"\"\n2016-02-27 10:58:10,645 INFO      Job starting: show indices\n2016-02-27 10:58:10,654 ERROR     Connection failure.\n\"\"\"\n. @untergeek \nI am already using curator 3.4.1 version\n- Should I give any OPTION when giving the command \"curator show indices --all-indices\"\n- because when giving curl command you have to give the option \"curl -XPOST -u \" if you have shield installed. similarly do we have to give any option when using curator.\n. ",
    "sagh0900": "curator v3.5.1: (with SSL) [Elasticsearch 2.3.4(secured, not running behind any proxy), Search-Guard-2]\nI'm running curator from elected master-node:\nWhen I run command with out master-only flag, it return output though with exceptions.\ncurator --loglevel DEBUG --use_ssl --host 192.168.0.1 --http_auth admin:admin --certificate \"/usr/local/bin/root-ca.pem\" --port 9200 show indices --all-indices\noutput: \n\nINFO      Matching indices:\n/usr/lib/python2.7/site-packages/urllib3/connectionpool.py:770: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html\n  InsecureRequestWarning)\nmonitoring-2016.08.09\n/usr/lib/python2.7/site-packages/urllib3/connectionpool.py:770: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html\n  InsecureRequestWarning)\nsearchguard\n\nBut when I include, master-only flag, it says connection failure, non-master node detected.\ncurator --master-only --loglevel DEBUG --use_ssl --host 192.168.0.1 --http_auth admin:admin --certificate \"/usr/local/bin/root-ca.pem\" --port 9200 show indices --all-indices\noutput:\n\n/usr/lib/python2.7/site-packages/urllib3/connectionpool.py:770: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html\n  InsecureRequestWarning)\n2016-08-12 08:23:15,939 INFO      Master-only flag detected. Connected to non-master node. Aborting.\n\nAlso, how to verify certificate?, when I remove --master-only flag, again I get output with exceptions irrespective of --ssl-no-validate flag\ncurator --loglevel DEBUG --use_ssl --ssl-no-validate --host 172.16.189.41 --http_auth elk-master:VGTCloudInfra --certificate \"/usr/local/bin/root-ca.pem\" --port 9200 show indices --all-indices\n\n/usr/lib/python2.7/site-packages/urllib3/connectionpool.py:770: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html\n  InsecureRequestWarning)\nmonitoring-2016.08.09\n/usr/lib/python2.7/site-packages/urllib3/connectionpool.py:770: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html\n  InsecureRequestWarning)\nsearchguard\n\n\nI'm trying to avoid the exceptions in output and also want to make it run while using --master-only flag. \nPlease help me.\n. no its not behind in any proxy. I'm using search-guard. But that is irrelavent for curator, right?\nEDIT: I'm running curator on the same host as elasticsearch is installed and secured.\n. Thanks for your replies. I downgraded by curator version to 3.5.1. The following commands before elasticsearch secured was working perfectly, of course security flags weren't included.\ncommand:\n\ncurator --loglevel DEBUG --use_ssl --host 172.16.189.41 --http_auth elk-master:VGTCloudInfra --certificate \"/usr/local/bin/root-ca.pem\" --port 9200 show indices --all-indices\n\noutput:\n\nINFO      Matching all indices. Ignoring flags other than --exclude.\n....\n/usr/lib/python2.7/site-packages/urllib3/connectionpool.py:770: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html\n  InsecureRequestWarning)\nmonitoring-2016.08.09\n/usr/lib/python2.7/site-packages/urllib3/connectionpool.py:770: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html\n  InsecureRequestWarning)\n searchguard\n\nNOTE: If I include --master-only flag in the above working command, then again its says the following: (But I'm running this command on elected master host)\ncommand:\n\ncurator --master-only --loglevel DEBUG --use_ssl --host 172.16.189.41 --http_auth elk-master:VGTCloudInfra --certificate \"/usr/local/bin/root-ca.pem\" --port 9200 show indices --all-indices\n\noutput: \n\n2016-08-12 09:58:11,322 INFO      Job starting: show indices\n2016-08-12 09:58:11,322 DEBUG     Params: {'url_prefix': u'', 'http_auth': u'admin:admin', 'dry_run': False, 'certificate': u'/usr/local/bin/root-ca.pem', 'loglevel': u'DEBUG', 'logformat': u'default', 'client_cert': None, 'host': u'192.168.0.1', 'quiet': False, 'timeout': 30, 'debug': False, 'use_ssl': True, 'logfile': None, 'master_only': True, 'port': 9200, 'ssl_no_validate': False, 'client_key': None}\n2016-08-12 09:58:11,322 DEBUG     kwargs = {'url_prefix': u'', 'http_auth': u'admin:admin', 'dry_run': False, 'certificate': u'/usr/local/bin/root-ca.pem', 'loglevel': u'DEBUG', 'host': u'192.168.0.1', 'quiet': False, 'port': 9200, 'logformat': u'default', 'timeout': 30, 'debug': False, 'use_ssl': True, 'logfile': None, 'master_only': True, 'client_cert': None, 'ssl_no_validate': False, 'client_key': None}\n2016-08-12 09:58:11,323 INFO      Attempting to verify SSL certificate.\n/usr/lib/python2.7/site-packages/urllib3/connectionpool.py:770: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html\n  InsecureRequestWarning)\n2016-08-12 09:58:11,336 DEBUG     Detected Elasticsearch version 2.3.4\n/usr/lib/python2.7/site-packages/urllib3/connectionpool.py:770: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html\n  InsecureRequestWarning)\n/usr/lib/python2.7/site-packages/urllib3/connectionpool.py:770: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html\n  InsecureRequestWarning)\n2016-08-12 09:58:11,341 INFO      Master-only flag detected. Connected to non-master node. Aborting.\n\nNOTE: The following is not working, but still how to avoid \"certificate add\" exceptions?\n\ncurator --loglevel DEBUG --use_ssl --ssl-no-validate --host 192.168.0.1 --http_auth admin:admin --certificate \"/usr/local/bin/root-ca.pem\" --port 9200 show indices --all-indices\nJob starting: show indices\n2016-08-12 09:54:00,476 DEBUG     Params: {'url_prefix': u'', 'http_auth': u'admin:admin', 'dry_run': False, 'certificate': u'/usr/local/bin/root-ca.pem', 'loglevel': u'DEBUG', 'logformat': u'default', 'client_cert': None, 'host': u'192.168.0.1', 'quiet': False, 'timeout': 30, 'debug': False, 'use_ssl': True, 'logfile': None, 'master_only': False, 'port': 9200, 'ssl_no_validate': True, 'client_key': None}\n2016-08-12 09:54:00,476 DEBUG     kwargs = {'url_prefix': u'', 'http_auth': u'admin:admin', 'dry_run': False, 'certificate': u'/usr/local/bin/root-ca.pem', 'loglevel': u'DEBUG', 'host': u'192.168.0.1', 'quiet': False, 'port': 9200, 'logformat': u'default', 'timeout': 30, 'debug': False, 'use_ssl': True, 'logfile': None, 'master_only': False, 'client_cert': None, 'ssl_no_validate': True, 'client_key': None}\n/usr/lib/python2.7/site-packages/elasticsearch/connection/http_urllib3.py:70: UserWarning: Connecting to 192.168.0.1 using SSL with verify_certs=False is insecure.\n  'Connecting to %s using SSL with verify_certs=False is insecure.' % host)\n2016-08-12 09:54:00,485 ERROR     Connection failure.\n. @untergeek Sorry I was down with flu for past three days. I got your point. But isn't there really a way to get rid of them by either working around or adding certs as it is saying? We are using self-signed certificates. This warning is so annoying. \n\nRegarding --master-only flag: Yes we have curator deployed in all nodes in the cluster. And I even tried to run curator with --master-only in elected master node, still it throws \"non-master node detected\" error. \n. Aah.. I tried certifi as well. I really appreciate you both for trying to help me out. This info helps. If I do have anything else, I will update here or open new issue if it falls in different category. For now, I'm good.\n. ",
    "panda87": "I wanted to achieve sort of HA in the curator commands, so if one instance will be down the second will run the commands.\nBut, I understand I will have to change my design to not loose the curator automation commands.\nThanks @untergeek \n. @untergeek this is exactly the idea I thought about :)\n. https://groups.google.com/forum/#!msg/elasticsearch/cXl9UrqgwBI/gEX0gSj2yqgJ\n. ",
    "HenryCook": "Thanks for the quick response! \nI've now got it working after realising I hadn't specified the port or for it to use https.\nFor anyone else who's experiencing the same issue, make sure to specify the options --use_ssl to use https and --port 443 to use port 443 instead of the default 9200.\n. ",
    "RuBiCK": "I have tried the same command in a different ES and it works fine:\n[root@eslog01 ~]# curator --host localhost delete indices --older-than 180 --time-unit days --timestring '%Y%m%d'\n2016-01-26 11:17:10,497 INFO      Job starting: delete indices\n2016-01-26 11:17:10,519 INFO      Pruning Kibana-related indices to prevent accidental deletion.\n2016-01-26 11:17:10,519 INFO      Deleting indices as a batch operation:\n2016-01-26 11:17:10,519 INFO      ---deleting index imm-20141231\n2016-01-26 11:17:10,519 INFO      ---deleting index imm-20150101\n2016-01-26 11:17:11,819 INFO      Job completed successfully.\n. @untergeek after upgrading it works great.\nMy fault was upgrading curator with yum in which the latest version was obsolete instead using pip which really provides the latest version.\nThanks!\n. ",
    "fiunchinho": "Sorry, I didn't make myself clear. I meant that I can't instantiate the curator object to create a snapshot repository. The API has these methods available, but I miss that one in particular, since it's needed to create the repository to start creating snapshots.\nI was using the curator object inside an Ansible module, that's why I need this method available. What would be the best option?\nThank you\n. You make a total valid point. But we have several teams that want to create their own ELK infrastructure, and I wrote a small ansible module to do it. I did it without the curator API, but I was just wondering if there were any plans to add the method.\nThank you! :)\n. Oh my! You are awesome, man. Thank you very much\n. ",
    "iz4blue": "@untergeek sorry too late. i am reading CLA...\n. ",
    "larsf": "Hi, when I try to  run python setup.py build_exe it complains build_exe is an invalid command, using python-setuptools-0.9.8-4 and python 2.7.5 on curator 4.0.0a10 - why might that be?\n. I am building on a linux machine. Here the output:\nython ./setup.py build_exe\nusage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n   or: setup.py --help [cmd1 cmd2 ...]\n   or: setup.py --help-commands\n   or: setup.py cmd --help\nerror: invalid command \u2018build_exe'\n\nOn May 13, 2016, at 19:18, Aaron Mildenstein notifications@github.com wrote:\nAre you building on a Windows machine?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly or view it on GitHub https://github.com/elastic/curator/pull/554#issuecomment-219188007\n. Hi,\nDuh - I hadn\u2019t checked to see if cx_Freeze was installed on the build machine.\n\nThanks for the eyeopener\nLars\n\nOn May 14, 2016, at 19:34, Aaron Mildenstein notifications@github.com wrote:\nYou probably don't need the ./. I did a clean build with a fresh git clone of master on a brand new Ubuntu 16.04 box:\npython setup.py build_exe\nrunning build_exe\ncreating directory build/exe.linux-x86_64-2.7\ncopying /usr/local/lib/python2.7/dist-packages/cx_Freeze/bases/Console -> build/exe.linux-x86_64-2.7/curator\ncopying /usr/lib/x86_64-linux-gnu/libpython2.7.so.1.0 -> build/exe.linux-x86_64-2.7/libpython2.7.so.1.0\nwriting zip file build/exe.linux-x86_64-2.7/lib/python27.zip\nName                      File\n\nm BUILD_CONSTANTS\nm StringIO                  /usr/lib/python2.7/StringIO.py\n... LOTS OF OUTPUT\n... ENDS SUCCESSFULLY\nI did have to hack and install cx_Freeze from source, as it wouldn't build from pip. But it did build. But the beta release of cx_Freeze 5.0 still has some bugs:\n\u00bb cd build/exe.linux-x86_64-2.7/\nbuh@macbuntu1 (06:32 PM) ~/git/curator/build/exe.linux-x86_64-2.7\n\u00bb ./curator --help\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/cx_Freeze/initscripts/Console.py\", line 7, in \n    import os\nImportError: No module named os\nbuh@macbuntu1 (06:32 PM) ~/git/curator/build/exe.linux-x86_64-2.7\n\u00bb ls lib\npython2.7  python27.zip\nbuh@macbuntu1 (06:32 PM) ~/git/curator/build/exe.linux-x86_64-2.7\n\u00bb file curator\ncurator: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, for GNU/Linux 2.6.32, BuildID[sha1]=8f1852d4de0f340b37478fc11e43746e01e3bed6, stripped\n\u2014\nYou are receiving this because you commented.\nReply to this email directly or view it on GitHub https://github.com/elastic/curator/pull/554#issuecomment-219259274\n. \n",
    "alexandrz": "Thank you! Yes, this would do it! Is it now possible to configure curator to use it?\n. @untergeek I can not easily do that, since I don't know python, but I would test it for sure.\n. I am having the same issue after I started proxying requests thorough aws-auth-proxy. Although, it worked before without authentication. \n. ",
    "servomac": "I have been working a little on this issue and I have a first usable version. I will write some tests, refactor a little bit the code and make a pull request.\nhttps://github.com/servomac/curator/tree/aws\n. ",
    "thinkspill": "Thanks for your reply.\nRemoving --master-only from the close command still results in 401s, but it does allow the delete command to work. So it seems that AWS ES does not allow closing of indexes, perhaps.\n. ",
    "thib-ack": "Hi,\nI just signed it.\nThanks,\n. ",
    "maxvu": "I notice now that there is an open pull request (#517) that looks to solve a similar problem.\n. After studying the Elasticsearch documentation a little closer, it seems as though aliases are endorsed pretty strongly (\"Aliases are cheap and should be used liberally\"). Although I feel both proposals given would solve the problem given, I think a solution that involves accommodating aliases would be more useful on-the-whole.\n. ",
    "mpuchol-netquest": "Same problem here.\nFound the following error executing with --debug:\n2016-02-28 09:14:36,396 DEBUG     urllib3.connectionpool          _make_request:383  \"GET /_snapshot/s3_repository/_all HTTP/1.1\" 500 439\n2016-02-28 09:14:36,396 WARNING            elasticsearch       log_request_fail:82   GET /_snapshot/s3_repository/_all [status:500 request:0.323s]\nElasticSearch is returning a 500. ES logs show:\n[2016-02-28 09:14:36,395][INFO ][rest.suppressed          ] /_snapshot/s3_repository/_all Params: {repository=s3_repository, snapshot=_all}\ncom.amazonaws.services.s3.model.AmazonS3Exception: The operation is not valid for the object's storage class (Service: Amazon S3; Status Code: 403; Error Code: InvalidObjectState; Request ID: 196D6A8F3DD3A2DA), S3 Extended Request ID: qJTKmYTIZ3yTWejeAA2ZJ4Sm2PRulS4BqsIRnGtV76ukrzanq3ngswVLhBKJukTbcf3/ZyGB8Ps=\n        at com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:1239)\n        at com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:823)\n        at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:506)\n        at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:318)\n        at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3595)\n        at com.amazonaws.services.s3.AmazonS3Client.getObject(AmazonS3Client.java:1116)\n        at com.amazonaws.services.s3.AmazonS3Client.getObject(AmazonS3Client.java:1005)\n        at org.elasticsearch.cloud.aws.blobstore.S3BlobContainer.openInput(S3BlobContainer.java:83)\n        at org.elasticsearch.common.blobstore.support.AbstractLegacyBlobContainer.readBlob(AbstractLegacyBlobContainer.java:62)\n        at org.elasticsearch.repositories.blobstore.LegacyBlobStoreFormat.readBlob(LegacyBlobStoreFormat.java:55)\n        at org.elasticsearch.repositories.blobstore.BlobStoreFormat.read(BlobStoreFormat.java:85)\n        at org.elasticsearch.repositories.blobstore.BlobStoreRepository.readSnapshot(BlobStoreRepository.java:444)\n        at org.elasticsearch.snapshots.SnapshotsService.snapshots(SnapshotsService.java:150)\n        at org.elasticsearch.action.admin.cluster.snapshots.get.TransportGetSnapshotsAction.masterOperation(TransportGetSnapshotsAction.java:77)\n        at org.elasticsearch.action.admin.cluster.snapshots.get.TransportGetSnapshotsAction.masterOperation(TransportGetSnapshotsAction.java:46)\n        at org.elasticsearch.action.support.master.TransportMasterNodeAction$3.doRun(TransportMasterNodeAction.java:130)\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n. Glacier objects seems to be the problem (according to https://discuss.elastic.co/t/s3-input-plugin-choking-on-glacier-files/28119).\n. Yes, you're right.\nIt's because snapshot metadata is stored on S3 and this metadata is needed by GET http://localhost:9200/_snapshot/s3_repository/_all. If your lifecycle moves these metadata files to Glacier, the problem appears.\nTo fix it, I just moved all metadata files from Glacier to S3 and forced the lifecycle to just move to Glacier on path indices.\nIt seems to work.\n. ",
    "sqshq": "Thanks for your quick response. It would be really nice to have this feature.\n. ",
    "berglh": "Hi @untergeek, \nSorry to dig up an old issue; I tried having a search through the curator issues, the commits you mentioned above and the curator documentation and I still can't understand how to delete the oldest indices based upon an over-all cluster size (accumulated size of all indices) threshold value. I have a cluster I want to keep under a threshold value of 25.5 TB and delete the oldest indices until the overall cluster size is below the threshold specified.\nI'm guessing I just haven't understood the documentation correctly; would appreciate if you could shed some light on this issue and how I can address it. I can script around it for now; would just love to use the official toolkit.\n@sqshq Did you manage to get this working?. ",
    "catalincatana": "Thank you for your fast response! \nI have looked through the logs of elasticsearch and it seems it tries several times(4) to take the snapshot but without success. Despite this fact, the snapshot file is created and is not empty. So if i try twice to take the snapshot..both will be valid files. \nShould i address this issue to elasticsearch team? I have put it here because curator says job not completed successfully but the snapshot is created (and valid). \nThank you!\n. I have 2 cron jobs: one is running every hour, and the other one is running every 2 hours. The commands are like these:\n1. curator --host localhost --port 9200 snapshot --repository backup_1_$currentDate indices --time-unit days --older-than 360 --timestring %Y.%m.%d\n2. curator --host localhost --port 9200 snapshot --repository backup_2_$currentDate indices --all-indices\nWhen they need to run in the same hour, that's when one of them fails (randomly).\nElasticsearch version is: 1.1.1\nLucene version: 4.7\nAfter taking the snapshot i see both of them in show snapshots command.\nAlso this behavior is not happening every time..it can happen 3 times, and the 4th it creates correctly the snapshot file. (only one). It seems to be a random issue.\n. I will try to do that and see if i encounter any other issues. \nThank you very much for your time and responses. You are doing a great job!\n. ",
    "niemyjski": "We are using curator 3.41 and elastic 1.7.5 with azure elastic 2.8.2. We never had a problem with a 30 second timeout in the past ever until about a month ago. Then it seems to time out.. As per our wiki we run backups hourly and every 12 hours depending on the indexes (big ones get it done every 12 hours).\nRight now there are quite a few as they are backed up.. but even the ones with like 14 snapshots (12 hour ones) take forever and fail.\n. But we keep them in separate containers/folders in azure so it shouldn't be that bad. I've tried directly from sense and they do take a while to delete but getting the list is pretty quick. but is kinda slow when doing it from curator. We are still a few months away from upgrading to 2/3x.\n. I increased the timeout to 1200 seconds and it seems to help.. I just wonder if its blocking snapshot backups from running.\n. Looks like it's taking 11 minutes to delete a small snapshot (like really 400k docs on one index)\nsh\n2016-03-18 15:57:18,914 INFO      Job starting: delete snapshots\n2016-03-18 15:57:38,036 INFO      Deleting snapshot 2016-03-07-07-40\n2016-03-18 16:08:03,681 INFO      Deleting snapshot 2016-03-07-08-40\n. Yeah, I get that, the thing is running commands from curator should be just as fast as curl. I really wish there was a way to time the delete of the same snapshot in both so I could see if there is any extra time spent.. Do you have a way to do a dry run of the delete?\n. I really appreciate your help!\n. I'll try that out.. I just noticed that I'm getting this exception now:\nsh\n2016-03-18 21:34:33,685 INFO      Deleting snapshot 2016-03-06-16-30\nTraceback (most recent call last):\n  File \"/usr/local/bin/curator\", line 9, in <module>\n    load_entry_point('elasticsearch-curator==3.4.1', 'console_scripts', 'curator')()\n  File \"/usr/local/lib/python2.7/dist-packages/curator/curator.py\", line 5, in main\n    cli( obj={ \"filters\": [] } )\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 716, in __call__\n    return self.main(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 696, in main\n    rv = self.invoke(ctx)\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 1060, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 1060, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 889, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 534, in invoke\n    return callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/click/decorators.py\", line 17, in new_func\n    return f(get_current_context(), *args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/curator/cli/snapshot_selection.py\", line 93, in snapshots\n    retval = delete_snapshot(client, snapshot=snap, repository=repository)\n  File \"/usr/local/lib/python2.7/dist-packages/curator/api/snapshot.py\", line 123, in delete_snapshot\n    client.snapshot.delete(repository=repository, snapshot=snapshot)\n  File \"/usr/local/lib/python2.7/dist-packages/elasticsearch/client/utils.py\", line 69, in _wrapped\n    return func(*args, params=params, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/elasticsearch/client/snapshot.py\", line 40, in delete\n    _make_path('_snapshot', repository, snapshot), params=params)\n  File \"/usr/local/lib/python2.7/dist-packages/elasticsearch/transport.py\", line 329, in perform_request\n    status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)\n  File \"/usr/local/lib/python2.7/dist-packages/elasticsearch/connection/http_urllib3.py\", line 106, in perform_request\n    self._raise_error(response.status, raw_data)\n  File \"/usr/local/lib/python2.7/dist-packages/elasticsearch/connection/base.py\", line 105, in _raise_error\n    raise HTTP_EXCEPTIONS.get(status_code, TransportError)(status_code, error_message, additional_info)\nelasticsearch.exceptions.TransportError\n. Wonder if this is caused by any shapshots that are currently in the process of being deleted and curator is calling delete on them.\n. Could we add more logging to this in a 3.4.2? I'd do a pull request but I haven't done Python in over a decade.\n. Thanks for the patch, I'll try it out!\n. After updating I just ran into this issue:\n2016-03-22 13:09:27,831 INFO      Job starting: delete snapshots\n2016-03-22 13:09:56,698 INFO      Deleting snapshot 2016-03-08-11-30\n2016-03-22 13:26:57,826 INFO      Deleting snapshot 2016-03-08-12-30\n2016-03-22 13:43:07,548 INFO      Deleting snapshot 2016-03-08-13-30\nTraceback (most recent call last):\n  File \"/usr/local/bin/curator\", line 9, in <module>\n    load_entry_point('elasticsearch-curator==3.5.1', 'console_scripts', 'curator')()\n  File \"/usr/local/lib/python2.7/dist-packages/curator/curator.py\", line 5, in main\n    cli( obj={ \"filters\": [] } )\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 716, in __call__\n    return self.main(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 696, in main\n    rv = self.invoke(ctx)\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 1060, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 1060, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 889, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 534, in invoke\n    return callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/click/decorators.py\", line 17, in new_func\n    return f(get_current_context(), *args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/curator/cli/snapshot_selection.py\", line 93, in snapshots\n    retval = delete_snapshot(client, snapshot=snap, repository=repository)\n  File \"/usr/local/lib/python2.7/dist-packages/curator/api/snapshot.py\", line 126, in delete_snapshot\n    logger.error(\"Unable to delete snapshot {0} from repository {1}. Error message: {2}.   Run with --debug flag and/or check Elasticsearch logs for more information.\".format(snapshot, repository, e))\n  File \"/usr/local/lib/python2.7/dist-packages/elasticsearch/exceptions.py\", line 55, in __str__\n    cause = ', %r' % self.info['error']['root_cause'][0]['reason']\nTypeError: string indices must be integers\n. I narrowed it down with the debug flag:\n2016-03-22 14:50:11,964 DEBUG         urllib3.util.retry               from_int:156  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-03-22 14:50:11,965 DEBUG     urllib3.connectionpool          _make_request:386  \"DELETE /_snapshot/ex_organizations/2016-03-08-17-30 HTTP/1.1\" 503 150\n2016-03-22 14:50:11,966 WARNING            elasticsearch       log_request_fail:82   DELETE /_snapshot/ex_organizations/2016-03-08-17-30 [status:503 request:0.002s]\n2016-03-22 14:50:11,966 DEBUG              elasticsearch       log_request_fail:90   > None\n2016-03-22 14:50:11,966 DEBUG              elasticsearch       log_request_fail:93   < {\"error\":\"ConcurrentSnapshotExecutionException[[ex_organizations:2016-03-08-17-30] another snapshot is currently running cannot delete]\",\"status\":503}\nTraceback (most recent call last):\n  File \"/usr/local/bin/curator\", line 9, in <module>\n    load_entry_point('elasticsearch-curator==3.5.1', 'console_scripts', 'curator')()\n  File \"/usr/local/lib/python2.7/dist-packages/curator/curator.py\", line 5, in main\n    cli( obj={ \"filters\": [] } )\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 716, in __call__\n    return self.main(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 696, in main\n    rv = self.invoke(ctx)\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 1060, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 1060, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 889, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 534, in invoke\n    return callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/click/decorators.py\", line 17, in new_func\n    return f(get_current_context(), *args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/curator/cli/snapshot_selection.py\", line 93, in snapshots\n    retval = delete_snapshot(client, snapshot=snap, repository=repository)\n  File \"/usr/local/lib/python2.7/dist-packages/curator/api/snapshot.py\", line 126, in delete_snapshot\n    logger.error(\"Unable to delete snapshot {0} from repository {1}. Error message: {2}.   Run with --debug flag and/or check Elasticsearch logs for more information.\".format(snapshot, repository, e))\n  File \"/usr/local/lib/python2.7/dist-packages/elasticsearch/exceptions.py\", line 55, in __str__\n    cause = ', %r' % self.info['error']['root_cause'][0]['reason']\nTypeError: string indices must be integers\n. We don't have any other plugins other then hq, sense, and azure. I think from the log right before this happened. We get the bad response when a snapshot is running from another process / machine and it can't delete it. Should this snapshot be skipped and we continue deleting and we boil this message up to the end user?\n. Or do snapshots need to be deleted in sequence so it knows what files to cleanup?\n. that check would be good, We are telling curator to delete the last 7 days so it might be hard to add an if check to our scripts.\n. ",
    "rhorii": "@untergeek Thank you!\n. ",
    "choffee": "You can have that for free. I don't think it's worth signing up to the CLA for.\njohn\n. ",
    "scottnonnenberg": "This is my crontab entry:\n0 11 * * * /usr/local/bin/curator close indices --older-than 30 --time-unit days --timestring '\\%Y.\\%m.\\%d' > /dev/null\n. It's ubuntu 14.04. I'll try the double percent format for tonight's run.\n. Actually, I just looked at my setup and I have a curator delete command in my crontab as well. Now I suspect that the delete is still propagating through elasticsearch when the close command tries to resolve. I'll put a five-minute delay between them.\n. ",
    "jakauppila": "curator.exe --use_ssl --host elasticsearch.contoso.com --port 9200 --http_auth USERNAME:PASSWORD --debug show indices --timestring '%Y.%m.%d'\n. I wrote the output to a file instead, still only seeing the same info as above:\nhttps://gist.github.com/Jakauppila/113a4d6aee761e6934e5b89bc5e5ef79\n. So I pulled down the dependencies and used Curator via Python and that worked correctly, so I'm assuming it may just be a problem with the built binary?\n. Sounds good. Thanks.\nI'd also be interested in testing out the v4 alphas, any chance the binary could be built for those releases?\n. Did you test them with Shield and SSL? Looks like I'm getting the same error on the updated 3.5.1 binaries:\nhttps://gist.github.com/Jakauppila/fed762c773f2a14d7a1c9e61261c3785\nInterestingly enough, it looks like it's working with the 4.0.0.a8 binary with the following command curator.exe --config \"config\\config.txt\" \"config\\action.txt\" and the below config files:\nhttps://gist.github.com/Jakauppila/19beb387c9d4346389f097993d094165\n. Looking back at the commands I supplied above, it looks like I wasn't supplying the cert or --ssl-no-validate\n. Yep, It looks like the actual error was just obscured in the debug messaging.\ncurator.exe --host elasticsearch.contoso.com --use_ssl --port 9200 --http_auth USERNAME:PASSWORD show indices --timestring '%Y.%m.%d'\nOutput without debug:\n2016-04-27 09:25:32,713 INFO      Job starting: show indices\n2016-04-27 09:25:32,713 INFO      Attempting to verify SSL certificate.\n2016-04-27 09:25:33,365 ERROR     Connection failure.\nSorry for wasting your time.\n. ",
    "debraj-manna": "Hours are in local.\nSent from GMail on Android\nOn Apr 23, 2016 6:29 PM, \"Aaron Mildenstein\" notifications@github.com\nwrote:\n\nAre the hours in UTC or local tie?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/elastic/curator/issues/602#issuecomment-213736977\n. Thanks.\n\nIs there a way I can specify the timezone in curator? I am not able to find\nanything on doing --help.\nSent from GMail on Android\nOn Apr 23, 2016 6:42 PM, \"Aaron Mildenstein\" notifications@github.com\nwrote:\n\nThat might explain the discrepancy. Calculations are all in UTC.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/elastic/curator/issues/602#issuecomment-213738622\n. Thanks again Aaron.\n\nSent from GMail on Android\nOn Apr 23, 2016 6:57 PM, \"Aaron Mildenstein\" notifications@github.com\nwrote:\n\nNot in 3.x. I haven't added it to the 4.0 alpha yet either, but would\nappreciate the feature request being added. It will not be back ported to\n3.x\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/elastic/curator/issues/602#issuecomment-213742832\n. @untergeek - To verify of timezone is causing an issue. I created an index with about 1 month old date. Below is the output of http://localhost:9200/_cat/indices?v\n\n```\nhealth status index                        pri rep docs.count docs.deleted store.size pri.store.size \nyellow open   jabong-console-2016-03-30t16   5   1          1            0      6.4kb          6.4kb \n```\nWhile executing curator --host localhost delete indices --older-than 1 --time-unit hours --timestring %Y-%m-%dt%H --prefix jabong-console \nI am still getting the below output:-\n```\n2016-04-25 14:04:35,411 INFO      Job starting: delete indices\n2016-04-25 14:04:35,423 INFO      Pruning Kibana-related indices to prevent accidental deletion.\n2016-04-25 14:04:35,423 WARNING   No indices matched provided args: {'regex': None, 'index': (), 'suffix': None, 'newer_than': None, 'closed_only': False, 'prefix': u'jabong-console', 'time_unit': 'hours', 'timestring': u'%Y-%m-%dt%H', 'exclude': (), 'older_than': 1, 'all_indices': False}\nNo indices matched provided args: {'regex': None, 'index': (), 'suffix': None, 'newer_than': None, 'closed_only': False, 'prefix': u'jabong-console', 'time_unit': 'hours', 'timestring': u'%Y-%m-%dt%H', 'exclude': (), 'older_than': 1, 'all_indices': False}\ndebraj@debraj1247:~/code/github.com/jabong/orchestrator-service/basecomponent$ curator --host localhost delete indices --older-than 1 --time-unit hours --timestring '%Y-%m-%dt%H' --prefix jabong-console\n2016-04-25 14:05:02,383 INFO      Job starting: delete indices\n2016-04-25 14:05:02,392 INFO      Pruning Kibana-related indices to prevent accidental deletion.\n2016-04-25 14:05:02,392 WARNING   No indices matched provided args: {'regex': None, 'index': (), 'suffix': None, 'newer_than': None, 'closed_only': False, 'prefix': u'jabong-console', 'time_unit': 'hours', 'timestring': u'%Y-%m-%dt%H', 'exclude': (), 'older_than': 1, 'all_indices': False}\nNo indices matched provided args: {'regex': None, 'index': (), 'suffix': None, 'newer_than': None, 'closed_only': False, 'prefix': u'jabong-console', 'time_unit': 'hours', 'timestring': u'%Y-%m-%dt%H', 'exclude': (), 'older_than': 1, 'all_indices': False}\n```\n. ",
    "geoffroya": "Thanks @untergeek, works fine\n. ",
    "Gueust": "I see, the default documentation (i.e. the links in the README on the master branch) is indeed 3.5 while the examples files are 4.0 features. So one should look at the 4.0 documentation !\nThank you so much, and sorry for not being able to find it myself. This possibility to use configuration file seems very cool :) I think we can close the issue (maybe we can remedy my error by not pushing the example into the master branch, keeping them only in the 4.0 branch. Just an idea).\n. Great ! The example is not that simple and could be a very good example for the documentation (It helps me understand some options of the space filter, so it can helps others).\nIf I understood correctly, for the space filtertype, one could have also used:\n- filtertype: space\n      disk_space: 20\n      reverse: \n      use_age: True\n      source: name\n      timestring: '%Y.%m.%d'\n      field:\n      stats_result:\nDoes the reverse:True also work ? (the descriptions states : \"This means that if all index names share the same pattern with a date\u2014e.g. index-2016.03.01\u2014older indices will be selected first.\" However, I don't know if older indices are selected to compute the aggregated size of the indices first (in that case it will not be selected by the filter), or are selected first by the filter (meaning it will begin adding indices size on the opposite order). It is the second option for me (so the following should work).\n- filtertype: space\n      disk_space: 20\n      reverse: True\n      use_age: \n      source:\n      timestring:\n      field:\n      stats_result:\nThe two last solutions could even be better ideas, if we consider that maybe some indices have been created at a different moment compared to what is stored in its name: maybe an old index containing corrupted data was deleted and created/filled again quite recently.\n. The relation between the 'direction' and 'use_age' filter elements is unclear to me.\nThe text for the 'use_age' states:\nIn other words, it sorts all indices in the list by age, then starts adding the space consumed by each index, beginning from the youngest. Once the value of disk_space is reached, all remaining indices, which are the oldest, will be selected, omitting all of the younger indices.\nWhat I understand it that the direction decides whether we are looking at older indices or younger indices compared to  the reference point. Among these indices, 'use_age' will always try to select the oldest (leaving the youngest unselected).\nIn that case, an even more accurate sentence for the PR could be:\nThis setting must be either older or younger. This setting is used to determine whether the filter should select among the indices or snapshots that are older or younger than the reference point in time determined by  fe_unit,unit, fe_unit_count,unit_count, and optionally, fe_epoch,epoch.\n. ",
    "michaelseto": "Also, if I try this ::\n[root@selk01 curator]# python -m curator\nTraceback (most recent call last):\n  File \"/usr/lib64/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"main\", fname, loader, pkg_name)\n  File \"/usr/lib64/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/lib/python2.7/site-packages/curator/curator.py\", line 2, in \n    from .cli import cli\nValueError: Attempted relative import in non-package\n. Hello,\n[root@selk01 curator]# curator --version\ncurator, version 3.5.1\nHow I installed ::\npip install elasticsearch-curator\nAfter I installed, I tried ::\n[root@selk01 curator]# curator --host localhost -d 30\nError: no such option: -d\n. ",
    "reqless": "I got it to work using the current command line flags. Yay! I do have one last question - is this automated? I just deleted indices older than 30 days, but not sure if that was a one time thing or.....does this need to be a cronjob?\n. Ok thank you!\n. Hi Corey, it doesn't move the indices no matter if being ran from cronjobs or if I run it manually.\nSamples of index names that aren't being moved ::\nad-app-integration-2016.05.10\nad-app-integration-2016.05.11\nad-app-pbm-2016.05.04\nad-app-pbm-2016.05.05\nad-app-services-2016.05.04\nad-app-services-2016.05.05\n. I executed that and it did not find anything. However, when I executed and removed the apostrophes and backslashes, ie --timestring %Y.%m.%d , it did list a bunch of indices older than 5 days.\n. Corey, as I stated in the original post, I have manually executed the command to move the indices older than 5 days (without the apostrophes, slashes, quotes, etc), yet it still does not move the indices.\n. ",
    "danielmotaleite": "Please add this! I have a big setup and just updated to 5.1.2... of course, ES choose to recover first OLD indexes instead of today. Manually set this is annoying and one automatic way would be great\nWhy not reuse the action delete_indices and add the action priority_indices, with the same filtering ? i would imagine that the commands aren't that different (but of course, i do not know curator code). Also add support to change the refresh, it is similar to priority, just take one argument with the value and the filter. one example of the call:\n```\nPUT /syslog-*/_settings\n{\n    \"settings\": {\n      \"index\": {\n        \"refresh_interval\": \"15s\"\n      }\n  }\n}\n```\nhttps://sematext.com/blog/2013/07/08/elasticsearch-refresh-interval-vs-indexing-performance/\nIf we could use curator to set a long refresh time to old indices, we could get better performance and higher elasticsearch cache hits. @untergeek  you are right! sorry! forget about the refresh_interval ! \npriority_indices is still important :)\nHow about using the same code layout for delete_indices to update the priority_indices. ",
    "DravenJohnson": "Thanks @untergeek. I found out that --snapshot flag is not working.\nFor example:\ni have a list of snapshots with naming MONTH-YEAR (05-2016) and i try to run the curator to delete one of them.\ni used curator to delete 12-2015 snapshot in s3_snapshots repository:\ncurator --host 'localhost' --port 9200 delete snapshots --repository s3_snapshots --snapshot 12-2015\nBut it will alway run from first snapshot which is 05-2016 and try delete everything.\nAm I wrong or it's bug for this cli?\n. @untergeek \nSorry i can't post any more information for security reason. But i think you can have a general idea about how this works\nGetting All Snapshot\n```\n\ncurator --host '192.168.1.100' --port 12345 show snapshots --repository es-snapshots --all-snapshots\n2016-05-12 10:39:54,767 INFO      Job starting: show snapshots\n2016-05-12 10:39:56,233 INFO      Matching all snapshots. Ignoring flags other than --exclude.\n2016-05-12 10:39:56,234 INFO      Matching snapshots:\n08-2015\n09-2015\n10-2015\n11-2015\n12-2015\n```\n\nI am getting same output when use \nDEBUG OUTPUT (Parts)\nWhen i ran: > curator --host '192.168.1.100' --port 12345 show snapshots --repository es-snapshots --snapshot 12-2015\n```\n2016-05-12 10:42:02,894 INFO      curator.cli.snapshot_selection              snapshots:50   Job starting: show snapshots\n2016-05-12 10:42:02,894 DEBUG          curator.cli.utils             get_client:112  kwargs = {'url_prefix': u'', 'http_auth': None, 'dry_run': False, 'certificate': None, 'loglevel': u'INFO', 'host': u'192.168.1.100', 'logformat': u'default', 'timeout': 30, 'debug': True, 'use_ssl': False, 'logfile': None, 'master_only': False, 'port': 12345, 'ssl_no_validate': False}\n2016-05-12 10:42:02,895 DEBUG         urllib3.util.retry               from_int:164  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-05-12 10:42:02,895 INFO      urllib3.connectionpool              _new_conn:213  Starting new HTTP connection (1): 192.168.1.100\n2016-05-12 10:42:03,755 DEBUG     urllib3.connectionpool          _make_request:395  \"GET / HTTP/1.1\" 200 356\n2016-05-12 10:42:03,756 INFO               elasticsearch    log_request_success:63   GET http://192.168.1.100:12345/ [status:200 request:0.861s]\n2016-05-12 10:42:03,756 DEBUG              elasticsearch    log_request_success:65   > None\n2016-05-12 10:42:03,756 DEBUG              elasticsearch    log_request_success:66   < {...}\n2016-05-12 10:42:03,757 DEBUG          curator.cli.utils          check_version:90   Detected Elasticsearch version 1.6.0\n2016-05-12 10:42:03,757 DEBUG         urllib3.util.retry               from_int:164  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-05-12 10:42:04,268 DEBUG     urllib3.connectionpool          _make_request:395  \"GET /_snapshot/es-snapshots/_all HTTP/1.1\" 200 19871\n2016-05-12 10:42:04,320 INFO               elasticsearch    log_request_success:63   GET http://192.168.1.100:12345/_snapshot/es-snapshots/_all [status:200 request:0.563s]\n2016-05-12 10:42:04,320 DEBUG              elasticsearch    log_request_success:65   > None\n....\n2016-05-12 10:42:04,335 DEBUG     curator.cli.snapshot_selection              snapshots:68   All filters: []\n2016-05-12 10:42:04,335 INFO           curator.cli.utils                in_list:208  Adding 12-2015 from command-line argument\n2016-05-12 10:42:04,335 DEBUG     curator.cli.snapshot_selection              snapshots:82   ACTION: show will be executed against the following snapshots: [u'08-2015', u'09-2015', u'10-2015', u'11-2015', u'12-2015']\n2016-05-12 10:42:04,335 INFO      curator.cli.snapshot_selection              snapshots:84   Matching snapshots:\n08-2015\n09-2015\n10-2015\n11-2015\n12-2015\n```\nSame thing when delete the snapshot, it will always delete all snapshots from that repo\n. ",
    "wkruse": "Ok, thanks for the info.\n. ",
    "iainlbc": "Relevant: https://discuss.elastic.co/t/timeouts-while-deleting/24564/24\n\nIain Wright\nThis email message is confidential, intended only for the recipient(s)\nnamed above and may contain information that is privileged, exempt from\ndisclosure under applicable law. If you are not the intended recipient, do\nnot disclose or disseminate the message to anyone except the intended\nrecipient. If you have received this message in error, or are not the named\nrecipient(s), please immediately notify the sender by return email, and\ndelete all copies of this message.\nOn Wed, May 25, 2016 at 1:17 PM, reqless notifications@github.com wrote:\n\nCommand ::\nsudo curator --master-only --logfile /var/log/deletelog.log delete indices\n--time-unit days --older-then 30 --timestring %Y.%m.%d\nIt takes a while and then finishes. It does not delete any indices.\nThe log file says the following ::\n2016-05-25 19:54:23,658 ERROR Error deleting one or more indices. Run with\n--debug flag and/or check Elasticsearch logs for more information.\n2016-05-25 19:54:23,658 ERROR Got a TIMEOUT response from Elasticsearch.\nError message: HTTPConnectionPool(host=u'localhost', port=9200): Read timed\nout. (read timeout=30)\nSome Elasticsearch Logs ::\n[2016-05-25 20:10:41,498][DEBUG][action.admin.indices.delete] [Prod1]\n[ad-mssql-2016.03.27] failed to delete index\nProcessClusterEventTimeoutException[failed to process cluster event\n(delete-index [ad-mssql-2016.03.27]) within 30s]\nat\norg.elasticsearch.cluster.service.InternalClusterService$2$1.run(InternalClusterService.java:290)\nat\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nat\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nat java.lang.Thread.run(Thread.java:745)\n[2016-05-25 20:10:41,498][DEBUG][action.admin.indices.delete] [Prod1]\n[ad-app-sharepoint-2016.04.04] failed to delete index\nProcessClusterEventTimeoutException[failed to process cluster event\n(delete-index [ad-app-sharepoint-2016.04.04]) within 30s]\nat\norg.elasticsearch.cluster.service.InternalClusterService$2$1.run(InternalClusterService.java:290)\nat\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nat\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nat java.lang.Thread.run(Thread.java:745)\n[2016-05-25 20:10:41,498][DEBUG][action.admin.indices.delete] [Prod1]\n[ad-iis-2016.04.23] failed to delete index\nProcessClusterEventTimeoutException[failed to process cluster event\n(delete-index [ad-iis-2016.04.23]) within 30s]\nat\norg.elasticsearch.cluster.service.InternalClusterService$2$1.run(InternalClusterService.java:290)\nat\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nat\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nat java.lang.Thread.run(Thread.java:745)\nAny ideas?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/elastic/curator/issues/638\n. \n",
    "yissachar": "The snapshot action checks for any running snapshots and fails if there are any: https://github.com/elastic/curator/blob/master/curator/actions.py#L716\nIt's not clear to me what the intent of this check is. If it's just to prevent accidentally running the same snapshot twice, then the fix I mentioned above should work.\n. Ah, fair enough. I hadn't realized that bit about the versions. If this change doesn't make it into 3.x, I can perform the snapshots manually. Working with Curator is so much nicer though.\n. Understood, thanks for looking into this.\n. @untergeek FWIW, I reported the same thing for AWS ES 1.x a while ago: https://github.com/elastic/curator/issues/639. It would be good to add this to the compatibility matrix.\n. @iain17 The problem is not with the IAM policy rules, but the fact that Curator makes certain API calls that AWS Elasticsearch has blocked off for whatever reason.\nOur workaround was to manually write a script that performed the snapshot and deletes old indexes. We would have preferred to use Curator since it has many nice features and is more battle-tested, but until AWS opens up their endpoints our custom script works well enough.. ",
    "cjuroz": "@untergeek, as you may know AWS ES 5.3 has been released recently and they claim compatibility with Curator. I have been using Curator 5 API for index management through Lambda function.\nSo far, no problem with index cleaning BUT create snapshots is failing because AWS guys still didn't expose /_snapshot/_status endpoint. I am opening a bug through them https://forums.aws.amazon.com/thread.jspa?threadID=257549.\nJust to let you know about this or if @yissachar suggestion or another workaround could be applied...  . I'll just like to add this python script I adapted from @jarpy. It uses curator api and aws4auth to sign requests to AWS ES and can be run as a Lambda function with appropriate iam role.\nMost important: it works! It's aws es, curator and serverless :)\nHere the code: https://gist.github.com/cjuroz/d45f4d73e74f068892c5e4f3d1c7fa7c\n  . @jimmycuadra it's not a limitation of requests_aws4auth as you can see in my previous comment here: \nhttps://gist.github.com/cjuroz/d45f4d73e74f068892c5e4f3d1c7fa7c#file-serverless-curator-py-L32 \nA PR could be created to try open connection using IAM credentials. I think it should:\n\nread configurations, if missing\nread IAM credentials, try open ES connection with IAM credentials\nif IAM credentials missing then use default ES connection kwargs\n\n. I was on vacation these days :-) I created a new PR #1114 so I am going to close this one.. Created PR #1116 . ",
    "fgimian": "Hey @untergeek, we are about to deploy this tomorrow but it was hard to know what versions of Python are supported due to this omission.\nHowever, we have decided to use the YUM repos anyway and it is clear that our OS (RHEL 6 / Python 2.6.x) is supported which is great \ud83d\ude04 \nThanks\nFotis\n. Thanks :smile: \n. ",
    "benohara": "Thats was on the 3 tree, on the 4\npkg_resources.DistributionNotFound: The 'click>=3.3' distribution was not found and is required by elasticsearch-curator\nlooks like a package of the same release, version was made a couple of days apart, on depend on python <= 2.7, the other = 2.7 (on the 3 tree) \n. yum install python-click\nand\npkg_resources.DistributionNotFound: The 'elasticsearch<5.1.0,>=2.3.0' distribution was not found and is required by elasticsearch-curator\nbut\nyum info python-elasticsearch\nLoaded plugins: rhnplugin\nThis system is receiving updates from RHN Classic or RHN Satellite.\nInstalled Packages\nName        : python-elasticsearch\nArch        : noarch\nVersion     : 2.3.0\nRelease     : 1\nSize        : 441 k\nRepo        : installed\nFrom repo   : elasticsearch-curator-el6\nSummary     : Official low-level client for Elasticsearch. Its goal is to provide\nURL         : https://github.com/elastic/elasticsearch-py\nLicense     : Apache-2.0\nDescription : Official low-level client for Elasticsearch. Its goal is to provide\n            : common ground for all Elasticsearch-related code in Python; because of this it\n            : tries to be opinion-free and very extendable.\nthis is el6.7\n. ",
    "pavanraotk": "Thanks :). Would the elastic-search 2.2.0 still work with version 4 of curator, or would there be any version change in elastic search necessary\n. Last query, you mean to say the extra filters would look something like this:\ncurator delete indices --older-than {{ delete_indices_after_in_days }}   --time-unit days --timestring %Y.%m.%d --regex '^{{ elasticsearch_index_name }}'\n< ---- > would be the content of the added new filter or we create an action file(even if it's a single action) https://www.elastic.co/guide/en/elasticsearch/client/curator/4.0/actionfile.html as mentioned here.\nIf it's the action file, say action.yaml, how do I run this action.yaml, curator action.yaml would work?\n. ",
    "kdubezerra": "Is this feature going to be part of 4.1.0? If not, how can I achieve the same with, say, curator version 4.0.6?\n. Sweet! I don't know if this is the right place to ask, but how would you use the filter-by-count feature to remove all indices, except for the N most recent indices?\n. Thanks a lot for replying! I'll be using this feature as soon as the curator 4.1.0 gets to PyPI. And I'm sorry, my question was wrong, as I meant to ask how to remove all but the N most recent ones. I've updated the comment above to reflect what I really intended to ask. In that case, all I'd have to change from the code you posted would be setting reverse to True, correct? And you're right, I'dd also add a filter to get only indices that match a \"logstash-\" prefix. Thanks again!\n. Hmmm thinking again, wouldn't that always delete only the 10 oldest indices? I think that matching everything that begins with 'logstash-' (thanks for the single vs. double quote heads up!), then excluding the 10 youngest ones would be what we want, no?\n```\nactions:\n  1:\n    action: delete_indices\n    description: >-\n      Delete all indices preffixed with \"logstash-\", except for the $count most\n      recent ones. Ignore the error if the filter does not result in an\n      actionable list of indices (ignore_empty_list) and exit cleanly.\n    options:\n      ignore_empty_list: True\n      continue_if_exception: False\n      disable_action: False\n    filters:\n    - filtertype: pattern\n      kind: prefix\n      value: 'logstash-'\n    - filtertype: count\n      count: 10\n      reverse: False\n      use_age: True\n      source: creation_date\n      exclude: True\n```\n. Got it, and thanks a lot for all the feedback, Aaron!\n. ",
    "skundrik": "But you still need to bootstrap curator into particular ES in the first place before you can retrieve action file. Or am I missing something?\n. Yes, I was suggesting that even client configuration file might come over URL. Securing the connection/information is of course another problem that should be considered by the user. However this is less of an issue in dev/test environments.\n. ",
    "pierro42100": "\nI use a wrapper script, and call the script via cron.\n\nOk, I try to do something like that, but it's not working.\nCan you give me an example ?\nThank you\n. ",
    "evanv": "I'll clone and see what I can do as well. The compression settings would be a nice add imo. But then again maybe just moving the indices to \"cold\" nodes would work... might be overkill. Not sure. \n. That's a good question. I know changing the allocation type doesn't trigger a force merge. I'll check compression settings in a moment. I should also have time to work on this later this week, if you need a hand with it. \n. Ah, bummer.  Gotta close the index first. \n```\ncurl -XPUT 'https://localhost:9200/prediction_index/_settings' -d '{\"settings\": {\"index.codec\": \"best_compression\" } }'\n{\"error\":{\"root_cause\":[{\"type\":\"remote_transport_exception\",\"reason\":\"[localhost][localhost:9300][indices:admin/settings/update]\"}],\"type\":\"illegal_argument_exception\",\"reason\":\"Can't update non dynamic settings[[index.codec]] for open indices [[prediction_index]]\"},\"status\":400}%\n```\n. I was thinking the exact same thing (nice call on YAML/command chaining by the way). Tomorrow's pretty busy. I should be able to work on this Thursday/Friday unless you beat me to it. \n. Xref my stupid typo here: https://github.com/elastic/curator/issues/730  \nArguably some acceptable keywords in the config might make sense....\n. Derp. And... of course that fixes it. I'm an idiot. Thank you sir!\n. I'm good at typos. If oyu need any help testing that out, I'm more than hippy to help. \n. If you're still looking for use cases, there are three that come to mind immediately for me:\n1.  Reindex daily data into weekly or monthly indices after then end of the week / month. There are two clusters I'm working on right now that would benefit from functionality like this. The pattern in both cases would be combining daily indices into either weekly or monthly, once the week/month is over. \n2. Reduce shard count after index becomes immutable. This use case would probably be replaced by https://www.elastic.co/guide/en/elasticsearch/reference/5.0/indices-shrink-index.html. But then again the cases where I've done this have always had to do with starting with a higher shard count to parallelize writes and then shrinking the shard count back down once the index is immutable. Other words, the first use case is much more generic and powerful than this use case. \n3. Reindexing old data to account for schema incompatibilities introduced by dynamic mappings and/or schema evolution. It is not uncommon to see log data evolve as users gain more experience with Kibana / ES. A great example that came to mind recently was status code in Nginx logs. Initially IP addresses were indexed as strings, before the devs realized they could be indexed as IP. We changed this to an IPv4 mapping in the index template. But, in doing so, we also introduced a backwards incompatibility in the mappings for the alias. The conflict itself can be avoided by never changing field names -- but a lot of times it's important to be able to retroactively fix some mapping in log data, and that usually involves ad hoc scripts either scanning and scrolling or using the _reindex API. \n. The unexpected behavior occurs when using delete aliases. sorry that wasn't clear. Let me be more concrete.\nWith /cat/indices output of\nhealth status index                        pri rep docs.count docs.deleted store.size pri.store.size \ngreen  open   .marvel-es-1-2016.09.17        1   1   21591010        48656     19.9gb          9.9gb \ngreen  open   .kibana                        1   1         10            0    146.6kb         73.3kb \ngreen  open   .marvel-es-1-2016.09.19        1   1   27449391        46799     20.3gb         10.1gb \ngreen  open   .marvel-es-data-1              1   1       1509          116      1.4mb        350.2kb \ngreen  open   .marvel-es-1-2016.09.18        1   1   36045047        63832     26.4gb         13.1gb \ngreen  open   .marvel-es-1-2016.09.20        1   1   26587477       123860     19.9gb          9.9gb \ngreen  open   .marvel-es-1-2016.09.22        1   1   23443172        41974     16.6gb          8.3gb \ngreen  open   .marvel-es-1-2016.09.21        1   1   26336111        95626       21gb         11.9gb\nand /cat/aliases of\nalias  index   filter routing.index routing.search \nkibana .kibana -      -             -\nand /tasks/marvel.yaml of\n1:\n    action: close\n    description: >-\n      close indices\n    options:\n      continue_if_exception: True\n      ignore_empty_list: True      \n      delete_aliases: True\n      timeout_override: 300\n    filters:\n      - filtertype: pattern\n        kind: prefix\n        value: .marvel-es-1\n        exclude: False\n      - filtertype: age\n        source: creation_date\n        direction: older\n        unit: days\n        unit_count: 2\n        exclude: False \n  2:\n    action: delete_indices\n    description: >-\n      delete indices\n    options:\n      continue_if_exception: True\n      ignore_empty_list: True      \n      timeout_override: 300\n    filters:\n      - filtertype: pattern\n        kind: prefix\n        value: .marvel-es-1\n        exclude: False\n      - filtertype: age\n        source: creation_date\n        direction: older\n        unit: days\n        unit_count: 5\n        exclude: False\nI would expect that several of these marvel indices are closed, despite the fact they have no aliases. Instead, they produce errors like:\n{\"@timestamp\": \"2016-09-22T11:49:14.759Z\", \"function\": \"log_request_fail\", \"linenum\": 82, \"loglevel\": \"WARNING\", \"message\": \"DELETE /.marvel-es-1-2016.09.17,.marvel-es-1-2016.09.18,.marvel-es-1-2016.09.19,.marvel-es-1-2016.09.20/_alias/_all [status:404 request:0.019s]\", \"name\": \"elasticsearch\"}\n{\"@timestamp\": \"2016-09-22T11:49:14.760Z\", \"function\": \"cli\", \"linenum\": 242, \"loglevel\": \"ERROR\", \"message\": \"Failed to complete action: close.  <class 'curator.exceptions.FailedExecution'>: Exception encountered.  Rerun with loglevel DEBUG and/or check Elasticsearch logs for more information. Exception: TransportError(404, u'aliases_not_found_exception', u'aliases [kibana] missing')\", \"name\": \"curator.cli\"}\nThis is not problematic on this cluster. But it is a bit counterintuitive. Also, on clusters which have both aliased and non aliased indices (which we have in our other clusters), this forces you to write an additional close task that closes and does not remove aliases. \n. I should also clarify, if I remove the delete_aliases: True stanza, it closes the indices correctly. For this particular cluster it is not an issue. But on clusters in which some of the indices are aliased and some are not, it leads to Bad Things\u00ae\n. ```\ncurator --version\ncurator, version 4.0.0\n```\nSo upgrade? \n. Ah yes, I see https://github.com/elastic/curator/issues/736.  upgrading now. Will test and then close. \n. Yep. That's what it was. Closing. Thank you. \n. It makes sense what you are saying and I share that concern... but then again if it takes a minute or two to generate the metadata (which it does on large clusters), there's nothing Curator can do to guarantee the state hasn't changed underneath it from the time it generated its metadata to the time it begins executing tasks. \nI'm on the fence here. Is giving the user the ability to cache the metadata it generates making an assumption that Curator doesn't already make, or is it just changing how long Curator regards that assumption as valid? I definitely wouldn't default to caching it between chained tasks... but I'm not sure that I can argue that giving Curator the ability to cache its index lists between tasks (instead of creating them anew) is inherently more or less risky. \n. Actually if you consider long running tasks like force merging, you do wind up making an assumption about the state not changing out from underneath you. To avoid that assumption, Curator would need to regenerate an index list between each index it force merged. I don't believe it does that today, does it? \n. > I have some ideas on how to preserve metadata, and check for index presence if using cached metadata.\nThat's an interesting idea. Cache the metadata, potentially, but perform existence and openness checks (since they are cheaper). Is that what you're thinking? \nI haven't had a chance to play with the task API in 5.0 yet but I vaguely recall a ticket / discussion about task history. If that is a thing and I'm not totally pulling that out of thin air, it's possible that could play into this as well. \n. ",
    "mayoung8": "Is this still a targeted feature? I force merge my indices after one day as they are time based and would love to also compress them via curator if possible. I'd prefer not to have to compress the current days indices as they are very hot.. @untergeek, would you expect curator 6.0 to be available with elasticsearch 6.0 GA release?. ",
    "ryancurrah": "The timestamp's timezone produced by Curator (Great job on 4.0 btw) is in the CET timezone (Same as the server). When collecting logs using Filebeat (Which I assumed most users do), Filebeat expects the timestamp to be in UTC. So in Eleasticsearch the Curator events are showing up 6 hours behind when they acutally happened. See Filebeat docs about @timestamp field here. \nThanks for the quick response!\n. The servers timezone is set for CET. As for the beats timestamp i agree but filebeats input on logstash expects the @timstamp field to be in UTC. Curators @timestamp field needs to be UTC to or not use that field name. So when the json filter in logstash overwrites the filebeat field with curator field @timestamp there will be no issue.\n. @untergeek wow that was fast. I'm AFK today I will give it a try first thing tomorrow, or tonight if I get a chance.\n. @untergeek FYI i made some comments on your PR #662 , thanks again!\n. I just patched this and while the formatting did change as expected, the timezone is reporting as UTC (+0000) while it should be -0600, so it doesn't seem to work. I did some googling to figure out how get the correct timezone seems it's a common issue. Stackoverflow article here shows you could just log everything in GMT/UTC timezone to simplify things or another option pointed out is to use an external library like the pytz module Python logging module emits wrong timezone information. As it seems the logging function formatTime uses the time module which has poor support for timezone awareness.\njson\n{\"@timestamp\": \"2016-06-28T12:39:33.977+0000\", \"function\": \"cli\", \"linenum\": 236, \"loglevel\": \"ERROR\", \"message\": \"Unable to complete action \\\"delete_indices\\\".  No actionable items in list: <class 'curator.exceptions.NoIndices'>\", \"name\": \"curator.cli\"}\nPrinted the date on server\nbash\n[rcurrah@HOSTNAMEREDACTED ~]# date\nTue Jun 28 13:26:15 CDT 2016\nIf I print out the GMT/UTC time it is correct... \n\n. Just to add another comment I did get it to work by forcing to log in UTC time..\nSo on top of your changes I added...\nImported time module...\npython\nimport time\nSet logging converter attribute to a function with the same signature as time.gmtime right above your timestamp variable\npython\nself.converter = time.gmtime\nAnd ran Curator... the logs are now in proper UTC time...\njson\n{\"@timestamp\": \"2016-06-28T18:50:17.240+0000\", \"function\": \"cli\", \"linenum\": 236, \"loglevel\": \"ERROR\", \"message\": \"Unable to complete action \\\"delete_indices\\\".  No actionable items in list: <class 'curator.exceptions.NoIndices'>\", \"name\": \"curator.cli\"}\nKibana/Elasticsearch is happy now too... (EDIT: The reason it shows 14'00 hours not 13'00 in Kibana is because my locality/browser time is 1 hour ahead of the server)\n\n. @untergeek sorry the delayed response, i applied the changes you made (reverting back the timestamp) and tested it. It is working fine. Thanks for the quick turn around!\n. ",
    "leonhoffman": "You are correct and that is a requirement, however that does not make any difference.  I removed the repo name from the file before I uploaded it.  I should have stated that.\n. I am attaching it (output with the Debug flag on) since it is a bit of info to paste.\noutput-with-debug.txt\n. Ok @untergeek  Thanks for digging into this: I thought for some reason that when you use the \"pattern\" filtertype that it would first just look at the value that I had listed then look at the creation_date in the age filter.  I wasn't aware that it would first retrieve all creation_data meta data, so I was just looking for the creation_date meta data for the index I had listed, and that threw me.\nThat makes sense though, Thank you!\n. Yeah - those are - I deleted them\n2016-06-28 13:02:11,137 INFO      Snapshot curator-20160628190210 successfully completed.\n. Thanks for your help.\n. Cool that's fixed.  Thanks\n. ",
    "morero": "Thanks! \nI'm not really creating them often. And it's not critical. Just wanted to automate the process (planned on a few test cases in a clean environment to verify my curator setup).\n. ",
    "PMDubuc": "Yes.  Thanks very much!\n. ",
    "kylehendricks": "I just thought that the snapshot per index approach made more sense for my use case (logs).\n2 reaons:\n1.  Easier to move old snapshot data from AWS S3 storage to glacier storage.\n2.  Easier to restore just a few days of logs from some point in the past.\nI'm new to snapshotting with elasticsearch.  Is there a better way to do this?\n. Thanks!  I didn't realize that.\n. ",
    "hongyuan1306": "It was my fault not having spent time to carefully read the output of dry run.\nActually I did use --dry-run. But as a new user of curator, the output seems somewhat cryptic to understand. By the way, it did not say there was any error with my configuration. So I just let it go.\nHowever, with due respect, it did show which indices were to be removed.\n. ",
    "shavo007": "Thanks for the reply mate. So i changed the config to be like so:\n```\nRemember, leave a key empty if there is no value.  None will be a string,\nnot a Python \"NoneType\"\nclient:\n  hosts:\n    - \"search-sl-es-cluster-xyz.ap-southeast-2.es.amazonaws.com:9200\"\n  port:\n  url_prefix:\n  use_ssl: False\n  certificate:\n  client_cert:\n  client_key:\n  aws_key: \"xyz\"\n  aws_secret_key: \"xyz\"\n  aws_region: \"ap-southeast-2\"\n  ssl_no_validate: False\n  http_auth:\n  timeout: 30\n  master_only: False\nlogging:\n  loglevel: DEBUG\n  logfile:  /usr/share/curator/curator.log\n  logformat: default\n```\nI removed the port.\nI run the command \ncurator --config config/curator.yml config/create\n_location_index_curator.yml\nThe logs are:\n```\n2016-07-04 12:06:40,999 DEBUG                curator.cli                    cli:165  default_timeout = 30\n2016-07-04 12:06:41,005 DEBUG                curator.cli                    cli:170  Full list of actions: {1: {'action': 'create_index', 'description': 'Create the index as named, with the specified extra settings.', 'options': {'continue_if_exception': False, 'extra_settings': {'mappings': {'type1': {'properties': {'field1': {'index': 'not_analyzed', 'type': 'string'}}}}, 'settings': {'number_of_replicas': 1, 'number_of_shards': 1}}, 'timeout_override': None, 'name': 'locations', 'disable_action': False}}}\n2016-07-04 12:06:41,005 INFO                 curator.cli                    cli:177  Action #1: create_index\n2016-07-04 12:06:41,005 DEBUG                curator.cli                    cli:190  continue_if_exception = False\n2016-07-04 12:06:41,005 DEBUG                curator.cli                    cli:191  timeout_override = None\n2016-07-04 12:06:41,006 DEBUG              curator.utils             get_client:497  kwargs = {'aws_secret_key': 'xyz', 'url_prefix': '', 'http_auth': None, 'certificate': None, 'aws_key': 'xyz', 'aws_region': 'ap-southeast-2', 'client_cert': None, 'hosts': ['search-sl-es-cluster-xyz.ap-southeast-2.es.amazonaws.com:9200'], 'timeout': 30, 'use_ssl': False, 'master_only': False, 'port': 9200, 'ssl_no_validate': False, 'client_key': None}\n```\nConsole error is:\n```\nTraceback (most recent call last):\n  File \"/usr/bin/curator\", line 9, in \n    load_entry_point('elasticsearch-curator==4.0.1', 'console_scripts', 'curator')()\n  File \"/usr/lib/python2.7/site-packages/curator/curator.py\", line 5, in main\n    cli()\n  File \"/usr/lib/python2.7/site-packages/click/core.py\", line 716, in call\n    return self.main(args, kwargs)\n  File \"/usr/lib/python2.7/site-packages/click/core.py\", line 696, in main\n    rv = self.invoke(ctx)\n  File \"/usr/lib/python2.7/site-packages/click/core.py\", line 889, in invoke\n    return ctx.invoke(self.callback, ctx.params)\n  File \"/usr/lib/python2.7/site-packages/click/core.py\", line 534, in invoke\n    return callback(args, kwargs)\n  File \"/usr/lib/python2.7/site-packages/curator/cli.py\", line 215, in cli\n    client = get_client(client_args)\n  File \"/usr/lib/python2.7/site-packages/curator/utils.py\", line 537, in get_client\n    kwargs['region'], 'es')\n  File \"/usr/lib/python2.7/site-packages/requests_aws4auth/aws4auth.py\", line 247, in init\n    self.regenerate_signing_key(secret_key=secret_key)\n  File \"/usr/lib/python2.7/site-packages/requests_aws4auth/aws4auth.py\", line 302, in regenerate_signing_key\n    store_secret_key)\n  File \"/usr/lib/python2.7/site-packages/requests_aws4auth/aws4signingkey.py\", line 97, in init\n    self.service, self.date)\n  File \"/usr/lib/python2.7/site-packages/requests_aws4auth/aws4signingkey.py\", line 116, in generate_key\n    region_key = cls.sign_sha256(date_key, region)\n  File \"/usr/lib/python2.7/site-packages/requests_aws4auth/aws4signingkey.py\", line 136, in sign_sha256\n    return hmac.new(key, msg, hashlib.sha256).digest()\n  File \"/usr/lib/python2.7/hmac.py\", line 136, in new\n    return HMAC(key, msg, digestmod)\n  File \"/usr/lib/python2.7/hmac.py\", line 78, in init\n    self.update(msg)\n  File \"/usr/lib/python2.7/hmac.py\", line 86, in update\n    self.inner.update(msg)\nTypeError: update() argument 1 must be string or buffer, not bool\n```\nAny clue!?\nThanks,\nShane.\n. no problem at all!\nIt is not important. Enjoy your holiday. I just wanted to test it out with my docker image...\nI did try below but still get the same error FYI:\n```\nhosts:\n    - \"search-sl-es-cluster-xyz.ap-southeast-2.es.amazonaws.com\"\n  port: 443\n```\nSpeak again when you get back. \n. No rush! \n. @untergeek is there a new release to verify this?\n. I can confirm that this works with version 4.0.5. Verified against aws managed es service 2.3\n. awesome! let me know @nfelsen  how you go.\nmy team uses aws 5.1 and we will look to upgrade and test out curator in the future. . Awesome news  @jitran . thanks for the update @untergeek  i assumed the other issue was just for replicas. . ",
    "porto88": "+1, running into this same issue.\nCurator version 4.0.1, python 2.7\n. ",
    "kuttor": "Also having an issue with this... \nrator-actions.yml\n2016-07-13 00:49:03,907 INFO      Action #1: snapshot\nTraceback (most recent call last):\n  File \"/usr/local/bin/curator\", line 9, in \n    load_entry_point('elasticsearch-curator==4.0.1', 'console_scripts', 'curator')()\n  File \"/usr/local/lib/python2.7/dist-packages/curator/curator.py\", line 5, in main\n    cli()\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 716, in call\n    return self.main(_args, *_kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 696, in main\n    rv = self.invoke(ctx)\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 889, in invoke\n    return ctx.invoke(self.callback, ctx.params)\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 534, in invoke\n    return callback(_args, *_kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/curator/cli.py\", line 215, in cli\n    client = get_client(client_args)\n  File \"/usr/local/lib/python2.7/dist-packages/curator/utils.py\", line 537, in get_client\n    kwargs['region'], 'es')\n  File \"/usr/local/lib/python2.7/dist-packages/requests_aws4auth/aws4auth.py\", line 247, in init\n    self.regenerate_signing_key(secret_key=secret_key)\n  File \"/usr/local/lib/python2.7/dist-packages/requests_aws4auth/aws4auth.py\", line 302, in regenerate_signing_key\n    store_secret_key)\n  File \"/usr/local/lib/python2.7/dist-packages/requests_aws4auth/aws4signingkey.py\", line 97, in init\n    self.service, self.date)\n  File \"/usr/local/lib/python2.7/dist-packages/requests_aws4auth/aws4signingkey.py\", line 116, in generate_key\n    region_key = cls.sign_sha256(date_key, region)\n  File \"/usr/local/lib/python2.7/dist-packages/requests_aws4auth/aws4signingkey.py\", line 136, in sign_sha256\n    return hmac.new(key, msg, hashlib.sha256).digest()\n  File \"/usr/lib/python2.7/hmac.py\", line 133, in new\n    return HMAC(key, msg, digestmod)\n  File \"/usr/lib/python2.7/hmac.py\", line 75, in init\n    self.update(msg)\n  File \"/usr/lib/python2.7/hmac.py\", line 83, in update\n    self.inner.update(msg)\nTypeError: must be string or buffer, not bool\n. I definitely will... Currently in my stack now tasked on getting this to function. \nDefine the changes. \n. \"This suggests that the hosts entry in the config needs to be a single host, with no port assigned, the port entry needs to be 443, and kwargs['hosts'] = [{'host': kwargs['host'], 'port': 443}] needs to be inserted after line 530 in utils.py in the source.\"\n. The line \"kwargs['hosts'] = [{'host': kwargs['host'], 'port': 443}]\" produced:\n\nFile \"/tmp/curator/curator/utils.py\", line 531, in get_client\n    kwargs['hosts'] = [{'host': kwargs['host'], 'port': 443}]\nKeyError: 'host'\n\nchanged kwargs['host'] to kwargs['hosts'] which produced: \n\nTypeError: must be string or buffer, not bool\n\nSnip of code insert:\n\n# Override these kwargs\n        kwargs['hosts'] = [{'host': kwargs['hosts'], 'port': 443}]\n        kwargs['use_ssl'] = True\n        kwargs['verify_certs'] = True\n        kwargs['connection_class'] = elasticsearch.RequestsHttpConnection\n        kwargs['http_auth'] = (\n            AWS4Auth(\n                kwargs['aws_key'], kwargs['aws_secret_key'],\n                kwargs['region'], 'es')\n        )\n\nConfig: \n\nhosts:\n    - \"10.200.65.109\"\n  port: 443\n. Traceback Snip: \nFile \"/tmp/curator/curator/utils.py\", line 535, in get_client\n    kwargs['http_auth'] = AWS4Auth(kwargs['aws_key'], kwargs['aws_secret_key'],kwargs['region'], 'es')\n\nCode Snip: \n\n# Override these kwargs\n        kwargs['hosts'] = [{'host': kwargs['hosts'], 'port': 443}]\n        kwargs['use_ssl'] = True\n        kwargs['verify_certs'] = True\n        kwargs['connection_class'] = elasticsearch.RequestsHttpConnection\n        kwargs['http_auth'] = AWS4Auth(kwargs['aws_key'], kwargs['aws_secret_key'],kwargs['region'], 'es')\n. > Traceback (most recent call last):\n  File \"./run_curator.py\", line 8, in \n    main()\n  File \"/tmp/curator/curator/curator.py\", line 5, in main\n    cli()\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 716, in call\n    return self.main(_args, *_kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 696, in main\n    rv = self.invoke(ctx)\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 889, in invoke\n    return ctx.invoke(self.callback, ctx.params)\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 534, in invoke\n    return callback(_args, *_kwargs)\n  File \"/tmp/curator/curator/cli.py\", line 215, in cli\n    client = get_client(client_args)\n  File \"/tmp/curator/curator/utils.py\", line 535, in get_client\n    kwargs['http_auth'] = AWS4Auth(kwargs['aws_key'], kwargs['aws_secret_key'],kwargs['region'], 'es')\n  File \"/usr/local/lib/python2.7/dist-packages/requests_aws4auth/aws4auth.py\", line 247, in init\n    self.regenerate_signing_key(secret_key=secret_key)\n  File \"/usr/local/lib/python2.7/dist-packages/requests_aws4auth/aws4auth.py\", line 302, in regenerate_signing_key\n    store_secret_key)\n  File \"/usr/local/lib/python2.7/dist-packages/requests_aws4auth/aws4signingkey.py\", line 97, in init\n    self.service, self.date)\n  File \"/usr/local/lib/python2.7/dist-packages/requests_aws4auth/aws4signingkey.py\", line 116, in generate_key\n    region_key = cls.sign_sha256(date_key, region)\n  File \"/usr/local/lib/python2.7/dist-packages/requests_aws4auth/aws4signingkey.py\", line 136, in sign_sha256\n    return hmac.new(key, msg, hashlib.sha256).digest()\n  File \"/usr/lib/python2.7/hmac.py\", line 133, in new\n    return HMAC(key, msg, digestmod)\n  File \"/usr/lib/python2.7/hmac.py\", line 75, in init\n    self.update(msg)\n  File \"/usr/lib/python2.7/hmac.py\", line 83, in update\n    self.inner.update(msg)\nTypeError: must be string or buffer, not bool\n. Ah, before executing the console will quickly import requests. \n\nBy the way, the page does apparently suggest that. \n. In [2]: AWS4Auth('key', 'secret', 'us-west-2', 'es')\nOut[2]: <requests_aws4auth.aws4auth.AWS4Auth at 0x7f5baac049d0>\nIn addition importing requests produced no apparent change in the traceback. \n. Dang, negative. \n\nfrom datetime import timedelta, datetime, date\nimport requests\nfrom requests_aws4auth import AWS4Auth\nimport elasticsearch\nimport copy\nimport time\nimport re\nimport sys\nimport logging\nimport yaml\n...\n...\n  File \"/usr/local/lib/python2.7/dist-packages/requests_aws4auth/aws4signingkey.py\", line 116, in generate_key\n    region_key = cls.sign_sha256(date_key, region)\n  File \"/usr/local/lib/python2.7/dist-packages/requests_aws4auth/aws4signingkey.py\", line 136, in sign_sha256\n    return hmac.new(key, msg, hashlib.sha256).digest()\n  File \"/usr/lib/python2.7/hmac.py\", line 133, in new\n    return HMAC(key, msg, digestmod)\n  File \"/usr/lib/python2.7/hmac.py\", line 75, in init\n    self.update(msg)\n  File \"/usr/lib/python2.7/hmac.py\", line 83, in update\n    self.inner.update(msg)\n TypeError:  must be string or buffer, not bool\n. Yea, I been working on getting this implemented for almost two days now (until I stumbled on this thread earlier today). Have a fairly large ELK stack environment and would rather implement curator than some data node auto-scaling and/or cron data exporting to S3 solution. \n\nAs you appreciate mine, thanks for your time as well. \n. Ah, I see -- Yeah, absolutely. \n. Within the AWS enviro (Even directly in the Master cluster) would that affect the configuration -- figured it would be the same...  hmmm, I suppose there could be a instance of Curator per data node installed locally? If so, then the AWS option wouldn't be required...?\n. Rock on. So, let's say I have a standard three ES Master / three ES Data cluster in the same VPC...  would Curator need to live locally within one of the master nodes or just be within that VPC?\n. Thanks much. \n. ",
    "basex": "I'm having exactly the same problem as above with the same traceback. I'm unable to sign the request. Is there a solution for this?\ncurator, version 4.0.4\n. I'm having the same problem. Made a post on the AWS forums related with this https://forums.aws.amazon.com/thread.jspa?threadID=236643&tstart=0\nI think there is no point in support the AWS Credentials in Curator 4 if there is no way this tool can work with AWS ES.\n. Thanks. I have signed and changed the CONTRIBUTORS file and Changelog.rst.\n. I signed again with the email used on Github.\n. I understand. I couldn't find the Getting Started page on the current path.\nFor me this pull request can be closed.. Sorry for the delay, corrected with your link.. ",
    "joshuaspence": "Fair enough, thanks for the response.\n. ",
    "swladyka": "It is possible to use boto3 session for retrieving credentials (this is probably the most common way of injecting these), please see: https://github.com/sam-washington/requests-aws4auth/issues/14\nThat would enable option to use either STS temporary credentials or roles for signing. User credentials are obviously also possible with this approach.\nThe question is whether this could replace current config parameters, or these should remain?. ",
    "sherzberg": "edited a typo in the empty list check call\n. @untergeek thanks for this info. I am not going to be able make this change today.  Please move forward with however you feel best. I would have some time this weekend or next week to get back to this though if you have anything left overt todo.\nThanks again!\n. \ud83d\udc4d thanks for fixing this!\n. ",
    "jpsandiego": "Which version of curator are you running with?\nIs that command line syntax valid with the current versions (v4) of curator? I think everything has moved to YAML.\n. BTW - here's an example from an older Curator running against a different (ES 1.7) cluster:\n\n2016-07-07 16:25:01,834 INFO      Job starting: delete indices\n2016-07-07 16:25:01,840 INFO      Pruning Kibana-related indices to prevent accidental deletion.\n2016-07-07 16:25:01,840 INFO      Action delete will be performed on the following indices: [u'logstash-2016.07.01']\n2016-07-07 16:25:01,841 INFO      Deleting indices as a batch operation:\n2016-07-07 16:25:01,841 INFO      ---deleting index logstash-2016.07.01\n2016-07-07 16:25:03,337 INFO      Job completed successfully.\n. I think the selection criteria log entries could be made DEBUG:\n\"Removed from actionable list...\"\n\"Index ... is not actionable, removing from list.\"\n\"Index ... is actionable and remains in the list.\"\n\nPerhaps an alternative is a \"summary\" option? Then I could just turn up the logging to WARNING/ERROR and still get some output.\n. ",
    "slmingol": "Yeah I did a install using pip. I'm using 4.0.1.\n. OK, thanks @untergeek - I'll go do my homework and read up on that, sorry I bothered everyone, I didn't realize there was a significant change to the switches. My bad!\n. ",
    "IlyaSukhanov": "Thank you for clarification!\n. ",
    "flybd5": "Hmm. \"Curator 3.x persists for those who are compelled, for whatever reason, to stay in that space.\" Seem to me that \"whatever reason\" is that this doesn't work on AWS ES. Just wasted three hours before I found about this. Maybe you want to put this severe incompatibility up front on the docs and not wait until people find this issue.\n. > I'll see about adding a blurb to the README.\nThis is really all that you had to say.\n. ",
    "dzavalkinolx": "@untergeek What is the point to have aws key and secret options in the config if Curator is not compatible with AWS ES at all? First I had to find out out that I have to install requests-aws4auth manually (hilarious) and then spend 3 hours to play with IAM role permissions to find out that you decided for some reason to make optional API required and you refuse to support the way Curator 3 worked? The worst documentation and attitude I've ever seen...\n. ",
    "Mogztter": "Hello @untergeek,\nThe first task is doing exactly what I want but the second task is removing all my indices where filters are:\nfilters:\n    - filtertype: pattern\n      kind: prefix\n      value: logstash-\n      exclude:\n    - filtertype: age\n      source: creation_date\n      direction: older\n      unit: weeks\n      unit_count: 5\n      exclude:\n2016-07-11 11:45:29,582 INFO      Index logstash-2016.27 is actionable and remains in the list.\n2016-07-11 11:45:29,582 INFO      Index logstash-2016.26 is actionable and remains in the list.\n2016-07-11 11:45:29,582 INFO      Index logstash-2016.25 is actionable and remains in the list.\n2016-07-11 11:45:29,582 INFO      Index logstash-2016.24 is actionable and remains in the list.\n2016-07-11 11:45:29,582 INFO      Index logstash-2016.23 is actionable and remains in the list.\n2016-07-11 11:45:29,582 INFO      Index logstash-2016.28 is actionable and remains in the list.\n2016-07-11 11:45:29,582 INFO      Index logstash-2016.27 is actionable and remains in the list.\nAll good all my indices are prefixed by logstash-.\nThen Curator should remove indices that are older than 5 weeks but...\n2016-07-11 11:45:29,582 INFO      Remains in actionable list: Index \"logstash-2016.27\" age (1467712937), direction: \"older\", point of reference, (1465206329)\n2016-07-11 11:45:29,582 INFO      Index logstash-2016.26 is actionable and remains in the list.\n2016-07-11 11:45:29,582 INFO      Remains in actionable list: Index \"logstash-2016.26\" age (1467108137), direction: \"older\", point of reference, (1465206329)\n2016-07-11 11:45:29,582 INFO      Index logstash-2016.25 is actionable and remains in the list.\n2016-07-11 11:45:29,582 INFO      Remains in actionable list: Index \"logstash-2016.25\" age (1466503337), direction: \"older\", point of reference, (1465206329)\n2016-07-11 11:45:29,582 INFO      Index logstash-2016.24 is actionable and remains in the list.\n2016-07-11 11:45:29,582 INFO      Remains in actionable list: Index \"logstash-2016.24\" age (1465898537), direction: \"older\", point of reference, (1465206329)\n2016-07-11 11:45:29,582 INFO      Index logstash-2016.23 is actionable and remains in the list.\n2016-07-11 11:45:29,582 INFO      Remains in actionable list: Index \"logstash-2016.23\" age (1465293737), direction: \"older\", point of reference, (1465206329)\n2016-07-11 11:45:29,582 INFO      Index logstash-2016.28 is actionable and remains in the list.\n2016-07-11 11:45:29,582 INFO      Remains in actionable list: Index \"logstash-2016.28\" age (1468317737), direction: \"older\", point of reference, (1465206329)\n2016-07-11 11:45:29,582 INFO      Deleting selected indices\n2016-07-11 11:45:29,582 INFO      ---deleting index logstash-2016.27\n2016-07-11 11:45:29,582 INFO      ---deleting index logstash-2016.26\n2016-07-11 11:45:29,582 INFO      ---deleting index logstash-2016.25\n2016-07-11 11:45:29,582 INFO      ---deleting index logstash-2016.24\n2016-07-11 11:45:29,582 INFO      ---deleting index logstash-2016.23\n2016-07-11 11:45:29,582 INFO      ---deleting index logstash-2016.28\n. Sorry here's the debug output:\n2016-07-21 17:45:30,940 DEBUG          curator.indexlist     iterate_over_stats:119  Index: logstash-2016.27  Size: 318.0B  Docs: 0\n2016-07-21 17:45:30,940 DEBUG          curator.indexlist     iterate_over_stats:119  Index: logstash-2016.26  Size: 318.0B  Docs: 0\n2016-07-21 17:45:30,940 DEBUG          curator.indexlist     iterate_over_stats:119  Index: logstash-2016.25  Size: 318.0B  Docs: 0\n2016-07-21 17:45:30,940 DEBUG          curator.indexlist     iterate_over_stats:119  Index: logstash-2016.24  Size: 318.0B  Docs: 0\n2016-07-21 17:45:30,940 DEBUG          curator.indexlist     iterate_over_stats:119  Index: logstash-2016.23  Size: 318.0B  Docs: 0\n2016-07-21 17:45:30,940 DEBUG          curator.indexlist     iterate_over_stats:119  Index: logstash-2016.28  Size: 318.0B  Docs: 0\n2016-07-21 17:45:30,940 DEBUG          curator.indexlist        iterate_filters:694  Iterating over a list of filters\n2016-07-21 17:45:30,940 DEBUG          curator.indexlist        iterate_filters:700  All filters: [{'exclude': None, 'kind': 'prefix', 'filtertype': 'pattern', 'value': 'logstash-'}, {'source': 'creation_date', 'direction': 'older', 'unit_count': 5, 'exclude': None, 'filtertype': 'age', 'unit': 'weeks'}]\n2016-07-21 17:45:30,940 DEBUG          curator.indexlist        iterate_filters:702  Top of the loop: [u'logstash-2016.27', u'logstash-2016.26', u'logstash-2016.25', u'logstash-2016.24', u'logstash-2016.23', u'logstash-2016.28']\n2016-07-21 17:45:30,940 DEBUG              curator.utils        iterate_filters:703  Un-parsed filter args: {'exclude': None, 'kind': 'prefix', 'filtertype': 'pattern', 'value': 'logstash-'}\n2016-07-21 17:45:30,940 DEBUG              curator.utils        iterate_filters:729  Filter args: {'exclude': False, 'kind': 'prefix', 'value': 'logstash-'}\n2016-07-21 17:45:30,941 DEBUG              curator.utils        iterate_filters:730  Pre-instance: [u'logstash-2016.27', u'logstash-2016.26', u'logstash-2016.25', u'logstash-2016.24', u'logstash-2016.23', u'logstash-2016.28']\n2016-07-21 17:45:30,941 DEBUG          curator.indexlist        filter_by_regex:281  Filtering indices by regex\n2016-07-21 17:45:30,941 DEBUG          curator.indexlist       empty_list_check:179  Checking for empty list\n2016-07-21 17:45:30,941 DEBUG          curator.indexlist           working_list:190  Generating working list of indices\n2016-07-21 17:45:30,941 DEBUG          curator.indexlist        filter_by_regex:302  Filter by regex: Index: logstash-2016.27\n2016-07-21 17:45:30,941 INFO           curator.indexlist           __actionable:35   Index logstash-2016.27 is actionable and remains in the list.\n2016-07-21 17:45:30,941 DEBUG          curator.indexlist        filter_by_regex:302  Filter by regex: Index: logstash-2016.26\n2016-07-21 17:45:30,941 INFO           curator.indexlist           __actionable:35   Index logstash-2016.26 is actionable and remains in the list.\n2016-07-21 17:45:30,941 DEBUG          curator.indexlist        filter_by_regex:302  Filter by regex: Index: logstash-2016.25\n2016-07-21 17:45:30,941 INFO           curator.indexlist           __actionable:35   Index logstash-2016.25 is actionable and remains in the list.\n2016-07-21 17:45:30,941 DEBUG          curator.indexlist        filter_by_regex:302  Filter by regex: Index: logstash-2016.24\n2016-07-21 17:45:30,941 INFO           curator.indexlist           __actionable:35   Index logstash-2016.24 is actionable and remains in the list.\n2016-07-21 17:45:30,941 DEBUG          curator.indexlist        filter_by_regex:302  Filter by regex: Index: logstash-2016.23\n2016-07-21 17:45:30,941 INFO           curator.indexlist           __actionable:35   Index logstash-2016.23 is actionable and remains in the list.\n2016-07-21 17:45:30,941 DEBUG          curator.indexlist        filter_by_regex:302  Filter by regex: Index: logstash-2016.28\n2016-07-21 17:45:30,941 INFO           curator.indexlist           __actionable:35   Index logstash-2016.28 is actionable and remains in the list.\n2016-07-21 17:45:30,941 DEBUG              curator.utils        iterate_filters:732  Post-instance: [u'logstash-2016.27', u'logstash-2016.26', u'logstash-2016.25', u'logstash-2016.24', u'logstash-2016.23', u'logstash-2016.28']\n2016-07-21 17:45:30,941 DEBUG          curator.indexlist        iterate_filters:702  Top of the loop: [u'logstash-2016.27', u'logstash-2016.26', u'logstash-2016.25', u'logstash-2016.24', u'logstash-2016.23', u'logstash-2016.28']\n2016-07-21 17:45:30,941 DEBUG              curator.utils        iterate_filters:703  Un-parsed filter args: {'source': 'creation_date', 'direction': 'older', 'unit_count': 5, 'exclude': None, 'filtertype': 'age', 'unit': 'weeks'}\n2016-07-21 17:45:30,941 DEBUG              curator.utils        iterate_filters:729  Filter args: {'direction': 'older', 'stats_result': 'min_value', 'field': None, 'source': 'creation_date', 'epoch': None, 'timestring': None, 'exclude': True, 'unit_count': 5, 'unit': 'weeks'}\n2016-07-21 17:45:30,941 DEBUG              curator.utils        iterate_filters:730  Pre-instance: [u'logstash-2016.27', u'logstash-2016.26', u'logstash-2016.25', u'logstash-2016.24', u'logstash-2016.23', u'logstash-2016.28']\n2016-07-21 17:45:30,941 DEBUG          curator.indexlist          filter_by_age:339  Filtering indices by age\n2016-07-21 17:45:30,941 DEBUG          curator.indexlist           working_list:190  Generating working list of indices\n2016-07-21 17:45:30,941 INFO           curator.indexlist           __actionable:35   Index logstash-2016.27 is actionable and remains in the list.\n2016-07-21 17:45:30,941 INFO           curator.indexlist            __excludify:58   Remains in actionable list: Index \"logstash-2016.27\" age (1467712937), direction: \"older\", point of reference, (1466091930)\n2016-07-21 17:45:30,941 INFO           curator.indexlist           __actionable:35   Index logstash-2016.26 is actionable and remains in the list.\n2016-07-21 17:45:30,941 INFO           curator.indexlist            __excludify:58   Remains in actionable list: Index \"logstash-2016.26\" age (1467108137), direction: \"older\", point of reference, (1466091930)\n2016-07-21 17:45:30,941 INFO           curator.indexlist           __actionable:35   Index logstash-2016.25 is actionable and remains in the list.\n2016-07-21 17:45:30,941 INFO           curator.indexlist            __excludify:58   Remains in actionable list: Index \"logstash-2016.25\" age (1466503337), direction: \"older\", point of reference, (1466091930)\n2016-07-21 17:45:30,941 INFO           curator.indexlist       __not_actionable:39   Index logstash-2016.24 is not actionable, removing from list.\n2016-07-21 17:45:30,941 INFO           curator.indexlist            __excludify:58   Removed from actionable list: Index \"logstash-2016.24\" age (1465898537), direction: \"older\", point of reference, (1466091930)\n2016-07-21 17:45:30,941 INFO           curator.indexlist       __not_actionable:39   Index logstash-2016.23 is not actionable, removing from list.\n2016-07-21 17:45:30,941 INFO           curator.indexlist            __excludify:58   Removed from actionable list: Index \"logstash-2016.23\" age (1465293737), direction: \"older\", point of reference, (1466091930)\n2016-07-21 17:45:30,941 INFO           curator.indexlist           __actionable:35   Index logstash-2016.28 is actionable and remains in the list.\n2016-07-21 17:45:30,941 INFO           curator.indexlist            __excludify:58   Remains in actionable list: Index \"logstash-2016.28\" age (1468317737), direction: \"older\", point of reference, (1466091930)\n2016-07-21 17:45:30,941 DEBUG              curator.utils        iterate_filters:732  Post-instance: [u'logstash-2016.27', u'logstash-2016.26', u'logstash-2016.25', u'logstash-2016.28']\n2016-07-21 17:45:30,942 DEBUG     curator.actions.delete_indices               __init__:328  master_timeout value: 30s\n2016-07-21 17:45:30,942 INFO               curator.utils           show_dry_run:593  DRY-RUN MODE.  No changes will be made.\n2016-07-21 17:45:30,942 INFO               curator.utils           show_dry_run:596  (CLOSED) indices may be shown that may not be acted on by action \"delete_indices\".\n2016-07-21 17:45:30,942 INFO               curator.utils           show_dry_run:603  DRY-RUN: delete_indices: logstash-2016.25 with arguments: {}\n2016-07-21 17:45:30,942 INFO               curator.utils           show_dry_run:603  DRY-RUN: delete_indices: logstash-2016.26 with arguments: {}\n2016-07-21 17:45:30,942 INFO               curator.utils           show_dry_run:603  DRY-RUN: delete_indices: logstash-2016.27 with arguments: {}\n2016-07-21 17:45:30,942 INFO               curator.utils           show_dry_run:603  DRY-RUN: delete_indices: logstash-2016.28 with arguments: {}\nYou were right, the filter is wrong:\nFilter args: {'direction': 'older', 'stats_result': 'min_value', 'field': None, 'source': 'creation_date', 'epoch': None, 'timestring': None, 'exclude': True, 'unit_count': 5, 'unit': 'weeks'}\nIn my configuration exclude is undefined but at runtime, exclude is True\n. Thanks for the fix! Do you know when the new version (4.0.2 ?) will be available? \n. The file curator/defaults/settings.py has been updated in Curator 4.2.6 to be \"compatible\" with Elaticsearch 5.x\n. ",
    "flix444": "Thanks, now we use pip. \n. ",
    "myoung34": "docker, yea\n. The flags I was hoping to provide were SSL, ES username and password, and host. \n. ",
    "dkirrane": "+1 - I'd like to use environment variables in the curator.yml and action-file.yml\nLike  curator.yml:\nclient:\n  hosts:\n    - ${HOSTNAME}\n. ",
    "mattdeboard": "Alright. Well if nothing else it was a good punch in the mouth to remind us to pin our dependency versions. \n. And thanks for your work getting out a fix quickly.\n. I would've reported it more quickly but our on-call guys are on airplanes, and it took awhile for me to get notified :)\n. ",
    "sshantveer": "Sorry for not being clear in the description. I was doing some changes to verify if I had some issues with filter. Indents were proper. \nBelow is the log generated when i run it. (Also tried the action you provided)\n\n2016-07-26 00:50:45,931 INFO      Action #1: snapshot\n2016-07-26 00:50:45,932 INFO      Starting new HTTP connection (1): 127.0.0.1\n2016-07-26 00:50:45,947 INFO      GET http://127.0.0.1:9200/ [status:200 request:0.015s]\n2016-07-26 00:50:45,949 INFO      GET http://127.0.0.1:9200/_all/_settings?expand_wildcards=open%2Cclosed [status:200 request:0.002s]\n2016-07-26 00:50:45,954 INFO      GET http://127.0.0.1:9200/_cluster/state/metadata/.marvel-es-1-2016.06.27,.marvel-es-1-2016.06.28,.marvel-es-1-2016.06.29,.marvel-es-1-2016.06.30,.marvel-es-data-1,som-claim-1 [status:200 request:0.005s]\n2016-07-26 00:50:45,957 INFO      GET http://127.0.0.1:9200/.marvel-es-1-2016.06.27,.marvel-es-1-2016.06.28,.marvel-es-1-2016.06.29,.marvel-es-1-2016.06.30,.marvel-es-data-1,som-claim-1/_stats/store,docs [status:200 request:0.002s]\n2016-07-26 00:50:45,958 ERROR     Failed to complete action: snapshot.  <class 'TypeError' at 0x000000001DE2C400>: Not an IndexList object. Type: <class 'curator.indexlist.IndexList' at 0x0000000002D86808>.\n. @untergeek Thanks for reply.\n- Yes, I have set blacklist to empty array. (Was trying) I am using v4.0.3\n- I tried to install using MSI installer. Uninstalled it and installed it using PIP.\n\nPlease find the debug logs for the action file.\nactions:\n  1:\n    action: snapshot\n    description: >-\n      Snapshot all indices older than 1 day (based on index\n      creation_date) with the default snapshot name pattern of\n      'curator-%Y%m%d%H%M%S'.  Wait for the snapshot to complete.  Do not skip\n      the repository filesystem access check.  Use the other options to create\n      the snapshot.\n    options:\n      repository: my_backup    \n      name: \n      ignore_unavailable: False\n      include_global_state: True\n      partial: False\n      wait_for_completion: True\n      skip_repo_fs_check: False\n      timeout_override:\n      continue_if_exception: False\n      disable_action: False\n    filters:\n    - filtertype:\n      state:\n      exclude:\nLogs\n```\n2016-07-26 18:03:33,977 DEBUG                curator.cli                    cli:165  default_timeout = 30\n2016-07-26 18:03:33,981 DEBUG                curator.cli                    cli:170  Full list of actions: {1: {'options': {'partial': False, 'ignore_unavailable': False, 'disable_action': False, 'name': None, 'repository': 'my_backup', 'continue_if_exception': False, 'skip_repo_fs_check': False, 'timeout_override': None, 'wait_for_completion': True, 'include_global_state': True}, 'action': 'snapshot', 'description': \"Snapshot all indices older than 1 day (based on index creation_date) with the default snapshot name pattern of 'curator-%Y%m%d%H%M%S'.  Wait for the snapshot to complete.  Do not skip the repository filesystem access check.  Use the other options to create the snapshot.\", 'filters': [{'state': None, 'filtertype': None, 'exclude': None}]}}\n2016-07-26 18:03:33,981 INFO                 curator.cli                    cli:177  Action #1: snapshot\n2016-07-26 18:03:33,981 DEBUG                curator.cli                    cli:190  continue_if_exception = False\n2016-07-26 18:03:33,981 DEBUG                curator.cli                    cli:191  timeout_override = None\n2016-07-26 18:03:33,981 DEBUG              curator.utils             get_client:503  kwargs = {'use_ssl': False, 'aws_region': None, 'http_auth': None, 'timeout': 30, 'master_only': False, 'certificate': None, 'aws_secret_key': None, 'aws_key': None, 'hosts': ['127.0.0.1'], 'client_cert': None, 'port': 9200, 'url_prefix': '', 'client_key': None, 'ssl_no_validate': False}\n2016-07-26 18:03:33,981 DEBUG              curator.utils             get_client:548  Not using \"requests_aws4auth\" python module to connect.\n2016-07-26 18:03:33,998 DEBUG              curator.utils          check_version:412  Detected Elasticsearch version 2.3.3\n2016-07-26 18:03:33,998 DEBUG                curator.cli                    cli:216  client is \n2016-07-26 18:03:33,998 DEBUG                curator.cli                    cli:222  TRY: actions: {'options': {'partial': False, 'ignore_unavailable': False, 'name': None, 'repository': 'my_backup', 'skip_repo_fs_check': False, 'wait_for_completion': True, 'include_global_state': True}, 'action': 'snapshot', 'description': \"Snapshot all indices older than 1 day (based on index creation_date) with the default snapshot name pattern of 'curator-%Y%m%d%H%M%S'.  Wait for the snapshot to complete.  Do not skip the repository filesystem access check.  Use the other options to create the snapshot.\", 'filters': [{'state': None, 'filtertype': None, 'exclude': None}]} kwargs: {'master_timeout': 30, 'timeout': 30, 'dry_run': False}\n2016-07-26 18:03:33,998 DEBUG                curator.cli         process_action:46   Configuration dictionary: {'options': {'partial': False, 'ignore_unavailable': False, 'name': None, 'repository': 'my_backup', 'skip_repo_fs_check': False, 'wait_for_completion': True, 'include_global_state': True}, 'action': 'snapshot', 'description': \"Snapshot all indices older than 1 day (based on index creation_date) with the default snapshot name pattern of 'curator-%Y%m%d%H%M%S'.  Wait for the snapshot to complete.  Do not skip the repository filesystem access check.  Use the other options to create the snapshot.\", 'filters': [{'state': None, 'filtertype': None, 'exclude': None}]}\n2016-07-26 18:03:33,998 DEBUG                curator.cli         process_action:47   kwargs: {'master_timeout': 30, 'dry_run': False, 'timeout': 30}\n2016-07-26 18:03:33,998 DEBUG                curator.cli         process_action:50   opts: {'partial': False, 'ignore_unavailable': False, 'name': None, 'repository': 'my_backup', 'skip_repo_fs_check': False, 'wait_for_completion': True, 'include_global_state': True}\n2016-07-26 18:03:33,998 DEBUG                curator.cli         process_action:68   MYKWARGS = {'skip_repo_fs_check': False, 'partial': False, 'ignore_unavailable': False, 'wait_for_completion': True, 'name': 'curator-%Y%m%d%H%M%S', 'include_global_state': True, 'repository': None}\n2016-07-26 18:03:33,998 DEBUG                curator.cli         process_action:72   Action kwargs: {'skip_repo_fs_check': False, 'partial': False, 'ignore_unavailable': False, 'wait_for_completion': True, 'name': 'curator-%Y%m%d%H%M%S', 'include_global_state': True, 'repository': 'my_backup'}\n2016-07-26 18:03:33,998 DEBUG              curator.utils            verify_args:1018 Arguments for action \"snapshot\": {'skip_repo_fs_check': False, 'partial': False, 'ignore_unavailable': False, 'wait_for_completion': True, 'name': 'curator-%Y%m%d%H%M%S', 'include_global_state': True, 'repository': 'my_backup'}\n2016-07-26 18:03:33,998 DEBUG              curator.utils           matches_keys:1022 options.keys = dict_keys(['skip_repo_fs_check', 'partial', 'ignore_unavailable', 'wait_for_completion', 'name', 'include_global_state', 'repository']) mydict.keys = dict_keys(['skip_repo_fs_check', 'partial', 'ignore_unavailable', 'wait_for_completion', 'name', 'include_global_state', 'repository'])\n2016-07-26 18:03:33,998 DEBUG                curator.cli         process_action:102  Running \"SNAPSHOT\"\n2016-07-26 18:03:33,998 DEBUG          curator.indexlist          __get_indices:64   Getting all indices\n2016-07-26 18:03:34,002 DEBUG              curator.utils            get_indices:369  All indices: ['.marvel-es-1-2016.06.28', 'som-claim-1', '.marvel-es-1-2016.06.30', '.marvel-es-1-2016.06.29', '.marvel-es-1-2016.06.27', '.marvel-es-data-1']\n2016-07-26 18:03:34,002 DEBUG          curator.indexlist       empty_list_check:178  Checking for empty list\n2016-07-26 18:03:34,002 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 18:03:34,002 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 18:03:34,002 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 18:03:34,002 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 18:03:34,002 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 18:03:34,002 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 18:03:34,002 DEBUG          curator.indexlist          _get_metadata:141  Getting index metadata\n2016-07-26 18:03:34,002 DEBUG          curator.indexlist       empty_list_check:178  Checking for empty list\n2016-07-26 18:03:34,022 DEBUG          curator.indexlist       _get_index_stats:109  Getting index stats\n2016-07-26 18:03:34,022 DEBUG          curator.indexlist       empty_list_check:178  Checking for empty list\n2016-07-26 18:03:34,022 DEBUG          curator.indexlist           working_list:189  Generating working list of indices\n2016-07-26 18:03:34,022 DEBUG          curator.indexlist           working_list:189  Generating working list of indices\n2016-07-26 18:03:34,098 DEBUG          curator.indexlist     iterate_over_stats:118  Index: .marvel-es-1-2016.06.28  Size: 8.7MB  Docs: 28333\n2016-07-26 18:03:34,098 DEBUG          curator.indexlist     iterate_over_stats:118  Index: som-claim-1  Size: 31.7MB  Docs: 35882\n2016-07-26 18:03:34,098 DEBUG          curator.indexlist     iterate_over_stats:118  Index: .marvel-es-1-2016.06.30  Size: 26.1MB  Docs: 77987\n2016-07-26 18:03:34,098 DEBUG          curator.indexlist     iterate_over_stats:118  Index: .marvel-es-1-2016.06.29  Size: 27.9MB  Docs: 92236\n2016-07-26 18:03:34,098 DEBUG          curator.indexlist     iterate_over_stats:118  Index: .marvel-es-1-2016.06.27  Size: 7.0MB  Docs: 20786\n2016-07-26 18:03:34,098 DEBUG          curator.indexlist     iterate_over_stats:118  Index: .marvel-es-data-1  Size: 33.6KB  Docs: 10\n2016-07-26 18:03:34,098 DEBUG          curator.indexlist        iterate_filters:678  Iterating over a list of filters\n2016-07-26 18:03:34,098 DEBUG          curator.indexlist        iterate_filters:684  All filters: [{'state': None, 'filtertype': None, 'exclude': None}]\n2016-07-26 18:03:34,098 DEBUG          curator.indexlist        iterate_filters:686  Top of the loop: ['.marvel-es-1-2016.06.28', 'som-claim-1', '.marvel-es-1-2016.06.30', '.marvel-es-1-2016.06.29', '.marvel-es-1-2016.06.27', '.marvel-es-data-1']\n2016-07-26 18:03:34,099 DEBUG              curator.utils        iterate_filters:687  Un-parsed filter args: {'state': None, 'filtertype': None, 'exclude': None}\n2016-07-26 18:03:34,099 ERROR                curator.cli                    cli:242  Failed to complete action: snapshot.  : Invalid value for \"filtertype\": None\n```\n. I updated the comment with action file and debug logs. Not sure if I am doing anything wrong.\nI will also uninstall Curator and do a clean install again using PIP and check. \nThanks for looking into this.\n. - Elastic version 2.3.3 . This is running on windows.\n- Curator v4.0.3 \n- Python 3.6\n$ pip --version\npip 8.1.2 from \\appdata\\local\\programs\\python\\python36\\lib\\site-packages (python 3.6)\nBelow are the logs as requested. Made changes to my filter to make sure I include the index I want to take snapshot on :\n```\n2016-07-26 18:36:13,797 DEBUG                curator.cli                    cli:165  default_timeout = 30\n2016-07-26 18:36:13,800 DEBUG                curator.cli                    cli:170  Full list of actions: {1: {'filters': [{'value': 'som-', 'exclude': None, 'filtertype': 'pattern', 'kind': 'prefix'}], 'description': \"Snapshot indices with prefix som- with the default snapshot name pattern of 'curator-%Y%m%d%H%M%S'.  Wait for the snapshot to complete.  Do not skip the repository filesystem access check.  Use the other options to create the snapshot.\", 'options': {'repository': 'my_backup', 'continue_if_exception': False, 'skip_repo_fs_check': False, 'ignore_unavailable': False, 'partial': False, 'include_global_state': True, 'name': None, 'timeout_override': None, 'disable_action': False, 'wait_for_completion': True}, 'action': 'snapshot'}}\n2016-07-26 18:36:13,800 INFO                 curator.cli                    cli:177  Action #1: snapshot\n2016-07-26 18:36:13,800 DEBUG                curator.cli                    cli:190  continue_if_exception = False\n2016-07-26 18:36:13,800 DEBUG                curator.cli                    cli:191  timeout_override = None\n2016-07-26 18:36:13,800 DEBUG              curator.utils             get_client:503  kwargs = {'timeout': 30, 'http_auth': None, 'aws_secret_key': None, 'master_only': False, 'client_key': None, 'aws_key': None, 'ssl_no_validate': False, 'aws_region': None, 'url_prefix': '', 'client_cert': None, 'hosts': ['127.0.0.1'], 'use_ssl': False, 'port': 9200, 'certificate': None}\n2016-07-26 18:36:13,801 DEBUG              curator.utils             get_client:548  Not using \"requests_aws4auth\" python module to connect.\n2016-07-26 18:36:13,801 DEBUG         urllib3.util.retry               from_int:170  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-07-26 18:36:13,801 INFO      urllib3.connectionpool              _new_conn:214  Starting new HTTP connection (1): 127.0.0.1\n2016-07-26 18:36:13,807 DEBUG     urllib3.connectionpool          _make_request:401  \"GET / HTTP/1.1\" 200 313\n2016-07-26 18:36:13,807 INFO               elasticsearch    log_request_success:63   GET http://127.0.0.1:9200/ [status:200 request:0.006s]\n2016-07-26 18:36:13,808 DEBUG              elasticsearch    log_request_success:65   > None\n2016-07-26 18:36:13,808 DEBUG              elasticsearch    log_request_success:66   < {\n  \"name\" : \"Plug\",\n  \"cluster_name\" : \"elasticsearch\",\n  \"version\" : {\n    \"number\" : \"2.3.3\",\n    \"build_hash\" : \"218bdf10790eef486ff2c41a3df5cfa32dadcfde\",\n    \"build_timestamp\" : \"2016-05-17T15:40:04Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"5.5.0\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n2016-07-26 18:36:13,808 DEBUG              curator.utils          check_version:412  Detected Elasticsearch version 2.3.3\n2016-07-26 18:36:13,808 DEBUG                curator.cli                    cli:216  client is \n2016-07-26 18:36:13,808 DEBUG                curator.cli                    cli:222  TRY: actions: {'filters': [{'value': 'som-', 'exclude': None, 'filtertype': 'pattern', 'kind': 'prefix'}], 'description': \"Snapshot indices with prefix som- with the default snapshot name pattern of 'curator-%Y%m%d%H%M%S'.  Wait for the snapshot to complete.  Do not skip the repository filesystem access check.  Use the other options to create the snapshot.\", 'options': {'repository': 'my_backup', 'skip_repo_fs_check': False, 'ignore_unavailable': False, 'partial': False, 'include_global_state': True, 'name': None, 'wait_for_completion': True}, 'action': 'snapshot'} kwargs: {'dry_run': False, 'timeout': 30, 'master_timeout': 30}\n2016-07-26 18:36:13,808 DEBUG                curator.cli         process_action:46   Configuration dictionary: {'filters': [{'value': 'som-', 'exclude': None, 'filtertype': 'pattern', 'kind': 'prefix'}], 'description': \"Snapshot indices with prefix som- with the default snapshot name pattern of 'curator-%Y%m%d%H%M%S'.  Wait for the snapshot to complete.  Do not skip the repository filesystem access check.  Use the other options to create the snapshot.\", 'options': {'repository': 'my_backup', 'skip_repo_fs_check': False, 'ignore_unavailable': False, 'partial': False, 'include_global_state': True, 'name': None, 'wait_for_completion': True}, 'action': 'snapshot'}\n2016-07-26 18:36:13,808 DEBUG                curator.cli         process_action:47   kwargs: {'dry_run': False, 'timeout': 30, 'master_timeout': 30}\n2016-07-26 18:36:13,808 DEBUG                curator.cli         process_action:50   opts: {'repository': 'my_backup', 'skip_repo_fs_check': False, 'ignore_unavailable': False, 'partial': False, 'include_global_state': True, 'name': None, 'wait_for_completion': True}\n2016-07-26 18:36:13,808 DEBUG                curator.cli         process_action:68   MYKWARGS = {'repository': None, 'partial': False, 'skip_repo_fs_check': False, 'include_global_state': True, 'ignore_unavailable': False, 'wait_for_completion': True, 'name': 'curator-%Y%m%d%H%M%S'}\n2016-07-26 18:36:13,808 DEBUG                curator.cli         process_action:72   Action kwargs: {'repository': 'my_backup', 'partial': False, 'skip_repo_fs_check': False, 'include_global_state': True, 'ignore_unavailable': False, 'wait_for_completion': True, 'name': 'curator-%Y%m%d%H%M%S'}\n2016-07-26 18:36:13,808 DEBUG              curator.utils            verify_args:1018 Arguments for action \"snapshot\": {'repository': 'my_backup', 'partial': False, 'skip_repo_fs_check': False, 'include_global_state': True, 'ignore_unavailable': False, 'wait_for_completion': True, 'name': 'curator-%Y%m%d%H%M%S'}\n2016-07-26 18:36:13,809 DEBUG              curator.utils           matches_keys:1022 options.keys = dict_keys(['repository', 'partial', 'skip_repo_fs_check', 'include_global_state', 'ignore_unavailable', 'wait_for_completion', 'name']) mydict.keys = dict_keys(['repository', 'partial', 'skip_repo_fs_check', 'include_global_state', 'ignore_unavailable', 'wait_for_completion', 'name'])\n2016-07-26 18:36:13,809 DEBUG                curator.cli         process_action:102  Running \"SNAPSHOT\"\n2016-07-26 18:36:13,809 DEBUG          curator.indexlist          __get_indices:64   Getting all indices\n2016-07-26 18:36:13,809 DEBUG         urllib3.util.retry               from_int:170  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-07-26 18:36:13,810 DEBUG     urllib3.connectionpool          _make_request:401  \"GET /_all/_settings?expand_wildcards=open%2Cclosed HTTP/1.1\" 200 1768\n2016-07-26 18:36:13,811 INFO               elasticsearch    log_request_success:63   GET http://127.0.0.1:9200/_all/_settings?expand_wildcards=open%2Cclosed [status:200 request:0.002s]\n2016-07-26 18:36:13,811 DEBUG              elasticsearch    log_request_success:65   > None\n2016-07-26 18:36:13,811 DEBUG              elasticsearch    log_request_success:66   < {\".marvel-es-1-2016.06.29\":{\"settings\":{\"index\":{\"codec\":\"best_compression\",\"marvel\":{\"plugin\":{\"version\":\"2.3.2\"},\"template\":{\"version\":\"1\"}},\"number_of_shards\":\"1\",\"mapper\":{\"dynamic\":\"false\"},\"creation_date\":\"1467158407084\",\"number_of_replicas\":\"1\",\"uuid\":\"j--sbxxGQ5mtW8UVjA_JNQ\",\"version\":{\"created\":\"2030299\"}}}},\".marvel-es-1-2016.06.28\":{\"settings\":{\"index\":{\"codec\":\"best_compression\",\"marvel\":{\"plugin\":{\"version\":\"2.3.2\"},\"template\":{\"version\":\"1\"}},\"number_of_shards\":\"1\",\"mapper\":{\"dynamic\":\"false\"},\"creation_date\":\"1467072007095\",\"number_of_replicas\":\"1\",\"uuid\":\"01lOoc3sQPOugQCb4Ravmw\",\"version\":{\"created\":\"2030299\"}}}},\".marvel-es-1-2016.06.27\":{\"settings\":{\"index\":{\"codec\":\"best_compression\",\"marvel\":{\"plugin\":{\"version\":\"2.3.2\"},\"template\":{\"version\":\"1\"}},\"number_of_shards\":\"1\",\"mapper\":{\"dynamic\":\"false\"},\"creation_date\":\"1467049266583\",\"number_of_replicas\":\"1\",\"uuid\":\"1ikZ0VwkQA67PErklxIZrg\",\"version\":{\"created\":\"2030299\"}}}},\".marvel-es-1-2016.06.30\":{\"settings\":{\"index\":{\"codec\":\"best_compression\",\"marvel\":{\"plugin\":{\"version\":\"2.3.2\"},\"template\":{\"version\":\"1\"}},\"number_of_shards\":\"1\",\"mapper\":{\"dynamic\":\"false\"},\"creation_date\":\"1467244806166\",\"number_of_replicas\":\"1\",\"uuid\":\"Yf4gG2iLSQ6G3xpJkRIthQ\",\"version\":{\"created\":\"2030299\"}}}},\".marvel-es-data-1\":{\"settings\":{\"index\":{\"codec\":\"best_compression\",\"marvel\":{\"plugin\":{\"version\":\"2.3.2\"},\"template\":{\"version\":\"1\"}},\"number_of_shards\":\"1\",\"mapper\":{\"dynamic\":\"false\"},\"creation_date\":\"1467049266368\",\"number_of_replicas\":\"1\",\"uuid\":\"WSxnea9gS9-psO2bLepn2A\",\"version\":{\"created\":\"2030299\"}}}},\"som-claim-1\":{\"settings\":{\"index\":{\"creation_date\":\"1467333351569\",\"number_of_shards\":\"5\",\"number_of_replicas\":\"1\",\"uuid\":\"7Fc0Smp0QzyoUG19LydoBA\",\"version\":{\"created\":\"2030399\"}}}}}\n2016-07-26 18:36:13,811 DEBUG              curator.utils            get_indices:369  All indices: ['som-claim-1', '.marvel-es-1-2016.06.27', '.marvel-es-1-2016.06.30', '.marvel-es-1-2016.06.29', '.marvel-es-1-2016.06.28', '.marvel-es-data-1']\n2016-07-26 18:36:13,811 DEBUG          curator.indexlist       empty_list_check:178  Checking for empty list\n2016-07-26 18:36:13,812 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 18:36:13,812 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 18:36:13,812 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 18:36:13,812 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 18:36:13,812 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 18:36:13,812 DEBUG          curator.indexlist     __build_index_info:78   Building index info dictionary\n2016-07-26 18:36:13,812 DEBUG          curator.indexlist          _get_metadata:141  Getting index metadata\n2016-07-26 18:36:13,812 DEBUG          curator.indexlist       empty_list_check:178  Checking for empty list\n2016-07-26 18:36:13,812 DEBUG         urllib3.util.retry               from_int:170  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-07-26 18:36:13,815 DEBUG     urllib3.connectionpool          _make_request:401  \"GET /_cluster/state/metadata/.marvel-es-1-2016.06.27,.marvel-es-1-2016.06.28,.marvel-es-1-2016.06.29,.marvel-es-1-2016.06.30,.marvel-es-data-1,som-claim-1 HTTP/1.1\" 200 48668\n2016-07-26 18:36:13,816 INFO               elasticsearch    log_request_success:63   GET http://127.0.0.1:9200/_cluster/state/metadata/.marvel-es-1-2016.06.27,.marvel-es-1-2016.06.28,.marvel-es-1-2016.06.29,.marvel-es-1-2016.06.30,.marvel-es-data-1,som-claim-1 [status:200 request:0.004s]\n2016-07-26 18:36:13,816 DEBUG              elasticsearch    log_request_success:65   > None\n2016-07-26 18:36:13,832 DEBUG          curator.indexlist       _get_index_stats:109  Getting index stats\n2016-07-26 18:36:13,832 DEBUG          curator.indexlist       empty_list_check:178  Checking for empty list\n2016-07-26 18:36:13,832 DEBUG          curator.indexlist           working_list:189  Generating working list of indices\n2016-07-26 18:36:13,832 DEBUG          curator.indexlist           working_list:189  Generating working list of indices\n2016-07-26 18:36:13,832 DEBUG         urllib3.util.retry               from_int:170  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n2016-07-26 18:36:13,907 DEBUG     urllib3.connectionpool          _make_request:401  \"GET /.marvel-es-1-2016.06.27,.marvel-es-1-2016.06.28,.marvel-es-1-2016.06.29,.marvel-es-1-2016.06.30,.marvel-es-data-1,som-claim-1/_stats/store,docs HTTP/1.1\" 200 1766\n2016-07-26 18:36:13,908 INFO               elasticsearch    log_request_success:63   GET http://127.0.0.1:9200/.marvel-es-1-2016.06.27,.marvel-es-1-2016.06.28,.marvel-es-1-2016.06.29,.marvel-es-1-2016.06.30,.marvel-es-data-1,som-claim-1/_stats/store,docs [status:200 request:0.076s]\n2016-07-26 18:36:13,908 DEBUG              elasticsearch    log_request_success:65   > None\n2016-07-26 18:36:13,908 DEBUG              elasticsearch    log_request_success:66   < {\"_shards\":{\"total\":20,\"successful\":10,\"failed\":0},\"_all\":{\"primaries\":{\"docs\":{\"count\":255234,\"deleted\":397},\"store\":{\"size_in_bytes\":106390989,\"throttle_time_in_millis\":0}},\"total\":{\"docs\":{\"count\":255234,\"deleted\":397},\"store\":{\"size_in_bytes\":106390989,\"throttle_time_in_millis\":0}}},\"indices\":{\".marvel-es-1-2016.06.30\":{\"primaries\":{\"docs\":{\"count\":77987,\"deleted\":150},\"store\":{\"size_in_bytes\":27378552,\"throttle_time_in_millis\":0}},\"total\":{\"docs\":{\"count\":77987,\"deleted\":150},\"store\":{\"size_in_bytes\":27378552,\"throttle_time_in_millis\":0}}},\".marvel-es-data-1\":{\"primaries\":{\"docs\":{\"count\":10,\"deleted\":8},\"store\":{\"size_in_bytes\":34456,\"throttle_time_in_millis\":0}},\"total\":{\"docs\":{\"count\":10,\"deleted\":8},\"store\":{\"size_in_bytes\":34456,\"throttle_time_in_millis\":0}}},\".marvel-es-1-2016.06.29\":{\"primaries\":{\"docs\":{\"count\":92236,\"deleted\":109},\"store\":{\"size_in_bytes\":29257327,\"throttle_time_in_millis\":0}},\"total\":{\"docs\":{\"count\":92236,\"deleted\":109},\"store\":{\"size_in_bytes\":29257327,\"throttle_time_in_millis\":0}}},\".marvel-es-1-2016.06.28\":{\"primaries\":{\"docs\":{\"count\":28333,\"deleted\":32},\"store\":{\"size_in_bytes\":9151537,\"throttle_time_in_millis\":0}},\"total\":{\"docs\":{\"count\":28333,\"deleted\":32},\"store\":{\"size_in_bytes\":9151537,\"throttle_time_in_millis\":0}}},\".marvel-es-1-2016.06.27\":{\"primaries\":{\"docs\":{\"count\":20786,\"deleted\":98},\"store\":{\"size_in_bytes\":7321700,\"throttle_time_in_millis\":0}},\"total\":{\"docs\":{\"count\":20786,\"deleted\":98},\"store\":{\"size_in_bytes\":7321700,\"throttle_time_in_millis\":0}}},\"som-claim-1\":{\"primaries\":{\"docs\":{\"count\":35882,\"deleted\":0},\"store\":{\"size_in_bytes\":33247417,\"throttle_time_in_millis\":0}},\"total\":{\"docs\":{\"count\":35882,\"deleted\":0},\"store\":{\"size_in_bytes\":33247417,\"throttle_time_in_millis\":0}}}}}\n2016-07-26 18:36:13,908 DEBUG          curator.indexlist     iterate_over_stats:118  Index: som-claim-1  Size: 31.7MB  Docs: 35882\n2016-07-26 18:36:13,908 DEBUG          curator.indexlist     iterate_over_stats:118  Index: .marvel-es-1-2016.06.27  Size: 7.0MB  Docs: 20786\n2016-07-26 18:36:13,908 DEBUG          curator.indexlist     iterate_over_stats:118  Index: .marvel-es-1-2016.06.30  Size: 26.1MB  Docs: 77987\n2016-07-26 18:36:13,909 DEBUG          curator.indexlist     iterate_over_stats:118  Index: .marvel-es-1-2016.06.29  Size: 27.9MB  Docs: 92236\n2016-07-26 18:36:13,909 DEBUG          curator.indexlist     iterate_over_stats:118  Index: .marvel-es-1-2016.06.28  Size: 8.7MB  Docs: 28333\n2016-07-26 18:36:13,909 DEBUG          curator.indexlist     iterate_over_stats:118  Index: .marvel-es-data-1  Size: 33.6KB  Docs: 10\n2016-07-26 18:36:13,909 DEBUG          curator.indexlist        iterate_filters:678  Iterating over a list of filters\n2016-07-26 18:36:13,909 DEBUG          curator.indexlist        iterate_filters:684  All filters: [{'value': 'som-', 'exclude': None, 'filtertype': 'pattern', 'kind': 'prefix'}]\n2016-07-26 18:36:13,909 DEBUG          curator.indexlist        iterate_filters:686  Top of the loop: ['som-claim-1', '.marvel-es-1-2016.06.27', '.marvel-es-1-2016.06.30', '.marvel-es-1-2016.06.29', '.marvel-es-1-2016.06.28', '.marvel-es-data-1']\n2016-07-26 18:36:13,909 DEBUG              curator.utils        iterate_filters:687  Un-parsed filter args: {'value': 'som-', 'exclude': None, 'filtertype': 'pattern', 'kind': 'prefix'}\n2016-07-26 18:36:13,909 DEBUG              curator.utils        iterate_filters:713  Filter args: {'value': 'som-', 'exclude': False, 'kind': 'prefix'}\n2016-07-26 18:36:13,909 DEBUG              curator.utils        iterate_filters:714  Pre-instance: ['som-claim-1', '.marvel-es-1-2016.06.27', '.marvel-es-1-2016.06.30', '.marvel-es-1-2016.06.29', '.marvel-es-1-2016.06.28', '.marvel-es-data-1']\n2016-07-26 18:36:13,909 DEBUG          curator.indexlist        filter_by_regex:280  Filtering indices by regex\n2016-07-26 18:36:13,909 DEBUG          curator.indexlist       empty_list_check:178  Checking for empty list\n2016-07-26 18:36:13,909 DEBUG          curator.indexlist           working_list:189  Generating working list of indices\n2016-07-26 18:36:13,909 DEBUG          curator.indexlist        filter_by_regex:301  Filter by regex: Index: som-claim-1\n2016-07-26 18:36:13,909 DEBUG          curator.indexlist           __actionable:34   Index som-claim-1 is actionable and remains in the list.\n2016-07-26 18:36:13,909 DEBUG          curator.indexlist        filter_by_regex:301  Filter by regex: Index: .marvel-es-1-2016.06.27\n2016-07-26 18:36:13,910 DEBUG          curator.indexlist       __not_actionable:38   Index .marvel-es-1-2016.06.27 is not actionable, removing from list.\n2016-07-26 18:36:13,910 DEBUG          curator.indexlist        filter_by_regex:301  Filter by regex: Index: .marvel-es-1-2016.06.30\n2016-07-26 18:36:13,910 DEBUG          curator.indexlist       __not_actionable:38   Index .marvel-es-1-2016.06.30 is not actionable, removing from list.\n2016-07-26 18:36:13,910 DEBUG          curator.indexlist        filter_by_regex:301  Filter by regex: Index: .marvel-es-1-2016.06.29\n2016-07-26 18:36:13,910 DEBUG          curator.indexlist       __not_actionable:38   Index .marvel-es-1-2016.06.29 is not actionable, removing from list.\n2016-07-26 18:36:13,910 DEBUG          curator.indexlist        filter_by_regex:301  Filter by regex: Index: .marvel-es-1-2016.06.28\n2016-07-26 18:36:13,910 DEBUG          curator.indexlist       __not_actionable:38   Index .marvel-es-1-2016.06.28 is not actionable, removing from list.\n2016-07-26 18:36:13,910 DEBUG          curator.indexlist        filter_by_regex:301  Filter by regex: Index: .marvel-es-data-1\n2016-07-26 18:36:13,910 DEBUG          curator.indexlist       __not_actionable:38   Index .marvel-es-data-1 is not actionable, removing from list.\n2016-07-26 18:36:13,910 DEBUG              curator.utils        iterate_filters:716  Post-instance: ['som-claim-1']\n2016-07-26 18:36:13,910 ERROR                curator.cli                    cli:242  Failed to complete action: snapshot.  : Not an IndexList object. Type: .\n```\n. Thanks @untergeek . It was Python version.\nI tried with python v3.5.2, it worked.\n. ",
    "chiefy": "The whole log is about 6mb large, so I am just cutting what I believe is relevant, if you need the whole thing, let me know but there's a bunch of stuff in it I don't want to post here:\n18:26:00 2016-07-29 18:26:00,849 DEBUG                curator.cli                    cli:165  default_timeout = 30\n18:26:00 2016-07-29 18:26:00,870 DEBUG                curator.cli                    cli:170  Full list of actions: {1: {'action': 'snapshot', 'description': 'snapshot app-* older than 7 days', 'filters': [{'exclude': None, 'kind': 'prefix', 'filtertype': 'pattern', 'value': 'app-'}, {'source': 'name', 'direction': 'older', 'unit_count': 7, 'timestring': '%Y.%m.%d', 'exclude': None, 'filtertype': 'age', 'unit': 'days'}], 'options': {'ignore_unavailable': False, 'partial': False, 'name': 'app-appdevprod-%Y%m%d%H', 'repository': 'backup', 'continue_if_exception': False, 'skip_repo_fs_check': False, 'timeout_override': None, 'disable_action': False, 'include_global_state': True, 'wait_for_completion': True}}, 2: {'action': 'delete_indices', 'description': 'Delete snapshotted APP indices', 'filters': [{'exclude': None, 'kind': 'prefix', 'filtertype': 'pattern', 'value': 'app-'}, {'source': 'name', 'direction': 'older', 'unit_count': 7, 'timestring': '%Y.%m.%d', 'exclude': None, 'filtertype': 'age', 'unit': 'days'}], 'options': {'continue_if_exception': False, 'timeout_override': None, 'disable_action': False}}, 3: {'action': 'snapshot', 'description': 'snapshot os-* older than 7 days', 'filters': [{'exclude': None, 'kind': 'prefix', 'filtertype': 'pattern', 'value': 'os-'}, {'source': 'name', 'direction': 'older', 'unit_count': 7, 'timestring': '%Y.%m.%d', 'exclude': None, 'filtertype': 'age', 'unit': 'days'}], 'options': {'ignore_unavailable': False, 'partial': False, 'name': 'os-appdevprod-%Y%m%d%H', 'repository': 'backup', 'continue_if_exception': False, 'skip_repo_fs_check': False, 'timeout_override': None, 'disable_action': False, 'include_global_state': True, 'wait_for_completion': True}}, 4: {'action': 'delete_indices', 'description': 'Delete snapshotted OS indices', 'filters': [{'exclude': None, 'kind': 'prefix', 'filtertype': 'pattern', 'value': 'os-'}, {'source': 'name', 'direction': 'older', 'unit_count': 7, 'timestring': '%Y.%m.%d', 'exclude': None, 'filtertype': 'age', 'unit': 'days'}], 'options': {'continue_if_exception': False, 'timeout_override': None, 'disable_action': False}}}\n18:26:00 2016-07-29 18:26:00,870 INFO                 curator.cli                    cli:177  Action #1: snapshot\n18:26:00 2016-07-29 18:26:00,870 DEBUG                curator.cli                    cli:190  continue_if_exception = False\n18:26:00 2016-07-29 18:26:00,870 DEBUG                curator.cli                    cli:191  timeout_override = None\n18:26:00 2016-07-29 18:26:00,870 DEBUG              curator.utils             get_client:503  kwargs = {'aws_secret_key': None, 'url_prefix': '/logging-store', 'http_auth': None, 'certificate': None, 'aws_key': None, 'aws_region': None, 'client_cert': None, 'hosts': ['xx.xxx.xx.xxx'], 'timeout': 30, 'use_ssl': False, 'master_only': False, 'port': 80, 'ssl_no_validate': False, 'client_key': None}\n...\nsnip\n...\n18:26:03 2016-07-29 18:26:03,200 DEBUG              curator.utils        iterate_filters:716  Post-instance: []\n18:26:03 2016-07-29 18:26:03,200 DEBUG         urllib3.util.retry               from_int:170  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\n18:26:03 2016-07-29 18:26:03,204 DEBUG     urllib3.connectionpool          _make_request:401  \"GET /logging-store/_snapshot/backup HTTP/1.1\" 200 87\n18:26:03 2016-07-29 18:26:03,205 INFO               elasticsearch    log_request_success:63   GET http://xx.xxx.xx.xxx:80/logging-store/_snapshot/backup [status:200 request:0.004s]\n18:26:03 2016-07-29 18:26:03,205 DEBUG              elasticsearch    log_request_success:65   > None\n18:26:03 2016-07-29 18:26:03,205 DEBUG              elasticsearch    log_request_success:66   < {\"backup\":{\"type\":\"s3\",\"settings\":{\"bucket\":\"my-backup-bucket\",\"compress\":\"true\"}}}\n18:26:03 2016-07-29 18:26:03,205 DEBUG              curator.utils      repository_exists:911  Repository backup exists.\n18:26:03 2016-07-29 18:26:03,205 DEBUG              curator.utils     parse_date_pattern:995  Partially rendered name: a\n18:26:03 2016-07-29 18:26:03,205 DEBUG              curator.utils     parse_date_pattern:995  Partially rendered name: ap\n18:26:03 2016-07-29 18:26:03,205 DEBUG              curator.utils     parse_date_pattern:995  Partially rendered name: app\n18:26:03 2016-07-29 18:26:03,205 DEBUG              curator.utils     parse_date_pattern:995  Partially rendered name: app-\n18:26:03 2016-07-29 18:26:03,205 DEBUG              curator.utils     parse_date_pattern:995  Partially rendered name: app-a\n18:26:03 2016-07-29 18:26:03,206 DEBUG              curator.utils     parse_date_pattern:995  Partially rendered name: app-ap\n18:26:03 2016-07-29 18:26:03,206 DEBUG              curator.utils     parse_date_pattern:995  Partially rendered name: app-app\n18:26:03 2016-07-29 18:26:03,206 DEBUG              curator.utils     parse_date_pattern:995  Partially rendered name: app-appd\n18:26:03 2016-07-29 18:26:03,206 DEBUG              curator.utils     parse_date_pattern:995  Partially rendered name: app-appde\n18:26:03 2016-07-29 18:26:03,206 DEBUG              curator.utils     parse_date_pattern:995  Partially rendered name: app-appdev\n18:26:03 2016-07-29 18:26:03,206 DEBUG              curator.utils     parse_date_pattern:995  Partially rendered name: app-appdevp\n18:26:03 2016-07-29 18:26:03,206 DEBUG              curator.utils     parse_date_pattern:995  Partially rendered name: app-appdevpr\n18:26:03 2016-07-29 18:26:03,206 DEBUG              curator.utils     parse_date_pattern:995  Partially rendered name: app-appdevpro\n18:26:03 2016-07-29 18:26:03,206 DEBUG              curator.utils     parse_date_pattern:995  Partially rendered name: app-appdevprod\n18:26:03 2016-07-29 18:26:03,206 DEBUG              curator.utils     parse_date_pattern:995  Partially rendered name: app-appdevprod-\n18:26:03 2016-07-29 18:26:03,206 DEBUG              curator.utils     parse_date_pattern:995  Partially rendered name: app-appdevprod-\n18:26:03 2016-07-29 18:26:03,206 DEBUG              curator.utils     parse_date_pattern:995  Partially rendered name: app-appdevprod-2016\n18:26:03 2016-07-29 18:26:03,206 DEBUG              curator.utils     parse_date_pattern:995  Partially rendered name: app-appdevprod-2016\n18:26:03 2016-07-29 18:26:03,206 DEBUG              curator.utils     parse_date_pattern:995  Partially rendered name: app-appdevprod-201607\n18:26:03 2016-07-29 18:26:03,206 DEBUG              curator.utils     parse_date_pattern:995  Partially rendered name: app-appdevprod-201607\n18:26:03 2016-07-29 18:26:03,206 DEBUG              curator.utils     parse_date_pattern:995  Partially rendered name: app-appdevprod-20160729\n18:26:03 2016-07-29 18:26:03,207 DEBUG              curator.utils     parse_date_pattern:995  Partially rendered name: app-appdevprod-20160729\n18:26:03 2016-07-29 18:26:03,207 DEBUG              curator.utils     parse_date_pattern:995  Partially rendered name: app-appdevprod-2016072918\n18:26:03 2016-07-29 18:26:03,207 DEBUG              curator.utils     parse_date_pattern:997  Fully rendered name: app-appdevprod-2016072918\n18:26:03 2016-07-29 18:26:03,207 ERROR              curator.utils   create_snapshot_body:746  Missing required repository parameter\n18:26:03 2016-07-29 18:26:03,207 INFO      curator.actions.snapshot             do_dry_run:724  DRY-RUN MODE.  No changes will be made.\n18:26:03 2016-07-29 18:26:03,207 INFO      curator.actions.snapshot             do_dry_run:727  DRY-RUN: snapshot: app-appdevprod-2016072918 in repository backup with arguments: False\n18:26:03 2016-07-29 18:26:03,208 INFO                 curator.cli                    cli:252  Action #1: completed\n18:26:03 2016-07-29 18:26:03,208 INFO                 curator.cli                    cli:177  Action #2: delete_indices\n18:26:03 2016-07-29 18:26:03,208 DEBUG                curator.cli                    cli:190  continue_if_exception = False\n18:26:03 2016-07-29 18:26:03,208 DEBUG                curator.cli                    cli:191  timeout_override = None\n18:26:03 2016-07-29 18:26:03,208 DEBUG              curator.utils             get_client:503  kwargs = {'aws_secret_key': None, 'url_prefix': '/logging-store', 'http_auth': None, 'certificate': None, 'aws_key': None, 'aws_region': None, 'client_cert': None, 'hosts': ['xx.xxx.xx.xxx'], 'timeout': 30, 'use_ssl': False, 'master_only': False, 'port': 80, 'ssl_no_validate': False, 'client_key': None}\n18:26:03 2016-07-29 18:26:03,208 DEBUG              curator.utils             get_client:548  Not using \"requests_aws4auth\" python module to connect.\n18:26:03 2016-07-29 18:26:03,208 DEBUG         urllib3.util.retry               from_int:170  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)\nHere's the action.yml:\n```\nactions:\n  1:\n    action: snapshot\n    description: \"snapshot app- older than 7 days\"\n    options:\n      repository: \"backup\"\n      name: app-appdevprod-%Y%m%d%H\n      ignore_unavailable: False\n      include_global_state: True\n      partial: False\n      wait_for_completion: True\n      skip_repo_fs_check: False\n      timeout_override:\n      continue_if_exception: False\n      disable_action: False\n    filters:\n    - filtertype: pattern\n      kind: prefix\n      value: app-\n      exclude:\n    - filtertype: age\n      source: name\n      direction: older\n      timestring: '%Y.%m.%d'\n      unit: days\n      unit_count: 7\n      exclude:\n  2:\n    action: delete_indices\n    description: Delete snapshotted APP indices\n    options:\n      timeout_override:\n      continue_if_exception: False\n      disable_action: False\n    filters:\n    - filtertype: pattern\n      kind: prefix\n      value: app-\n      exclude:\n    - filtertype: age\n      source: name\n      direction: older\n      timestring: '%Y.%m.%d'\n      unit: days\n      unit_count: 7\n      exclude:\n  3:\n    action: snapshot\n    description: \"snapshot os- older than 7 days\"\n    options:\n      repository: \"backup\"\n      name: os-appdevprod-%Y%m%d%H\n      ignore_unavailable: False\n      include_global_state: True\n      partial: False\n      wait_for_completion: True\n      skip_repo_fs_check: False\n      timeout_override:\n      continue_if_exception: False\n      disable_action: False\n    filters:\n    - filtertype: pattern\n      kind: prefix\n      value: os-\n      exclude:\n    - filtertype: age\n      source: name\n      direction: older\n      timestring: '%Y.%m.%d'\n      unit: days\n      unit_count: 7\n      exclude:\n  4:\n    action: delete_indices\n    description: Delete snapshotted OS indices\n    options:\n      timeout_override:\n      continue_if_exception: False\n      disable_action: False\n    filters:\n    - filtertype: pattern\n      kind: prefix\n      value: os-\n      exclude:\n    - filtertype: age\n      source: name\n      direction: older\n      timestring: '%Y.%m.%d'\n      unit: days\n      unit_count: 7\n      exclude:\n```\n. \ud83d\udc4d thanks for the quick work @untergeek \n. ",
    "fasher": "In addtion I can see in the index settings the following field:\n\"version\": {\n          \"created\": \"1030699\",\n          \"upgraded\": \"2030199\"\n        }\nis this not what being mapped into create_date? \n. no, I do not have any filters on the snapshot action. \nthis is how my action file looks like :\nactions:\n   1:\n     action: snapshot\n     description: >-\n       Snapshot all indices with snapshot pattern automatic_%Y-%m-%d-%H-%M-%S\n     options:\n        repository: backup\n        name: automatic_%Y-%m-%d-%H-%M-%S\n        ignore_unavailable: False\n        include_global_state: True\n        partial: False\n        wait_for_completion: True\n        skip_repo_fs_check: False\n        timeout_override:\n        continue_if_exception: False\n        disable_action: False\n   2:\n     action: delete_snapshots\n     descriptions: >-\n       delete snapshot older then 24 hours based on creation_date for automatic_ prefixed snapshots\n     options:\n        repository: backup\n        timeout_override:\n        continue_if_exception: False\n        ignore_empty_list: True\n        disable_action: False\n     filters:\n     - filtertype: pattern\n       kind: prefix\n       value: automatic_\n       exclude:\n     - filtertype: age\n       source: creation_date\n       direction: older\n       unit: hours\n       unit_count: 24\n       exclude:\n. Maybe it will be better to add an option to explicitly ignore this check on certain indices mentioned in the action file? \n. How would you upgrade indices? do I really need to reindex them? that's not very practical on very large index only to get a create date\n. ",
    "ajohnstone": "It seems these maybe a regression?\n\nhttps://github.com/elastic/curator/issues/491\nhttps://github.com/elastic/curator/issues/685\n. Thanks @untergeek I've referenced this ticket in my support case with aws. So hopefully will be listening :)\n. \n",
    "malpani": "@untergeek I am from AWS ES and we want to ideally be able to support curator which is a super useful plugin. While exposing all the information in cluster/state will be harder for us, I would like to explore the path of opening this API and exposing enough information for curator to work.\nWhile I know the best answer is to look up code, could you please share some relevant pointers on the portions of _cluster/state response which get used here?\n. ",
    "chetandhembre": "@untergeek have check out /_all api? I guess it  provides all required details.\n. @untergeek \nself.client.indices.get('_all') is the api.\nIt wont give you index state and routing info, sadly.\n. ",
    "dpgaspar": "Testing AWS ES, already found this issue related with td-agent or logstash:\nhttps://forums.aws.amazon.com/thread.jspa?messageID=694137#694137\nNon related but... \nIs this issue solved? since it's closed, it's not clear to me.\n. Thanks @untergeek for the quick reply.\nDoes curator 3 work with AWS ES.\n. ",
    "RyanBowlby-Reflektion": "I'm certain that tens of people have attempted to use curator 4 with AWS only to end up on this issue. I know it's quite squarely AWS who needs to implement use of _cluster/state, but in the meantime any chance you can add a banner warning to curator 4 mentioning it's not compatible? It'd save users a good bit of hassle.\n. ",
    "algesten": "Too small notice for me to not waste 2h just to end up at this dead end. How about making it clear in the version matrix in the README?\n. ",
    "jimmycuadra": "In case anyone lands on this issue wondering about Curator support for the new version 5 of the AWS Elasticsearch Service, take a look at this issue: https://github.com/elastic/curator/issues/880. @streylight Do you have any updates from AWS about this? AWS ES is not really useable for production for us without this, unfortunately. Is there another issue I should follow to track progress? Or a thread external to GitHub? Thanks much!. Is curator supposed to automatically get credentials using the server's IAM instance role? Or does it need to be given an access key ID and secret via the configuration file? I'm guessing the latter because we're getting this in our logs from curator running on a server that has access to Elasticsearch Service via its IAM instance role:\n2017-06-08 01:32:44,271 INFO      Preparing Action ID: 1, \"delete_indices\"\nTraceback (most recent call last):\n  File \"/usr/local/bin/curator\", line 11, in <module>\n    sys.exit(cli())\n  File \"/usr/local/lib/python2.7/site-packages/click/core.py\", line 722, in __call__\n    return self.main(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/site-packages/click/core.py\", line 697, in main\n    rv = self.invoke(ctx)\n  File \"/usr/local/lib/python2.7/site-packages/click/core.py\", line 895, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/usr/local/lib/python2.7/site-packages/click/core.py\", line 535, in invoke\n    return callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/site-packages/curator/cli.py\", line 167, in cli\n    client = get_client(**client_args)\n  File \"/usr/local/lib/python2.7/site-packages/curator/utils.py\", line 784, in get_client\n    'Error: {0}'.format(e)\nelasticsearch.exceptions.ElasticsearchException: Unable to create client connection to Elasticsearch.  Error: ConnectionError((<urllib3.connection.VerifiedHTTPSConnection object at 0x7f2bbcaf5290>, u'Connection to redacted.us-east-1.es.amazonaws.com timed out. (connect timeout=30)')) caused by: ConnectTimeoutError((<urllib3.connection.VerifiedHTTPSConnection object at 0x7f2bbcaf5290>, u'Connection to redacted.us-east-1.es.amazonaws.com timed out. (connect timeout=30)'))\nWhitelisting an IP address won't work for us because curator is being run from nodes in a Kubernetes cluster where the IPs are not static and will change over time.\nThis is using curator version 5.0.4.. > it does require the IAM credentials to be added to the configuration file. It cannot pull the credentials from Elasticsearch.\nI'm a little confused by this. I'm not trying to pull credentials from Elasticsearch itself. I was asking about using an IAM role from an instance profile: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html. Generally programs that integrate with the AWS API support this automated system.\nI found this in the changelog for curator v4:\n\nExperimental! Use AWS IAM credentials to sign requests to Elasticsearch. This requires the end user to manually install the requests_aws4auth python module.\n\nThat implies that it does support what I was asking about. I'll try adding the requests_aws4auth Python package to my deployment of Curator and see if that makes it work. I quickly glanced at the code and didn't see where this module is being used to provide this behavior. I'll dig in further if just installing the package doesn't work.. I found where the integration with requests_aws4auth happens: https://github.com/elastic/curator/blob/02a5028f18127fc351c9d7190befbb94ce21a4cb/curator/utils.py#L737\nIt actually does not attempt to get credentials using an instance profile, which seems to be a limitation of requests_aws4auth itself. I'll just generate credentials manually.. @cjuroz The code in the gist you linked is not using the type of instance profile authentication I am talking about. It's pulling the credentials out of environment variables. Instance profile authentication works by making HTTP requests to the internal EC2 metadata service to get credentials.. ",
    "oji": "Hi, @untergeek, I'm trying to use curator 3.5.1 with AWS ES 2.3, but I can't find in the docs how to pass aws_key or aws_secret to curator 3.5. Can you give me any pointer on how to do this?\nThanks!. ",
    "Ruposh": "Thank you @untergeek . I corrected all syntax error. But later I found that problem is with \"repository path\". I have updated action scripts and I found \"Repository /home/curator/repository not found\" error. What is the actual pattern to write any directory path. There is another error also \" Failed to complete action: snapshot. : argument of type 'bool' is not iterable\". Please take a look at that.\n. Thank you @untergeek . It helped me a lot.\nI am working in CENTOS 6. I have created a repository at \"/home/curator/repository\" using Elasticsearch API :\ncurl -XPUT 'http://localhost:9200/_snapshot/es_backup' -d '{\n    \"type\": \"fs\",\n    \"settings\": {\n        \"location\": \"/home/curator/repository\",\n        \"compress\": true\n    }\n}'\nAfter that I can take snapshot of my elasticsearch indices by ES API:\ncurl -XPUT \"localhost:9200/_snapshot/es_backup/snapshot_1?wait_for_completion=true\".\nBut If I use curator, same problem arises. Though I can take snapshot but I want to use curator for more flexibility.\n. Ok. I got it. \nWRONG:      repository:  '/home/curator/repository'\nCORRECT:    repository:  es_backup\nThank you @untergeek for your help :)\n. ",
    "inqueue": "curator --version && python -V\ncurator, version 4.0.6\nPython 3.4.3\n@untergeek Using Curator against Elastic Cloud will reproduce the behavior. \ncat config.yml\nclient:\n  hosts:\n    - 9a6ed25bbc72800abad46167e3fb5152.us-east-1.aws.found.io:9243\n  use_ssl: True\n  ssl_no_validate: False\n  http_auth: some_user:anyonesguess\n/usr/lib/python3.4/site-packages/elasticsearch/connection/http_urllib3.py:70: UserWarning: Connecting to 9a6ed25bbc72800abad46167e3fb5152.us-east-1.aws.found.io using SSL with verify_certs=False is insecure.\n  'Connecting to %s using SSL with verify_certs=False is insecure.' % host)\n/usr/lib/python3.4/site-packages/urllib3/connectionpool.py:838: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/security.html\n  InsecureRequestWarning)\nFrom my research, it looks like urllib3 needs a reference to the OS CA bundle since, by default, it does not do any certificate verification. This configuration on CentOS7 allows Curator to work with EC's public cert without warnings:\ncertificate: /etc/ssl/certs/ca-bundle.crt\nAlternatively, if I only want EC's trust in my Curator config, then I can place its CA root and intermediate in a bundle and configure the certificate setting:\nssl_no_validate: False\n  certificate: /opt/es-curator/es-cloud-ca-bundle.pem\nThis will again ignore the OS bundle since, by default, it is not referenced by urllib3 and uses the certificate configuration for verification. I have not tested it yet, but the same should be true for self-signed certificates since urllib3 makes no distinction.\nHopefully we can at least better document how to make this work.\n. ",
    "jbwl": "Thank you for considering it!\n. ",
    "pshroads": "Thanks! I really thought I looked carefully but not carefully enough :/\n. ",
    "elJoeyJojo": "@untergeek i just installed Elasticsearch so how is it that it's so old? How can I upgrade to Elasticsearch Python Module 2.4.0 ? it is really just for testing to see how it goes so I am not too concerned about losing log info in there. Btw, thanks for your help\n. I installed ELK based on Digital Ocean's KB https://www.digitalocean.com/community/tutorials/how-to-install-elasticsearch-1-7-logstash-1-5-and-kibana-4-1-elk-stack-on-centos-7 and all I did was install curator by installing python-pip and then elasticsearch-curator on this CentOS VM\nI will admit, I am not a Linux Admin, seemed like a cool thing to check out and I like it but the logs are filling up too quickly and I can't add disk space so I thought curator could help get rid of old logs but creating a cronjob\n. @untergeek looks like it was 0.4.5\nelasticsearch (0.4.5)\nelasticsearch-curator (0.6.2)\n. k I upgraded them\nelasticsearch (2.4.0)\nelasticsearch-curator (4.0.6)\nhowever after i run /usr/bin/curator i keep getting the same errors, all i am trying to do is run this:\n$ /usr/bin/curator --host localhost delete --older-than 10\n. mmm... i can't even run curator --help \nsame error but the elasticsearch and curator seems to be up to date\ni get the same error, tried rebooting it\n. sorry about that, i totally missed that! thanks @untergeek \n. @untergeek hey Aaron, sorry I guess I saw the the chart and I assumed 'cause I read it quite quickly. I will try to setup Curator for ES 5.0. Thank You\n. hey @untergeek \nthanks for your quick response! I really appreciate it.\nall looks pretty clear except I don't use Logstash, it simply outputs with ElasticSearch, so would the value be according to my metrics? For example I am only really using metricbeat, so would my value be \"metricbeat-*\" as it is in the index patterns of Kibana? \nmy indices in /var/lib/elasticsearch/nodes/0/indices how as this:\n4.0GiB [##########] /XIKf-5NJQR-6rqxg3ql9gQ\n3.8GiB [######### ] /Ob5z2GU3SiWRpy2_oCh2BA\n3.8GiB [######### ] /2ye87jZfQO2U1Rtc5z5Ckg\n3.7GiB [######### ] /ZZ7uG0K6QrqnE3ibkxD8ew\n3.6GiB [######### ] /FAxFrGoHR5Wcu_VpQTdUbg\n3.6GiB [######### ] /8mANDmBoSIGrPik65918SA\n3.6GiB [######### ] /eDMhPPjnT3WS9EC55LBiLA\n3.6GiB [######### ] /qAFjD_IOTVek6yjTN7UIAA\n3.6GiB [######### ] /8IyOE-DTT_Cs6AQZr_RF8g\n2.9GiB [#######   ] /SZLcXCyQSjatitIpUdyONQ\n1.2GiB [###       ] /OyRlR3C9Q0uXU5xq_jAEVg\n. ",
    "nickydu": "Thanks for your quick response! \nThis is the config file\n```\nRemember, leave a key empty if there is no value.  None will be a string,\nnot a Python \"NoneType\"client:\nhosts:\n    - 127.0.0.1:\n  port: 9200\n  url_prefix:\n  use_ssl: False\n  certificate:\n  client_cert:\n  client_key:\n  aws_key:\n  aws_secret_key:\n  aws_region:\n  ssl_no_validate: False\n  http_auth:\n  timeout: 30\n  master_only: False\nlogging:\n  loglevel: INFO\n  logfile:\n  logformat: default\n  blacklist: ['elasticsearch', 'urllib3']\n```\nHere is the action file\n```\nRemember, leave a key empty if there is no value.  None will be a string,\nnot a Python \"NoneType\"\n\nAlso remember that all examples have 'disable_action' set to True.  If you\nwant to use this action as a template, be sure to set this to False after\ncopying it.\nactions:\n  1:\n    action: delete_indices\n    description: >-\n      Delete indices.  Find which to delete by first limiting the list to logstash-\n      prefixed indices. Next filter by space, to those indices in excess of\n      20g of usage.  Then further filter those to prevent deletion of anything\n      less than 30 days old.\n    options:\n      continue_if_exception: False\n      disable_action: False\n    filters:\n    - filtertype: pattern\n      kind: prefix\n      value: logstash-\n    - filtertype: space\n      disk_space: 20\n      reverse: True\n      use_age: True\n      source: creation_date\n      timestring:\n      field:\n      stats_result:\n    - filtertype: age\n      source: creation_date\n      direction: older\n      timestring:\n      unit: days\n      unit_count: 30\n```\nI already moved the config file to the curator directory. \nThe Exception is:\nTraceback (most recent call last):\n  File \"/usr/bin/curator\", line 9, in <module>\n    load_entry_point('elasticsearch-curator==4.0.6', 'console_scripts', 'curator')()\n  File \"/usr/lib/python2.6/site-packages/curator/curator.py\", line 5, in main\n    cli()\n  File \"/usr/lib/python2.6/site-packages/click/core.py\", line 716, in __call__\n    return self.main(*args, **kwargs)\n  File \"/usr/lib/python2.6/site-packages/click/core.py\", line 696, in main\n    rv = self.invoke(ctx)\n  File \"/usr/lib/python2.6/site-packages/click/core.py\", line 889, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/usr/lib/python2.6/site-packages/click/core.py\", line 534, in invoke\n    return callback(*args, **kwargs)\n  File \"/usr/lib/python2.6/site-packages/curator/cli.py\", line 128, in cli\n    yaml_config  = get_yaml(config)\n  File \"/usr/lib/python2.6/site-packages/curator/utils.py\", line 38, in get_yaml\n    cfg = yaml.load(raw)\n  File \"/usr/lib64/python2.6/site-packages/yaml/__init__.py\", line 71, in load\n    return loader.get_single_data()\n  File \"/usr/lib64/python2.6/site-packages/yaml/constructor.py\", line 37, in get_single_data\n    node = self.get_single_node()\n  File \"/usr/lib64/python2.6/site-packages/yaml/composer.py\", line 39, in get_single_node\n    if not self.check_event(StreamEndEvent):\n  File \"/usr/lib64/python2.6/site-packages/yaml/parser.py\", line 98, in check_event\n    self.current_event = self.state()\n  File \"/usr/lib64/python2.6/site-packages/yaml/parser.py\", line 174, in parse_document_start\n    self.peek_token().start_mark)\nyaml.parser.ParserError: expected '<document start>', but found '<block mapping start>'\n  in \"<string>\", line 20, column 1:\n    logging:\n    ^\n. Thanks! I removed it, but still see the same error...\n. Thank you so much! It's working now!\n. ",
    "cinhtau": ":+1: currently we are doing it with Jenkins and the ES REST API, but Curator would be the desired way to go. Yes tried that too with opposite of exclude. Not working. \n```yaml\nactions:\n  1:\n    action: snapshot\n    description: >-\n      Snapshot kibana to nas\n    options:\n      repository: six-nas\n      name: 'kibana-%Y%m%d'\n      ignore_unavailable: False\n      include_global_state: True\n      partial: False\n      wait_for_completion: True\n      skip_repo_fs_check: False\n      timeout_override:\n      continue_if_exception: False\n      disable_action: False\n    filters:\n    - filtertype: kibana\n      exclude: False\n```\nThe problem is that the .kibana index is not listed in the indices list.\n2017-02-10 15:25:58,066 DEBUG              curator.utils        iterate_filters:847  Post-instance: ['foo-2017.01.01', foo-2017.01.02', ...]. Hi, great advice about the filter pattern. :+1:  I think there is a misunderstanding here. Curator didn't get any system indices ( starts with . like .kibana, .security, .watcher) to begin with. So any filter will do me no good. I using Curator 4.2.5 against Elasticsearch 5.1.2. Curator gives me this error:\n2017-02-12 10:54:55,279 ERROR curator.cli\ncli:187  Unable to complete action \"snapshot\".  No actionable items in list: <class 'curator.exceptions.NoIndices'>\nAfter a little test, I found out that my created x-pack user has no permissions to see the system indices, just used the admin or superuser and .kibana was listed. After that the filter pattern worked.\n2017-02-12 11:03:21,299 DEBUG          curator.indexlist        filter_by_regex:383  Filter by regex: Index: .monitoring-kibana-2-2017.02.06\n2017-02-12 11:03:21,299 DEBUG          curator.indexlist       __not_actionable:39   Index .monitoring-kibana-2-2017.02.06 is not actionable, removing from list.\n2017-02-12 11:03:21,300 DEBUG          curator.indexlist        filter_by_regex:383  Filter by regex: Index: .monitoring-kibana-2-2017.02.08\n2017-02-12 11:03:21,300 DEBUG          curator.indexlist       __not_actionable:39   Index .monitoring-kibana-2-2017.02.08 is not actionable, removing from list.\n2017-02-12 11:03:21,303 DEBUG          curator.indexlist        filter_by_regex:383  Filter by regex: Index: .monitoring-kibana-2-2017.02.12\n2017-02-12 11:03:21,303 DEBUG          curator.indexlist       __not_actionable:39   Index .monitoring-kibana-2-2017.02.12 is not actionable, removing from list.\n2017-02-12 11:03:21,309 DEBUG          curator.indexlist        filter_by_regex:383  Filter by regex: Index: .monitoring-kibana-2-2017.02.07\n2017-02-12 11:03:21,309 DEBUG          curator.indexlist       __not_actionable:39   Index .monitoring-kibana-2-2017.02.07 is not actionable, removing from list.\n2017-02-12 11:03:21,309 DEBUG          curator.indexlist        filter_by_regex:383  Filter by regex: Index: .monitoring-kibana-2-2017.02.11\n2017-02-12 11:03:21,309 DEBUG          curator.indexlist       __not_actionable:39   Index .monitoring-kibana-2-2017.02.11 is not actionable, removing from list.\n2017-02-12 11:03:21,309 DEBUG          curator.indexlist        filter_by_regex:383  Filter by regex: Index: .monitoring-kibana-2-2017.02.10\n2017-02-12 11:03:21,309 DEBUG          curator.indexlist       __not_actionable:39   Index .monitoring-kibana-2-2017.02.10 is not actionable, removing from list.\n2017-02-12 11:03:21,311 DEBUG          curator.indexlist        filter_by_regex:383  Filter by regex: Index: .monitoring-kibana-2-2017.02.05\n2017-02-12 11:03:21,311 DEBUG          curator.indexlist       __not_actionable:39   Index .monitoring-kibana-2-2017.02.05 is not actionable, removing from list.\n2017-02-12 11:03:21,314 DEBUG          curator.indexlist        filter_by_regex:383  Filter by regex: Index: .kibana\n2017-02-12 11:03:21,314 DEBUG          curator.indexlist           __actionable:35   Index .kibana is actionable and remains in the list.\n2017-02-12 11:03:21,314 DEBUG          curator.indexlist        filter_by_regex:383  Filter by regex: Index: .monitoring-kibana-2-2017.02.09\n2017-02-12 11:03:21,314 DEBUG          curator.indexlist       __not_actionable:39   Index .monitoring-kibana-2-2017.02.09 is not actionable, removing from list.\n2017-02-12 11:03:21,314 DEBUG              curator.utils        iterate_filters:847  Post-instance: ['.kibana']\nSo snapshot done. :tada: \n. ",
    "martinhynar": "I installed elasticsearch-curator pip package as recommended and started to transform my cron jobs to use action files instead of passing bunch of flags. It is extra work, but from the very beginning I see it is way more flexible and feature rich, so it is worth making.\nIt is that python-elasticsearch-curator-4.x.x rpm package has those rich command line flags that make its usage handy for ad-hoc checks in terminal.\nAnyway, closing this. There is nothing to work on. Maybe, have pyaml as official package, but that different story.\n. ",
    "JD557": "Done.\nI'm not sure if I need to do anything else to retry the check.\n. > What this means is that in order for it to coerce a string value of 'true'|'True' or 'false'|'False' into a true bool, I need to do it myself.\nI'm sorry for bringing this up again (as I previously stated, I'm not really familiar with Voluptuous), but wouldn't this be fixed if the code called Boolean() instead of All(Any(bool, int), Coerce(bool))?\nFrom the Boolean source code:\n``` python\ndef Boolean(v):\n    \"\"\"Convert human-readable boolean values to a bool.\nAccepted values are 1, true, yes, on, enable, and their negatives.\nNon-string values are cast to bool.\n\n>>> validate = Schema(Boolean())\n>>> validate(True)\nTrue\n>>> validate(\"1\")\nTrue\n>>> validate(\"0\")\nFalse\n>>> with raises(MultipleInvalid, \"expected boolean\"):\n...   validate('moo')\n>>> try:\n...  validate('moo')\n... except MultipleInvalid as e:\n...   assert isinstance(e.errors[0], BooleanInvalid)\n\"\"\"\nif isinstance(v, basestring):\n    v = v.lower()\n    if v in ('1', 'true', 'yes', 'on', 'enable'):\n        return True\n    if v in ('0', 'false', 'no', 'off', 'disable'):\n        return False\n    raise ValueError\nreturn bool(v)\n\n```\nThis seems to be exactly what you are looking for, isn't it?\n. Using the/_cat/tasks endpoint, you can look for cluster:admin/snapshot/delete tasks.\nThere's probably a better way, using the JSON API, though.. Yes, that's what's happening here. Curator starts to delete one snapshot and then attempts to delete a second one and fails.\nAlso, I have multiple delete snapshot actions with continue_if_exception: True, so if the first one fails the next ones will fail as well, as there's a snapshot being deleted.\nI'm not sure why curator is not waiting for the first snapshot to finish being deleted (I am connecting to elasticsearch via an internal load balancer, so I might have some timeout defined somewhere which drops the connection while the snapshot is being deleted).. You are right, I forgot that curator was using a Load Balancer, while I was connecting directly to the machines.\nSorry, my bad.. ",
    "emanjogu": "Okay, I will do that. Thanks for the quick reply!\n. ",
    "galkinrost": "Yes, after ci failure I've signed\n. ",
    "elliott-davis": "I thought so too - so I did a build of master and tested it out - the config file I used looks like:\nactions:\n    action: delete_indices\n    options:\n      disable_action: ${DISABLE_DELETE:False}\n    filters:\n      - filtertype: age\n        source: name\n        timestring: 'foo-%Y.%m.%d'\n        direction: older\n        unit: days\n        unit_count: ${DAYS_TO_KEEP:14}\nWhen I run: DAYS_TO_KEEP=30 DISABLE_DELETE=True curator action_file_above.yml \nI get: curator.exceptions.ConfigurationError: Configuration: options: Location: Action ID \"1\", action \"snapshot\": Bad Value: \"True\", expected bool for dictionary value @ data['disable_action']. Check configuration file.\n. No inconvenience at at @untergeek. I'm glad that I could find an edge case. Thank you for taking it on!\n. ",
    "geek876": "Sure. So we have the ES Cluster within AWS and want to Orchestrate curator jobs from within Jenkins so having a command line would help as we could then simply do it via a 'shell script build step' via Jenkins. This will keep the jenkins job very light weight with no dependency on any repository etc to get the config files.\n. I am just using a shell script now which works but if you could please prioritize snapshot and index deletion actions than that would be much appreciated. Thanks.\n. ",
    "FGRibreau": "This is indeed a serious regression compared to 3.x :+1:\n. ",
    "mikej1688": "I am using curator 4.2.6. I'd like to run a command line, like\ncurator_cli --host \"host1,host2,host3\" ...\nCan I do this way for multiple hosts? If not, what's the correct way?  I only want to run the singleton command line for curator.. Thanks for the reply.\nHere are the reasons why we want to use multi hosts: \n1). if we run a command like, \"curator_cli --host host1 ...\", and if the ES node host1 is down, a connection exception would be thrown and the curator job could not be done. But in a cluster, there would be more than one ES nodes available. We would like to resolve the above issue by connecting to another active ES node if the current one was down;\n2). if I use a command like \"curator --config myconfig.yml ...\" and inside the config file, I can specify a hosts in the client,\n  client:\n    hosts:\n      - \"host1\"\n      - \"host2\"\n    port: 9200\n    ...\nhere 2 hosts are specified. My question is: when I run the curator command, if the host1 is down, will the host2 be automatically connected thus no failure would be occurred?\n. sweet, thanks.\nWith curator_cli, I can still set the config.yml and run an action.yml file, right?\nThat's what I expected.. ",
    "donbeave": "Oh, sorry for the duplicate.\n. ",
    "mdconner": "I\u2019m using elasticsearch/bin/plugin install file://curator.zip.\nMichael D. Conner\nValley Forge, PA\n(610) 354 - 4916\nFrom: Aaron Mildenstein [mailto:notifications@github.com]\nSent: Thursday, September 29, 2016 1:34 PM\nTo: elastic/curator curator@noreply.github.com\nCc: Conner, Michael D (US) michael.d.conner@lmco.com; Author author@noreply.github.com\nSubject: EXTERNAL: Re: [elastic/curator] No plugin-descriptor.properties file (#774)\nCan you provide more context? I've never seen that error before.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHubhttps://github.com/elastic/curator/issues/774#issuecomment-250536655, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AIHxiQJe48ty7jlEy1HSZhuql5RE7Zmvks5qu_abgaJpZM4KKPtd.\n. ",
    "gmarziou": "I'm using Curator 4.1.1 on top of ES 2.3.0.\nI installed curator using pip:\n-   urllib3==1.12\n- click==6.3\n- PyYAML==3.11\n- elasticsearch==2.4.0\n- elasticsearch-curator==4.1.1\n. I'm calling it from a cron defined as below\n/bin/curator --config=/home/example/curator/curator.yml /home/example/curator/close_indices.yml\n\nThe config file: /home/example/curator/curator.yml\n\n```\nRemember, leave a key empty if there is no value.  None will be a string,\nnot a Python \"NoneType\"\nclient:\n  hosts: \"127.0.0.1\"\n  port: 9200\n  url_prefix:\n  use_ssl: False\n  certificate:\n  client_cert:\n  client_key:\n  ssl_no_validate: False\n  http_auth:\n  timeout: 30\n  master_only: False\nlogging:\n  loglevel: INFO\n  logfile:\n  logformat: default\n  blacklist: ['elasticsearch', 'urllib3']\n```\n\n\nThe action file for closing: /home/example/curator/close_indices.yml\n\n```\nactions:\n  1:\n    action: close\n    description: >-\n      Close indices older than 30 days (based on index name), for logstash-\n      prefixed indices.\n    options:\n      delete_aliases: False\n      timeout_override:\n      continue_if_exception: False\n      disable_action: False\n    filters:\n    - filtertype: pattern\n      kind: prefix\n      value: logstash-\n      exclude:\n    - filtertype: age\n      source: name\n      direction: older\n      timestring: '%Y.%m.%d'\n      unit: days\n      unit_count: 30\n      exclude:\n```\n\nI also have another cron with an action file to delete the older indices and I can't remember if I had to increase the url length for closing or deleting. Maybe this could be another path for getting the error.\n\nThe action file for deleting: /home/example/curator/delete_indices.yml\n\n```\nactions:\n  1:\n    action: delete_indices\n    description: >-\n      Delete indices older than 180 days (based on index name), for logstash-\n      prefixed indices. Ignore the error if the filter does not result in an\n      actionable list of indices (ignore_empty_list) and exit cleanly.\n    options:\n      ignore_empty_list: True\n      timeout_override:\n      continue_if_exception: False\n      disable_action: False\n    filters:\n    - filtertype: pattern\n      kind: prefix\n      value: logstash-\n      exclude:\n    - filtertype: age\n      source: name\n      direction: older\n      timestring: '%Y.%m.%d'\n      unit: days\n      unit_count: 180\n      exclude:\n```\n\n. My longest index name is something like logstash-supervision-2016.09.01.\nI'll try to get the log file tomorrow though I may have only one system left in similar state but probably with less indices open than the original one.\n. I should be able to work on this next week.\n. Hello, I failed to reproduce the long URL problem.\nSo, I'm closing this issue and I'm sorry for wasting your time, thanks.\n. ",
    "floragunncom": "\n$ apt-get install -y python3-pip python3-elasticsearch-curator=4.1.2\n$ pip3 install -U setuptools\nSuccessfully installed setuptools-28.3.0\n\nresults in\n\nFile \"/usr/local/lib/python3.5/dist-packages/pkg_resources/__init__.py\", line 856, \nin resolve raise DistributionNotFound(req, requirers)\npkg_resources.DistributionNotFound: The 'elasticsearch-curator==4.1.2' distribution \nwas not found and is required by the application\n\npip version is pip 8.1.2, python version is Python 3.5.1+\n\n$ lsb_release -a\nDistributor ID: Ubuntu\nDescription:    Ubuntu 16.04 LTS\nRelease:    16.04\nCodename:   xenial\n$ uname -a\nLinux 721c971d0d37 4.4.20-moby #1 SMP Thu Sep 15 12:10:20 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n$ cat /proc/version\nLinux version 4.4.20-moby (root@3934ed318998) (gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.2) ) #1 SMP Thu Sep 15 12:10:20 UTC 2016\n$ python -V\nPython 2.7.12\n\nThe certificates are signed by an own root CA (private PKI)\n. apt-get install -y python3-setuptools python3-elasticsearch-curator fails with ImportError: No module named 'pkg_resources'\nIf i install with\n\npip3 install --upgrade pip\npip3 install -U setuptools\npip3 install -U elasticsearch-curator \n\nthen curator failed with\n\n Traceback (most recent call last):\n   File \"/usr/local/lib/python3.5/dist-packages/elasticsearch/connection/http_urllib3.py\", line 94, in perform_request\n     response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)\n   File \"/usr/local/lib/python3.5/dist-packages/urllib3/connectionpool.py\", line 594, in urlopen\n     chunked=chunked)\n   File \"/usr/local/lib/python3.5/dist-packages/urllib3/connectionpool.py\", line 350, in _make_request\n     self._validate_conn(conn)\n   File \"/usr/local/lib/python3.5/dist-packages/urllib3/connectionpool.py\", line 833, in _validate_conn\n     conn.connect()\n   File \"/usr/local/lib/python3.5/dist-packages/urllib3/connection.py\", line 324, in connect\n     cert = self.sock.getpeercert()\n   File \"/usr/lib/python3.5/ssl.py\", line 814, in getpeercert\n     return self._sslobj.getpeercert(binary_form)\n   File \"/usr/lib/python3.5/ssl.py\", line 591, in getpeercert\n     return self._sslobj.peer_certificate(binary_form)\n SystemError:  returned NULL without setting an error\n During handling of the above exception, another exception occurred:\n Traceback (most recent call last):\n   File \"/usr/local/lib/python3.5/dist-packages/curator/utils.py\", line 580, in get_client\n     check_version(client)\n   File \"/usr/local/lib/python3.5/dist-packages/curator/utils.py\", line 424, in check_version\n     version_number = get_version(client)\n   File \"/usr/local/lib/python3.5/dist-packages/curator/utils.py\", line 397, in get_version\n     version = client.info()['version']['number']\n   File \"/usr/local/lib/python3.5/dist-packages/elasticsearch/client/utils.py\", line 69, in _wrapped\n     return func(*args, params=params, **kwargs)\n   File \"/usr/local/lib/python3.5/dist-packages/elasticsearch/client/__init__.py\", line 220, in info\n     return self.transport.perform_request('GET', '/', params=params)\n   File \"/usr/local/lib/python3.5/dist-packages/elasticsearch/transport.py\", line 327, in perform_request\n     status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)\n   File \"/usr/local/lib/python3.5/dist-packages/elasticsearch/connection/http_urllib3.py\", line 105, in perform_request\n     raise ConnectionError('N/A', str(e), e)\n elasticsearch.exceptions.ConnectionError: ConnectionError( returned NULL without setting an error) caused by: SystemError( returned NULL without setting an error)\n During handling of the above exception, another exception occurred:\n Traceback (most recent call last):\n   File \"/usr/local/bin/curator\", line 11, in \n     load_entry_point('elasticsearch-curator==4.1.2', 'console_scripts', 'curator')()\n   File \"/usr/local/lib/python3.5/dist-packages/curator/curator.py\", line 5, in main\n     cli()\n   File \"/usr/local/lib/python3.5/dist-packages/click/core.py\", line 716, in __call__\n     return self.main(*args, **kwargs)\n   File \"/usr/local/lib/python3.5/dist-packages/click/core.py\", line 696, in main\n     rv = self.invoke(ctx)\n   File \"/usr/local/lib/python3.5/dist-packages/click/core.py\", line 889, in invoke\n     return ctx.invoke(self.callback, **ctx.params)\n   File \"/usr/local/lib/python3.5/dist-packages/click/core.py\", line 534, in invoke\n     return callback(*args, **kwargs)\n   File \"/usr/local/lib/python3.5/dist-packages/curator/cli.py\", line 165, in cli\n     client = get_client(**client_args)\n   File \"/usr/local/lib/python3.5/dist-packages/curator/utils.py\", line 587, in get_client\n     'Error: {0}'.format(e)\n elasticsearch.exceptions.ElasticsearchException: Unable to create client connection to Elasticsearch.  Error: ConnectionError( returned NULL without setting an error) caused by: SystemError( returned NULL without setting an error)\n\nI use https://github.com/floragunncom/search-guard-ssl to secure elasticsearch and its configured i that kind that the client is required to present a client certificate.\nI know this kind of error from when i try to connect via urllib3 directly and i had to call urllib3.contrib.pyopenssl.inject_into_urllib3() first to make it work. RequestsHttpConnection seems to work out of the box.\n. Did you try Shield with client certificates enforced?\nshield.http.ssl.client.auth: required\n. ",
    "jayant-biz4": "Thanks for your help. The scenario is the second scenario. I was little confused I guess. Those were  keys for S3 storage as the snapshot is being stored on S3. Did not realize that those keys are stored as part of snapshot repository itself.\nI have removed the keys from the configuration file and everything is working now.\n. ",
    "serialdoom": "https://gist.github.com/serialdoom/46087038f7e458a6b6905f09de45fefa\n. damn, that is indeed correct - the files are old enough and that create the logstash indices with the dates that are in the past (older than N days) but with a creation date of today.\nCheers for spotting that, ill close the issue.\n. ",
    "umarmir": "Output while using curator curator --dry-run  ~/.curator/deleteclose.yml and setting ignore_empty_list: True, But issue is that i am unable to delete my old close indices.\n2016-10-19 07:47:09,450 INFO      Preparing Action ID: 1, \"close\"\n2016-10-19 07:47:09,466 INFO      Trying Action ID: 1, \"close\": Close 7 days older indices via creation_date\n2016-10-19 07:47:12,113 INFO      DRY-RUN MODE.  No changes will be made.\n2016-10-19 07:47:12,113 INFO      (CLOSED) indices may be shown that may not be acted on by action \"close\".\n2016-10-19 07:47:12,114 INFO      DRY-RUN: close: ***-***-***-2016-10-08 (CLOSED) with arguments: {'delete_aliases': True}\n2016-10-19 07:47:12,114 INFO      DRY-RUN: close: ***-***-***-2016-10-09 (CLOSED) with arguments: {'delete_aliases': True}\n2016-10-19 07:47:12,121 INFO      Action ID: 1, \"close\" completed.\n2016-10-19 07:47:12,121 INFO      Preparing Action ID: 2, \"delete_indices\"\n2016-10-19 07:47:12,125 INFO      Trying Action ID: 2, \"delete_indices\": Delete 10 days older indices via creation_date\n2016-10-19 07:47:13,030 INFO      DRY-RUN MODE.  No changes will be made.\n2016-10-19 07:47:13,030 INFO      (CLOSED) indices may be shown that may not be acted on by action \"delete_indices\".\n2016-10-19 07:47:13,031 INFO      Action ID: 2, \"delete_indices\" completed.\n2016-10-19 07:47:13,031 INFO      Job completed.\nThanks for quick response BTW\n. It's a huge output.. last lines for action id 2, if it can help... \nwhile running with loglevel: DEBUG\n2016-10-19 08:29:19,067 DEBUG          curator.indexlist            __excludify:58   Removed from actionable list: Index \"***-all-***-2016-10-19\" age (1476849551), direction: \"older\", point of reference, (1476016158)\n2016-10-19 08:29:19,068 DEBUG          curator.indexlist       __not_actionable:39   Index ***-all-***-2016-10-18 is not actionable, removing from list.\n2016-10-19 08:29:19,068 DEBUG          curator.indexlist            __excludify:58   Removed from actionable list: Index \"***-all-***-2016-10-18\" age (1476763151), direction: \"older\", point of reference, (1476016158)\n2016-10-19 08:29:19,068 DEBUG          curator.indexlist       __not_actionable:39   Index ***-***-***-2016-10-18 is not actionable, removing from list.\n2016-10-19 08:29:19,068 DEBUG          curator.indexlist            __excludify:58   Removed from actionable list: Index \"***-***-***-2016-10-18\" age (1476787306), direction: \"older\", point of reference, (1476016158)\n2016-10-19 08:29:19,068 DEBUG          curator.indexlist       __not_actionable:39   Index ***-***-***-2016-10-19 is not actionable, removing from list.\n2016-10-19 08:29:19,068 DEBUG          curator.indexlist            __excludify:58   Removed from actionable list: Index \"***-***-***-2016-10-19\" age (1476871167), direction: \"older\", point of reference, (1476016158)\n2016-10-19 08:29:19,068 DEBUG          curator.indexlist       __not_actionable:39   *** ***-***-en-2016-10-15 is not actionable, removing from list.\n2016-10-19 08:29:19,069 DEBUG          curator.indexlist            __excludify:58   Removed from actionable list: Index \"***-***-en-2016-10-15\" age (1476523102), direction: \"older\", point of reference, (1476016158)\n2016-10-19 08:29:19,069 DEBUG          curator.indexlist       __not_actionable:39   Index ***-***-en-2016-10-14 is not actionable, removing from list.\n2016-10-19 08:29:19,069 DEBUG          curator.indexlist            __excludify:58   Removed from actionable list: Index \"***-***-en-2016-10-14\" age (1476438943), direction: \"older\", point of reference, (1476016158)\n2016-10-19 08:29:19,069 DEBUG              curator.utils        iterate_filters:847  Post-instance: [u'***-***-si-2016-10-09']\n2016-10-19 08:29:19,069 DEBUG     curator.actions.delete_indices               __init__:334  master_timeout value: 300s\n2016-10-19 08:29:19,069 INFO               curator.utils           show_dry_run:620  DRY-RUN MODE.  No changes will be made.\n2016-10-19 08:29:19,069 INFO               curator.utils           show_dry_run:623  (CLOSED) indices may be shown that may not be acted on by action \"delete_indices\".\n2016-10-19 08:29:19,070 INFO               curator.utils           show_dry_run:630  DRY-RUN: delete_indices: ***-***-si-2016-10-09 (CLOSED) with arguments: {}\n2016-10-19 08:29:19,070 INFO                 curator.cli                    cli:202  Action ID: 2, \"delete_indices\" completed.\n2016-10-19 08:29:19,071 INFO                 curator.cli                    cli:203  Job completed.\n. All output that i have\nOutput.txt\n. We know about this issue that we are still getting data in old date named indices, but it's separate thing. Here i am mentioning two entries which were created on 09 Oct 2016 with same date convention currently they are closed. But action: delete_indices unable to delete them, as they are 10 days old and closed indices.\n2016-10-19 08:29:18,984 DEBUG          curator.indexlist            __excludify:58   Removed from actionable list: Index \"***-***-en-2016-10-09\" age (1476022761), direction: \"older\", point of reference, (1476016158) \ncreation date 1476022761 Sun, 09 Oct 2016 14:19:21 GMT\n2016-10-19 08:29:19,024 DEBUG          curator.indexlist            __excludify:58   Removed from actionable list: Index \"***-***-en-2016-10-09\" age (1476019759), direction: \"older\", point of reference, (1476016158) \ncreation date 1476022761 Sun, 09 Oct 2016 13:29:19 GMT\n. Thanks, Now it make sense. \nNow that indices gone. \nIt's working fine.\n. ",
    "jjm": "I've completed the CLA  now, created the pull request too early :-(.\n. Your welcome. liking the way to configure 4.0 so much more than 3.x.\n. ",
    "anhlqn": "Thanks for the clarification.\n. ",
    "pramod08": "Thank you for reply @untergeek\n. ",
    "tschroeder-zendesk": "I have my own branch using beaker caching (http://beaker.readthedocs.io/en/latest/caching.html) to cache a few methods for an amount of time I haven't submitted this because I just hacked it to do what I needed to not look up every node every time, but we could look at using this to address many issues and it would be easy enough to set it up to be used only if enabled and doing configuration etc. on the client side.  Here is my branch if you are curious how I did it: https://github.com/tschroeder-zendesk/curator/commit/46198d15388a95061bd957f7938b8f0e21960cde\n  . Yeah I'm adding tests now. I will make sure it works. Thanks.. This will work for anyone who wants to do a daily rollover like this:\n name-2017.07.26\n name-2017.07.27\n etc.\nAnd that use case will work with this code. The case I want is really to be able to take in a bad named index and start rollover on it and then continue rollover with the same curator script which I can do but will need more work so I am going to do that as a different pull request. This one adds a feature and works fine. It's not amazing, but it doesn't introduce any new issues. Using new_index avoids the requirement of -#, but even before my change you could do new_index=blah and it would fail on the next run since blah would already exist. Anyways as such short of any more needed testing you all think I should add I am calling this done.. I'm not sure why you think this is a bug. You are allowed to rollover any index if you provide a new_index name. It's in the documentation right here: \nhttps://www.elastic.co/guide/en/elasticsearch/reference/master/indices-rollover-index.html#_naming_the_new_index\n. I ended up working around this issue. I will try to get to it to add more testing if I have time at the moment I need to put this on hold.. Yeah I did that for my filters, but in my testing environment I had a few indexes that already had 1 shard so they caused this issue. I can easily work around it, but it seemed like it would be a good feature to have.. Ok.. will update.. Retargeted and rebased. My changelog is in the wrong place with this switch. Do you want me to add a new version name or will you fix that up? I wasn't sure how to format this. It's just the one entry in 5.3 that needs to be moved.. No worries. Updated.. The two tests that failed appear to be travis issues can you re-run them?. Since the change is inside the _sort_by_age each pattern group would be sorted first alphabetically and then by age. So same affect on those indices. I can add an additional test on that if you would like.. Done!. \ud83d\udc4d . Sorry I can put them back they just looked like a commented out old block. ",
    "iain17": "Is there any alternative of doing this? I did find this repo: https://github.com/cldcvr/elasticsearch-s3-backup\nWould it be wise calling curator afterwards to just delete a section of the data?\nAlso based from a little searching I found an answer from Amazon about this. Seems it is just a question of adding the right IAM policy rules: https://forums.aws.amazon.com/thread.jspa?messageID=697248 see HernanV@AWS his response.\nWas this looked into at the time? A signed request should do it then?\nOr is it better to just install elasticsearch-cloud-aws plugin? But that'll only work for elastic.co's hosted aws version right? Or is there some other way of doing this?. ",
    "soenkeliebau": "Ok, fair enough. It is not an issue anyway, more of a minor nuisance. Thanks for the quick response.\nI'll close the issue.\n. I agree with you on the regex that would be necessary for this to work. While in theory we could probably come up with a defined marker for the unit count and a syntax similar to the index name pattern the longer I thought about it the more complex I had to make it to accomodate the cases I came up with. Regexes have been around for a long time, are extremely powerful and well documented (as in there is a lof of documentation, not necessarily saying all of it is good) - so no need to reinvent the wheel.\nI won't pretend to know the background of the average curator user, but I think a fair few of them have grep-ed some log files back in the day and used regular expressions to do so, so I am confident that this should work out nicely.\nOn your second question, I think that this will play quite nicely with the rollover feature, since the underlying principle is the same.\nFor example, if we have two different retention times, 30 and 60 days, if I don't misunderstand the rollover pattern, the indices for this might look like this:\nlogs-30-1\nlogs-30-2\nlogs-30-3\nlogs-60-1\nlogs-60-2\nlogs-60-3\nIf we tell curator to delete indices containing data older than the retention time and use index names to determine calculation times, we'd probably have a regex like this: \"logs-([0-9]+)-[0-9]+\". Apply this to the index names and extract 30 or 60 and store this as unit_count per index. Processing after this doesn't need to change much I think.\nI've looked through the code a bit and unless I totally miss my mark (which I probably am), I don't think that this would be too large of a change to implement.\nMy idea was to add a new parameter to the age filter \"unit_count_pattern\" which stores the regex. Maybe change the parameters so that at least one of \"unit_count\" or \"unit_count\" pattern has to be present, but both are allowed too. If both are there we can fall back to the given unit count if the pattern doesn't match, if only the pattern is present indices that don't match the pattern are ignored.\nThe actual implementation would I think go here. Check if a pattern was defined, if yes compare it to the index name, extract the retention time and change the \"agetest\" variable to compare against a different point of reference.\nThere are probably a whole lot of issues that would creep up while implementing this, but in principle I think it could work.\n. Right, I'll give it a shot. I am sure I'll need some help with tests though, thanks for offering!\nQuick question up front, is there an existing way to link two filter elements so that one of them has to be present? If not, I'd probably rethink the way to configure this to avoid using a dependency like that.. I've created a pull request with some code purely for a high level review of the approach taken. This does work for the tests that I performed and doesn't seem to break any unit tests. I had a few failures when running them locally, but those were about snapshots which my ES instance is probably not correctly set up for.\nI've changed the parameter behavior for now as follows:\nunit_count remains mandatory\nunit_count_pattern is optional, if it is present, it is matched against the index name, if a successful match is returned that is a number that number is used instead of unit_count - if a match is not possible, unit_count is used as fallback, unless it is set to -1 in which case the index is ignored.\nThis is probably not how we want to keep, but was easiest for me to implement for intial testing.. No worries, there really is no rush on this.\nI'll see if I find some time to look at existing tests and try to come up with a few cases to test this functionality in the meantime.. I've rebased on master and added some integration tests for the filtering functionality as well as fixed two small things regarding error handling, when an illegal regex is given or it contains no capture group.\nI've also added documentation for the new parameter and the value -1 for unit_count.\nDo we also want to add unit tests? I was unsure because I'd have to add new cluster and index settings to the testvars and it seemed like duplication of what we already test with the integration tests to me. \nBut I am of course happy to do that as well.. thanks for catching and fixing this @untergeek !\nDo you want me to add a test for months with fixed epoch also?. ",
    "karmi": "Hi @DanilSerd, we have found your signature in our records, but it seems like you have signed with a different e-mail than the one used in yout Git commit. Can you please add both of these e-mails into your Github profile (they can be hidden), so we can match your e-mails to your Github profile?\n. Hi @tahaderouiche, we have found your signature in our records, but it seems like you have signed with a different e-mail than the one used in yout Git commit. Can you please add both of these e-mails into your Github profile (they can be hidden), so we can match your e-mails to your Github profile?. Hi @kobuskc, we have found your signature in our records, but it seems like you have signed with a different e-mail than the one used in yout Git commit. Can you please add both of these e-mails into your Github profile (they can be hidden), so we can match your e-mails to your Github profile?. Hi @kobuskc, now the e-mail in the Git commit is the same as in the CLA signature, thanks for whatever you did to fix that! :). Hi @monkey3199, we have found your signature in our records, but it seems like you have signed with a different e-mail than the one used in yout Git commit. Can you please add both of these e-mails into your Github profile (they can be hidden), so we can match your e-mails to your Github profile?. ",
    "arturmartins": "deb package seems to be ok.\ndpkg-deb --info  /var/cache/apt/archives/elasticsearch-curator_4.2.1_amd64.deb:\nOutput:\nnew debian package, version 2.0.\n size 9321894 bytes: control archive=3178 bytes.\n     561 bytes,    13 lines      control\n    4842 bytes,    44 lines      md5sums\n    1305 bytes,    43 lines   *  postinst             #!/bin/sh\n    1426 bytes,    46 lines   *  postrm               #!/bin/sh\n     821 bytes,    33 lines   *  preinst              #!/bin/sh\n     718 bytes,    33 lines   *  prerm                #!/bin/sh\n Package: elasticsearch-curator\n Version: 4.2.1\n License: Apache-2.0\n Vendor: Elastic\n Architecture: amd64\n Maintainer: 'Elastic Developers <info@elastic.co>'\n Installed-Size: 20640\n Conflicts: python-elasticsearch-curator, python3-elasticsearch-curator\n Provides: elasticsearch-curator\n Section: tools\n Priority: extra\n Homepage: http://example.com/no-uri-given\n Description: Have indices in Elasticsearch? This is the tool for you!\\n\\nLike a museum curator manages the exhibits and collections on display, \\nElasticsearch Curator helps you curate, or manage your indices.```\n. It does not work even by moving the directory and create a fresh new directory:\nmv /opt/elasticsearch-curator /opt/elasticsearch-curator-old\nmkdir /opt/elasticsearch-curator\nStill says \nUnpacking elasticsearch-curator (4.2.1) over (4.1.2) ...\nrmdir: failed to remove \u2018/opt/elasticsearch-curator\u2019: Directory not empty\n. Now, I can't even purge the package:\naptitude purge elasticsearch-curator -y\nThe following packages will be REMOVED:\n  elasticsearch-curator{p}\n0 packages upgraded, 0 newly installed, 1 to remove and 0 not upgraded.\nNeed to get 0 B of archives. After unpacking 9,438 kB will be freed.\ndpkg: error processing package elasticsearch-curator (--purge):\n package is in a very bad inconsistent state; you should\n reinstall it before attempting a removal\nErrors were encountered while processing:\n elasticsearch-curator\nE: Sub-process /usr/bin/dpkg returned an error code (1)\nA package failed to install.  Trying to recover:\n. ## WORKAROUND:\n1. Force purge: sudo dpkg --purge --force-remove-reinstreq elasticsearch-curator\n2. Reinstall it: sudo aptitude install elasticsearch-curator\nRelated:\nhttps://askubuntu.com/questions/122699/how-to-remove-package-in-bad-state-software-center-freezes-no-synaptic\n. ",
    "fabiendelpierre": "Thanks for getting back to me.\nYou mentioned disabling a safety check -- is that a command-line flag or would that mean modifying the code? Just wanna know what I'm getting into. I'm not a Python dev (or any kind of dev), I can probably figure it out but any leads are appreciated :)\nIs this the safety check you mentioned in the version I'm using?\n. I'll give it a shot then, thanks!\n. Following up: no luck.\n```\n$ tail utils.py\n        if '%d' in timestring:\n            retval = True\n    elif time_unit == 'weeks':\n        if '%W' in timestring:\n            retval = True\n    elif time_unit == 'months':\nif '%m' in timestring:\n    if '%m' in timestring or '%-m' in timestring:\n        retval = True\nreturn retval\n\n$ curl -XPUT localhost:9200/tracking_9_2015\n{\"acknowledged\":true}$\n$ curl -XPUT localhost:9200/tracking_10_2015\n{\"acknowledged\":true}$\n$ /usr/bin/curator --dry-run --loglevel INFO --logformat default  --host localhost --port 9200 close indices --prefix 'tracking_' --time-unit months --older-than 3 --timestring '%m_%Y'\n2016-11-10 21:12:29,766 INFO      Job starting: close indices\n2016-11-10 21:12:29,791 INFO      Action close will be performed on the following indices: [u'tracking_10_2015']\n2016-11-10 21:12:29,791 INFO      DRY RUN MODE.  No changes will be made.\n2016-11-10 21:12:29,792 INFO      DRY RUN: close: tracking_10_2015\n$ /usr/bin/curator --dry-run --loglevel INFO --logformat default  --host localhost --port 9200 close indices --prefix 'tracking_' --time-unit months --older-than 3 --timestring '%-m_%Y'\n2016-11-10 21:12:33,337 INFO      Job starting: close indices\n2016-11-10 21:12:33,361 WARNING   No indices matched provided args: {'regex': None, 'index': (), 'suffix': None, 'newer_than': None, 'closed_only': False, 'prefix': u'tracking_', 'time_unit': 'months', 'timestring': u'%-m_%Y', 'exclude': (), 'older_than': 3, 'all_indices': False}\nNo indices matched provided args: {'regex': None, 'index': (), 'suffix': None, 'newer_than': None, 'closed_only': False, 'prefix': u'tracking_', 'time_unit': 'months', 'timestring': u'%-m_%Y', 'exclude': (), 'older_than': 3, 'all_indices': False}\n```\nGuess it's back to convincing the devs to do it differently \ud83d\ude04 \n. ",
    "FlorinAndrei": "Same error if I install from pip\n. I must say, trying to use this tool is very frustrating. SSL doesn't work well due to a bug. What seems to be valid syntax with --host doesn't work either.\nThe documentation is extremely poor quality - appears written to pass some documentation benchmarks and fill up a predetermined number of pages, but it does nothing for those who are just learning to use the software (but this is an issue with all ElasticSearch documentation). Where are the examples for show indices with the --host option? The HOWTOs on Digital Ocean are far more useful.\n. Downgrading to curator 3.5.1 installed from pip removes the error.\n. 3.5.1 works great and makes more sense. Thanks.\n. ",
    "benlavender": "Just adding some info for users on ES 5.x.x that elasticsearch-curator version 3.5.1 doesn\u2019t support ES 5.x.x. I\u2019m on 5.1.1 and having to use the singleton command section as noted here:\nhttps://www.elastic.co/guide/en/elasticsearch/client/curator/current/singleton-cli.html\n. ",
    "rafaelfc-olx": "thanks for the fast reply!. ",
    "PeterGrace": "Thanks @untergeek, I have not had time to revisit curator to give it a test, but I'm certain your write-up will lead me to the results I desire.. ",
    "clintongormley": "See https://www.elastic.co/guide/en/shield/current/limitations.html#_accessing_the_literal_security_literal_index. ",
    "aakashrs": "\nCan you attach more logs from Curator? The error alone does not paint a complete picture.\n\nUnfortunately that is the only log line I get from curator about the error.\nDetails:\n2016-11-28 10:00:04,021 DEBUG              curator.utils      repository_exists:930  Repository es-snapshots exists.\n2016-11-28 10:00:04,021 DEBUG              curator.utils     parse_date_pattern:1014 Partially rendered name: c\n2016-11-28 10:00:04,021 DEBUG              curator.utils     parse_date_pattern:1014 Partially rendered name: cu\n2016-11-28 10:00:04,021 DEBUG              curator.utils     parse_date_pattern:1014 Partially rendered name: cur\n2016-11-28 10:00:04,021 DEBUG              curator.utils     parse_date_pattern:1014 Partially rendered name: cura\n2016-11-28 10:00:04,021 DEBUG              curator.utils     parse_date_pattern:1014 Partially rendered name: curat\n2016-11-28 10:00:04,021 DEBUG              curator.utils     parse_date_pattern:1014 Partially rendered name: curato\n2016-11-28 10:00:04,021 DEBUG              curator.utils     parse_date_pattern:1014 Partially rendered name: curator\n2016-11-28 10:00:04,021 DEBUG              curator.utils     parse_date_pattern:1014 Partially rendered name: curator-\n2016-11-28 10:00:04,021 DEBUG              curator.utils     parse_date_pattern:1014 Partially rendered name: curator-\n2016-11-28 10:00:04,021 DEBUG              curator.utils     parse_date_pattern:1014 Partially rendered name: curator-2016\n2016-11-28 10:00:04,021 DEBUG              curator.utils     parse_date_pattern:1014 Partially rendered name: curator-2016\n2016-11-28 10:00:04,021 DEBUG              curator.utils     parse_date_pattern:1014 Partially rendered name: curator-201611\n2016-11-28 10:00:04,021 DEBUG              curator.utils     parse_date_pattern:1014 Partially rendered name: curator-201611\n2016-11-28 10:00:04,021 DEBUG              curator.utils     parse_date_pattern:1014 Partially rendered name: curator-20161128\n2016-11-28 10:00:04,021 DEBUG              curator.utils     parse_date_pattern:1014 Partially rendered name: curator-20161128\n2016-11-28 10:00:04,021 DEBUG              curator.utils     parse_date_pattern:1014 Partially rendered name: curator-2016112810\n2016-11-28 10:00:04,022 DEBUG              curator.utils     parse_date_pattern:1014 Partially rendered name: curator-2016112810\n2016-11-28 10:00:04,022 DEBUG              curator.utils     parse_date_pattern:1014 Partially rendered name: curator-201611281000\n2016-11-28 10:00:04,022 DEBUG              curator.utils     parse_date_pattern:1014 Partially rendered name: curator-201611281000\n2016-11-28 10:00:04,022 DEBUG              curator.utils     parse_date_pattern:1014 Partially rendered name: curator-20161128100004\n2016-11-28 10:00:04,022 DEBUG              curator.utils     parse_date_pattern:1016 Fully rendered name: curator-20161128100004\n2016-11-28 10:00:04,023 DEBUG                curator.cli         process_action:98   Doing the action here.\n2016-11-28 10:00:05,274 DEBUG              curator.utils           test_repo_fs:946  All nodes can write to the repository\n2016-11-28 10:00:05,274 DEBUG              curator.utils           test_repo_fs:948  Nodes with verified repository access: {u'ABC': {u'name': u'H'}, u'Pho': {u'name': u'Mary'}, u'9N': {u'name': u'Po'}, u'q4': {u'name': u'Kid'}}\n2016-11-28 10:00:05,277 INFO      curator.actions.snapshot              do_action:834  Creating snapshot \"curator-20161128100004\" from indices: [u'', ..... ]\n2016-11-28 10:01:04,780 ERROR                curator.cli                    cli:193  Failed to complete action: snapshot.  <class 'curator.exceptions.FailedExecution'>: Exception encountered.  Rerun with loglevel DEBUG and/or check Elasticsearch logs for more information. Exception: TransportError(503, u'concurrent_snapshot_execution_exception', u'[es-snapshots:curator-20161128100004] a snapshot is already running')\nTimeout value: 86400\nThe error comes in much sooner.\nHere is my snapshot action file:\nactions:\n  1:\n    action: snapshot\n    options:\n      repository: es-snapshots\n      # Leaving name blank will result in the default 'curator-%Y%m%d%H%M%S'\n      name:\n      ignore_unavailable: False\n      include_global_state: True\n      partial: True\n      wait_for_completion: True\n      skip_repo_fs_check: False\n      timeout_override: 86400\n      continue_if_exception: False\n      disable_action: False\n    filters:\n    - filtertype: pattern\n      kind: prefix\n      value: .marvel-\n      exclude: True\n    - filtertype: closed\n      exclude: True. I can confirm this is an issue with Load Balancer based access.\nI tried the same over port 9200 - directly connecting to the machines (and not over an ELB) and it worked perfectly fine.\nIt may be possible to write a special \"wait_for_completion\" action that will cover cases like this.\nIt will simply check that a snapshot, forceMerge, or whatever task is running, and keep checking\nevery so many seconds until either a timeout is reached, or the action completes successfully.\nSince 5.0 supports this kind of task architecture already, I may put that into Curator 5.\nThis makes sense.\nThis does not block us right now. I have pretty much a similar WAR wherein a cron runs to check and alert in case of failures by directly querying Elasticsearch for the snapshot status.\nHaving this in-built in Curator-5 would be really useful.\nAnd you may chose to skip Curator 4 if the !/$ is not worth it.. Thanks @untergeek for filing #836 . ",
    "aaron-miller": "I believe I am having the exact same problem with my curator jobs as well. It appears that after waiting 10 minutes for a snapshot to complete curator attempts to create the snapshot again and encounters the \"snapshot is already running\" error. I'm not setting the timeout_override at all and just using the default 21600 seconds. \nElasticsearch 2.4.1\nCurator 4.1.2\nMy YAML file:\n```\n\nactions:\n  1:\n    action: open\n    options:\n      ignore_empty_list: True\n    filters:\n      - filtertype: pattern\n        kind: prefix\n        value: prod.app-2016.06.01\n      - filtertype: closed\n        exclude: False\n  2:\n    action: snapshot\n    options:\n      repository: swift\n      name: prod.app-2016.06.01\n      wait_for_completion: True\n      include_global_state: False\n    filters:\n      - filtertype: pattern\n        kind: prefix\n        value: prod.app-2016.06.01\n```\nThe end of the debug log:\n2016-12-01 10:23:02,946 DEBUG              curator.utils     parse_date_pattern:1018 Fully rendered name: prod.app-2016.06.01\n2016-12-01 10:23:02,946 DEBUG                curator.cli         process_action:97   Doing the action here.\n2016-12-01 10:23:03,905 DEBUG              curator.utils           test_repo_fs:948  All nodes can write to the repository\n2016-12-01 10:23:03,905 DEBUG              curator.utils           test_repo_fs:950  Nodes with verified repository access: {u'RiNwi89ZSnCYvirxlvBMGQ': {u'name': u'elasticsearch-data-8'}, u'0ry2LFOhRoeC4_eANXy6Zg': {u'name': u'elasticsearch-data-3'}, u'oor56ftAQvKNBAXMGJ5SyA': {u'name': u'elasticsearch-data-11'}, u'PYQGpl4aTyC3LPj7GIOQ7w': {u'name': u'elasticsearch-master-1'}, u'BHxW7oLhQGSaXwf895eunA': {u'name': u'elasticsearch-data-6'}, u'OdH37hlyT3yBKRc_iczbxg': {u'name': u'elasticsearch-data-13'}, u'_8Ytx0Z1SsmvYaVBz62zyw': {u'name': u'elasticsearch-data-12'}, u'ZBvmCHEGT7eGtMWqW5XoZw': {u'name': u'elasticsearch-master-3'}, u'iRY-pUanSvKq3xUyFZTE8Q': {u'name': u'elasticsearch-data-7'}, u'Ceo8uNaSSCqXoRI9XmpzfA': {u'name': u'elasticsearch-data-5'}, u'8t2qYUGJRg-_JIPPqs12Pw': {u'name': u'elasticsearch-data-15'}, u'rHlcidv6TkusQIwAVhW7Ig': {u'name': u'elasticsearch-data-9'}, u'cxskSeZpSNi1W6lMWrIBxw': {u'name': u'elasticsearch-data-2'}, u'BfGzQlF6SWuOyXhuUEC4ww': {u'name': u'elasticsearch-data-10'}, u'GBmzlZqDQfGlrcrLOYjVWw': {u'name': u'elasticsearch-data-14'}, u'9XzcGcu1RH6fOlCxel5a6A': {u'name': u'elasticsearch-master-2'}, u'OuJSLRzwTZ65cQYxJCJtGQ': {u'name': u'elasticsearch-data-1'}}\n2016-12-01 10:23:03,908 INFO      curator.actions.snapshot              do_action:747  Creating snapshot \"prod.app-2016.06.01\" from indices: [u'prod.app-2016.06.01']\n2016-12-01 10:33:03,941 ERROR                curator.cli                    cli:192  Failed to complete action: snapshot.  <class 'curator.exceptions.FailedExecution'>: Exception encountered.  Rerun with loglevel DEBUG and/or check Elasticsearch logs for more information. Exception: TransportError(503, u'concurrent_snapshot_execution_exception', u'[swift:prod.app-2016.06.01] a snapshot is already running'). ",
    "martbhell": "Oops. I've been using \"https://artifacts.elastic.co/packages/5.x/yum\" to get the packages. Thought that was the latest stable :). ",
    "kmroz0991": "Will this work with ES 5.0.2 because it seems i am also receiving this same error. . calling the tar.gz worked. thanks!. Yes it seems to be working now! WEIRD. ",
    "ksmets": "Thx, you're right, apparently package name changed from python-elasticsearch-curator to elasticsearch-curator. Installing it via apt-get removes python-elasticsearch-curator properly and installs fine.\nClosing this issue.. ",
    "acasademont": "Sorry to chime in here again but I'm facing the same issue...APT reports 4.2.5 installed but \"curator_cli --version\" reports 4.2.3.post1\nAm I missing something?. Nevermind...someone had installed curator via PIP and there were 2 versions.... ",
    "hydrapolic": "Right now,I've just commented out the test.\nThe condition to run the test based on the host:port seems like the best solution for this case.\nAnother approach would be to skip the test with something like:\npython setup.py test --skip-tests=test_no_config. Thanks. @untergeek, this is solely an issue of elasticsearch-py, not elasticsearch itself.. @untergeek, we can, however I tried to set the dependencies as close to yours as possible. That's why we had a dependency on >=elasticsearch-py-5.5.2, but since it fails I'll raise to >=6.1.0 (as 6.0 seems to be banned). Can you please adjust https://github.com/elastic/curator/blob/master/setup.cfg#L24 so that it won't be a surprise for any package maintainer? :). Thank you :). Having curator working with click 6/7 would be greatly appreciated on rolling distros like Gentoo. Currently we have a strict restriction on <click-7. This means that even though all other packages on the system can work with click 7, only curator is blocking the upgrade and forcing 6. If it's too much work I understand, but if curator could be compatible with both, it would be really great.. ",
    "breml": "I hust found out about --ignore_empty_list. Unfortunately the traceback is still printed.\nMy command:\ncurator_cli --host 127.0.0.1 --port 9200 --http_auth admin:admin --use_ssl --ssl-no-validate delete_indices --ignore_empty_list --filter_list '[{\"filtertype\":\"age\",\"source\":\"name\",\"direction\":\"older\",\"unit\":\"days\",\"unit_count\":5,\"timestring\": \"%Y.%m.%d\"},{\"filtertype\":\"pattern\",\"kind\":\"prefix\",\"value\":\"logstash\"}]'\nOutput:\nTraceback (most recent call last):\n  File \"/var/vcap/packages/curator/bin/curator_cli\", line 9, in <module>\n    load_entry_point('elasticsearch-curator==4.2.4', 'console_scripts', 'curator_cli')()\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/curator/curator_cli.py\", line 5, in main\n    cli(obj={})\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/click/core.py\", line 716, in __call__\n    return self.main(*args, **kwargs)\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/click/core.py\", line 696, in main\n    rv = self.invoke(ctx)\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/click/core.py\", line 1060, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/click/core.py\", line 889, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/click/core.py\", line 534, in invoke\n    return callback(*args, **kwargs)\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/click/decorators.py\", line 17, in new_func\n    return f(get_current_context(), *args, **kwargs)\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/curator/singletons.py\", line 268, in delete_indices_singleton\n    ilo.iterate_filters(clean_filters)\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/curator/indexlist.py\", line 846, in iterate_filters\n    method(**f)\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/curator/indexlist.py\", line 380, in filter_by_regex\n    self.empty_list_check()\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/curator/indexlist.py\", line 184, in empty_list_check\n    raise NoIndices('index_list object is empty.')\ncurator.exceptions.NoIndices: index_list object is empty.. If I use curator with action file, it works as expected and also the output is nice (without traceback even if an exception is raised).\nignore_empty_list: False and continue_if_exception: False:\n2016-12-20 13:52:21,317 INFO      Preparing Action ID: 1, \"delete_indices\"\n2016-12-20 13:52:21,328 INFO      Trying Action ID: 1, \"delete_indices\": No description given\n2016-12-20 13:52:21,362 ERROR     Unable to complete action \"delete_indices\".  No actionable items in list: <class 'curator.exceptions.NoIndices'>\nignore_empty_list: True and continue_if_exception: True:\n2016-12-20 13:51:23,894 INFO      Preparing Action ID: 1, \"delete_indices\"\n2016-12-20 13:51:23,906 INFO      Trying Action ID: 1, \"delete_indices\": No description given\n2016-12-20 13:51:23,941 INFO      Skipping action \"delete_indices\" due to empty list: <class 'curator.exceptions.NoIndices'>\n2016-12-20 13:51:23,941 INFO      Action ID: 1, \"delete_indices\" completed.\n2016-12-20 13:51:23,942 INFO      Job completed.. @untergeek thanks for your reply. We provide services based on Cloud Foundry and therefore use BOSH for our deployments. BOSH has the concept of stemcells, which are very rudimentary Linux base images, on which every software is deployed as so called BOSH releases. These releases must bring all their dependecies with them and all the components are usualy compiled from source code (it has some analogies with Gentoo).\nThis is also the reason, why I am not able to use the binary packages (DEB, RPM) you mentioned.\nWith these preconditions I started to put together all the dependencies, as noted on Installation from source. While double-checking the dependencies I found a small bug in the documentation (#850). Because of this I was using version 6.2 of click. So I fixed this, but the problem persists.\nI use the following exact versions:\n\nPython 3.4.3 from https://www.python.org/ftp/python/3.4.3/Python-3.4.3.tgz\ncertifi 2016.09.26 from https://github.com/certifi/python-certifi/archive/2016.09.26.tar.gz\nclick 6.3 from https://pypi.python.org/packages/source/c/click/click-6.3.tar.gz\nelasticsearch-curator 4.2.4 from https://github.com/elastic/curator/archive/v4.2.4.tar.gz\nelasticsearch 2.4.0 from https://github.com/elastic/elasticsearch-py/archive/2.4.0.tar.gz\nPyYAML 3.11 from http://pyyaml.org/download/pyyaml/PyYAML-3.11.tar.gz\nurllib3 1.12 from https://pypi.python.org/packages/source/u/urllib3/urllib3-1.12.tar.gz\nvoluptuous 0.9.3 from https://github.com/alecthomas/voluptuous/archive/0.9.3.tar.gz\n\nAlso double checked:\n```\nls /var/vcap/packages/curator/lib/python3.4/site-packages/ -1\ncertifi\ncertifi-2016.9.26-py3.4.egg-info\nclick\nclick-6.3-py3.4.egg-info\ncurator\nelasticsearch\nelasticsearch-2.4.0-py3.4.egg-info\nelasticsearch_curator-4.2.4-py3.4.egg-info\nPyYAML-3.11-py3.4.egg-info\nurllib3\nurllib3-1.12-py3.4.egg-info\nvoluptuous\nvoluptuous-0.9.3-py3.4.egg-info\nyaml\n_yaml.cpython-34m.so\n```\nSo I started with the examples you provided, with the following results:\ncurator_cli --loglevel INFO --host 127.0.0.1 --port 9200 --http_auth admin:admin --use_ssl --ssl-no-validate delete_indices --filter_list '{\"filtertype\":\"pattern\",\"kind\":\"prefix\",\"value\":\"not_here_dude\"}'\nOutput:\n2016-12-22 11:13:55,886 ERROR     Singleton action failed due to empty index list\nPut query into an array:\ncurator_cli --loglevel INFO --host 127.0.0.1 --port 9200 --http_auth admin:admin --use_ssl --ssl-no-validate delete_indices --filter_list '[{\"filtertype\":\"pattern\",\"kind\":\"prefix\",\"value\":\"not_here_dude\"}]'\nOutput:\n2016-12-22 11:14:54,564 ERROR     Singleton action failed due to empty index list\nAdded second part of query:\ncurator_cli --loglevel INFO --host 127.0.0.1 --port 9200 --http_auth admin:admin --use_ssl --ssl-no-validate delete_indices --filter_list '[{\"filtertype\":\"pattern\",\"kind\":\"prefix\",\"value\":\"not_here_dude\"},{\"filtertype\":\"age\",\"source\":\"creation_date\",\"direction\":\"older\",\"unit\":\"days\",\"unit_count\":5}]'\nOutput:\n2016-12-22 11:17:18,050 ERROR     Singleton action failed due to empty index list\nChange order of filters:\ncurator_cli --loglevel INFO --host 127.0.0.1 --port 9200 --http_auth admin:admin --use_ssl --ssl-no-validate delete_indices --filter_list '[{\"filtertype\":\"age\",\"source\":\"creation_date\",\"direction\":\"older\",\"unit\":\"days\",\"unit_count\":5},{\"filtertype\":\"pattern\",\"kind\":\"prefix\",\"value\":\"not_here_dude\"}]'\nOutput:\nTraceback (most recent call last):\n  File \"/var/vcap/packages/curator/bin/curator_cli\", line 9, in <module>\n    load_entry_point('elasticsearch-curator==4.2.4', 'console_scripts', 'curator_cli')()\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/curator/curator_cli.py\", line 5, in main\n    cli(obj={})\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/click/core.py\", line 716, in __call__\n    return self.main(*args, **kwargs)\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/click/core.py\", line 696, in main\n    rv = self.invoke(ctx)\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/click/core.py\", line 1060, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/click/core.py\", line 889, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/click/core.py\", line 534, in invoke\n    return callback(*args, **kwargs)\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/click/decorators.py\", line 17, in new_func\n    return f(get_current_context(), *args, **kwargs)\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/curator/singletons.py\", line 268, in delete_indices_singleton\n    ilo.iterate_filters(clean_filters)\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/curator/indexlist.py\", line 846, in iterate_filters\n    method(**f)\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/curator/indexlist.py\", line 380, in filter_by_regex\n    self.empty_list_check()\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/curator/indexlist.py\", line 184, in empty_list_check\n    raise NoIndices('index_list object is empty.')\ncurator.exceptions.NoIndices: index_list object is empty.\nHere we go! - looks like it is a problem with the ordering of the filters.\nThe error persists, if I change the age filter to source = name:\ncurator_cli --loglevel INFO --host 127.0.0.1 --port 9200 --http_auth admin:admin --use_ssl --ssl-no-validate delete_indices --filter_list '[{\"filtertype\":\"age\",\"source\":\"name\",\"direction\":\"older\",\"unit\":\"days\",\"unit_count\":5,\"timestring\": \"%Y.%m.%d\"},{\"filtertype\":\"pattern\",\"kind\":\"prefix\",\"value\":\"not_here_dude\"}]'\nOutput:\nTraceback (most recent call last):\n  File \"/var/vcap/packages/curator/bin/curator_cli\", line 9, in <module>\n    load_entry_point('elasticsearch-curator==4.2.4', 'console_scripts', 'curator_cli')()\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/curator/curator_cli.py\", line 5, in main\n    cli(obj={})\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/click/core.py\", line 716, in __call__\n    return self.main(*args, **kwargs)\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/click/core.py\", line 696, in main\n    rv = self.invoke(ctx)\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/click/core.py\", line 1060, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/click/core.py\", line 889, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/click/core.py\", line 534, in invoke\n    return callback(*args, **kwargs)\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/click/decorators.py\", line 17, in new_func\n    return f(get_current_context(), *args, **kwargs)\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/curator/singletons.py\", line 268, in delete_indices_singleton\n    ilo.iterate_filters(clean_filters)\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/curator/indexlist.py\", line 846, in iterate_filters\n    method(**f)\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/curator/indexlist.py\", line 380, in filter_by_regex\n    self.empty_list_check()\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/curator/indexlist.py\", line 184, in empty_list_check\n    raise NoIndices('index_list object is empty.')\ncurator.exceptions.NoIndices: index_list object is empty.\nIt also happens, if I add an none in front of the age filter:\ncurator_cli --loglevel INFO --host 127.0.0.1 --port 9200 --http_auth admin:admin --use_ssl --ssl-no-validate delete_indices --filter_list '[{\"filtertype\":\"none\"},{\"filtertype\":\"age\",\"source\":\"creation_date\",\"direction\":\"older\",\"unit\":\"days\",\"unit_count\":5},{\"filtertype\":\"pattern\",\"kind\":\"prefix\",\"value\":\"not_here_dude\"}]'\nOutput:\nTraceback (most recent call last):\n  File \"/var/vcap/packages/curator/bin/curator_cli\", line 9, in <module>\n    load_entry_point('elasticsearch-curator==4.2.4', 'console_scripts', 'curator_cli')()\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/curator/curator_cli.py\", line 5, in main\n    cli(obj={})\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/click/core.py\", line 716, in __call__\n    return self.main(*args, **kwargs)\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/click/core.py\", line 696, in main\n    rv = self.invoke(ctx)\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/click/core.py\", line 1060, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/click/core.py\", line 889, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/click/core.py\", line 534, in invoke\n    return callback(*args, **kwargs)\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/click/decorators.py\", line 17, in new_func\n    return f(get_current_context(), *args, **kwargs)\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/curator/singletons.py\", line 268, in delete_indices_singleton\n    ilo.iterate_filters(clean_filters)\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/curator/indexlist.py\", line 846, in iterate_filters\n    method(**f)\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/curator/indexlist.py\", line 380, in filter_by_regex\n    self.empty_list_check()\n  File \"/var/vcap/packages/curator/lib/python3.4/site-packages/curator/indexlist.py\", line 184, in empty_list_check\n    raise NoIndices('index_list object is empty.')\ncurator.exceptions.NoIndices: index_list object is empty.\nBut if I remove the age filter again, it works again:\ncurator_cli --loglevel INFO --host 127.0.0.1 --port 9200 --http_auth admin:admin --use_ssl --ssl-no-validate delete_indices --filter_list '[{\"filtertype\":\"none\"},{\"filtertype\":\"pattern\",\"kind\":\"prefix\",\"value\":\"not_here_dude\"}]'\nOutput:\n2016-12-22 11:59:37,742 ERROR     Singleton action failed due to empty index list\nI also changed the none filter to an opened filter with the same results (works without the age filter, does not work with the age filter).\nI hope, this information does help you to localize the problem.. @untergeek thanks for the fix.. I observed this log line during a snapshot restore operation. log output of curator:\n2019-02-13 07:17:38,507 ERROR                curator.cli                    run:184  Failed to complete action: restore.  <class 'curator.exceptions.FailedExecution'>: Exception encountered.  Rerun with loglevel DEBUG and/or check Elasticsearch logs for more information. Exception: Unable to obtain recovery information for specified indices. Error: TransportError(400, 'too_long_frame_exception', 'An HTTP line is larger than 4096 bytes.')\nThe restore operation restores ~280 indices, which is a full restore of a repository.\nAfter this line curator exited, the restore operation in Elasticsearch continued).\n```\ncurator --version\ncurator, version 5.5.1\n```\n@untergeek what else do you need?. @untergeek I just filed #1360 for the issue I had during restore.. ",
    "shortdudey123": "@untergeek how do you propose doing dynamic alias creation without the CLI?  i.e. creating monthly aliases on daily indexes. With Curator 3 you could do a bash script and get date info from bash commands but with the YAML config, its not dynamic.. 4.2.6 is the newest 4.x release per the changelog which does not include a mention of the ES version changes, but i do see it in the code for 4.2.6. ",
    "alexef": "@untergeek I know, I signed, I'm waiting for the confirmation email :). I don't mind, updated.. I'll take care of rebase and  consistency, and update this PR.. Rebased against master, updated Changelog, changed flag name to skip_repo_fs_check. Was not able to test the latest changes.. Updated.. Was stuck with other work, will update it asap.. Rebased against master, added the changelog under 5.3.0.\nI'm a bit confused, there are two Changelog files, one in the root, another one under docs/. Should I update both @untergeek ?. ",
    "reedflinch": "Just wanted to drop a line that I'm also seeing this with Curator 4.2.6 and ES 5.1.2. Thanks for your work on this project!. ",
    "cp2587": "Opened a PR: https://github.com/elastic/curator/pull/862 (i signed the CLA after opening it, will it automatically update the checks ?). OK. Thank you very much for the quick reply. Do you know when you will release curator 5 ?. ",
    "abraxxa": "Our legal department has identified multiple, strong reasons why we can't sign your contributor agreement, like no liability limitations.\nI won't contribute any patches in the future, please don't ask me to send a pull-request as fix for a reported issue.. ",
    "alexisCata": "I found the solution with the filterype: kibana and exclude: True\nNow is working! \nBut anyways @untergeek many thanks for your help\nACTION_FILE.YML\nactions:\n  1:\n    action: delete_indices\n    description: >-\n      Delete indices older than 20 days (based on index creation_date). \n      Ignore the error if the filter does not result in an\n      actionable list of indices (ignore_empty_list) and exit cleanly.\n    options:\n      ignore_empty_list: True\n      timeout_override:\n      continue_if_exception: False\n      disable_action: False\n    filters:\n    - filtertype: age\n      source: creation_date\n      direction: older\n      timestring: '%Y.%m.%d'\n      unit: days\n      unit_count: 20\n      exclude:\n    - filtertype: kibana\n      exclude: True. ",
    "mvisonneau": "Yes that is a pem formated cert that I was referring to with /etc/mycert\nI wanted to use a puppet module that only supports 3.x but I've been able to make it run successfully using 4.1.2. I guess I'm going to write a new module for 4.x\n. Yes I've updated my repository accordingly to use 4.2.5. Working fine now. Also made a puppet module if worth of interest to some people : https://github.com/mvisonneau/puppet-curator. ",
    "Joelp": "I tried to another string format, but same result.\nthis command works OK:\ncurl -XDELETE ':/filebeat-2017.01.17.17'\nsee curator dry-run debug log:\ncurator.log.zip\n. Thanks a lot for reply and exhaustive explanation :)\nSomewhere I read that I can't use H in Logstash. But as I see in example here https://www.elastic.co/guide/en/logstash/5.2/event-dependent-configuration.html , I can.\nSo I think that best way for me is use this index format (in logstash):\nindex => \"filebeat-%{+YYYY.MM.dd.HH}\" instead of\nindex => \"filebeat-%{+YYYY.MM.dd.kk}\"\n. ",
    "willyaranda": "Having the same problem here with a clean instalation of curator + elastic. I'm running it on a data node, but there is literally nothing on ES logs, even on the master node at that time (just run it the script again).. It seems that it just stalls with a tiny index (34MB!)\ncurl -XPOST 'localhost:9200/wormhole_2017-02-03/_forcemerge'\nBut it does work with an empty index\n```\ncurl -XPOST 'localhost:9200/rumor_test/_forcemerge'\n{\"_shards\":{\"total\":84,\"successful\":84,\"failed\":0}}\n``. Usingmax_num_segments` parameter does the same, just stalls again.\nrumor_test use a template which defines 42 shards, and replication of 1, then 84 shards.\nwormhole defines a replication of 1, and 5 shards, so a total of 10. I got 30 primary segments, and 30 replica segments (should be just 5 / 5)\n. Yes, we have a big cluster, a hot-warm architecture with nearly 50 boxes with plenty \nBy stall (sorry for my bad English :)) I mean that there is no output, and it seems that it never ends. I do not expect this to take more than a few seconds, given that the index size where I'm testing it is just 37.5MB and contains only 27.5k documents. We have pretty big indexes, around 2TB of space, that we would like to merge to single segments, that's why we are working on small indexes first.. I don't think that's the issue, because if I only try a single index (even on the kibana console), it does not perform any action that I can see. In Kibana, I don't see the number of segments going down for that index (and this is on warm nodes, only a few searchs, but no index, so no high IO)\nI have send the command for a single index, not for multiple ones.. {\n  \"error\" : {\n    \"root_cause\" : [\n      {\n        \"type\" : \"illegal_argument_exception\",\n        \"reason\" : \"request [/wormhole_2017-02-03/_forcemerge] contains unrecognized parameter: [wait_for_completion]\"\n      }\n    ],\n    \"type\" : \"illegal_argument_exception\",\n    \"reason\" : \"request [/wormhole_2017-02-03/_forcemerge] contains unrecognized parameter: [wait_for_completion]\"\n  },\n  \"status\" : 400\n}\nThat field is not on the documentation.. Thanks! I'm going to close this. Thanks for your support @untergeek . ",
    "IngaFeick": "Same issue here.\nelasticsearch.exceptions.ElasticsearchException: Unable to create client connection to Elasticsearch.  Error: Elasticsearch version 5.2.0 incompatible with this version of Curator (5.2.0)\n$ curator --version\ncurator, version 4.2.5. Probably this line: https://github.com/elastic/curator/blob/master/curator/utils.py#L698. Thank you! @untergeek \n. I can imagine that it is specific to Docker or Nomad rather, yes, as I have not seen this behaviour before either. My Dockerfile is simply this:\n```\nFROM \"artifactory.internal:9090/perf/curator:5.5.4\"\nCOPY \"conf/foo/config.yml\" /curator/config.yml\nCOPY \"conf/foo/actions.yml\" /curator/actions.yml\nCMD [\"--config\", \"/curator/config.yml\", \"/curator/actions.yml\"]\n```\nI have seen that Nomad has restarted the image 2 times, making it 3 runs in total and I was wondering if that messed up the logs but I see the error message 4 times in there, so the numbers don't match (assuming that our Nomad is not missing logs - I'm suspecting something like this).\nI will try that ignore_empty_list option, thank you. Have you considered making this the default behaviour? I think it might be more intuitive. \nThank you lots!\n. I'm closing the ticket because I assume that the repetetiveness of the logs was maybe/probably caused by Nomad, and the flag is going to solve the other issue. Thank you!. ",
    "sherry-ger": "That works! This could be a nice workaround for ES template files too as the functionality was removed in ES 2.x.\nThank you @untergeek !. ",
    "lglavas": "But when asking for snapshots:\nGET _snapshot/_status\nI get no results back:\n{\n  \"snapshots\": []\n}\nI know that if I PUT a snapshot name XXX and then change curator \"ACTION_FILE.YML\" to look for \"repository: XXX\". Then curator will find it. \nBut i whant to be configured from start alredy when ES starts.\nThis is confusing curator want a repo. And in ES we make a snapshot!\nBut if it is a snapshot I need in ES in the docker image. What is the setting/name on the variable I probably put in the elasticsearch.yml??? :1st_place_medal: . Thank you for your reply!\nI am with a docker file making a image \"FROM elasticsearch:5.1.2\". And loading my own elasticsearch.yml: \ncluster.name: \"docker_MPST-cluster\"\nnetwork.host: 0.0.0.0\npath.repo: /usr/share/elasticsearch/data/MPST\nnode.name: \"LTesting\"\n(Yes, the above \"docker_MPST-cluster\" is only a single node cluster.)\nI am aiming on to bring up some containers that are managing network traffic between them. And I want ES to get logs from logstash and I also have containers with grafana and kibana to be able to present graphs/trends/results from ES.\nThe thing is that the containers and systems should be automatically/easy to set up and tear down. For shorter tests and configuration setups. So it is like you say running on one disc and it is yellow state without backup. \nI aiming for curator to delete the oldest data in ES when the one hard drive fills up. And a mandatory field are the \"repository:\" in the \"action: delete_snapshots\". \nSo I have set the path.repo and what API call are you referring to? Maybe something i have overseen?. Yes, thank you. \nBut is it not strange that you can not set a specific repository automatically after you started your docker ES container/image?\nI mean either in the Dockerfile for ES with something like ENTRYPOINT (and a curl command) mekanism (but should instead execute after the docker image are up and running when started not before..)\nBut more naturally a setting in the elasticsearch.yml file. Regardless of filesystem.. Thank you for your help and explanation untergeek!\nYou have helped me to gain insights on ES and curator. Closing this issue. . ",
    "mtunstill": "Just to add some more info, this is the command we're using... (even following the docs where you need the dash)\n/usr/bin/curator --host hostname --http_auth curator:password --port 9200 --use_ssl --certificate /opt/curator/certs/root.cer delete indices --older-than 90 --time-unit days --timestring \\%Y.\\%m.\\%d --prefix .marvel-. Hi @untergeek,\nI wasn't sure if Curator has any other built in functions outside of ES remit to amend data via its APIs. It's a very odd one as we're applying the same command lines against an older version of Curator in a different environment (v3.0.3) and that has no issues as far as I can tell. The main difference is that ES has shield installed where we're experiencing the issue.\nI'll dig deeper into the logs and if it's something I can't figure out with the information you've given me, I'll post them here. The other thing is that the backup directory is an NFS mount hosted on a SAN device.\nI'd of thought Curator would fail if ES was unable to delete the indices (as I've seen before).. ",
    "xamox": "@untergeek Thanks for the quick feedback.  Okay, I'll give it a test run and see how it goes.. ",
    "volcomism": "I run into the following issue when trying to connect to an ES cluster (v5.1.2) hosted by AWS. Running this locally against the same ES version, works fine. Has anyone else been able to get this working?\nVersion 4.2.6\nCommand: \ncurator_cli --logfile ./curator.log --loglevel INFO --logformat default --host aws-elasticsearch-domain --port 80 delete_indices --filter_list '[{\"filtertype\":\"age\",\"source\":\"creation_date\",\"direction\":\"older\",\"unit\":\"days\",\"unit_count\":13},{\"filtertype\":\"pattern\",\"kind\":\"prefix\",\"value\":\"logstash\"}]'\nTraceback:\nTraceback (most recent call last):\n  File \"/usr/local/bin/curator_cli\", line 9, in <module>\n    load_entry_point('elasticsearch-curator==4.2.4', 'console_scripts', 'curator_cli')()\n  File \"/usr/local/lib/python2.7/dist-packages/curator/curator_cli.py\", line 5, in main\n    cli(obj={})\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 722, in __call__\n    return self.main(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 697, in main\n    rv = self.invoke(ctx)\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 1066, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 895, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 535, in invoke\n    return callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/click/decorators.py\", line 17, in new_func\n    return f(get_current_context(), *args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/curator/singletons.py\", line 267, in delete_indices_singleton\n    ilo = IndexList(client)\n  File \"/usr/local/lib/python2.7/dist-packages/curator/indexlist.py\", line 31, in __init__\n    self.__get_indices()\n  File \"/usr/local/lib/python2.7/dist-packages/curator/indexlist.py\", line 71, in __get_indices\n    self._get_metadata()\n  File \"/usr/local/lib/python2.7/dist-packages/curator/indexlist.py\", line 158, in _get_metadata\n    if not 'creation_date' in wl['settings']['index']:\nKeyError: 'settings'. Sorry for the confusion regarding versions, I was running 4.2.6 but had rolled back temporarily to 4.2.4 to troubleshoot and pasted the previous traceback. The traceback below comes from 4.2.6.\nI seem to receive the same traceback when using --loglevel DEBUG.\nTraceback (most recent call last):\n  File \"/usr/local/bin/curator_cli\", line 11, in <module>\n    sys.exit(main())\n  File \"/usr/local/lib/python2.7/dist-packages/curator/curator_cli.py\", line 5, in main\n    cli(obj={})\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 722, in __call__\n    return self.main(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 697, in main\n    rv = self.invoke(ctx)\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 1066, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 895, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 535, in invoke\n    return callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/click/decorators.py\", line 17, in new_func\n    return f(get_current_context(), *args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/curator/singletons.py\", line 266, in delete_indices_singleton\n    ilo = IndexList(client)\n  File \"/usr/local/lib/python2.7/dist-packages/curator/indexlist.py\", line 31, in __init__\n    self.__get_indices()\n  File \"/usr/local/lib/python2.7/dist-packages/curator/indexlist.py\", line 71, in __get_indices\n    self._get_metadata()\n  File \"/usr/local/lib/python2.7/dist-packages/curator/indexlist.py\", line 158, in _get_metadata\n    if not 'creation_date' in wl['settings']['index']:\nKeyError: 'settings'. ",
    "streylight": "@volcomism The issue is that while /_cluster/state is exposed in Amazon ES 5 the/_cluster/state/metadata call returns everything but Settings for indices.  This fails because Curator tries to pull the index settings from the metadata here, which is why you're getting \"KeyError: 'settings'\" error. \nAWS is aware of the issue so hopefully it'll be resolved soon.. @untergeek I'm not aware of any public facing tickets but I work for AWS so I can assure you we're aware of it. If you'd like you can post on the forum and I can reply there for tracking purposes.. @barakalon Looks like the thread got locked.  Delete and resubmit it and I'll follow up with you there.. @barakalon I removed it.  Feel free to recreate it.. Hey @barakalon direct message me (Jeremiah-AWS) on the forum and I'll help you. I don't want to clutter up this issue with AWS problems :). ",
    "barakalon": "Added a ticket for this: https://forums.aws.amazon.com/thread.jspa?threadID=249967&tstart=0. @streylight doesn't look like deleting it is an option \ud83e\udd14 \nI'm not sure why it got locked either.\n. \n\ud83d\ude06 . ",
    "gregsterin": "I just ran into this issue as well, and the AWS /_cluster/state/metadata endpoint still has this issue. \nThis may be naive, but I was able to patch it by getting the data it seems to expect from the 'settings' from /index/_settings. Not sure if this introduces any unintended side effects, as this is my first time looking into this code base, but if it looks OK I'm happy to submit a PR for it.\nIt works for my use case of deleting old indices on AWS ES 5.\nhttps://github.com/gregsterin/curator/commit/800fe19ce9792f5440688f0d41d99d18f78b7516\n. ",
    "ryananguiano": "Even though this isn't merged due to tests, I got it to work for me with the following Dockerfile:\n```\nFROM python:2.7-slim\nADD https://github.com/Talend/curator/archive/638e4789ae02045d8a9153ba920b918fad2d01b5.tar.gz /curator.tgz\nRUN tar zxf /curator.tgz && mv /curator- /curator && pip install /curator/ && rm -rf /curator\n```. ",
    "nfelsen": "AWS ES 5.3 was released today and it's fixing the support for Curator\nhttps://aws.amazon.com/about-aws/whats-new/2017/06/elasticsearch-5-3-now-available-on-amazon-elasticsearch-service/. ",
    "jitran": "i can confirm that curator works with aws elasticsearch 5.3\n```\ncurator_cli --host mydomain --port 80 show_indices --verbose\n.kibana              open   28.0KB      4   1   1 2017-06-02T00:26:34Z\nlogstash-2017.04.27  open  237.0KB    180   5   1 2017-06-02T00:48:46Z\nlogstash-2017.06.01  open    1.1MB   1740   5   1 2017-06-02T00:48:46Z\nlogstash-2017.06.02  open    9.2MB  30906   5   1 2017-06-02T00:48:45Z\ncurator_cli --host mydomain --port 80 delete_indices --filter_list '[{\"filtertype\":\"age\",\"source\":\"creation_date\",\"direction\":\"older\",\"unit\":\"days\",\"unit_count\":0},{\"filtertype\":\"pattern\",\"kind\":\"prefix\",\"value\":\"logstash-\"}]'\n2017-06-02 11:05:43,894 INFO      Deleting selected indices: [u'logstash-2017.04.27', u'logstash-2017.06.01', u'logstash-2017.06.02']\n2017-06-02 11:05:43,894 INFO      ---deleting index logstash-2017.04.27\n2017-06-02 11:05:43,894 INFO      ---deleting index logstash-2017.06.01\n2017-06-02 11:05:43,894 INFO      ---deleting index logstash-2017.06.02\n2017-06-02 11:05:45,407 INFO      Singleton \"delete_indices\" action completed.\n. @untergeek, IAM credentials in my curator.yml file didn't work with curator 5.0.4\nelasticsearch.exceptions.ElasticsearchException: Unable to create client connection to Elasticsearch.  Error: TransportError(403, u'{\"Message\":\"User: anonymous is not authorized to perform: es:ESHttpGet on resource: mydomain\"}')\n```\nI modified my elasticsearch access policy to allow my instance's ip address access and from there I was able to get curator to connect.. ",
    "ralph-tice": "@jimmycuadra you can whitelist your NAT gateway's EIP if your kubernetes nodes are in private subnet\nbut I think the right fix is to use the same sort of credentialsProviderChain that boto implements, example: https://github.com/sam-washington/requests-aws4auth/issues/14#issuecomment-231197951\nI think #914 is the issue you are looking for in the Curator project.. ",
    "hexian55": "curator --dry-run ~/.curator/action.yml. thinks !\nI see. I see \nthank you very much!. ",
    "laraprabhu": "I have to use the port 80 at this context. Using the port 80 would give a similar effect like skipping the port part.. ",
    "jhammerman79": "Awesome, thanks!. ",
    "dreampuf": "@untergeek Thank you explain. I got an error when I trying to delete monthly indices, which don't include the day unit. I will post the traceback late.. @untergeek I think I found the root cause. Some weekly indices are in the ES. When I execute this action, it will throw an error while the curator trying to parse the weekly indices.\nI think I would change the name convention to identify which rule are they use.. ",
    "kylegoch": ":facepalm:\nI didnt even think of permissions. I had only given curator permissions to \"logs-\". Adding a permission for \".monitoring-\" makes them show up now.\nAlso, thanks for the tip on the xpack auto pruning. I didn't know that, we didnt want to keep too much since we use DataDog so we have most stats there, but XPack Monitoring has been a super useful replacement for Kopf for insight into what the shards/etc are up to.\nThanks again!. @untergeek awesome, thanks for that. We are sending the monitoring data from our main cluster to the log cluster, so auto-prune isnt going to work there.. ",
    "surekhabalaji": "Thanks for the response. I ran the command with loglevel DEBUG and the logs are in the attached file.\ncurator_cli_log.txt\n. My ElasticSearch server is running inside a docker container. Will that cause a problem?  From the curator logs, look like the get indices commandis the one failing when the index is empty. When I tried the GET /_all command using sense plugin directly on the ElasticSearch server, I am getting \"index_not_found_exception\" and status 404 when the index is empty. So this error is getting reflected by the curator_cli command.\n. Thanks for the help. So this is not an issue then.. ",
    "psychoche": "You're right. I've just missed it when reproduced it last time. \nActual error is: \n2017-03-15 14:26:07,527 ERROR                curator.cli                    cli:193  Failed to complete action: allocation.  <class 'curator.exceptions.FailedExecution'>: Exception encountered.  Rerun with loglevel DEBUG and/or check Elasticsearch logs for more information. Exception: health() got an unexpected keyword argument 'wait_for_no_relocating_shards'. ",
    "psaiz": "Hi, \nThanks for your reply. The issue is that the one I want to 'unset' is the 'include' (not exclude). If I set  an arbitrary value, it will not do the allocation correctly\nFrom the command line, I can do something like\ncurl -X PUT localhost:9200/my_index/_settings -d  '{\"index.routing.allocation.include.my_key\":\"\"}'\nand that works as expected. \nWould it be possible to drop the restriction that 'value' has to be defined? \nCheers, \npablo\n. ",
    "jhmartin": "https://github.com/Talend/curator/pull/1 appears to be sufficient to get the delete-indices action working, at least to the point it complains about auth for deletion:\n2017-03-29 18:22:28,356 INFO      Deleting selected indices: [u'logstash-2017.03.28', u'logstash-2017.03.29']\n2017-03-29 18:22:28,356 INFO      ---deleting index logstash-2017.03.28\n2017-03-29 18:22:28,356 INFO      ---deleting index logstash-2017.03.29\n2017-03-29 18:22:28,456 ERROR     Failed to complete action: delete_indices.  <class 'curator.exceptions.FailedExecution'>: Exception encountered.  Rerun with loglevel DEBUG and/or check Elasticsearch logs for more information. Exception: TransportError(403, u'{\"Message\":\"User: anonymous is not authorized to perform: es:ESHttpDelete on resource: REDACTED\"}'). Thank you for clarifying that.  I put in a request for AWS to natively support Curator, FWIW.. ",
    "cafuego": "Weird. It's unlikely I have multiple curators installed, this is all running in docker images. With python 2.7 and not 3.x.\nI don't have an OS config for you, but the next best thing: docker images :-)\nThe image that throws the error is unocha/curator:4.2.6-201703-01, the one that works fine is unocha/curator:4.2.6-201704-04.\nNote that in both cases, curator is installed via pip. The reason I mention the packaged one is that I tried using it after I found that the 4.2.6 images did NOT work after I bumped the version.\nThe previous image I had been using (unocha/curator:4.2.4-201612-01) did work fine and I had not modified the Dockerfile at all between those two builds.. Yup, I'm happy to use the work-around via pip.\nI expect then that my problem is that I use the python:2.7 base docker image to build the curator one. That comes with python 2.7 pre-installed and undoubtedly it gets picked as interpreter, meaning it can't load the bundled 3.5 yaml extension and making it barf^W exit ungracefully ;-). I did try installing libyaml with the packaged one but that seemed to not sort it :-/ That's not an image I ended up pushing, though.. ",
    "geekpete": "It would be cool to have the shrink_node picked dynamically by Curator optionally.\nThen for a set of indices that need shrinking, different nodes could be used for each index to be route shards to to spread the impact of clumping shards to one node to the various nodes perhaps. Either by a round robin or a pre-check to see which node has the most available resources to do so or whatever makes sense.\n. Composite agg might solve the accuracy issue here now, since with composite agg it's no longer a top list with a terms agg, it's all terms? But limited to versions supporting composite agg feature.\nYou'd also need to page through the results of a composite agg.\n. The delete+alias option could also be interesting for the reindex action using migration mode, but would have more risks involved compared to shrink where the mapping is guaranteed to be the same.. \nYes new index creation at UTC for daily or whatever period indices.\nSo if you just had a wait for green and created indices sequentially for existing create index functionality, this might mean that some events timestamped for the next period land in the previous period's index if there turns out to be enough delay in the index creation for whatever reason (very large list of indices or loaded cluster making each create a non-neglible delay), a situation you'd want to avoid?\nThe sequential and wait for green option would work if it was used with creating n+1 period indices to ensure index creation was always ahead by one period and the current index receiving ingest is changed by just being flipped to the next index that was already precreated from a previous curator run.\nSo the point around Beats/Logstash and ILM was that indices might roll over more frequently and not just at UTC time, so would dilute the create impact on the cluster more?\nDepending on the difficulty of adding n+1 precreate logic combined with wait for green+sequential functionality to the existing create_index code, there has to be a weighing up of the usefulness and need for this feature vs the amount of time before most deployments are using the newer ILM stuff.\nSupport will exist for the pre-ILM versions for a while so anyone needing this feature due to being on pre-ILM capable stacks could certainly benefit from it.\nThe other side of the coin is how much of an edge case this scenario is and how many users it might affect, I've seen it a handful of times now anecdotally.\nThe ability to write a script to pre-create indices might also be fairly simple for the edge case users to do themselves as well, but it'd be a nice to have feature built into Curator.\n. I was thinking about scenarios where you have something indexing against an alias and are creating indices sequentially before switching the alias and any delay causes that alias to stay on the previous index past the UTC rollover time then you'd have events in the wrong index potentially.\nRollover Index API does both the create new index and move alias steps in one go and these cannot be separated so that wouldn't happen there. Logstash indexing to a dynamic index name with the period in the index name will create a missing index for the current period if UTC rollover has occurred, so wouldn't occur there. This risk would only be for a workflow that has these steps separated, such as a Curator recipe that has index create actions followed by dependent alias switch actions.\nIf you're creating many indices at UTC rollover and are seeing some impact from that, then creating 10k indices concurrently but ahead of time still has the same impact. What matters is mitigating that potential impact (via sequential and wait for green or whatever mechanism), so it's not only creating ahead of time but doing so with a strategy for reduced impact.\nThe purpose of this feature might already be mostly achievable with existing capability.\nIf you're creating 100 indices at UTC roll over time, then you could use index create actions and alias actions after that.\nIndex create already has date math to be able to create tomorrow's indices ahead of time:\nhttps://www.elastic.co/guide/en/elasticsearch/client/curator/current/create_index.html#_date_math\nThe only question is then how do you avoid creating all the indices at once.\nYou could just split/group the index creations into separate curation runs and use the cron RANDOM_DELAY environment variable to add splay or add randomization to your script execution directly with a preceding delay using scripting or just space them out manually to avoid overlapping.\nSo there's a bunch of ways to achieve it and I think you might be right in suggesting that future direction of ILM (Index Lifecycle Management) might largely mitigate this type of impact, but I think the wait for green might be handy addition to create_index perhaps. I guess you'll hit the timeout if you only get to yellow for whatever reason (eg replicas)?\n. ",
    "dworlton": "Looking forward to this. Will close the loop on being able to do a complete roll-over pattern as described here: https://www.elastic.co/blog/managing-time-based-indices-efficiently with Curator only.. Thanks for adding this feature!. This needs to reset all index.routing.allocation options in order to ensure the allocation works properly.. ",
    "DustinChaloupka": "@untergeek Works now on a fresh machine! Thanks!. Upgrading is what we went with! Unsure if you want to keep this open or not so I'll leave that up to you.. ",
    "joskfg": "I did something like you comment. I moved information from the last day daily index to monthly index using a watcher like this:\nPUT _xpack/watcher/watch/my_watcher\n{\n  \"trigger\": {\n    \"schedule\": {\n      \"daily\" : { \"at\" : \"01:00\" }\n    }\n  },\n  \"input\": {\n    \"search\": {\n      \"request\": {\n        \"indices\": [\n          \"<my_index-{now-1d\\/d}>\"\n        ],\n        \"body\": {\n          \"query\": {\n            \"match_all\": {}\n          }\n        }\n      }\n    }\n  },\n  \"actions\": {\n    \"index_payload\": {\n      \"index\": {\n        \"index\": \"<my_index-{now-1d\\/M{YYYY.MM}}>\",\n        \"doc_type\": \"my_type\"\n      }\n    }\n  }\n}\nThis watcher move the data from <my_index-{now-1d\\/d}> (my_index-2017.04.20) to <my_index-{now-1d\\/d}> (my_index-2017.04). \nWith this you have the reindex part (you can add transformation too), but you cannot remove the index from here, so I used the curator_cli to remove it using a cron that runs every day with the curator execition \ncurator_cli --use_ssl --dry-run --host=\"https://my-elasticsearch-host\" delete_indices --ignore_empty_list --filter_list '[{\"filtertype\":\"age\",\"source\":\"creation_date\",\"direction\":\"older\",\"unit\":\"days\",\"unit_count\":2},{\"filtertype\":\"pattern\", \"kind\":\"regex\", \"value\":\"^my_index-[a-z]{2}-\\\\d{4}.\\\\d{2}.\\\\d{2}$$\"}]'`.\nThis will remove all indices older than 2 days that have that pattern. For example two days after the creation of my_index-2017.04.20, this index will be remove. I let it two days just for safety.\nI hope this help you.. Thanks!, You have helped me a lot. I'll try to contact with elastic cloud too. Do you have an opened public issue?. ",
    "masahirotoriumi": "+1. ",
    "jurajseffer": "@zumo64 I've just implemented it in https://github.com/mooprint/curator/commit/e3cd06e07db8bb7185196dc88b5aebaacf798b50?w=1\nWe need this to automate index/alias creation because increasing counter as suffix is not predictable which is a requirement when relying on default rollover behaviour.. @untergeek Will do.. ",
    "davekonopka": "@untergeek Thank you for the quick response and pointers to details. It's clearly not Curator's issue.. ",
    "olalonde": "Ok well, that was a feature request then :) Published 4.3.0 to https://hub.docker.com/r/olalonde/curator/ in the meantime.. ",
    "patrick-vonsteht": "Awesome, I didn't expect this to be fixed so fast :) Thank you very much!. ",
    "rokka": "I have got the same problem. curator and curator_cli are doing nothing with no error message or something else.\n```\n$ cat ~/.curator/curator.yml\n\nlogging:\n  loglevel: DEBUG\n```\n$ curator_cli show_indices --verbose --header\n2017-12-31 00:57:53,309 DEBUG              curator.utils             get_client:803  kwargs = {'url_prefix': '', 'timeout': 30, 'hosts': ['127.0.0.1'], 'ssl_no_validate': False, 'master_only': False, 'port': 9200, 'aws_sign_request': False, 'use_ssl': False, 'certificate': None, 'aws_token': None, 'aws_secret_key': None, 'aws_key': None, 'client_cert': None, 'client_key': None, 'http_auth': None}\n2017-12-31 00:57:53,312 DEBUG              curator.utils             get_client:880  Not using \"requests_aws4auth\" python module to connect.\nAny ideas?\nGreetings\nAndre. ```\n$ cat ~/.curator/curator.yml\n\nlogging:\n  loglevel: DEBUG\n```. Ah I have got the problem:\nhttp_auth is missing\nA little information that auth was failing would be nice ;-)\nThanks for your super fast answer!\n. ",
    "ejoso": "I'm having this same problem but have http_auth configured.  I installed via rpm/yum.\nAny suggestions?  curator/config.yml below:\n```\nRemember, leave a key empty if there is no value.\u00a0 None will be a string,\nnot a Python \"NoneType\"\nclient:\n  hosts:\n    - [\"eth0\", \"local\"]\n  port: 9200\n  url_prefix:\n  use_ssl: True\n  certificate:\n  client_cert:\n  client_key:\n  ssl_no_validate: True\n  http_auth: elastic:\n  timeout: 60\n  master_only: False\nlogging:\n  loglevel: DEBUG #INFO\n  logfile: /var/log/curator/curator.log\n  logformat: default\n  blacklist: ['elasticsearch', 'urllib3']\n```. ",
    "Strijd": "config\nactions:\n  1:\n    action: restore\n    description: restore via curator\n    options:\n      repository: force-merge\n      name: force-merge\n      indices:\n      wait_for_completion: True\n      max_wait: 3600\n      wait_interval: 10\n      ignore_unavailable: False\n      include_global_state: False\n      disable_action: False\n    filters:\n    - filtertype: state\n      state: SUCCESS\n```\n\"shards\":{\"total\":8,\"failed\":0,\"successful\":8}}]}\n2017-05-15 13:40:39,214 DEBUG       curator.snapshotlist        iterate_filters:470  All filters: [{'exclude': False, 'state': 'SUCCESS', 'filtertype': 'state'}]\n2017-05-15 13:40:39,214 DEBUG       curator.snapshotlist        iterate_filters:472  Top of the loop: [u'force-merge']\n2017-05-15 13:40:39,214 DEBUG       curator.snapshotlist        iterate_filters:473  Un-parsed filter args: {'exclude': False, 'state': 'SUCCESS', 'filtertype': 'state'}\n2017-05-15 13:40:39,215 DEBUG     curator.validators.SchemaCheck               init:26   Schema: {'timestring': Any([, , None]), 'max_num_segments': Coerce(int, msg=None), 'exclude': Any([, , , , None]), 'disk_space': , 'unit': Any([, ]), 'aliases': Any([, [], , []]), 'field': Any([, , None]), 'range_to': Coerce(int, msg=None), 'use_age': , 'filtertype': Any([In(['age', 'alias', 'allocated', 'closed', 'count', 'forcemerged', 'kibana', 'none', 'opened', 'pattern',\n 'period', 'space', 'state'])]), 'state': Any([, ]), 'source': Any([, ]), 'epoch': Any([Coerce(int, msg=None), None]), 'unit_count': Coerce(int, msg=None), 'direction': Any([, ]), 'range_from': Coerce(int, msg=None), 'key': Any([, ]), 'week_starts_on': Any([, , None]), 'count': Coerce(int, msg=None), 'kind': Any([, ]), 'stats_result': Any([, , None]), 'reverse': Any([, , , , None]), 'value': Any([, , , , ]), 'allocation_type': Any([, ])}\n2017-05-15 13:40:39,215 DEBUG     curator.validators.SchemaCheck               init:27   \"filter\" config: {'exclude': False, 'state': 'SUCCESS', 'filtertype': 'state'}\n2017-05-15 13:40:39,215 DEBUG       curator.snapshotlist        iterate_filters:479  Parsed filter args: {'exclude': False, 'state': 'SUCCESS', 'filtertype': 'state'}\n2017-05-15 13:40:39,215 DEBUG              curator.utils        iterate_filters:488  Filter args: {'exclude': False, 'state': 'SUCCESS'}\n2017-05-15 13:40:39,215 DEBUG              curator.utils        iterate_filters:489  Pre-instance: [u'force-merge']\n2017-05-15 13:40:39,215 DEBUG       curator.snapshotlist        filter_by_state:319  Filter by state: Snapshot: force-merge\n2017-05-15 13:40:39,215 DEBUG       curator.snapshotlist           actionable:46   Snapshot force-merge is actionable and remains in the list.\n2017-05-15 13:40:39,216 DEBUG              curator.utils        iterate_filters:491  Post-instance: [u'force-merge']\n2017-05-15 13:40:39,216 DEBUG     curator.actions.snapshot               __init:1380 \"most_recent\" snapshot: force-merge\n2017-05-15 13:40:39,216 DEBUG     curator.actions.snapshot               init:1451 REPOSITORY: force-merge\n2017-05-15 13:40:39,216 DEBUG     curator.actions.snapshot               init:1452 WAIT_FOR_COMPLETION: True\n2017-05-15 13:40:39,216 DEBUG     curator.actions.snapshot               init:1454 SKIP_REPO_FS_CHECK: False\n2017-05-15 13:40:39,216 DEBUG     curator.actions.snapshot               init:1455 BODY: {'ignore_unavailable': False, 'partial': False, 'include_aliases': False, 'rename_replacement': '', 'rename_pattern': '', 'indices': [u'index_v0.2_2017-05-04_fa837b4253af4d48b21e9e0a83041cd1'], 'include_global_state': False}\n2017-05-15 13:40:39,216 DEBUG                curator.cli         process_action:97   Doing the action here.\n2017-05-15 13:40:39,216 DEBUG         urllib3.util.retry               from_int:200  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)\n2017-05-15 13:40:40,282 DEBUG     urllib3.connectionpool          _make_request:395  http://x.x.x.x.7:9200 \"POST /_snapshot/force-merge/_verify HTTP/1.1\" 200 66\n2017-05-15 13:40:40,283 INFO               elasticsearch    log_request_success:82   POST http://x.x.x.x.7:9200/_snapshot/force-merge/_verify [status:200 request:1.067s]\n2017-05-15 13:40:40,283 DEBUG              elasticsearch    log_request_success:84   > None\n2017-05-15 13:40:40,283 DEBUG              elasticsearch    log_request_success:85   < {\"nodes\":{\"K4xQPaOFSWSPLwhb0P47aQ\":{\"name\":\"staging-es5-forcem\"}}}\n2017-05-15 13:40:40,283 DEBUG              curator.utils           test_repo_fs:1128 All nodes can write to the repository\n2017-05-15 13:40:40,283 DEBUG              curator.utils           test_repo_fs:1130 Nodes with verified repository access: {u'K4xQPaOFSWSPLwhb0P47aQ': {u'name': u'staging-es5-forcem'}}\n2017-05-15 13:40:40,283 DEBUG         urllib3.util.retry               from_int:200  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)\n2017-05-15 13:40:40,284 DEBUG     urllib3.connectionpool          _make_request:395  http://x.x.x.x:9200 \"GET /_snapshot/_status HTTP/1.1\" 200 16\n2017-05-15 13:40:40,284 INFO               elasticsearch    log_request_success:82   GET http://x.x.xx:9200/_snapshot/_status [status:200 request:0.001s]\n2017-05-15 13:40:40,284 DEBUG              elasticsearch    log_request_success:84   > None\n2017-05-15 13:40:40,284 DEBUG              elasticsearch    log_request_success:85   < {\"snapshots\":[]}\n2017-05-15 13:40:40,284 INFO      curator.actions.snapshot              do_action:1539 Restoring indices \"[u'index_v0.2_2017-05-04_fa837b4253af4d48b21e9e0a83041cd1']\" from snapshot: force-merge\n2017-05-15 13:40:40,285 DEBUG         urllib3.util.retry               from_int:200  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)\n2017-05-15 13:40:40,472 DEBUG     urllib3.connectionpool          _make_request:395  http://x.x.x.x.7:9200 \"POST /_snapshot/force-merge/force-merge/_restore?wait_for_completion=false HTTP/1.1\" 200 17\n2017-05-15 13:40:40,473 INFO               elasticsearch    log_request_success:82   POST http://x.x.x.x.7:9200/_snapshot/force-merge/force-merge/_restore?wait_for_completion=false [status:200 request:0.188s]\n2017-05-15 13:40:40,473 DEBUG              elasticsearch    log_request_success:84   > {\"ignore_unavailable\": false, \"partial\": false, \"include_aliases\": false, \"rename_replacement\": \"\", \"rename_pattern\": \"\", \"indices\": [\"index_v0.2_2017-05-04_fa837b4253af4d48b21e9e0a83041cd1\"], \"include_global_state\": false}\n2017-05-15 13:40:40,473 DEBUG              elasticsearch    log_request_success:85   < {\"accepted\":true}\n2017-05-15 13:40:40,473 DEBUG              curator.utils            wait_for_it:1570 Elapsed time: 0 seconds\n2017-05-15 13:40:40,473 DEBUG         urllib3.util.retry               from_int:200  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)\n2017-05-15 13:40:40,475 DEBUG     urllib3.connectionpool          _make_request:395  http://x.x.x.x.7:9200 \"GET /index_v0.2_2017-05-04_fa837b4253af4d48b21e9e0a83041cd1/_recovery?human=true HTTP/1.1\" 200 2\n2017-05-15 13:40:40,475 INFO               elasticsearch    log_request_success:82   GET http://x.x.x.x.7:9200/index_v0.2_2017-05-04_fa837b4253af4d48b21e9e0a83041cd1/_recovery?human=true [status:200 request:0.002s]\n2017-05-15 13:40:40,475 DEBUG              elasticsearch    log_request_success:84   > None\n2017-05-15 13:40:40,475 DEBUG              elasticsearch    log_request_success:85   < {}\n2017-05-15 13:40:40,476 ERROR                curator.cli                    cli:194  Failed to complete action: restore.  : Exception encountered.  Rerun with loglevel DEBUG and/or check Elasticsearch logs for more information. Exception: u'index_v0.2_2017-05-04_fa837b4253af4d48b21e9e0a83041cd1'\nubuntu@ip-x.x.x.x-203:~/curator$\nubuntu@ip-x.x.x.x-203:~/curator$\n```\n. closed by mistake . elasticsearch=5.1.1.1\ncurator=5.0.2\nindex_size=4.16GB\noutput:\n{\n   \"snapshots\": [\n      {\n         \"snapshot\": \"force-merge\",\n         \"uuid\": \"UiI-Kq8uTAmwNTTbVoW4mw\",\n         \"version_id\": 5010199,\n         \"version\": \"5.1.1\",\n         \"indices\": [\n            \"index.x.x_fa837b4253af4d48b21e9e0a83041cd1\"\n         ],\n         \"state\": \"SUCCESS\",\n         \"start_time\": \"2017-05-15T11:32:13.549Z\",\n         \"start_time_in_millis\": 1494847933549,\n         \"end_time\": \"2017-05-15T11:36:38.894Z\",\n         \"end_time_in_millis\": 1494848198894,\n         \"duration_in_millis\": 265345,\n         \"failures\": [],\n         \"shards\": {\n            \"total\": 8,\n            \"failed\": 0,\n            \"successful\": 8\n         }\n      }\n   ]\n}\n. working  with latest version \nThanks . It will take a while till the restore is done \nproviding some some info for a start\n```\n2017-05-16 11:55:37,944 INFO               curator.utils          restore_check:1448 Index \"my_indexv0.2_2017-02-12_536a9247f9fa4fc7a7942ad46ea14e0d\" is still in stage \"INDEX\"\n2017-05-16 11:55:37,944 DEBUG              curator.utils            wait_for_it:1578 Response: False\n2017-05-16 11:55:37,944 DEBUG              curator.utils            wait_for_it:1598 Action \"restore\" not yet complete, 10 total seconds elapsed. Waiting 10 seconds before checking again.\n2017-05-16 11:55:47,955 DEBUG              curator.utils            wait_for_it:1575 Elapsed time: 60 seconds\n2017-05-16 11:55:47,955 DEBUG         urllib3.util.retry               from_int:200  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)\n2017-05-16 11:55:47,957 DEBUG     urllib3.connectionpool          _make_request:395  http://30.0.0.7:9200 \"GET /my_indexv0.2_2017-02-12_536a9247f9fa4fc7a7942ad46ea14e0d/_recovery?human=true HTTP/1.1\" 200 4482\n2017-05-16 11:55:47,957 INFO               elasticsearch    log_request_success:82   GET http://30.0.0.7:9200/my_indexv0.2_2017-02-12_536a9247f9fa4fc7a7942ad46ea14e0d/_recovery?human=true [status:200 request:0.002s]\n2017-05-16 11:55:47,957 DEBUG              elasticsearch    log_request_success:84   > None\n2017-05-16 11:55:47,958 DEBUG              elasticsearch    log_request_success:85   < {\"my_indexv0.2_2017-02-12_536a9247f9fa4fc7a7942ad46ea14e0d\":{\"shards\":[{\"id\":3,\"type\":\"SNAPSHOT\",\"stage\":\"INDEX\",\"primary\":true,\"start_time\":\"2017-05-16T11:54:48.166Z\",\"start_time_in_millis\":1494935688166,\"total_time\":\"1m\",\"total_time_in_millis\":60039,\"source\":{\"repository\":\"force-merge\",\"snapshot\":\"force-merge\",\"version\":\"5.1.1\",\"index\":\"my_indexv0.2_2017-02-12_536a9247f9fa4fc7a7942ad46ea14e0d\"},\"target\":{\"id\":\"K4xQPaOFSWSPLwhb0P47aQ\",\"host\":\"30.0.0.7\",\"transport_address\":\"30.0.0.7:9300\",\"ip\":\"30.0.0.7\",\"name\":\"staging-es5-forcem\"},\"index\":{\"size\":{\"total\":\"2.9gb\",\"total_in_bytes\":3156050994,\"reused\":\"0b\",\"reused_in_bytes\":0,\"recovered\":\"297mb\",\"recovered_in_bytes\":311480965,\"percent\":\"9.9%\"},\"files\":{\"total\":15,\"reused\":0,\"recovered\":0,\"percent\":\"0.0%\"},\"total_time\":\"1m\",\"total_time_in_millis\":60033,\"source_throttle_time\":\"-1\",\"source_throttle_time_in_millis\":0,\"target_throttle_time\":\"-1\",\"target_throttle_time_in_millis\":0},\"translog\":{\"recovered\":0,\"total\":0,\"percent\":\"100.0%\",\"total_on_start\":0,\"total_time\":\"0s\",\"total_time_in_millis\":0},\"verify_index\":{\"check_index_time\":\"0s\",\"check_index_time_in_millis\":0,\"total_time\":\"0s\",\"total_time_in_millis\":0}},{\"id\":2,\"type\":\"SNAPSHOT\",\"stage\":\"INDEX\",\"primary\":true,\"start_time\":\"2017-05-16T11:54:48.173Z\",\"start_time_in_millis\":1494935688173,\"total_time\":\"1m\",\"total_time_in_millis\":60033,\"source\":{\"repository\":\"force-merge\",\"snapshot\":\"force-merge\",\"version\":\"5.1.1\",\"index\":\"my_indexv0.2_2017-02-12_536a9247f9fa4fc7a7942ad46ea14e0d\"},\"target\":{\"id\":\"K4xQPaOFSWSPLwhb0P47aQ\",\"host\":\"30.0.0.7\",\"transport_address\":\"30.0.0.7:9300\",\"ip\":\"30.0.0.7\",\"name\":\"staging-es5-forcem\"},\"index\":{\"size\":{\"total\":\"2.9gb\",\"total_in_bytes\":3148003580,\"reused\":\"0b\",\"reused_in_bytes\":0,\"recovered\":\"297.6mb\",\"recovered_in_bytes\":312104422,\"percent\":\"9.9%\"},\"files\":{\"total\":15,\"reused\":0,\"recovered\":2,\"percent\":\"13.3%\"},\"total_time\":\"1m\",\"total_time_in_millis\":60023,\"source_throttle_time\":\"-1\",\"source_throttle_time_in_millis\":0,\"target_throttle_time\":\"-1\",\"target_throttle_time_in_millis\":0},\"translog\":{\"recovered\":0,\"total\":0,\"percent\":\"100.0%\",\"total_on_start\":0,\"total_time\":\"0s\",\"total_time_in_millis\":0},\"verify_index\":{\"check_index_time\":\"0s\",\"check_index_time_in_millis\":0,\"total_time\":\"0s\",\"total_time_in_millis\":0}},{\"id\":1,\"type\":\"SNAPSHOT\",\"stage\":\"INDEX\",\"primary\":true,\"start_time\":\"2017-05-16T11:54:48.183Z\",\"start_time_in_millis\":1494935688183,\"total_time\":\"1m\",\"total_time_in_millis\":60023,\"source\":{\"repository\":\"force-merge\",\"snapshot\":\"force-merge\",\"version\":\"5.1.1\",\"index\":\"my_indexv0.2_2017-02-12_536a9247f9fa4fc7a7942ad46ea14e0d\"},\"target\":{\"id\":\"K4xQPaOFSWSPLwhb0P47aQ\",\"host\":\"30.0.0.7\",\"transport_address\":\"30.0.0.7:9300\",\"ip\":\"30.0.0.7\",\"name\":\"staging-es5-forcem\"},\"index\":{\"size\":{\"total\":\"2.9gb\",\"total_in_bytes\":3171596177,\"reused\":\"0b\",\"reused_in_bytes\":0,\"recovered\":\"299.2mb\",\"recovered_in_bytes\":313828089,\"percent\":\"9.9%\"},\"files\":{\"total\":15,\"reused\":0,\"recovered\":1,\"percent\":\"6.7%\"},\"total_time\":\"1m\",\"total_time_in_millis\":60014,\"source_throttle_time\":\"-1\",\"source_throttle_time_in_millis\":0,\"target_throttle_time\":\"-1\",\"target_throttle_time_in_millis\":0},\"translog\":{\"recovered\":0,\"total\":0,\"percent\":\"100.0%\",\"total_on_start\":0,\"total_time\":\"0s\",\"total_time_in_millis\":0},\"verify_index\":{\"check_index_time\":\"0s\",\"check_index_time_in_millis\":0,\"total_time\":\"0s\",\"total_time_in_millis\":0}},{\"id\":0,\"type\":\"SNAPSHOT\",\"stage\":\"INDEX\",\"primary\":true,\"start_time\":\"2017-05-16T11:54:48.191Z\",\"start_time_in_millis\":1494935688191,\"total_time\":\"1m\",\"total_time_in_millis\":60014,\"source\":{\"repository\":\"force-merge\",\"snapshot\":\"force-merge\",\"version\":\"5.1.1\",\"index\":\"my_indexv0.2_2017-02-12_536a9247f9fa4fc7a7942ad46ea14e0d\"},\"target\":{\"id\":\"K4xQPaOFSWSPLwhb0P47aQ\",\"host\":\"30.0.0.7\",\"transport_address\":\"30.0.0.7:9300\",\"ip\":\"30.0.0.7\",\"name\":\"staging-es5-forcem\"},\"index\":{\"size\":{\"total\":\"2.9gb\",\"total_in_bytes\":3153750393,\"reused\":\"0b\",\"reused_in_bytes\":0,\"recovered\":\"298.6mb\",\"recovered_in_bytes\":313166832,\"percent\":\"9.9%\"},\"files\":{\"total\":15,\"reused\":0,\"recovered\":1,\"percent\":\"6.7%\"},\"total_time\":\"1m\",\"total_time_in_millis\":60009,\"source_throttle_time\":\"-1\",\"source_throttle_time_in_millis\":0,\"target_throttle_time\":\"-1\",\"target_throttle_time_in_millis\":0},\"translog\":{\"recovered\":0,\"total\":0,\"percent\":\"100.0%\",\"total_on_start\":0,\"total_time\":\"0s\",\"total_time_in_millis\":0},\"verify_index\":{\"check_index_time\":\"0s\",\"check_index_time_in_millis\":0,\"total_time\":\"0s\",\"total_time_in_millis\":0}}]}}\n. Same log loop\n11:54:48.191Z\",\"start_time_in_millis\":1494935688191,\"stop_time\":\"2017-05-16T12:04:51.366Z\",\"stop_time_in_millis\":1494936291366,\"total_time\":\"10m\",\"total_time_in_millis\":603174,\"source\":{\"repository\":\"force-merge\",\"snapshot\":\"force-merge\",\"version\":\"5.1.1\",\"index\":\"indexv0.2_2017-02-12_536a9247f9fa4fc7a7942ad46ea14e0d\"},\"target\":{\"id\":\"K4xQPaOFSWSPLwhb0P47aQ\",\"host\":\"x.x.x.7\",\"transport_address\":\"x.x.x.7:9300\",\"ip\":\"x.x.x.7\",\"name\":\"staging-es5-forcem\"},\"index\":{\"size\":{\"total\":\"2.9gb\",\"total_in_bytes\":3153750393,\"reused\":\"0b\",\"reused_in_bytes\":0,\"recovered\":\"2.9gb\",\"recovered_in_bytes\":3153750393,\"percent\":\"100.0%\"},\"files\":{\"total\":15,\"reused\":0,\"recovered\":15,\"percent\":\"100.0%\"},\"total_time\":\"10m\",\"total_time_in_millis\":602897,\"source_throttle_time\":\"-1\",\"source_throttle_time_in_millis\":0,\"target_throttle_time\":\"-1\",\"target_throttle_time_in_millis\":0},\"translog\":{\"recovered\":0,\"total\":0,\"percent\":\"100.0%\",\"total_on_start\":0,\"total_time\":\"271ms\",\"total_time_in_millis\":271},\"verify_index\":{\"check_index_time\":\"0s\",\"check_index_time_in_millis\":0,\"total_time\":\"0s\",\"total_time_in_millis\":0}}]}}\n2017-05-16 12:17:29,958 INFO               curator.utils          restore_check:1448 Index \"indexv0.2_2017-02-12_536a9247f9fa4fc7a7942ad46ea14e0d\" is still in stage \"DONE\"\n2017-05-16 12:17:29,958 DEBUG              curator.utils            wait_for_it:1578 Response: False\n2017-05-16 12:17:29,958 DEBUG              curator.utils            wait_for_it:1598 Action \"restore\" not yet complete, 10 total seconds elapsed. Waiting 10 seconds before checking again.\n^[[A^[[A^[[A^[[A^[[A2017-05-16 12:17:39,968 DEBUG              curator.utils            wait_for_it:1575 Elapsed time: 1372 seconds\n2017-05-16 12:17:39,969 DEBUG         urllib3.util.retry               from_int:200  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)\n2017-05-16 12:17:39,970 DEBUG     urllib3.connectionpool          _make_request:395  http://x.x.x.7:9200 \"GET /indexv0.2_2017-02-12_536a9247f9fa4fc7a7942ad46ea14e0d/_recovery?human=true HTTP/1.1\" 200 9593\n2017-05-16 12:17:39,971 INFO               elasticsearch    log_request_success:82   GET http://x.x.x.7:9200/indexv0.2_2017-02-12_536a9247f9fa4fc7a7942ad46ea14e0d/_recovery?human=true [status:200 request:0.002s]\n2017-05-16 12:17:39,971 DEBUG              elasticsearch    log_request_success:84   > None\n2017-05-16 12:17:39,971 DEBUG              elasticsearch    log_request_success:85   < {\"indexv0.2_2017-02-12_536a9247f9fa4fc7a7942ad46ea14e0d\":{\"shards\":[{\"id\":1,\"type\":\"SNAPSHOT\",\"stage\":\"DONE\",\"primary\":true,\"start_time\":\"2017-05-16T11:54:48.183Z\",\"start_time_in_millis\":1494935688183,\"stop_time\":\"2017-05-16T12:04:54.815Z\",\"stop_time_in_millis\":1494936294815,\"total_time\":\"10.1m\",\"total_time_in_millis\":606631,\"source\":{\"repository\":\"force-merge\",\"snapshot\":\"force-merge\",\"version\":\"5.1.1\",\"index\":\"indexv0.2_2017-02-12_536a9247f9fa4fc7a7942ad46ea14e0d\"},\"target\":{\"id\":\"K4xQPaOFSWSPLwhb0P47aQ\",\"host\":\"x.x.x.7\",\"transport_address\":\"x.x.x.7:9300\",\"ip\":\"x.x.x.7\",\"name\":\"staging-es5-forcem\"},\"index\":{\"size\":{\"total\":\"2.9gb\",\"total_in_bytes\":3171596177,\"reused\":\"0b\",\"reused_in_bytes\":0,\"recovered\":\"2.9gb\",\"recovered_in_bytes\":3171596177,\"percent\":\"100.0%\"},\"files\":{\"total\":15,\"reused\":0,\"recovered\":15,\"percent\":\"100.0%\"},\"total_time\":\"10.1m\",\"total_time_in_millis\":606577,\"source_throttle_time\":\"-1\",\"source_throttle_time_in_millis\":0,\"target_throttle_time\":\"-1\",\"target_throttle_time_in_millis\":0},\"translog\":{\"recovered\":0,\"total\":0,\"percent\":\"100.0%\",\"total_on_start\":0,\"total_time\":\"45ms\",\"total_time_in_millis\":45},\"verify_index\":{\"check_index_time\":\"0s\",\"check_index_time_in_millis\":0,\"total_time\":\"0s\",\"total_time_in_millis\":0}},{\"id\":5,\"type\":\"SNAPSHOT\",\"stage\":\"DONE\",\"primary\":true,\"start_time\":\"2017-05-16T12:04:51.606Z\",\"start_time_in_millis\":1494936291606,\"stop_time\":\"2017-05-16T12:14:54.305Z\",\"stop_time_in_millis\":1494936894305,\"total_time\":\"10m\",\"total_time_in_millis\":602698,\"source\":{\"repository\":\"force-merge\",\"snapshot\":\"force-merge\",\"version\":\"5.1.1\",\"index\":\"indexv0.2_2017-02-12_536a9247f9fa4fc7a7942ad46ea14e0d\"},\"target\":{\"id\":\"K4xQPaOFSWSPLwhb0P47aQ\",\"host\":\"x.x.x.7\",\"transport_address\":\"x.x.x.7:9300\",\"ip\":\"x.x.x.7\",\"name\":\"staging-es5-forcem\"},\"index\":{\"size\":{\"total\":\"2.9gb\",\"total_in_bytes\":3162299781,\"reused\":\"0b\",\"reused_in_bytes\":0,\"recovered\":\"2.9gb\",\"recovered_in_bytes\":3162299781,\"percent\":\"100.0%\"},\"files\":{\"total\":15,\"reused\":0,\"recovered\":15,\"percent\":\"100.0%\"},\"total_time\":\"10m\",\"total_time_in_millis\":602302,\"source_throttle_time\":\"-1\",\"source_throttle_time_in_millis\":0,\"target_throttle_time\":\"-1\",\"target_throttle_time_in_millis\":0},\"translog\":{\"recovered\":0,\"total\":0,\"percent\":\"100.0%\",\"total_on_start\":0,\"total_time\":\"389ms\",\"total_time_in_millis\":389},\"verify_index\":{\"check_index_time\":\"0s\",\"check_index_time_in_millis\":0,\"total_time\":\"0s\",\"total_time_in_millis\":0}},{\"id\":3,\"type\":\"SNAPSHOT\",\"stage\":\"DONE\",\"primary\":true,\"start_time\":\"2017-05-16T11:54:48.166Z\",\"start_time_in_millis\":1494935688166,\"stop_time\":\"2017-05-16T12:04:54.904Z\",\"stop_time_in_millis\":1494936294904,\"total_time\":\"10.1m\",\"total_time_in_millis\":606737,\"source\":{\"repository\":\"force-merge\",\"snapshot\":\"force-merge\",\"version\":\"5.1.1\",\"index\":\"indexv0.2_2017-02-12_536a9247f9fa4fc7a7942ad46ea14e0d\"},\"target\":{\"id\":\"K4xQPaOFSWSPLwhb0P47aQ\",\"host\":\"x.x.x.7\",\"transport_address\":\"x.x.x.7:9300\",\"ip\":\"x.x.x.7\",\"name\":\"staging-es5-forcem\"},\"index\":{\"size\":{\"total\":\"2.9gb\",\"total_in_bytes\":3156050994,\"reused\":\"0b\",\"reused_in_bytes\":0,\"recovered\":\"2.9gb\",\"recovered_in_bytes\":3156050994,\"percent\":\"100.0%\"},\"files\":{\"total\":15,\"reused\":0,\"recovered\":15,\"percent\":\"100.0%\"},\"total_time\":\"10.1m\",\"total_time_in_millis\":606692,\"source_throttle_time\":\"-1\",\"source_throttle_time_in_millis\":0,\"target_throttle_time\":\"-1\",\"target_throttle_time_in_millis\":0},\"translog\":{\"recovered\":0,\"total\":0,\"percent\":\"100.0%\",\"total_on_start\":0,\"total_time\":\"38ms\",\"total_time_in_millis\":38},\"verify_index\":{\"check_index_time\":\"0s\",\"check_index_time_in_millis\":0,\"total_time\":\"0s\",\"total_time_in_millis\":0}},{\"id\":4,\"type\":\"SNAPSHOT\",\"stage\":\"DONE\",\"primary\":true,\"start_time\":\"2017-05-16T12:04:51.369Z\",\"start_time_in_millis\":1494936291369,\"stop_time\":\"2017-05-16T12:14:53.944Z\",\"stop_time_in_millis\":1494936893944,\"total_time\":\"10m\",\"total_time_in_millis\":602575,\"source\":{\"repository\":\"force-merge\",\"snapshot\":\"force-merge\",\"version\":\"5.1.1\",\"index\":\"indexv0.2_2017-02-12_536a9247f9fa4fc7a7942ad46ea14e0d\"},\"target\":{\"id\":\"K4xQPaOFSWSPLwhb0P47aQ\",\"host\":\"x.x.x.7\",\"transport_address\":\"x.x.x.7:9300\",\"ip\":\"x.x.x.7\",\"name\":\"staging-es5-forcem\"},\"index\":{\"size\":{\"total\":\"2.9gb\",\"total_in_bytes\":3153017440,\"reused\":\"0b\",\"reused_in_bytes\":0,\"recovered\":\"2.9gb\",\"recovered_in_bytes\":3153017440,\"percent\":\"100.0%\"},\"files\":{\"total\":15,\"reused\":0,\"recovered\":15,\"percent\":\"100.0%\"},\"total_time\":\"10m\",\"total_time_in_millis\":602010,\"source_throttle_time\":\"-1\",\"source_throttle_time_in_millis\":0,\"target_throttle_time\":\"-1\",\"target_throttle_time_in_millis\":0},\"translog\":{\"recovered\":0,\"total\":0,\"percent\":\"100.0%\",\"total_on_start\":0,\"total_time\":\"558ms\",\"total_time_in_millis\":558},\"verify_index\":{\"check_index_time\":\"0s\",\"check_index_time_in_millis\":0,\"total_time\":\"0s\",\"total_time_in_millis\":0}},{\"id\":6,\"type\":\"SNAPSHOT\",\"stage\":\"DONE\",\"primary\":true,\"start_time\":\"2017-05-16T12:04:54.817Z\",\"start_time_in_millis\":1494936294817,\"stop_time\":\"2017-05-16T12:14:55.764Z\",\"stop_time_in_millis\":1494936895764,\"total_time\":\"10m\",\"total_time_in_millis\":600946,\"source\":{\"repository\":\"force-merge\",\"snapshot\":\"force-merge\",\"version\":\"5.1.1\",\"index\":\"indexv0.2_2017-02-12_536a9247f9fa4fc7a7942ad46ea14e0d\"},\"target\":{\"id\":\"K4xQPaOFSWSPLwhb0P47aQ\",\"host\":\"x.x.x.7\",\"transport_address\":\"x.x.x.7:9300\",\"ip\":\"x.x.x.7\",\"name\":\"staging-es5-forcem\"},\"index\":{\"size\":{\"total\":\"2.9gb\",\"total_in_bytes\":3153347402,\"reused\":\"0b\",\"reused_in_bytes\":0,\"recovered\":\"2.9gb\",\"recovered_in_bytes\":3153347402,\"percent\":\"100.0%\"},\"files\":{\"total\":15,\"reused\":0,\"recovered\":15,\"percent\":\"100.0%\"},\"total_time\":\"10m\",\"total_time_in_millis\":600492,\"source_throttle_time\":\"-1\",\"source_throttle_time_in_millis\":0,\"target_throttle_time\":\"-1\",\"target_throttle_time_in_millis\":0},\"translog\":{\"recovered\":0,\"total\":0,\"percent\":\"100.0%\",\"total_on_start\":0,\"total_time\":\"445ms\",\"total_time_in_millis\":445},\"verify_index\":{\"check_index_time\":\"0s\",\"check_index_time_in_millis\":0,\"total_time\":\"0s\",\"total_time_in_millis\":0}},{\"id\":2,\"type\":\"SNAPSHOT\",\"stage\":\"DONE\",\"primary\":true,\"start_time\":\"2017-05-16T11:54:48.173Z\",\"start_time_in_millis\":1494935688173,\"stop_time\":\"2017-05-16T12:04:51.602Z\",\"stop_time_in_millis\":1494936291602,\"total_time\":\"10m\",\"total_time_in_millis\":603429,\"source\":{\"repository\":\"force-merge\",\"snapshot\":\"force-merge\",\"version\":\"5.1.1\",\"index\":\"indexv0.2_2017-02-12_536a9247f9fa4fc7a7942ad46ea14e0d\"},\"target\":{\"id\":\"K4xQPaOFSWSPLwhb0P47aQ\",\"host\":\"x.x.x.7\",\"transport_address\":\"x.x.x.7:9300\",\"ip\":\"x.x.x.7\",\"name\":\"staging-es5-forcem\"},\"index\":{\"size\":{\"total\":\"2.9gb\",\"total_in_bytes\":3148003580,\"reused\":\"0b\",\"reused_in_bytes\":0,\"recovered\":\"2.9gb\",\"recovered_in_bytes\":3148003580,\"percent\":\"100.0%\"},\"files\":{\"total\":15,\"reused\":0,\"recovered\":15,\"percent\":\"100.0%\"},\"total_time\":\"10m\",\"total_time_in_millis\":603194,\"source_throttle_time\":\"-1\",\"source_throttle_time_in_millis\":0,\"target_throttle_time\":\"-1\",\"target_throttle_time_in_millis\":0},\"translog\":{\"recovered\":0,\"total\":0,\"percent\":\"100.0%\",\"total_on_start\":0,\"total_time\":\"225ms\",\"total_time_in_millis\":225},\"verify_index\":{\"check_index_time\":\"0s\",\"check_index_time_in_millis\":0,\"total_time\":\"0s\",\"total_time_in_millis\":0}},{\"id\":7,\"type\":\"SNAPSHOT\",\"stage\":\"DONE\",\"primary\":true,\"start_time\":\"2017-05-16T12:04:54.905Z\",\"start_time_in_millis\":1494936294905,\"stop_time\":\"2017-05-16T12:14:56.408Z\",\"stop_time_in_millis\":1494936896408,\"total_time\":\"10m\",\"total_time_in_millis\":601503,\"source\":{\"repository\":\"force-merge\",\"snapshot\":\"force-merge\",\"version\":\"5.1.1\",\"index\":\"indexv0.2_2017-02-12_536a9247f9fa4fc7a7942ad46ea14e0d\"},\"target\":{\"id\":\"K4xQPaOFSWSPLwhb0P47aQ\",\"host\":\"x.x.x.7\",\"transport_address\":\"x.x.x.7:9300\",\"ip\":\"x.x.x.7\",\"name\":\"staging-es5-forcem\"},\"index\":{\"size\":{\"total\":\"2.9gb\",\"total_in_bytes\":3168132171,\"reused\":\"0b\",\"reused_in_bytes\":0,\"recovered\":\"2.9gb\",\"recovered_in_bytes\":3168132171,\"percent\":\"100.0%\"},\"files\":{\"total\":15,\"reused\":0,\"recovered\":15,\"percent\":\"100.0%\"},\"total_time\":\"10m\",\"total_time_in_millis\":601453,\"source_throttle_time\":\"-1\",\"source_throttle_time_in_millis\":0,\"target_throttle_time\":\"-1\",\"target_throttle_time_in_millis\":0},\"translog\":{\"recovered\":0,\"total\":0,\"percent\":\"100.0%\",\"total_on_start\":0,\"total_time\":\"43ms\",\"total_time_in_millis\":43},\"verify_index\":{\"check_index_time\":\"0s\",\"check_index_time_in_millis\":0,\"total_time\":\"0s\",\"total_time_in_millis\":0}},{\"id\":0,\"type\":\"SNAPSHOT\",\"stage\":\"DONE\",\"primary\":true,\"start_time\":\"2017-05-16T11:54:48.191Z\",\"start_time_in_millis\":1494935688191,\"stop_time\":\"2017-05-16T12:04:51.366Z\",\"stop_time_in_millis\":1494936291366,\"total_time\":\"10m\",\"total_time_in_millis\":603174,\"source\":{\"repository\":\"force-merge\",\"snapshot\":\"force-merge\",\"version\":\"5.1.1\",\"index\":\"indexv0.2_2017-02-12_536a9247f9fa4fc7a7942ad46ea14e0d\"},\"target\":{\"id\":\"K4xQPaOFSWSPLwhb0P47aQ\",\"host\":\"x.x.x.7\",\"transport_address\":\"x.x.x.7:9300\",\"ip\":\"x.x.x.7\",\"name\":\"staging-es5-forcem\"},\"index\":{\"size\":{\"total\":\"2.9gb\",\"total_in_bytes\":3153750393,\"reused\":\"0b\",\"reused_in_bytes\":0,\"recovered\":\"2.9gb\",\"recovered_in_bytes\":3153750393,\"percent\":\"100.0%\"},\"files\":{\"total\":15,\"reused\":0,\"recovered\":15,\"percent\":\"100.0%\"},\"total_time\":\"10m\",\"total_time_in_millis\":602897,\"source_throttle_time\":\"-1\",\"source_throttle_time_in_millis\":0,\"target_throttle_time\":\"-1\",\"target_throttle_time_in_millis\":0},\"translog\":{\"recovered\":0,\"total\":0,\"percent\":\"100.0%\",\"total_on_start\":0,\"total_time\":\"271ms\",\"total_time_in_millis\":271},\"verify_index\":{\"check_index_time\":\"0s\",\"check_index_time_in_millis\":0,\"total_time\":\"0s\",\"total_time_in_millis\":0}}]}}\n2017-05-16 12:17:39,973 INFO               curator.utils          restore_check:1448 Index \"indexv0.2_2017-02-12_536a9247f9fa4fc7a7942ad46ea14e0d\" is still in stage \"DONE\"\n2017-05-16 12:17:39,973 DEBUG              curator.utils            wait_for_it:1578 Response: False\n2017-05-16 12:17:39,973 DEBUG              curator.utils            wait_for_it:1598 Action \"restore\" not yet complete, 10 total seconds elapsed. Waiting 10 seconds before checking again.\n.\n11:54:48.191Z\",\"start_time_in_millis\":1494935688191,\"stop_time\":\"2017-05-16T12:04:51.366Z\",\"stop_time_in_millis\":1494936291366,\"total_time\":\"10m\",\"total_time_in_millis\":603174,\"source\":{\"repository\":\"force-merge\",\"snapshot\":\"force-merge\",\"version\":\"5.1.1\",\"index\":\"indexv0.2_2017-02-12_536a9247f9fa4fc7a7942ad46ea14e0d\"},\"target\":{\"id\":\"K4xQPaOFSWSPLwhb0P47aQ\",\"host\":\"x.x.x.7\",\"transport_address\":\"x.x.x.7:9300\",\"ip\":\"x.x.x.7\",\"name\":\"staging-es5-forcem\"},\"index\":{\"size\":{\"total\":\"2.9gb\",\"total_in_bytes\":3153750393,\"reused\":\"0b\",\"reused_in_bytes\":0,\"recovered\":\"2.9gb\",\"recovered_in_bytes\":3153750393,\"percent\":\"100.0%\"},\"files\":{\"total\":15,\"reused\":0,\"recovered\":15,\"percent\":\"100.0%\"},\"total_time\":\"10m\",\"total_time_in_millis\":602897,\"source_throttle_time\":\"-1\",\"source_throttle_time_in_millis\":0,\"target_throttle_time\":\"-1\",\"target_throttle_time_in_millis\":0},\"translog\":{\"recovered\":0,\"total\":0,\"percent\":\"100.0%\",\"total_on_start\":0,\"total_time\":\"271ms\",\"total_time_in_millis\":271},\"verify_index\":{\"check_index_time\":\"0s\",\"check_index_time_in_millis\":0,\"total_time\":\"0s\",\"total_time_in_millis\":0}}]}}\n2017-05-16 12:17:29,958 INFO               curator.utils          restore_check:1448 Index \"indexv0.2_2017-02-12_536a9247f9fa4fc7a7942ad46ea14e0d\" is still in stage \"DONE\"\n2017-05-16 12:17:29,958 DEBUG              curator.utils            wait_for_it:1578 Response: False\n2017-05-16 12:17:29,958 DEBUG              curator.utils            wait_for_it:1598 Action \"restore\" not yet complete, 10 total seconds elapsed. Waiting 10 seconds before checking again.\n^[[A^[[A^[[A^[[A^[[A2017-05-16 12:17:39,968 DEBUG              curator.utils            wait_for_it:1575 Elapsed time: 1372 seconds\n2017-05-16 12:17:39,969 DEBUG         urllib3.util.retry               from_int:200  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)\n2017-05-16 12:17:39,970 DEBUG     urllib3.connectionpool          _make_request:395  http://x.x.x.7:9200 \"GET /indexv0.2_2017-02-12_536a9247f9fa4fc7a7942ad46ea14e0d/_recovery?human=true HTTP/1.1\" 200 9593\n2017-05-16 12:17:39,971 INFO               elasticsearch    log_request_success:82   GET http://x.x.x.7:9200/indexv0.2_2017-02-12_536a9247f9fa4fc7a7942ad46ea14e0d/_recovery?human=true [status:200 request:0.002s]\n2017-05-16 12:17:39,971 DEBUG              elasticsearch    log_request_success:84   > None\n. testing ..... I get a an exception on the delete action :\n2017-05-17 12:28:26,072 INFO      GET http://x.x.x.7:9200/index_v0.2_2017-05-16_270b7c8edcc74744b39bbf304753b06e/_recovery?human=true [status:200 request:0.002s]\n2017-05-17 12:28:26,073 INFO      Action ID: 1, \"restore\" completed.\n2017-05-17 12:28:26,073 INFO      Job completed.\n2017-05-17 12:28:30,299 INFO      Preparing Action ID: 1, \"forcemerge\"\n2017-05-17 12:28:30,307 INFO      GET http://x.x.x.7:9200/ [status:200 request:0.007s]\n2017-05-17 12:28:30,309 INFO      GET http://x.x.x.7:9200/_nodes/_local [status:200 request:0.002s]\n2017-05-17 12:28:30,311 INFO      GET http://x.x.x.7:9200/_cluster/state/master_node [status:200 request:0.001s]\n2017-05-17 12:28:30,311 INFO      Trying Action ID: 1, \"forcemerge\": forcem and mergem\n2017-05-17 12:28:30,313 INFO      GET http://x.x.x.7:9200/_all/_settings?expand_wildcards=open%2Cclosed [status:200 request:0.001s]\n2017-05-17 12:28:30,314 INFO      GET http://x.x.x.7:9200/ [status:200 request:0.001s]\n2017-05-17 12:28:30,332 INFO      GET http://x.x.x.7:9200/_cluster/state/metadata/index_v0.2_2017-05-16_270b7c8edcc74744b39bbf304753b06e [status:200 request:0.018s]\n2017-05-17 12:28:30,336 INFO      GET http://x.x.x.7:9200/index_v0.2_2017-05-16_270b7c8edcc74744b39bbf304753b06e/_stats/store,docs [status:200 request:0.002s]\n2017-05-17 12:28:30,400 INFO      GET http://x.x.x.7:9200/index_v0.2_2017-05-16_270b7c8edcc74744b39bbf304753b06e/_segments [status:200 request:0.063s]\n2017-05-17 12:28:30,402 INFO      forceMerging selected indices\n2017-05-17 12:28:30,402 INFO      forceMerging index index_v0.2_2017-05-16_270b7c8edcc74744b39bbf304753b06e to 1 segments per shard.  Please wait...\n2017-05-17 13:02:46,662 INFO      POST http://x.x.x.7:9200/index_v0.2_2017-05-16_270b7c8edcc74744b39bbf304753b06e/_forcemerge?max_num_segments=1 [status:200 request:2056.259s]\n2017-05-17 13:02:46,662 INFO      Pausing for 10.0 seconds before continuing...\n2017-05-17 13:02:56,672 INFO      Action ID: 1, \"forcemerge\" completed.\n2017-05-17 13:02:56,673 INFO      Preparing Action ID: 2, \"delete_snapshots\"\n2017-05-17 13:02:56,676 INFO      GET http://x.x.x.7:9200/ [status:200 request:0.002s]\n2017-05-17 13:02:56,678 INFO      GET http://x.x.x.7:9200/_nodes/_local [status:200 request:0.002s]\n2017-05-17 13:02:56,679 INFO      GET http://x.x.x.7:9200/_cluster/state/master_node [status:200 request:0.001s]\n2017-05-17 13:02:56,680 INFO      Trying Action ID: 2, \"delete_snapshots\": Delete force-merge snapshot\n2017-05-17 13:02:56,681 INFO      GET http://x.x.x.7:9200/_snapshot/force-merge [status:200 request:0.001s]\n2017-05-17 13:02:56,824 INFO      GET http://x.x.x.7:9200/_snapshot/force-merge/_all [status:200 request:0.143s]\n2017-05-17 13:02:56,825 INFO      Deleting selected snapshots\n2017-05-17 13:02:56,903 INFO      GET http://x.x.x.7:9200/_snapshot/force-merge/_all [status:200 request:0.078s]\n2017-05-17 13:02:56,903 INFO      Deleting snapshot force-merge...\n2017-05-17 13:03:26,934 WARNING   DELETE http://x.x.x.7:9200/_snapshot/force-merge/force-merge [status:N/A request:30.031s]\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/elasticsearch/connection/http_urllib3.py\", line 114, in perform_request\n    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)\n  File \"/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.py\", line 649, in urlopen\n    _stacktrace=sys.exc_info()[2])\n  File \"/usr/local/lib/python2.7/dist-packages/urllib3/util/retry.py\", line 333, in increment\n    raise six.reraise(type(error), error, _stacktrace)\n  File \"/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.py\", line 600, in urlopen\n    chunked=chunked)\n  File \"/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.py\", line 388, in _make_request\n    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n  File \"/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.py\", line 308, in _raise_timeout\n    raise ReadTimeoutError(self, url, \"Read timed out. (read timeout=%s)\" % timeout_value)\nReadTimeoutError: HTTPConnectionPool(host=u'x.x.x.7', port=9200): Read timed out. (read timeout=30)\n2017-05-17 13:03:26,936 ERROR     Failed to complete action: delete_snapshots.  : Exception encountered.  Rerun with loglevel DEBUG and/or check Elasticsearch logs for more information. Exception: ConnectionTimeout caused by - ReadTimeoutError(HTTPConnectionPool(host=u'x.x.x.7', port=9200): Read timed out. (read timeout=30))\n. thanks \nbut can you post an example, cause its till not working for me\nI tried :\nextra_settings:                                    \n           settings:                                     \n              index.number_of_replicas: 0\n```\nandnumber_of_replicas: 0but still no success . https://discuss.elastic.co/t/curator-restore-extra-settings/91472/4. I don't think  extra_settings affects the operation of restore action. \nReopening the issue \nfor the record, using curl method with \n\"index_settings\": {\n        \"index.number_of_replicas\": 0\nworks great.. extra_settings:                                    \n  index_settings:                                     \n      number_of_replicas: 0\nworks great -- please keep an eye on the Docs :dagger: . Thanks . ",
    "brandond": "1.5GB isn't a specific constraint; this was just a test run. I've seen it easily get up to 3GB on slightly larger clusters. My intent was to push out curator to all the ES nodes and use the master-only flag to have it only execute daily cleanups on a single node. Unfortunately, this bug means that curator uses so much memory that it frequently ends up either hanging or OOM killing the master node, since they're running in EC2 with no swap, and ElasticSearch is configured to use the majority of memory on the host - there's not enough RAM left for a multi-GB Python process.\nThese indexes are primarily storing data from AWS CloudTrail, which contains audit logs for API calls to AWS services. Since many different API calls use similar parameter names with different value types, I was having trouble with mapping templates getting the fields wrong. The same parameter may look like a float for one API, a date string for another, and a complex type for a third. In order to work around this, Logstash's ElasticSearch output sets:\nelse if [type] == \"CloudTrail\" {\n        elasticsearch {\n            document_id => \"%{eventID}\"\n            document_type => \"%{eventType}-%{eventName}\"\n            template => \"/etc/logstash/template.json\"\n            template_name => \"logstash\"\n            template_overwrite => \"true\"\n        }\nThis results in a unique document_type for each API call so that I don't end up with mapping conflicts. Are you saying that this would no longer work in ES6? How will it handle multiple document schemas within the same index, when different documents might contain varying types for a given field?. ",
    "diurnalist": "@untergeek not yet, awaiting approval from my employer. Shouldn't take too long though.. @untergeek yeah... still working on it. Because technically this was work-related, I need to clear the corporate CLA, as we didn't have an existing one.. @untergeek thanks for the ping. Yep, should be done now. I bumped the build.. ",
    "robinsmidsrod": "I've signed and sent in the CLA. I'd appreciate if someone could approve it and comment on this change.. ",
    "croccam": "Hi,\nI am using python 3.6, I thought mock was already in the standard library from version 3 onwards, but I guess I was wrong.\nBest regards!. ",
    "mkubenka": "It looks like /_snapshot/_status endpoint is working after AWS ES add support for 5.5. Probably related with availalibity to retore automated snapshots:\n\nSelf-service restore of Amazon Elasticsearch Service domains is now available from the automated snapshots taken by the service. All automated snapshots are available for restore from the Elasticsearch API or Curator CLI. You no longer have to request support assistance for restore requests.\n\nI have tried examples/actions/snapshot.yml and it worked even on 5.3!. Also, please don't forget link to boto3 Credentials manual page in documentation for all possible ways of setting credentials.. Sure, I will try updating documentation with more info and examples.. Sorry for delay, #1012 is better solution for AWS authentication.\nI have added couple comments there with my suggestions. Closing this.. Is this still necessary? /_snapshot/_status endpoint is working after AWS ES add support for 5.5. Missing error message if boto3 is not present.. Actually, this PR is making credentials handling more standardised because it's delegated to boto3 Session. There are many ways how user can configure credentials - environment variables, profiles, STS credentials,...\nBut, I definitely agree there should be backward compatibility and maybe deprecate options with next major release.. ",
    "raj-krrish": "I am still facing this issue. Restore throws error while recovery check, but runs smoothly when I set wait_for_completion flag to False. I am using elasticsearch-curator (5.1.1). ",
    "Nodens2k": "Hi, my bad, I should have realized that. I guess I was too tired :-D\n~~We are trying to use the latest version, so I guess it is not really compatible with ES 5.4 or 5.4.1 yet, right?~~\n~~Do you have a date in mind for a new release supporting the latest ES versions?~~\nWe were trying to use the latest version, but for some reason the installed version was 4.0.1. Sooo, I'm glad my mistake was the trigger for a bug correction :-). ",
    "alexcurtin": "@untergeek Do you have a roadmap for when a version of Curator will be released thats compatible with Elastic Search  v5.4.x?. @untergeek Thanks sorry. I bumped my curator version to 5.1.1 and I no longer see that error. . ",
    "wvidana": "@untergeek is right about this lines https://github.com/elastic/curator/blob/v5.1.1/curator/utils.py#L615-L619\nI also got the same missleading error message. Upgrading curator solved my problem but took me some time to realize what the problem was since the error message reported version 5.5 for curator (when that is my ES cluster version, not curator version).. ",
    "tahaderouiche": "Hi @karmi , there was a typo on my git config on this machine. Corrected that now!. ",
    "ThorbenJ": "Are there Debian source packages available? So that one could add deb-src to sources.list.d/..; apt-get source elasticsearch-curator; and then build with dpkg-buildpackage.\nI note currently there are no Debian source packages in the repository. Is there a reason to not build source packages?\nTo support Deb8 and Deb9 you will probably need two package indexes each pointing to one of two package versions (which could share the same pool package hierarchy).\nAlthough I assume you already know, just in case: There is list of repository tools, maybe switching to another would make things easier? https://wiki.debian.org/DebianRepository/Setup\nAlso where Debian goes Ubuntu and friends will follow, so this will likely also break in an up coming release thereof.. ",
    "pfremm": "My issue is we have a batch job to cleanup old indices as we use ES for aggregated logging.  If no indices exist to cleanup right now curator_cli throws and exception and the exit code is not zero.  Am I missing something to exit with a 0 exit code if no indices exist for my filters?. I thought the CLI was going to be easier since I was using env variables.  Just saw https://www.elastic.co/guide/en/elasticsearch/client/curator/5.1/envvars.html.  I guess I can specify them in config files too.. ",
    "diegolopmon": "Thank you @corey-hammerton, now I understand what was happening. My filters only match by date.. ",
    "kobuskc": "Hi @karmi, I'm new to this. The e-mail address in my Github profile and the one I used in my commit are the same. It is the only one I use. In my Github profile, it only shows my address though, not my full name as well. Will that make a difference?. Hi @untergeek , I have moved away from trying to keep aws_flag as a global variable and put it in settings.py instead.. If this PR is accepted, request signing can be configured by adding the following to curator.yml:\naws_sign_request: True\n  aws_region: 'your-aws-region'\nThe credentials required to sign the requests will then be retrieved from the environment by checking these locations:\n- Environment variables\n- AWS credentials file\n- Instance profile credentials\nIf aws_sign_request is set to True, the aws_flag variable will be set, allowing the snapshot function to still check for current running snapshots before attempting a new one.\nThis will require the boto3 package to be installed as well.. The version information doesn't include a proper build hash. I have checked on multiple clusters and it is always \"Unknown\"\nI can also see that there is only limited information available in the os key, but that could help. I can check for the absence of os.name, os.arch and os.version and the presence of the discovery-ec2 plugin. \nHere's an example output:\n```\nclient.nodes.info()['nodes']['FLzHlSuSQkmfP4to9MJR0A']['os']\n{'allocated_processors': 2,\n 'available_processors': 2,\n 'refresh_interval_in_millis': 1000}\n\nclient.info()\n{'cluster_name': '733061676469:helixsearch',\n 'cluster_uuid': 'tFl0cnoQSVSOuOmheMswZw',\n 'name': 'LmYkw-v',\n 'tagline': 'You Know, for Search',\n 'version': {'build_date': '2017-04-28T21:44:52.237Z',\n  'build_hash': 'Unknown',\n  'build_snapshot': False,\n  'lucene_version': '6.4.2',\n  'number': '5.3.2'}}\n```\nPlease let me know if this approach is acceptable.. I have updated the tests that broke due to the new line I had added earlier. I will add AWS specific tests as soon as possible.. @untergeek, sorry it took so long. I have added the unit tests, new AWS ES 5.5 version and rebased.. @untergeek since the AWS ES 5.x releases now support /_snapshot/_status, I suggest we close this pull request and I create a new one with just the authentication updates. Do you agree?. New pull request: #1084 . Not sure why the Travis build failed. This is my test output:\n\nXML: /curator/nosetests.xml\nName                                  Stmts   Miss  Cover\n\ncurator/init.py                      11      0   100%\ncurator/_version.py                       1      0   100%\ncurator/actions.py                      870    113    87%\ncurator/cli.py                          113      0   100%\ncurator/config_utils.py                  34      5    85%\ncurator/defaults/init.py              0      0   100%\ncurator/defaults/client_defaults.py       5      0   100%\ncurator/defaults/filter_elements.py      66      0   100%\ncurator/defaults/filtertypes.py          60      0   100%\ncurator/defaults/option_defaults.py     147      0   100%\ncurator/defaults/settings.py             32      0   100%\ncurator/exceptions.py                     9      0   100%\ncurator/indexlist.py                    369      0   100%\ncurator/logtools.py                      36      0   100%\ncurator/repomgrcli.py                    78      1    99%\ncurator/snapshotlist.py                 186      0   100%\ncurator/utils.py                        688     11    98%\ncurator/validators/init.py            1      0   100%\ncurator/validators/actions.py            19      0   100%\ncurator/validators/config_file.py         4      0   100%\ncurator/validators/filters.py            32      0   100%\ncurator/validators/options.py            13      0   100%\ncurator/validators/schemacheck.py        41      0   100%\n\nTOTAL                                  2815    130    95%\nRan 570 tests in 136.390s\nOK (SKIP=4)\nShutting down....\n[root@xxxxxxxxxxx curator]# \nnosetests.xml.gz\n. Changed the import of botocore exceptions for clarity and fixed the logic error that prevented Client instances from being created. Local test results:\n\nXML: /curator/nosetests.xml\nName                                  Stmts   Miss  Cover\n\ncurator/init.py                      11      0   100%\ncurator/_version.py                       1      0   100%\ncurator/actions.py                      870    113    87%\ncurator/cli.py                          113      0   100%\ncurator/config_utils.py                  34      5    85%\ncurator/defaults/init.py              0      0   100%\ncurator/defaults/client_defaults.py       5      0   100%\ncurator/defaults/filter_elements.py      66      0   100%\ncurator/defaults/filtertypes.py          60      0   100%\ncurator/defaults/option_defaults.py     147      0   100%\ncurator/defaults/settings.py             32      0   100%\ncurator/exceptions.py                     9      0   100%\ncurator/indexlist.py                    369      0   100%\ncurator/logtools.py                      36      0   100%\ncurator/repomgrcli.py                    78      1    99%\ncurator/snapshotlist.py                 186      0   100%\ncurator/utils.py                        710     29    96%\ncurator/validators/init.py            1      0   100%\ncurator/validators/actions.py            19      0   100%\ncurator/validators/config_file.py         4      0   100%\ncurator/validators/filters.py            32      0   100%\ncurator/validators/options.py            13      0   100%\ncurator/validators/schemacheck.py        41      0   100%\n\nTOTAL                                  2837    148    95%\nRan 570 tests in 136.873s\nOK (SKIP=4)\nShutting down....\n. Yes, it connects with boto as expected. Here's an example test with --dry-run:\nWith aws_sign_requests: True\nxxxxxxxxxx  ~/actions  \u00bb curator --dry-run snapaction.yml \n2017-10-19 08:24:21,613 INFO      Preparing Action ID: 1, \"snapshot\"\n2017-10-19 08:24:21,756 INFO      Found credentials in shared credentials file: ~/.aws/credentials\n2017-10-19 08:24:21,817 INFO      Trying Action ID: 1, \"snapshot\": Snapshot members prefixed indices older than 1 day (based on index creation_date) with the default snapshot name pattern of 'curator-%Y%m%d%H%M%S'.  Wait for the snapshot to complete.  Do not skip the repository filesystem access check.  Use the other options to create the snapshot.\n2017-10-19 08:24:21,875 INFO      DRY-RUN MODE.  No changes will be made.\n2017-10-19 08:24:21,875 INFO      DRY-RUN: snapshot: curator-20171018212421 in repository backups with arguments: {'ignore_unavailable': False, 'partial': False, 'indices': 'members', 'include_global_state': True}\n2017-10-19 08:24:21,875 INFO      Action ID: 1, \"snapshot\" completed.\n2017-10-19 08:24:21,875 INFO      Job completed. \nWith no aws_sign_requests, and both aws_key and aws_secret_key specified\nxxxxxxxxxx  ~/actions  \u00bb curator --dry-run snapaction.yml\n2017-10-19 08:26:03,700 INFO      Preparing Action ID: 1, \"snapshot\"\n2017-10-19 08:26:03,839 INFO      Trying Action ID: 1, \"snapshot\": Snapshot members prefixed indices older than 1 day (based on index creation_date) with the default snapshot name pattern of 'curator-%Y%m%d%H%M%S'.  Wait for the snapshot to complete.  Do not skip the repository filesystem access check.  Use the other options to create the snapshot.\n2017-10-19 08:26:03,890 INFO      DRY-RUN MODE.  No changes will be made.\n2017-10-19 08:26:03,890 INFO      DRY-RUN: snapshot: curator-20171018212603 in repository backups with arguments: {'partial': False, 'ignore_unavailable': False, 'include_global_state': True, 'indices': 'members'}\n2017-10-19 08:26:03,890 INFO      Action ID: 1, \"snapshot\" completed.\n2017-10-19 08:26:03,890 INFO      Job completed.\nI will update the documentation today.. Thanks. That makes a lot of sense. I have made the required changes.. I have tested the original code agains an AWS ES 5.3 and 5.5 cluster and it works as expected now, so this is not necessary anymore.. Thanks. I have added a catch to print an error message if the import fails and exit.. This makes sense. I have added an update to allow for backward compatibility for any existing functionality in a minor version change.. Makes sense. I have removed the default.. This makes sense. I have implemented your suggestions, so now credentials retrieved with boto3 will overwrite anything that was in the config file if the user specified both.\n. ",
    "algestam": "Ok, thanks for the info \ud83d\udc4d . ",
    "shaharmor": "Any update on this?. Fixed all issues & rebased. Whoops, sorry. Edited on github \ud83d\udc4e . ",
    "dmcd": "The problem I'm having is that curator uses the click package which stops me from being able to pass in parameters from my handler function (https://github.com/pallets/click/issues/40). \nI've worked around this issue by taking a copy of the source for curator and commenting out the click attributes on the cli function:\n``` python\n@click.command()\n@click.option('--config',\nhelp=\"Path to configuration file. Default: ~/.curator/curator.yml\",\ntype=click.Path(exists=True), default=settings.config_file()\n)\n@click.option('--dry-run', is_flag=True, help='Do not perform any changes.')\n@click.argument('action_file', type=click.Path(exists=True), nargs=1)\n@click.version_option(version=version)\ndef cli(config, dry_run, action_file):\n```\nThis allows me to invoke curator from my handler function:\n``` python\n!/usr/bin/env python\nfrom curator.cli import cli\ndef run(event, context):\n    cli('curator.yml', False, 'actions.yml')\n```. Thanks @untergeek. I understand that I'm not following the intended usage, sorry about that. However, I really like the CLI interface because it allows me to ignore the underlying implementation and focus on the config. If I'm not mistaken, the blog post you referenced suggests hand writing some of the functionality that you get for free with the CLI interface. It seems I would need to reimplement quite a large block of code to achieve the same flexibility that the CLI offers. Where as, the above modifications allow me to get up and running quickly and it is working flawlessly so far. \nBut I agree, it means I need to bundle the curator source with my deployment and therefore makes upgrading a little trickier. Would it be possible to move some of the functionality that is offered by the CLI API into the curator API? i.e. so that someone could use curator from source or from CLI without needing to write code, just supply an action file and configuration file?\n. Sounds good to me and yes I imagine a bit of work in the documentation too. \nI think that's a clean solution. Its not a high priority for me at the moment as I'm fairly content with my work around for now. \nThanks for building curator! I'm really happy with how its working in my environment. . How about \"run\" for the name of the function?. ",
    "subbu-sw": "Our data nodes are having 16G heap each. On checking the node stats (jvm), i realize that both of them are having >70% full. Let me try closing certain old indices and see if that improves the situation.. we added a new data node and see that the backup process is running successfully. closing this issue.. ",
    "letharion": "@untergeek Sorry, no progress, and I'm currently traveling, so it'll be a while. Feel free to close and replicate in a different branch, it's a fairly trivial change after all. :). ",
    "netanel246": "+1. ",
    "mikemchau": "Thank you for your quick reply.\n1) I also noticed the warning about GET http://localhost:9200/. However, I don't understand why it calls the localhost (and not GET http:// myhostname:9200) when I state specifically the hostname in my es_repo_mgr statement.\nAgain, I run my cron script from a separate instance. Here is my script:\nes_repo_mgr --host myhostname delete --yes --repository $REPOSITORY_NAME\nes_repo_mgr --host myhostname create s3 --repository $REPOSITORY_NAME --bucket $BUCKET_NAME --base_path $BASE_PATH\ncurator  --host myhostname --timeout 21600 snapshot --name $SNAPSHOT_NAME --repository $REPOSITORY_NAME indices --all-indices\nOn this instance, I am able to run curl -XGET  myhostname:9200/_snapshot?pretty. Of course, GET http://localhost:9200/ fails. The cron job used to work \n2) Will also look into your suggestion of upgrading curator to 3.5.1\n3) Another question, do you know what elasticsearch-curator==3? In the other cron script, I see the following:\nif ! type \"curator\" > /dev/null; then\n  apt-get -y install python-pip\n  pip install elasticsearch-curator\n  pip install -U elasticsearch-curator==3\nfi\nLooking at the timestamp, after setting up this second script, both scripts failed\nThanks\n. Never mind about  elasticsearch-curator==3. That is to specify the version of curator.. Ok, upgrading curator to 3.5.1 seems to fix the problem. I don't see Connection Failure anymore.\npip install -U elasticsearch-curator==3.5.1\nThanks. ",
    "monkey3199": "@karmi Maybe my email on the profile was hidden. I have set the email to public. Thanks.\n@untergeek But, The tests for shrink is not in the source. Can I make the tests only for the feature \"copy_aliases\"?. @untergeek I am so sorry. I'm going to get the CLA and let you know how it is going.. @untergeek I got the CLA again(email: monkey3199@gmail.com). And I will add the test soon.. I have just added the test code as well. I'm so sorry and apologize for being late. Please review this code. . I have added the line in the change log. Please confirm it.. @untergeek Thanks a lot!! Please let me know whatever I have to do.. @untergeek Oh! I have not been realizing about the review. I've just added the space words. Please check it again.. Sorry, That's only my configuration.. Oh, sorry. I might delete when fixing some conflicts in merge. I will fix it.. ",
    "dustin-decker": "Done!. PR enclosed in https://github.com/elastic/curator/pull/1067. Maybe a bool? smaller_than=True (default False)\n. Rebased. I think that's a big improvement in clarity. What do you think of disk_space_behavior for the key? You already have the disk_space key, so it's slightly more obvious than 'threshold'. It also places it in close proximity to disk_space when alphabetically sorted, which is helpful for documentation.\nI'll try to get the changes done, whatever the end up being, tomorrow or the day after.. Looks like I missed this one in https://github.com/elastic/curator/pull/1063. That issue does not capture this difference:\n- curator will rollover if ALL conditions match\n- elasticsearch will rollover if ANY conditions match\nAccording to the curator docs, at least. I have not verified the behavior.. Good catch. I'm still reviewing my changes :). I should probably have a test for the default behavior. ",
    "dandrestor": "Possible solution:\nHave an option to enable assigning the index to an alias, following the successful and unsuccessful processing of that index by the shrink action. This could even be a good idea to have as a generic option for all actions. Subsequent actions could use the alias filter to perform cleanup or follow-up actions on indexes as required based on their alias membership.\nJust to clarify, it's the original (unshrunk) index that would be assigned to the alias. For the shrunk index, there is already the option to do that with the extra_settings option.\n. Thanks, that clarifies it. I submitted a PR with a small change in the docs.. An easy workaround for this is setting cluster.routing.rebalance.enable to none before the shrink action.. Yeah, using cluster_routing is definitely a good idea. But I still think that this is a workaround, and that it's worth enhancing curator so that it does not wait for all recoveries, but rather just the ones that are relevant.. I understand. I guess you should close this, then. \nJust for my own understanding, though, what are the problems that might occur? We are talking about two possible situations:\n Trigger a shrink operation, possibly while recoveries for other indexes are ongoing\n Trigger shard relocations, subject to cluster-wide relocation limits, possibly while other recoveries are ongoing.\nWhat would be the problems with either of those?. I'm not talking about automatically disabling shard rebalancing. I only mentioned that as a workaround to what I see as the underlying problem, and I agree that this is something that the user should explicitly perform.\nHowever, if the user does not disable rebalancing, and still calls the shrink operation, that is an explicit indication that the user wants the two to occur simultaneously. This is the situation in which I mean that the shrink operation should not wait for other, unrelated indexes to recover before executing. Also, the shrink command will still be subject to the limits regarding per-node and cluster-wide simultaneous recoveries, so the user's safeguards regarding I/O and network bandwidth will still be observed.. What I mean is, the two operations (shrink and disabling rebalancings via cluster_routing) are orthogonal. The user is free to choose one or both. If a user chooses the former only, it is an indication that the shrink operation may be performed simultaneously with the cluster rebalancing, otherwise the user would have chosen both operations.. ",
    "Howard-Chang": "yes. it show indices, but when I run the command:\ncurator --config curator.yml delete_indices.yml\nit still say they're not there.. it always show this : \nE:\\ELK\\elasticsearch-curator-5.2.0-amd64\\curator-5.2.0-amd64>curator --config cu\nrator.yml delete_indeces.yml\n2017-09-26 11:16:24,912 INFO      Preparing Action ID: 1, \"delete_indices\"\n2017-09-26 11:16:24,928 INFO      Trying Action ID: 1, \"delete_indices\": Delete\nindices older than 1 days (based on index name), for logstash- prefixed indices.\n Ignore the error if the filter does not result in an actionable list of indices\n (ignore_empty_list) and exit cleanly.\n2017-09-26 11:16:24,959 INFO      Skipping action \"delete_indices\" due to empty\nlist: \n2017-09-26 11:16:24,959 INFO      Action ID: 1, \"delete_indices\" completed.\n2017-09-26 11:16:24,959 INFO      Job completed.. I check the log in kibana. it actually exists some log in elasticsearch.\n.kibana\nlogstash-2017-08-09\nlogstash-2017-08-23\nlogstash-2017-09-19\nlogstash-2017-09-25. Oh haha.. thank you a lot \ud83d\udc4d . sorry may I ask another question.\nI want to delete log per week automatically , and my pc is windows. How can I do it : (. I have done it by schedule tool thank u : ). ",
    "orphaner": "+1. ",
    "cushind": "Any update on this? Currently we are running a shrink action on a large number of indices, and we have a few that cannot be shrunk because they are already at 1 shard. This essentially prevents the shrink action from working on any of the other indices that are valid shrink targets, and I do not want to have to manually exclude indices in the filter list.. @untergeek If my understanding is correct the shrink action will fail entirely if the current index already has the target number of shards. It will not move on to the next index, apply any post-shrink prefix or suffix, or any post allocation settings.\nThis is actually relevant for other actions as well. I know you can continue_if_exception to continue to the next action, but can you continue to next index in a range of indices? So lets say your filter selects 5 indices and an action fails on the first one, then the action will not continue on the other 4? Is this the expected default behavior?. Should this filter type be exposed through the curator api or be implicit on every shrink action? In other words, the default filter should exclude any index whose current number of shards is less than or equal to the target shrink number of shards, but would it ever make sense to explicitly set this value.\nI'm wondering if someone might want to shrink only those indices with some upper percentile of shards, instead of every index that meets the default criteria. And would a # of shards filter be relevant for any other curator action, like possibly forcemerge which is ofter done in conjunction with shrinking?. Ok sounds good. I can get a PR for this in the next couple of days if that would help you out.. @untergeek Got a PR in. Let me know what you think.. I can put in a PR with a fix if needed.. @untergeek Tried with index_settings, and curator complains that \"routing\" is not a valid index setting. So I actually don't think this works with index_settings.. ",
    "jonathancoon": "Thank you for your reply. I uninstalled the YUM package and installed Curator via pip. But I still have the same problem. . 5.2.0. 2.7.5. Ah! I figured it out! It wasn't letting me import curator because my Python script was named curator.py! . Thanks for your help!. ",
    "chjohnst": "I would find this useful as well since I roll my indexes daily and taking a snapshot of yesterdays index with a snapshot name that matches that indexes keeps thinks tidy.  . ",
    "lkpatel": "I also need this features. This feature is there in create_index action. In my opinion, this should extended to other actions also. \n  . ",
    "neilrberry": "I agree - this feature will be helpful for exactly the same reasons above.. ",
    "Alsheh": "Can this feature be extended (or has been extended) to the repository name as well?. @untergeek I am trying to move a daily snapshot of indices to Amazon Glacier with the ability to restore them later as needed. However, moving a snapshot individually from a repository doesn't seem to be possible. So, I am creating a daily repository (hence the need for a dynamic date name) and moving that entire repository to Glacier. Extending the date math operation to the repository name would be great!. ",
    "tomsommer": "That's fair enough, but I could solve my performance problem with Curator and individual allocation-queries per index (archiving), so an option to use this method, would be very helpful.\nThe timeout is not client-side here, it's serverside.. ",
    "outworlder": "\ud83d\udc4d for this feature.\nI haven't figured out a way to use curator to reliably shrink indices when you have a hot/warm architecture(without a dedicated shrink node). It is very common that a hot node will have more space than warm nodes, after the shards have migrated away. This is ok, except if curator selects an index which is sitting in a warm node.\nWhen that happens, it tries to move the index to a given hot node. Elasticsearch will now have a conflicting allocation, we asked for a specific node, which is not a 'warm' node. \n. ",
    "ruflin": "\nThe templates have the following naming scheme: metricbeat-6.0.0-beta1. First the beat name followed by it's version. So for one beat they can easily be found with beatname-*\nBesides the version in the name, we also have it in the mapping. Here an example:\n\n\"mappings\": {\n      \"_default_\": {\n        \"_meta\": {\n          \"version\": \"6.0.0-beta1\"\n        },\nOne thing that would be pretty nice if I could say remove all templates older then 5.6.0. But that would require curator to understand semantic versioning or would need such queries in ES.\nOne important thing to note is that we did not have this version scheme of templates from day one in beats and only introduced it in version 5.? (I need to check that). So before it was just metricbeat. As long as we include a - in our queries for templates, these should not be affected.. The version in the meta information exists since the 5.0 release. But all 5.x releases just load one template with the name metricbeat. So only people starting with the 6.x releases have multiple templates and the version in the name. Someone running Metricbeat 6.0 with Elasticsearch 5.x will also have multiple templates.\nSo this is a problem that is only starting to happen. I'm hoping if we can get such a feature into curator that it's rather generic as people also modify templates and index patterns sometimes.. ",
    "anyat8": "Ok, thanks! \nAdded the question here:\nhttps://discuss.elastic.co/t/elasticsearch-curator-deleting-indices-by-priorities-with-python-api/107895. ",
    "ypid": "@untergeek Thanks for you help. I have not checked this in detail, but the !single syntax looks like it depends on unsafe yaml.load. The risk of local code execution is not as bad as remote code execution which the use of yaml.load had in other projects. Feel free to use other ways of environment variable substitution.. ",
    "dashford": "Hey @untergeek, I've made those changes now - let me know if there's anything else needed here.. ",
    "allansene": "Yes, @untergeek! I had tried to execute a shrink before, but it failed because the index was split between 2 nodes. I thought that Curator 'rollback' if some operation fails. So I think we can close this issue. But maybe is worthy look this behavior when the shrink operation fails.\nThanks a lot \ud83d\ude04 . Right, I got your point :) thanks for the clarification. \ud83d\ude04 . ",
    "andreausu": "I've tried with:\ncurator_cli --timeout 1800 --host 10.33.3.38 close --ignore_empty_list --filter_list '[{\"filtertype\":\"age\",\"source\":\"creation_date\",\"direction\":\"older\",\"unit\":\"days\",\"unit_count\":60},{\"filtertype\":\"pattern\",\"kind\":\"prefix\",\"value\":\"logstash-\"},{\"filtertype\":\"closed\",\"exclude\":\"true\"}]'\nand it works as you described, thank you!. ",
    "ShivanshuBagga2": "Both node have 3 indices and indices data uploaded from Put request though curl.I uploaded bulk data as \ncurl -H 'Content-Type: application/x-ndjson' -XPOST 'localhost:9200/_bulk?pretty' --data-binary @logs.jsonl\nwhere I found logs.jsonl from https://download.elastic.co/demos/kibana/gettingstarted/logs.jsonl.gz\nThis file have three indices named  logstash-2015.05.18,logstash-2015.05.19 and logstash-2015.05.20.\nFor other node I changed json file manually ,and change names to  logstash-2015.06.18,logstash-2015.06.19 and logstash-2015.06.20. Otherwise both node have same data with different indices name.\nAt a time I'm trying to delete all indices from node1 and node2 i.e.   logstash-2015.05.18,logstash-2015.05.19 ,logstash-2015.05.20,logstash-2015.06.18,logstash-2015.06.19 and logstash-2015.06.20.\nWhat other information I can provide to you.. Yes both nodes are in same cluster , both are different systems and connected though local area network.I'm trying that for development purpose not for production. There are only nodes in cluster and both node have 3 indices , data is same as I provide you in last comment.. ",
    "mikn": "@untergeek Sorry, you're right! There you go. :). ",
    "christianvozar": "@untergeek This seems very reasonable; anything to get image sizes smaller while maintaining same functionality is more than welcome. \ud83d\udc4d . ",
    "abhilashnayak": "Sorry --dry-run was the issue. Removed it while running the curator and it worked. . ",
    "bamb00": "I'm running the provided configmap file and is not working correctly according to the settings in the action_file.. @untergeek Not sure why you close this issue #1113.  I cannot go on the elasticsearch community and ask how the configmap action_file.yml settings is correct or not.  It does not hurt just to review the file contains the correct multi filter.. @untergeek  Thanks for pointing me to the elasticsearch discussion page.  I was able to see what I'm doing wrong.. This is the idex at the time of the grooming,\n   green open logstash-2018.08.07 WnIpi4-yTDqPtT9e30iHZg 5 1 4464 0 8.1mb 4mb\n\nWhich one of the store size doesit target? store.size or primary.store.size?. @untergeek Thanks for pointing out.\nI have a question regarding having multiple filtertype in an action.  There are three filter type in the action.  For the grooming to trigger all three condition has to be met like age, space, and pattern not just one filtertype.  Is it AND or OR condition need to be met?\n. can you pls explain? my understanding is if the disk_space is set to 10 and the daily indexes size is,\nindex1  - 2 gb\nindex2 - 2 gb\nindex3 - 3 gb\nindex4 - 3 gb\nindex5 - 2 gb\nthen index1 (oldest) will be deleted 1st. For index1 store size is 3 gb and index2 store size is 2.5 gb and the disk_space is set to 5GB.  Then index index1 is deleted.  Is that the behavior?. I ran it in debug mode.  The log show index that is actionable remain in the list to be deleted and index that was not actionable removed from list.\nThanks for the help.. ",
    "igalarzab": "I changed the base branch.\nAbout the value of ssl_no_validate, I assumed the default value was being assigned before.\nCorrect me if I'm wrong. Sorry, forget what I've said, I misunderstood your comment.. Yes, I'm actually using that authentication mechanism. cool, and thanks for the great support! \ud83d\ude04 . \ud83e\udd26\u200d\u2642\ufe0f Yes, completely right, sorry.\nActually, just out of curiosity, as they are booleans we can probably use kwargs['verify_certs'] = not kwargs['ssl_no_validate']. ",
    "4383": "@untergeek Done! :). Same refactor on all run commands . @untergeek No problems I keep it up-to-date with 5.x. ",
    "reswob10": "If this is true, the bottom of the README still needs to be fixed.\nIt says at https://github.com/elastic/curator#versioning:\nVersioning\nVersion 5 of Curator is the current master branch. It supports only 5.x versions of Elasticsearch.\n. ",
    "bosinm": "I can confirm that in Red Hat 7.0 this happens even when downgrading to Curator 5.3. Upgrading the openssl library version to 1.0.2 is a must. The documentation needs to also be updated. ",
    "CamiloSierraH": "i have the same issue, workaround if needed (updating \"openssl-libs\" package): \n$sudo yum update openssl-libs . ",
    "vanetix": "Having the same issue here on Ubuntu 16.04, running 5.4.1.\nThe above action file was running successfully on curator version 5.4.0, but after upgrading seems to just exit with the following output: \nbash\nroot@logstash-dev:~/.curator# curator prune-indices.yml\n2018-01-22 17:31:24,782 INFO      Preparing Action ID: 1, \"delete_indices\". Thanks for the reply - It seemed to be an issue with the 5.4.1 build in the debian repository, removing the debian package and installed with pip fixed the issue with no alterations to the configuration.. Sorry for the confusing wording, I was using elasticsearch-curator version 5.4.1 from the elasticsearch debian repository, https://packages.elastic.co/curator/5/debian stable main.. ",
    "TeroPihlaja": "I'm seeing similar error on Debian with the 5.4.1 package from the elastic repository.\ncli run: curator --config /root/curator-config.yml /root/curator-actions.yml\nLogs:\n2018-03-02 10:34:20,269 DEBUG                curator.cli                    run:108  Client and logging options validated.\n2018-03-02 10:34:20,269 DEBUG                curator.cli                    run:112  default_timeout = 30\n2018-03-02 10:34:20,270 DEBUG                curator.cli                    run:116  action_file: /root/curator-actions.yml\n2018-03-02 10:34:20,278 DEBUG                curator.cli                    run:118  action_config: {'actions': {1: {'action': 'delete_indices', 'description': 'Delete old indices.', 'options': {'ignore_empty_list': True, 'timeout_override': None, 'continue_if_exception': False, 'disable_action': False}, 'filters': [{'filtertype': 'pattern', 'kind': 'prefix', 'value': 'airrep2-', 'exclude': None}, {'filtertype': 'age', 'source': 'name', 'direction': 'older', 'timestring': '%Y.%m', 'unit': 'months', 'unit_count': 6, 'exclude': None}]}}}\n2018-03-02 10:34:20,278 DEBUG     curator.validators.SchemaCheck               __init__:26   Schema: {'actions': <class 'dict'>}\n2018-03-02 10:34:20,279 DEBUG     curator.validators.SchemaCheck               __init__:27   \"Actions File\" config: {'actions': {1: {'action': 'delete_indices', 'description': 'Delete old indices.', 'options': {'ignore_empty_list': True, 'timeout_override': None, 'continue_if_exception': False, 'disable_action': False}, 'filters': [{'filtertype': 'pattern', 'kind': 'prefix', 'value': 'airrep2-', 'exclude': None}, {'filtertype': 'age', 'source': 'name', 'direction': 'older', 'timestring': '%Y.%m', 'unit': 'months', 'unit_count': 6, 'exclude': None}]}}}\n2018-03-02 10:34:20,279 DEBUG     curator.validators.SchemaCheck               __init__:26   Schema: {'action': Any([In(['alias', 'allocation', 'close', 'cluster_routing', 'create_index', 'delete_indices', 'delete_snapshots', 'forcemerge', 'index_settings', 'open', 'reindex', 'replicas', 'restore', 'rollover', 'shrink', 'snapshot'])])}\n2018-03-02 10:34:20,279 DEBUG     curator.validators.SchemaCheck               __init__:27   \"action type\" config: {'action': 'delete_indices', 'description': 'Delete old indices.', 'options': {'ignore_empty_list': True, 'timeout_override': None, 'continue_if_exception': False, 'disable_action': False}, 'filters': [{'filtertype': 'pattern', 'kind': 'prefix', 'value': 'airrep2-', 'exclude': None}, {'filtertype': 'age', 'source': 'name', 'direction': 'older', 'timestring': '%Y.%m', 'unit': 'months', 'unit_count': 6, 'exclude': None}]}\n2018-03-02 10:34:20,280 DEBUG     curator.validators.SchemaCheck               __init__:26   Schema: {'action': Any([In(['alias', 'allocation', 'close', 'cluster_routing', 'create_index', 'delete_indices', 'delete_snapshots', 'forcemerge', 'index_settings', 'open', 'reindex', 'replicas', 'restore', 'rollover', 'shrink', 'snapshot'])]), 'description': Any([<class 'str'>, <class 'str'>]), 'options': <class 'dict'>, 'filters': <class 'list'>}\n2018-03-02 10:34:20,280 DEBUG     curator.validators.SchemaCheck               __init__:27   \"structure\" config: {'action': 'delete_indices', 'description': 'Delete old indices.', 'options': {'ignore_empty_list': True, 'timeout_override': None, 'continue_if_exception': False, 'disable_action': False}, 'filters': [{'filtertype': 'pattern', 'kind': 'prefix', 'value': 'airrep2-', 'exclude': None}, {'filtertype': 'age', 'source': 'name', 'direction': 'older', 'timestring': '%Y.%m', 'unit': 'months', 'unit_count': 6, 'exclude': None}]}\n2018-03-02 10:34:20,291 DEBUG     curator.validators.SchemaCheck               __init__:26   Schema: {'continue_if_exception': Any([<class 'bool'>, All(Any([<class 'str'>, <class 'str'>]), <function Boolean at 0x7f05ec837950>, msg=None)]), 'disable_action': Any([<class 'bool'>, All(Any([<class 'str'>, <class 'str'>]), <function Boolean at 0x7f05ec837e18>, msg=None)]), 'ignore_empty_list': Any([<class 'bool'>, All(Any([<class 'str'>, <class 'str'>]), <function Boolean at 0x7f05ec839378>, msg=None)]), 'timeout_override': Any([Coerce(int, msg=None), None])}\n2018-03-02 10:34:20,292 DEBUG     curator.validators.SchemaCheck               __init__:27   \"options\" config: {'ignore_empty_list': True, 'continue_if_exception': False, 'disable_action': False}\n2018-03-02 10:34:20,292 DEBUG     curator.validators.SchemaCheck               __init__:26   Schema: <function Filters.<locals>.f at 0x7f05ec86e730>\n2018-03-02 10:34:20,292 DEBUG     curator.validators.SchemaCheck               __init__:27   \"filters\" config: [{'filtertype': 'pattern', 'kind': 'prefix', 'value': 'airrep2-', 'exclude': None}, {'filtertype': 'age', 'source': 'name', 'direction': 'older', 'timestring': '%Y.%m', 'unit': 'months', 'unit_count': 6, 'exclude': None}]\n2018-03-02 10:34:20,293 DEBUG     curator.validators.SchemaCheck               __init__:26   Schema: {'filtertype': Any([In(['age', 'alias', 'allocated', 'closed', 'count', 'forcemerged', 'kibana', 'none', 'opened', 'pattern', 'period', 'space', 'state'])]), 'kind': Any(['prefix', 'suffix', 'timestring', 'regex']), 'value': Any([<class 'str'>, <class 'str'>]), 'exclude': Any([<class 'bool'>, All(Any([<class 'str'>, <class 'str'>]), <function Boolean at 0x7f05ec86e268>, msg=None)])}\n2018-03-02 10:34:20,293 DEBUG     curator.validators.SchemaCheck               __init__:27   \"filter\" config: {'filtertype': 'pattern', 'kind': 'prefix', 'value': 'airrep2-'}\n2018-03-02 10:34:20,293 DEBUG     curator.validators.filters                      f:48   Filter #0: {'filtertype': 'pattern', 'kind': 'prefix', 'value': 'airrep2-', 'exclude': False}\n2018-03-02 10:34:20,294 DEBUG     curator.defaults.filtertypes                    age:56   AGE FILTER = [{'direction': Any(['older', 'younger'])}, {'unit': Any(['seconds', 'minutes', 'hours', 'days', 'weeks', 'months', 'years'])}, {'unit_count': Coerce(int, msg=None)}, {'unit_count_pattern': Any([<class 'str'>, <class 'str'>])}, {'epoch': Any([Coerce(int, msg=None), None])}, {'exclude': Any([<class 'bool'>, All(Any([<class 'str'>, <class 'str'>]), <function Boolean at 0x7f05ec821bf8>, msg=None)])}, {'source': Any(['name', 'creation_date', 'field_stats'])}, {'stats_result': Any(['min_value', 'max_value'])}, {'timestring': Any([<class 'str'>, <class 'str'>])}]\n2018-03-02 10:34:20,295 DEBUG     curator.validators.SchemaCheck               __init__:26   Schema: {'filtertype': Any([In(['age', 'alias', 'allocated', 'closed', 'count', 'forcemerged', 'kibana', 'none', 'opened', 'pattern', 'period', 'space', 'state'])]), 'direction': Any(['older', 'younger']), 'unit': Any(['seconds', 'minutes', 'hours', 'days', 'weeks', 'months', 'years']), 'unit_count': Coerce(int, msg=None), 'unit_count_pattern': Any([<class 'str'>, <class 'str'>]), 'epoch': Any([Coerce(int, msg=None), None]), 'exclude': Any([<class 'bool'>, All(Any([<class 'str'>, <class 'str'>]), <function Boolean at 0x7f05ec821bf8>, msg=None)]), 'source': Any(['name', 'creation_date', 'field_stats']), 'stats_result': Any(['min_value', 'max_value']), 'timestring': Any([<class 'str'>, <class 'str'>])}\n2018-03-02 10:34:20,295 DEBUG     curator.validators.SchemaCheck               __init__:27   \"filter\" config: {'filtertype': 'age', 'source': 'name', 'direction': 'older', 'timestring': '%Y.%m', 'unit': 'months', 'unit_count': 6}\n2018-03-02 10:34:20,295 DEBUG     curator.validators.filters                      f:48   Filter #1: {'filtertype': 'age', 'source': 'name', 'direction': 'older', 'timestring': '%Y.%m', 'unit': 'months', 'unit_count': 6, 'stats_result': 'min_value', 'epoch': None, 'exclude': False}\n2018-03-02 10:34:20,295 DEBUG                curator.cli                    run:121  Full list of actions: {1: {'action': 'delete_indices', 'description': 'Delete old indices.', 'options': {'ignore_empty_list': True, 'continue_if_exception': False, 'disable_action': False, 'timeout_override': None}, 'filters': [{'filtertype': 'pattern', 'kind': 'prefix', 'value': 'airrep2-', 'exclude': False}, {'filtertype': 'age', 'source': 'name', 'direction': 'older', 'timestring': '%Y.%m', 'unit': 'months', 'unit_count': 6, 'stats_result': 'min_value', 'epoch': None, 'exclude': False}]}}\n2018-03-02 10:34:20,296 DEBUG                curator.cli                    run:126  action_disabled = False\n2018-03-02 10:34:20,296 DEBUG                curator.cli                    run:130  continue_if_exception = False\n2018-03-02 10:34:20,296 DEBUG                curator.cli                    run:132  timeout_override = None\n2018-03-02 10:34:20,296 DEBUG                curator.cli                    run:134  ignore_empty_list = True\n2018-03-02 10:34:20,296 INFO                 curator.cli                    run:144  Preparing Action ID: 1, \"delete_indices\"\n2018-03-02 10:34:20,296 DEBUG              curator.utils             get_client:803  kwargs = {'hosts': ['localhost'], 'port': 9200, 'use_ssl': False, 'ssl_no_validate': False, 'http_auth': '<removed>', 'master_only': False, 'client_key': None, 'client_cert': None, 'certificate': None, 'aws_key': None, 'aws_sign_request': False, 'url_prefix': '', 'aws_token': None, 'aws_secret_key': None, 'timeout': 30}\n2018-03-02 10:34:20,303 DEBUG              curator.utils             get_client:880  Not using \"requests_aws4auth\" python module to connect.\ncurator-actions.yml:\nactions:\n  1:\n    action: delete_indices\n    description: >-\n      Delete old indices.\n    options:\n      ignore_empty_list: True\n      timeout_override:\n      continue_if_exception: False\n      disable_action: False\n    filters:\n    - filtertype: pattern\n      kind: prefix\n      value: airrep2-\n      exclude:\n    - filtertype: age\n      source: name\n      direction: older\n      timestring: '%Y.%m'\n      unit: months\n      unit_count: 6\n      exclude:\ncurator-config.yml:\n```\nclient:\n  hosts:\n    - localhost\n  port: 9200\n  url_prefix:\n  use_ssl: False\n  certificate:\n  client_cert:\n  client_key:\n  ssl_no_validate: False\n  http_auth: \n  timeout: 30\n  master_only: False\nlogging:\n  loglevel: DEBUG\n  logfile: /var/log/curator\n  logformat: default\n  blacklist: ['elasticsearch', 'urllib3']\n```. Looks like our issue was caused by incorrect x-pack permissions.. ",
    "Aketzu": "In @TeroPihlaja case it was just missing permissions. We have X-Pack with security and curator user requires\n Cluster privileges: monitor\n Indices: \n Index privileges: monitor, delete_index. ",
    "sbellone": "Hi, it's indeed a configuration problem, but having an error message about unauthorized access, SSL failure, etc... could be a good improvement, it's misleading to have it failing silently.. I was not talking about the formatting, but the configuration itself. For example if you make a mistake in the path of the certificate, you'll get no output even in DEBUG, it just fails silently.. ",
    "andrassy": "+1. ",
    "efazenda": "+1. ",
    "dguendisch": "Currently also struggling with curator's alias action not having the remove_index option of elasticsearch's alias action to swap an index test with an alias test pointing to some other index.\nMakes it hard to do a bullet-proof migration of data .... ",
    "queszama": "I installed curator which is for RHEL 6 and this seems is working\n$sudo rpm -ivh https://packages.elastic.co/curator/5/centos/6/Packages/elasticsearch-curator-5.4.1-1.x86_64.rpm\nRetrieving https://packages.elastic.co/curator/5/centos/6/Packages/elasticsearch-curator-5.4.1-1.x86_64.rpm\nPreparing...                          ################################# [100%]\nUpdating / installing...\n   1:elasticsearch-curator-5.4.1-1    ################################# [100%]\n$/usr/bin/curator\nUsage: curator [OPTIONS] ACTION_FILE\nError: Missing argument \"action_file\".\n. ",
    "deiwin": "\nThis is necessary behavior, unfortunately. It's not about indices without docs, it's about indices where field is not present. This behavior is documented:\n\nThis field must be present in the indices being filtered or an exception will be raised, and execution will halt.\n\n\nCould the error logging be improved for this case? Instead of an obscure TypeError it could say that the field is missing (aggregation returned null) and it could include the name of the index that failed. Would greatly simplify debugging.. ",
    "pratikjoy7": "@untergeek Thanks!. ",
    "rewiko": "I think for any cluster with a big number of indices, your payload will become bigger and bigger. At some point we will reach the limit for the payload size. I was thinking to do it in a smarter way, fetching a number x of indices per requests. Then we will have a few requests to get metadata. \n . @untergeek Actually the cluster is quite small, 7 nodes 32 gb, number of index is 420. I think It's more related to the number of index and the mappings associated to (not a big mapping definition), curator will get the metadata for all the indices in one request.\nThe parameter to modify is \nhttp.max_content_length | The max content of an HTTP request. Defaults to 100mb. If set to greater than Integer.MAX_VALUE, it will be reset to 100mb.\n-- | -- \nsource https://www.elastic.co/guide/en/elasticsearch/reference/5.5/modules-http.html\nWe are currently using the Instance type r3.2xlarge.elasticsearch where the http.max_content_length is 100 MB.\n. Number of shard is 4 per index and 0 replica.\nexample of mapping\n{\n      \"template\": \"logs-{{ env }}-*\",\n      \"settings\": {\n        \"number_of_shards\": 4,\n        \"number_of_replicas\": 0\n      },\n      \"mappings\": {\n        \"flb_type\": {\n          \"dynamic\": true,\n          \"dynamic_templates\": [\n            {\n              \"custom_app_fields\" : {\n                \"match\" : \"*\",\n                \"match_mapping_type\" : \"string\",\n                \"mapping\": { \"type\": \"keyword\" }\n              }\n            }\n          ],\n          \"properties\": {\n            \"message\": {\n              \"type\": \"text\"\n            },\n            \"@shipper-timestamp\": {\n              \"type\": \"date\",\n              \"format\": \"strict_date_optional_time||epoch_millis\"\n            },\n            \"@timestamp\": {\n              \"type\": \"date\",\n              \"format\": \"strict_date_optional_time||epoch_millis\"\n            },\n            \"docker\": {\n              \"properties\": {\n                \"container_id\": {\n                  \"type\": \"keyword\"\n                }\n              }\n            },\n            \"kubernetes\": {\n              \"properties\": {\n                \"annotations\": {\n                  \"type\": \"object\"\n                },\n                \"container_name\": {\n                  \"type\": \"keyword\"\n                },\n                \"host\": {\n                  \"type\": \"keyword\"\n                },\n                \"labels\": {\n                  \"properties\": {\n                    \"app\": {\n                      \"type\": \"keyword\"\n                    },\n                    \"deployment\": {\n                      \"type\": \"keyword\"\n                    },\n                    \"k8s-app\": {\n                      \"type\": \"keyword\"\n                    },\n                    \"kubernetes_io/cluster-service\": {\n                      \"type\": \"keyword\"\n                    },\n                    \"pod-template-hash\": {\n                      \"type\": \"keyword\"\n                    },\n                    \"version\": {\n                      \"type\": \"keyword\"\n                    }\n                  }\n                },\n                \"cluster\": {\n                  \"type\": \"keyword\"\n                },\n                \"namespace_name\": {\n                  \"type\": \"keyword\"\n                },\n                \"pod_id\": {\n                  \"type\": \"keyword\"\n                },\n                \"pod_name\": {\n                  \"type\": \"keyword\"\n                }\n              }\n            },\n            \"log\": {\n              \"type\": \"text\"\n            },\n            \"stream\": {\n              \"type\": \"keyword\"\n            }\n          }\n        }\n      }\n    }\nconfig curator action:\nactions:\n      1:\n        action: delete_indices\n        description: \"Delete indices older than 21 of days\"\n        options:\n          ignore_empty_list: True\n          continue_if_exception: False\n        filters:\n            - filtertype: age\n              source: name\n              direction: older\n              timestring: '%Y.%m.%d'\n              unit: days\n              unit_count: 21\nthat's means we query \"_cluster/state/metadata/\" with 400 index in on go, so it get bigger than 100MB.\nA bulk option to fetch metadata per X (ex:10 by 10)makes more sense for me.. @untergeek ok let me update the PR. I can't reproduce locally the huge payload even with hundred of indices, let's close it for the moment.. I was able to reproduce the bug, I've updated the PR.. ",
    "hansbogert": "@untergeek I did add that. Please see the supplied curator action file https://gist.github.com/hansbogert/97d16b7c8fbb001d181b47ccddb9a638#file-shrink-yml-L14\nNote that the lines you picked from my log would not have shown in the first place if the permit_masters: true was not set. In fact, the lines you picked from  log even explicitly show that I set permit_masters to true:\nNot skipping node \"node-0\" which is a master node (not recommended), *but permit_masters is True*\n\n. Okay.. checking node logs doesn't show anything. Doing an API call to move a shard actually fails, about version mismatch. I updated the only node, (the only node that actually could do the shrink), and indeed it works now. If the API call I did to move a shard manually shows an error, couldn't curator have catched that? \n/update Why does a minor release version difference actually matter? . No time to reproduce precisely for now. If I get hit by this again I'll look at the error/pass given by elasticsearch and see if that makes sense semantically from a Elasticsearch perspective and possibly file a bug report there.\nThanks for thorough handling of this issue.. ",
    "satish-chef": "@untergeek Thank you very much for your reply.\nI un-installed and re-installed the rpm of elasticsearch-curator-4.2.6-1.x86_64 three times but it didn't help.\nI ran all the 3 command that you gave and it has the same results from both the nodes:\n```\nResults from old node:\n[root@elasticsearch-01 ~]# ls -l /opt/elasticsearch-curator//curator\n-rwxrwxr-x 1 root root 2633352 Jan 27  2017 /opt/elasticsearch-curator//curator\n[root@elasticsearch-01 ~]# file /opt/elasticsearch-curator//curator\n/opt/elasticsearch-curator//curator: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.18, stripped\n[root@elasticsearch-01 ~]# md5sum /opt/elasticsearch-curator//curator\n6427474f1fc8d25af26f549d476cc4cc  /opt/elasticsearch-curator//curator\n\nResults from new node\n[root@elasticsearch-02 ~]# ls -l /opt/elasticsearch-curator//curator\n-rwxrwxr-x 1 root root 2633352 Jan 27  2017 /opt/elasticsearch-curator//curator\n[root@elasticsearch-02 ~]# file /opt/elasticsearch-curator/curator\n/opt/elasticsearch-curator/curator: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.18, stripped\n[root@elasticsearch-02 ~]# md5sum /opt/elasticsearch-curator/curator\n6427474f1fc8d25af26f549d476cc4cc  /opt/elasticsearch-curator/curator\nThe \"e\" is the extent format - https://en.wikipedia.org/wiki/Chattr#Attributes\nThat \"e\" is not relevant because the binary /opt/elasticsearch-curator/curator_cli has the same attribute but is still executable using the symink present in /usr/bin/:\n[ Apologies for providing extra but irrelevant info, i thought this might be the issue, but its not ]\nResults from new node\n[root@elasticsearch-02 ~]# /usr/bin/curator_cli\nUsage: curator_cli [OPTIONS] COMMAND [ARGS]...\nOptions:\n  --config PATH       Path to configuration file. Default:\n                      ~/.curator/curator.yml\n  --host TEXT         Elasticsearch host.\n  --url_prefix TEXT   Elasticsearch http url prefix.\n  --port TEXT         Elasticsearch port.\n  --use_ssl           Connect to Elasticsearch through SSL.\n  --certificate TEXT  Path to certificate to use for SSL validation.\n  --client-cert TEXT  Path to file containing SSL certificate for client auth.\n  --client-key TEXT   Path to file containing SSL key for client auth.\n  --ssl-no-validate   Do not validate SSL certificate\n  --http_auth TEXT    Use Basic Authentication ex: user:pass\n  --timeout INTEGER   Connection timeout in seconds.\n  --master-only       Only operate on elected master node.\n  --dry-run           Do not perform any changes.\n  --loglevel TEXT     Log level\n  --logfile TEXT      log file\n  --logformat TEXT    Log output format [default|logstash|json].\n  --version           Show the version and exit.\n  --help              Show this message and exit.\nCommands:\n  allocation        Shard Routing Allocation\n  close             Close indices\n  delete_indices    Delete indices\n  delete_snapshots  Delete snapshots\n  forcemerge        forceMerge index/shard segments\n  open              Open indices\n  replicas          Change replica count\n  show_indices      Show indices\n  show_snapshots    Show snapshots\n  snapshot          Snapshot indices\n[root@elasticsearch-02 ~]# ls -lrth /usr/bin/curator_cli\nlrwxrwxrwx 1 root root 38 Jan 26 17:20 /usr/bin/curator_cli -> /opt/elasticsearch-curator/curator_cli\n[root@elasticsearch-02 ~]# file /opt/elasticsearch-curator/curator_cli\n/opt/elasticsearch-curator/curator_cli: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.18, stripped\n```\nBelow is the list of libc packages installed on the node on which i created rpm for elasticsearch-curator(i used fpm tool to create the rpm):\n[root@rpm-server ~]# rpm -qa  | grep -i libc\nglibc-headers-2.12-1.166.el6_7.7.x86_64\nglibc-common-2.12-1.166.el6_7.7.x86_64\nlibcgroup-0.40.rc1-5.el6_5.1.x86_64\nlibcom_err-1.41.12-18.el6_5.1.x86_64\nlibcap-ng-0.6.4-3.el6_0.1.x86_64\nlibcap-2.16-5.5.el6.x86_64\nglibc-devel-2.12-1.166.el6_7.7.x86_64\nlibcurl-7.19.7-37.el6_5.3.x86_64\nglibc-2.12-1.166.el6_7.7.x86_64\nlibcom_err-devel-1.41.12-18.el6_5.1.x86_64\n[root@rpm-server ~]# more /etc/system-release\nCentOS release 6.5 (Final)\n. Yes you read that correctly, curator_cli is getting executed correctly but curator is not.\nWhen i execute /opt/elasticsearch-curator/curator directly, it runs fine.  As a temporary patch, i have setup the cron to use this binary, but frankly i need the symlink /usr/bin/curator to work as it used to work on the old instance :\n```\n[root@ elasticsearch-02 ~]# /opt/elasticsearch-curator/curator\nUsage: curator [OPTIONS] ACTION_FILE\nError: Missing argument \"action_file\".\n``. I tried deleting the creating the symlink/usr/bin/curator`.  I also tried creating symlink on other paths on the instance, but none of it works.\nIs it possible to compile the rpm for elasticsearch-curator-4.2.6 for Centos 6.9 ?\nIf yes, can you please share the steps, i will spin up an instance and create the rpm using fpm.. ",
    "joernott": "We have the same problem. A colleague of mine tested the following:\n- Relative symlink works as intended, the absolute symlink doesn't\n- curator residing on XFS file system or on ext4 does not change anything\nVersion in use: 4.2.6 on RHEL 7.4. ",
    "jamshid": "FWIW upgrading to elasticsearch-curator 4.3.1 (that's the latest in repo http://packages.elastic.co/curator/4/centos/6 and 7) fixed this weird symlink problem.. ",
    "arne-cl": "I have signed the CLA now.. Dear @untergeek, I changed the merge branch as requested.. Sorry for the delay. This seems like an awful lot of work just to merge a one-word change\nin the documentation.. Dear Aaron,\nI just saw that there is a Allow edits from maintainers checkbox for pull requests.\nI guess this would have solved our problem, but I never noticed it before.\nBest,\nArne. ",
    "fpalladoro": "I've done a little more research and figured out that curator is already checking for an empty list after every filter and in most of the actions, except for the reindex. I think this is related to the possibility of a remote source (https://github.com/elastic/curator/blob/master/curator/actions.py#L1047).\nConsidering the Elasticsearch behavior on a reindex with an empty source, I think it's necessary to add the empty list check for the reindex action. I'm not sure about the implications of the remote source case but I suppose that it's possible the exclude the verification in that escenario \n. ",
    "rossengeorgiev": "I'm using ES 5.6.3 and I get the same error. That is great since reindexing everything is DEFINITELY not what I want. However, since it errors out any further actions do not get executed.\n```bash\ncurator --version\ncurator, version 5.4.1\n/usr/share/elasticsearch/bin/elasticsearch --version\nVersion: 5.6.3, Build: 1a2f265/2017-10-06T20:33:39.012Z, JVM: 1.8.0_151\ncurator --dry-run --config curator.yml actions.yaml\n2018-02-07 15:15:47,237 INFO      Preparing Action ID: 3, \"reindex\"\n2018-02-07 15:15:47,258 INFO      Trying Action ID: 3, \"reindex\": description text\n2018-02-07 15:15:47,369 INFO      DRY-RUN MODE.  No changes will be made.\n2018-02-07 15:15:47,369 INFO      DRY-RUN: REINDEX: request body: {'source': {'index': []}, 'dest': {'index': 'mydestindex-2018'}} with arguments: refresh=True requests_per_second=-1 slices=1 timeout=6\n0s wait_for_active_shards=1 wait_for_completion=True\n2018-02-07 15:15:47,369 INFO      Action ID: 3, \"reindex\" completed.\n2018-02-07 15:15:47,370 INFO      Job completed.\n2018-02-07 15:15:47,471 INFO      Preparing Action ID: 4, \"delete_indices\"\n...\ncurator --config curator.yml actions.yaml\n2018-02-07 15:15:53,937 INFO      Preparing Action ID: 3, \"reindex\"\n2018-02-07 15:15:53,948 INFO      Trying Action ID: 3, \"reindex\": description text\n2018-02-07 15:15:54,021 INFO      Commencing reindex operation\n2018-02-07 15:15:54,027 ERROR     Failed to complete action: reindex.  : Exception encountered.  Rerun with loglevel DEBUG and/or check Elasticsearch logs for mor\ne information. Exception: TransportError(400, 'action_request_validation_exception', 'Validation Failed: 1: use _all if you really want to copy from all existing indexes;')\n. ",
    "evilezh": "I just put idea. \nFor me it worked - just patching that file.\nDockerfile\nFROM python:3-alpine\nWORKDIR /curator\nCOPY size.patch /\nRUN ln -s /curator /root/.curator && \\\n    pip install elasticsearch-curator==5.4.1 && \\\n    cd /usr/local/lib/python3.6/site-packages/curator/defaults && \\\n    patch < /size.patch\nENTRYPOINT [\"/usr/local/bin/curator\"]\nAs regards to version checking, you can just rely on elasticsearch itself - older version will throw error, which can be forwarded. . ",
    "LucaWintergerst": "@untergeek done . ",
    "woodlee": "@untergeek Yep, was just trying to clean things up for conciseness. The real file starts like so:\nactions:\n  1:\n    action: snapshot\n    description: Snapshot production request event indices\n    options:\n      repository: s3_snapshot_repository\n      name: prod_request_event_v1-%Y%m%d%H%M%S\n    filters:\n    - filtertype: alias\n      aliases: prod_request_event_v1\n  2:\n    action: rollover\n    description: Rollover large request event indices (prod)\n    options:\n      name: prod_request_event_v1_writes\n      conditions:\n        max_docs: 13000000  # Targeting 10GB/shard\n  3:\n<...several other actions...>\nEverything was working fine for us as well on Voluptuous 0.10.5, but sometime in the past 24 hours they published a new version 0.11.1 to PyPI which I believe to be the issue. I intend to verify this soon by pinning the Voluptuous version in our own local pip requirements, but am having to wait for another cluster maintenance task to finish first.\n. I can confirm that pinning the version of Voluptuous to 0.10.5 in our local virtualenv resolves the issue.. ",
    "matthagenbuch": "+1 regarding volutpuous==0.10.5. My bad, I didn't realize a newer version was released.  This can be closed. ",
    "baddogdown": "+1 getting this issue.. ",
    "nguyening": "@untergeek - is there going to be an associated release?. ",
    "Krieffer": "So, I'm using the \"DEBUG\" to show the logs that Curator is \"seeing\". And it's has only two ['myindex', 'megacorp'].\nI'm running the \"delete_indices.yml\" command.\nMy config file (those numbers are fictional):\n```\nRemember, leave a key empty if there is no value.  None will be a string,\nnot a Python \"NoneType\"\nclient:\n  hosts:\n    - 127.0.0.1\n  port: 9200\nlogging:\n  loglevel: DEBUG\n  logfile:\n  logformat: default\n  blacklist: ['elasticsearch', 'urllib3']\n```\nI have some indexes in Elasticsearch, and the Curator doesn't seeing them.. ```\nRemember, leave a key empty if there is no value.  None will be a string,\nnot a Python \"NoneType\"\n\nAlso remember that all examples have 'disable_action' set to True.  If you\nwant to use this action as a template, be sure to set this to False after\ncopying it.\nactions:\n  1:\n    action: delete_indices\n    description: >-\n      Delete indices older than 1 minute (based on index name), for any\n      prefixed indices. Ignore the error if the filter does not result in an\n      actionable list of indices (ignore_empty_list) and exit cleanly.\n    options:\n      ignore_empty_list: True\n      timeout_override:\n      continue_if_exception: False\n      disable_action: False\n    filters:\n    - filtertype: pattern\n      kind: prefix\n      value: elastalert\n      exclude: False\n    - filtertype: age\n      source: creation_date\n      direction: older\n      unit: minutes\n      unit_count: 1\n```. I removed the all the \"pattern\" and it still seeing just two indexes. \n```\n2018-02-15 16:09:00,728 DEBUG              curator.utils            get_indices:644  Detected Elasticsearch version 6.1.2\n2018-02-15 16:09:00,728 DEBUG              curator.utils            get_indices:646  All indices: ['myindex', 'megacorp']\n```. So...\n```\n2018-02-15 16:25:06,743 DEBUG     curator.validators.SchemaCheck               init:26   Schema: {'actions': }\n2018-02-15 16:25:06,743 DEBUG     curator.validators.SchemaCheck               init:27   \"Actions File\" config: {'actions': {1: {'action': 'delete_indices', 'description': 'Delete indices older than 1 minute (based on index name), for any prefixed indices. Ignore the error if the filter does not result in an actionable list of indices (ignore_empty_list) and exit cleanly.', 'options': {'ignore_empty_list': True, 'timeout_override': None, 'continue_if_exception': False, 'disable_action': False}, 'filters': [{'filtertype': 'none'}]}}}\n2018-02-15 16:25:06,743 DEBUG     curator.validators.SchemaCheck               init:26   Schema: {'action': Any([In(['alias', 'allocation', 'close', 'cluster_routing', 'create_index', 'delete_indices', 'delete_snapshots', 'forcemerge', 'index_settings', 'open', 'reindex', 'replicas', 'restore', 'rollover', 'shrink', 'snapshot'])])}\n2018-02-15 16:25:06,743 DEBUG     curator.validators.SchemaCheck               init:27   \"action type\" config: {'action': 'delete_indices', 'description': 'Delete indices older than 1 minute (based on index name), for any prefixed indices. Ignore the error if the filter does not result in an actionable list of indices (ignore_empty_list) and exit cleanly.', 'options': {'ignore_empty_list': True, 'timeout_override': None, 'continue_if_exception': False, 'disable_action': False}, 'filters': [{'filtertype': 'none'}]}\n2018-02-15 16:25:06,744 DEBUG     curator.validators.SchemaCheck               init:26   Schema: {'action': Any([In(['alias', 'allocation', 'close', 'cluster_routing', 'create_index', 'delete_indices', 'delete_snapshots', 'forcemerge', 'index_settings', 'open', 'reindex', 'replicas', 'restore', 'rollover', 'shrink', 'snapshot'])]), 'description': Any([, ]), 'options': , 'filters': }\n2018-02-15 16:25:06,744 DEBUG     curator.validators.SchemaCheck               init:27   \"structure\" config: {'action': 'delete_indices', 'description': 'Delete indices older than 1 minute (based on index name), for any prefixed indices. Ignore the error if the filter does not result in an actionable list of indices (ignore_empty_list) and exit cleanly.', 'options': {'ignore_empty_list': True, 'timeout_override': None, 'continue_if_exception': False, 'disable_action': False}, 'filters': [{'filtertype': 'none'}]}\n2018-02-15 16:25:06,750 DEBUG     curator.validators.SchemaCheck               init:26   Schema: {'continue_if_exception': Any([, All(Any([, ]), , msg=None)]), 'disable_action': Any([, All(Any([, ]), , msg=None)]), 'ignore_empty_list': Any([, All(Any([, ]), , msg=None)]), 'timeout_override': Any([Coerce(int, msg=None), None])}\n2018-02-15 16:25:06,750 DEBUG     curator.validators.SchemaCheck               init:27   \"options\" config: {'ignore_empty_list': True, 'continue_if_exception': False, 'disable_action': False}\n2018-02-15 16:25:06,750 DEBUG     curator.validators.SchemaCheck               init:26   Schema: .f at 0x7f861ac1bc80>\n2018-02-15 16:25:06,750 DEBUG     curator.validators.SchemaCheck               init:27   \"filters\" config: [{'filtertype': 'none'}]\n2018-02-15 16:25:06,751 DEBUG     curator.validators.SchemaCheck               init:26   Schema: {'filtertype': Any([In(['age', 'alias', 'allocated', 'closed', 'count', 'forcemerged', 'kibana', 'none', 'opened', 'pattern', 'period', 'space', 'state'])])}\n2018-02-15 16:25:06,751 DEBUG     curator.validators.SchemaCheck               init:27   \"filter\" config: {'filtertype': 'none'}\n2018-02-15 16:25:06,751 DEBUG     curator.validators.filters                      f:48   Filter #0: {'filtertype': 'none'}\n2018-02-15 16:25:06,751 DEBUG                curator.cli                    run:121  Full list of actions: {1: {'action': 'delete_indices', 'description': 'Delete indices older than 1 minute (based on index name), for any prefixed indices. Ignore the error if the filter does not result in an actionable list of indices (ignore_empty_list) and exit cleanly.', 'options': {'ignore_empty_list': True, 'continue_if_exception': False, 'disable_action': False, 'timeout_override': None}, 'filters': [{'filtertype': 'none'}]}}\n2018-02-15 16:25:06,751 DEBUG                curator.cli                    run:126  action_disabled = False\n2018-02-15 16:25:06,751 DEBUG                curator.cli                    run:130  continue_if_exception = False\n2018-02-15 16:25:06,751 DEBUG                curator.cli                    run:132  timeout_override = None\n2018-02-15 16:25:06,752 DEBUG                curator.cli                    run:134  ignore_empty_list = True\n2018-02-15 16:25:06,752 INFO                 curator.cli                    run:144  Preparing Action ID: 1, \"delete_indices\"\n2018-02-15 16:25:06,752 DEBUG              curator.utils             get_client:803  kwargs = {'hosts': ['173.173.0.16'], 'port': 9201, 'use_ssl': False, 'ssl_no_validate': False, 'master_only': False, 'client_cert': None, 'aws_key': None, 'aws_sign_request': False, 'aws_secret_key': None, 'http_auth': None, 'url_prefix': '', 'client_key': None, 'certificate': None, 'aws_token': None, 'timeout': 30}\n2018-02-15 16:25:06,752 DEBUG              curator.utils             get_client:880  Not using \"requests_aws4auth\" python module to connect.\n2018-02-15 16:25:06,759 DEBUG              curator.utils          check_version:689  Detected Elasticsearch version 6.1.2\n2018-02-15 16:25:06,759 DEBUG                curator.cli                    run:159  client is \n2018-02-15 16:25:06,759 INFO                 curator.cli                    run:165  Trying Action ID: 1, \"delete_indices\": Delete indices older than 1 minute (based on index name), for any prefixed indices. Ignore the error if the filter does not result in an actionable list of indices (ignore_empty_list) and exit cleanly.\n2018-02-15 16:25:06,759 DEBUG                curator.cli         process_action:44   Configuration dictionary: {'action': 'delete_indices', 'description': 'Delete indices older than 1 minute (based on index name), for any prefixed indices. Ignore the error if the filter does not result in an actionable list of indices (ignore_empty_list) and exit cleanly.', 'options': {}, 'filters': [{'filtertype': 'none'}]}\n2018-02-15 16:25:06,759 DEBUG                curator.cli         process_action:45   kwargs: {'master_timeout': 30, 'dry_run': True}\n2018-02-15 16:25:06,759 DEBUG                curator.cli         process_action:50   opts: {}\n2018-02-15 16:25:06,759 DEBUG                curator.cli         process_action:62   Action kwargs: {'master_timeout': 30}\n2018-02-15 16:25:06,759 DEBUG                curator.cli         process_action:91   Running \"DELETE_INDICES\"\n2018-02-15 16:25:06,760 DEBUG          curator.indexlist          get_indices:66   Getting all indices\n2018-02-15 16:25:06,764 DEBUG              curator.utils            get_indices:644  Detected Elasticsearch version 6.1.2\n2018-02-15 16:25:06,764 DEBUG              curator.utils            get_indices:646  All indices: ['myindex', 'megacorp']\n2018-02-15 16:25:06,764 DEBUG          curator.indexlist     __build_index_info:81   Building preliminary index metadata for myindex\n2018-02-15 16:25:06,764 DEBUG          curator.indexlist     __build_index_info:81   Building preliminary index metadata for megacorp\n2018-02-15 16:25:06,764 DEBUG          curator.indexlist          _get_metadata:147  Getting index metadata\n2018-02-15 16:25:06,764 DEBUG          curator.indexlist       empty_list_check:184  Checking for empty list\n2018-02-15 16:25:06,768 DEBUG          curator.indexlist       _get_index_stats:115  Getting index stats\n2018-02-15 16:25:06,768 DEBUG          curator.indexlist       empty_list_check:184  Checking for empty list\n2018-02-15 16:25:06,768 DEBUG          curator.indexlist           working_list:195  Generating working list of indices\n2018-02-15 16:25:06,768 DEBUG          curator.indexlist           working_list:195  Generating working list of indices\n2018-02-15 16:25:06,772 DEBUG          curator.indexlist     iterate_over_stats:124  Index: megacorp  Size: 6.8KB  Docs: 1\n2018-02-15 16:25:06,772 DEBUG          curator.indexlist     iterate_over_stats:124  Index: myindex  Size: 466.0B  Docs: 0\n2018-02-15 16:25:06,772 DEBUG          curator.indexlist        iterate_filters:1059 Iterating over a list of filters\n2018-02-15 16:25:06,772 DEBUG          curator.indexlist        iterate_filters:1065 All filters: [{'filtertype': 'none'}]\n2018-02-15 16:25:06,772 DEBUG          curator.indexlist        iterate_filters:1067 Top of the loop: ['myindex', 'megacorp']\n2018-02-15 16:25:06,772 DEBUG          curator.indexlist        iterate_filters:1068 Un-parsed filter args: {'filtertype': 'none'}\n2018-02-15 16:25:06,774 DEBUG     curator.validators.SchemaCheck               __init:26   Schema: {'aliases': Any([, [], , []]), 'allocation_type': Any([, ]), 'count': Coerce(int, msg=None), 'date_from': Any([, , None]), 'date_from_format': Any([, , None]), 'date_to': Any([, , None]), 'date_to_format': Any([, , None]), 'direction': Any([, ]), 'disk_space': , 'epoch': Any([Coerce(int, msg=None), None]), 'exclude': Any([, , , , None]), 'field': Any([, , None]), 'intersect': Any([, , , , None]), 'key': Any([, ]), 'kind': Any([, ]), 'max_num_segments': Coerce(int, msg=None), 'pattern': Any([, ]), 'period_type': Any([, ]), 'reverse': Any([, , , , None]), 'range_from': Coerce(int, msg=None), 'range_to': Coerce(int, msg=None), 'source': Any([, ]), 'state': Any([, ]), 'stats_result': Any([, , None]), 'timestring': Any([, , None]), 'threshold_behavior': Any([, ]), 'unit': Any([, ]), 'unit_count': Coerce(int, msg=None), 'unit_count_pattern': Any([, ]), 'use_age': , 'value': Any([, , , , ]), 'week_starts_on': Any([, , None]), 'filtertype': Any([In(['age', 'alias', 'allocated', 'closed', 'count', 'forcemerged', 'kibana', 'none', 'opened', 'pattern', 'period', 'space', 'state'])])}\n2018-02-15 16:25:06,775 DEBUG     curator.validators.SchemaCheck               init:27   \"filter\" config: {'filtertype': 'none'}\n2018-02-15 16:25:06,775 DEBUG          curator.indexlist        iterate_filters:1075 Parsed filter args: {'filtertype': 'none'}\n2018-02-15 16:25:06,775 DEBUG          curator.indexlist            filter_none:735  \"None\" filter selected.  No filtering will be done.\n2018-02-15 16:25:06,775 DEBUG     curator.actions.delete_indices               init:468  master_timeout value: 30s\n2018-02-15 16:25:06,775 INFO               curator.utils           show_dry_run:914  DRY-RUN MODE.  No changes will be made.\n2018-02-15 16:25:06,775 INFO               curator.utils           show_dry_run:917  (CLOSED) indices may be shown that may not be acted on by action \"delete_indices\".\n2018-02-15 16:25:06,775 INFO               curator.utils           show_dry_run:924  DRY-RUN: delete_indices: megacorp with arguments: {}\n2018-02-15 16:25:06,775 INFO               curator.utils           show_dry_run:924  DRY-RUN: delete_indices: myindex with arguments: {}\n2018-02-15 16:25:06,776 INFO                 curator.cli                    run:194  Action ID: 1, \"delete_indices\" completed.\n2018-02-15 16:25:06,776 INFO                 curator.cli                    run:195  Job completed.\n```. Yep! That's the right host and port.. Oh my... what a pity. :(. So I can't create indexes in Kibana and delete them with Curator?. What advices can you give to me in this situation? I created a lot of indices in Kibana and they didn't show in Curator. What things I need to check to make everything connected?. ",
    "jantoniogarcia": "I guess i am missing something, i know i have to improve my Elastic knowledge.\nBut, taking in consideration the shard size recomendations, it is hard to take any action to prevent that growth if you can not use the index or shard size as a determining characteristic.\nAnother example is the Rollover API.  It has a max_size condition filter (for 6.X versions). I guess curator does not has it for 5.X compatibility.\nMaybe i need a hint on how i have to work with my data and curator to prevent that shard growth, instead of adding some custom scripts in between curators jobs.\nThank you\n. ",
    "kunisen": "Thanks!! @untergeek . ",
    "wanix": "Error rebasing my branch. I closed this one and created https://github.com/elastic/curator/pull/1172. ",
    "xargs": "Is this also applicable for SUSE 11 SP4 ? I am getting the same error.. ",
    "yangas": "@untergeek Thanks a bunch, hadn't realized the overlap and yes it was indeed both clusters writing to the same base_path within an s3 repo. Thanks for the quick resolution and guidance \ud83d\udc4d . ",
    "sravankumar777": "Thanks for detailed information @untergeek.\nWith other dependencies on our system, we are stuck with curator version of 3.4.0.\nHowever, we plan to upgrade with the latest version. . ",
    "duylong": "\"date math\" can't extract date from old indices, right ?. ",
    "erdarun": "@untergeek Reindex failed due to source index name mentioned was index alias name. After mentioning explicit source index name instead of alias it worked.\nI have query like after reindex operation source index will exist or not? \nBecause as per our testing source index still exists along with destination index.. ",
    "danielkasen": "ok, I can live with this for now and fix it when the new version is released. Thanks.. NVM found it as new_index as an option. Looks like the docs don't specify this though, had to dig through a commit: https://github.com/elastic/curator/commit/2bb0f377f5285028d5eb08f2db53ccb243eb8735. Also, important not for anyone trying this. It only accepts date format so you have to do something like \nnew_index: \"\". Ok, I guess I was under the impression that the conditions check was done before the check to see if the action could be performed. Thank you.. Ahh looks like some of this is in DEBUG:\n{\"@timestamp\": \"2018-08-07T21:51:59.775Z\", \"function\": \"wait_for_it\", \"linenum\": 1751, \"loglevel\": \"DEBUG\", \"message\": \"Elapsed time: 18 seconds\", \"name\": \"curator.utils\"} will dive into that.. Right, but it won't block another call. For instance if you run a FM API call against 1 index then another 1 it'll put the second call into the FM queue or start processing it if you have increased the FM thread_pool:\nGET _cat/thread_pool/force_merge?v&s=name\nAnd here is a snippet of increasing the thread_pool in the elasticsearch.yml:\nthread_pool:\n  force_merge:\n    size: \"2\"\n. So, i'm not sure if curator can fork here. Sry as I haven't dug into the code yet. But I would see it creating a child thread for each FM operation and letting us specify the number of child threads.. haha yea, I figure i'm sort of an outlier here and totally understand protecting other users. As far as the approach goes, like I said i'm unfamiliar with your code so I'm just brainstorming here. . Well I believe it'll block the call to that specific shard and prevent another call against that shard to proceed. But I'm just trying to have all shards that can FM do it at the same time or allow a node to FM 2 shards at once in my case. The shards are just from different indices. As far as disk IO goes I still have a lot of head room on these NVME devices. . I also think if you do a Curl API call, something like \nPOST *2018.09.30.*/_forcemerge?max_num_segments=5&flush=true\nIt will spin up a FM thread for every matched index, with some getting queued if it doesn't have a free thread.. Is there anyway to pass a string to this call or does it have to be built out as an filtered array? As that would be one way I can deal with this special case.. Sure, so if I do\nPOST test-2018.09.30/_forcemerge?max_num_segments=5&flush=true\nPOST test-2018.09.29/_forcemerge?max_num_segments=5&flush=true\nThen check the merge threads:\nGET _cat/thread_pool/force_merge?v&s=name\nI can see multiple running:\nnode_name           name        active queue rejected\nnode-1                  force_merge      2     0        0\nnode-2                  force_merge      2     0        0\nnode-3                  force_merge      2     0        0\nnode-4                 force_merge      2     0        0\nAnd after I go past my thread_pool.force_merge.size of 2 it'll start adding them to the queue\nIt's important to also note that if I had the 2 indexes but the shards were not on the same nodes then the FM operation would start on that segment too. For EX:\nnode-1 contains index-1 shard-0\nnode-2 contains index-1 shard-1\nnode-3 contains index-2 shard-0\nnode-4 contains index-2 shard-1\nThese would all run at the same time.. Most people would just see the job added to the queue as the thread_pool is 1 by default.. Ahh yes I see, if I use a wildcard it'll only execute the FM operation to 1 thread, but it'll pick any shard that's available so this becomes more spread out in my cluster as all nodes are running an operation at once vs. only the nodes that have test-index-2018.09.30 it isn't adding all the other shards into the queue in this case so it doesn't take advantage of multiple thread_pools.\noption 1) Allow wildcards in the call to allow all nodes to contribute at the same time\noption 2) Allow the returned array by curator to be parallelized so that multiple POST operations happen at the same time.. Yup, on a node with 2 FM threads going:\n{\n  \"nodes\": {\n    \"GdJF3lveTeu8HGxXNv4zig\": {\n      \"name\": \"XXXX\",\n      \"transport_address\": \"XXXX\",\n      \"host\": \"XXXX\",\n      \"ip\": \"XXXX\",\n      \"roles\": [\n        \"data\"\n      ],\n      \"attributes\": {\n        \"storage_type\": \"warm\",\n        \"rack_id\": \"ap-southeast-1c\",\n        \"instance_id\": \"XXXX\",\n        \"xpack.installed\": \"true\",\n        \"instance_type\": \"large\",\n        \"instance_ip\": \"XXXX\"\n      },\n      \"tasks\": {\n        \"GdJF3lveTeu8HGxXNv4zig:11959020\": {\n          \"node\": \"GdJF3lveTeu8HGxXNv4zig\",\n          \"id\": 11959020,\n          \"type\": \"netty\",\n          \"action\": \"indices:admin/forcemerge[n]\",\n          \"start_time_in_millis\": 1539283160172,\n          \"running_time_in_nanos\": 4159561258352,\n          \"cancellable\": false,\n          \"parent_task_id\": \"PKOs3zbfQwaFF9s5JkSWtQ:4517418\",\n          \"headers\": {}\n        },\n        \"GdJF3lveTeu8HGxXNv4zig:11958857\": {\n          \"node\": \"GdJF3lveTeu8HGxXNv4zig\",\n          \"id\": 11958857,\n          \"type\": \"netty\",\n          \"action\": \"indices:admin/forcemerge[n]\",\n          \"start_time_in_millis\": 1539283118261,\n          \"running_time_in_nanos\": 4201472806632,\n          \"cancellable\": false,\n          \"parent_task_id\": \"8ozhh95EQlySvjsUX0sVPA:4415633\",\n          \"headers\": {}\n        }\n      }\n    }\n  }\n}. ",
    "agomerz": "I did in fact have several closed indices. . ",
    "hyksos": "Same problem on my system. Was working before 5.5.0.\nI can't explain why but it works in that context: (across python versions)\ndocker run --rm python pip install elasticsearch-curator\nWhich is weird if the pypi package is broken.. ",
    "fedelemantuano": "Yes I have the same issue on my installation. I'm solving with previous version:\n$ pip install elasticsearch-curator==5.4.1. Hi @untergeek,\nyou should check what's happen in setup.py steps, because the error is here. Try to check this command\ngit diff origin/5.4 origin/5.5\nOr this link\nThe setup.py fails in 5.x and 5.5. I tryed in a clean virtualenv with Python 2.7.. Hi @untergeek,\nif you want replicate the error in clean environment you should follow these steps:\n$ mkdir test\n$ virtualenv -p python2 venv\n$ source venv/bin/activate\n$ git clone https://github.com/elastic/curator.git\n$ cd curator\n$ git checkout origin/5.5\n$ python setup.py install\nThat's it.\n. ",
    "triat": "On mac clean python 3.6 (tried 3.5 too, same results)\n```\n$ pip list                                                                                                                                                                        \npip (9.0.2)\nsetuptools (28.8.0)\n$ pip --version                                                                                                                                                                       \npip 9.0.2 from /Users/triat/.pyenv/versions/3.6.4/lib/python3.6/site-packages (python 3.6)\n$ pip install elasticsearch-curator                                                                                                                                                  \nCollecting elasticsearch-curator\n  Using cached elasticsearch-curator-5.5.0.tar.gz\n  Running setup.py (path:/private/var/folders/ld/578m39fx2pz105pmwwxz263r0000gn/T/pip-build-6bx1dzpy/elasticsearch-curator/setup.py) egg_info for package elasticsearch-curator produced metadata for project name unknown. Fix your #egg=elasticsearch-curator fragments.\nInstalling collected packages: unknown\n  Running setup.py install for unknown ... done\nSuccessfully installed unknown-0.0.0\n```\nLet me know if you need more information. Yes sorry, I saw about the support of elasticsearch 6 and that was about elasticsearch-py 6.2, there is a limit in the requirement file of curator saying elasticsearch>=5.4.0,<6.0.0 but I couldn't see any issue with letting elasticsearch-py install the 6.x. \nI am certainly missing something but I wanted to check. Ok, I guess we're misunderstanding each other, my apologize: what I am doing:\npip install elasticsearch-curator\npip install elasticsearch==6.2.0\npython setup.py test\nWhich results in no tests error. Is there any other reason than your requirement that you keep elasticsearch>=5.4.0,<6.0.0 ?. ",
    "patrobinson": "Thanks for fixing this @untergeek I hadn't thought to try a different python version.. ",
    "cbuchacher": "@undergeek The Docker build on v5.5.1  is still broken for me:\n$ docker build .\nSending build context to Docker daemon   4.54MB\nStep 1/13 : FROM python:3.6-alpine3.6 as builder\n ---> 0cdaeba796e2\nStep 2/13 : RUN apk --no-cache add build-base tar musl-utils openssl-dev\n ---> Using cache\n ---> c8cbfe7fc414\nStep 3/13 : RUN pip3 install setuptools cx_Freeze==6.0b1 requests-aws4auth boto3\n ---> Using cache\n ---> e72e5de2f62f\nStep 4/13 : COPY . .\n ---> 7fa039a33fb9\nStep 5/13 : RUN ln -s /lib/libc.musl-x86_64.so.1 ldd\n ---> Running in 74dcf88d4784\nRemoving intermediate container 74dcf88d4784\n ---> 0df68e0d0dee\nStep 6/13 : RUN ln -s /lib /lib64\n ---> Running in 8467215f94e4\nRemoving intermediate container 8467215f94e4\n ---> 7bef3a6fd45c\nStep 7/13 : RUN pip3 install -r requirements.txt\n ---> Running in 89e67a5dc2b1\nCollecting voluptuous>=0.9.3 (from -r requirements.txt (line 1))\n  Downloading https://files.pythonhosted.org/packages/f5/09/a0e20a0bd743131e237128bad3a4f83b283f70c032b7e7c0f06baf7f6862/voluptuous-0.11.1-py2.py3-none-any.whl\nCollecting elasticsearch==5.5.2 (from -r requirements.txt (line 2))\n  Downloading https://files.pythonhosted.org/packages/7f/b0/c87bc0744c3193c7b7f07b0469bbeaaf7df363cb452d882783c283572345/elasticsearch-5.5.2-py2.py3-none-any.whl (59kB)\nCollecting click>=6.7 (from -r requirements.txt (line 3))\n  Downloading https://files.pythonhosted.org/packages/34/c1/8806f99713ddb993c5366c362b2f908f18269f8d792aff1abfd700775a77/click-6.7-py2.py3-none-any.whl (71kB)\nCollecting pyyaml>=3.10 (from -r requirements.txt (line 4))\n  Downloading https://files.pythonhosted.org/packages/4a/85/db5a2df477072b2902b0eb892feb37d88ac635d36245a72a6a69b23b383a/PyYAML-3.12.tar.gz (253kB)\nRequirement already satisfied: certifi>=2018.1.18 in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 5))\nRequirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/site-packages (from elasticsearch==5.5.2->-r requirements.txt (line 2))\nBuilding wheels for collected packages: pyyaml\n  Running setup.py bdist_wheel for pyyaml: started\n  Running setup.py bdist_wheel for pyyaml: finished with status 'done'\n  Stored in directory: /root/.cache/pip/wheels/03/05/65/bdc14f2c6e09e82ae3e0f13d021e1b6b2481437ea2f207df3f\nSuccessfully built pyyaml\nInstalling collected packages: voluptuous, elasticsearch, click, pyyaml\nException:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/site-packages/pip/basecommand.py\", line 215, in main\n    status = self.run(options, args)\n  File \"/usr/local/lib/python3.6/site-packages/pip/commands/install.py\", line 342, in run\n    prefix=options.prefix_path,\n  File \"/usr/local/lib/python3.6/site-packages/pip/req/req_set.py\", line 784, in install\n    **kwargs\n  File \"/usr/local/lib/python3.6/site-packages/pip/req/req_install.py\", line 851, in install\n    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)\n  File \"/usr/local/lib/python3.6/site-packages/pip/req/req_install.py\", line 1064, in move_wheel_files\n    isolated=self.isolated,\n  File \"/usr/local/lib/python3.6/site-packages/pip/wheel.py\", line 247, in move_wheel_files\n    prefix=prefix,\n  File \"/usr/local/lib/python3.6/site-packages/pip/locations.py\", line 141, in distutils_scheme\n    d.parse_config_files()\n  File \"/usr/local/lib/python3.6/site-packages/setuptools/dist.py\", line 494, in parse_config_files\n    ignore_option_errors=ignore_option_errors)\n  File \"/usr/local/lib/python3.6/site-packages/setuptools/config.py\", line 106, in parse_configuration\n    meta.parse()\n  File \"/usr/local/lib/python3.6/site-packages/setuptools/config.py\", line 382, in parse\n    section_parser_method(section_options)\n  File \"/usr/local/lib/python3.6/site-packages/setuptools/config.py\", line 355, in parse_section\n    self[name] = value\n  File \"/usr/local/lib/python3.6/site-packages/setuptools/config.py\", line 173, in __setitem__\n    value = parser(value)\n  File \"/usr/local/lib/python3.6/site-packages/setuptools/config.py\", line 430, in _parse_version\n    version = self._parse_attr(value)\n  File \"/usr/local/lib/python3.6/site-packages/setuptools/config.py\", line 305, in _parse_attr\n    module = import_module(module_name)\n  File \"/usr/local/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 941, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n  File \"/curator/__init__.py\", line 4, in <module>\n    from .validators import *\n  File \"/curator/validators/__init__.py\", line 1, in <module>\n    from .schemacheck import SchemaCheck\n  File \"/curator/validators/schemacheck.py\", line 1, in <module>\n    from voluptuous import *\nModuleNotFoundError: No module named 'voluptuous'\nYou are using pip version 9.0.3, however version 10.0.0 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\nThe command '/bin/sh -c pip3 install -r requirements.txt' returned a non-zero code: 2\nUpgrading to python:3.6.4-alpine does not fix it for me (no change). Instead I use a workaround similar to the .travis.yml fix which you mentioned above:\n```diff\ndiff --git a/Dockerfile b/Dockerfile\nindex 5412f8d..6a504d9 100644\n--- a/Dockerfile\n+++ b/Dockerfile\n@@ -3,10 +3,11 @@ FROM python:3.6-alpine3.6 as builder\n RUN apk --no-cache add build-base tar musl-utils openssl-dev\n RUN pip3 install setuptools cx_Freeze==6.0b1 requests-aws4auth boto3\n-COPY . .\n+COPY requirements.txt .\n RUN ln -s /lib/libc.musl-x86_64.so.1 ldd\n RUN ln -s /lib /lib64\n RUN pip3 install -r requirements.txt\n+COPY . .\n RUN python3 setup.py build_exe\nFROM alpine:3.6\n```. ",
    "servergeeks": "This is still an open issue... \nI'm trying to install curator on REL7.4 and it fails for python v2 & 3.. ",
    "basvandijk": "I'm upgrading the elasticsearch-curator package in nixpkgs from 5.5.4 to 5.6.0 in https://github.com/NixOS/nixpkgs/pull/51256. Unfortunately building the package fails with a similar error as reported in this issue:\n$ nix-build -A python3Packages.elasticsearch-curator\nthese derivations will be built:\n  /nix/store/vbx2jrfy19q21ivdpv3l04n96m34jg4j-python3.7-elasticsearch-curator-5.6.0.drv\nbuilding '/nix/store/vbx2jrfy19q21ivdpv3l04n96m34jg4j-python3.7-elasticsearch-curator-5.6.0.drv'...\nunpacking sources\nunpacking source archive /nix/store/z1s4dr36v0js0r8csfgsw8dvii5d8hwi-elasticsearch-curator-5.6.0.tar.gz\nsource root is elasticsearch-curator-5.6.0\nsetting SOURCE_DATE_EPOCH to timestamp 1542168764 of file elasticsearch-curator-5.6.0/setup.cfg\npatching sources\nconfiguring\nbuilding\nrunning bdist_wheel\nrunning build\nrunning build_py\ncreating build\ncreating build/lib\ncreating build/lib/curator\ncopying curator/singletons.py -> build/lib/curator\ncopying curator/curator_cli.py -> build/lib/curator\ncopying curator/__main__.py -> build/lib/curator\ncopying curator/snapshotlist.py -> build/lib/curator\ncopying curator/repomgrcli.py -> build/lib/curator\ncopying curator/indexlist.py -> build/lib/curator\ncopying curator/utils.py -> build/lib/curator\ncopying curator/cli.py -> build/lib/curator\ncopying curator/exceptions.py -> build/lib/curator\ncopying curator/_version.py -> build/lib/curator\ncopying curator/actions.py -> build/lib/curator\ncopying curator/config_utils.py -> build/lib/curator\ncopying curator/__init__.py -> build/lib/curator\ncopying curator/logtools.py -> build/lib/curator\nrunning egg_info\nwriting elasticsearch_curator.egg-info/PKG-INFO\nwriting dependency_links to elasticsearch_curator.egg-info/dependency_links.txt\nwriting entry points to elasticsearch_curator.egg-info/entry_points.txt\nwriting requirements to elasticsearch_curator.egg-info/requires.txt\nwriting top-level names to elasticsearch_curator.egg-info/top_level.txt\nreading manifest file 'elasticsearch_curator.egg-info/SOURCES.txt'\nreading manifest template 'MANIFEST.in'\nwarning: no files found matching 'Changelog.rst'\nwarning: no previously-included files matching '__pycache__' found under directory '*'\nwarning: no previously-included files matching '*.py[co]' found under directory '*'\nwarning: no previously-included files matching '*.pyc' found under directory 'curator'\nwarning: no previously-included files matching '*.pyo' found under directory 'curator'\nwarning: no previously-included files matching '*.pyc' found under directory 'docs'\nwarning: no previously-included files matching '*.pyo' found under directory 'docs'\nwarning: no previously-included files matching '*.pyc' found under directory 'test'\nwarning: no previously-included files matching '*.pyo' found under directory 'test'\nno previously-included directories found matching 'docs/_build'\nno previously-included directories found matching 'docs/asciidoc/html_docs'\nwriting manifest file 'elasticsearch_curator.egg-info/SOURCES.txt'\ncreating build/lib/curator/cli_singletons\ncopying curator/cli_singletons/__init__.py -> build/lib/curator/cli_singletons\ncopying curator/cli_singletons/alias.py -> build/lib/curator/cli_singletons\ncopying curator/cli_singletons/allocation.py -> build/lib/curator/cli_singletons\ncopying curator/cli_singletons/close.py -> build/lib/curator/cli_singletons\ncopying curator/cli_singletons/delete.py -> build/lib/curator/cli_singletons\ncopying curator/cli_singletons/forcemerge.py -> build/lib/curator/cli_singletons\ncopying curator/cli_singletons/object_class.py -> build/lib/curator/cli_singletons\ncopying curator/cli_singletons/open_indices.py -> build/lib/curator/cli_singletons\ncopying curator/cli_singletons/replicas.py -> build/lib/curator/cli_singletons\ncopying curator/cli_singletons/restore.py -> build/lib/curator/cli_singletons\ncopying curator/cli_singletons/rollover.py -> build/lib/curator/cli_singletons\ncopying curator/cli_singletons/show.py -> build/lib/curator/cli_singletons\ncopying curator/cli_singletons/shrink.py -> build/lib/curator/cli_singletons\ncopying curator/cli_singletons/snapshot.py -> build/lib/curator/cli_singletons\ncopying curator/cli_singletons/utils.py -> build/lib/curator/cli_singletons\ncreating build/lib/curator/defaults\ncopying curator/defaults/__init__.py -> build/lib/curator/defaults\ncopying curator/defaults/client_defaults.py -> build/lib/curator/defaults\ncopying curator/defaults/filter_elements.py -> build/lib/curator/defaults\ncopying curator/defaults/filtertypes.py -> build/lib/curator/defaults\ncopying curator/defaults/option_defaults.py -> build/lib/curator/defaults\ncopying curator/defaults/settings.py -> build/lib/curator/defaults\ncreating build/lib/curator/validators\ncopying curator/validators/__init__.py -> build/lib/curator/validators\ncopying curator/validators/actions.py -> build/lib/curator/validators\ncopying curator/validators/config_file.py -> build/lib/curator/validators\ncopying curator/validators/filters.py -> build/lib/curator/validators\ncopying curator/validators/options.py -> build/lib/curator/validators\ncopying curator/validators/schemacheck.py -> build/lib/curator/validators\nrunning build_exe\nerror: [Errno 2] No such file or directory: 'run_curator.py'\nAny idea how to fix this?. ",
    "amccool": "hmmm on debian got this\nUnable to create client connection to Elasticsearch.  Error: Elasticsearch version 6.2.4 incompatible with this version of Curator (6.2.4)\nyup - didnt have the correct curator dpkg installed.   However the error message (to me) reads like I do have the correct package version.\n. ",
    "topkool": "Same issue here on Debian 9 using apt to install elasticsearch-curator package.. ",
    "k5for2": "@untergeek Thank you for your reply.\nUnfortunately, I installed it using RPM. Is there any way I can modify index to idx using RPM?\nIf not, I'll try to build test environment using pip installation to replicate this error.. @untergeek \n```\n2018-03-30 07:51:32,288 DEBUG              curator.utils            wait_for_it:1759 Action \"allocation\" finished executing (may or may not have been successful)\n2018-03-30 07:51:32,288 DEBUG              curator.utils            wait_for_it:1777 Result: True\n2018-03-30 07:51:32,365 DEBUG              curator.utils           health_check:1498 KWARGS= \"{'status': 'green'}\"\n2018-03-30 07:51:32,370 DEBUG              curator.utils           health_check:1518 MATCH: Value for key \"green\", health check data: green\n2018-03-30 07:51:32,370 INFO               curator.utils           health_check:1521 Health Check for all provided keys passed.\n2018-03-30 07:51:32,370 INFO      curator.actions.shrink              do_action:2131 Shrinking index \"nginx-log-2018.03.14\" to \"nginx-log-2018.03.14-shrink\" with settings: {'settings': {'index.number_of_replicas': 1, 'index.number_of_shards': 4, 'index.codec': 'best_compression', 'index.routing.allocation.include.disk_type': 'hdd'}}, wait_for_active_shards=1\n2018-03-30 07:52:02,403 ERROR     curator.actions.shrink              do_action:2143 Deleting target index \"nginx-log-2018.03.14-shrink\" due to failure to complete shrink\n2018-03-30 07:52:07,994 ERROR                curator.cli                    run:184  Failed to complete action: shrink.  : Exception encountered.  Rerun with loglevel DEBUG and/or check Elasticsearch logs for more information. Exception: Unable to shrink index \"nginx-log_ncp-internal-apiserver-2018.03.14\" -- Error: ConnectionTimeout caused by - ReadTimeoutError(HTTPConnectionPool(host='{IP Address DELETED}', port={Port number DELETED}): Read timed out. (read timeout=30))\n```\nOk. I did it again. I got the error log.\nAnd cause seems to be 'timeout'. (due to 'slow' shard initialization).\nI think this response is partial success, also 'timeout message'.\n{\n  \"acknowledged\": true,\n  \"shards_acknowledged\": false,\n  \"index\": \"nginx-log-2018.03.12-shrink\"\n}\nI can avoid this error by changing general elasticsearch request timeout setting temporarily. (in Curator config files https://www.elastic.co/guide/en/elasticsearch/client/curator/5.5/configfile.html)\nBut, Curator should be able to deal with this problem without 'stop'. Because the request is not failed, \"acknowledged\": true.. @untergeek \nWait... timeout_override can be set to shrink action? I missed that options. \nNow I understand everything.\nThank you for your reply. I think I can solve this problem myself.\n. ",
    "guitmz": "@untergeek hmm my google search didnt showed me this, thanks!\nso yes, indeed its 3 separated clusters.\nin order for me to get this working I need then 3 curator configs and run curator command 3 times, one for each cluster, is that correct?. Thank you very much @untergeek . ",
    "kellycampbell": "I think it was something thing with our AWS s3 credentials. I guess I\nexpected a failure like that to cause an exception. In this case it's a\ndata loss without that exception. Fortunately this was on a test cluster so\nnot a major loss.\nOn Mon, Apr 2, 2018, 3:39 PM Aaron Mildenstein notifications@github.com\nwrote:\n\nThis represents a slight misunderstanding of what happens under the hood.\nA snapshot completing with state FAILED can still represent successful\nexecution of the Snapshot API\u2014which is exactly what happened, based on the\nlog lines you provided. In other words, continue_if_exception only\ncatches code exceptions, and one did not happen here.\nSnapshot failure is never a good thing. Do you know what's going on with\nthis? Have you checked the Elasticsearch logs to see why it's failing? That\nmight be a more pressing concern than altering this feature in Curator, as\nyou're not even getting a PARTIAL snapshot state\u2014it's outright saying the\nsnapshot has FAILED. (See more on snapshot status codes here\nhttps://www.elastic.co/guide/en/elasticsearch/reference/6.2/modules-snapshots.html#_snapshot\n).\nWith that said, it might be a good feature to add an option flag to force\nCurator to raise an exception if a snapshot doesn't complete with state\nSUCCESS, for example.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/elastic/curator/issues/1192#issuecomment-378021720,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAmNTrcHI01Tt72BfOVbGbLBXwWu8Barks5tkn6OgaJpZM4TD9gL\n.\n. Just curious, will this new flag be defaulted to true?\n\nPerhaps I'm not aware of the use-cases you are supporting by considering a failed snapshot as non-exceptional, but it seems like raising the exception should be the expected behavior, even though different from current behavior. . Yes, that is a bit surprising. I'm using it via kubernetes, and I am on an older version of that which doesn't yet support periodically scheduled jobs, so I've only been running curator manually about once a month. I have noticed that I've had to recreate my s3 snapshot in ES sometimes, and that was the case when it failed yesterday.\nI think the \"principle of least astonishment\" might be a good guide here for determining the default behavior of this new flag in 6.x.. ",
    "alexlance": "I have experienced this recently too. It was alarming to discover that the snapshots were only partial snapshots. It would be excellent if curator threw an error on a failed or partial snapshot.. ",
    "lucas044": "Yes this is a huge problem for me, a PARTIAL snapshot is not really useful to anyone. Would love some hard failures, curator deleted all my older snapshots automatically (as per retention configuration) so any last SUCCESS snapshots were removed.. ",
    "bhavenamrata": "@untergeek, thanks for your reply.\nInitially the tests failed with below error: \nTransportError: TransportError(500, u'repository_exception', u\"[TEST_REPOSITORY] location [/tmp/tmp0tlhNbAFKPCKY3] doesn't match any of the locations specified by path.repo because this setting is empty\")\nHence I updated path.repo: /tmp in elasticsearch.yml, which started giving AccessDeniedException.\nElasticsearch and Curator tests are being run by the same user. Also I tried giving all permissions to /tmp along with changing the owner of /tmp to the user. However, its not helping.\nAm I missing some other configuration in Elasticsearch?. Got it! Had initially run the install as sudo. \nHad to change the owner of Curator sub folders like '.egg-info' to current user and the tests completed.\nThanks @untergeek for the help.. ",
    "keshara": "First of all, thank you very much for the quick response.\nYes, indeed it worked. However, this is a python work-around. Don't we actually have a stanza to set this \"ignore_empty_list: True\" in a python script.\nThanks.\n. Well that is clear enough to understand the concept behind this curator python implementation.\nThanks for your valuable time.. ",
    "rshivane": "\nYou can see more by changing from command-line configuration to configuration file, and setting blacklist: [] in the logging section. It will show the urllib3 and elasticsearch messages which are suppressed by default.\n\nThanks for the tip. Here is the message:\n2018-04-20 14:48:53,133 WARNING            elasticsearch       log_request_fail:97   GET http://127.0.0.1:9200/.kibana,.monitoring-alerts-6,.monitoring-es-6-2018.04.10,.monitoring-es-6-2018.04.11,.monitoring-es-6-2018.04.12,.monitoring-es-6-2018.04.13,.monitoring-es-6-2018.04.14,.monitoring-es-6-2018.04.15,.monitoring-es-6-2018.04.16,.monitoring-kibana-6-2018.04.10,.monitoring-kibana-6-2018.04.11,.monitoring-kibana-6-2018.04.12,.monitoring-kibana-6-2018.04.13,.monitoring-kibana-6-2018.04.14,.monitoring-kibana-6-2018.04.15,.monitoring-kibana-6-2018.04.16,.monitoring-kibana-6-2018.04.17,.monitoring-kibana-6-2018.04.18,.monitoring-kibana-6-2018.04.19,.security-6,.triggered_watches,.watcher-history-7-2018.03.20,.watcher-history-7-2018.03.21,.watcher-history-7-2018.03.22,.watcher-history-7-2018.03.23,.watcher-history-7-2018.03.24,.watcher-history-7-2018.03.25,.watcher-history-7-2018.03.26,.watcher-history-7-2018.03.27,.watcher-history-7-2018.03.28,.watcher-history-7-2018.03.29,.watcher-history-7-2018.03.30,.watcher-history-7-2018.03.31,.watcher-history-7-2018.04.01,.watcher-history-7-2018.04.02,.watcher-history-7-2018.04.03,.watcher-history-7-2018.04.04,.watcher-history-7-2018.04.05,.watcher-history-7-2018.04.06,.watcher-history-7-2018.04.07,.watcher-history-7-2018.04.08,.watcher-history-7-2018.04.09,.watcher-history-7-2018.04.10,.watcher-history-7-2018.04.11,.watcher-history-7-2018.04.12,.watcher-history-7-2018.04.13,.watcher-history-7-2018.04.14,.watcher-history-7-2018.04.15,.watcher-history-7-2018.04.16,.watches,filebeat-6.2.2-2018.04.12,filebeat-6.2.2-2018.04.13,filebeat-6.2.2-2018.04.14,filebeat-6.2.2-2018.04.15,filebeat-6.2.2-2018.04.16,filebeat-6.2.2-2018.04.17,filebeat-6.2.2-2018.04.18,filebeat-6.2.2-2018.04.19,filebeat-6.2.3-2018.04.12,filebeat-6.2.3-2018.04.13,filebeat-6.2.3-2018.04.14,filebeat-6.2.3-2018.04.15,filebeat-6.2.3-2018.04.16,filebeat-6.2.3-2018.04.17,filebeat-6.2.3-2018.04.18,filebeat-6.2.3-2018.04.19,filebeat-6.2.4-2018.04.19,filebeat-6.2.4-2018.04.20,logstash-2018.03.27,metricbeat-6.2.2-2018.04.03,metricbeat-6.2.2-2018.04.04,metricbeat-6.2.2-2018.04.05,metricbeat-6.2.2-2018.04.06,metricbeat-6.2.2-2018.04.07,metricbeat-6.2.2-2018.04.08,metricbeat-6.2.2-2018.04.09,metricbeat-6.2.2-2018.04.10,metricbeat-6.2.2-2018.04.11,metricbeat-6.2.2-2018.04.12,metricbeat-6.2.2-2018.04.13,metricbeat-6.2.2-2018.04.14,metricbeat-6.2.2-2018.04.15,metricbeat-6.2.2-2018.04.16,metricbeat-6.2.2-2018.04.17,metricbeat-6.2.2-2018.04.18,metricbeat-6.2.2-2018.04.19,metricbeat-6.2.3-2018.04.03,metricbeat-6.2.3-2018.04.04,metricbeat-6.2.3-2018.04.05,metricbeat-6.2.3-2018.04.06,metricbeat-6.2.3-2018.04.09,metricbeat-6.2.3-2018.04.10,metricbeat-6.2.3-2018.04.11,metricbeat-6.2.3-2018.04.12,metricbeat-6.2.3-2018.04.13,metricbeat-6.2.3-2018.04.14,metricbeat-6.2.3-2018.04.15,metricbeat-6.2.3-2018.04.16,metricbeat-6.2.3-2018.04.17,metricbeat-6.2.3-2018.04.18,metricbeat-6.2.3-2018.04.19,metricbeat-6.2.4-2018.04.19,metricbeat-6.2.4-2018.04.20/_stats/store,docs [status:403 request:0.001s]\n2018-04-20 14:48:53,133 DEBUG              elasticsearch       log_request_fail:105  > None\n2018-04-20 14:48:53,133 DEBUG              elasticsearch       log_request_fail:110  < {\"error\":{\"root_cause\":[{\"type\":\"security_exception\",\"reason\":\"current license is non-compliant for [security]\",\"license.expired.feature\":\"security\"}],\"type\":\"security_exception\",\"reason\":\"current license is non-compliant for [security]\",\"license.expired.feature\":\"security\"},\"status\":403}\nI tried splitting up the URL into two parts and invoking them curl. The first part to get index details works fine. The last part of the URL to get /_stats/store,docs is the one that fails.\nroot@es:/home/ubuntu# curl -u $ES_AUTH \"http://localhost:9200/_stats/store,docs\"\n{\"error\":{\"root_cause\":[{\"type\":\"security_exception\",\"reason\":\"current license is non-compliant for [security]\",\"license.expired.feature\":\"security\"}],\"type\":\"security_exception\",\"reason\":\"current license is non-compliant for [security]\",\"license.expired.feature\":\"security\"},\"status\":403}\nI was using curator just fine before we started the X-pack trial. Now that the trial has ended, shouldn't curator work as it used to before instead of failing?!. Thanks @untergeek I fully uninstalled the X-Pack plugin and curator_cli works fine now.\nFor anyone else facing similar issues, when X-Pack trial fails, curator_cli stops working, though filebeat/metricbeat continue to work fine.. ",
    "eskibars": "Relates https://github.com/elastic/elasticsearch/issues/29823. ",
    "ondrejchmelar": "PR https://github.com/elastic/curator/pull/1210. I missed it in the docs but you can see from the code that it has no effect in this usecase. \nwarn_if_no_indices comes into effect when empty_list_check fails. For me it doesn't raise an exception - the problem begins a couple of lines later when if index in aliases is false and nothing gets pushed in self.actions.\n2018-05-04 09:55:33,092 INFO      Preparing Action ID: remove1, \"alias\"\n2018-05-04 09:55:52,065 WARNING   No indices found after processing filters. Nothing to remove from logstash-app.xxx-yyy\n2018-05-04 09:55:52,065 INFO      Updating aliases...\n2018-05-04 09:55:52,065 ERROR     Failed to complete action: alias.  : No \"add\" or \"remove\" operations\n```\nactions:\nremove1:\n    action: alias\n    description: \"Remove indices to be closed from aliases.\"\n    options:\n      ignore_empty_list: True\n      warn_if_no_indices: True\n      name: logstash-app.xxx-yyy\n    remove:\n      filters:\n        - filtertype: pattern\n          kind: prefix\n          value: \"logstash-app.yyy-\"\n        - filtertype: age\n          source: name\n          timestring: '%Y-%m-%d'\n          direction: older\n          unit: days\n          unit_count: 16\n``. Recreated the PR from 5.x branch. Added the check, only thing I'm unsure of is the difference betweenwarn_if_no_indicesandignore_empty_list`. Maybe both should be checked in this case?. The only difference I can see is that warn_if_no_indices comes into play when add OR remove has no indices, ignore_empty_list when BOTH of them are empty? Aside from WARN/INFO severity.\nIf that is the case then warn_if_no_indices==true implies ignore_empty_list==true so both should be checked, right?\nhttps://www.elastic.co/guide/en/elasticsearch/client/curator/5.5/option_warn_if_no_indices.html\nhttps://www.elastic.co/guide/en/elasticsearch/client/curator/5.5/option_ignore_empty.html. Thank you. What is the timeline of a new release?. Moved to https://github.com/elastic/curator/pull/1211. ",
    "sergii-rolskii": "@untergeek thanks for your reply.. ",
    "sadiesh": "\"http.max_initial_line_length\" has to added in ES yaml file. ",
    "MohammadRamadan": "Thank you, adding this parameter to the yml file solved it.. Hi, as told before am not a developer so I would reply to you with any information you would but explain plainly and how to answer you ques if necessary.\nThank you. ",
    "vdmitrief": "Fixed !\ndeleted files, created them again with the same content and worked. ",
    "Nostalgiac": "Signed the CLA.. ",
    "alxchk": "\nFrom what I can tell, you may have triggered a try/except exception by getting a TransportError other than a 413, and that would prevent iterate_over_stats from having a populated dictionary.\n\nIt's possible that there was timeout exception. Node was overloaded and I searched for fast way to delete old stuff. After I add  'if not stats: return' workaround timeout exception fired after some processing. So I increased timeout in curator config 10 times and finished cleanup successfully. \n\nAlso, why are you building from github instead of just pip install elasticsearch-curator? \n\nSomehow I made strange decision that this should be 'curator' (not elasticsearch-curator). . ",
    "JavaCS3": "Any update about this, I met the same issue. I encounter this problem on docker-compose. let me try to create a sample to reproduce this issue.. Seems that my X-Pack is expired @untergeek \nlog\n2018-09-19 10:04:08,930 DEBUG          curator.indexlist       _get_index_stats:115  Getting index stats\n2018-09-19 10:04:08,930 DEBUG          curator.indexlist       empty_list_check:224  Checking for empty list\n2018-09-19 10:04:08,931 DEBUG          curator.indexlist           working_list:235  Generating working list of indices\n2018-09-19 10:04:08,931 DEBUG          curator.indexlist           working_list:235  Generating working list of indices\n2018-09-19 10:04:08,932 DEBUG         urllib3.util.retry               from_int:210  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)\n2018-09-19 10:04:08,934 DEBUG     urllib3.connectionpool          _make_request:393  http://elasticsearch:9200 \"GET /.monitoring-kibana-6-2018.08.09,.monitoring-kibana-6-2018.08.11,.monitoring-kibana-6-2018.08.12,.monitoring-kibana-6-2018.08.13,.monitoring-kibana-6-2018.08.14,bundle-02a19310cc3151390b1754d021c9c6b3,bundle-0303f94f1c408d10c3e79f3fac344874,bundle-047a355ff0f66dce2fe6c34a61e4bdcc,bundle-08a9719f7c44c112f84fdb0f9e0fa2a9,bundle-0cf0feeea21723e650a50a72cacc3fea,bundle-0d21b69916deaa8d07b85dd8e50f98aa,bundle-0ddbc4d28fb80a6a483ea9a4d669804e,bundle-1086396284707fd7248f69817eb8de6a,bundle-1359ba06bd261d3704938d4590d3cc75,bundle-16bd07c8ef71ce461adf5672faba66c2,bundle-16f270672b9381d7413b4d1ca00aacd4,bundle-1872d4501f03e73318083c0c4872fdd7,bundle-19b6c2466908600ca15304df73ecacdc,bundle-22ce25762b728bcc3570f7c45f1317e0,bundle-28625373a2eacda32cc7a7dc79b51dbc,bundle-2be01fac69e45a686b992f8f81bdd900,bundle-2d9f65c0328f93e85d12853fd236358b,bundle-2e32d1c5cce51fdd174adb6c816d7056,bundle-3254cb4e7c0321947de4439ce1ff12ac,bundle-3388c709ccbd49671caf7212b955c766,bundle-33f015891eefa5564966cd3611116172,bundle-34e514bd5a706eb42e4384db0e24a7c4,bundle-3ca605a8c569f49ce967dc5e753f8ad2,bundle-4265c9f7b8c683e119daeb45e3145ae0,bundle-456058a8a640dbaf93c8cf53a4edf1b1,bundle-495f93175dbf2a09c3fa96133db6b493,bundle-4a89ead5085dedf60a91b7e26a5b4685,bundle-4bf484badd65758856b798ec25f5f7ce,bundle-4cb5f81dfe433f90bb9afd761b54f8a8,bundle-54c366d64311c46abb71329120ed722d,bundle-54e81a144454894cd112c7d81686bf05,bundle-5ac53f7ff7b436e31b75b0f3b1a048b4,bundle-5b68962e23d3be8cb0ada4dcdfed94a4,bundle-62a6264d8b1181b9e56477b03d81efe6,bundle-62ceb67b01dbd94a243b24a264a22297,bundle-632862b65637aed258ad6fb162b3ec43,bundle-6adeaa17d3399b7bff65103842284f1a,bundle-7093a3477f2d9f5575528ef0a5dae8c2,bundle-77fdc802ec7073fdfee136b9234abe49,bundle-7b6ff30e5ea0a4ecae2e5868a15ddd2d,bundle-835a9efdfadf383ed1dd789667e560db,bundle-841e1e83385e74624aaabdfea46ff4e7,bundle-886d921396e9830623131bb9b29a2a62,bundle-9460985f85599229aeea52721dc3b4e2,bundle-94c22797f1604446e6aa3e5f4cbbe9f1,bundle-9549b62ee33d2453ecaddb55dc61f24b,bundle-9a26a81049f9ed06f7e414249b5d36d1,bundle-9d789201bf4fc27acada6b836e1d7b61,bundle-9d8cf71915ff70edf69e539975be89ad,bundle-a1e659f2b0695913da51d3e5cedf0f2b,bundle-a8ff07a826b8a2c3b6a5266f9ae26211,bundle-aa7b0e55d3e189f6d74ea737ce9a104c,bundle-aaf546f7d1b815f100d03e5b621f1301,bundle-ac195feaa8b68ce77143199ecc05713b,bundle-adfb1fd143a33c6fba470603987783e5,bundle-aeb29ae7f794d0c678248128e2785754,bundle-af10e92f8362fd6f1b0ac2efb96fedf0,bundle-b27b42f084bdc674419c8e5d8e7ee7ea,bundle-b28f84747c0eb39eb2045c789857fc92,bundle-b83f45d9bc32190be69410f122029e3f,bundle-b84a9bb781e45f160bfc1ee350d0f62e,bundle-bc4d275fef56d442993dcb2722394955,bundle-ca308fe56106bbf2e006fcf9f01d6043,bundle-cb97064911d565445f4df23334428b00,bundle-cce8a5157f1337cf8f0ffedfc237c1a3,bundle-ce51311a17407009c13c35eb292a0371,bundle-d5dd65813e4f2804254da00c5c04b34b,bundle-e4859f6af7cd869d3ecb07f84b930820,bundle-ee93300ec42af9eaac8f514d4c24588d,bundle-f297c7924d335adf1ef39b9d3cf36566,bundle-f48f3d1cadb0202ce023462aabfa2763,bundle-f568d229b20e6a712565f3cbfada49d0,bundle-f9ba0643818cf780d728d8aa5796f7b1/_stats/store,docs HTTP/1.1\" 403 289\n2018-09-19 10:04:08,935 WARNING            elasticsearch       log_request_fail:97   GET http://elasticsearch:9200/.monitoring-kibana-6-2018.08.09,.monitoring-kibana-6-2018.08.11,.monitoring-kibana-6-2018.08.12,.monitoring-kibana-6-2018.08.13,.monitoring-kibana-6-2018.08.14,bundle-02a19310cc3151390b1754d021c9c6b3,bundle-0303f94f1c408d10c3e79f3fac344874,bundle-047a355ff0f66dce2fe6c34a61e4bdcc,bundle-08a9719f7c44c112f84fdb0f9e0fa2a9,bundle-0cf0feeea21723e650a50a72cacc3fea,bundle-0d21b69916deaa8d07b85dd8e50f98aa,bundle-0ddbc4d28fb80a6a483ea9a4d669804e,bundle-1086396284707fd7248f69817eb8de6a,bundle-1359ba06bd261d3704938d4590d3cc75,bundle-16bd07c8ef71ce461adf5672faba66c2,bundle-16f270672b9381d7413b4d1ca00aacd4,bundle-1872d4501f03e73318083c0c4872fdd7,bundle-19b6c2466908600ca15304df73ecacdc,bundle-22ce25762b728bcc3570f7c45f1317e0,bundle-28625373a2eacda32cc7a7dc79b51dbc,bundle-2be01fac69e45a686b992f8f81bdd900,bundle-2d9f65c0328f93e85d12853fd236358b,bundle-2e32d1c5cce51fdd174adb6c816d7056,bundle-3254cb4e7c0321947de4439ce1ff12ac,bundle-3388c709ccbd49671caf7212b955c766,bundle-33f015891eefa5564966cd3611116172,bundle-34e514bd5a706eb42e4384db0e24a7c4,bundle-3ca605a8c569f49ce967dc5e753f8ad2,bundle-4265c9f7b8c683e119daeb45e3145ae0,bundle-456058a8a640dbaf93c8cf53a4edf1b1,bundle-495f93175dbf2a09c3fa96133db6b493,bundle-4a89ead5085dedf60a91b7e26a5b4685,bundle-4bf484badd65758856b798ec25f5f7ce,bundle-4cb5f81dfe433f90bb9afd761b54f8a8,bundle-54c366d64311c46abb71329120ed722d,bundle-54e81a144454894cd112c7d81686bf05,bundle-5ac53f7ff7b436e31b75b0f3b1a048b4,bundle-5b68962e23d3be8cb0ada4dcdfed94a4,bundle-62a6264d8b1181b9e56477b03d81efe6,bundle-62ceb67b01dbd94a243b24a264a22297,bundle-632862b65637aed258ad6fb162b3ec43,bundle-6adeaa17d3399b7bff65103842284f1a,bundle-7093a3477f2d9f5575528ef0a5dae8c2,bundle-77fdc802ec7073fdfee136b9234abe49,bundle-7b6ff30e5ea0a4ecae2e5868a15ddd2d,bundle-835a9efdfadf383ed1dd789667e560db,bundle-841e1e83385e74624aaabdfea46ff4e7,bundle-886d921396e9830623131bb9b29a2a62,bundle-9460985f85599229aeea52721dc3b4e2,bundle-94c22797f1604446e6aa3e5f4cbbe9f1,bundle-9549b62ee33d2453ecaddb55dc61f24b,bundle-9a26a81049f9ed06f7e414249b5d36d1,bundle-9d789201bf4fc27acada6b836e1d7b61,bundle-9d8cf71915ff70edf69e539975be89ad,bundle-a1e659f2b0695913da51d3e5cedf0f2b,bundle-a8ff07a826b8a2c3b6a5266f9ae26211,bundle-aa7b0e55d3e189f6d74ea737ce9a104c,bundle-aaf546f7d1b815f100d03e5b621f1301,bundle-ac195feaa8b68ce77143199ecc05713b,bundle-adfb1fd143a33c6fba470603987783e5,bundle-aeb29ae7f794d0c678248128e2785754,bundle-af10e92f8362fd6f1b0ac2efb96fedf0,bundle-b27b42f084bdc674419c8e5d8e7ee7ea,bundle-b28f84747c0eb39eb2045c789857fc92,bundle-b83f45d9bc32190be69410f122029e3f,bundle-b84a9bb781e45f160bfc1ee350d0f62e,bundle-bc4d275fef56d442993dcb2722394955,bundle-ca308fe56106bbf2e006fcf9f01d6043,bundle-cb97064911d565445f4df23334428b00,bundle-cce8a5157f1337cf8f0ffedfc237c1a3,bundle-ce51311a17407009c13c35eb292a0371,bundle-d5dd65813e4f2804254da00c5c04b34b,bundle-e4859f6af7cd869d3ecb07f84b930820,bundle-ee93300ec42af9eaac8f514d4c24588d,bundle-f297c7924d335adf1ef39b9d3cf36566,bundle-f48f3d1cadb0202ce023462aabfa2763,bundle-f568d229b20e6a712565f3cbfada49d0,bundle-f9ba0643818cf780d728d8aa5796f7b1/_stats/store,docs [status:403 request:0.003s]\n2018-09-19 10:04:08,935 DEBUG              elasticsearch       log_request_fail:105  > None\n2018-09-19 10:04:08,935 DEBUG              elasticsearch       log_request_fail:110  < {\"error\":{\"root_cause\":[{\"type\":\"security_exception\",\"reason\":\"current license is non-compliant for [security]\",\"license.expired.feature\":\"security\"}],\"type\":\"security_exception\",\"reason\":\"current license is non-compliant for [security]\",\"license.expired.feature\":\"security\"},\"status\":403}\n2018-09-19 10:04:08,935 ERROR                curator.cli                    run:186  Failed to complete action: delete_indices.  <type 'exceptions.KeyError'>: 'indices'\n2018-09-19 10:04:08,935 INFO                 curator.cli                    run:192  Continuing execution with next action because \"continue_if_exception\" is set to True for action delete_indices\n2018-09-19 10:04:08,935 INFO                 curator.cli                    run:196  Action ID: 1, \"delete_indices\" completed.\n2018-09-19 10:04:08,936 INFO                 curator.cli                    run:197  Job completed.. Help~. ",
    "fizch": "I am also getting this error message. I don't know if it is because I have a lot of indexes at the moment or what. I am trying to set this up for the first time.\n\nactions:\n  api_1:\n    action: close\n    description: Close indexes for API logs\n    options:\n        continue_if_exception: False\n    filters:\n    - filtertype: pattern\n      kind: prefix\n      value: logstash-\n    - filtertype: age\n      source: name\n      timestring: '%Y.%m.%d'\n      direction: older\n      unit: days\n      unit_count: 60\n  api_2:\n    action: delete_indices\n    description: Delete indexes for API logs\n    options:\n        ignore_empty_list: True\n    filters:\n    - filtertype: pattern\n      kind: prefix\n      value: logstash-\n    - filtertype: age\n      source: name\n      timestring: '%Y.%m.%d'\n      direction: older\n      unit: days\n      unit_count: 60\n2018-09-15 17:20:18,116 DEBUG          curator.indexlist          _get_metadata:175  Getting index metadata\n2018-09-15 17:20:18,116 DEBUG          curator.indexlist       empty_list_check:224  Checking for empty list\n2018-09-15 17:21:40,895 DEBUG          curator.indexlist       _get_index_stats:115  Getting index stats\n2018-09-15 17:21:40,895 DEBUG          curator.indexlist       empty_list_check:224  Checking for empty list\n2018-09-15 17:21:40,895 DEBUG          curator.indexlist           working_list:235  Generating working list of indices\n2018-09-15 17:21:40,895 DEBUG          curator.indexlist           working_list:235  Generating working list of indices\n2018-09-15 17:24:40,916 ERROR                curator.cli                    run:186  Failed to complete action: close.  : 'indices'\n. It turns out that curator didn't like me having 370+ indexes. I manually deleted the excess and I am no longer seeing that error message.. \n",
    "willejs": "@untergeek cheers. I didnt realise that master wasnt the latest code and tags wernt cut against it and instead there were branches for each version. . ",
    "wanshishi": "Oh my bad, I thought I was in the right forum. sorry I will post this there :D. https://discuss.elastic.co/t/curator-snapshot-error-with-closed-indices/134259. ",
    "SvenDowideit": "I'd like to make it trivial for my developers to auto-expire indices - is there any chance you guys can start shipping this image?. closing - origin/5.x is already using more upto date stuff. ",
    "chrislujan": "I might be misunderstanding how index templates work, but where I noticed a need (want) is when a rollover occurs, the index is created but until a document is indexed, the index doesn't have it's mapping/template applied from logstash's output. When that empty index is queried, it results in org.elasticsearch.transport.RemoteTransportException. Nothing to be worried about, but can result in larger than normal logs, albeit debug loglines. My feature request was more curiosity than anything I guess :) I can understand not wanting to support a feature that results in more headache for you and adds very little function to curator. Thanks for the reply @untergeek . thanks @untergeek for the explanation, I can work on a solution from here \ud83d\udc4d . ",
    "changchanglccc": "Hey, thanks for your reply. \n1. And i also run it with a large index(size>10GB and it needs one hour to complete), and i still have the same problem, which is the reason i created a small index locally to test.\n2. for self.loggit.error, after it happens, who will take over it?\n. ",
    "benkeil": "I'm relative new to this theme... If you look into the task (or many task if you configured slices) you see the current position and how much the task must process. Maybe we can work with this information, if the task api don't returned a reason for finished tasks.. Nice! Thank you!. It was very stupid of me... we have a query to skip health checks for reindexing and the index contains only health check logs.... ",
    "frankh": "I've reproduced this by building the Dockerfile from the v5.5.4 tag\n. ",
    "eirc": "You are right, I'm sorry for that. Thanks a lot!!. ",
    "heroInCommunity": "Had the same problem, but firstly found working solution https://unix.stackexchange.com/a/419737/85467,\nonly after found this issue and official solution.. I have the same issue with Docker debian:stable-slim\nDockerfile is:\nFROM debian:stable-slim\nRUN apt-get update && apt-get install apt-transport-https curl gnupg gnupg1 gnupg2 cron -y &&\\\n    curl -L https://packages.elastic.co/GPG-KEY-elasticsearch | apt-key add - &&\\\n    echo \"deb [arch=amd64] https://packages.elastic.co/curator/5/debian9 stable main\" > /etc/apt/sources.list.d/curator.list &&\\\n    apt-get update &&\\\n    apt-get install elasticsearch-curator -y &&\\\n    chmod a+x /etc/cron.daily/* &&\\\n    rm -rf /var/lib/apt/lists/*\nCMD cron -f\nDebian version is 9.3 (output of cat /etc/debian_version), curator - 5.5.4 (it was displayed during installation).\nCurator does not show any output on any parameters (including --help, --config) running from both locations /usr/bin/curator and /opt/elasticsearch-curator.\nPrevious version (it was like 5.4.*) worked fine on Debian 9.3 (with manual installation of libssl1.0.0 and setting unicode locales LC_ALL and LANG). However, I decided to try without that workarounds, as version 5.5.4 was promised to fix that issues, and failed now.\n. @untergeek could you please tell if you've managed to reproduce the issue on either CentOS or Debian?\nPlease tell if you need any additional info.. @OmisNomis you are proposing a workaround, while curator seems still have the problem with no output.\nShould the ticket be closed?. Indeed, with pip it works, although again I needed to export LC_ALL=C.UTF-8 and LANG=C.UTF-8, like it was with previous version of curator.. Does curator return any output on curator --help?\nProbably it is related to https://github.com/elastic/curator/issues/1243.. ",
    "OmisNomis": "Some examples:\nserver ~$ curator --help\nserver ~$ curator doesntexit\nserver ~$ curator --pleeeease\nserver ~$ curator --. Hi @untergeek - neither of those work, no. I tried both of them (sorry, I didn't put it in the description). I'm assuming it doesn't need Python to run, as it has been installed via the RPM package? I have python installed, but it's V2.... @untergeek 100%... I even removed everything and then got a colleague to sit along side me to make sure I wasn't doing anything stupid. How bizarre! Is there anything else I can check? I've tried installing it three times now without success. . There is the standard ELK software, that I installed using the RPM files, and node/Go but nothing that isn't standard. . I'll have to do that tomorrow, if that's ok. . cat /etc/redhat-release -> CentOS Linux release 7.4.1708 (Core)\nuname -a -> Linux 3.10.0-693.21.1.el7.x86_64 #1 SMP Wed Mar 7 19:03:37 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux (server name removed)\n. I actually gave up on using Curator and just made a simple Node process that runs on a cron job every night.. @ceeeekay I'd suggest you have a look at using the Elasticube API and run a small application on a cron job to get rid of the indices you don't want. . @heroInCommunity I don't need this open, but perhaps I closed it in haste from a 'community' point of view, I just didn't see much movement on the underlying issue actually being fixed. \n@untergeek  - do you want this re-opened?. @ceeeekay - happy to re-open it. I'll leave it in the hands of @untergeek on how he wants to progress it.. ",
    "ceeeekay": "I have exactly the same problem, although I have one box that works (prod), and one that doesn't (dev). Ubuntu 16.04.4 LTS this time.\nThe only notable difference between the two is that dev has been hacked around a bit for different purposes, and used to have a pip-installed version of elasticsearch-curator on it, but this was removed prior to apt-installing 5.5.4 from the Elastic repo.  I'm starting to suspect that left something behind which is affecting Curator.\nI've spent about half a day on this so far and have no clue where to look, because the dev box refuses to give me any output at any time.\nThe only notable difference between the boxes is the affected box has the following extra packages on it, although I have no idea if they're related or not. Attempting to remove these threatens to uninstall ubuntu-server, so I'm not touching them for now:\ndiff of dpkg -l | grep python\n-python3-apport\n-python3-chardet\n-python3-debian\n-python3-newt\n-python3-pkg-resources\n-python3-problem-report\n-python3-pycurl\n-python3-six\n-python3-software-properties\nHappy to try anything you like on the dev box. It's got to the point where I'm considering rebuilding it for this issue.. @untergeek any suggestions?. @OmisNomis @untergeek Why has this been closed? I still don't see any way to get any output from Curator. None of the suggestions here work in my environment, and it's completely unusable.. @OmisNomis we use curator for a whole bunch of things, not just deleting indices. We close, delete, forcemerge and create snapshots in cron and it's all integrated with our automation tools. Changing to another application isn't a small task for us.. In my case, missing LC_ALL was the cause of the problem, and the difference between the working and failing machines. Adding it to /etc/environment has solved the problem. Note that this is an apt install.\n```\ncurator\n\n\nexport LC_ALL=en_NZ.UTF-8\ncurator\nUsage: curator [OPTIONS] ACTION_FILE\nError: Missing argument \"action_file\".\n```\nI guess an error at this point would have saved a lot of time?. ",
    "anthonyloukinas": "Also having this issue with Docker using the latest Elasticsearch image from elastic.co running CentOS 7.\n```\n[root@4a06effd36e8 ~]# yum install elasticsearch-curator\nLoaded plugins: fastestmirror, ovl\nLoading mirror speeds from cached hostfile\n * base: mirrors.liquidweb.com\n * extras: ftp.osuosl.org\n * updates: distro.ibiblio.org\nResolving Dependencies\n--> Running transaction check\n---> Package elasticsearch-curator.x86_64 0:5.5.4-1 will be installed\n--> Finished Dependency Resolution\nDependencies Resolved\n================================================================================================================================================================================================================================================================\n Package                                                                 Arch                                                     Version                                                     Repository                                                   Size\n================================================================================================================================================================================================================================================================\nInstalling:\n elasticsearch-curator                                                   x86_64                                                   5.5.4-1                                                     curator-5                                                    14 M\nTransaction Summary\nInstall  1 Package\nTotal download size: 14 M\nInstalled size: 54 M\nIs this ok [y/d/N]: y\nDownloading packages:\nelasticsearch-curator-5.5.4-1.x86_64.rpm                                                                                                                                                                                                 |  14 MB  00:00:01\nRunning transaction check\nRunning transaction test\nTransaction test succeeded\nRunning transaction\n  Installing : elasticsearch-curator-5.5.4-1.x86_64                                                                                                                                                                                                         1/1\n  Verifying  : elasticsearch-curator-5.5.4-1.x86_64                                                                                                                                                                                                         1/1\nInstalled:\n  elasticsearch-curator.x86_64 0:5.5.4-1\nComplete!\n[root@4a06effd36e8 ~]#\n[root@4a06effd36e8 ~]# curator --help\n[root@4a06effd36e8 ~]# curator_cli --help\n[root@4a06effd36e8 ~]# ls /usr/bin/ | grep curator\ncurator\ncurator_cli\n[root@4a06effd36e8 opt]# cd /opt/elasticsearch-curator/\n[root@4a06effd36e8 elasticsearch-curator]# ls\ncacert.pem  curator  curator_cli  es_repo_mgr  lib\n[root@4a06effd36e8 ~]# cat /etc/redhat-release\nCentOS Linux release 7.5.1804 (Core)\n[root@4a06effd36e8 yum.repos.d]# cat curator.repo\n[curator-5]\nname=CentOS/RHEL 7 repository for Elasticsearch Curator 5.x packages\nbaseurl=https://packages.elastic.co/curator/5/centos/7\ngpgcheck=1\ngpgkey=https://packages.elastic.co/GPG-KEY-elasticsearch\n``\nUsing this specific imagedocker.elastic.co/elasticsearch/elasticsearch:6.3.0`. Looks like I'll do the same thing with Python. I actually like that solution better.. gives us some more customizability. \nThe bug still stands however, and I'd like to see it debugged some more. I was able to reproduce the same issue I pasted above on another host system running the same 6.3.0 latest container image.. @untergeek Have you tried to reproduce this using Docker? Try to use the latest elastic.co elasticsearch image on any base operating system and see if you can make it work.\ndocker pull docker.elastic.co/elasticsearch/elasticsearch:6.3.0\ndocker run -it docker.elastic.co/elasticsearch/elasticsearch:6.3.0 bash\nCopy the repo into /etc/yum.repos.d/ and then yum install elasticsearch-curator. \nThat's all I'm doing and I get no output. Elasticsearch itself is working completely fine. I've now tried this on CentOS + Ubuntu + OSX, as a docker image.. @untergeek The Curator API looks nice, but it doesn't state what the actual PIP package is in the Documentation, and it also states it only support 5.x versions of Elasticsearch which doesn't help me as I'm running 6.3. I imagine nothing major has changed and it will most likely work.. Might take some tinkering. \nLet me know if you have any working examples + which packages to install if you know off top. I installed pip install curator but i'm not sure this is the correct package.\nEDIT: I located the actual package, elasticsearch-curator. They don't mention that in the documentation anywhere.... Yup I can confirm that ES 6.x works fine with elasticsearch-curator. I'm pasting a working example in case anyone else here is having trouble getting the native package to work. I'll be running this in cron nightly at 12:30am.\nSide Note: My indices look like this in ES netflow-index-2018.08.02. Logstash every night at midnight automatically creates new indices so that I can close them out easier. \n```\nfrom elasticsearch import Elasticsearch\nimport curator\nes = Elasticsearch([{'host': 'localhost'}])\ndef delete_indices(index, days):\n    \"\"\" Delete indices older than X days \"\"\"\n    ilo = curator.IndexList(es)\n    ilo.filter_by_regex(kind=\"prefix\", value=index)\n    ilo.filter_by_age(source=\"creation_date\", direction=\"older\", unit=\"days\", unit_count=days)\n    delete = curator.DeleteIndices(ilo)\n    try:\n        delete.do_action()\n        print \"Removed \"+index+\" indices older than \"+days+\" days old.\"\n    except curator.NoIndices:\n        print \"No \"+index+\" available for removal.\"\ndelete_indices(\"netflow-index\", 7)\ndelete_indices(\"syslog-index\", 60)\n``. Good call! That is working on my localhost OSXcurator --help`.\nWhen I install python + elasticsearch-curator on my Elasticsearch container I will try that and report back.. ",
    "sferry": "Hi, I ve install curator via rpm, and though \n$ curator --help \ndoes give me the output there is nothing if I try\n$ curator_cli --config config.yml show_indices\nI turned loggig on to DEBUG and left blacklist to []  to get the maximum info.\nsome lines puzzle me : \n<<\nTraceback (most recent call last):\n  File \"/home/buh/.local/lib/python3.6/site-packages/urllib3/connection.py\", line 141, in _new_conn\n  File \"/home/buh/.local/lib/python3.6/site-packages/urllib3/util/connection.py\", line 83, in create_connection\n  File \"/home/buh/.local/lib/python3.6/site-packages/urllib3/util/connection.py\", line 73, in create_connection\nConnectionRefusedError: [Errno 111] Connection refused\n\n\nand \n<<\n File \"/usr/local/lib/python3.6/http/client.py\", line 1239, in request\n  File \"/usr/local/lib/python3.6/http/client.py\", line 1285, in _send_request\n  File \"/usr/local/lib/python3.6/http/client.py\", line 1234, in endheaders\n  File \"/usr/local/lib/python3.6/http/client.py\", line 1026, in _send_output\n  File \"/usr/local/lib/python3.6/http/client.py\", line 964, in send\n  File \"/home/buh/.local/lib/python3.6/site-packages/urllib3/connection.py\", line 166, in connect\n\n\n\nThese directories does not exist.\nAnd I don't see where the setting of theses directories is comming from.\nFor some obscure reason the installed python is quite old:\npython-2.7.5-69.el7_5.x86_64\nthe server is\n$cat /etc/redhat-release \nCentOS Linux release 7.5.1804 (Core) \n$uname -a\nLinux xxxx.xxxx.xxxx.x 3.10.0-862.2.3.el7.x86_64 #1 SMP Wed May 9 18:05:47 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\nIs there somewhere a PATH definition to set ?\nCould it be related to to the current issue?\n. ",
    "ambis": "Can confirm. CentOS Linux release 7.5.1804 (Core) (SELinux as enforcing)\nInstalled via yum repo. No output unless I run LC_ALL=fi_FI.UTF-8 LANG=fi_FI.UTF-8 curator_cli\nIs this a Python problem, our could maybe the tool recongnize this situation and maybe tell the user what to do? This is pretty bad DX.. :). ",
    "marvin-w": "Setting LC_ALL to any available locale solved it for me as well, thanks!. ",
    "iget-esoares": "I can confirm that it doesn't run if there's any problem with LC_ALL being unset.. ",
    "konstantin-kornienko-epam": "Same thing (Centos 7), thanks for this topic!. ",
    "guidoilbaldo": "Ah, you're totally right! I also tried setting blacklist to an empty value, but still got nothing. With the empty array, I'm able to see that I need authentication\n2018-07-12 13:43:37,494 DEBUG         urllib3.util.retry               from_int:200  Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)\n2018-07-12 13:43:37,495 DEBUG     urllib3.connectionpool              _new_conn:208  Starting new HTTP connection (1): ad-eslb01\n2018-07-12 13:43:37,501 DEBUG     urllib3.connectionpool          _make_request:396  http://xxxxxx:9200 \"GET / HTTP/1.1\" 401 211\n2018-07-12 13:43:37,501 WARNING            elasticsearch       log_request_fail:97   GET http://ad-eslb01:9200/ [status:401 request:0.007s]\n2018-07-12 13:43:37,501 DEBUG              elasticsearch       log_request_fail:105  > None\n2018-07-12 13:43:37,502 DEBUG              elasticsearch       log_request_fail:110  < {\"error\":{\"root_cause\":[{\"type\":\"status_exception\",\"reason\":\"forbidden\",\"header\":{\"WWW-Authenticate\":\"Basic\"}}],\"type\":\"status_exception\",\"reason\":\"forbidden\",\"header\":{\"WWW-Authenticate\":\"Basic\"}},\"status\":401}\n. ",
    "bschneiders": "This appears to be specific to using environment variables for a templated action file.  If I test with a static example like above, it actually works as documented.  However, if I use environment variables, while they work reliably for individual indices and all other settings, it gets the error described for lists.\n$ export MYLIST=\"['dataset_1802941547','dataset_1799202629']\"\n$ cat test.yml\nactions:\n  1:\n    action: create_index\n    description: \"Create index as named\"\n    options:\n      name: dataset_r\n      extra_settings:\n        settings:\n          number_of_shards: 5\n          number_of_replicas: 1\n          refresh_interval: -1\n  2:\n    action: reindex\n    description: \"merge small indices together\"\n    options:\n      disable_action: False\n      ignore_empty_list: True\n      wait_for_active_shards: all\n      wait_for_completion: True\n      wait_interval: 30\n      max_wait: -1\n      request_body:\n        source:\n          index: ${MYLIST}\n        dest:\n          index: dataset_r\n    filters:\n    - filtertype: none\n  3:\n    action: index_settings\n    description: \"Enable refresh on new index\"\n    options:\n      index_settings:\n        index:\n          refresh_interval: 60s\n      ignore_unavailable: False\n      preserve_existing: False\n    filters:\n    - filtertype: pattern\n      kind: prefix\n      value: dataset_r\n$ curator ./test.yml\n...\n2018-07-20 18:56:47,798 DEBUG              elasticsearch    log_request_success:86   < {\"completed\":true,\"task\":{\"node\":\"JX_1361sSSe6ftc9R_cLJA\",\"id\":72235171,\"type\":\"transport\",\"action\":\"indices:data/write/reindex\",\"status\":{\n  \"total\" : 0,\n  \"updated\" : 0,\n  \"created\" : 0,\n  \"deleted\" : 0,\n  \"batches\" : 0,\n  \"version_conflicts\" : 0,\n  \"noops\" : 0,\n  \"retries\" : {\n    \"bulk\" : 0,\n    \"search\" : 0\n  },\n  \"throttled_millis\" : 0,\n  \"requests_per_second\" : -1.0,\n  \"throttled_until_millis\" : 0\n},\"description\":\"reindex from [['dataset_1802941547','dataset_1799202629']] to [dataset_r]\",\"start_time_in_millis\":1532112977762,\"running_time_in_nanos\":127688,\"cancellable\":true},\"error\":{\n  \"type\" : \"index_not_found_exception\",\n  \"reason\" : \"no such index\",\n  \"resource.type\" : \"index_or_alias\",\n  \"resource.id\" : \"['dataset_1802941547','dataset_1799202629']\",\n  \"index_uuid\" : \"_na_\",\n  \"index\" : \"['dataset_1802941547','dataset_1799202629']\"\n}}\n2018-07-20 18:56:47,799 DEBUG              curator.utils             task_check:1645 running_time_in_nanos = 0.000127688\n2018-07-20 18:56:47,799 INFO               curator.utils             task_check:1653 Task \"reindex from [['dataset_1802941547','dataset_1799202629']] to [dataset_r]\" completed at 2018-07-20T18:56:17Z.\n2018-07-20 18:56:47,799 DEBUG              curator.utils            wait_for_it:1754 Response: True\n2018-07-20 18:56:47,799 DEBUG              curator.utils            wait_for_it:1759 Action \"reindex\" finished executing (may or may not have been successful)\n...\nBut if I use a static file with the same list, it succeeds:\nactions:\n  1:\n    action: create_index\n    description: \"Create index as named\"\n    options:\n      name: dataset_r\n      extra_settings:\n        settings:\n          number_of_shards: 5\n          number_of_replicas: 1\n          refresh_interval: -1\n  2:\n    action: reindex\n    description: \"merge small indices together\"\n    options:\n      disable_action: False\n      ignore_empty_list: True\n      wait_for_active_shards: all\n      wait_for_completion: True\n      wait_interval: 30\n      max_wait: -1\n      request_body:\n        source:\n          index: ['dataset_1802941547','dataset_1799202629']\n        dest:\n          index: dataset_r\n    filters:\n    - filtertype: none\n  3:\n    action: index_settings\n    description: \"Enable refresh on new index\"\n    options:\n      index_settings:\n        index:\n          refresh_interval: 60s\n      ignore_unavailable: False\n      preserve_existing: False\n    filters:\n    - filtertype: pattern\n      kind: prefix\n      value: dataset_r. ",
    "rahst12": "+1. @untergeek Any progress on this?  We've got a lot of indices to move and 3 failed because of this.. unfortunately my next step was to delete the indexes, so I lost a fair amount of data because Curator continued instead of short-circuiting on the failure.\nVery surprising to have that happen.  Is there a way to verify either before or after Curator runs that either (1) nothing will error, or (2) something did error?  We're thinking maybe a Bash Script that runs the ReIndex command, manually counts docs and verifies, then run another Curator command?\nThanks. @untergeek :1st_place_medal: Awesome! Thanks for looking into this.. This would be very useful for what I'm doing right now.  We're looking at implementing this with a bash script in the meantime.  If you (@untergeek) have a workaround, I'd love to see it.. Optional would be great!. If you reindex going from a mapping file without nested documents to a mapping file with nested documents, the document count will be different.  I assume the doc count verification should be optional... . ",
    "epellizzer": "Ok, sorry for not being very clear.\nI have an action where I want to select the latest index of a sequence. I would do that with:\nyaml\nfiltertype: count\ncount: 1\npattern: ^(myindex)-\\d{6}$\nexclude: false\nreverse: true\nExcept the exclude: false part is ignored. I still get all but the last index of the sequence, that is exactly the same result as if I had used exclude: true.\nThough I'm not use to Python, I can see from the code of the function filter_by_count in indexlist.py that as soon as at least one index is rejected because it doesn't match the regex, the variable exclude is set to True. But this variable is the method's parameter, and as a result the rest of the method effectively behaves as if exclude had been set to True.\nNow I realize I should be able to work around that by first filtering with a pattern filter. That way, the count filter would not reject any index based on the regex, and the exclude parameter should not be overwritten.\nI hope it's clearer :).. It works with the workaround. Thank you, I'm closing.. ",
    "jfreedman0": "Thank you for this. This was the kind of explanation I was looking for. I appreciate your response.. ",
    "nikolay": "@untergeek Okay, let me give it a try!. ",
    "srolskyi": "@untergeek yeah, thx!. ",
    "rolodato": "Hey @untergeek, thanks for the super detailed response.\nI think my original question wasn't clear but I was looking for the opposite behaviour - I'd like Curator to \"fire and forget\" when calling _forcemerge, assuming that the merge request was valid and accepted (not sure if Elasticsearch provides a way to know this). In the current situation, I wouldn't be able to wait for a response since AWS Elasticsearch would return a 504 long before the merging is complete, and likely the Lambda function would time out as well. Hope that makes sense, thanks again!. I understand. Would it make sense for a forcemerge request made by Curator to be \"best effort\", that is, make the request and ignore the response (if any), even if the original request was invalid or rejected? If I write some custom code to do this, that is likely what I will end up doing to periodically forcemerge my indices.. I see, thanks very much for the suggestions! Do you know if there's an issue that I or others can track for forcemerge improvements in Elasticsearch?. ",
    "raynigon": "@untergeek Is this a way to go, or do you have a better idea to solve the Proxy Problem?. When do you think this will be available for Curator?. ",
    "Iril": "Yep now I see where I went wrong..\nGet:7 http://ftp.au.debian.org/debian stretch/main amd64 elasticsearch-curator all 4.2.5-1 [53.9 kB]\nIt seems version 4.2.5 is the default when you install via the logstash and elasticsearch instructions.\nie its in\ndeb https://artifacts.elastic.co/packages/6.x/apt stable main\nbut I needed\ndeb [arch=amd64] https://packages.elastic.co/curator/5/debian stable main\nApologies... ",
    "tbennett6421": "sorry, that is what I meant.\nI'm not sure if curator/urllib3 is seeing the certificate file I'm passing in, at least that's not what I can readily tell in the debug output. Is there anyway to tell?\nWe use this same CA to secure our entire elasticstack. Just moved the pem file into my home directory and tested with the same results.\nThis is running as myself in an interactive terminal, switching to root didn't seem to make a difference.. If I run curl -u admin:pass --cacert /etc/ssl/certs/sub_ca.pem https://m1.example.com:9200 \nI get the elasticsearch version and build info\nTo your second; there is, we have a corporate Root CA and several Sub CAs.\nWith that in mind, I generate the CA chain using cat sub_ca.pem root_ca.pem > chain.pem and retry\nwith the follow options\ncertificate: /etc/ssl/certs/chain.pem \nssl_no_validate: False\nand it appears to work as expected.. ",
    "elasticcla": "Hi @jrask, we have found your signature in our records, but it seems like you have signed with a different e-mail than the one used in yout Git commit. Can you please add both of these e-mails into your Github profile (they can be hidden), so we can match your e-mails to your Github profile?. ",
    "jrask": "Need to create a new PR with correct e-mail, not sure if I am allowed to use the one the used in this one.. @untergeek  - Seems like the errors where not related to my fix but stalled build during installation of java. Is it possible to force a travis rebuild?. ",
    "Tedderouni": "@cdenneen I didn't find a way to do this in Curator, so I just wrote up a bash script as a temporary hack to emulate what Curator does, until it supports this.  \nHere's what I use, feel free to adapt it to your environment.  In my environment, I define $ESUSER and $ESPASS in /etc/sysconfig/elasticsearch, and my alias names end with _rollover, so you'll need to update get_rollover_aliases() to match yours.  I run this out of cron.  And of course if you use this, make sure to run it as a dry run first (in the roll_over() function) :)\n```\n!/bin/bash\n. /etc/sysconfig/elasticsearch\nESHOST=\"localhost\"\nESNODE=\"${ESHOST}:9299\" # Must be a master node\nMAX_AGE=\"31d\"\nMAX_DOCS=\"2000000000\"\nMAX_SIZE=\"1tb\"\nfunction esapi-get() {\n    echo $(curl -s -XGET -u ${ESUSER}:${ESPASS} -H'Content-Type: application/json' http://${ESNODE}/$1)\n}\nfunction esapi-post() {\n    echo $(curl -s -XPOST -u ${ESUSER}:${ESPASS} -d \"$2\" -H'Content-Type: application/json' http://${ESNODE}/$1)\n}\nfunction exit_if_not_master() {\n    master_ip=$(get_master_ip)\n    my_ip=$(hostname -i)\nif [ \"$my_ip\" != \"$master_ip\" ]; then\n    log_write \"This node is not the master. Exiting.\"\n    exit 1\nelse\n    log_write \"This node is the master. Continuing.\"\nfi\n\n}\nfunction get_rollover_aliases() {\n    echo $(esapi-get \"_cat/aliases/*_rollover?h=a\")\n}\nfunction get_master_ip() {\n    echo $(esapi-get \"_cat/master?h=ip\")\n}\nfunction log_write() {\n    DATE=$(date +%Y-%m-%d\\ %H:%M:%S)\n    echo \"$DATE $1\"\n}\nfunction roll_over_all_aliases() {\n    for alias in ${alias_list}; do\n        log_write \"Attempting to roll over $alias: \"\n        roll_over \"$alias\"\n    done\n}\nfunction roll_over() {\n    esapi-post \"$1/_rollover?dry_run\" '\n    #esapi-post \"$1/_rollover\" '\n        {\n            \"conditions\": {\n                \"max_age\":   \"'${MAX_AGE}'\",\n                \"max_docs\":  '${MAX_DOCS}',\n                \"max_size\":  \"'${MAX_SIZE}'\"\n            }\n        }'\n}\nexit_if_not_master\nalias_list=$(get_rollover_aliases)\nroll_over_all_aliases\n```\n. @cdenneen Also I should mention you might be able to do this with the new Index Lifecycle Management feature, which was just released with 6.6.  I haven't had a chance to look into it yet, but if it works for this use case, it would be preferable over a temporary hack.\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/index-lifecycle-management.html . ",
    "vbohata": "For some indexes I already use period filter. Our agent (beats/nxlog/...) indexes are always created by logstash as we have a big environment and use message broker etc ... not important for this case.\nIn the past I used creation_date but had to switch to timestring.\n1. It is not usable as we often index old data (older then month for example) to appropriate indexes. So even if index creation date is today, it actually contains old data. Also sometimes we need to reindex the data and each reindex create a new index with current creation date even for very old data. So the only solution is to use time based index naming and timestring filtering.\n2. Filtering by creation_date is impossible so if not using timestring I have to use field_stats which is far slower (hours vs. seconds for thousands of indexes). Not possible to use - is slow and generates significant load. Also not possible to use if time field is not in the same field (causes curator to exit with error).. I am prefiltering it to reduce overhead, after prefiltering still thousands left. I found the ILM is is still under development, do you know the schedule for release date/ES version?\n\nThis is a limited scope use case that few users will need\n\nYes and now. Everyone who is using timestring needs it. The current behaviour is not a problem until it will become ... This is the type of issue when everything is OK, everything is OK, ... WTF! why my indexes disappears, where are my data ..... Maybe it will be easier to solve it. In curator/utils.py get_date_regex seems to allow arbitrary char, also TimestringSearch should be handle it so I should be able to set timestring to \"%Y.%m.%d$\". Am I right or is there anything I do not see?. ",
    "zafarella": "true, just saw it, but frankly you cannot notice it until you actually search deeply.\nI was looking into the way listing snapshots and this is how came to this page.\nMbe add it as \"Info\" or \"Tip\" section?. I think this is what I meant https://github.com/elastic/curator/pull/1273\nSorry for confusing, not much familiar with ascidocs yet . ",
    "markuchi": "Thanks for the reply.\nWe had curator setup to snapshot changes in the past 24 hours each day.\nHowever this started to fail and so we had a backlog of data to snapshot. \nEach day we index about 100GB of data. I estimate it would have needed to catch up on about a 2TB of data which has then put it into the state we are in.\nI have posted in discuss.elastic.co \nhttps://discuss.elastic.co/t/curator-snapshot-stuck-in-progress/148552\n. ",
    "Battleroid": "I might have spoke too soon, it works as expected, but it gets weird when we have two aliases that point to the same index, each being the same index. For example:\na                  i\nmysql-slow-session mysql-slow-session-2018.09.18-000003\nrequest-id         mysql-slow-session-2018.09.18-000003\nThis is due to the fact that the template for mysql-slow-session maps to the request-id alias on creation. Exception that's returned can look like this:\nRollover alias [request-id] can point to multiple indices, found duplicated alias [[request-id]] in index template [mysql-slow-session]. IIRC request-id is an alias for multiple different index patterns, it's not actually set for an individual rollover alias. We also do not create rollover aliases by using a template, so no worries there. This is good info though, thank you.\nThe simple solution for our use case is to just do the bare minimum and check if the index itself begins with the alias name, e.g. does abc-foo-2018.11.16-000001 start with abc-foo. Since request-id isn't attached to anything actually named that.. ",
    "gallardo": "Closing then, as it is on purpose.. ",
    "pavolloffay": "+1 for this feature.. Are there any updates on this?. Would you accept a PR adding aliases to https://curator.readthedocs.io/en/latest/actionclasses.html#rollover action?. Could you please propose a workaround with the current version of curator? \nWe have to call _rollover API and add the newly created index into the read alias. How this can be accomplished with the current curator?. I have played with curator and created the following action files which can be used to \nCreate index jaeger-span-archive-000001 and put it into read and write aliases.\nyml\nactions:\n  1:\n    action: create_index\n    description: Create archive index.\n    options:\n      name: jaeger-span-archive-000001\n      continue_if_exception: False\n      disable_action: False\n  2:\n    action: alias\n    description: Create read alias for archive index.\n    options:\n      name: jaeger-span-archive-read\n      continue_if_exception: False\n      disable_action: False\n    add:\n      filters:\n        - filtertype: pattern\n          kind: prefix\n          value: jaeger-span-archive\n  3:\n    action: alias\n    description: Create write alias for archive index.\n    options:\n      name: jaeger-span-archive-write\n      continue_if_exception: False\n      disable_action: False\n    add:\n      filters:\n        - filtertype: pattern\n          kind: prefix\n          value: jaeger-span-archive\nRollover the index and put the newly created index into read alias.\n```yml\nactions:\n  1:\n    action: rollover\n    description: Rollover the index associated with write alias.\n    options:\n      name: jaeger-span-archive-write\n      conditions:\n        max_age: 1s\n      continue_if_exception: False\n      disable_action: False\n3:\n    action: alias\n    description: Create write alias for archive index.\n    options:\n      name: jaeger-span-archive-read\n      continue_if_exception: False\n      disable_action: False\n    add:\n      filters:\n        - filtertype: alias\n          aliases:\n            - jaeger-span-archive-write\n```\nThe second action is an alternative to:\njson\nurl -ivX POST -H \"Content-Type: application/json\" localhost:9200/jaeger-span-archive-write/_rollover -d '{\n  \"conditions\": {\n    \"max_age\":  \"1s\",\n    \"max_docs\":  1\n  },\n  \"aliases\": {\n    \"jaeger-span-archive-read\": {}\n  }\n}'\n@untergeek I am wondering if the action file can be done atomically, eg. if the last item fails would  it rollback previous changes? Is the curl command I have posted done atomically?  Is it guaranteed that new index is also put to read alias? . ",
    "Sujeet1604": "@untergeek , I think it won't change much as elasticsearch.py accepts body as parameter for rollover. And that body is getting created in actions.py under rollover class with settings & conditions. So if we modify actions.py as \ndef body(self):\n        \"\"\"\n        Create a body from conditions and settings\n        \"\"\"\n        retval = {}\n        retval['conditions'] = self.conditions\n        if self.settings:\n            retval['settings'] = self.settings\n        if self.additionalalias:\n            retval['aliases'] = self.additionalalias\n        return retval\nand required definitions for the same in class as optional. We can add it from action file as \noptions:\n  name: aliasname\n  conditions:\n    max_age: 1d\n    max_docs: 1000000\n    max_size: 5gb\n   additionalalias:\n      xg_test_sql_read: {}\nI may have few syntax or small issues in code. But see if it works.. ",
    "AlexClineBB": "pip install click==6.7 elasticsearch-curator==5.5.4 can be used as a workaround.. ",
    "bungoume": "@untergeek \nCould you release the patched version?\nwe need to change many install batch by this issue.. ",
    "Andrewsville": "Merged as part of #1284. ",
    "geethasrihub": "Does curator work with elastic search 6.3 to delete the indices which are 30 days old without deleting the index.. ",
    "Careidas77": "Hi @untergeek, and thank you for replying :)\nI'm using Elastic v. 6.2.2. I can also tell from the the logs I get that Curator batches the calls. Still, this error occurs for every single batch. I've also tried another command, trying to close open indices, but get similar errors.\nI changed the log level to debug, and dumped everything into a file which I attached:\ncurator_log.txt\nAs you can see, the request is indeed capped around 3 k UTF-8 bytes in the URL, so perhaps my choice of headline for this issue was misleading. It's just that shorting the URL actually makes the request go through, and Elastic clearly thinks that the original request deserves a 400 response. The true reason why it fails may actually be something else, but the HTTP Error 400. The request URL is invalid bit suggests somethings fishy in these parts, right?\nI changed the headline for the issue to something perhaps less suggestive.. @untergeek I don't think Curator is the culprit. There seems to be a proxy in between Curator and Elastic that's messing things up. I managed to bypass it, and now the request batches goes through just fine.\nSo, sorry for taking up your time, but thank you again for responding.. ",
    "1oglop1": "Good to hear that. \nI'll make a note of this issue into my code in case something breaks.\nYou @untergeek can close it if you wish.. ",
    "mrtmexx": "@untergeek, when do you plan to implement this functionality?. ",
    "rsteneteg": "I created the the fork of the repo a day or so ago, the last commit to master was 14 days ago so I expected my fork to be the same as your master. I just verified that I am in sync with your master.. so I am not sure why all those tests fail. Since those tests failed before I made any changes, I thought it was expected that those would fail, so I just made sure the test for the function I changed worked.. The fix_epoch function should work better now.. I was a bit to strict on the type check and did not think of strings that correctly converts to int. However I still get lots of other test failures that to me seems unrelated.. ok.. Travis CI seems to have not had the same problems with the other tests that I had.. probably some environment issue for the integration tests.. Thank you, we have mitigated the issue on our side so no need to rush, do it when you find time.. Thanks, but those lines are outdated, replaced them with doing the int(epoch) within a try/catch block to capture bad input values.. ",
    "scriptdb": "Short one liner until this gets committed:\ncurl -s \"http://localhost:9200/_cat/indices/$INDEX?v&h=docs.count&pretty\" | awk '{s+=$1}END{print s}'\nWhere INDEX='logstash-2018.11.*'\nThis will return the sum of docs.count for all daily indices. Later you can run the same command on the new reindexed index and compare the two sums. . ",
    "17721524631": "\nRPM\u548cDEB\u5b89\u88c5\u7a0b\u5e8f\u4e5f\u4e0d\u63d0\u4f9b\u9ed8\u8ba4\u7684YAML\u914d\u7f6e\u6587\u4ef6\u3002\n\u6709\u5173\u793a\u4f8b\u548c\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\u8bf4\u660e\uff0c\u8bf7\u53c2\u9605\u5b98\u65b9\u6587\u6863\u3002\n\n\u6211\u53ef\u4ee5\u81ea\u5df1\u5199\u914d\u7f6e\u6587\u4ef6\uff0c\u4f46\u662f\u600e\u4e48\u548c\u5b83\u8fde\u63a5\u8d77\u6765\u5462\uff1f  \u6587\u6863\u91cc\u5e76\u6ca1\u6709\u8bf4\u660e. > > RPM\u548cDEB\u5b89\u88c5\u7a0b\u5e8f\u4e5f\u4e0d\u63d0\u4f9b\u9ed8\u8ba4\u7684YAML\u914d\u7f6e\u6587\u4ef6\u3002\n\n\n\u6709\u5173\u793a\u4f8b\u548c\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\u8bf4\u660e\uff0c\u8bf7\u53c2\u9605\u5b98\u65b9\u6587\u6863\u3002\n\n\u5df2\u7ecf\u521b\u5efa\u4e86\u8fd9\u4e2a\u76ee\u5f55 \u5f53\u6267\u884c\u8fd9\u6bb5\u547d\u4ee4\u65f6 \u4f9d\u7136\u62a5\u9519\ncurator --config ~/.curator/curator.yml. > \u8fd8\u6709\u8bb8\u591a\u5176\u4ed6\u793a\u4f8b\u64cd\u4f5c\u6587\u4ef6\u3002\n\u8fd9\u662f\u4e00\u4e2adelete_indices\u884c\u52a8\u3002\n\n\u539f\u6765\u662f\u8981\u4e00\u6b21\u6027\u914d\u7f6e\u4e24\u4e2a   \u7528\u60a8\u7684\u8fd9\u79cd\u683c\u5f0f\u8bd5\u4e86\u4e00\u904d    \u6210\u529f\u914d\u7f6e\u4e86   \u8c22\u8c22. ",
    "TBeijen": "This effectively prevents curator from running without errors on new cluster that not yet have a certain amount of indexes.. ",
    "kciredor": "Awesome, thx @untergeek . ",
    "like-inspur": "OK\uff0c I will try it , thank you!. ",
    "3h4x": "No, sorry for being ambiguous. \nFor ES 5.5.2 cluster I use curator 5.5.2.\nFor ES 6.4.2 I have used most recent curator version 5.6.0.. Should curator 5.6.0 theoretically work with ES 6.x? is it preferred way to rotate indexes on ES6.x?\nI'm missing this information from README, ES 6.x is not in compatibility matrix.. Hey, \nthanks for input. I'm using docker containers so there is not a chance for mess in pip packages.\nIn line 318 of elasticsearch/transport.py library get status, headers_response, data from ES.\nFor our old elastic.co cluster, version 5.6.10 it returns data with application/json content type:\n'{\\n  \"name\" : \"instance-x\",\\n  \"cluster_name\" : \"x\",\\n  \"cluster_uuid\" : \"x\",\\n  \"version\" : {\\n    \"number\" : \"5.6.10\",\\n    \"build_hash\" : \"b727a60\",\\n    \"build_date\" : \"2018-06-06T15:48:34.860Z\",\\n    \"build_snapshot\" : false,\\n    \"lucene_version\" : \"6.6.1\"\\n  },\\n  \"tagline\" : \"You Know, for Search\"\\n}\\n'\nFor new cluster 6.4.3 it returns text/html content type data:\n\"<script>var hashRoute = '/app/kibana';\\nvar defaultRoute = '/app/kibana';\\n\\nvar hash = window.location.hash;\\nif (hash.length) {\\n  window.location = hashRoute + hash;\\n} else {\\n  window.location = defaultRoute;\\n}</script>\"\nWe are using elastic.co cluster. Is it any different than normal ES?\nHaving said that, it's not problem of curator per se. Issue can be closed.\nI think it would be beneficial to add  to README\n\nCurator 5.x releases are mostly platform agnostic for Elasticsearch versions 5.x and 6.x. Obiously. I'm such a dummy. . \n",
    "rectalogic": "This PR makes curator compatible with both click 6.x and 7.x with no breaking changes in curator. I think the failing tests are tests that were skipped when I ran tests (all tests pass for me with both click 6.7 and 7.0, but 4 tests were skipped for some reason). I'll push a change that should fix these failures.\nIt looked to me like curator disallowed 7.0 because 7.0 has breaking changes, so fixing curator to be compatible with both click 6.x and 7.x seemed like an alternative to forcing an older version of click.. I don't know why travis is failing now. The only error I see is related to downloading jdk:\nDownloading Oracle Java 8...\n--2018-11-29 22:06:59--  http://download.oracle.com/otn-pub/java/jdk/8u191-b12/2787e4a523244c269598db4e85c51e0c/jdk-8u191-linux-x64.tar.gz\nResolving build-cache.travisci.net (build-cache.travisci.net)... 10.80.1.2\nConnecting to build-cache.travisci.net (build-cache.travisci.net)|10.80.1.2|:80... connected.\nProxy request sent, awaiting response... 302 Moved Temporarily\nLocation: https://edelivery.oracle.com/otn-pub/java/jdk/8u191-b12/2787e4a523244c269598db4e85c51e0c/jdk-8u191-linux-x64.tar.gz [following]\n--2018-11-29 22:06:59--  https://edelivery.oracle.com/otn-pub/java/jdk/8u191-b12/2787e4a523244c269598db4e85c51e0c/jdk-8u191-linux-x64.tar.gz\nResolving false (false)... failed: Name or service not known.\nwget: unable to resolve host address \u2018false\u2019\ndownload failed\nOracle JDK 8 is NOT installed.\ndpkg: error processing package oracle-java8-installer (--configure):\n subprocess installed post-installation script returned error exit status 1\nErrors were encountered while processing:\n oracle-java8-installer\nE: Sub-process /usr/bin/dpkg returned an error code (1)\nThe command \"sudo apt-get update && sudo apt-get install oracle-java8-installer\" exited with 100.\n. ",
    "hueyg": "Thanks unter.  I definitely installed via yum repo so not sure what dependency is off on my side.  Will check for updates on my openssl.. Here is what I used.  Installed this a few weeks ago without configuring because I was pulled off on another project.\n[curator-5]\nname=CentOS/RHEL 7 repository for Elasticsearch Curator 5.x packages\nbaseurl=https://packages.elastic.co/curator/5/centos/7\ngpgcheck=1\ngpgkey=https://packages.elastic.co/GPG-KEY-elasticsearch\nenabled=1\n. ",
    "stefancoetzeetakealot": "It looks like windows is \"interpreting\" the string, try escaping the \" with a backslash.\neg\ncurator_cli --host localhost --port 9200 delete_indices --filter_list '{\\\"filtertype\\\":\\\"none\\\"}'\n. ",
    "samcro1967": "I tried adding back slash to escape the parenthesis.  It still errs out.\ncurator_cli --host localhost --port 9200 delete_indices --filter_list '{\\\"filtertype\\\":\\\"none\\\"}'\nUsage: curator_cli delete_indices [OPTIONS]\nError: Invalid value for \"--filter_list\": Filter list is invalid JSON: '{\"filtertype\":\"none\"}'. Replacing the single quotes around the filtertype JSON and escaping the double quotes with a back slash inside the filtertype JSON seems to have worked.\nC:\\Program Files\\elasticsearch-curator>curator_cli --host localhost show_indices --verbose --header --filter_list \"{\"filtertype\":\"age\",\"source\":\"name\",\"timestring\":\"%Y.%m.%d\",\"unit\":\"days\",\"unit_count\":30,\"direction\":\"older\"}\"\nIndex State Size Docs Pri Rep Creation Timestamp\ntelegraf-2018.12.13 open 103.8MB 1105152 5 1 2018-12-13T22:09:42Z\ntelegraf-2018.12.14 open 1.5GB 16806115 5 1 2018-12-14T00:00:00Z\ntelegraf-2018.12.15 open 1.5GB 17311397 5 1 2018-12-15T00:00:06Z\ntelegraf-2018.12.16 open 1.6GB 17499439 5 1 2018-12-16T00:00:08Z\ntelegraf-2018.12.17 open 1.6GB 17921179 5 1 2018-12-17T00:00:02Z\ntelegraf-2018.12.18 open 1.6GB 18098332 5 1 2018-12-18T00:00:06Z\ntelegraf-2018.12.19 open 1.6GB 18118504 5 1 2018-12-19T00:00:01Z\ntelegraf-2018.12.20 open 1.5GB 17397983 5 1 2018-12-20T00:00:04Z\ntelegraf-2018.12.21 open 1.3GB 16636635 5 1 2018-12-21T00:00:00Z\ntelegraf-2018.12.22 open 1.3GB 16824327 5 1 2018-12-22T00:00:05Z\ntelegraf-2018.12.23 open 1.3GB 17055585 5 1 2018-12-23T00:00:00Z\ntelegraf-2018.12.24 open 1.3GB 17043244 5 1 2018-12-24T00:00:03Z\ntelegraf-2018.12.25 open 1.1GB 14056419 5 1 2018-12-25T00:00:07Z\nSwitching to delete_indices seems to execute, but errored out.\nC:\\Program Files\\elasticsearch-curator>curator_cli --host localhost delete_indices --filter_list \"{\"filtertype\":\"age\",\"source\":\"name\",\"timestring\":\"%Y.%m.%d\",\"unit\":\"days\",\"unit_count\":30,\"direction\":\"older\"}\"\n2019-01-24 10:32:52,885 INFO Deleting selected indices: ['telegraf-2018.12.23', 'telegraf-2018.12.13', 'telegraf-2018.12.25', 'telegraf-2018.12.18', 'telegraf-2018.12.20', 'telegraf-2018.12.19', 'telegraf-2018.12.22', 'telegraf-2018.12.16', 'telegraf-2018.12.21', 'telegraf-2018.12.17', 'telegraf-2018.12.14', 'telegraf-2018.12.15', 'telegraf-2018.12.24']\n2019-01-24 10:32:52,885 INFO ---deleting index telegraf-2018.12.23\n2019-01-24 10:32:52,888 INFO ---deleting index telegraf-2018.12.13\n2019-01-24 10:32:52,889 INFO ---deleting index telegraf-2018.12.25\n2019-01-24 10:32:52,892 INFO ---deleting index telegraf-2018.12.18\n2019-01-24 10:32:52,892 INFO ---deleting index telegraf-2018.12.20\n2019-01-24 10:32:52,893 INFO ---deleting index telegraf-2018.12.19\n2019-01-24 10:32:52,894 INFO ---deleting index telegraf-2018.12.22\n2019-01-24 10:32:52,896 INFO ---deleting index telegraf-2018.12.16\n2019-01-24 10:32:52,896 INFO ---deleting index telegraf-2018.12.21\n2019-01-24 10:32:52,897 INFO ---deleting index telegraf-2018.12.17\n2019-01-24 10:32:52,901 INFO ---deleting index telegraf-2018.12.14\n2019-01-24 10:32:52,903 INFO ---deleting index telegraf-2018.12.15\n2019-01-24 10:32:52,904 INFO ---deleting index telegraf-2018.12.24\n2019-01-24 10:33:22,905 CRITICAL Failed to complete action: delete_indices. : Exception encountered. Rerun with loglevel DEBUG and/or check Elasticsearch logs for more information. Exception: ConnectionTimeout caused by - ReadTimeoutError(HTTPConnectionPool(host='localhost', port=9200): Read timed out. (read timeout=30))\ndelete_indices appearsd to have partially worked. I suspect this is a different issue and not related to MS DOS command line syntax errors. When I rerun show_indicies, the same ones are listed, but there size is all zero now with the exception of one of them.\nC:\\Program Files\\elasticsearch-curator>curator_cli --host localhost show_indices --verbose --header --filter_list \"{\"filtertype\":\"age\",\"source\":\"name\",\"timestring\":\"%Y.%m.%d\",\"unit\":\"days\",\"unit_count\":30,\"direction\":\"older\"}\"\nIndex State Size Docs Pri Rep Creation Timestamp\ntelegraf-2018.12.13 open 0.0B 0 5 1 2018-12-13T22:09:42Z\ntelegraf-2018.12.14 open 0.0B 0 5 1 2018-12-14T00:00:00Z\ntelegraf-2018.12.15 open 0.0B 0 5 1 2018-12-15T00:00:06Z\ntelegraf-2018.12.16 open 0.0B 0 5 1 2018-12-16T00:00:08Z\ntelegraf-2018.12.17 open 1.6GB 17921179 5 1 2018-12-17T00:00:02Z\ntelegraf-2018.12.18 open 0.0B 0 5 1 2018-12-18T00:00:06Z\ntelegraf-2018.12.19 open 0.0B 0 5 1 2018-12-19T00:00:01Z\ntelegraf-2018.12.20 open 0.0B 0 5 1 2018-12-20T00:00:04Z\ntelegraf-2018.12.21 open 0.0B 0 5 1 2018-12-21T00:00:00Z\ntelegraf-2018.12.22 open 0.0B 0 5 1 2018-12-22T00:00:05Z\ntelegraf-2018.12.23 open 0.0B 0 5 1 2018-12-23T00:00:00Z\ntelegraf-2018.12.24 open 0.0B 0 5 1 2018-12-24T00:00:03Z\ntelegraf-2018.12.25 open 0.0B 0 5 1 2018-12-25T00:00:07Z. updated findings on #1329. ",
    "keshrisohit": "Apologies it seems it has this feature.. ",
    "holiiveira": "Hmmm, you're right.\nWorks with the wrapper.. ",
    "thomasryck": "Es and Curator are not running in different containers for now, ES is running on the curator container host. I will try to investigate that.\nthanks,. You got it. \n. ",
    "osigida": "I don't try to create additional values, I have one host per line, but I want to be able to config HOST and PORT\nand I cannot do it in the HOST part of the config\nexample, I want to have this config, and be able to define host and port from env variables + have default values if nothing was set \ntarget config\nclient:\n  hosts:\n    - logstash.host.com:80\nexpected config should looks like - ${ELASTIC_SEARCH_HOST:logstash.host.com}:${ELASTIC_SEARCH_PORT:80}\nbut this won't work... \n. it took me some time to realise what is actually not supported, now I see, sorry for bothering . ",
    "ctong-ttc": "I deleted the indices and did the restore with the double quotes. The indices were restored correctly, within about 40 minutes, but Curator did not recognize this and timed out.\n2019-01-07 13:31:03,145 ERROR     Unable to complete action \"restore\" within max_wait (3600) seconds.\n2019-01-07 13:31:03,145 ERROR     Failed to complete action: restore.  <class 'curator.exceptions.FailedExecution'>: Exception encountered.  Rerun with loglevel DEBUG and/or check Elasticsearch logs for more information. Exception: Action \"restore\" failed to complete in the max_wait period of 3600 seconds\n\n. ",
    "madisonb": "Is there a solution for folks using an ES version less than 6.4? Right now running on a cluster version 6.3 the copy_settings parameter is inconsistent and the curator.Shrink() operation sometimes needs multiple tries on a single index in order to get copied and shrank correctly. \nIn some cases:\n The index is created but fails to allocate (due to the field count being higher than 1000) - this causes the cluster to go red\n The operation fails during the DELETE phase, being called with an invalid parameter - this causes duplicate data in your cluster until you manually delete the original index\n* Other situations where the aliases is not copied over and the operation fails\nIf you clean up the cluster and get it back to green, and try the Shrink() enough times, the operation will succeed.. ",
    "IzekChen": "@untergeek Hi Aaron, I try to find out how to remove the step or exclude the it of   \"test_multi_data_path_node\"\nBut not sure how to do it. Can you give me some advice?. ",
    "bhansenmis": "@untergeek Thanks for the quick reply.  If you could tag this issue if you find out any additional info in the future as to if this would be possible, I'd appreciate it.  Thank you!. ",
    "gauchoux": "I forgot to mention that I have an action file that deletes indexes older than 10 days and works without any problem so it shouldn't be a connection issue.. Strangely enough, today it worked but only for a few indexes.\nlog2.txt\n. So it seems that only indexes with more than 10 segments get Forcemerged, is that a normal behavior ?. First, thank you very much for your help !\nOkay so I added,\n- filtertype: forcemerged\n          max_num_segments: 1\n          exclude: True\nto my Action file but the same outcome still happens.\nEvery index has more than 1 segment, so shouldn't every index be selected (and forcemerged), or am I missing something ?. Okay I didn't get the part that it was per shard.\nIt all makes sense now.\nThanks again for your help.. ",
    "matthewdupre": "Thanks for the suggestion @untergeek - snapshot list updated.  I think this should be ready now.. ",
    "aogg": "2019-01-23 02:42:10,354 DEBUG curator.cli run:130 continue_if_exception = False\n2019-01-23 02:42:10,354 DEBUG curator.cli run:132 timeout_override = None\n2019-01-23 02:42:10,354 DEBUG curator.cli run:134 ignore_empty_list = True\n2019-01-23 02:42:10,354 DEBUG curator.cli run:136 allow_ilm_indices = False\n2019-01-23 02:42:10,354 INFO curator.cli run:146 Preparing Action ID: 1, \"delete_indices\"\n2019-01-23 02:42:10,354 DEBUG curator.utils get_client:802 kwargs = {'url_prefix': '', 'aws_secret_key': None, 'http_auth': 'user:pass', 'certificate': None, 'aws_key': None, 'aws_sign_request': False, 'port': 9200, 'hosts': ['elasticsearch'], 'timeout': 30, 'aws_token': None, 'use_ssl': False, 'master_only': False, 'client_cert': None, 'ssl_no_validate': False, 'client_key': None}\n2019-01-23 02:42:10,356 DEBUG curator.utils get_client:877 \"requests_aws4auth\" module present, but not used.\n2019-01-23 02:42:10,361 DEBUG curator.utils check_version:688 Detected Elasticsearch version6.2.4\n2019-01-23 02:42:10,361 DEBUG curator.cli run:161 client is \n2019-01-23 02:42:10,361 ERROR curator.cli run:186 Failed to complete action: delete_indices. : 'ascii' codec can't encode characters in position 0-1: ordinalnot in range(128). ",
    "qqshfox": "\nCan you better describe the problem this fixes? I\u2019ve seen no issue submitted for it.\n\nupdated with more details. ",
    "rmorandell-pgum": "Hi Aaron, thank you for the answer. I edited the issue so u can see the yml formatted.. Great. That solved my problem. Thank you very much.. ",
    "nerophon": "This has been tested on ES 6.5.4 in a local environment and it works as expected. Therefore we may have confidence that the version isn't the problem here.. ",
    "igajsin": "I took the docker image python:3-alpine and install curator in it via pip install elasticsearch-curator.\nThan built a new image and run it with docker run -e VAR=VALUE syntax. \nAlso, let me try to use another version of python, maybe it helps.. The same for the python 2.7 version:\ncurator.log\n. Interesting. A shell within a Docker container shows me those variables. So it should work in theory. But when I move it out of docker it starts to actually work.\nI think I can close the ticket, cause the problem is out of curator.. P.S. And anyway thanks for responce.. ",
    "perlpunk": "It should actually not be necessary to change to UnsafeLoader, and we will fix that in the next release.\nI should note that PyYAML does not know anything about loading environment variables, so I suspect this project adds some custom constructors.\nThe custom constructors added via yaml.add_constructor currently (in 5.1) go to the wrong default which is UnsafeLoader. This will be fixed in the next release. So it's probably a good idea to wait (not more than a couple of days I hope) for 5.2.. "
}
{
    "tjake": "Not sure how this relates to Lucandra?  In many ways Lucandra provides the same benefits as zoie.  distributed search, real-time, etc.\n. k\n. I've tried it but stopped when all the tests broke :) I'll want todo this at some point, preferably when there is a real reason.  I'd happily accept a patch if you are willing to work on it. thx.\n. thank you kumm, committed!\n. Looks like you are using the RandomPartitioner instead of the OrderPreservingPartitioner. This is required in the cassandras storage-conf.xml\n. This number is a hardcoded limit currently (1000000) you can up it if you wish. Though pulling millions of columns from Cassandra to sort is going to be very expensive. \nI'm working on a better way to handle sorts (within Cassandra and not lucene)\nJake \n. nice find! thanks, this affected exact searches with more than 1 of the same term \n\"foo foo\".  Added a test case as well, thx!\n. Ok, finally fixed this. fields with same name are not concatenated together and separated by a space.\n. Ok thanks, I'll take a look\n. Is this still happening?\n. Actually, cassandra guys decided to ditch String keys for byte[], this will fix the issue, I assume its going in 0.6.5 but you can see it now in trunk.\n. fixed in cassandra 0.6.5\n. I'm working on a custom query filter for lucandra, in the meantime have you considered creating 3 indexes representing each of your types and search from the one you need rather than filtering from one big index?\n. This has been addressed as part of http://github.com/tjake/Lucandra/issues/closed#issue/16\nPlease see testLucandraFilter unit test for example.\n. IndexReader.terms() is not currently supported. To support this I'll need to add incremental chunking of all terms from an index.  not a very high priority at this point.\n. Nevermind, i found a workaround, this page works now but the index stats are bogus.\n. Reader should be reopened whenever you want to see changes. this does nothing more then clear its internal cache.\n. I'm not able to reproduce this. is this happening with solr? or with lucene directly?\nthe solrconfig.xml in example should do no caching.\n. Looks like solr is still caching sorts. I need to replace the default QueryComponent with my own\n. fixed by issue 37\n. There are still some solr caches I need to bypass that could be causing this. I'm going to create my own SearchComponent for this purpose\n. The connections are now more tolerant, could you give it another try?\n. this is fixed\n. This was inherited from lucene. see http://wiki.apache.org/lucene-java/LuceneFAQ#How_do_I_update_a_document_or_a_set_of_documents_that_are_already_indexed.3F\nWhile this may be doable, I think it's going to end up having a lot of edge conditions and complexity.  But feel free to submit a patch :)\n. What if you really want to delete a field from a previous document? this only adds or replaces?\n. Yes this is a known issue. I'm considering writing a custom queryfilter class to deal with this situation. Can you filter it in your app layer for now?\n. If the filter class knew which documents were fetched for all other search results then the query filter could simply filter those specific documents. \nAnother short term idea for you: Try creating an index per category, so you can query from one or the other. rather than filtering from one.\n. Cool! If you know git then you can fork this project and patch it then I can merge it in. Otherwise you can email me the patch jakers at gmail. Thanks! \n. No. I didn't. Did you send it to jakers at gmail dot com?\n. Yeah thanks. I'll look at this further tonight but what I saw looked great!\nYou should pick up git so you can contribute more easily :) look at the github tutorials\n. Checked in! thanks a ton, just what I was planning :)\n. I think this was just caused by the bookmark using the wrong analyzer. I checked in a fix, can you try again?\n. Thanks for the example. I was able to fix it now.\n. closed\n. Is your Cassandra config upto date? I stared using supercolumns a few weeks ago so you'll need to recreate your indexes. \n. Fixed. thx for the report\n. Yes that's the idea.  I'm working on this.  Its also possible to do something like the query filter but for sorting. \n. This is a fundemental problem with lucandra, the newer solandra should handle this problem better.\n. Hmm, do the boxes have the same system Locale setting?\n. I think I've fixed this by ensuring all byte arrays use UTF-8 encoding. you'll need to delete and reload your index\n. fixed\n. Not able to reproduce your error,  in the first query.  Could it be you didn't refresh your index after I updated the internal format a week or so ago? try a fresh index.  \nAs for your second query, I don't think Lucene allows this.  You can enable something like this with  qp.setAllowLeadingWildcard( true );\nThen \"fieldname:IN_2000\" will work but this is REALLY expensive, unless you have relatively few fieldname values.  \nmore info: http://wiki.apache.org/lucene-java/LuceneFAQ#What_wildcard_search_support_is_available_from_Lucene.3F\n. Hmm, let me think of a way to do this better. so the fields are returned as multiple fields in the document\n. This is now fixed, Document.getFields(\"fieldname\") will now return you back multiple fields.\n. thx missed that\n. Why do you need multiple keyspaces? You can have any number of indexes within a keyspace.\n. I need keyspace to be a static global parameter, I could add a static method to alter the keyspace at runtime, but it would be upto you to be sure this is safe for your application.  Does this work for you?\n. Done\n. fixed, sorry i removed it by mistake last night\n. fixed, sorry i removed it by mistake last night\n. empty\n. Hi,  Lucandra was written with the idea of calling reopen()/clearCache() after each search.  Although it's possible to address this use case so it works. But it's meant to be called as you have it (clearCache())\n. Are you saying you are seeing more than one being added?\nI don't think this is the case.  If the field isn't tokenized then the there will be no tokens added, the second condition handles this (the case when its not tokenized).\nThanks, and let me know\n. Hmm, well a unchecked exception probably isn't the best choice. but in this situation what would work for you?\n. What if I kept retrying the supplied address?  This is how the code works for all other cases (failed node, etc).\n. Problem is if I make it checked exception then I'd pollute the Lucene API. Perhaps I can embed it in a IOException or CorruptedIndexException?\n. Cool thanks. I'll make this configurable. \n. ignored\n. Ah. I think this can be fixes by overriding indexreader.getfieldcachekey()\nSo each thread returns a diff key.\n. fixed, thanks for the reminder!\n. Thx\n. fixed.\n. Thanks for the feedback!  The good news is the v2 of Lucandra, which is about to be pushed, handles these issues.  Specifically MatchAllDocs delete.\nAre you opposed to using solr?\n. properly fixed. https://github.com/tjake/Lucandra/commit/ed9ce5199f691f974a5becbda6454679bd1c177e\n. Would you be able to supply a test so I can look into reproducing this? thanks!\n. Is pt_BR locale considered UTF-8?  at this point all terms are stored as UTF-8.\n. Thanks, good to know\n. Can you tell me what branch of lucandra you are using? What version of cassandra? and if you are calling reopen on your IndexReaders after each query?\n. Let me try to reproduce this, but it sounds like a bug in the 0.6 code\n. Could you try the latest version as of this morning?\n. I think the latest commit to 0.7 branch fixes this problem by refetching if it encounters deleted data\n. thanks, fixed\n. Did you update recently? I may have broken this while renaming things, I'll look at this tomorrow on the plane\n. Well I added your fix for now. Have your upgraded all nodes to the latest? \n. Yes, just set SHARDS_AT_ONCE=\"1\" in start-solandra.sh\n. Can you post the log?\n. Hmm, have you been deleting documents? could be a bug in the delete logic leaving behind old references\n. What changes have you made to the code and/or settings?\nBTW, you can chat me directly on irc freenode #solandra\n. Could you also post the schema.xml you are using?\n. This ended up being the  in schema.xml wasn't marked stored=\"true\"\n. Thanks, I'll be pushing a bunch of internal changes soon and will address this\n. I've pushed the changes I mentioned, please take a look.\nNo longer getting exceptions but the dismax queries seem to only return results after a similar query on standard search handler. I must be doing something wrong.\n. Cool is it live? what's the url?\n. How many many documents in the indexes?\nAre you running this on a single node?  Solandra won't be faster than Solr on a single node keep in mind.  Also you have shards.at.once=2 which doesn't make sense if you have 1 node.  It should roughly be the same as the number of nodes in your cluster.\n. it will help with write tps but it will increase the amount of memory needed :)\n. How many documents in your index?\n. any info on this?\n. I'll close as I cannot re-produce this\n. Are you only indexing at this point?\n. For a lower memory machine (1.7 gigs is low)\nyou can:\nchange the following in cassandra.yaml\nThe threshold size in megabytes the binary memtable must grow to,\nbefore it's submitted for flushing to disk.\nbinary_memtable_throughput_in_mb: 256\nYou can also change the caching parameters in solandra.cml\n. Well both Solr and Cassandra running on the same box will require more memory than just Cassandra or Solr :)\n. Can you give me an idea of how many documents you have? how much ram do you see solr using for the same?\n. closed\n. Looks like you had a big GC that caused the reads to back up and the JVM was running out of memory.\nWhat kind of writes per second were you seeing? How many solr cores?\n. This should be fixed. Please retest\n. Verified fix\n. Hi, What branch is this happening on?\n. What flags are you storing the field with?\nI have a testExactQuery() case in LucandraTests does this fail for you?\n. This normally happens when you quickly search after adding docs.  To make this work make sure you call commit before you search. Can you verify?\n. I made some of these changes locally, could you rebase and merge?\n. Fixed by setting cacheInvalidationInterval = 0\n. Good point, the problem here is the terms still exist in the term cache but the docs are no longer available.  I suppose I should delete the docs later on to avoid this.\n. fixed by https://github.com/tjake/Solandra/commit/3b661d2a6e4e947c1dc287fcb1483ee525ad04a5\n. Yeah if you look at SolandraIndexWriter.commit() it explicitly flushes the queued mutations.\nAre you running with RF>1?\n. Commit now works.\n. great, thanks\n. This still compiles solandra with the maven repository jar, it should probably use the one you specify when using cassandra-deploy?\n. I can fix this issue...\n. Thanks todd! can you verify its working for you?\n. did this locally\n. Yeah, I noticed this too. I'll fix it.\n. The best I can do here is ignore this completely and just re-search every-time.  I was hoping to think of a way to avoid doing this since you don't want to do this when there is no writes happening...\n. Hi Todd, I added a fix for this, basically cachetime=0 bypasses the async queue \n. Merged, thx, I'll push later\n. Thanks for the change!  I committed this with some tweaks to reduce the number of objects created.\n. Thanks, I think it would be better to make this pick a sensible time based on the size of the reserve.\n. This looks like your client side error? \n. ok thanks\n. I can't reproduce with your test\n. This was reproducible by re-ordering the tests, turns out it was a edge case when searching with wildcards. now fixed in c04e2ac3edd5720cb0fe8be6c52d1093aac5dfa9 \n. Thanks, added. All tests pass after latest fix.\n. Ah I think I have 8983 hardcoded. I need to fix that. Thanks \n. The admin interface isn't supported yet in Solandra. What specifically do you need it for?\n. Looks like you don't have curl installed on that system. if you look at /2-import-data.sh it uses curl to post the reuters schema to solandra\n. Great, thank you\n. Good distinction between put and post.  I can change this.\nThe only issue is POST to /solandra/core-id/ doesn't do anything, Solr api uses /solandra/core-id/update, and I don't want to break their api.  This is also why I put the schema api under /solandra/schema to avoid confusion. though, perhaps /solandra/core-id/schema is better?\n. Do you have any details on the underlying facet query?  Some types of faceting don't work with distributed search\n. Can you provide me with a way to reproduce this?\n. You mean reads? If you want it to work you would need to increase the replication factor of cassandra for the L keyspace.\n. You are saying you can't even start solandra?\nTo change the replication factor use the supplied cassandra-cli tool:\ncassandra-tool/cassandra-cli --host localhost\n\nupdate keyspace L with replication_factor=2;\n. Why should queries continue when there is missing data?  If you have a replication factor of 1 and you take down a box then it should error IMO.\n\nI think once you turn it back on the cluster should start working again if thats not the case then that's a bug and I need to fix it....\n. Correct.  If you change the replication factor and repair the nodes using cassandr-tools/nodetool -h localhost repair L on each node then it will work.\n. I misspoke. Rf=2 is tricky because a quorum is 2. Quorum is used internally for document Id and shard tracking. \nRf=3 should work\n. That is very odd... So you are saying when you add documents and search they show up but after 30 seconds they disappear?\nAre you able to reproduce with the demo app that comes with Solandra?\n. thanks! I'll dig in\n. Are you connecting to solandra via cassandra-cli? That is strange, but I don't think it's related.\nI changed the index id generation code which may help with the problem. Could you give the latest (about 5 hours ago) a try?\n-Jake\n. Chris,\nWe've pinpointed the problem and are working on a fix,  should be done soon.\n-Jake\n. Yes, this is now verified fixed.\n. It looks like the core wasn't added somehow?\nDo you not have curl installed? If you look at 2-import-data.sh you can see it needs to apply the schema.xml\n. Are you able to access http://localhost:8983/solandra/schema/reuters ? if you get NOT_FOUND then this the schema wasn't posted. \nIf this is the case can you re-try running 2-import-data.sh?\n. The api changed for comparators. This class will need to be updated in Solandra\n. I have't tried that search type is that new in 3.1?  In theory it should work.\nLet me add a test case and see what's happening\n. Hi Jimmy,\nWorking on a different issue atm. I will get to this soon after.\nThanks!\n. Hi Jimmy,\nThis requires a custom TermsComponent, I'll work on it this week.\n. Just committed. Please take a look. \n. This is fixed by bfbeed5\n. Tried a different approach in d288e02\n. Fixed by moving to a using a collector api. tested with 1M docs.\n. I believe this is now fixed, if you could re-test\n. There are 2 things you can do:\n1. Call commit when you are done writing documents and you want docs to be reflected in the readers.\n2. In solandra.properties you can set the solandra.cache.invalidation.check.interval=0 to get \"real-time\" results.\n. This is fixed by https://github.com/tjake/Solandra/commit/89851030eacc27474b5f99e5cc8323c17894897f\n. Any info on how to reproduce this with the reuters demo?\n. yes please if you could that would be great\n. turns out this is caused by not specifying a query string\n. Hmm... I'll test that.\n. Can you post the schema info about the timestamp field so I can try to reproduce locally?\nA workaround would be to comment out this line:\nhttps://github.com/tjake/Solandra/blob/solandra/src/lucandra/CassandraUtils.java#L94\n. thanks, I will use this testcase to figure out whats going on\n. @cwesdorp I wasn't able to reproduce with your scripts... I was able to get the sorted results...\ncurl 'http://localhost:8983/solandra/thoughtbucket/select?q=:&wt=json&indent=on&sort=timestamp%20desc'\n. interesting some users have TB of data in here and can't reproduce...  I can try reproducing with the benchmark test how many documents do you have in your index?\n. Suddenly I can reproduce!  I'll try fixing asap\n. Ok the above fixes it. the issue was caused by the way Solandra identifies which index shards are in use... \n. what URL are you pointing the DIH at?\n. try that, the offending code isn't in use anymore anyway :)\n. That may be the fix.  DIH spawns threads to import \n. should be fixed\n. What are you doing to get that error?  Are you not using solr?  You can't use IndexWriter directly... you must use the solr delete api\n. Thx!\n. Hmm, I just did a fresh checkout and wiped out all my ivy/maven caches and was able to find all the dependencies... can you try again? perhaps it was a intermittent error? or are you behind a firewall?\n. not able to reproduce\n. Hi, \nAre you using the latest code?  I ran your test and it matched 1M\n\n. oh tjake-Solandra-00b188b is a bit old...  Can you try with the latest?\n. Are there any errors the log? \n. It could be 20 shards is too much for windows?  I updated the defaults in solandra.properties to be ~1M per shard.  could you try the latest?\n. I believe this issue is fixed now can you re-test?\n. Hi, Yes Solandra uses distributed searching internally to fulfill requests.  \nThe solandra.shards.at.once property in solandra.properties allows paralell indexing across shards, so the default of 4 should work for you.   Does your indexing code send add requests to both hosts using many threads?\n. For the unbalanced cluster you need to pre-select your initial_token in cassandra or move the tokens around to make it more balanced:\nhttp://www.datastax.com/docs/0.8/operations/clustering#calculating-tokens\nThis will put the data on both nodes\n. Is the on disk size roughly even between the 2 nodes now?\nIt does index using both nodes however the problem is IO. Have you tried this experiment with the bulk loading URL? Add a ~ before the index name. This removes the read before write check needed for updates. \nYou won't get a 50% improvement going from 1 to 2 nodes but adding more nodes will add to overall indexing throughput\n. The bulk url would be http://localhost:8983/solandra/~solr_data/update\nIf you look at this url in the browser you can get info about which nodes the shard live on:\nhttp://localhost:8983/solandra/schema/solr_data\nIt looks like 3/4 of them are on one node vs 1/2... The shard placement is random so this is quite possible. The more nodes you have the less of a issue this becomes.\n. Hi David. \nThere was a change recently that allowed you to post ant solr resource to solandra not just schema.xml so this may hbe broken it. in general the solandra Datamodel has not yet stabilized. It's close but I'm pretty sure you will need to re-index. \nI haven't changed anything related to schemas but I have changed the term data format recently.\n. Sorry for the garbled message. Sending from my phone. \n. I think it's close, next month or so. just clearing out the last of the facet bugs and I think we can release.\n. Sorry, I'm confused...  Why are you not using SolandraServer?\n. solandra has a client mode \"-c\" flag that lets you run it against a existing cassandra cluster.  internally this calls the CassandraUtils startupClient() method.  Is this what you need?\n. Hi the admin interface is not supported (yet) in solandra. What specifically do you want to see from it?\n. Hi,\nThe demo isn't hosted through solandra.  You need to open the index.html file through your browser's File->Open File menu option.\n. Thanks for the info, this was fixed by https://github.com/tjake/Solandra/commit/d1e0ff1df1161c5f067beff980043e615170e6e8\n. There is currently no such tool\n. This mean's the classpath isn't correct, not seeing the lib dir, I don't have windows can you see if you can patch solandra.bat to work? :)\n. By default solandra put the data-dir in /tmp see  resources/cassandra/cassandra.yaml\nRebooting often wipes /tmp in linux.  So move this if you want to keep it.\n. Thanks Chris!\n. thx!\n. thx!\n. I can try to add a ant target for eclipse.  I just did it manually\n. Anything in the logs?\n. This is very odd. Do you get the same problem with a small number of docs?\nCan you reproduce with a clustered version of the reuters demo?\n. Could you supply your schema as well?\n. I don't think this is possible you must use SolrJ\n. The query caches can not be used for this reason. \nThe sample solrconfig.XML has these commented out. \n. Looks good, thanks!\n. What platform?\nOn Oct 10, 2011, at 6:08 PM, blinderreply@reply.github.com wrote:\n\nSomething very odd, when trying to write to solandra. We get this exception:\nCould not initialize class org.xerial.snappy.Snappy  java.lang.NoClassDefFoundError: Could not initialize class\n org.xerial.snappy.Snappy at lucandra.CassandraUtils.compress(CassandraUtils.java:771)    at \n lucandra.IndexWriter.toBytesUsingThrift(IndexWriter.java:620)    at \n lucandra.IndexWriter.addDocument(IndexWriter.java:328)    at \n solandra.SolandraIndexWriter.addDoc(SolandraIndexWriter.java:265)    at \n org.apache.solr.update.processor.RunUpdateProcessor.processAdd(RunUpdateProcessorFactory.java:61)    at \n org.apache.solr.handler.XMLLoader.processUpdate(XMLLoader.java:147)     at \n org.apache.solr.handler.XMLLoader.load(XMLLoader.java:77)    at \n org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:67)    at \n org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:129)    at \n org.apache.solr.core.SolrCore.execute(SolrCore.java:1368)    at \n org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:356)    at \n solandra.SolandraDispatchFilter.execute(SolandraDispatchFilter.java:171)    at \n org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:252)    at \n solandra.SolandraDispatchFilter.doFilter(SolandraDispatchFilter.java:137)    at \n org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)    at \n org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:388)    at \n org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)     at \n org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)     at \n org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)    at \n org.mortbay.jetty.Server.handle(Server.java:326)    at \n org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:536)     at \n org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:930)    at \n org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:834)    at \n org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)    at \n org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:405)    at org.mortbay.jetty\nit doesn't appear to do this on reads (querying) \nAlso the snappy-java-1.0.3-rc2.jar is in cassandra's lib directory.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/tjake/Solandra/issues/137\n. thx!\n. This happens when you don't do any writes the caches will keep getting larger to accommodate more terms.  The workaround is to send some writes or updates to the index.  This will flush the caches.  In the meantime, the caches should be changed to use an LRU\n. No, There are tests verifying sorting does work:\n\nhttps://github.com/tjake/Solandra/blob/solandra/test/solandra/SolandraTests.java#L832\nDo these pass for you?  ant test\n. ah pardon me, I fixed the tests it seems the solr resource url parsing was busted. try again?\n. yeah, sounds like asc sorting is busted, i havent had any cycles to fix, could you try commenting out? and retesting?\nhttps://github.com/tjake/Solandra/blob/solandra/src/lucandra/CassandraUtils.java#L121\n. Yeah 1.0 won't work with Solandra, it will with 1.1 so once thats out I'll upgrade\n. you can workaround by removing the custom field cache. see https://github.com/tjake/Solandra/issues/145\n. reverts to standard, a bit slower.\n. what revision of solandra is this? last commit?\n. Don't update to the latest since it includes a breaking change.\nDid this start happening once you updated?\nMy guess is it's related to this change https://github.com/tjake/Solandra/commit/386746a55756ee5a5aebcba69a441350133590b3 \nperhaps you can revert that and see if that helps\n. The actual fix for this is https://github.com/tjake/Solandra/commit/7b18f06f286820d4f3181c1ddafda6dfb2cc9672 but it requires re-indexing.  if you want to just fix the problem without updating then i think reverting  https://github.com/tjake/Solandra/commit/386746a55756ee5a5aebcba69a441350133590b3 will do that\n. thanks!\n. I think it is related to your version of ant. please try 1.8.2\n. That one liner was to avoid pulling too much data at once.  Seems like if you delete then perhaps the logic pulls only tombstoned columns and gives up.\n. I think 2/3 will work.  \nI've been M.I.A. due to my time being spent on DataStax Enterprise Search which provides native Solr access to Cassandra column families.  Also Cassandra 1.0 broke Solandra's partitioner. 1.1 will fix it so I will upgrade it then.\n. Thanks!\n. I've just published a blog post about this: http://www.datastax.com/dev/blog/cassandra-with-solr-integration-details\nI plan to keep Solandra working with Cassandra but I don't have resources to work on it heavily. \n. @jkusar does DSE fit your use case? \n. Cool.  Why do you think there are many nodes?  You can have all nodes be solr\n. No as the diagram shows in the blog post solr and cassandra are running in the same jvm. With DSE you can run Cassandra, Cassandra+Hadoop, or Cassandra+Solr, its upto you on how you mix and match.  Yes you can execute solr queries from CQL with all solr nodes.\n. Hi Ben thanks for doing this.  With 1.1 we no longer need our version of DecoratedKey either, let me do some testing and I'll get back to you\n. Correct, Solandra came before composite types.\n. ",
    "kumm": "Hi!\nI have a patch for Lucene 3.0.1. The unit test is working, and bookmarksdemo too.\nWhere (how) can i send it? I'm new at github...\n. ",
    "ceocoder": "that fixed it. thx\n. you are on the money with complexity, this is what I tried,\nDeleting columns from TermVector CF but only for the field that needs to be updated, \npublic String deleteDocuments(Term term) throws CorruptIndexException, IOException {\n        String docId = \"\";\n        try {\n```\n        ColumnParent cp = new ColumnParent(CassandraUtils.termVecColumnFamily);\n        String key = indexName + CassandraUtils.delimeter + CassandraUtils.createColumnName(term);\n        logger.debug(\"deleteDocuments term \" + key);\n        List docs = client.get_slice(CassandraUtils.keySpace, CassandraUtils.hashKey(key), cp, new SlicePredicate()\n            .setSlice_range(new SliceRange(new byte[] {}, new byte[] {}, true, Integer.MAX_VALUE)), ConsistencyLevel.ONE);\n    // delete by documentId\n    for (ColumnOrSuperColumn docInfo : docs) {\n        //get docId from doc being deleted  \n        docId = new String(docInfo.column.name);\n        logger.debug(\"in docInfo loop \" + docId);\n        deleteLucandraDocument(docInfo.column.name, term);\n    }\n\n} catch (InvalidRequestException e) {\n    throw new RuntimeException(e);\n} catch (UnavailableException e) {\n    throw new RuntimeException(e);\n} catch (TException e) {\n    throw new RuntimeException(e);\n} catch (TimedOutException e) {\n    throw new RuntimeException(e);\n} catch (NotFoundException e) {\n    throw new RuntimeException(e);\n} catch (ClassNotFoundException e) {\n    throw new RuntimeException(e);\n}\n\nreturn docId;\n\n```\n}\nprivate void deleteLucandraDocument(byte[] docId, Term term) throws InvalidRequestException, NotFoundException, UnavailableException, TimedOutException, TException,\n            IOException, ClassNotFoundException {\n```\n    String key = indexName + CassandraUtils.delimeter + new String(docId);\n    String fieldToUpdate = indexName + CassandraUtils.delimeter + term.field() + CassandraUtils.delimeter;\nColumnOrSuperColumn column = client.get(CassandraUtils.keySpace, CassandraUtils.hashKey(key), CassandraUtils.metaColumnPath, ConsistencyLevel.ONE);\n\nList<String> terms = (List<String>) CassandraUtils.fromBytes(column.column.value);\n\nfor (String termStr : terms) {\n\n    key = indexName + CassandraUtils.delimeter + termStr;\n\n    if (key.contains(fieldToUpdate)) {\n        logger.debug(\"found a termVec match to remove \" + key);\n        CassandraUtils.addToMutationMap(mutationMap, CassandraUtils.termVecColumnFamily, docId, CassandraUtils.hashKey(key), null);\n    } else {\n        logger.debug(\"ignorig term vec with key  \" + key);\n    }\n}\n\n\nlogger.debug(mutationMap);\n\nif (autoCommit)\n    CassandraUtils.robustBatchInsert(client, mutationMap);\n\n//got rid of the self delete part\n\n```\n}\nprivate void addDocumentWithDocId(Document doc, Analyzer analyzer, String docId) {\n    using supplied docId instead of randomly generated\n}\npublic void updateDocument(Term updateTerm, Document doc, Analyzer analyzer) {\n    String docId = deleteDocuments(updateTerm);\n    log.debug(docId)\n    addDocumentWithDocId(doc, analyzer, docId);\n}\nam I on the right track, comments?\n. haven't thought it thru but I  guess you call updatedocumer with empty document object, \n. little more background, \n4 machine cluster with replicatin_level =1,\njvm set to 6G (I've tried with jvm at 12 and 18 to same effect)\non boxes swappiness is set to 0\nfor disks I have 3PAR iSCSI mounts of 500GB each\n. # UPDATE\nTurning off row_cache completely and disabling swap I was able to ingest using one of the 4 nodes for about 15 hours and counting, and just started ingesting on a second node, and keep getting this error frequently -\nERROR\n10:09:26,663  INFO SolrCore:1324 - [htmlnew] webapp=/solandra path=/update params={commit=true} status=500 QTime=1112 \n10:09:26,664 ERROR SolrDispatchFilter:139 - java.lang.IllegalStateException: Unable to reserve an id\n        at lucandra.cluster.CassandraIndexManager.getNextId(CassandraIndexManager.java:341)\n        at solandra.SolandraIndexWriter.addDoc(SolandraIndexWriter.java:194)\n        at org.apache.solr.update.processor.RunUpdateProcessor.processAdd(RunUpdateProcessorFactory.java:61)\n        at org.apache.solr.handler.XMLLoader.processUpdate(XMLLoader.java:139)\n        at org.apache.solr.handler.XMLLoader.load(XMLLoader.java:69)\n        at org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:54)\n        at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131)\n        at org.apache.solr.core.SolrCore.execute(SolrCore.java:1316)\n        at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:338)\n        at solandra.SolandraDispatchFilter.execute(SolandraDispatchFilter.java:170)\n        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:241)\n        at solandra.SolandraDispatchFilter.doFilter(SolandraDispatchFilter.java:134)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:388)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\n        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)\n        at org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114)\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:536)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:930)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:747)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:405)\n        at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n        at org.mortbay.thread.BoundedThreadPool$PoolThread.run(BoundedThreadPool.java:451)\n. -jake opened 53\n. @davidstrauss\nif you change your schema.xml L225 to ,\nhttps://gist.github.com/967633\nhttp://localhost:8983/solandra/reuters/select?q=text:\"difficult times\"\nphrase search should work, solandra needs termPositions=\"true\" in order to perform phrase search\n. @ecrxs it should work now\n. @derryx did you recently upgrade solandra version? this commit https://github.com/tjake/Solandra/commit/1afe5e3a26b341386b7d67559a248993ae3563b1 added field cache CF, if you did an in place upgrade of solandra.war with old data files, you can get this error\n. did not have right yaml\n. closing this one\n. ",
    "eurospy": "I think you're right, I've faced the same issue. However indexing on multivalued fields are working fine\n. thanks for the update, I'm now getting the performance in filterin enumerations\n. if it's somehow possible to make the default sort order to be the date by some trick (like tampering with the docId or so) I think the main problem might be solved.\n. So, currently what is the default sort order? can you scheme out it?\n. it seems like it's the based on the first field in such a query like \"date:[2010.01 TO 2010.02] AND id:[100 TO 200]\"\n. the same code works on the local by the way\n. It's fixed with your latest commit, we can close this I think\n. ",
    "sdonelow": "I am finding the same issue.  Indexing and search on multivalued fields works but storing the values does not work.  As a temporary work around I am going to add a field which has all the values concatenated together separated by a delimiter.\n. I think I found the solution to this.  Take this with a grain of salt, I'm new to Lucandra/Solr.  We had the same problem with long and used slong data type instead and that fixed the problem.  So, try changing the popularity field data type to sint in your schema.xml file.\n. tjake, what does this mean \"ditch String keys for byte[]\"?\n. After researching this it seems that the problem is being caused in the class IndexReader, in the method document, at this line:\nif (fieldNames == null || fieldNames.size() == 0) {\n            // get all columns ( except this skips meta info )\n            slicePredicate.setSlice_range(new SliceRange(new byte[] {}, CassandraUtils.finalToken.getBytes(\"UTF-8\"), false, 100));\n        } else {\nslicePredicate.setColumn_names(fieldNames);\n    }\nI'm not using a field selector so fieldNames is null when the above code is reached.\n. Yep, that was the problem.  I changed 100 to 1000 and recompiled and no more truncation of the fields.\n. Set it to 1 billion so we don't have to futz with it.  LOL\n. ",
    "leoz-xx": "I'm having similar issues (with long and double instead of int though). Range queries not working... slong in solr might be the walk arround as sdonelow commented, but I prefer not using solr in my project...\n. ",
    "tnine": "Hey guys.  I'm assuming you're still having this issue?  I'm trying to sort it out, and it appears to be functionally impossible with the current implementation.  Basically, the number bits of the data type is right shifted 4 bits at a time.  The first byte then holds the number of bits shifted off.  You can view the logic here for creating the trie structures.\nhttp://lucene.apache.org/java/3_0_2/api/core/org/apache/lucene/search/NumericRangeQuery.html\nThis allows for faster range scanning and in makes seeks faster.  However according to the IndexReader spec here,\nhttp://lucene.apache.org/java/3_0_2/api/core/org/apache/lucene/index/IndexReader.html#terms(org.apache.lucene.index.Term)\n\"If the given term does not exist, the enumeration is positioned at the first term greater than the supplied term.\"\nThe current implementation does not do this, it merely returns no results since it scans over 2 keys, then returns only 2 key spaces and returns an empty result set.  I'm looking into trying to rectify this problem.   Correcting it may involve reading far more than 2 keys initially, so it will not be a very efficient operation. \n. I've created a simple unit test that mimics how keys are written with my own IndexReader and TermEnum here.\nhttp://github.com/tnine/Lucandra/blob/master/test/lucandra/BytesOrderingEnumTest.java\nIt doesn't complete because I haven't implemented all the document scoring.  However, it does correctly identify records to enumerate over when no prefix is present.  If you uncomment my commented lines, you will see the byte comparator used no longer seeks to the correct index when index\\docfield prefixes are used.  Therefore, something isn't quite right with the prefix and the byte ordering.  I just can't put my finger on it, since all the prefix bytes should be the same in common fields, and hence irrelevant in the byte comparison up to the first byte in the trie structure.\n. After much digging this appears to be an encoding issue with thrift and batch mutate itself.  The issue and corresponding unit test is here.\nhttps://issues.apache.org/jira/browse/CASSANDRA-1235\n. see the underlying bug.  We can't properly encode any numeric fields, as a result, you can't perform sorting on them.  Until Cassandra fixes this issue, no numeric field searching/sorting will work.\n. I could be wrong in how solr stores and retrieves indexes.  However I know I'm accurate in stating that we currently can't store numeric values in Cassandra correctly/consistently.  Run my test cases and you'll see exactly what I mean.  You will occasionally get correct behavior as the encoding problem does not present itself with all values.  It seems to depend on the byte value that is stored.  This fix was bumped from 0.6.4 to 0.6.5, so it doesn't seem to be getting fixed anytime soon.  Check out the Solr code, and see if it's using numeric values in the underlying fields.  If it is, you can't use it until the Cassandra bug is fixed.\n. Currently all keys in Cassandra are UTF8 strings.  This has been removed in favor of using native bytes in the new version.  This should eliminate the issues we see with shifting 7 bits of numeric types into the lower 7 bits of a UTF8 byte.  Hence removing the limitation of numeric fields in Lucandra.  Note that this will require a decent amount of rework of Lucandra, but I plan on doing that as soon at 0.7 is release since we really need  numeric functionality.\n. Just an fyi guys.  This has been fixed in release 0.6.5 of Cassandra, so numeric fields should now work.\n. I'm connecting to two different clusters from the same VM.  One cluster used the Keyspace \"Spidertracks\" to hold all the secondary indexing I'm building for my Cassandra plugin.  I'm connecting to another cluster with the keyspace \"GeoData\".  Since I'm connecting to 2 separate clusters, I have 2 different keyspaces I need to connect to.\n. Hey Jake,\n  Unfortunately this fix won't work for me.  You have configured it as a system property, which doesn't resolve my use case above.  I'm in 1 JVM, and connecting to 2 separate index keyspaces.  Check out my fork, I pass in a context for each reader and writer that allows it to connect to the key space I need.  I've been steadily merging in your changes, so you should be a able to pull my fork into a branch with your current code base and the context added as well.\n. Hey Jake,\n  I've investigated this further, and I have determined the issue.  The LuceneTermEnum does not properly match the spec when enumerating numeric trie terms.  I've added some debug output when using the default RamDirectory on version 2.9.3 and running the TestNumericRangeQuery32 tests.  I receive this enumeration order when the \"term()\" method is invoked on their SegmentTermEnum class.\n\nReturning term for field 'field8' hex value is : 60077f7e6814\nReturning term for field 'field8' hex value is : 60077f7e6814\nReturning term for field 'field8' hex value is : 60077f7e6814\nReturning term for field 'field8' hex value is : 60077f7e6814\nReturning term for field 'field8' hex value is : 68037f7f00\nReturning term for field 'field8' hex value is : 68037f7f00\nReturning term for field 'field8' hex value is : 68037f7f00\nReturning term for field 'field8' hex value is : 68037f7f00\nReturning term for field 'field8' hex value is : 68037f7f34\nReturning term for field 'field8' hex value is : 68037f7f34\nReturning term for field 'field8' hex value is : 68037f7f34\nReturning term for field 'field8' hex value is : 68037f7f34\nReturning term for field 'field8' hex value is : 68037f7f34\nReturning term for field 'field8' hex value is : 68037f7f34\nReturning term for field 'field8' hex value is : 68037f7f4e\nReturning term for field 'field8' hex value is : 68037f7f34\nReturning term for field 'field8' hex value is : 68037f7f4e\nReturning term for field 'field8' hex value is : 68037f7f4e\nReturning term for field 'field8' hex value is : 68037f7f68\nReturning term for field 'field8' hex value is : 68037f7f4e\nReturning term for field 'field8' hex value is : 68037f7f68\nReturning term for field 'field8' hex value is : 68037f7f68\nReturning term for field 'field8' hex value is : 6804000002\nReturning term for field 'field8' hex value is : 68037f7f68\nReturning term for field 'field8' hex value is : 70017f7f\nReturning term for field 'field8' hex value is : 70017f7f\nReturning term for field 'field8' hex value is : 70017f7f\nReturning term for field 'field8' hex value is : 70017f7f\nReturning term for field 'field8' hex value is : 78007f\nReturning term for field 'field8' hex value is : 78007f\nReturning term for field 'field8' hex value is : 78007f\nReturning term for field 'field8' hex value is : 78007f\nReturning term for field 'field8' hex value is : 780100\nReturning term for field 'field8' hex value is : 780100\nReturning term for field 'field8' hex value is : 780100\nReturning term for field 'field8' hex value is : 780100\nReturning term for field 'field8' hex value is : 780100\nReturning term for field 'field8' hex value is : 780100\n\nThese are the results with LucandraTermEnum\n\nReturning term for field 'field8' hex value is : 60077f7e6814\nReturning term for field 'field8' hex value is : 600809433244\nReturning term for field 'field8' hex value is : 68037f7f34\nReturning term for field 'field8' hex value is : 68037f7f34\nReturning term for field 'field8' hex value is : 68037f7f4e\nReturning term for field 'field8' hex value is : 68037f7f4e\nReturning term for field 'field8' hex value is : 68037f7f68\nReturning term for field 'field8' hex value is : 68037f7f68\nReturning term for field 'field8' hex value is : 6804000002\nReturning term for field 'field8' hex value is : 6804046008\nReturning term for field 'field8' hex value is : 6804046008\n\nAs you can see the results are not properly enumerated.  Given that you're using a Tree for the cached terms, they should be ordered properly after insert.  It seems that this may be an issue with the way loadTerms is invoked \n. Hi Jake, \n  I've been digging into this one all day.  After searching a bit more, I found an issue in my local copy of the TermEnum which I have corrected.  This resolves the enumeration issue I described above.  However, the documents are not returned in \"default\" order.  I.E. the order they were added to the index as the test expects.  Im assuming this is a bug in the LucandraTermDocs, but I'm having a hard time locating it.  Thoughts?\n. I've updated my test case on my fork that shows the issue.\nhttp://github.com/tnine/Lucandra/blob/master/test/lucandra/NumericRangeTests.java\nIt appears to still be term enum related.  The calls to IndexReader.addDocument are occurring in a different order than the insertion.\n. I'm also really interested in this.  We need the ability to configure the CL in our application.  We're starting with Quorum but we'll be migrating to a 2 DC setup.  We use Solandra for real time indexing of alert data, so making sure our data is always available on commit is a must for us.\n. Hey Jake.  Shouldn't this work with caches and quick writes and deletes and caching enabled?  We still need to perform modifications rapidly even if the results aren't immediately available for viewing.\n. Scratch that.  Just tried to reproduce this error and couldn't.  Strangely I was seeing this issue when I was also seeing the NPE error.\n. No yet, but I will be.  I changed the code to take a replication factor from the System properties, though a configuration file would be ideal.  \nhttps://github.com/tnine/Lucandra/commit/c7d92719843abf3075311dac4faaf37c4fd7acaf\nThis works, but it's a bit of a fast hack to get a working dev environment working while a proper configuration is created.\n. Good point\n. Hey Jake,\n  This is a client side error, but I always receive it when connecting to a single node in a test context.  It appears to be an issue with starting and stopping the Solandra runtime within the same JVM over multiple integration tests.  I haven't changed anything, but your latest code in combination with 0.7.3 seems to have resolved the issue.  If it occurs again I'll let you know.\n. ",
    "tmahesh": "Is sorting on integer/float fields supported in solr-cassandra?\nI tried the below query on the index of example docs. But did not get results in correct order\nhttp://localhost:8983/solr/select/?q=cat:electronics&sort=price%20asc\nI have tried changing field type to \"sint\" \"tint\" but no success. Sorting on string field type works though. Any suggestion on how to fix sorting issue for integer and float? \n. 1. We can store integer/float data and fetch it out correctly (i.e., price filed fetched from the index is as it was stored) \n2. From what i understand, sorting of result set happens inside solr indexsearcher\nShouldn't sorting work in such a case? \nI'm confused on how the cassandra bug impacts sorting while we can fetch the stored data correctly from the index.\n. ",
    "difranco": "For my setup with a core called 120, I am getting a 500 error again here (using latest code from the repo as of this writing). Also, meanwhile, I appear to be able to index data fine and disk fills up quite nicely while I do that but querying returns no results for queries that most certainly should have results.\n```\nHTTP ERROR 500\nProblem accessing /solr/120/admin/luke. Reason:\nnull\njava.lang.NullPointerException\nat lucandra.LucandraTermEnum.next(LucandraTermEnum.java:88)\nat org.apache.solr.handler.admin.LukeRequestHandler.getIndexInfo(LukeRequestHandler.java:462)\nat org.apache.solr.handler.admin.LukeRequestHandler.handleRequestBody(LukeRequestHandler.java:100)\nat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131)\nat org.apache.solr.core.SolrCore.execute(SolrCore.java:1316)\nat org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:338)\nat solandra.SolandraDispatchFilter.execute(SolandraDispatchFilter.java:158)\nat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:241)\nat solandra.SolandraDispatchFilter.doFilter(SolandraDispatchFilter.java:124)\nat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\nat org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:388)\nat org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\nat org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\nat org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)\nat org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\nat org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)\nat org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114)\nat org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\nat org.mortbay.jetty.Server.handle(Server.java:326)\nat org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:536)\nat org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:915)\nat org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:539)\nat org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\nat org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:405)\nat org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\nat org.mortbay.thread.BoundedThreadPool$PoolThread.run(BoundedThreadPool.java:451)\n```\n. ",
    "kwiggen": "OK this seems to be my misunderstanding.  With a file system backed lucene I think you need to close a writer, before a reader can see the changes.\nIf I add \nindexWriter.commit();\nto the code it does nothing, however\nindexReader.clearCache();\ndoes the trick.\nWhat is the lifespan of a Reader and Writer in the Cassandra world?  Should I be getting a new Reader/Writer for each user interaction?\nSorry for the noob questions, I am off to read more...\n. But the 2nd read in my example gives you a TopDocs size of 1, but when you get the Document it has no Field objects.  I would consider this a bug as you get reference to the Document, but it is empty.  Either the Document should be filled in, or not returned.\n. ",
    "ghost": "OK. It was with solr but not with the example. I will check my solrconfig.xml (pretty much copied from a vanilla solr setup) against the one in the example and see what I can figure out. \n. I am definitely able to repeat this using the solr example and the standard request handler. \nUsing a clean copy of the source tree, after setting up the solr example as per the README file\n1. Setup Cassandra 0.6 with storage-conf.xml in config\n2. ant lucandra.jar\n3. cd solr-example; java -jar start.jar\n4. cd exampledocs; ./post.sh *.xml\nRun this query\nhttp://localhost:8983/solr/select/?q=ipod&version=2.2&start=0&rows=10&indent=on&sort=price%20desc&fl=id,name,price&wt=json\nThe first time I run it, the sort order will be correct. Now run it again and the sort order changes.\nKill and restart solr. \nRun this query (using the dismax request handler)\nhttp://localhost:8983/solr/select/?q=ipod&version=2.2&start=0&rows=10&indent=on&sort=price%20desc&fl=id,name,price&wt=json&qt=dismax\nNow the sort order is correct and remains stable when I refresh / re-run the search.\n. ",
    "snez": "Two additional notes on this:\na) If you restart cassandra & solr then you do get the results back without an exception\nb) Using the same configuration, if you insert something from node B and try to read it from node A, solr returns numFound = 1 but the doc xml tag is empty\n. Amazing\n. ",
    "mircouleur": "It's difficult to filter in our app beacause we can't know witch Term will be inpacted by this issue. What is the idea of your custom queryfilter class? Maybe we could help you?\nthanx\n. Hi Jake,\nI've made a queryfilter for this issue. How could I submit it?\n. hello Jake,\nIt's in your mailbox. Did you receive it?\n. Yes I did it sunday... the mail is from herve.dominguez at gmail dot com\n. Hi Jake,\nDid you receive it, I made a reply on your message...\n. ",
    "vegashacker": "The issue remains, but I've forked and checked in a unit test which reproduces the issue. See\nhttp://github.com/vegashacker/Lucandra/commit/24a38696f72f183fb3441a74ccc910110152db7d\nWhen you run the test, you get a NPE:\n    [junit] java.lang.NullPointerException\n    [junit]     at lucandra.LucandraTermDocs.nextPosition(LucandraTermDocs.java:184)\n    [junit]     at org.apache.lucene.search.PhrasePositions.nextPosition(PhrasePositions.java:76)\n    [junit]     at org.apache.lucene.search.PhrasePositions.firstPosition(PhrasePositions.java:65)\n    [junit]     at org.apache.lucene.search.ExactPhraseScorer.phraseFreq(ExactPhraseScorer.java:34)\n    [junit]     at org.apache.lucene.search.PhraseScorer.doNext(PhraseScorer.java:105)\n    [junit]     at org.apache.lucene.search.PhraseScorer.nextDoc(PhraseScorer.java:89)\n    [junit]     at org.apache.lucene.search.Scorer.score(Scorer.java:74)\n    [junit]     at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:248)\n    [junit]     at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:173)\n    [junit]     at org.apache.lucene.search.Searcher.search(Searcher.java:181)\n    [junit]     at org.apache.lucene.search.Searcher.search(Searcher.java:191)\n    [junit]     at lucandra.LucandraTests.testSimpleAnalyzerWriteRead(LucandraTests.java:361)\n. Yeah, the ColumnFamily tags now look like this, as per what's checked in to the lucandra github. This exception is from a fresh build of today's code and a wiped cassandra db, so I expect it's reproducible with the code I provided.\n<ColumnFamily CompareWith=\"BytesType\" Name=\"Documents\"   KeysCached=\"10%\" />\n<ColumnFamily ColumnType=\"Super\" CompareWith=\"BytesType\" CompareSubcolumnsWith=\"BytesType\"\n            Name=\"TermInfo\" KeysCached=\"10%\" />\n. Works on my end now. Thanks!\n. ",
    "cjhulen": "** Test code to reproduce bug *\nimport java.io.StringReader;\nimport java.util.ArrayList;\nimport lucandra.CassandraUtils;\nimport lucandra.IndexReader;\nimport lucandra.IndexWriter;\n//import java.io.File;\n//import org.apache.lucene.index.IndexReader;\n//import org.apache.lucene.index.IndexWriter;\n//import org.apache.lucene.store.Directory;\n//import org.apache.lucene.store.FSDirectory;\nimport org.apache.lucene.analysis.Analyzer;\nimport org.apache.lucene.analysis.cjk.CJKAnalyzer;\nimport org.apache.lucene.document.Document;\nimport org.apache.lucene.document.Field;\nimport org.apache.lucene.search.IndexSearcher;\nimport org.apache.lucene.search.Query;\nimport org.apache.lucene.search.ScoreDoc;\nimport org.apache.lucene.search.similar.MoreLikeThis;\nimport org.apache.lucene.util.Version;\npublic class ReproduceBug {\n```\nstatic ArrayList testRun1 = new ArrayList();\nstatic ArrayList testRun2 = new ArrayList();\nstatic ArrayList testRun3 = new ArrayList();\nstatic {\n    TestObj testObj1 = new TestObj();\n    testObj1.id = \"af52bc87-d721-46f8-8e96-fc1afa11b259\";\n    testObj1.title = \"\"; // This is a problem\n    testObj1.body = \"Term gsm carrier Term Blackberry Term htc Iphones 3 Term Iphones \";        \nTestObj testObj1a = new TestObj();\ntestObj1a.id = \"af52bc87-d721-46f8-8e96-fc1afa11b259\";\ntestObj1a.title = \"no-op\"; // This is a OK\ntestObj1a.body = \"Term gsm carrier Term Blackberry Term htc Iphones 3 Term Iphones \";\n\nTestObj testObj1b = new TestObj();\ntestObj1b.id = \"af52bc87-d721-46f8-8e96-fc1afa11b259\";\ntestObj1b.title = \"the\"; // this is problem.  Because of stop words ???\ntestObj1b.body = \"Term gsm carrier Term Blackberry Term htc Iphones 3 Term Iphones \";\n\nTestObj testObj2 = new TestObj();\ntestObj2.id = \"6abd1a3a-4d08-4189-a692-6095bd09c65e\";\ntestObj2.title = \"Term touch screen cell phone \";\ntestObj2.body = \"Term mobile at&t touch screen iphone clone Term sim Term touch screen cell phone \";\n\ntestRun1.add(testObj1);     \ntestRun1.add(testObj2);\n\ntestRun2.add(testObj1a);\ntestRun2.add(testObj2);\n\ntestRun3.add(testObj1b);\ntestRun3.add(testObj2);\n\n}\npublic static class TestObj {\n    public String id;\n    public String title;\n    public String body;\n}\npublic static void main(String[] args) throws Exception {\nArrayList<TestObj> testRun = testRun1;\n//ArrayList<TestObj> testRun = testRun2;\n//ArrayList<TestObj> testRun = testRun3;\n\nString indexName = \"ReproduceBugTest\" + System.currentTimeMillis();\n\nAnalyzer analyzer = new CJKAnalyzer(Version.LUCENE_30);\nIndexWriter writer = new IndexWriter(indexName, CassandraUtils.createRobustConnection(\"127.0.0.1\", 9160, false, true));\nIndexReader reader = new IndexReader(indexName, CassandraUtils.createRobustConnection(\"127.0.0.1\", 9160, false, true));\nIndexSearcher searcher = new IndexSearcher(reader);\n\n// Sanity Check to make sure it works with regular Lucene (part1)\n// Uncomment part1 + part2 below to check with regular lucene\n// File indexFile = new File(\"itemdata/\" + indexName);\n// Directory index = FSDirectory.open(indexFile);\n// IndexWriter writer = new IndexWriter(index, analyzer,\n// IndexWriter.MaxFieldLength.LIMITED);\n\nfor (TestObj tobj : testRun) {\n    writer.addDocument(exportAnnotationsToLucene(tobj), analyzer);\n}\n\nwriter.commit();\n\n// Sanity Check to make sure it works with regular Lucene (part 2)\n// IndexReader reader = IndexReader.open(index);\n// IndexSearcher searcher = new IndexSearcher(index);\n\nfor (TestObj tobj : testRun) {\n\n    ArrayList<String> matches = findSimilarItems(tobj, reader, searcher);\n\n    for (String match : matches) {\n        System.out.println(match);\n    }\n\n    // ***** Clearing the cache will allow it to run without failures\n    // *****\n    // reader.clearCache();\n}\n\n}\npublic static Document exportAnnotationsToLucene(TestObj tobj) {\nDocument doc = new Document();\ndoc.add(new Field(\"Title\", tobj.title, Field.Store.NO, Field.Index.ANALYZED));\ndoc.add(new Field(\"Body\", tobj.body, Field.Store.NO, Field.Index.ANALYZED));\ndoc.add(new Field(\"IdKey\", tobj.id, Field.Store.YES, Field.Index.NOT_ANALYZED));\nreturn doc;\n\n}\npublic static ArrayList findSimilarItems(TestObj tobj, IndexReader reader, IndexSearcher searcher) throws Exception {\nString contents = tobj.title;\ncontents += \" \" + tobj.body;\n\nMoreLikeThis mlt = new MoreLikeThis(reader);\nmlt.setFieldNames(new String[] { \"Title\", \"Body\" });\nStringReader sr = new StringReader(contents);\nmlt.setMinWordLen(2);\nmlt.setBoost(true);\nmlt.setMinTermFreq(1);\nmlt.setMinDocFreq(1);\nQuery q = mlt.like(sr);\nScoreDoc[] hits = searcher.search(q, 24).scoreDocs;\n\nArrayList<String> keys = new ArrayList<String>();\nfor (int i = 0; i < hits.length; i++) {\n    keys.add(searcher.doc(hits[i].doc).get(\"IdKey\"));\n}\n\nreturn keys;\n\n}\n```\n}\n. ah...thanks\n. ",
    "cc2010": "I did see duplicate values for NOT_ANALYZED field:\non line 102, the the tokens will be assigned the value of the untokenized field: \n    if (tokens == null) {\n      tokens = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));\n    }\n. ",
    "marianosimone": "Right now, I'm using a try/catch block, catching the NullPointerException and rethrowing an exception of a type that I created (with the NPE as cause) adding a description that says that the Cassandra instance MAY be down. Whenever the app fails, at least I know that the problem was generated while using Cassandra (because of the type, without having to take a look at the stacktrace) what the problem may be (from previous experience), and don't get just a NPE.\nThe problem is not that the exception is unchecked, but that a NPE doesn't represent or give a clue about what might have happened. What do you think?\n. I guess you could retry a couple of times... but eventually, IMHO, you have to give up and throw and exception.\n. If you don't want to add new types, an IOException should be OK (nicer if you add a string with the possible reason).\nI would prefer an unchecked exception (something like CouldNotConnectToDatabaseException), but that's personal taste ;)\n. ",
    "lixinchinaren": "my code for  input\n...\nDocument doc = new Document();\nString entryID = \"id1\";\nString floorNum = \"1\";\ndoc.add(new Field(ENTRYNAME , entryID , Field.Store.YES , Field.Index.NOT_ANALYZED));\ndoc.add(new Field(FLOORNUM , floorNum , Field.Store.NO , Field.Index.NOT_ANALYZED));\nindexWriter.addDocument(doc, analyzer);\n. my code for output\n...\nBooleanQuery booleanQuery = new BooleanQuery(false);\nTermQuery termQ = new TermQuery(new Term(floorNum, \"1\"));\nbooleanQuery.add(termQ, Occur.MUST);\nTopDocs docs = indexSearcher.search(termQ)\n...\n. ignore\n. ",
    "mayah": "Hi, tjake.\nI overrode IndexReader#getFieldCacheKey() so that each thread returns a different key, then it worked! I hope this issue is fixed in your master branch.\nThank you.\n. ",
    "amcjen": "Hey there-\nHas this fix already been added to the master branch?  If not, mayah, maybe you could fork it and add the patch.  That'd let others get the fix, and let tjake merge it back in when he's ready/has a moment.\nThx!\n. Super, thanks!  Just saw it on my pull.  :)\n. ",
    "Laurent-Chavet": "So here are the minimal repro steps:\nSchema:\n    \n\nAdd:\ncurl http://hqsslr01:8080/solr/update?commit=true -H \"Content-Type: text/xml\" --data-binary  '<add>  <doc><field name=\"id\">3</field> </doc></add>'\nand get that error:\nSep 10, 2010 11:40:31 AM org.apache.solr.common.SolrException log\nSEVERE: java.lang.NumberFormatException: For input string: \"^@^C\"\n        at java.lang.NumberFormatException.forInputString(Unknown Source)\n        at java.lang.Integer.parseInt(Unknown Source)\n        at java.lang.Integer.parseInt(Unknown Source)\n        at org.apache.solr.util.NumberUtils.int2sortableStr(NumberUtils.java:36)\n        at org.apache.solr.schema.SortableIntField.toInternal(SortableIntField.java:52)\n        at org.apache.solr.schema.FieldType$DefaultAnalyzer$1.incrementToken(FieldType.java:320)\n        at lucandra.IndexWriter.addDocument(IndexWriter.java:135)\n        at lucandra.IndexWriter.updateDocument(IndexWriter.java:356)\n        at solandra.SolandraIndexWriter.addDoc(SolandraIndexWriter.java:133)\n        at org.apache.solr.update.processor.RunUpdateProcessor.processAdd(RunUpdateProcessorFactory.java:61)\n        at org.apache.solr.handler.XMLLoader.processUpdate(XMLLoader.java:139)\n        at org.apache.solr.handler.XMLLoader.load(XMLLoader.java:69)\n        at org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:54)\n        at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131)\n        at org.apache.solr.core.SolrCore.execute(SolrCore.java:1316)\n        at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:338)\n        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:241)\n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)\n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)\n        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)\n        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)\n        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:128)\n        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102)\n        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)\n        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:293)\n        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:847)\n        at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:583)\n        at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:454)\n        at java.lang.Thread.run(Unknown Source)\n. Using\n    \ninstead of sint seem to fix the problem; I hope this post can help other with the same issue.\n. ",
    "magloven": "Great! Looking forward to a v2 of Lucandra.\nWe're migrating our portal application to use Cassandra as backend and we've been using Lucene ever since it was created. Haven't really looked that much at Solr since Lucene will do the job and we have implemented all functions and UIs we need.\nWill for sure check out Solr sometime in the future.\n. ",
    "levmatta": "Please Some help.\n. Yes, but It will take a while.\nThanks.\n. I am using UTF-8, but Latin extended does suffice.\n. I am sorry but this is not a Lucandra problem. Lucene is to blame here.\nThe problem is that when Lucene sees a Wildcard it goes crazy with fear and does NOT\nuser my Analyzer. So ASCIIFoldingFilter is not being use.\n. ",
    "heyvishy": "Hi Jake,\nThanks for getting back.\nI am using Master branch  (which was updated on Nov 2 for issue #37)\nI am using Cassandra 0.6.4 version.\nAlso i have a search function which takes query as input.\nIn the function i am creating a fresh IndexReader object as below.\nIndexReader indexReader = new IndexReader(\"bookmarks\", client);\nSo basically everytime the function is called, a new instance of indexReader object is created , which i think is same as indexReader re-open ?\nLet me know if some more information is required.\nThanks\nVishal Shukla\n. Hi Jake,\nRecently we migrated to Cassandra 0.7 version.\nSo I installed Lucandra 0.7 version, and found that the index corruption bug stills exists in this version too.\nI replicated the same scenario ( as i had mentioned in my first mail) , and it consistently failed.\nAny ideas or suggestions would be highly appreciated.\n. Yes the fix works.I tested out the scenario where it used to consistently fail before , but it's working perfectly fine now with the latest code of 0.7 version.\nThanks Jake.\n. this is happening in 0.7 branch.\n. I am using Store.YES and Index.ANALYZED flags.\nSo for e.g. any entry added into document is like,\ndoc.add(new Field(rule, value, Store.YES, Index.ANALYZED));\nwhere rule , value are variables.\ni tested testExactQuery()  , and it failed.\nHowever, the testCase testWriter() passed.\n. ",
    "compfix": "I was able to get past the original issue by changing the String shard = addr.getHostAddress() + \":8983/solr/\" + indexName + \"~\" + i; to String shard = addr.getHostAddress() + \":8983/solandra/\" + indexName + \"~\" + i;\nHowever we are now getting a null pointer exception:\n15:18:58,292 ERROR SolrCore:139 - java.lang.NullPointerException\n        at org.apache.solr.handler.component.QueryComponent.mergeIds(QueryComponent.java:462)\n        at org.apache.solr.handler.component.QueryComponent.handleResponses(QueryComponent.java:298)\n        at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:290)\n        at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131)\n        at org.apache.solr.core.SolrCore.execute(SolrCore.java:1316)\n        at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:338)\n        at solandra.SolandraDispatchFilter.execute(SolandraDispatchFilter.java:169)\n        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:241)\n        at solandra.SolandraDispatchFilter.doFilter(SolandraDispatchFilter.java:133)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1084)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:360)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:722)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:404)\n        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:206)\n        at org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114)\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:139)\n        at org.mortbay.jetty.Server.handle(Server.java:324)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:505)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:828)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:514)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:211)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:380)\n        at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n        at org.mortbay.thread.BoundedThreadPool$PoolThread.run(BoundedThreadPool.java:451)\nwhen we run a query.\n. I'm planning on pulling your latest tomorrow.  I think my issue right now is that some of the documents come back with null column families which cause the returned document to be null.  Perhaps an issue with cassandra?  I'll let you know if my results are better after I pull the latest.\nThanks,\nCliff\n. Jake,\nIt appears as those Solandra is based on the assumption that there will be multiple nodes in the cassandra cluster that will store the \"shards\".  Is this the case?  What if we only have one node?  Can we prevent Solandra from thisnking there are multiple shards once our doc count goes beyond the 2^17 limit on maxDocsPerShard ?\nThanks,\nCliff\n. I did see that setting and it has been set that way.  Unfortunately we still experience a problem.  Based on the log information, it seems to pull a result from the database, however it fails to merge the results to present to the user because it reports \"Missing document in multiget_slice for\" ... I'm not sure if this is because the doc isn't in the database or because it thinks it actually resides in a different shard the cassandra isn't aware of.\nI'm able to reproduce this fairly easily but haven't made much headway in determining root cause.  Any guidance on this would be appreciated.\nThanks,\nCliff\n. Here's the excerpt from the log when I query:  (There are two Solr cores one called config and one called log)  The one called log is the one with issues.\n16:17:57,306  INFO SolandraCoreContainer:72 - Loading Solandra core: log~0\n16:17:57,306  INFO SolandraCoreContainer:72 - Loading Solandra core: log~1\n16:17:57,338  INFO SolandraCoreContainer:72 - Loading Solandra core: config\n16:17:57,340  INFO UpdateRequestProcessor:171 - {add=[3e7d197b-2582-4847-a6fa-6bea1b13aecf:current]} 0 2\n16:17:57,360  WARN IndexReader:254 - Missing document in multiget_slice for: 8441519560042444830915831745756986754\u00ef\u00bf\u00bf5b88\n16:17:57,342  INFO SolandraCoreContainer:72 - Loading Solandra core: log\n16:17:57,362  WARN IndexReader:254 - Missing document in multiget_slice for: 8441519560042444830915831745756986754\u00ef\u00bf\u00bf5b89\n16:17:57,362  WARN IndexReader:254 - Missing document in multiget_slice for: 8441519560042444830915831745756986754\u00ef\u00bf\u00bf5b82\n16:17:57,362  INFO SolrCore:1324 - [config] webapp=/solandra path=/update params={} status=0 QTime=2 \n16:17:57,363  WARN IndexReader:254 - Missing document in multiget_slice for: 8441519560042444830915831745756986754\u00ef\u00bf\u00bf5b83\n16:17:57,364  INFO UpdateRequestProcessor:171 - {add=[bc28ab8a-c92d-40b5-b164-d6cd028b92f1]} 0 2\n16:17:57,364  WARN IndexReader:254 - Missing document in multiget_slice for: 8441519560042444830915831745756986754\u00ef\u00bf\u00bf5b80\n16:17:57,364  INFO SolrCore:1324 - [log] webapp=/solandra path=/update params={} status=0 QTime=2 \n16:17:57,365  WARN IndexReader:254 - Missing document in multiget_slice for: 8441519560042444830915831745756986754\u00ef\u00bf\u00bf5b81\n16:17:57,365  WARN IndexReader:254 - Missing document in multiget_slice for: 8441519560042444830915831745756986754\u00ef\u00bf\u00bf5b86\n16:17:57,365  WARN IndexReader:254 - Missing document in multiget_slice for: 8441519560042444830915831745756986754\u00ef\u00bf\u00bf5b87\n16:17:57,365  WARN IndexReader:254 - Missing document in multiget_slice for: 8441519560042444830915831745756986754\u00ef\u00bf\u00bf5b84\n16:17:57,365  WARN IndexReader:254 - Missing document in multiget_slice for: 8441519560042444830915831745756986754\u00ef\u00bf\u00bf5b85\n16:17:57,366  INFO SolrCore:1324 - [log] webapp=/solandra path=/select params={fl=id,score&start=0&q=uuid:&isShard=true&wt=javabin&fsv=true&rows=10&version=1} hits=547 status=0 QTime=60 \n16:17:57,389  WARN IndexReader:254 - Missing document in multiget_slice for: 89333738597305042842099228572829323411\u00ef\u00bf\u00bf189\n16:17:57,393  WARN IndexReader:254 - Missing document in multiget_slice for: 89333738597305042842099228572829323411\u00ef\u00bf\u00bf188\n16:17:57,393  WARN IndexReader:254 - Missing document in multiget_slice for: 89333738597305042842099228572829323411\u00ef\u00bf\u00bf183\n16:17:57,393  WARN IndexReader:254 - Missing document in multiget_slice for: 89333738597305042842099228572829323411\u00ef\u00bf\u00bf182\n16:17:57,393  WARN IndexReader:254 - Missing document in multiget_slice for: 89333738597305042842099228572829323411\u00ef\u00bf\u00bf181\n16:17:57,394  WARN IndexReader:254 - Missing document in multiget_slice for: 89333738597305042842099228572829323411\u00ef\u00bf\u00bf180\n16:17:57,394  WARN IndexReader:254 - Missing document in multiget_slice for: 89333738597305042842099228572829323411\u00ef\u00bf\u00bf187\n16:17:57,394  WARN IndexReader:254 - Missing document in multiget_slice for: 89333738597305042842099228572829323411\u00ef\u00bf\u00bf186\n16:17:57,394  WARN IndexReader:254 - Missing document in multiget_slice for: 89333738597305042842099228572829323411\u00ef\u00bf\u00bf185\n16:17:57,394  WARN IndexReader:254 - Missing document in multiget_slice for: 89333738597305042842099228572829323411\u00ef\u00bf\u00bf184\n16:17:57,394  INFO SolrCore:1324 - [log] webapp=/solandra path=/select params={fl=id,score&start=0&q=uuid:&isShard=true&wt=javabin&fsv=true&rows=10&version=1} hits=33536 status=0 QTime=88 \n16:17:57,396  INFO SolandraCoreContainer:72 - Loading Solandra core: config\n16:17:57,401  INFO SolandraCoreContainer:72 - Loading Solandra core: log\n16:17:57,402  INFO UpdateRequestProcessor:171 - {add=[29595742-3036-4dab-bd38-c7fecec28da0:current]} 0 5\n16:17:57,402  INFO SolrCore:1324 - [config] webapp=/solandra path=/update params={} status=0 QTime=5 \n16:17:57,408  INFO UpdateRequestProcessor:171 - {add=[a0c7f456-5a98-4ed7-bbac-3ef8cbc4f146]} 0 6\n16:17:57,408  INFO SolrCore:1324 - [log] webapp=/solandra path=/update params={} status=0 QTime=6 \n16:17:57,408 ERROR SolrCore:139 - java.lang.NullPointerException\n    at org.apache.solr.handler.component.QueryComponent.mergeIds(QueryComponent.java:462)\n    at org.apache.solr.handler.component.QueryComponent.handleResponses(QueryComponent.java:298)\n    at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:290)\n    at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131)\n    at org.apache.solr.core.SolrCore.execute(SolrCore.java:1316)\n    at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:338)\n    at solandra.SolandraDispatchFilter.execute(SolandraDispatchFilter.java:169)\n    at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:241)\n    at solandra.SolandraDispatchFilter.doFilter(SolandraDispatchFilter.java:133)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1084)\n    at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:360)\n    at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n    at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)\n    at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:722)\n    at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:404)\n    at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:206)\n    at org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114)\n    at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:139)\n    at org.mortbay.jetty.Server.handle(Server.java:324)\n    at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:505)\n    at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:828)\n    at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:514)\n    at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:211)\n    at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:380)\n    at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n    at org.mortbay.thread.BoundedThreadPool$PoolThread.run(BoundedThreadPool.java:451)\n16:17:57,408  INFO SolrCore:1324 - [log] webapp=/solandra path=/select params={q=uuid:*} status=500 QTime=568 \n16:17:57,409 ERROR SolrDispatchFilter:139 - java.lang.NullPointerException\n    at org.apache.solr.handler.component.QueryComponent.mergeIds(QueryComponent.java:462)\n    at org.apache.solr.handler.component.QueryComponent.handleResponses(QueryComponent.java:298)\n    at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:290)\n    at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131)\n    at org.apache.solr.core.SolrCore.execute(SolrCore.java:1316)\n    at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:338)\n    at solandra.SolandraDispatchFilter.execute(SolandraDispatchFilter.java:169)\n    at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:241)\n    at solandra.SolandraDispatchFilter.doFilter(SolandraDispatchFilter.java:133)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1084)\n    at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:360)\n    at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n    at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)\n    at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:722)\n    at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:404)\n    at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:206)\n    at org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114)\n    at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:139)\n    at org.mortbay.jetty.Server.handle(Server.java:324)\n    at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:505)\n    at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:828)\n    at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:514)\n    at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:211)\n    at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:380)\n    at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n    at org.mortbay.thread.BoundedThreadPool$PoolThread.run(BoundedThreadPool.java:451)\n16:17:57,438  INFO SolandraCoreContainer:72 - Loading Solandra core: config\nYou can get the full log here:\nhttps://docs.google.com/leaf?id=0BxyBcfQyaiLCMzM1NTA1NmMtZjc3My00NGY1LWJlZWMtMDFkOGQ4YzU2MDk1&hl=en&authkey=CNriscMO\n. No.  Everything is an add at this point.  We aren't deleting anything.\n. ",
    "karussell": "Could it be that simply the following is missing from the dismax handler section?\n<arr name=\"components\">\n  <str>query</str>\n  <str>solandraComponent</str>\n  <str>facet</str>\n  <str>mlt</str>\n  <str>highlight</str>\n  <str>stats</str>\n  <str>debug</str>\n </arr>\nWithout that I didn't got results. With that all seems to be ok.\nI'll report in tomorrow (or in the next days ;)) if my usecase for dismax still fails with that addition.\n. Ok, nice. Feeding and querying from jetwick works now (with the above change) :-)\nSo, thanks for the changes!\n. sadly not yet, due too some other problems (see new issues). Also sorting seems to be slow. Or is only sort by latest date supported?\n. > How many many documents in the indexes?\nthere were only 5K I think\n\nAre you running this on a single node?\n\nfor this test, yes.\n\nSolandra won't be faster than Solr on a single node keep in mind.\n\nAlso for large indices >10GB?\n\nAlso you have shards.at.once=2 which doesn't make sense if you have 1 node. \n\nok\n. > Also you have shards.at.once=2 which doesn't make sense if you have 1 node.\nalso if the node has more than one core?\n. ah, now I understand. so \"at.once\" is important here ... sorry\n. ok, but this issue here is fixed. I'll close, ok?\n. > How many documents in your index?\nthere were only 5K I think\n. It did not occur after the latest changes ... but maybe some special condition was raised :-/\n. Yes, only indexing. I'll try your suggestion\n. \"1.7 gigs is low\", ok. I hoped that cassandra wouldn't consume that much RAM with less data in the index\n. > Well both Solr and Cassandra running on the same box \nok\n\nYes, only indexing. I'll try your suggestion\n\nforget about this: I'm querying before indexing e.g. to count retweets of existing tweets etc\nso: I'm heavily querying and then indexing (in an \"endless loop\"). I even have the unnecessary facets added (they are only necessary for normal queries from UI). This could lead to heavy RAM use!?\n. Not that many: that were about 1k-10k\nTried you suggestions: only one shard + the memory setting to only 160 mb\nThe jvm didn't crash yet. Still having some issues ... I'll investigate :)\n\nhow much ram do you see solr using for the same?\n\nyou mean pure solr? I have no problems to run jetwick with 1 mio docs/tweets for the same RAM settings on my laptop@home\n. ",
    "davidstrauss": "I seem to be running into this issue, too.\n. Here's a way to reproduce the issue:\n1. Download and index the Reuters data.\n2. curl \"http://localhost:8983/solandra/reuters/select?q=arden\" # Returns results\n3. curl \"http://localhost:8983/solandra/reuters/select?q=arden%20group\" # Returns results\n4. curl \"http://localhost:8983/solandra/reuters/select?q=%22arden%20group%22\" # Doesn't return results, but should\nTested on a build of commit 9078ad30e54087731dc4, which is the current HEAD.\n. @ceocoder Awesome. That seems to do the trick!\n. I see where you're going with not touching Solr's API once you reach /solandra/core-id/. That makes sense, and it makes me reluctant to recommend moving the non-Solr API operation of Solandra core setup under that path (which would be the case with /solandra/core-id/schema).\nI guess the only change I'd make is supporting PUT for /solandra/schema/core-id in order to recognize the whole \"PUT at the same URL replaces the existing data\" part of REST.\n. My mistake. Closing this.\n. It's a simple equality filter on a uid field.\n. I'll try to develop a simple set of steps for reproduction. Is 64-bit Ubuntu 10.04 okay?\n. I cannot reproduce this issue on the latest HEAD.\n. @JimKerwood Can you reproduce this issue with a fresh cluster set to RL=2 before you set any schemas or index any data?\n. @Gasol Does this problem still occur for you when you escape the space with \"%20\" or \"+\"? I can't reproduce this issue using normal URL escaping.\n. I definitely have curl installed.\n. After rebuilding everything, I can't reproduce this issue. Closing for now.\n. It's worth noting that Drupal's Solr module requires Luke for index introspection as part of the faceting support.\n. Interesting that it seems to be using the org.apache.lucene.index.IndexReader class instead of the Lucandra IndexReader one would expect from the use of SolandraIndexReaderFactory.\n. @tjake Does that mean we're required to specify one? Normal Solr seems to work fine without one as long as other criteria for the search exist.\n. Well, I assume it works because it's how Drupal's Solr module constructs some of its queries.\n. I've checked on the cassandra.yaml and CFs for the \"L\" keyspace, and I can't see any changes that would clearly break things.\n. Do you have any idea when the data model in Cassandra will stabilize or support upgrades from version-to-version? Or should we make plans to fully re-provision cores and reindex frequently?\n. ",
    "rcrezende": "Yes, you are right.\n. ",
    "issuu": "Thanks for changes, I made a rebase and revoking the pull request. As cassandra 0.7.2 looks stable I'll start my test with solandra again but we should wait until our reindexing is ready.\n. ",
    "stewsters": "Figured it out.  My xml file I uploaded had a missing fieldtype in it, so any attempt to access it was returning error code 0.\n. ",
    "ozkanpakdil": "I get invalid index exception when I call\nhttp://localhost:8983/solandra/reuters/admin/ping?wt=javabin&version=2.2\nHTTP ERROR 500\nProblem accessing /solandra/reuters/admin/ping. Reason:\njava.io.IOException: invalid index\njava.lang.RuntimeException: java.io.IOException: invalid index\n    at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:101)\n    at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:194)\n    at solandra.SolandraDispatchFilter.doFilter(SolandraDispatchFilter.java:122)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n    at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n    at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n    at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n    at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n    at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\n    at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)\n    at org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114)\n    at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n    at org.mortbay.jetty.Server.handle(Server.java:326)\n    at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n    at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n    at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n    at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n    at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n    at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n    at org.mortbay.thread.BoundedThreadPool$PoolThread.run(BoundedThreadPool.java:451)\nCaused by: java.io.IOException: invalid index\n    at org.apache.solr.core.SolandraCoreContainer.readSchema(SolandraCoreContainer.java:166)\n    at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:97)\n    ... 19 more\nPowered by Jetty://\nI hope it helps to solve \n. actually I compiled it and test it under linux. there was no problem like this but when I tried it under AIX 5.3 with IBM jdk I get this kind of errors. I am just trying to make it work. and move our production search system into solandra.\nwhen I run ./2-import-data.sh this comes\nGWEB4:/home/BUILD/tmp/tjake-Solandra-c27c46d/reuters-demo # ./2-import-data.sh\n./2-import-data.sh[7]: curl:  not found.\nPosted schema.xml to http://localhost:8983/solandra/schema/reuters\nLoading data to solandra, note: this importer uses a slow xml parser\nException in thread \"main\" java.lang.RuntimeException: unable to connect to solr server: http://localhost:8983/solandra/reuters\n        at org.apache.solr.solrjs.sgml.reuters.ReutersService.(ReutersService.java:93)\n        at org.apache.solr.solrjs.sgml.reuters.ReutersService.main(ReutersService.java:63)\nCaused by: org.apache.solr.common.SolrException: java.io.IOException: invalid index  java.lang.RuntimeException: java.io.IOException: invalid index     at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:101)   at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:194)     at solandra.SolandraDispatchFilter.doFilter(SolandraDispatchFilter.java:122)    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)      at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)     at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)  at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)     at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)     at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)         at org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114)       at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)     at org.mortbay.jetty.Server.handle(Server.java:326)     at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)      at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)      at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)  at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)     at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)     at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)       at org.mortbay.thread.BoundedThreadPool$PoolThread.run(BoundedThreadPool.java:451) Caused by: java.io.IOException: invalid index        at org.apache.solr.core.SolandraCoreContainer.readSchema(SolandraCoreContainer.java:166)        at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:97)    ... 19 more\njava.io.IOException: invalid index  java.lang.RuntimeException: java.io.IOException: invalid index      at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:101)   at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:194)     at solandra.SolandraDispatchFilter.doFilter(SolandraDispatchFilter.java:122)    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)      at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)     at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)  at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)     at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)     at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)         at org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114)       at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)     at org.mortbay.jetty.Server.handle(Server.java:326)     at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)      at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)      at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)  at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)     at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)     at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)       at org.mortbay.thread.BoundedThreadPool$PoolThread.run(BoundedThreadPool.java:451) Caused by: java.io.IOException: invalid index        at org.apache.solr.core.SolandraCoreContainer.readSchema(SolandraCoreContainer.java:166)        at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:97)    ... 19 more\nrequest: http://localhost:8983/solandra/reuters/admin/ping?wt=javabin&version=2.2\n        at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:343)\n        at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:183)\n        at org.apache.solr.client.solrj.request.SolrPing.process(SolrPing.java:60)\n        at org.apache.solr.client.solrj.SolrServer.ping(SolrServer.java:105)\n        at org.apache.solr.solrjs.sgml.reuters.ReutersService.(ReutersService.java:91)\n        ... 1 more\nData loaded, now open ./website/index.html in your favorite browser!\nand at /home/BUILD/tmp/tjake-Solandra-c27c46d/solandra-app/logs/2011_05_06.stderrout.log\n10:02:09,021 ERROR SolrDispatchFilter:151 - java.lang.RuntimeException: java.io.IOException: invalid index\n        at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:101)\n        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:194)\n        at solandra.SolandraDispatchFilter.doFilter(SolandraDispatchFilter.java:122)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\n        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)\n        at org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114)\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n        at org.mortbay.thread.BoundedThreadPool$PoolThread.run(BoundedThreadPool.java:451)\nCaused by: java.io.IOException: invalid index\n        at org.apache.solr.core.SolandraCoreContainer.readSchema(SolandraCoreContainer.java:166)\n        at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:97)\n        ... 19 more\n10:02:23,855 ERROR SolrDispatchFilter:151 - java.lang.RuntimeException: java.io.IOException: invalid index\n        at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:101)\n        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:194)\n        at solandra.SolandraDispatchFilter.doFilter(SolandraDispatchFilter.java:122)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\n        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)\n        at org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114)\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n        at org.mortbay.thread.BoundedThreadPool$PoolThread.run(BoundedThreadPool.java:451)\nCaused by: java.io.IOException: invalid index\n        at org.apache.solr.core.SolandraCoreContainer.readSchema(SolandraCoreContainer.java:166)\n        at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:97)\n        ... 19 more\nthose comes.\n. thanks for the info now it works :)\n. ",
    "JimKerwood": "We don't even get that far.  Either it will time out with the 1024 tries or if while it is trying I bring back up the node it will throw an exception with connection refused (since it isn't initilaized I'm guessing but the port is there).\n. No here is the use case:\n1) All 6 boxes running.  Querys all work.\n2) Bring 1 box down for maint.  Querys now start timing out.  Assume querys should continue on running boxes.\n3) Bring box back up.  Any query trying gets a socket timeout.\n4) When all back running all querys work.\nI think the HTTP request is trying to hit all 6 boxes.  It is failing there not even down at the Cassandra level.\nSome of the stacktrace:\nHTTP ERROR 500\nProblem accessing /solandra/checks/select. Reason: \n    org.apache.solr.client.solrj.SolrServerException: java.net.ConnectException: Connection refused\norg.apache.solr.common.SolrException: org.apache.solr.client.solrj.SolrServerException: java.net.ConnectException: Connection refused\n    at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:282)\n    at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131)\n    at org.apache.solr.core.SolrCore.execute(SolrCore.java:1316)\n...\nCaused by: org.apache.solr.client.solrj.SolrServerException: java.net.ConnectException: Connection refused\n    at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:483)\n    at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:244)\nat org.apache.solr.handler.component.HttpCommComponent$1.call(SearchHandler.java:422)\n. Agree with the replication factor of 1.\nSo you are saying if I have a replication factor of 2 and I have one machine down this will not error anymore?  If so I am satisfied.\nThough if I set the replication factor to 2 and it still errors with one machine down I would say this should be fixed.\n. Even after changing replication and repairing problem exists.  If a node is down all other nodes wait (timeout if left long enough)\n. ",
    "topoqdm": "Hi\nI have two  nodes running, set replication_factor:3 and run repair tool on L keyspace.\nWhen one of the nodes goes down, search fails on the remaining node. \nI get this exception\nread command failed after 1024attempts  java.io.IOException: Read command failed after 1024attempts     at lucandra.CassandraUtils.robustRead(CassandraUtils.java:625)  at lucandra.CassandraUtils.robustRead(CassandraUtils.java:634)  at solandra.SolandraComponent.flushCache(SolandraComponent.java:67)     at solandra.SolandraComponent.prepare(SolandraComponent.java:115)   at solandra.SolandraQueryComponent.prepare(SolandraQueryComponent.java:45)  at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:173)    at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:129)    at org.apache.solr.core.SolrCore.execute(SolrCore.java:1368)    at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:356)  at solandra.SolandraDispatchFilter.execute(SolandraDispatchFilter.java:171)     at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:252)     at solandra.SolandraDispatchFilter.doFilter(SolandraDispatchFilter.java:137)    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)  at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)     at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)     at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)     at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)     at org.mortbay.jetty.Server.handle(Server.java:326)     at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)  at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:945)     at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:756)  at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)     at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)     at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)   at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\nrequest: http://192.168.1.99:8983/solandra/reuters~0/select\n    at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:430)\n    at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:244)\nI also tried changing solandra.consistency from QUORUM to ONE on solandra.properties, but this didn't help.\nAny ideas how to fix this or if i'm doing something wrong?\n. ",
    "leonardlabuneti": "Hi Jake, I tried replication 2 and 3, the problem persists, once you have a node down you cannot do any request to any other live nodes.\nThanks \n. ",
    "Gasol": "i'll give it a try...\n. ",
    "cwesdorp": "The items disappear after a new document is added and the logging shows \"ShardInfo for thoughtbucket has expired\" and \"32767 reserved ids for thoughtbucket have expired\"\nThe reuters demo does not show this problem. \nThere was one difference I noticed. When I add a document the commit is in the same message posted to Solandra. I have changed my program to send the submit separately and things seem to go better now. Could there be a problem in submitting a 'multi' document?\n. I was able to reproduce the issue for a few times now based on the solandra branch.\nI opened a repository, https://github.com/cwesdorp/solandra-issue81-docs, with some documents I used for testing. The upload_docs.sh expects a JSON handler to be added to the solr properties. \nFirst execute upload_schema.sh. \nThen execute \"upload_doc.sh doc1 commit\" which commits the first document to the store. \nExecute query.sh or open \"http://localhost:8983/solandra/thoughtbucket/select?wt=json&indent=on&q=:\" in a browser, one result is shown.\nWait 30 seconds.\nThen execute \"upload_doc.sh doc2 commit\".\nExecute query.sh or open \"http://localhost:8983/solandra/thoughtbucket/select?wt=json&indent=on&q=:\" in a browser, in my case the first document has disappeared.\n. Maybe this is something: I just pulled the latest commits and had another try because I wanted to see what is stored in the cassandra index. After committing a document a warning/error is shown when listing the TI column family.\n```\n[default@L] list TI; \nUsing default limit of 100\n\nRowKey: 3660269691873133950988201044714607423?allText?http\nUnknown comparator 'lucandra.VIntType'. Available functions: bytes, integer, long, lexicaluuid, timeuuid, utf8, ascii.\n```\n. Hi Jake,\nyes I used the cassandra-cli in the solandra-app/cassandra-tools folder.\nUnfortunately the behavior hasn't changed for the better. Now I experience disappearing of documents some time after the last commit, the timeout doesn't seem to be a fixed time. Also, I had a situation where a document was posted and not returned in the query result. When I shutdown solandra I remove the /tmp/cassandra-data and the /tmp/index folder it seems to create as well, before starting it again. For my test I did a complete new clone of the repository.\nChris\n. Hi Jake,\na lot of commits related to indexes and ids have come by so I thought to give it a try. I pulled the latest and did a quick test using the test docs provided earlier. At this point I can't reproduce the issue. Are the recent commits related or do you also consider the issue fixed?  If not I will test again later again.\nChris\n. Using the latest pull I also ran into this problem, in my case the sort was on a long field. It is reproduce-able using the schema from my testcase for the other issue, https://github.com/cwesdorp/solandra-issue81-docs . Add some documents and the fire a query with a sort, e.g. \"select?q=:&wt=json&indent=on&sort=timestamp desc\".\n. Just pulled the lasted code compiled and tried again. To reproduce I only need to upload two documents.\n- ./upload_schema.sh\n- ./upload_doc.sh doc1 doc2 commit\n- curl 'http://localhost:8983/solandra/thoughtbucket/select?q=:&wt=json&indent=on&sort=timestamp%20asc'\n``` plain\n21:56:36,302 ERROR SolrCore:151 - org.apache.solr.common.SolrException: Field cache data missing\njava.io.IOException: Field cache data missing   \n    at org.apache.lucene.search.LucandraFieldCache.getFieldCacheEntries(LucandraFieldCache.java:849)  \n    at org.apache.lucene.search.LucandraFieldCache.access$000(LucandraFieldCache.java:36) \n    at org.apache.lucene.search.LucandraFieldCache$LongCache.createValue(LucandraFieldCache.java:622) \n    at org.apache.lucene.search.LucandraFieldCache$Cache.get(LucandraFieldCache.java:233) \n    at org.apache.lucene.search.LucandraFieldCache.getLongs(LucandraFieldCache.java:594)  \n    at org.apache.lucene.search.LucandraFieldCache$LongCache.createValue(LucandraFieldCache.java:613) \n    at org.apache.lucene.search.LucandraFieldCache$Cache.get(LucandraFieldCache.java:233) \n    at org.apache.lucene.search.LucandraFieldCache.getLongs(LucandraFieldCache.java:594)  \n    at org.apache.lucene.search.FieldComparator$LongComparator.setNextReader(FieldComparator.java:503)\n    at org.apache.lucene.search.TopFieldCollector$OneComparatorNonScoringCollector.setNextReader(TopFieldCollector.java:95)   \n    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:516)  \n    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:313)  \n    at org.apache.solr.search.SolrIndexSearcher.getDocListNC(SolrIndexSearcher.java:1177) \n    at org.apache.solr.search.SolrIndexSearcher.getDocListC(SolrIndexSearcher.java:1065)  \n    at org.apache.solr.search.SolrIndexSearcher.search(SolrIndexSearcher.java:358)\n    at org.apache.solr.handler.component.QueryComponent.process(QueryComponent.java:182)  \n    at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:194)  \n    at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:129)  \n    at org.apache.solr.core.SolrCore.execute(SolrCore.java:1360)  \n    at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:356)\n    at solandra.SolandraDispatchFilter.execute(SolandraDispatchFilter.java:156)   \n    at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:252)     \nrequest: http://127.0.0.1:8983/solandra/thoughtbucket~0/select\n    at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:436)\n    at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:245)\n    at org.apache.solr.handler.component.HttpCommComponent$1.call(SearchHandler.java:421)\n    at org.apache.solr.handler.component.HttpCommComponent$1.call(SearchHandler.java:393)\n    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:138)\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)\n    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:138)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n    at java.lang.Thread.run(Thread.java:680)\n```\n. My test also runs successful now. Thanks!\n. Ran into the same issue using Java 1.7 (build 1.7.0_04-ea-b18) on OS X 10.7. Tried to upgrade to snappy 1.0.4.1 and add \"-Dorg.xerial.snappy.tempdir=/tmp\" as JVM option, as described on the Cassandra troubleshooting page. But the only solution for me now seems to set \"solandra.compression\" to false in conf/solandra.properties. \n. ",
    "jschappet": "In the Cassandra Log:\nINFO [347500362@qtp-698491110-3] 2011-05-10 08:05:53,005 SolrCore.java (line 1370) [reuters] webapp=/solandra path=/terms params={terms.regex=Coffee.*&terms.regex.flag=case_insensitive&terms.fl=combo} status=0 QTime=170 \n. Jake,\nIs there anything I might be able to try on my side to get this working?\n--Jimmy\n. Search terms are working.  \nThanks for the updates Jake.\n. http://localhost:8983/solandra/geonames/dataimport?command=full-import&clean=false\n. Added to the solrconfig.xml\n\n<requestHandler name=\"/dataimport\" class=\"org.apache.solr.handler.dataimport.DataImportHandler\">\n<lst name=\"defaults\">\n  <str name=\"config\">data-config.xml</str>\n</lst>\n</requestHandler>\n\nDropped apache-solr-dataimporthandler-3.1.0.jar  into Lib.\n. Looks like I had the wrong code:\n\n            SolandraCoreInfo coreInfo = SolandraCoreContainer.coreInfo.get();\n                if (coreInfo == null) logger.error(\"CoreInfo Is Null\");\n\nERROR [Thread-11] 2011-06-01 07:29:15,587 SolandraIndexWriter.java (line 212) CoreInfo Is Null\n WARN [Thread-11] 2011-06-01 07:29:15,587 SolrWriter.java (line 75) Error creating document : SolrInputDocument[{combo=combo(1.0)={kharmat b? haf?fah b??in?yah,sabkhat b? haf?fah,kazakhstan|Kharmat B? Haf?fah B??in?yah,Sabkhat B? Haf?fah,Kazakhstan}}]\njava.lang.NullPointerException\n        at solandra.SolandraIndexWriter.addDoc(SolandraIndexWriter.java:220)\n. Worked.  Thanks\n. ",
    "derryx": "Your change now leads to an OutOfMemoryError.\n. I use the Solandra version from 5th of may. No previous index has existed. We started from scratch.\n. Field-type definition:\n<fieldType name=\"date\" class=\"solr.DateField\" sortMissingLast=\"true\" omitNorms=\"true\"/>\nField-definition:\n<field name=\"timestamp\" type=\"date\" indexed=\"true\" stored=\"true\"/>\n. Commenting out\nhttps://github.com/tjake/Solandra/blob/solandra/src/lucandra/CassandraUtils.java#L94\nsolved the problem.\n. The sorting worked in the beginning but stopped working after moving more and more data into Solandra.\n. I can confirm that it works now.\nThank you!\n. ",
    "fbrier": "Did David ever get this resolved?  My integration test has an identical stack trace.  I created a query with one parameter:\nSolrQuery query = new SolrQuery();\n    query.setParam( \"code\", \"CPRAV\" );\n    QueryResponse rsp = server.getSolrClient().query( query );\nI could try and duplicate the problem with the reuters demo if that would be helpful.\n. ",
    "jhorman": "Can I ask (not sure if you answered this somewhere) why you did this. We are experiencing a problem with sorting in solandra, and we aren't yet sure if the LucandraFieldCache is the issue. I am planning on compiling my own version with this change to see.\n. ",
    "mevivs": "It is located in lib folder inside.. looks like lib is missing from classpath?\n. It is working fine now. By mistake added a code snippet to delete col family.\n. Reason for this is i want to start this implicitly within my app. \nThanks, i am able to resolve this issue. But just wanted to ask, is there any specific reason for using CassandraServer cs = new CassandraServer() in CassandraUtils.java?\nShould it be flxible enough to configure a client via external app?\n. Thanks... But will that load my updated db, as same is done by CassandraDaemon.activate() ?\nI mean loading metadata, DatabaseDescription defs etc.\n. i also faced the same issue. But i cleaned up my .ivy/cache and tried build again.\nIt was succesfull\n. ",
    "billau": "I just downloaded the tar ball and am having the same problem.  I am behind a firewall.\n[ivy:retrieve] :: problems summary ::\n[ivy:retrieve] :::: WARNINGS\n[ivy:retrieve]  io problem while parsing ivy file: https://repository.cloudera.com/content/groups/cdh-build/org/apache/velocity/velocity/1.6.4/velocity-1.6.4.pom: Resetting to invalid mark\n[ivy:retrieve]  io problem while parsing ivy file: http://repo1.maven.org/maven2/org/apache/velocity/velocity/1.6.4/velocity-1.6.4.pom: Resetting to invalid mark\n[ivy:retrieve]          module not found: org.apache.velocity#velocity;1.6.4\n[ivy:retrieve]  ==== java.net2: tried\n[ivy:retrieve]    http://download.java.net/maven/2/org/apache/velocity/velocity/1.6.4/velocity-1.6.4.pom\n[ivy:retrieve]    -- artifact org.apache.velocity#velocity;1.6.4!velocity.jar:\n[ivy:retrieve]    http://download.java.net/maven/2/org/apache/velocity/velocity/1.6.4/velocity-1.6.4.jar\n[ivy:retrieve]  ==== cloudera: tried\n[ivy:retrieve]    https://repository.cloudera.com/content/groups/cdh-build/org/apache/velocity/velocity/1.6.4/velocity-1.6.4.pom\n[ivy:retrieve]  ==== ibiblio: tried\n[ivy:retrieve]    http://repo1.maven.org/maven2/org/apache/velocity/velocity/1.6.4/velocity-1.6.4.pom\n[ivy:retrieve]  io problem while parsing ivy file: https://repository.cloudera.com/content/groups/cdh-build/org/apache/velocity/velocity-tools/2.0/velocity-tools-2.0.pom: Resetting to invalid mark\n[ivy:retrieve]  io problem while parsing ivy file: http://repo1.maven.org/maven2/org/apache/velocity/velocity-tools/2.0/velocity-tools-2.0.pom: Resetting to invalid mark\n[ivy:retrieve]          module not found: org.apache.velocity#velocity-tools;2.0\n[ivy:retrieve]  ==== java.net2: tried\n[ivy:retrieve]    http://download.java.net/maven/2/org/apache/velocity/velocity-tools/2.0/velocity-tools-2.0.pom\n[ivy:retrieve]    -- artifact org.apache.velocity#velocity-tools;2.0!velocity-tools.jar:\n[ivy:retrieve]    http://download.java.net/maven/2/org/apache/velocity/velocity-tools/2.0/velocity-tools-2.0.jar\n[ivy:retrieve]  ==== cloudera: tried\n[ivy:retrieve]    https://repository.cloudera.com/content/groups/cdh-build/org/apache/velocity/velocity-tools/2.0/velocity-tools-2.0.pom\n[ivy:retrieve]  ==== ibiblio: tried\n[ivy:retrieve]    http://repo1.maven.org/maven2/org/apache/velocity/velocity-tools/2.0/velocity-tools-2.0.pom\n[ivy:retrieve]          ::::::::::::::::::::::::::::::::::::::::::::::\n[ivy:retrieve]          ::          UNRESOLVED DEPENDENCIES         ::\n[ivy:retrieve]          ::::::::::::::::::::::::::::::::::::::::::::::\n[ivy:retrieve]          :: org.apache.velocity#velocity;1.6.4: not found\n[ivy:retrieve]          :: org.apache.velocity#velocity-tools;2.0: not found\n[ivy:retrieve]          ::::::::::::::::::::::::::::::::::::::::::::::\n[ivy:retrieve] \n. setting log level and the statics pages, among other.  The admin interface\nis used by our operations staff so it would be nice if the same is available\nin Solandra for them.\nBill\nps  nice talking to you this morning at CassandraSF.  Definitely going to\nyour talk in the afternoon.\nOn Sun, Jul 10, 2011 at 5:05 PM, tjake \nreply@reply.github.comwrote:\n\nHi the admin interface is not supported (yet) in solandra. What\nspecifically do you want to see from it?\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/tjake/Solandra/issues/108#issuecomment-1543199\n. \n",
    "Kirill-Petersburg": "Thank you Jake for your quick response. I tried the latest Solandra build. It works with the xml generated. But I increased a number of items in documents and size of the items also. The bug can be reproduced again on the latest Solandra build.\nAll the code change were made in addRandomDocument () :\n```\npublic static void addRandomDocument (XMLStreamWriter xmlWriter, String [] words, int id) throws Exception {\n    xmlWriter.writeCharacters (\"\\n\");\n    xmlWriter.writeStartElement (\"http://www.w3.org/TR/REC-html40\", \"doc\");\n    xmlWriter.writeCharacters (\"\\n\");\nxmlWriter.writeStartElement (\"http://www.w3.org/TR/REC-html40\", \"field\");\nxmlWriter.writeAttribute (\"name\", \"id\");\nxmlWriter.writeCharacters (String.format (\"%08d\", id));\nxmlWriter.writeEndElement ();\nxmlWriter.writeCharacters (\"\\n\");\n\nxmlWriter.writeStartElement (\"http://www.w3.org/TR/REC-html40\", \"field\");\nxmlWriter.writeAttribute (\"name\", \"title\");\nxmlWriter.writeCharacters (\"Document: \" + String.format (\"%08d\", id));\nxmlWriter.writeEndElement ();\nxmlWriter.writeCharacters (\"\\n\");\n\nString [] additionalItems = {\"Item1\",  \"Item2\",  \"Item3\",\n                             \"Item4\",  \"Item5\",  \"Item6\",\n                             \"Item7\",  \"Item8\",  \"Item9\",\n                             \"Item10\", \"Item11\", \"Item12\",\n                             \"Item13\", \"Item14\", \"Item15\",\n                             \"Item16\", \"Item17\", \"Item18\",\n                             \"Item19\", \"Item20\"};\n\nfor (String item : additionalItems) {\n    xmlWriter.writeStartElement (\"http://www.w3.org/TR/REC-html40\", \"field\");\n    xmlWriter.writeAttribute (\"name\", item);\n    xmlWriter.writeCharacters (String.format (\"%08d\", id)\n                              + \"-\"\n                              + words [m_randomGenerator.nextInt (words.length)]\n                              + \"-\"\n                              + String.format (\"%08d\", m_randomGenerator.nextInt (maxRandomInt))\n                              + \"-\"\n                              + words [m_randomGenerator.nextInt (words.length)]\n                              + \"-\"\n                              + String.format (\"%08d\", m_randomGenerator.nextInt (maxRandomInt))\n                              + \"-\"\n                              + words [m_randomGenerator.nextInt (words.length)]\n                              + \"-\"\n                              + String.format (\"%08d\", m_randomGenerator.nextInt (maxRandomInt))\n                              + \"-\"\n                              + words [m_randomGenerator.nextInt (words.length)]);\n    xmlWriter.writeEndElement ();\n    xmlWriter.writeCharacters (\"\\n\");\n}\n\nxmlWriter.writeEndElement ();\nxmlWriter.writeCharacters (\"\\n\");\n\n}\n```\nAlso the schema was changed so as to index the new Items added.\nActual result: numDocs value returned as a response for ../select/?q=: is less than million (about 960000)\nO/S Platform: Windows Server 2008.\n. I don't see any errors in the log. All insert requests have status=0. This is the tail of the log containing ../select/?q=: :\n.....\n INFO 10:36:44,291 {add=[00000199, 00000198, 00000197, 00000196, 00000195, 00000194, 00000193, 00000192, ... (200 adds)]} 0 59961\n INFO 10:36:44,291 [solr_data] webapp=/solandra path=/update params={} status=0 QTime=59961 \n INFO 10:36:53,126 Completed flushing \\tmp\\cassandra-data\\data\\L\\TI-g-259-Data.db (81349402 bytes)\n INFO 10:36:53,126 Discarding obsolete commit log:CommitLogSegment(C:\\tmp\\cassandra-data\\commitlog\\CommitLog-1308159375897.log)\n INFO 10:36:53,126 Discarding obsolete commit log:CommitLogSegment(C:\\tmp\\cassandra-data\\commitlog\\CommitLog-1308159386678.log)\n INFO 10:37:04,742 Compacted to \\tmp\\cassandra-data\\data\\L\\TI-tmp-g-251-Data.db.  320,647,592 to 317,068,626 (~98% of original) bytes for 1,987,347 keys.  Time: 163,491ms.\n INFO 10:37:04,773 Minor@631894615(L, TI, 172868922/329115236) now compacting at 16777 bytes/ms.\n INFO 10:37:50,982 Compacted to \\tmp\\cassandra-data\\data\\L\\TI-tmp-g-256-Data.db.  329,115,236 to 324,791,074 (~98% of original) bytes for 2,038,944 keys.  Time: 119,367ms.\n INFO 01:34:20,014 ShardInfo for solr_data has expired\n INFO 01:34:20,014 Found reserved shard10(147190272376517611437769280282474584895):131070 TO 131072\n INFO 01:34:20,014 Found reserved shard11(147190272376517611437769280282474584895):131066 TO 131072\n INFO 01:34:20,014 Found reserved shard12(147190272376517611437769280282474584895):119878 TO 131072\n INFO 01:34:20,014 Found reserved shard13(147190272376517611437769280282474584895):37957 TO 49152\n INFO 01:34:20,014 Found reserved shard14(147190272376517611437769280282474584895):127803 TO 131072\n INFO 01:34:20,014 Found reserved shard16(147190272376517611437769280282474584895):103787 TO 114688\n INFO 01:34:20,014 Found reserved shard5(147190272376517611437769280282474584895):119860 TO 131072\n INFO 01:34:20,014 Found reserved shard6(147190272376517611437769280282474584895):131060 TO 131072\n INFO 01:34:20,014 Found reserved shard7(147190272376517611437769280282474584895):119886 TO 131072\n INFO 01:34:20,014 Found reserved shard9(147190272376517611437769280282474584895):119413 TO 131072\n INFO 01:34:20,014 solr_data has 20 shards\n INFO 01:34:21,028 [solr_data] webapp=/solandra path=/select params={fl=id,score&start=0&q=:&isShard=true&wt=javabin&fsv=true&rows=10&version=2} hits=5481 status=0 QTime=312 \n INFO 01:34:21,855 [solr_data] webapp=/solandra path=/select params={fl=id,score&start=0&q=:&isShard=true&wt=javabin&fsv=true&rows=10&version=2} hits=29496 status=0 QTime=1139 \n INFO 01:34:21,995 [solr_data] webapp=/solandra path=/select params={fl=id,score&start=0&q=:&isShard=true&wt=javabin&fsv=true&rows=10&version=2} hits=29859 status=0 QTime=1279 \n INFO 01:34:22,135 [solr_data] webapp=/solandra path=/select params={fl=id,score&start=0&q=:&isShard=true&wt=javabin&fsv=true&rows=10&version=2} hits=32765 status=0 QTime=1419 \n INFO 01:34:22,525 [solr_data] webapp=/solandra path=/select params={fl=id,score&start=0&q=:&isShard=true&wt=javabin&fsv=true&rows=10&version=2} hits=32768 status=0 QTime=1809 \n INFO 01:34:22,837 [solr_data] webapp=/solandra path=/select params={fl=id,score&start=0&q=:&isShard=true&wt=javabin&fsv=true&rows=10&version=2} hits=55281 status=0 QTime=2121 \n INFO 01:34:23,836 [solr_data] webapp=/solandra path=/select params={fl=id,score&start=0&q=:&isShard=true&wt=javabin&fsv=true&rows=10&version=2} hits=27733 status=0 QTime=3120 \n INFO 01:34:24,117 [solr_data] webapp=/solandra path=/select params={fl=id,score&start=0&q=:&isShard=true&wt=javabin&fsv=true&rows=10&version=2} hits=32768 status=0 QTime=3401 \n INFO 01:34:24,117 GC for ParNew: 213 ms, 1114756088 reclaimed leaving 18086261952 used; max is 34328281088\n INFO 01:34:24,491 [solr_data] webapp=/solandra path=/select params={fl=id,score&start=0&q=:&isShard=true&wt=javabin&fsv=true&rows=10&version=2} hits=65104 status=0 QTime=3775 \n INFO 01:34:24,600 [solr_data] webapp=/solandra path=/select params={fl=id,score&start=0&q=:&isShard=true&wt=javabin&fsv=true&rows=10&version=2} hits=71096 status=0 QTime=3869 \n INFO 01:34:24,647 [solr_data] webapp=/solandra path=/select params={fl=id,score&start=0&q=:&isShard=true&wt=javabin&fsv=true&rows=10&version=2} hits=46672 status=0 QTime=3931 \n INFO 01:34:24,834 [solr_data] webapp=/solandra path=/select params={fl=id,score&start=0&q=:&isShard=true&wt=javabin&fsv=true&rows=10&version=2} hits=65121 status=0 QTime=4118 \n INFO 01:34:25,318 [solr_data] webapp=/solandra path=/select params={fl=id,score&start=0&q=:&isShard=true&wt=javabin&fsv=true&rows=10&version=2} hits=65114 status=0 QTime=4602 \n INFO 01:34:25,318 [solr_data] webapp=/solandra path=/select params={fl=id,score&start=0&q=:&isShard=true&wt=javabin&fsv=true&rows=10&version=2} hits=42597 status=0 QTime=4587 \n INFO 01:34:25,318 [solr_data] webapp=/solandra path=/select params={fl=id,score&start=0&q=:&isShard=true&wt=javabin&fsv=true&rows=10&version=2} hits=32767 status=0 QTime=4602 \n INFO 01:34:25,412 [solr_data] webapp=/solandra path=/select params={fl=id,score&start=0&q=:&isShard=true&wt=javabin&fsv=true&rows=10&version=2} hits=65111 status=0 QTime=4681 \n INFO 01:34:25,505 [solr_data] webapp=/solandra path=/select params={fl=id,score&start=0&q=:&isShard=true&wt=javabin&fsv=true&rows=10&version=2} hits=58707 status=0 QTime=4789 \n INFO 01:34:26,348 [solr_data] webapp=/solandra path=/select params={isShard=true&wt=javabin&q=:&ids=00836829,00823158,00817590,00844223,00843426,00823962,00818188,00823961,00834699,00836830&version=2} status=0 QTime=765 \n INFO 01:34:26,348 [solr_data] webapp=/solandra path=/select/ params={q=:} status=0 QTime=6506 \nThe part of corresponding http response is:\n... name=\"responseHeader\"\n... name=\"status\">0<\n... name=\"QTime\">6506<\n... name=\"response\" numFound=\"758440\" ..\n\"\n\n\n0\n6506\n\n\n00823158-=-69117151-\"-54301014->-28746119-void....\n\"\nAlso I found out that changing the following values in solandra.properties\nNOTE This value should not be changed once documents are indexed\nNOTE This value must be a power of 2\nsolandra.maximum.docs.per.shard = 131072\nThe number of index ids to reserve at a time\nNOTE this value must be a power of 2\nsolandra.index.id.reserve.size = 16384\n(I did all the changes on fresh Solandra setup before insertion all the documents) affects numFound value.\n. 1M per shard is not enough. It started working with 4M docs per shard. Could you implement a general solution for the issue, e.g. provide instruction how to estimate solandra.maximum.docs.per.shard or/and implemet an http error response  if a number of documents returned is wrong.\n. Ok, I'll test on the latest release.\n. I tested the tjake-Solandra-3af3b0b build. It failed. Having inserted 2000000 documents I executed q=: query. And the response was \"result name=\"response\" numFound=\"1581806\" start=\"0\"\".\n. I tryed different techniques of sending add requests:\n1. sending all requests to one ip (node ip) sequentially\n2. sending requests from 10 different sources to one ip (node ip)\n3. sending 2 different series of requests to two ip (2 node ip of the cluster) sequentially for each\n4. sending 2 different series of requests to two ip (2 node ip of the cluster) using 10 different sources for each\nFor each experiment I measured data traffic. 1 and 2 gave about 20% perfomance gain compared to 1 node system. But there were some problems with 3 and 4 due to load balancing. There were different data traffics reached for nodes, e.g 600 doc/sec for 1-st (maximum) node and 150 doc/sec for 2-nd (maximum). Also node disc space usage was unsymmetrical:\nbin/nodetool -h node1_ip ring\nAddress         Status State   Load            Owns    Token\n\u2026\nnode1_ip      Up     Normal  70.87 KB        19.39% \nnode2_ip      Up     Normal  141.38 MB       80.61%\n. Thank you Jake, I followed the instruction and succeeded in balancing the Cassandra nodes (50%/50%). Then I made 2 experiments:\n1.  sending add requests to one ip (node ip) sequentially\n2.  sending add requests to two ip\u2019s of the 2 node cluster simultaneously.\nThe 2 node system gives a 20% performance gain compared to 1 node system in experiment #1. In experiment #2 the document traffic was stuck for one of the nodes (it was about 10 times less compared to traffic of another node). And the system performance was the same as in experiment #1.\nSo It seems only one node of the cluster is responsible for indexing. Am I right? Is it possible to combine CPU power of the two nodes?\n. - The on disk size is not even:\n  $ bin/nodetool -h node1_ip ring\n  Address         Status State   Load            Owns    Token\n                                                     85070591730234615865843651857942052864\n  node1_ip      Up     Normal  3.82 GB         50.00%  0\n  node2_ip      Up     Normal  1.11 GB         50.00%  85070591730234615865843651857942052864\n- I'm using the bulk loading. In the experiments I sent series of bulks containing 400-800 documents in one xml (the number is fixed during an experiment).\n- Could please explane how to add a ~ before the index name?\n  I'm using cURL to insert an xml:\n  URL=http://localhost:8983/solandra/solr_data/update\n  curl $URL --data-binary @$1 -H 'Content-type:text/xml; charset=utf-8' \n. ",
    "wwwill": "Hi Jake- I added a 2nd node to an existing single node, 6M doc index.  I saw some bootstrapping action, and your suggestion helped to get them to 50/50- thanks.  However, index performance got worse (166 to 111 docs/sec) and query performance is basically the same (mostly well under 500ms).  For both indexing/querying, I am doing serial requests going to 1 node (I understood from your earlier post that query distribution is handled internally anyway).  For shards settings, I'm using all \"out of the box\" defaults.  I'm more curious about the lack of query performance improvement.  Can you suggest anything there?  Do you expect much of a difference if I start with 2 nodes and an empty index?\n. I was not giving it a specific core.  After I pushed a schema (and created a named core) and then connected to that with\nSolrServer server = new CommonsHttpSolrServer(\"http://localhost:8983/solandra/myschema\");\neverything started to work.  No more NPEs and adding/querying index work now.  I followed the example of the reuters demo.\n. Thanks, Oleg.  I was able to stop the dropped reads and get reasonable response times by running queries after each large commit (so-called warming).  I will try your suggestion if I see the issue again.\n. ",
    "ycoe": "I got the same exception,what's the matter?\nwhy closed this issue?\nfixed?\n. thanks,it work!\n. ",
    "dansherpa": "Any suggestions for enjoying the Reuter's data load on a headless system (i.e. no GUI)?\n. ",
    "tksohishi": "Thanks!\n. ",
    "hlinski": "i have same problem on mac os x:\n---------------------------------------------------------------------\n    |                  |            modules            ||   artifacts   |\n    |       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n    ---------------------------------------------------------------------\n    |      default     |   96  |   0   |   0   |   11  ||   85  |   0   |\n    ---------------------------------------------------------------------\n[ivy:retrieve] \n[ivy:retrieve] :: problems summary ::\n[ivy:retrieve] :::: WARNINGS\n[ivy:retrieve]          ::::::::::::::::::::::::::::::::::::::::::::::\n[ivy:retrieve]          ::          UNRESOLVED DEPENDENCIES         ::\n[ivy:retrieve]          ::::::::::::::::::::::::::::::::::::::::::::::\n[ivy:retrieve]          :: jline#jline;0.9.94: configuration not found in jline#jline;0.9.94: 'compile'. It was required from org.apache.cassandra#cassandra-all;0.8.4 compile\n[ivy:retrieve]          :: javax.servlet#servlet-api;2.5: configuration not found in javax.servlet#servlet-api;2.5: 'compile'. It was required from org.apache.thrift#libthrift;0.6.1 compile\n[ivy:retrieve]          ::::::::::::::::::::::::::::::::::::::::::::::\n[ivy:retrieve] \n[ivy:retrieve] :: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\nBUILD FAILED\n/opt/local/share/solandra/build.xml:102: impossible to resolve dependencies:\n        resolve failed - see output for details\n. ",
    "xytxytxyt": "Seems to work with the line \nfor %%i in (\"%CASSANDRA_HOME%\\lib*.jar\") do call :append \"%%i\"\nreplaced with \nset CLASSPATH=%CLASSPATH%;\"%CASSANDRA_HOME%\\lib*\"\n. ",
    "smecsia": "That didn't work for me. So I've patched solandra.bat using the lines below.\n:append\nset \"baseFilePath=%1\"\ncall set \"baseFilePath=%%baseFilePath:%CASSANDRA_HOME%=..%%\"\nset CLASSPATH=%CLASSPATH%;%baseFilePath%\ngoto :eof\n. I had the same problem with solandra under cygwin. But I've patched solandra.bat using the lines below and the problem disappeared.\n:append\nset \"baseFilePath=%1\"\ncall set \"baseFilePath=%%baseFilePath:%CASSANDRA_HOME%=..%%\"\nset CLASSPATH=%CLASSPATH%;%baseFilePath%\ngoto :eof\n. ",
    "chaostheory": "I installed a package called util-linux to fix the getopt problem but now there's a new error:\n\n$ Exception in thread \"main\" java.lang.NoClassDefFoundError: solandra/SolandraServer\nCaused by: java.lang.ClassNotFoundException: solandra.SolandraServer\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:202)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:306)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:247)\nCould not find the main class: solandra.SolandraServer.  Program will exit.\n. I'll try - it's been a while since I used Windows hehe\n. \n",
    "hodgsonsam": "Just trying to get Solandra up and running on Debian here and getting the exact same error upon startup, using Cass 1.1.5, tested on ubuntu with Cass 1.1.5 in the same manner and it worked no probs. Any ideas how to set the correct classpath on linux?\nCheers\n. turning debug mode off prevents the error, think this is probably something im doing in the query as i cant replicate the error on basic queries.\n. Solr bug:\nhttps://issues.apache.org/jira/browse/SOLR-4290\n. ",
    "cnauroth": "No problem.  The last commit moves the param into solandra.properties.\nThanks,\n--Chris\n. ",
    "levmatta-umanni": "I cannot contain my self...\nWhy not Cassandra? It is a scalable NOSQL! As I understand, this should be in memory or not exist at all.\nSorry, but the question is legit. \nI will investigate the usage of the caching structure of Solandra (it was a blocker for the adoption of Lucandra some month ago for me).\n. ",
    "mykidong": "I got the IndexSearcher to be able to access the index in cassandra from my spellchecker.\nI have to improve my solandra spellchecker a bit further.\n. ",
    "pcoleman": "No errors in the logs, here are the last two lines:\nINFO 14:12:23,702 [datafiniti] webapp=/solandra path=/select params={fl=name,key&q=city:houston&ids=[us/de/houston/2419williamsvillerd],[us/de/houston/4074williamsvillerd],[us/de/houston/1155pinest],[us/de/houston/77millst],[us/de/houston/1098messobovrd],[us/de/houston/432schoolst],[us/de/houston/234blairspondrd],[us/de/houston/567thistlewoodrd],[us/de/houston/3880williamsvillerd],[us/de/houston/4075gunandrodclubrd]&isShard=true&wt=javabin&version=2} status=0 QTime=1 \nINFO 14:12:23,703 [datafiniti] webapp=/solandra path=/select params={fl=name&wt=javabin&q=city:houston&version=2} status=0 QTime=1781\n. An update on the issue:\nI have downloaded and installed the most recent Solandra build on a test server.\nServer specs:\n8 cores\n32 GB of memory (though we are only allocating 16GB for Solandra)\nUsing the default settings in Solandra Properties, we added in 2,000,000 documents to ensure there were two shards on the same machine. When we query against this data set we are seeing the same results. Solandra is able to find all the matching documents, but still does not return any results. I have copied the logs with Debugging turned on.\nFor a query that has not been cached:\nDEBUG 10:28:55,004 core: df\nDEBUG 10:28:55,005 Adding shard(df): 10.1.10.200:8983/solandra/df~0\nDEBUG 10:28:55,005 Adding shard(df): 10.1.10.200:8983/solandra/df~1\nDEBUG 10:28:55,014 Fetching 0 Docs\n INFO 10:28:55,015 [df] webapp=/solandra path=/select params={fl=key,score&start=0&q=province:az&isShard=true&wt=javabin&fsv=true&rows=10&version=2} hits=0 status=0 QTime=3 \n INFO 10:28:55,821 GC for ParNew: 258 ms, 586012000 reclaimed leaving 2122387984 used; max is 16955473920\nDEBUG 10:28:58,034 Fetching 10 Docs\nDEBUG 10:28:58,035 Going to bulk load 10 documents\nDEBUG 10:28:58,099 Document read took: 63ms\n INFO 10:28:58,099 [df] webapp=/solandra path=/select params={fl=key,score&start=0&q=province:az&isShard=true&wt=javabin&fsv=true&rows=10&version=2} hits=99470 status=0 QTime=3087 \nDEBUG 10:28:58,101 Document read took: 1ms\nDEBUG 10:28:58,102 Document read took: 1ms\nDEBUG 10:28:58,104 Document read took: 1ms\nDEBUG 10:28:58,105 Document read took: 1ms\nDEBUG 10:28:58,107 Document read took: 2ms\nDEBUG 10:28:58,108 Document read took: 1ms\nDEBUG 10:28:58,109 Document read took: 1ms\nDEBUG 10:28:58,110 Document read took: 1ms\nDEBUG 10:28:58,112 Document read took: 1ms\nDEBUG 10:28:58,113 Document read took: 1ms\nDEBUG 10:28:58,118 Fetching 0 Docs\n INFO 10:28:58,118 [df] webapp=/solandra path=/select params={isShard=true&wt=javabin&q=province:az&ids=[us/az/yuma/1152s4thave],[us/az/tempe/208sriverdr],[us/az/mundspark/475pinewoodblvd],[us/az/phoenix/2338wstellaln],[us/az/tucson/3341wwildwooddr],[us/az/surprise/15128wbellrd],[us/az/phoenix/3222egeorgiaave],[us/az/lakehavasucity/2250catamarandr],[us/az/huachucacity/264shuachucablvd],[us/az/tucson/6161sparkave]&version=2} status=0 QTime=1 \n INFO 10:28:58,119 [df] webapp=/solandra path=/select params={wt=javabin&q=province:az&version=2} status=0 QTime=3115\nFor a query that has been cached:\nDEBUG 10:27:36,350 core: df\n INFO 10:27:36,351 ShardInfo for df has expired\n INFO 10:27:36,353 Found reserved shard1(106758077800188110322537822484278066430):178410 TO 180224\nDEBUG 10:27:36,353 Adding shard(df): 10.1.10.200:8983/solandra/df~0\nDEBUG 10:27:36,353 Adding shard(df): 10.1.10.200:8983/solandra/df~1\nDEBUG 10:27:36,359 Fetching 0 Docs\n INFO 10:27:36,360 [df] webapp=/solandra path=/select params={fl=key,score&start=0&q=province:ak&isShard=true&wt=javabin&fsv=true&rows=10&version=2} hits=0 status=0 QTime=2 \nDEBUG 10:27:36,362 Fetching 10 Docs\nDEBUG 10:27:36,363 Found doc in cache\n INFO 10:27:36,363 [df] webapp=/solandra path=/select params={fl=key,score&start=0&q=province:ak&isShard=true&wt=javabin&fsv=true&rows=10&version=2} hits=14707 status=0 QTime=5 \nDEBUG 10:27:36,363 Found doc in cache\nDEBUG 10:27:36,363 Found doc in cache\nDEBUG 10:27:36,363 Found doc in cache\nDEBUG 10:27:36,364 Found doc in cache\nDEBUG 10:27:36,364 Found doc in cache\nDEBUG 10:27:36,364 Found doc in cache\nDEBUG 10:27:36,364 Found doc in cache\nDEBUG 10:27:36,364 Found doc in cache\nDEBUG 10:27:36,365 Found doc in cache\nDEBUG 10:27:36,365 Found doc in cache\nDEBUG 10:27:36,369 Fetching 0 Docs\n INFO 10:27:36,369 [df] webapp=/solandra path=/select params={isShard=true&wt=javabin&q=province:ak&ids=[us/ak/fairbanks/1483ballainerd],[us/ak/anchorage/4451etudorrd],[us/ak/anchorage/600cordovast],[us/ak/anchorage/6048e6thave],[us/ak/anchorage/940tyonekdr],[us/ak/fairbanks/3800universityaves],[us/ak/kenai/47189sherwoodcir],[us/ak/anchorage/12801oldsewardhwy],[us/ak/anchorage/8400raintreecir],[us/ak/juneau/9150skywoodln]&version=2} status=0 QTime=1 \n INFO 10:27:36,370 [df] webapp=/solandra path=/select params={wt=javabin&q=province:ak&version=2} status=0 QTime=20\nIn case it was due to the way we were adding the documents, here is the log for a single write/update:\nINFO 10:33:14,666 {add=[us/ak/anchorage/429industrialway]} 0 8\n INFO 10:33:14,666 [df] webapp=/solandra path=/update params={wt=javabin&version=2} status=0 QTime=8 \nDEBUG 10:33:14,683 update for document 606298\nDEBUG 10:33:14,683 Adding 606298 to df~0\nDEBUG 10:33:14,685 Deleted all terms for: 606298\nDEBUG 10:33:14,685 df~0 - firstTerm: 142390502986727170797762286641249888713\ufffd\ufffd\ufffdkey\nDEBUG 10:33:14,685 df~0 - firstTerm: 142390502986727170797762286641249888713\ufffd\ufffd\ufffdcategory\nDEBUG 10:33:14,686 df~0 - firstTerm: 142390502986727170797762286641249888713\ufffd\ufffd\ufffdcategory\nDEBUG 10:33:14,686 df~0 - firstTerm: 142390502986727170797762286641249888713\ufffd\ufffd\ufffdcategory\nDEBUG 10:33:14,686 df~0 - firstTerm: 142390502986727170797762286641249888713\ufffd\ufffd\ufffdcategory\nDEBUG 10:33:14,687 df~0 - firstTerm: 142390502986727170797762286641249888713\ufffd\ufffd\ufffdcategory\nDEBUG 10:33:14,687 df~0 - firstTerm: 142390502986727170797762286641249888713\ufffd\ufffd\ufffdsource\nDEBUG 10:33:14,687 df~0 - firstTerm: 142390502986727170797762286641249888713\ufffd\ufffd\ufffddateAdded\nDEBUG 10:33:14,687 df~0 - firstTerm: 142390502986727170797762286641249888713\ufffd\ufffd\ufffddateUpdated\nDEBUG 10:33:14,687 df~0 - firstTerm: 142390502986727170797762286641249888713\ufffd\ufffd\ufffdtype\nDEBUG 10:33:14,687 df~0 - firstTerm: 142390502986727170797762286641249888713\ufffd\ufffd\ufffdencoding\nDEBUG 10:33:14,688 df~0 - firstTerm: 142390502986727170797762286641249888713\ufffd\ufffd\ufffdpostalcode\nDEBUG 10:33:14,688 df~0 - firstTerm: 142390502986727170797762286641249888713\ufffd\ufffd\ufffdsic\nDEBUG 10:33:14,688 df~0 - firstTerm: 142390502986727170797762286641249888713\ufffd\ufffd\ufffdphone\nDEBUG 10:33:14,688 df~0 - firstTerm: 142390502986727170797762286641249888713\ufffd\ufffd\ufffdlong\nDEBUG 10:33:14,688 df~0 - firstTerm: 142390502986727170797762286641249888713\ufffd\ufffd\ufffdcity\nDEBUG 10:33:14,688 df~0 - firstTerm: 142390502986727170797762286641249888713\ufffd\ufffd\ufffdcountry\nDEBUG 10:33:14,689 df~0 - firstTerm: 142390502986727170797762286641249888713\ufffd\ufffd\ufffdaddress\nDEBUG 10:33:14,689 df~0 - firstTerm: 142390502986727170797762286641249888713\ufffd\ufffd\ufffdname\nDEBUG 10:33:14,689 df~0 - firstTerm: 142390502986727170797762286641249888713\ufffd\ufffd\ufffdprovince\nDEBUG 10:33:14,689 df~0 - firstTerm: 142390502986727170797762286641249888713\ufffd\ufffd\ufffdlat\nOther than this (frustrating) bug, we really love Solandra. Hopefully you can give us some guidance on where to go from here.\nThanks\n. When we search for an individual id (one of the ones listed in the logs) we get zero results as well. The problem is that we have tried using the same setup with a single shard and have no issue with getting the results back, and I have checked to make sure that we are storing the id/key field.\n. Here is the log for querying against one shard, when there is only 400 documents stored.\nDEBUG 12:52:33,422 core: datafiniti\nDEBUG 12:52:33,425 Flushed cache: datafiniti~0\nDEBUG 12:52:33,458 Document read took: 2ms\nDEBUG 12:52:33,459 Document read took: 1ms\nDEBUG 12:52:33,460 Document read took: 1ms\nDEBUG 12:52:33,461 Document read took: 1ms\nDEBUG 12:52:33,462 Document read took: 1ms\nDEBUG 12:52:33,463 Document read took: 1ms\nDEBUG 12:52:33,464 Document read took: 1ms\nDEBUG 12:52:33,465 Document read took: 1ms\nDEBUG 12:52:33,466 Document read took: 1ms\nDEBUG 12:52:33,466 Document read took: 0ms\nDEBUG 12:52:33,467 Fetching 10 Docs\nDEBUG 12:52:33,468 Found doc in cache\n INFO 12:52:33,468 [datafiniti] webapp=/solandra path=/select params={wt=javabin&q=province:ak&version=2} hits=388 status=0 QTime=91 \nDEBUG 12:52:33,469 Found doc in cache\nDEBUG 12:52:33,470 Found doc in cache\nDEBUG 12:52:33,472 Found doc in cache\nDEBUG 12:52:33,473 Found doc in cache\nDEBUG 12:52:33,473 Found doc in cache\nDEBUG 12:52:33,473 Found doc in cache\nDEBUG 12:52:33,473 Found doc in cache\nDEBUG 12:52:33,474 Found doc in cache\nDEBUG 12:52:33,474 Found doc in cache\nDEBUG 12:52:33,474 Found doc in cache\n. We have tried this with only 400 documents with a shard size of 256 and reserved index of 16. The same issue was present. I am running the reuters demo right now. I will update once it finishes. \n. Here is my schema:\n```\n <?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n            <fieldType class=\"solr.StrField\" name=\"string\"/>\n            <fieldType class=\"solr.SortableIntField\" name=\"sint\" omitNorms=\"true\"/>\n            <fieldType class=\"solr.IntField\" name=\"int\" omitNorms=\"true\"/>\n            <fieldType class=\"solr.DoubleField\" name=\"double\" omitNorms=\"true\"/>\n    </types>\n    <fields>\n            <field indexed=\"true\" name=\"key\" omitTermFreqAndPositions=\"true\" stored=\"true\" type=\"string\"/>\n            <field indexed=\"true\" name=\"long\" omitTermFreqAndPositions=\"true\" stored=\"true\" type=\"sdouble\"/>\n            <field indexed=\"true\" name=\"lat\" omitTermFreqAndPositions=\"true\" stored=\"true\" type=\"sdouble\"/>\n            <field indexed=\"true\" name=\"city\" omitTermFreqAndPositions=\"true\" stored=\"true\" type=\"string\"/>\n            <field indexed=\"true\" name=\"province\" omitTermFreqAndPositions=\"true\" stored=\"true\" type=\"string\"/>\n            <field indexed=\"true\" name=\"country\" omitTermFreqAndPositions=\"true\" stored=\"true\" type=\"string\"/>\n            <field indexed=\"true\" name=\"secondary\" omitTermFreqAndPositions=\"true\" stored=\"true\" type=\"string\"/>\n            <field indexed=\"true\" name=\"address\" omitTermFreqAndPositions=\"true\" stored=\"true\" type=\"icu_text\"/>\n            <field indexed=\"true\" name=\"name\" omitTermFreqAndPositions=\"true\" stored=\"true\" type=\"icu_text\"/>\n            <field indexed=\"true\" name=\"organization\" stored=\"true\" type=\"icu_text\"/>\n            <field indexed=\"true\" name=\"encoding\" multiValued=\"false\" omittermfreqandpositions=\"true\" stored=\"true\" type=\"string\"/>\n            <field indexed=\"true\" name=\"type\" omitTermFreqAndPositions=\"true\" stored=\"true\" type=\"string\"/>\n            <field indexed=\"true\" name=\"subtype\" omitTermFreqAndPositions=\"true\" stored=\"true\" type=\"string\"/>\n            <field indexed=\"true\" name=\"website\" omitTermFreqAndPositions=\"true\" stored=\"true\" type=\"string\"/>\n            <field indexed=\"true\" name=\"phone\" omitTermFreqAndPositions=\"true\" stored=\"true\" type=\"string\"/>\n            <field indexed=\"true\" name=\"postalcode\" omitTermFreqAndPositions=\"true\" stored=\"true\" type=\"sint\"/>\n            <field indexed=\"true\" name=\"source\" omitTermFreqAndPositions=\"true\" stored=\"true\" type=\"string\"/>\n            <field indexed=\"true\" name=\"dateAdded\" omitTermFreqAndPositions=\"true\" stored=\"true\" type=\"string\"/>\n            <field indexed=\"true\" name=\"dateUpdated\" omitTermFreqAndPositions=\"true\" stored=\"true\" type=\"string\"/>\n            <field indexed=\"true\" multiValued=\"true\" name=\"category\" omitTermFreqAndPositions=\"true\" stored=\"true\" type=\"icu_text\"/>\n            <field indexed=\"true\" multiValued=\"true\" name=\"sic\" omitTermFreqAndPositions=\"true\" stored=\"true\" type=\"string\"/>\n    </fields>\n    <uniqueKey>key</uniqueKey>\n    <defaultSearchField>category</defaultSearchField>\n\n\n```\n. The Reuters demo worked, so it is most likely something to do with the schema. I have tried removing omitTermFreqAnd Positions, but it didn't affect the results. Could it be the ICUTokenizer, I know its relatively new to Solr.\n. Thanks for the help!\nI was able to get this working using the reuters schema as a template. I am going to close the issue, but I am going to keep looking over the old schema to try to find what exactly caused the issue. If I find it I will update the ticket.\n. I noticed they were commented out, I wasn't sure if that was the case. I guess I will have to optimize another way.\nThanks\n. Forgot to include this in the first post. In the jetty xml.\nmaxFormContentSize = 10000000\n. Ended up using the -Dorg.mortbay.jetty.Request.maxFormContentSize=10000000 property flag. Fixed the issue\n. ",
    "VanitySoft": "for some reason this happens within my Eclipse, but solandra starts up ok via command line, so I am closing.\n. ",
    "blinder": "further clarifications, using the 0.7 branch of solandra\n. uname -a\n2.6.18-194.17.1.el5xen #1 SMP Wed Sep 29 13:30:21 EDT 2010 x86_64 x86_64 x86_64 GNU/Linux\n. ",
    "gt9000": "I had the same issue and discovered that I was using Java 1.7. 1.6 is required.\n. ",
    "mcmire": "I got that too... I got it when I did a git pull in an existing clone of solandra and re-ran ant. But when I wiped the repo and re-cloned it seemed to work fine. What about you?\n. ",
    "benmccann": "This issue can be closed.\n. Here are the changes necessary to get it to compile with 1.1\nhttps://github.com/tjake/Solandra/pull/169\n. It looks like updating LucandraFieldCache is the only slightly tricky part of the upgrade.\n. It would be cool to have a git branch with support for 1.1.0-beta2 (which is now available in the Maven central repo: http://search.maven.org/#browse%7C-1601434515)\n. Here's the code needed to compile with 1.1.0-beta2:\nhttps://github.com/tjake/Solandra/pull/169\n. It compiles fine for me with 1.8.2.\n. I'm a bit curious about this as well.  From what I gather DSE uses native solr indexes.  I'd be interested to hear what the pros and cons of the two approaches are.  I'm a bit worried about Solandra's future given that Datastax has said \"At this time, we now consider the current versions of Brisk and Solandra to be the final releases from us in open source form.\"\n. That statement was on the following page:\nhttp://www.datastax.com/2011/09/committing-hive-driver-into-apache-cassandra\n. Thanks for the update!\n. I'm not Jason, but I'll throw out where I am in my search.  I think I'm narrowing down to DSE or elasticsearch.  The big plus for DSE is its Cassandra integration.  However, right now I'm probably leaning towards elasticsearch because it seems easier to deal with nested JSON docs using it.  Still want to do some more prototyping and investigation before I make any decisions though.  I'm less comfortable with elasticsearch's durability and backup story right now just because I know less about it, so I have some more reading to do.\n. I don't think this runs correctly, but at least it is a start and will save some time on the upgrade hopefully.\n. Btw, I noticed the ivy file uses Lucene 3.3.0 and Solr 3.4.0.  We should probably bump the Lucene version to match (or maybe remove it if it's already pulled in by Solr)\n. I've submitted a few follow-on changes and this seems to be working now.  I'm pretty new to Solandra though, so there could be things I didn't test.\n. There's a bug open in the Cassandra JIRA for adding the snaptree and yammer metrics jars to the Cassandra pom, so we won't need to declare them here\nhttps://issues.apache.org/jira/browse/CASSANDRA-3676\n. ",
    "efalcao": "I ran into the same issue and made the same changes above. I was able to compile. Haven't been able to run tests yet either.\n. I've been debugging this today and it appears that something is wrong in the write path:\nWith a simple doc, hashing disabled, I get 10 rows in TI and 5 rows in SI.\nSame doc, hashing enabled, I get 1 row in TI and 4 rows in SI.\nStill trying to understand the root cause.\n. ",
    "huyle": "Hi Jake,\nTests failed.  Here is output from ant test:\n[junit]  INFO 20:08:05,798 [] Registered new searcher Searcher@20fc40ae main\n[junit]  INFO 20:08:05,798 user.dir=/u01/app/solandra/tjake-Solandra-462c13a\n[junit]  INFO 20:08:05,798 SolrDispatchFilter.init() done\n[junit]  INFO 20:08:05,801 Started SocketConnector@0.0.0.0:8983\n[junit]  INFO 20:08:06,035 [] webapp=/solandra path=/select params={q={!raw+f%3Djunit_test_query}ping} hits=0 status=0 QTime=126 \n[junit]  INFO 20:08:06,258 [] webapp=/solandra path=/select params={q={!raw+f%3Djunit_test_query}ping} hits=0 status=0 QTime=1 \n[junit]  INFO 20:08:06,259 [] webapp=/solandra path=/select params={q={!raw+f%3Djunit_test_query}ping} hits=0 status=0 QTime=0 \n[junit]  INFO 20:08:06,261 [] webapp=/solandra path=/select params={q={!raw+f%3Djunit_test_query}ping} hits=0 status=0 QTime=1 \n[junit]  INFO 20:08:06,262 [] webapp=/solandra path=/select params={q={!raw+f%3Djunit_test_query}ping} hits=0 status=0 QTime=0 \n[junit]  INFO 20:08:06,272 [] webapp=/solandra path=/select params={q={!raw+f%3Djunit_test_query}ping} hits=0 status=0 QTime=1 \n[junit]  INFO 20:08:06,274 [] webapp=/solandra path=/select params={q={!raw+f%3Djunit_test_query}ping} hits=0 status=0 QTime=1 \n[junit]  INFO 20:08:06,276 [] webapp=/solandra path=/select params={q={!raw+f%3Djunit_test_query}ping} hits=0 status=0 QTime=1 \n[junit]  INFO 20:08:06,285 [] webapp=/solandra path=/select params={q={!raw+f%3Djunit_test_query}ping} hits=0 status=0 QTime=1 \n[junit]  INFO 20:08:06,287 [] webapp=/solandra path=/select params={q={!raw+f%3Djunit_test_query}ping} hits=0 status=0 QTime=1 \n[junit]  INFO 20:08:06,288 [] webapp=/solandra path=/select params={q={!raw+f%3Djunit_test_query}ping} hits=0 status=0 QTime=0 \n[junit]  INFO 20:08:06,293 [] webapp=/solandra path=/select params={q={!raw+f%3Djunit_test_query}ping} hits=0 status=0 QTime=0 \n[junit]  INFO 20:08:06,295 [] webapp=/solandra path=/select params={q={!raw+f%3Djunit_test_query}ping} hits=0 status=0 QTime=1 \n[junit]  INFO 20:08:06,296 [] webapp=/solandra path=/select params={q={!raw+f%3Djunit_test_query}ping} hits=0 status=0 QTime=0 \n[junit]  INFO 20:08:06,298 [] webapp=/solandra path=/select params={q={!raw+f%3Djunit_test_query}ping} hits=0 status=0 QTime=1 \n[junit]  INFO 20:08:06,300 [] webapp=/solandra path=/select params={q={!raw+f%3Djunit_test_query}ping} hits=0 status=0 QTime=1 \n[junit]  INFO 20:08:06,302 [] webapp=/solandra path=/select params={q={!raw+f%3Djunit_test_query}ping} hits=0 status=0 QTime=1 \n[junit]  INFO 20:08:06,305 [] webapp=/solandra path=/select params={q={!raw+f%3Djunit_test_query}ping} hits=0 status=0 QTime=1 \n[junit]  INFO 20:08:06,308 [] webapp=/solandra path=/select params={q={!raw+f%3Djunit_test_query}ping} hits=0 status=0 QTime=1 \n[junit]  INFO 20:08:06,315 Running tests for core: 101961002530780\n[junit]  INFO 20:08:06,328 Wrote Schema for 101961002530780/schema.xml\n[junit]  INFO 20:08:06,330 row was marked empty: [Row(key=DecoratedKey(136947913395583345484838255810484242500, 3130313936313030323533303738302f736368656d61), cf=null)]\n[junit] java.io.FileNotFoundException: http://localhost:8983/solandra/schema/101961002530780/schema.xml\n[junit]     at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1434)\n[junit]     at java.net.URL.openStream(URL.java:1010)\n[junit]     at solandra.SolandraTestRunner.addSchema(SolandraTestRunner.java:132)\n[junit]     at solandra.SolandraTests.allTestsManyIndexes(SolandraTests.java:91)\n[junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n[junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n[junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n[junit]     at java.lang.reflect.Method.invoke(Method.java:597)\n[junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)\n[junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\n[junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)\n[junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\n[junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n[junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n[junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n[junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:44)\n[junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:180)\n[junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:41)\n[junit]     at org.junit.runners.ParentRunner$1.evaluate(ParentRunner.java:173)\n[junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n[junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n[junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:220)\n[junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)\n[junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:422)\n[junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:931)\n[junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:785)\n[junit]  INFO 20:08:06,388 Wrote Schema for 101961002592413/schema.xml\n[junit]  INFO 20:08:06,390 row was marked empty: [Row(key=DecoratedKey(151362468384239468700862044604825807141, 3130313936313030323539323431332f736368656d61), cf=null)]\n[junit] java.io.FileNotFoundException: http://localhost:8983/solandra/schema/101961002592413/schema.xml\n[junit]     at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1434)\n[junit]     at java.net.URL.openStream(URL.java:1010)\n[junit]     at solandra.SolandraTests.setAddOtherSchema(SolandraTests.java:403)\n[junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n[junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n[junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n[junit]     at java.lang.reflect.Method.invoke(Method.java:597)\n[junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)\n[junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\n[junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)\n[junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\n[junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n[junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n[junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n[junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:44)\n[junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:180)\n[junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:41)\n[junit]     at org.junit.runners.ParentRunner$1.evaluate(ParentRunner.java:173)\n[junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n[junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n[junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:220)\n[junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)\n[junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:422)\n[junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:931)\n[junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:785)\n[junit]  INFO 20:08:06,417 row was marked empty: [Row(key=DecoratedKey(9551188407454253498442306096745496329, 313031393631303032353932343133), cf=null)]\n[junit] ERROR 20:08:06,418 java.lang.RuntimeException: java.io.IOException: invalid core '101961002592413'\n[junit]     at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:104)\n[junit]     at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:194)\n[junit]     at solandra.SolandraDispatchFilter.doFilter(SolandraDispatchFilter.java:137)\n[junit]     at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n[junit]     at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n[junit]     at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n[junit]     at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n[junit]     at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n[junit]     at org.mortbay.jetty.Server.handle(Server.java:326)\n[junit]     at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n[junit]     at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:945)\n[junit]     at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:756)\n[junit]     at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)\n[junit]     at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n[junit]     at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n[junit]     at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n[junit] Caused by: java.io.IOException: invalid core '101961002592413'\n[junit]     at org.apache.solr.core.SolandraCoreContainer.readSchema(SolandraCoreContainer.java:157)\n[junit]     at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:100)\n[junit]     ... 15 more\n[junit] \n[junit]  INFO 20:08:06,490 row was marked empty: [Row(key=DecoratedKey(9551188407454253498442306096745496329, 313031393631303032353932343133), cf=null)]\n[junit] ERROR 20:08:06,490 java.lang.RuntimeException: java.io.IOException: invalid core '101961002592413'\n[junit]     at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:104)\n[junit]     at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:194)\n[junit]     at solandra.SolandraDispatchFilter.doFilter(SolandraDispatchFilter.java:137)\n[junit]     at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n[junit]     at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n[junit]     at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n[junit]     at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n[junit]     at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n[junit]     at org.mortbay.jetty.Server.handle(Server.java:326)\n[junit]     at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n[junit]     at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n[junit]     at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n[junit]     at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n[junit]     at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n[junit]     at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n[junit]     at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n[junit] Caused by: java.io.IOException: invalid core '101961002592413'\n[junit]     at org.apache.solr.core.SolandraCoreContainer.readSchema(SolandraCoreContainer.java:157)\n[junit]     at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:100)\n[junit]     ... 15 more\n[junit] \n[junit]  INFO 20:08:06,563 row was marked empty: [Row(key=DecoratedKey(9551188407454253498442306096745496329, 313031393631303032353932343133), cf=null)]\n[junit] ERROR 20:08:06,564 java.lang.RuntimeException: java.io.IOException: invalid core '101961002592413'\n[junit]     at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:104)\n[junit]     at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:194)\n[junit]     at solandra.SolandraDispatchFilter.doFilter(SolandraDispatchFilter.java:137)\n[junit]     at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n[junit]     at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n[junit]     at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n[junit]     at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n[junit]     at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n[junit]     at org.mortbay.jetty.Server.handle(Server.java:326)\n[junit]     at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n[junit]     at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:945)\n[junit]     at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:756)\n[junit]     at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)\n[junit]     at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n[junit]     at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n[junit]     at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n[junit] Caused by: java.io.IOException: invalid core '101961002592413'\n[junit]     at org.apache.solr.core.SolandraCoreContainer.readSchema(SolandraCoreContainer.java:157)\n[junit]     at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:100)\n[junit]     ... 15 more\n[junit] \n[junit]  INFO 20:08:06,575 row was marked empty: [Row(key=DecoratedKey(9551188407454253498442306096745496329, 313031393631303032353932343133), cf=null)]\n[junit] ERROR 20:08:06,576 java.lang.RuntimeException: java.io.IOException: invalid core '101961002592413'\n[junit]     at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:104)\n[junit]     at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:194)\n[junit]     at solandra.SolandraDispatchFilter.doFilter(SolandraDispatchFilter.java:137)\n[junit]     at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n[junit]     at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n[junit]     at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n[junit]     at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n[junit]     at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n[junit]     at org.mortbay.jetty.Server.handle(Server.java:326)\n[junit]     at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n[junit]     at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n[junit]     at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n[junit]     at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n[junit]     at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n[junit]     at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n[junit]     at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n[junit] Caused by: java.io.IOException: invalid core '101961002592413'\n[junit]     at org.apache.solr.core.SolandraCoreContainer.readSchema(SolandraCoreContainer.java:157)\n[junit]     at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:100)\n[junit]     ... 15 more\n[junit] \n[junit]  INFO 20:08:06,591 row was marked empty: [Row(key=DecoratedKey(9551188407454253498442306096745496329, 313031393631303032353932343133), cf=null)]\n[junit] ERROR 20:08:06,592 java.lang.RuntimeException: java.io.IOException: invalid core '101961002592413'\n[junit]     at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:104)\n[junit]     at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:194)\n[junit]     at solandra.SolandraDispatchFilter.doFilter(SolandraDispatchFilter.java:137)\n[junit]     at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n[junit]     at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n[junit]     at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n[junit]     at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n[junit]     at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n[junit]     at org.mortbay.jetty.Server.handle(Server.java:326)\n[junit]     at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n[junit]     at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n[junit]     at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n[junit]     at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n[junit]     at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n[junit]     at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n[junit]     at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n[junit] Caused by: java.io.IOException: invalid core '101961002592413'\n[junit]     at org.apache.solr.core.SolandraCoreContainer.readSchema(SolandraCoreContainer.java:157)\n[junit]     at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:100)\n[junit]     ... 15 more\n[junit] \n[junit]  INFO 20:08:06,598 row was marked empty: [Row(key=DecoratedKey(9551188407454253498442306096745496329, 313031393631303032353932343133), cf=null)]\n[junit] ERROR 20:08:06,599 java.lang.RuntimeException: java.io.IOException: invalid core '101961002592413'\n[junit]     at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:104)\n[junit]     at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:194)\n[junit]     at solandra.SolandraDispatchFilter.doFilter(SolandraDispatchFilter.java:137)\n[junit]     at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n[junit]     at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n[junit]     at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n[junit]     at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n[junit]     at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n[junit]     at org.mortbay.jetty.Server.handle(Server.java:326)\n[junit]     at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n[junit]     at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n[junit]     at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n[junit]     at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n[junit]     at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n[junit]     at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n[junit]     at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n[junit] Caused by: java.io.IOException: invalid core '101961002592413'\n[junit]     at org.apache.solr.core.SolandraCoreContainer.readSchema(SolandraCoreContainer.java:157)\n[junit]     at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:100)\n[junit]     ... 15 more\n[junit] \n[junit]  INFO 20:08:06,617 row was marked empty: [Row(key=DecoratedKey(9551188407454253498442306096745496329, 313031393631303032353932343133), cf=null)]\n[junit] ERROR 20:08:06,618 java.lang.RuntimeException: java.io.IOException: invalid core '101961002592413'\n[junit]     at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:104)\n[junit]     at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:194)\n[junit]     at solandra.SolandraDispatchFilter.doFilter(SolandraDispatchFilter.java:137)\n[junit]     at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n[junit]     at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n[junit]     at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n[junit]     at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n[junit]     at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n[junit]     at org.mortbay.jetty.Server.handle(Server.java:326)\n[junit]     at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n[junit]     at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:945)\n[junit]     at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:756)\n[junit]     at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)\n[junit]     at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n[junit]     at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n[junit]     at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n[junit] Caused by: java.io.IOException: invalid core '101961002592413'\n[junit]     at org.apache.solr.core.SolandraCoreContainer.readSchema(SolandraCoreContainer.java:157)\n[junit]     at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:100)\n[junit]     ... 15 more\n[junit] \n[junit]  INFO 20:08:06,628 row was marked empty: [Row(key=DecoratedKey(9551188407454253498442306096745496329, 313031393631303032353932343133), cf=null)]\n[junit] ERROR 20:08:06,628 java.lang.RuntimeException: java.io.IOException: invalid core '101961002592413'\n[junit]     at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:104)\n[junit]     at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:194)\n[junit]     at solandra.SolandraDispatchFilter.doFilter(SolandraDispatchFilter.java:137)\n[junit]     at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n[junit]     at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n[junit]     at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n[junit]     at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n[junit]     at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n[junit]     at org.mortbay.jetty.Server.handle(Server.java:326)\n[junit]     at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n[junit]     at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:945)\n[junit]     at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:843)\n[junit]     at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n[junit]     at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n[junit]     at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n[junit]     at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n[junit] Caused by: java.io.IOException: invalid core '101961002592413'\n[junit]     at org.apache.solr.core.SolandraCoreContainer.readSchema(SolandraCoreContainer.java:157)\n[junit]     at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:100)\n[junit]     ... 15 more\n[junit] \n[junit]  INFO 20:08:06,634 row was marked empty: [Row(key=DecoratedKey(9551188407454253498442306096745496329, 313031393631303032353932343133), cf=null)]\n[junit] ERROR 20:08:06,635 java.lang.RuntimeException: java.io.IOException: invalid core '101961002592413'\n[junit]     at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:104)\n[junit]     at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:194)\n[junit]     at solandra.SolandraDispatchFilter.doFilter(SolandraDispatchFilter.java:137)\n[junit]     at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n[junit]     at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n[junit]     at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n[junit]     at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n[junit]     at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n[junit]     at org.mortbay.jetty.Server.handle(Server.java:326)\n[junit]     at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n[junit]     at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:945)\n[junit]     at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:756)\n[junit]     at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)\n[junit]     at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n[junit]     at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n[junit]     at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n[junit] Caused by: java.io.IOException: invalid core '101961002592413'\n[junit]     at org.apache.solr.core.SolandraCoreContainer.readSchema(SolandraCoreContainer.java:157)\n[junit]     at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:100)\n[junit]     ... 15 more\n[junit] \n[junit]  INFO 20:08:06,639 row was marked empty: [Row(key=DecoratedKey(9551188407454253498442306096745496329, 313031393631303032353932343133), cf=null)]\n[junit] ERROR 20:08:06,640 java.lang.RuntimeException: java.io.IOException: invalid core '101961002592413'\n[junit]     at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:104)\n[junit]     at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:194)\n[junit]     at solandra.SolandraDispatchFilter.doFilter(SolandraDispatchFilter.java:137)\n[junit]     at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n[junit]     at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n[junit]     at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n[junit]     at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n[junit]     at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n[junit]     at org.mortbay.jetty.Server.handle(Server.java:326)\n[junit]     at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n[junit]     at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n[junit]     at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n[junit]     at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n[junit]     at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n[junit]     at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n[junit]     at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n[junit] Caused by: java.io.IOException: invalid core '101961002592413'\n[junit]     at org.apache.solr.core.SolandraCoreContainer.readSchema(SolandraCoreContainer.java:157)\n[junit]     at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:100)\n[junit]     ... 15 more\n[junit] \n[junit]  INFO 20:08:06,660 row was marked empty: [Row(key=DecoratedKey(9551188407454253498442306096745496329, 313031393631303032353932343133), cf=null)]\n[junit] ERROR 20:08:06,661 java.lang.RuntimeException: java.io.IOException: invalid core '101961002592413'\n[junit]     at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:104)\n[junit]     at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:194)\n[junit]     at solandra.SolandraDispatchFilter.doFilter(SolandraDispatchFilter.java:137)\n[junit]     at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n[junit]     at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n[junit]     at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n[junit]     at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n[junit]     at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n[junit]     at org.mortbay.jetty.Server.handle(Server.java:326)\n[junit]     at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n[junit]     at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n[junit]     at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n[junit]     at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n[junit]     at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n[junit]     at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n[junit]     at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n[junit] Caused by: java.io.IOException: invalid core '101961002592413'\n[junit]     at org.apache.solr.core.SolandraCoreContainer.readSchema(SolandraCoreContainer.java:157)\n[junit]     at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:100)\n[junit]     ... 15 more\n[junit] \n[junit]  INFO 20:08:06,666 row was marked empty: [Row(key=DecoratedKey(9551188407454253498442306096745496329, 313031393631303032353932343133), cf=null)]\n[junit] ERROR 20:08:06,667 java.lang.RuntimeException: java.io.IOException: invalid core '101961002592413'\n[junit]     at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:104)\n[junit]     at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:194)\n[junit]     at solandra.SolandraDispatchFilter.doFilter(SolandraDispatchFilter.java:137)\n[junit]     at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n[junit]     at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n[junit]     at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n[junit]     at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n[junit]     at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n[junit]     at org.mortbay.jetty.Server.handle(Server.java:326)\n[junit]     at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n[junit]     at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:945)\n[junit]     at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:756)\n[junit]     at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)\n[junit]     at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n[junit]     at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n[junit]     at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n[junit] Caused by: java.io.IOException: invalid core '101961002592413'\n[junit]     at org.apache.solr.core.SolandraCoreContainer.readSchema(SolandraCoreContainer.java:157)\n[junit]     at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:100)\n[junit]     ... 15 more\n[junit] \n[junit] ------------- ---------------- ---------------\n[junit] Testcase: allTestsManyIndexes(solandra.SolandraTests):  FAILED\n[junit] \n[junit] junit.framework.AssertionFailedError: \n[junit]     at solandra.SolandraTestRunner.addSchema(SolandraTestRunner.java:155)\n[junit]     at solandra.SolandraTests.allTestsManyIndexes(SolandraTests.java:91)\n[junit] \n[junit] \n[junit] Testcase: setAddOtherSchema(solandra.SolandraTests):    FAILED\n[junit] \n[junit] junit.framework.AssertionFailedError: \n[junit]     at solandra.SolandraTests.setAddOtherSchema(SolandraTests.java:434)\n[junit] \n[junit] \n[junit] Testcase: testOneOtherDocument(solandra.SolandraTests): FAILED\n[junit] expected:<200> but was:<500>\n[junit] junit.framework.AssertionFailedError: expected:<200> but was:<500>\n[junit]     at solandra.SolandraTests.testOneOtherDocument(SolandraTests.java:459)\n[junit] \n[junit] \n[junit] Testcase: testAllUUIDSearch(solandra.SolandraTests):    Caused an ERROR\n[junit] Error executing query\n[junit] org.apache.solr.client.solrj.SolrServerException: Error executing query\n[junit]     at org.apache.solr.client.solrj.request.QueryRequest.process(QueryRequest.java:95)\n[junit]     at org.apache.solr.client.solrj.SolrServer.query(SolrServer.java:118)\n[junit]     at solandra.SolandraTests.testAllUUIDSearch(SolandraTests.java:480)\n[junit] \n[junit] \n[junit] request: http://localhost:8983/solandra/101961002592413/select?q=uuid:[* TO *]&wt=javabin&version=2\n[junit]     at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:430)\n[junit]     at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:244)\n[junit]     at org.apache.solr.client.solrj.request.QueryRequest.process(QueryRequest.java:89)\n[junit] \n[junit] \n[junit] Testcase: testSomeOtherDocuments(solandra.SolandraTests):   FAILED\n[junit] expected:<200> but was:<500>\n[junit] junit.framework.AssertionFailedError: expected:<200> but was:<500>\n[junit]     at solandra.SolandraTests.writeDocs(SolandraTests.java:794)\n[junit]     at solandra.SolandraTests.testSomeOtherDocuments(SolandraTests.java:545)\n[junit] \n[junit] \n[junit] Testcase: testAllUUIDSearchAgain(solandra.SolandraTests):   Caused an ERROR\n[junit] Error executing query\n[junit] org.apache.solr.client.solrj.SolrServerException: Error executing query\n[junit]     at org.apache.solr.client.solrj.request.QueryRequest.process(QueryRequest.java:95)\n[junit]     at org.apache.solr.client.solrj.SolrServer.query(SolrServer.java:118)\n[junit]     at solandra.SolandraTests.testAllUUIDSearchAgain(SolandraTests.java:566)\n[junit] \n[junit] \n[junit] request: http://localhost:8983/solandra/101961002592413/select?q=uuid:[* TO *]&wt=javabin&version=2\n[junit]     at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:430)\n[junit]     at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:244)\n[junit]     at org.apache.solr.client.solrj.request.QueryRequest.process(QueryRequest.java:89)\n[junit] \n[junit] \n[junit] Testcase: testAllMessageTypeSearch(solandra.SolandraTests): Caused an ERROR\n[junit] Error executing query\n[junit] org.apache.solr.client.solrj.SolrServerException: Error executing query\n[junit]     at org.apache.solr.client.solrj.request.QueryRequest.process(QueryRequest.java:95)\n[junit]     at org.apache.solr.client.solrj.SolrServer.query(SolrServer.java:118)\n[junit]     at solandra.SolandraTests.testAllMessageTypeSearch(SolandraTests.java:576)\n[junit] \n[junit] \n[junit] request: http://localhost:8983/solandra/101961002592413/select?q=messageType:[* TO *]&wt=javabin&version=2\n[junit]     at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:430)\n[junit]     at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:244)\n[junit]     at org.apache.solr.client.solrj.request.QueryRequest.process(QueryRequest.java:89)\n[junit] \n[junit] \n[junit] Testcase: testAllMessageTypeTransportSearch(solandra.SolandraTests):    Caused an ERROR\n[junit] Error executing query\n[junit] org.apache.solr.client.solrj.SolrServerException: Error executing query\n[junit]     at org.apache.solr.client.solrj.request.QueryRequest.process(QueryRequest.java:95)\n[junit]     at org.apache.solr.client.solrj.SolrServer.query(SolrServer.java:118)\n[junit]     at solandra.SolandraTests.testAllMessageTypeTransportSearch(SolandraTests.java:586)\n[junit] \n[junit] \n[junit] request: http://localhost:8983/solandra/101961002592413/select?q=messageType:Transport&wt=javabin&version=2\n[junit]     at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:430)\n[junit]     at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:244)\n[junit]     at org.apache.solr.client.solrj.request.QueryRequest.process(QueryRequest.java:89)\n[junit] \n[junit] \n[junit] Testcase: testAddMoreOtherDocuments(solandra.SolandraTests):    FAILED\n[junit] expected:<200> but was:<500>\n[junit] junit.framework.AssertionFailedError: expected:<200> but was:<500>\n[junit]     at solandra.SolandraTests.writeDocs(SolandraTests.java:794)\n[junit]     at solandra.SolandraTests.testAddMoreOtherDocuments(SolandraTests.java:600)\n[junit] \n[junit] \n[junit] Testcase: testClearAllRecords(solandra.SolandraTests):  Caused an ERROR\n[junit] java.io.IOException: invalid core '101961002592413'  java.lang.RuntimeException: java.io.IOException: invalid core '101961002592413'    at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:104)   at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:194)     at solandra.SolandraDispatchFilter.doFilter(SolandraDispatchFilter.java:137)    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)  at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)     at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)     at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)     at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)     at org.mortbay.jetty.Server.handle(Server.java:326)     at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)  at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:945)     at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:843)  at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)     at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)     at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)   at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582) Caused by: java.io.IOException: invalid core '101961002592413'     at org.apache.solr.core.SolandraCoreContainer.readSchema(SolandraCoreContainer.java:157)    at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:100)   ... 15 more\n[junit] \n[junit] java.io.IOException: invalid core '101961002592413'  java.lang.RuntimeException: java.io.IOException: invalid core '101961002592413'    at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:104)   at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:194)     at solandra.SolandraDispatchFilter.doFilter(SolandraDispatchFilter.java:137)    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)  at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)     at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)     at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)     at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)     at org.mortbay.jetty.Server.handle(Server.java:326)     at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)  at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:945)     at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:843)  at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)     at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)     at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)   at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582) Caused by: java.io.IOException: invalid core '101961002592413'     at org.apache.solr.core.SolandraCoreContainer.readSchema(SolandraCoreContainer.java:157)    at org.apache.solr.core.SolandraCoreContainer.getCore(SolandraCoreContainer.java:100)   ... 15 more\n[junit] \n[junit] request: http://localhost:8983/solandra/101961002592413/update?wt=javabin&version=2\n[junit] \n[junit] \n[junit] request: http://localhost:8983/solandra/101961002592413/update?wt=javabin&version=2\n[junit]     at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:430)\n[junit]     at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:244)\n[junit]     at org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:105)\n[junit]     at org.apache.solr.client.solrj.SolrServer.deleteById(SolrServer.java:102)\n[junit]     at solandra.SolandraTests.testClearAllRecords(SolandraTests.java:653)\n[junit] \n[junit] \n[junit] Testcase: testAddMoreDocuments(solandra.SolandraTests): FAILED\n[junit] expected:<200> but was:<500>\n[junit] junit.framework.AssertionFailedError: expected:<200> but was:<500>\n[junit]     at solandra.SolandraTests.writeDocs(SolandraTests.java:794)\n[junit]     at solandra.SolandraTests.testAddMoreDocuments(SolandraTests.java:761)\n[junit] \n[junit] \n[junit] Testcase: testSingleSearchQuery(solandra.SolandraTests):    Caused an ERROR\n[junit] Error executing query\n[junit] org.apache.solr.client.solrj.SolrServerException: Error executing query\n[junit]     at org.apache.solr.client.solrj.request.QueryRequest.process(QueryRequest.java:95)\n[junit]     at org.apache.solr.client.solrj.SolrServer.query(SolrServer.java:118)\n[junit]     at solandra.SolandraTests.testSingleSearchQuery(SolandraTests.java:810)\n[junit] \n[junit] \n[junit] request: http://localhost:8983/solandra/101961002592413/select?q=key:stop&wt=javabin&version=2\n[junit]     at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:430)\n[junit]     at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:244)\n[junit]     at org.apache.solr.client.solrj.request.QueryRequest.process(QueryRequest.java:89)\n[junit] \n[junit] \n[junit] Testcase: testDualSearchQuery(solandra.SolandraTests):  Caused an ERROR\n[junit] Error executing query\n[junit] org.apache.solr.client.solrj.SolrServerException: Error executing query\n[junit]     at org.apache.solr.client.solrj.request.QueryRequest.process(QueryRequest.java:95)\n[junit]     at org.apache.solr.client.solrj.SolrServer.query(SolrServer.java:118)\n[junit]     at solandra.SolandraTests.testDualSearchQuery(SolandraTests.java:826)\n[junit] \n[junit] \n[junit] request: http://localhost:8983/solandra/101961002592413/select?q=messageType:InstructionDef AND key:stop&wt=javabin&version=2\n[junit]     at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:430)\n[junit]     at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:244)\n[junit]     at org.apache.solr.client.solrj.request.QueryRequest.process(QueryRequest.java:89)\n[junit] \n[junit] \n[junit] Testcase: testLongRangeQueries(solandra.SolandraTests): FAILED\n[junit] expected:<200> but was:<500>\n[junit] junit.framework.AssertionFailedError: expected:<200> but was:<500>\n[junit]     at solandra.SolandraTests.writeDocs(SolandraTests.java:794)\n[junit]     at solandra.SolandraTests.testLongRangeQueries(SolandraTests.java:843)\n[junit] \n[junit] \n[junit] Test solandra.SolandraTests FAILED\nBUILD FAILED\n/u01/app/solandra/tjake-Solandra-462c13a/build.xml:171: Some test(s) failed.\n. I used embed cassandra.  So I ran \"ant test\" without any other params.\n. ",
    "navjeetc": "Huy, how did you run the tests? I cannot seem to get any output either on console or in the files in build/output folder. I am running by using the command \"ant test -Dcassandra=\"\n. Well I can't seem to run the tests but I downloaded the latest code today, started solandra, created a core (njs_test) with schema.xml that comes with code and then added data using curl. e.g. curl http://localhost:8983/solandra/njs_test/update/json -H 'Content-type:application/json' -d '[ {\"url : \"x1\", \"title\" : \"apple\", \"text\" : \"apple\" } ]' and then after addding 8 records, when I do this query in browser http://localhost:8983/solandra/njs_test/select?q=:&fl=title&sort=title asc and in the output I do not always see the data sorted by title. Not only that the sort order changes sometimes (not always) if I refresh the url in the browser quite frequently.\n. ",
    "grimesp": "There is definitely a problem with sorting.\n. That seems to have worked. It wasn't just asc sorting that was broken, I was having problems with both asc and desc. I commented out that whole try/catch block and sorting now works as expected.\n. ",
    "lelit": "I tried both implementation of support for 1.0, this one and the one by \"grimesp\", and was not able to get the reuter demo working with either, when instead it works out-of-the-box with tjake's current head: the data load seems going ok, but the website remains completely empty... Issue #147 seems reporting similar effects. What could I miss?\njkusar's one needed some manual tweak to the cassandra.yaml, while grimesp's seems already took care about that.\nI'm planning to move my indexing solution to solr, and I'd like to jump on cassandra 1.0 at the same time (I'm currently using a suboptimal whoosh based solution with cassandra 0.8.x), to avoid two distinct data migrations.... so I'm looking forward to see this merged in! \nThanks in advance.\n. ",
    "jasonmk": "Actually, I've just discovered that Cassandra 1.0+ totally breaks sorting in Solandra. (Note that this is a different problem than the previous sort issue that has been discussed and worked around.)  Not exactly sure what's going on yet, but it appears that no matter what sort order you give it, it comes back in order indexed (or reverse order if you tell it desc).\nI'm going back to Cassandra 0.8.9 for now as I don't have time to investigate, but hopefully I'll have some time to look into it in the near future.\n. Any chance of getting a preview against the beta (as if you didn't have anything else to do)?  I'd love to see if this will fix some of the other issues I've been having.  There seems to still be continuing sort issues even with 0.8.9.\n. Awesome, we've confirmed that that does fix the issue.  Thanks for that.  What's the upshot of disabling the custom cache?  Does it revert to the standard one or does it just not cache fields at all?\n. Out of curiosity, where did you see that statement?  I've looked for anything official from Datastax and haven't found it.\n. Killer blog post.  Thanks for that!\n. I need to investigate further.  It's a departure from where we are, and the need for separate search and cassandra nodes is a bit of a turn off.  We don't have a big data problem; we have a replication/availability problem.  More nodes just means more expense and more things to go wrong.\nThat said, I'm definitely looking at it, but we have a pretty strong bias towards open-source software and try to contribute back where possible so Solandra is of high interest to me.\n. Did I miss something?  I thought you had to have a set of Solr nodes and a set of Cassandra nodes.  One set for your index and one for the data storage.  With RF3, that meant we would need 6 nodes per datacenter.  That times 4 data centers adds up fast.\nIf we only have Solr nodes, does that still let us retrieve the original data via CQL queries?\n. Ok, that's very interesting.  Looks like I'm going to be grabbing a copy and prototyping it properly.  There are a few things that we need to use normal cassandra secondary indexes against to guarantee consistency.  That was why I was interested in still having cassandra access, but now that you point it out, it seems obvious.  Of course a copy of the data would still need to reside in cassandra or else you would never be able to rebuild the index.  I'll pull down a copy and see if it wouldn't meet our needs.  I'd be lying if I didn't admit that the Admin console was a very sexy temptation and I'd love to have Solr 4.0.\n. ",
    "pnegri": "Guys lets fix this. Cassandra 0.8.9 isnt stable as 1.0 :/\n. ",
    "jasonwee": "it works for cassandra 1.0.8 too, verified that with reuters and wikissandra demo loaded.\n. ",
    "nix4": "I am looking for a similar solution. \nBasically, I am trying to import data in a local database into solandra. So in my data-config.xml I have:\n<dataConfig>\n    <dataSource type=\"JdbcDataSource\" driver=\"com.mysql.jdbc.Driver\" url=\"jdbc:mysql://localhost/posts_db\" user=\"root\" password=\"root\"/>\n    <document>\n        <entity name=\"item\" query=\"select * from posts\">\n            <field column=\"ID\" name=\"id\" />\n            <field column=\"text\" name=\"text\" />\n            <field column=\"author_name\" name=\"author_name\" />            \n        </entity>\n    </document>\n</dataConfig>\nAnd my solconfig.xml is setup as shown below.\n<requestHandler name=\"/dataimport\" class=\"org.apache.solr.handler.dataimport.DataImportHandler\">\n    <lst name=\"defaults\">\n      <str name=\"config\">/***/tools/solandra/solandra-app/conf/data-config.xml</str>\n      <lst name=\"datasource\">\n         <str name=\"driver\">com.mysql.jdbc.Driver</str>\n         <str name=\"url\">jdbc:mysql://localhost/posts_db</str>\n         <str name=\"user\">post_usr</str>\n         <str name=\"password\">***</str>\n      </lst>\n    </lst>\n  </requestHandler>\nThe solr documentation specifies that you can post data using the url: http://localhost:8983/solr/dataimport?command=delta-import. What is the corresponding URL in solandra? I've tried http://localhost:8983/solandra/dataimport?command=delta-import, but I get 404 error: \"Can not find: /solandra/dataimport\"\nAny help will be appreciated.\nThanks\n. ",
    "codingismy11to7": "I thought we were up to date, but it looks like we last pulled on Sept 29th, so there have been some new commits. Could this possibly be fixed? Is there a way to upgrade solandra without losing the data?\n. I de-tabbed lines of code that I removed from synchronized blocks, hence the huge diff. Looking at the diff with whitespace ignored is much more legible, and can be seen here: http://pastebin.com/R1L5YTYS\n. ",
    "sahlex": "Used the changes from https://github.com/tjake/Solandra/pull/154.\n. ",
    "grandmoffjake": "For anyone else stumbling on this one - for mine, I had to symlink the build directory in the solandra-app directory\n. ",
    "denizdurmus87": "I have experienced the same issue... For me, I just run the solandra from solandra-app folder, without going to bin folder and there is no error.\n. ",
    "dr10": "@jake can u help me with your new Updated. Solandra, actually make facing a problem when I make entry or add any user to my Cassandra Db it is reflected after few seconds, when check with my api call url, I have upgraded Solandra yesterday only but why is this happening? Do I need to check with Caching in solrconfig.xml or something else is the issue. \n   Please @jake Update me on this Waiting for your Reply\nThanks. In advance.\nXcuse if any Typos\n. ",
    "hopeuknow": "Embedding in an existing cassandra distribution\nTo use an existing Cassandra distribution perform the following steps.\n1)Download your Cassandra distribution\n2)Unzip it the directory of your choice\n3)Run the following solandra ant task to deploy the necessary files into the unzipped dir\n4)ant -Dcassandra={unzipped dir} cassandra-dist\n5) You can now start Solr within Cassandra by using $CASSANDRA_HOME/bin/solandra command. Cassandra now takes two optional properties: -Dsolandra.context and -Dsolandra.port for the context path and the Jetty port.\nI'm getting error in 4th step, most of the files were moved from solandra to cassandra. But finally its shows like BUILD FAILED D:\\Solandra-solandra\\build.xml:290: didn't replace anything\nI checked with build file in 290 line, it has the following code. classname=\"org.apache.cassandra.thrift.CassandraDaemon\" classname=\"solandra.SolandraServer\"\nDue to build fail, I couldn't achieve the 5th step. Thanks in advance if anybody replied.\n. ",
    "kRyszard": "i've tested the process (add docs, perform range queries, delete some data, range queries again) against few revisions of solandra and discovered, that there were working revisions. The last working revision was 513eda7c82 from 9-09-2011 and first non working rev was a32ec231de from 27-09-2011. The difference between them is as big as only one line. I've installed cassandra 1.0.8 + tjake's cfbce36811 and fixed the line - I've tested deletes+range queries - now it works ok, but I do not understand why and if it's not gonna break something else :(\nAnyone knows what this line means?\n. 1. Does it mean if I have broken range queries (on a cluster without this fix) i can perform cleanup to remove all tombstones and make range queries working?\n2. Are the old values (4/64) \"safe\"? I mean is this sufficient size of data to pull to make all queries work?\nbtw: wow, tjake, you're alive ;P since there's an opportunity to talk to you can you please give us a quick comment on how do you see the future of solandra, I mean are you still working on it, planning a release or sth?\n. ",
    "safdark": "Sorry...just realized an issue with my own code. Will re-open if that doesn't resolve the issue.\n. ",
    "olegdulin": "Your thrift size may be set too low, so Cassandra is dropping reads. I think default is 15 megabytes.\n. ",
    "firepub-pfreeman": "edit: I guess I was wrong... while trying to debug this, it appears that the 1.2 codebase has changed quite a bit.  It appears that while ivy downloaded the right jar, it compiled it against an old one... must have missed a dependency someplace.\n. I ported solandra to cassanda 1.2 and when running it I now get a thrift error.  Build works fine, must just be a config issue:\nINFO 17:06:01,835 Using TFramedTransport with a max frame size of 15728640 bytes.\n INFO 17:06:01,841 Using synchronous/threadpool thrift server on ember-cassandra : 9160\n INFO 17:06:01,842 Listening for thrift clients...\n INFO 17:06:02,843\nSleeping 1237ms to stagger solandra schema creation\njava.lang.reflect.InvocationTargetException\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.commons.daemon.support.DaemonLoader.start(DaemonLoader.java:243)\nCaused by: java.lang.AssertionError\n        at org.apache.cassandra.thrift.ThriftSessionManager.currentSession(ThriftSessionManager.java:51)\n        at org.apache.cassandra.thrift.CassandraServer.state(CassandraServer.java:88)\n        at org.apache.cassandra.thrift.CassandraServer.system_add_keyspace(CassandraServer.java:1344)\n        at lucandra.CassandraUtils.createCassandraSchema(CassandraUtils.java:405)\n        at lucandra.CassandraUtils.startupServer(CassandraUtils.java:285)\n        at solandra.SolandraDaemon.startRPCServer(SolandraDaemon.java:77)\n        at solandra.SolandraDaemon.start(SolandraDaemon.java:55)\n        ... 5 more\nCannot start daemon\nService exit with a return value of 5\n. ",
    "jnbdz": "Can you share what you have done... I will try to look into it myself.\n. "
}
{
    "WardCunningham": "I'd like to hold off on committing to any particular markup decisions for a few weeks. I'm thinking that both federation and refactoring on the iPad will color how we think of text.\nOn Jul 29, 2011, at 8:19 AM, gmhawash wrote:\n\nA proof of concept code, still needs clean up and handling the save of edits; Maybe remove language files of ckeditor (except for english).\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/pull/6\n. I think the wait-and-see period for markup will be longer. I'm going to close this request for now. Your contribution is appreciated none the less. Thanks.\n. Added double-click hander to factory items. Factored text_editor out of paragraph. Factories still don't handle drops as intended. \n. Perhaps the members of the journal should be called actions. Then this case would be class \"action edit\". \n. We crossed paths on this edit. I went ahead and changed the variable name too so that we can use the phrase \"journal actions\" with confidence.\n. To reproduce this bug one must look at the server-side rendering of a page. The client-side rendering (with view) works ok. Compare:\n\nhttp://127.0.0.1:4567/indie-web-camp\nhttp://127.0.0.1:4567/view/indie-web-camp\nIt has been a bit of a struggle to keep these two versions in sync. \n. I'm still wanting to employ delegate() but have to reorganize the code a bit to make it work when internal links add new pages client-side.\n. $(\".journal\").delegate(\".action\", \"hover\", ...\nI hate to be dense. And I could just try this, but ...\nIt seems to me that this creates delegates for all the .journals in existence when the statement runs. If more .journals are created then delegate() will have to run again to pick them up. This could happen when sortable gets set up as it is a similar workflow.\nThis is made more confusing in that the \"view\" url routing is just a work-around for code that hadn't been written when you offered this commit. \n. I like the $(\"body\") idea. I'm not opposed to having the dom work hard for us. In fact, since this is a global operation, that seems to be the right place to delegate from. If you revise it and check out all these cases, I'll pull it gladly. I'm also happy to do it too. \n. resolved by be2e970ac1c022f5e8fb83271aec9366d94bf816\n. I attached the delegation to .main, the div around the pages.\n(I thought it interesting that I could refer to a commit that I hadn't yet pushed. This simplified my workflow. However, there are drawbacks I hadn't considered. Might be time for a two-phase commit?)\n. I will read up on delegate and then merge this commit by hand. \nNote: client.js is generated by the coffeescript compiler, a node.js application. It is possible to dynamically run this compiler, like with sass and haml, but the install is more complex. See notes in client/ReadMe.md. I apologize for the confusion.\n. Sounds right. Event bubbling and jQuery's handling of it are powerful concepts that hadn't come to my attention. Forgive me for moving slowly here as I maximize my own education.\n. This is a huge capability. I'm so pleased to have had the chance to work with Don at the North Portland Coder's Night.\n. Good catch. (Aching for some tests.)\n. No need to write bugs for each change you make. \n. Got it. If I forget to close issues, maybe you can do it for me.\n. A feature of coffeescript is keeping the punctuation noise to a minimum. I find that I add excess punctuation out of habit. I'd like to break those habits. In this case, the first parens are required to make the chaining work. The second parens are optional so I left them out. Let me suggest that we omit optional punctuation unless this rule makes the code confusing. I agree that the \"unless\" applies in this case.\n. Wow. Lots to look at here. I'm on it.\n. The /remote/ requests have the local server work as a proxy (soon caching proxy). Right now it is only used to resolve 'fork' actions in the journal. The hope is that authoritative pages will be forked often and that these references will be the fabric of the federation. (I edit 'fork' actions into a journal with my text editor. A local storage copy of a page should have a 'fork' action that leads back to the original.)\n. Sven -- I'm thrilled that you've found the code provocative. I've found that most things I try to do are easier than I thought and then immediately suggest even cooler things to try. There are a number of smaller things that I've just left out because thinking them through hasn't been a priority.\nFor example, I've assumed that I would write a page title to slug converter soon enough (Welcome Visitors => welcome-visitors). In its absence I've just written slugs in links [[welcome-visitors]] instead of the more desirable title [[Welcome Visitors]]. (See commit and comments at 6332145b9979a8830c79547758a1b6eaaff1ea62) In the same way I've used HTML tags instead of markup, a practice that is very dangerous.\nI've resisted making the system usable for creating content independent of the JSON underneath. Hacking the JSON directly is a good way to get a feel for what sort of things the software could do. I'm especially interested in having a lot less markup than wikis have right now, and maybe diverse markups based on item.type (say, a calculator markup, or even reinventing mathematics in ways suggested by \"Kill Math\".)\nSo, by all means, hack the code to try anything that comes to mind. Lean from coding. Find out what federated wiki wants to be. Please, share you discoveries here so we can all learn from your experiments.\nAnd, open issues when you feel I should be working somewhere, or when you're about to work somewhere and want to know what I or others have been thinking. Healthy open source projects make important decisions in public. We'll get healthier as we practice this. Let's close this issue once we've taken all the points you've raised and created issues for each of them.\nAnd, put up a server or two running any old code base that you think might federate. Change it often. Move pages back and forth. That is how we will learn what has to be held stable (and what doesn't) to keep a federation of servers vital. I'll make a list of interesting servers on this site's wiki. That can be our bootstrap until the federated wiki is complete enough to reliably host its own content.\nThanks for joining us. -- Ward\n. The list of federated wiki pioneers appears here: https://github.com/WardCunningham/Smallest-Federated-Wiki/wiki/List-of-Pioneers\n. Here is an expense calculator I wrote 30 years ago.I was thinking it might make a good demo. Would have to translate AWK to CoffeeScript. Might be good to see the calculator work at the unix command line first.\nhttp://c2.com/doc/expense/\n. I'm going to close this issue under the assumption that Sven's Curiosities have been addressed. Just to be sure, I summarize as follows.\n1. make everything plugged -- title is special because it generates url & db key. I'd like to have some Mediawiki automatic redirect on rename. About half the names in Wikipedia are redirects. The slug-making is first step. Thanks. Further discussion in new issue.\n2. groups of pages as a forkable unit -- Interesting idea. We are certainly encouraging small pages. Acknowledge this as opportunity.\n3. local storage just like another server -- Agree. Will do this when the fork-on-edit logic is undertaken. Right now the notion of remote is too incomplete to work on this now.\n4. distributing code as content -- I'm hesitant to do this in that I'd like the JSON to be inert. A similar suggestion is that any story item of other than most familiar type (paragraph, image) should have a \"git\" parameter that cites an open-source plugin implementation.\n5. documentation -- goals here are: (a) house-clean the wiki to be as-built plus roadmap for new developers (b) an interactive tutorial, maybe made by playing back an annotated journal.\n. I tried adding a jQuery plugin that was suppose to make ios safari work. It fixed a lot of things and broke enough I decided to pull it. Have plans to dig into this tomorrow with a colleague with better jQuery chops than me. First thought is that tapping an item will induce it to offer more tap/drag handles. \n. I think the work you've done so far is awesome and very suggestive of what is possible. Thank you. Your first two points make sense and are timely since we will only be great on the mobile platforms if we get on them early. Regarding the thornier third point let me just recount what I have been thinking:\nWe've used hover, click, double-click and drag to express progressively more active gestures. I've been expecting hover/click/double-click to turn into an interactive sequence of taps with each tap exposing more capability through graphic elements (handles) that would appear attached to the object upon which they offer to act. I've studied Apple's Keynote for iPad and find this pattern occurs frequently. (Hopefully we will remain simpler than Keynote, but the authoring activities are similar.)\nI would like to see an obvious association between the handles offered by the interface and the actions recorded in the journal. Both should be identified by the same glyphs. These should be icons not letters as I currently use in the journal. JQuery UI offers a workable set of icons. I've also seen some interfaces make good use of \"ding-bat\" glyphs available in web fonts though this surely gets into compatibility issues. For example, \u2194 is an HTML element but its vertical brother isn't.\nIt would be good if a touch-based interface took the lead in designing for all platforms. We should recognize that the various sized platforms will be used in different ways:\n- phone -- always handy, used to recall information and possibly record small details where they happen.\n- pad -- comfortable reading, used for discovering information and collecting \"notes\" in response to it.\n- laptop -- comfortable writing, used creatively for reading, writing, editing and curating collections.\n- desktop -- room to work hard, used for massive reorganizations and integration with other complex sources.\n- projector -- focused attention, used to persuade based on well organized and easily interpreted visualizations\nThis last \"platform\" comes from my own speculation. I imagine a day when control of the large screen is passed gracefully from phone to phone as each presenter slides smoothly into the two or three displays most relevant to the conversation. Ok, this is fantasy. (I just watched a 25 minute conference presentation video that began with 9 minutes of fooling with the projector. What decade is this?)\n. You may have noticed that there is no convenient way to delete a paragraph from a story. (I've been known to make an article called \"trash\" and drag unwanted paragraphs there.)\nIt would seem that one of the actions (handles) offered by a paragraph when tapped would be the [x] handle that would remove it. I've resisted adding any other delete mechanism as this seems to be the most preferred method. \n. Discussion continues with issue #100.\n. Yes, there are lots of missing pieces. You can see something work if you grab this JSON and put it in data/pages on a server running current Sinatra version: http://sensors.c2.com/welcome-visitors.json\n. Federated wiki videos are on Vimeo and YouTube. I've found them on my iPhone by searching for federated wiki in the YouTube app. Or try this: http://www.youtube.com/results?search_query=federated+wiki&aq=f&aql=f.\n. This is how I have been thinking fork would work:\n1. in putAction, check to see if we own the page\n2. if not, fork the page to someplace we own. Add the fork action for reference.\n3. perform the desired action and make everything right (ownership, favicon, ect.)\nWe don't have any sense of login yet so how can you know if we can own the page. For that matter, where do we fork too? The local storage adds complexity to the story but can probably be abstracted into a storage layer. \n. Yes. Synchronizing this wiki's flat-file data with git works just as one would expect. I do exactly this to coordinate my own development on multiple machines. Wiki running on localhost is just another file editor.\nHowever, the default behavior of writing JSON as a single line does not play to git's strength in merging. I'm told there are work-arounds if git's merging were required in this workflow.\n. 9fc51f5eb07fedd436b213ff673fbb7a0ba148de enables cross-origin resource sharing (CORS). I've used this to improve the quality of citation made when one drops a web url on the factory. The documentation I've found online says this can be enabled by editing .htaccess files of static sites. I haven't made this work yet. However, it seems to be a safer alternative to jsonp.\n. Built -- That's the Journal. The letters are from the Action Types. You might find Concepts page useful:\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/wiki/Concepts\n. 36c2534ab1268c68afa784005af8cfff6ef7e42f adds automatic forking of remote pages.\nNow you can edit any page, including remote pages. Remote pages become your own when you edit them. You will see two new entries in the journal: the fork and the edit that caused it.\nYou will also see the page icon (favicon) change to be that of the local server. The remote server's icon becomes the background for the fork action.\nAmazingly, all this just works when the edit is a move from one remote page to another remote page. Both pages are forked before the move happens. Cool, eh?\n. Ah. I'd seen nulls show up but didn't know from whence they came. Reproducing is nine tenths of the fix. Thanks.\n. Fixed by 17002f78b0f9c2c5ec2edab0b5552d66bb383c50 wherein factory is prepared to participate in a page-to-page move.  (This was hard to find bug, even with excellent reproduction instructions. This is a sure sign that the runtime representation is toooo complex.)\n. I can't duplicate this. Perhaps I fixed it with this commit: ab7a62406b01457fde0bae59d5644cb68d77f54a.\n. Perhaps. But I don't know what the server could say that would be better than what it is already saying. \n. There is some leftover code server side for bringing up multiple pages before internal links simply did this:\nhttp://home.c2.com:1111/view/welcome-visitors/view/indie-web-camp/view/air-temperature\nTry view-source for this to see how simple it is.\nThere is some convention for manipulating \"history\" when building ajax views. I haven't looked into it but you are welcome to give it a try.\nhttp://ajaxhistory.com/\nI've looked into bfcache that preserves ajax constructions while back-forward navigating in mozilla. Safari does it too, but chrome doesn't. This doesn't give you a url you can email. That's worth some effort. Give it a try.\nhttps://developer.mozilla.org/En/Working_with_BFCache\n. This jQuery will answer an array of page ids (slugs) in the order they appear on the screen:\n$('.page').map(function(i,p){return $(p).attr('id');})\n. Any version of a page could be reconstructed by playing-forward the list of journal actions up to that version.\nReplay won't work for many of the sample pages because I haven't been meticulous about making the journal match the story. The way to fix that would be to make an empty page, say new-default-welcome-visitors and drag each item from welcome-visitors to the new page.\n. Awesome. The Twitter app on iPad does the side scrolling with clever use of overlap. It works similarly, but not the same, on the phone. A good strategy here is more important that perfection at the moment. I appreciate your help. \n. I'd like to incorporate remote server addresses into the Miller Column logic and History url formation.\nMy work on Issue #42 makes this possible on the server side. Right now each wiki page in the web page has:\n- id which is the page name slug.\n- data-site which is the remote domain name when the page comes from a remote site.\nThe Miller Columns act up when there are multiple welcome-visitor pages. This happens often when retrieving remote pages. It would seem that the correct solution would be to factor the site name into the page id logic somehow. \n. This sounds like right approach. data-site=\"server.com\" is already there (null if local). Using null for local makes some code simpler, other code more tedious. \n. Well, again my terminology has been careless. I should have said \"origin\", the site from which the javascript has been loaded. So if I started at foo.local then I would get my javascript from that site and it would be the origin and its pages would have data-site == null (or no data-site attribute).\n. Although not perfect, this capability is very useable now. Further discussion should be part of issue #100 or in a new issue.\n. Oops. Sorry about that sequence dependency. I've updated the ReadMe.\n. Hmm. Maybe we GET and PUT /local/welcome-visitors, and, GET /remote/example.com/welcome-visitors.\n. We are very close to an important milestone: Demonstrable Federation. The road map has one remaining item:\n- Cache remote versions locally so that they too can be edited.\nThis was written before we had much insight into what is actually required in the simplest form. It blurs caching and editing. Let's get federated editing working and start building some content amongst friends. Then we can ask how what we have can be simplified into a few fresh and powerful ideas. I'll distill my thoughts into a subsequent comment. Federated editing has been mentioned on Issue #34.\n. I'm on the verge of beefing up this \"view\" url syntax in order to make the history mechanism work correctly with remote pages. My thought is to make the url for a particular view (collection of wiki pages) be a collection of tuples, where each tuple is either a keyword-pagename or a domainname-pagename.\nIf I were viewing two pages from the wiki.example.com site then the url would currently be:\nwiki.example.com/view/welcome-visitors/view/air-temperature\nIf, however, the second page were on a remote site, the keyword view would be replaced by the domain name of that site:\nwiki.example.com/view/welcome-visitors/home.c2.com:1111/welcome-visitors\n. I was thinking of using my instead of view. A first-person pronoun seems an appropriate replacement for the remote domain name. This usage deviates slightly from convention in that the server owner is the me that owns the data, not the viewer. I always thought that a server should refer to me as \"you\" and itself as \"me\".\nAside: This gets into the notion of roles assumed within an interaction. I spent the morning reading about DCI which overlays specific roles over data for the duration of a use case. More here: http://en.wikipedia.org/wiki/Data,_Context_and_Interaction\n. Sam raises the very real issue of relegating a federated wiki server to a subdirectory within an existing domain. Presumably requests to that domain will be routed to a variety of servers of which federated wiki is only one. Is there anything about the client-server protocol I'm suggesting here that can't be routed reliably? A collision with respect to the interpretation of slash, perhaps?\nWhat is the way forward if this sort of configuration were a high priority? An additional level of escaping? Using the query parameter fields? Give up on using browser history the way we are? Are there sites that have faced this problem and solved it well? Google maps maybe?\n. Sven is right. Once we \"know\" another server we are free to reference that knowledge any way we want. Also, the details of how we retain this knowledge are less interesting than how we come to know it. I'm feeling a strong need to get the discovery process working. Here is my short-term plan:\n- make the current history mechanism work with remote pages -- done ea9169a & 768b33c0e6\n- develop the notion of a \"home\" server upon which you can write -- use origin -- done 6a3928d\n- make editing of a remote page cause it to be forked at home  -- done 36c2534\n- accumulate a map of the \"neighborhood\" while we browse the federation -- done af7780f0efc\n- consult this map when resolving page names (the map becomes the search path for internal links) -- done 9024dc119988ebdf03e92266c8c81cf004006d3e\n. ea9169a722b1fb75257f954e2516e7e4ee004ff1 handles the server side of \"make the current history mechanism work with the current notion of remote pages\". Try this link to see it work:\nhttp://home.c2.com:1111/view/welcome-visitors/fed.wiki.org/air-temperature/sensors.c2.com/list-of-sensors\nNotice that the History location rewriting doesn't handle this correctly. It fails in the same way other 'remore' references fail: it doesn't construct a url with the remote site info in it. Now this is possible, but has not been done yet. Maybe Adam can take a look at this? I couldn't see immediately what is required. \n. We're getting close to having this issue behind us. There are one and a half items left to do in the list posted here on November 11. Here is where we stand.\n- Nick has shown us how to assemble a list of pages client-side with issue #76.\n- Dumb servers, like Apache and Arduino, shouldn't need to fetch remotes, use direct CORS fetch instead.\n- Change of mind: Links on remote pages should link to remote content before local content.\nThis last point raises other usability issues but I'm thinking I will go this way just to be done here and get some more experience with that alternative. \n. Although we have evolved substantially from the initial issue, and we have made several todo lists which are not complete, the remaining issue seems better captured in the discussion of issue #48 so I am going to close this issue now.\n. My thought is that items of type paragraph and type image should be rendered in html so that search engines can see them. I don't bother with even this when its inconvenient.  Also, I'm not wedded to haml and sass since they seem to play a small part. \n. This needs some more escaping so that the popup does not trip over html tags that might be buried in the JSON text.\nI'm all for exploiting jQuery UI for popups. I'd especially like to have popup version of many item types:\n- type image --> nice image viewer\n- type chart --> maybe a flot view of the timeseries\n. I'm not sure how it happened but the use a jquery UI Dialog to show ... commit has shown up in your other branch (static-site) so this slightly broken mod is now in my master.\n. I'm learning too. I've used git and GitHub in simple ways for years but these workflows are new to me too. I figure if I claim to be using GitHub's social model as inspiration, I'd better have some serious practical experience. \"Debugging a repo\" is something I'd always left for others. \n. It would be great if you could take a look at it. The URL format will likely evolve. See discussion on Issue #42. I think it would be safe to ignore some complexity such as remote pages leaving this area to settle down before working too hard.\n. Adam -- I thank you for your patience with me as I've taken my own sweet time integrating these and your other awesome commits. I was (unnecessarily) cautious integrating changes the last few weeks only because I was on a demo trip to SF and didn't want to jinx my own copy. My git chops are improving.\n. I like this. I think it would be sweet if a server and the client code it offers could have the freedom to choose either approach: \n- client goes straight to server with the desired page\n- client asks server to retrieve the desired page\nAs you point out, to make this work all servers would need to understand the jsonp protocol. I am a little afraid of jsonp when applied to urls found on the internet. When I've used jsonp I've been in control of all sides of the dialog so I had no issues with eval of untrusted code.\nAllen Wirfs-Brock tells me that he has been trying to get browser authors to relax the same-origin policy for application/json text as simply parsing this is much safer than eval of random jsonp expressions. That would be an ideal.\n. We'd need to work out a way to prevent javascript injection before I'd feel comfortable with building on jsonp.\nAs I reread your previous comments on this issue I realize I don't completely understand your vision of how computation might be spread over a federation. Would you say more?\nAfter pondering this for a while more I've realized I'm operating from some unstated assumptions. I've imagined a federation of servers creating a medium where higher level structures could emerge. I'd also imagined that this emergent structure would be visible to the web and that this would be a strong motivator for participation. Participation would require provisioning some computational resource which would be under the sole control of the participant. I'm happy to elaborate why I think this to be a strong model. But let me instead echo what I now hear you saying.\nYou suggest that the federation provide a medium upon which structural subsets are formed as webs of trust. In this there need not be a distinction of client and server. All participants within the web are equals. Admission and expulsion from this web would be handled at a higher level. Jsonp is important to erase the distinction between client and server. A cryptographically sound web of trust makes any security concerns mute. A feature of this form is that I can unilaterally deploy distributed computations to any web where I am trusted.\nLet me summarize the distinction as read-only trust v. read-write trust. The read-only model supports innovation because anyone can deploy arbitrary computational resources and use the federation only for retrieving and distributing innert information between them. The read-write model supports innovation because uncommitted computational is contributed to a trusted subset of the federation to be used freely by innovators.\nFor either model, the participation of static sites is only a boundary case and not likely to inform architectural decision making. I hope I've captured our respective interests. If not, help me understand what I might be missing. As always, I thank you for your participation.\n. We'll have good stuff waiting for you.\n. Yes, client is overdue for some refactoring. My hope has been to get useful federation functionality working before worrying too much about filling out the client. Abstracting the localStorage is part of the federation story. I've added some BDD style use-cases to the Federation Details. I don't say localStorage in them but I was thinking that. Local storage is a \"place you own\". But if you're logged into a federated wiki server then that is a second place you own. \n. How about this: if it slips in nicely, then go for it. If it requires a lot of re-thinking, let's hold off because we don't want this to interfere with getting federation right. But then, if the current scheme gets in the way of federation, then re-thinking will be motivated by federation thinking. That way we're opportunistic but still paying attention to the roadmap.\n. It might be worth discussing what the general organization of the client side might look like eventually. I had this conversation with Stephen Judkins over the weekend. We came up with a four-layer organization based on philosophical arguments. We suggested a \"model\" layer with two screen facing interface layers above it and one server facing interface layer below it. This is what it looks like:\n- code for interacting with paragraphs and other story items through plugins and various jquery-like interactions such as sortable and textEdit. \n- code for building and interacting with dynamic wiki pages and other browser interactions such as history. This layer might sequence callbacks while loading rendering libraries like d3 or mathjax.\n- code that models the known state of continuously changing pages under the influence of layers above and below. This layer isolates upward facing layers from the downward facing layer. Decision making relating to ambiguous situations and late or erroneous results resides here. This could include a scheduler with a work queue.\n- code for managing interaction with page storage in local memory and on servers. Transient errors may or may not be hidden from other layers.\nHow this code might be distributed within one or more files was not discussed. Nor was services that might be provided by custom built model objects or standard libraries such as backbone.js. My preference would be to organize the structure of a single client.js along the above lines as a starting point.\n. I'm about to move new page creation to the client code. I'm forging ahead on this because, when following internal links, I need to know which of several servers has the page.\nI now GET from each possible server in preferred order and take the first one that doesn't 404. This appears to works great but it deserves lots of testing.\nIts going to be a big commit. I've done it without major code reorganization. It has been a real mind-bender because order in which information is learned isn't aligned nicely with the current control flow.\n(I'm following the implementation strategy enumerated in this comment. The last bullet is almost done.)\n. Well, I've got this working but now 10 tests fail. Darn. \nIt looks the same on the screen.\nOne small difference is that the old server would create an empty page even if you didn't have write permission. With client-side new page creation that page gets created in local storage. Sweet.\nI'll keep working on it.\n. Now have this working through a series of  commits ending at 9024dc119988ebdf03e92266c8c81cf004006d3e.\n. Started on this with commit 9f8b281590.\n. Ben -- Welcome to the project. Thanks for jumping in. There is no better way to learn code than by using it. I especially like your commit stream. I felt like I was sitting next to you.\nHad I been actually sitting next to you I would of mentioned that Ruby/Sinatra are playing increasingly less significant roles in this project. I like the use case you've chosen. But I'm not comfortable with your strongly server-side implementation approach. Let me summarize it and then suggest an alternative:\n- Sinatra detects a unique route (url syntax)\n- Ruby code collects data for a response\n- Haml formats this into a new web page\nThe direction we're heading puts the center of control on the client side with one script, client.js, interpreting JSON objects which are the wiki pages (soon to be call articles) which can co-exist with other articles within a single web page. The way to extend this repertoire of functionality is by adding new types of elements to the JSON object.\nHere is how it might work. Say your wiki started with welcome-visitors that included a paragraph-like section listing all available pages on your site. This then gets rendered as follows:\n- Sinatra generates the boiler-plate for the one-page application\n- A div in that page cites your welcome-visitors which is loaded via ajax and rendered client-side\n- While rendering, your \"all pages\" paragraph is rendered by consulting the plugin directory\n- Your plugin queries the server for a json object listing every page which it then renders as a list\nThe advantage of all this is that instead of creating a new web page you are creating a new type of paragraph (more correctly a new type of story item). This item can participate in drag-and-drop refactoring when you decide that the list should appear on another page.\nThere is a model for this in the \"changes\" plugin that lists pages stored in the browser's localStorage. There is a short-hand list of plugins in the client.coffee code. Look for \"changes\" there.\nFair warning: I'm not above editing JSON to call for a new plugin before there is any interface to create instances of that item type. This lets me feel what it is like to have something before I figure out how to make them. \n. We have been discussing the distribution of responsibilities between client and server in issue #47. There the question is how much the server participates in retrieving remote pages. So far Sinatra has been helpful in that pretty much anything I would expect of a server is easy to realize with Sinatra and standard gems.\n. Since raising this issue, i've been careful to say \"web page\" or \"wiki page\" where there might be confusion. I'm hesitant to make a wholesale conversion to article unless we agree it is a good idea.\nSee also Concepts where I emphasize preferred terminology.\n. I did a video where \"A visualization on one page can find data on another.\" I did this by looking for chart items in the dom. The code looks like this:\nwiki.getData = ->\n    $('.chart,.data').last().data('item').data\nThis \"associative\" addressing of story items seems pretty loose but actually feels great and demos well. I'd like to make it sensitive to the shape of the dataset and have it look through the visualization's own wiki page first. That way if it isn't finding the right data you can just drag the data \"under its nose\".\n. In the code, we'll say page to refer to a wiki page. In general discussion, we should say web page or wiki page to avoid confusion. \n. I look forward to replacing the List of Pioneers with a federated bibliography constructed from these items. I'm close. Um, maybe I should just edit some json and write the plugin instead of dwelling on the drag-and-drop.\n{ \"type\": \"webloc\",\n  \"url\": \"http://uwiki.me\",\n  \"title\": \"New Simplest Federated Wiki Install\",\n  \"text\": \"Static apache file server site - no server side script at all, and so properly read-only. Maintained by [[Sven Dowideit]]\"\n}\n. 312b9e862b0264d8680ca2371a59a93eee6f9349 adds bibliographic references plugin and add [[List of Pioneers]] page. I like Alan's suggestions for automatically creating these lists.\n. 2fea0250e4729d93624974d71161cc9786f2ea9c completes one narrow path through drag-and-drop construction of federated webographies. Some conditions:\n- works for chrome and safari\n- adds link to right-most viewed wiki page\n- prefetches page json to construct good citation\n- requires CORS enabled target server (see http://enable-cors.org/)\n. I'm hesitant to even think in spreadsheet terms. I'm a great fan of them in that they provide the only end-user programming environment where new models are routinely built and then important decisions are based on their predictions. The problem is that they have been too successful. You can't borrow even a little from spreadsheets without pretty much having to implement everything and then you just end up with one more bad spreadsheet program.\nSome things that spreadsheets don't do particularly well that I would like to tackle eventually:\n- units calculations\n- non-scalar values\n- modularity (exploiting refactoring & federation)\nThere are awesome calculators out there ranging from Google Calculator to Wolfram Alpha. I've suggested starting with a 20-line implementation, not because it is powerful, but because it would integrate easily and point in a direction.\n(I've now looked at jquery.sheet and agree that there is a lot to like.)\n. I've ported the awk script which nows runs in javascript on the client side. See http://home.c2.com:1111/view/expense-calculator\n. Well, I had fun on that ski trip in 1981. Did you notice the price of a fill-up back then? Also the lift tickets look expensive at $48 but that was for a week of the best tram skiing in the country.\nTry changing some numbers. The calculator feels spread-sheet like except that cells have names, not numbers, and the flow of information is always down the page.\n. I wanted to keep the calculator simple so that it is easy to experiment with manipulation. Like you, I am inspired by http://worrydream.com/ScrubbingCalculator/\nMy first thought this morning was to make the editor's textarea scroll to the point where one double-clicks. Although this is the right thing to do at some point, I don't believe such optimization is on the path to important discovery.\nWe're better off exploring (and organizing) event-oriented interplay between data, calculation, and visualization within the context of the modern dom.\n. We'll all learn to do this ourselves. Thanks.\n. I merged these changes and compiled the code in the arduino ide. I'll happily accept further pull requests on faith that you've tested them on your hardware. Thanks. -- Ward\n. One way is to drag the location field for one site to a new item factory (use [+] to get one) of the other. Current drop handler only works with Webkit browsers: chrome or safari.\nAnother way is to fork pages that already have links. This will work better when there are already many to choose from.\nI've been known to edit them into database files with a text editor.\n. Soon internal links (made with [[link]] syntax) will automatically search your neighborhood of the federation. You define that neighborhood with the mechanisms mentioned in my previous comment. \n. I expect most linking will come from following forks in the journal. However, there must be some way to introduce a new site to the federation so that this forking can get started. The current approach is as follows (coded only for chrome):\n- create a new Factory paragraph by clicking the [+] at the bottom of a page.\n- drag-and-drop the location bar contents of another federated wiki page to this Factory.\nThis creates a paragraph of type of FederatedWiki. I've encountered a number of issues that make it hard to understand what is going on when dealing with content from multiple sites. Some of these are known bugs. Others are as yet unresolved usability traps. I'm working through these slowly so as to arrive at the best possible solutions.\n. Daryl -- Some of this sounds right. A type federatedWiki story item is meant to refer to a page on another site. I copy the first paragraph of that page to make a \"fat\" link. You can edit that text if the first paragraph doesn't make a suitable citation. \nThe blank page sounds wrong. When you follow the fat link the client-side javascript appeals to the origin server for help retrieving the remote wiki page. This has the origin server working as a proxy to the other server. (One advantage of this scheme is that the origin server cache that page too so you will always have it.)\nAre you using the Ruby/Sinatra server? I'm not sure this proxy behavior has been implemented in the Node/Express server yet.\nI'm also considering having the client-side javascript fetch the page directly from the remote server. It does this to retrieve the first paragraph when it makes the fat link citation. Such direct access is referred to as Cross-Origin Resource Sharing (CORS for short). I didn't understand CORS when I wrote the server-side proxy code to access remote pages. \nYou now find yourself in an area of active development where the \"right way\" to do something isn't always obvious. I've written up some help pages that include a sample of a working citation. See if this works for you:\nhttp://fed.wiki.org/view/how-to-wiki/view/add-paragraphs/view/example-citation\n. Thanks for your patience. There is lots here that can be made much smoother.\n. Sweet.\n. Thanks.\n. Nick -- Welcome. I'm glad to see energy invested in node.js again. Sven had a port going but has other obligations for the moment. As I skim the code I gather that it is a pretty direct translation of the sinatra version. That makes sense.\nI'd like to reorganize the source tree to better support multiple server-side implementations before we get too far along. (I just tried to rationalize the sinatra code but couldn't pull it off. Damn.)\nThe feeling is that this github project should support one client-side application that talks to several server-side applications. I would like installation to be as simple as:\ngit clone ...\ncd Smallest-Federated-Wiki/server/<server-of-your-choice>\n<follow launch instructions found in ./ReadMe.md>\nI offer you /server/express as a place to locate your code.\nYour server should use ../../client and ../../default-data and probably ../../data if you are tracking the flat-file version of sinatra. Eventually there will be db versions. \nThanks and best regards. -- Ward\n. I'd like to find our way to one clean definition for the html/css upon which the client side javascript depends.\nRight now I think that would be having haml/sass generate these files and check them into git as we do with coffeescript generated js files now.\nI notice Sven has something sass related check in under node.js but it doesn't seem useful.\n(I serve one federated wiki site from static files with apache. I just curl supporting files from a running sinatra instance. Yuck.)\n. We've pulled this, refactored the file hierarchy, and started on porting the integration spec. A little git slip makes the commit history repetitive. Sorry for that confusion. It's all there. Welcome abord. -- Ward \nP.S. Thanks much to Stephen Judkins who did the heavy lifting.\n. Very good. Thank you. -- Ward\n. Nick -- Stephen and I cherry-picked your commits to get them on the main branch. We then reorganized the source tree to make space for multiple server implementations. The fact that we made some git mistakes and the fact that we were working in parallel means that we've stepped on each other's toes. \nPlease clone our work into a new directory, have a look at it, and then decide what improvements you want to make. Commit small steps there and the collect them into a pull request I can understand. Bonus points: if you can get the spec to run then I'm more likely to pull requests without understanding them.\nStephen and I worked hard to make a good place for your node work. Please be patient with us as we make co-development of two servers intellectually satisfying. \nBest regards. -- Ward\n. Oh, it's not so messed up. We're just facing a dozen button presses instead of one. Live and learn. \nWe're not big enough a community to keep an IRC channel occupied yet. There is talk of a G+ video hangout once or twice a week. What time would be good for you?\n. Here is how we can keep sinatra and express in sync. \n1. announce intended change to client-server protocol as issue for discussion\n2. commit change to client, server (i.e. sinatra) together with new test\n3. other server (i.e. express) catches up when convenient\n. Odd. I thought I pulled this earlier this morning. I'm glad I checked.\n. Awesome.\nIt looks like the tests are useful. You can specify a specific a specific test to run with the -l <line-number> option.\n. I tripped over that bug too. Sorry to be slow pushing the fix. \nHow about this for a policy: If you see a bunch of non-semantic diffs in client.js then upgrade to the most recent stable coffeescript. If you're already there then it is not your problem. Commit the client.js that you are testing. I'll try to catch up. \n. Kyle -- The demo is awesome. What is the best way to integrate it? Some ideas come to mind:\n- Search on page names, not page content.\n- Load a list of page names from the origin server (new function)\n- Possibly load and merge similar lists from nearby federated servers\nA server might be judged nearby when a page citing it is in the dom. As more pages are loaded, the breadth of the search would expand. This seems natural enough, no?\n. Each federated server has a gradient based favicon that looks good even when squeezed to font size proportions. Each page has a Title which includes mixed case and punctuation. This is converted to a \"slug\" which appears in urls. We sometimes include the first paragraph of a page with its title and favicon to make a bibliographic style citation. All of these would be good choices.\nA good way to proceed is for you to fork the repo and then code away. If you'd like my help, let me know how I can support you. \n. There isn't a public list yet. Since login logic doesn't exist yet any server is quite vulnerable. Here is one that doesn't save edits: http://home.c2.com:1111\nUpdate: There are several public (but not yet advertised) sites with suggestive content:\n- http://fed.wiki.org -- includes how-to advice\n- http://ward.fed.wiki.org -- has advice on learning to program\n- http://ddd.fed.wiki.org -- patterns from Eric Evan's DDD book\n. Kyle -- Let me know what would help you get started on a prototype. I'm excited to see this capability work before we have too many pages to find by browsing. -- Ward\n. In client.coffee:\ndoInternalLink = wiki.doInternalLink = (name, page) -> \n     << function to open new page >>\nand\nresolveFrom = wiki.resolveFrom = (addition, callback) ->\n    << function that manages resolutionContext for links >>\nin server/sinatra/server.rb:\nget '/system/slugs.json' do\n     << function to return list of pages as array of slugs >>\nI just pushed this server side route. Try http://ddd.fed.wiki.org/system/slugs.json for a demo.\nNone of this is perfect. I know.\nI suggest you add a search box in the footer next to the login stuff. Search only the origin server using /system/slugs.json. Don't worry that these aren't true pages titles, we'll get there. Don't worry about other sites mentioned in the dom yet. But resolutionContext is the sort of thing you will eventually want.\nWe'll have Backbone soon enough. The issues search surfaces will help guide the Backbone models. \n. I suspect many federated sites will be modest. How much can one person write?\nBut then, one could easily clone ten pages for every page they write themselves. Let's call that the fork-ratio. It might be 100 or more. So if you're thinking you'd have 100 pages then you probably need room for 10000.\nI've also found it convenient to generate pages from other databases using scripts like this. One site I use at work has 40,000 pages.\nI'd be content to know that a places has been preserved for pre-fetching page titles or even whole pages. This would have to be considered an optimization that might not always work.\nAside: I abandoned the notion of a \"red link\" that is known to not resolve. It was more valuable when turn-around was really low and the \"you write it\" page seemed a slap-in-the-face when it completely replaced the originating page. I've considered turning a blue-link red should it be found to not exist via 404. Maybe then a second click would mean go ahead and make the new page.\n. Drag-and-drop'd images are currently stored in base64 encoded data urls. It would be possible for a smart server to quietly move this data into an asset-management system based on only the current protocols. This would mean that when you fork the pages you get the asset reference instead of the asset. But maybe a smart fork could move related assets between asset management systems. Lets stick with the simple data urls for now even if it means avoiding huge pictures. It has the right semantics and is already working.\nOur very first images were hand-crafted into the JSON as http references. For exmaple, the Indie Web Camp page calls out a remote asset rather than embed it as a data url.\n. The client now uses the information collected in the resolutionContext when following internal links. This goes a long way toward making a corner of the federation feel like a single site. The current client-side complexity led me to dynamically construct a related fetchContext which is searched, 404 at a time, until an extant page is found.\nNote, if I had the slugs.json for each potential server in client-side memory then I wouldn't have to ping a sequence of servers looking for the page. Your search logic must anticipate where my links might lead. That would have you doing a breadth-first search over the space that I'm just threading through. Maybe we can work together.\nSuppose that we keep a list of servers for which we've retrieved the slugs.json. Then, when I follow a link, and it takes me to a server not on that list, I pull the slugs.json for it. Maybe in an hour of browsing I touch ten sites. Then you would have ten lists to search, all already in memory. Here is what that might seem like to the user:\n- When I search for something I may not find it.\n- Disappointed by search, I go looking on my own, still no luck.\n- So I try search again, this time I get lots more hits, all from the places I'd been looking.\nThis seems rather natural. When I search for food after browsing dog sites, I'm likely to find dog food.\nA good start would be to assume that the slug.json cache is present whenever searching. The client should preload this cache with the slug.json from the origin. Call it wiki.searchCache and put objects in there that you'd like to search. Maybe objects like: \nsite: wiki.example.com\nslug: welcome-visitors\ntitle: \"Welcome Visitors\"\nicon: http://wiki.example.com/favicon.png\nsynopsis: \"Welcome to the Smallest ...\"\nThe title: and synopsis: would be optional but could be present if the file has been read. Perhaps icon: is redundant  since it can be constructed from site:. This depends upon you and when you think that construction should happen. Maybe the server could deliver title and synopsis with a different protocol than slugs.json. Lots depends on how fast your code wants to work.\n(If we add a modification timestamp then we can generate a Recent Changes report from the same information. See 009694e)\n. Cool. Any mysteries?\n. We haven't done the incremental searching or the fuzzy matching. But we do have a pretty good approach to federated search so it seems like a good time to close this issue. Of course we're always interested in seeing something new work. And now there is data on the client to be searched. \n. Nick -- I'm having a little trouble understanding what this modification to the client does:\nurlPages = (a for a in $(location).attr('pathname').split('/') by 2)[1..]\nfor urlPage in urlPages when urlPage not in pagesInDom()\n   createPage(urlPage).appendTo('.main')\nI'm guessing that it relieves the server from any obligation to create the DIVs for requested pages in the \"view\" style url. I'm worried that it doesn't handle the case where \"view\" is replaced with a remote server domain name. (Regretfully the existing remote-domain logic is incomplete and poorly tested.)\n. I'll merge this pull request on my laptop and double check that the ruby server is not effected. Adam did most of the code that handles scrolling through multiple wiki pages in one web page. He also handles the interface with the browser history. There has been lots of meandering discussion about this under issue #40. \n. RubyInline is use in the lazy creation of favicon.png. The code is around line 77 of sinatra/server.rb and looks like this:\nget '/favicon.png' do\n   content_type 'image/png'\n   cross_origin\n   local = File.join farm_status, 'favicon.png'\n   Favicon.create local unless File.exists? local\n   File.read local\n end\nThe Favicon.create method would write to data/status/favicon.png. Is this file writable in your setup?\n. Sam -- I trust you got this working. Best regards. -- Ward\n. Good to know. Welcome to the project.\n. I would like to see the favicon generated client side so long as we can get that generation done at a convenient point during the server install.\nOne strategy would be to use the image from default-data/status/favicon.png until a better one is chosen.\nI favor generating favicons with gradients because that gives all federated wiki sites a family resemblance and they look good as backgrounds in the fork [ f ] actions.\nI once considered retrieving only the specification for a gradient from the server and always rendering it client side. This fails because the generated icon doesn't show in tabs and browser history.\n. I might give the client side a try. I'll go ahead and close this issue for now. \n. Yes, thanks. Most important is that instructions are both correct and of good practice. \n. Whoa, that's amazing. Now I get why moving this simple bit of logic, enumerating the wiki pages cited by a url, from the server to the client is so profound.\nTwo more client-side tweaks will complete this read-only utility: 1) remote references should go straight to the remote site (via CORS), not through the origin as a proxy, and, 2) edits should go immediately to html5 local storage.\n. This is the current gradient generator:\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/blob/master/server/sinatra/favicon.rb\nLight, dark and angle are the three values that would need to be substituted into a templatized version. Also, a server would need to remember the values of these three random variables so that a gradient can be reproduced once chosen. Would this be as simple as producing favicon.svg as a substitute for favicon.png?\nI remember looking into this but just happened to get the server side png generator working first. \n. There was a time before favicon.rb when new servers used the same default icon still available in default-data. There was a test for this but I updated it to expect generated icons with this commit: 6445bd595ea7102ca8c18a35711ef625ed7db01f.\nI suggest Nick adopt the default icon strategy and either ignore the failing test or find some way to make it pass with either approach. (Does the test fail? It looks pretty permissive right now.)\nOnce we have a clear notion of site ownership we can provide a utility for choosing a new icon from a collection of client-side generated icons. Richard Dawkins entertained many geeks with a program called Biomorphs. I'd like to see his \"evolutionary\" approach used to offer a site owner a simple mechanism for finding a likable icon that still adheres to the family resemblance goals I have for federated sites. \n. Marty -- Yes, that is what I am suggesting, except that the server needs to go to some modest effort to make sure that there is a /favicon even if it is a brand new server. I'm suggesting that the node implementation just copy the favicon from default-data if it doesn't find one where it belongs.\n(I did this for a while but it got too confusing when I made a server farm and all the sites there had the same favicon. That's when I added the random generated favicons.)\n. I would like to prototype a favicon picker coded as a federated wiki plugin. My strategy will be to:\n- Generate random gradient parameters (like in favicon.rb, or better).\n- Use these parameters to create many small canvas objects from which one will be picked.\n- Convert the chosen canvas to png by using canvas.toDataURL(\"image/png\").\n- Send the converted image/png to the server to be the new favicon for the site.\nI haven't done this yet so I don't know that it is actually possible. I would be pleased to see working code snippets, especially for the part about converting canvas to png and saving that on the server in a normal .png file.\n. Great tips. Thanks.\n. We now have a \"clear notion of site ownership\" so it is a good time to move forward with some owner focused site customization plugins.\n. I'll look forward to seeing this. Thanks for taking it on.\n. Nick -- Stephen and I added open-id authentication which required server-side enhancements including pulling in some libraries and adding identity to the persistent server state. Have a peek here. We've tried to make this upward compatible. Also, no tests to pass for this yet. But that stuff is coming soon. \n. sweet.\n. Two good ideas. Thanks. -- Ward\n. Hmm. I'm hesitant to write rm -r anything into instructions without knowing the unix skill level of the reader. The point here seems to be that it is easy to experiment. Maybe there is a way to helpfully explain this without going into dangerous command line commands. I appreciate the help.\nThe \"farm\" capability also makes it easy to experiment. I set my localhost up that way and add 127.0.0.1 foo.localhost lines to /etc/hosts when I want a new experimental place.\nSee also https://github.com/WardCunningham/ddd for an example of where I write a script to generate pages from another source.\n. But, hey, thanks for getting us started down this path.\n. Sounds like a good idea. Can we get a specific proposal?\nI'm a little worried that the development workflow might get too complex if we have to edit in default-data and then test by moving content to data with each run.\nMaybe we need a better deployment strategy. For \"devops\" I've been using git pull.\n. I understand this proposal as having two parts:\n- default-data like semantics for static content like html and css\n- default-data like semantics for plugins\nThe first part addresses how you would configure a new site. In particular, you wouldn't want a git pull to clobber customized css. I have imagined that css customization would be done by designers to match related properties.\nThe second part addresses how you would add new or revised plugin capability to an existing site. I had imagined plugin management would be done by programmers through sharing mechanisms like GitHub. I'm now thinking that some sort of automation might be possible without creating a virus distribution network. See http://code.google.com/p/google-caja/ for example.\nLet's not let deep thinking about part two get in the way of making small adjustments suggested in part one. Also, let's let development convenience dominate for a little longer. Thanks. \n. I'm no expert on this but I understand that it is easy to offer shortcuts to well known open-id providers like google, twitter and facebook. I think we just shove a well known address at each of these services and that service figures out what to do. We need to look up those well known addresses.\n. I'll read up on these tools. \n. I found http://openid.net/get-an-openid/ which lists major sites sharing their logins as OpenIDs. But heck if I can find out clear documentation what people like us are suppose to do to offer a simplified login to our users. \n. I found this list of OpenID providers with simple universal urls:\nGoogle : https://www.google.com/accounts/o8/id\nYahoo : https://me.yahoo.com\nAOL : https://www.aol.com\nLiveJournal : http://www.livejournal.com/openid/server.bml\nFrom http://qpleple.com/how-to-make-people-login-into-your-website-with-their-google-account/\n. 50a36cf9077b1a3d1015f2f7eb90af18d2adb6d5 adds shortcuts for loging in with googe, yahoo, aol and livejournal.\n. Good catch. Have now revised this documentation to acknowledge that we only use the arduino 1.0 suffix convention. \n. How about a test run tonight, 10pm pacific, 4pm in eastern au.\n. Yes. I'll be online 10am pacific time Wednesday mornings. (I only missed once.)\nToday @nrn and I read code together.\nThe fedwiki hangout remains open 24x7. \n. I moved this improved link up to the first comment in this issue. Sorry for the inconvenience.\n. Google seems to have discontinued this \"named hangout\" feature. Other changes have made public hangouts unproductive too. I've promised to solve some of these problems but don't actually have a solution. I'll take mention of hangouts out of our readme.\n. I launch the hangout each wednesday. I tweet the new address at https://twitter.com/WardCunningham. I also post the new address at http://fed.wiki.org/view/frequently-asked-questions.\nI will be traveling for the next two weeks. I may be able to launch the hangout during my travels. @nrn may launch them on my behalf. (Look for his newer version of the faq.)\nThe hangouts remain an active and rewarding event which will continue. You can count on it.\n. Nick -- I'll let you close this issue when you have a message.\nNo need to work any harder than we (Stephen and I) did. On the other hand where I work they had a contest to see who had the best 404 screen. It matters on the modern web.\n. Adam -- I have just a hint of the little-popup you suggest. Here is a page that includes a plugin to list locally stored pages:\n- http://fed.wiki.org/view/local-editing\nIt only shows edits made while fed.wiki.org was origin. Local storage is tied to origin. This was just enough for me to explore local storage. Now I've discovered Chrome has a pretty good local storage browser under Inspect Element's Resource tab.\n. Sven -- Lots of great ideas. Let me just respond to a couple.\nAdding converting proxies for non-wiki-json pages is a cool idea. This is a pull-approach for incorporating foreign data while I've been adopting a push-approach with converter scripts like this one: \n- https://github.com/WardCunningham/ddd/blob/master/scripts/convert.pl\nYou suggest all edits should remain in local storage, even when you have write access to the origin server. I can see the logic here. You make changes, review your changes, and then push/sync only the ones you like. Its an extra step, but one that would be appreciated should you have readers depending on logical consistency between pages (the case with software.) I'm going to have to think about this one. I get a bit of the effect by using my laptop as origin.\nI agree completely that a server should report lots of server-wide or even neighborhood-wide information. Having this sort of information show up as a data-thumbnail has implications I haven't reasoned through.\nI'm hoping that the visualization work I'm doing with d3.js will factor into server data reporting. I have force-relaxation graph layouts working where you can click on a node to open the associated wiki page.\nI also want to encourage big-data people to spider the whole of the federation and build totally awesome visualizations.\n. I'd like to be able to write an entire site to a local file. This might be a good way to transfer a bunch of Local Storage pages to another location. I haven't quite figured out how to do this. Suggestions?\n. The wiki page is whole.  All the parts needed to move it are part of the page.  A good whole-site file format would be a json of every page. I think this could be encoded into a data-url which the browser would offer to save-as into the user's filesystem. \n. The protocols are fluid until we start numbering versions.\nThere is a pressure to keep the federation protocol simple so that it can be easily and correctly met. The presentation/ux protocol is only constrained in that we have two server implementations that share common client code.\nI would like to have a protocol that could retrieve the json data from a story item of type == data. Perhaps it could exploit the random id attached to every story item. This would save a bit of unwrapping when using the data in other programs.\nThere are other federation protocol enhancements being discussed. Issue #70 suggests extending the federation protocol to support search.\nAlthough I don't completely understand every suggestion ever make here, I do take every one seriously. Thanks for participating. \n. I've enjoyed hacking simple sensors over the years and have sensor data online. Wikiduino was created from the Arduino based SensorServer which had become obsolete when I converted my sensor network to Txtzyme/Sinatra.\nI make lots of crazy sensors but always end up with a handful of one-wire digital thermometers of which I use three in the Wikiduino installation.\n. Thank you both. \n. Thanks Patrick. I'm getting the impression that the node implementation is already preferred to the ruby implementation because it is an easier install.\nNick has been working his way through our (ruby) integration tests. I'll leave it to him to pull this into his workflow first at his earliest convenience.\n. For our edification only: what would have been a good way to handle these commits and end up with a squeaky clean history. (My own understanding is limited to pull --rebase before pushing. I gather there has been more going on here.)\n. Nick is looking to rename \"server/express/bin/server\". Is there a problem with that name? The name \"sfw-express\" seems to encode information already present in the path to the file.\n. I'm excited to have serious attention focused on the touch platform. I've been afraid to get started with touch for fear of getting it wrong. We'll need effort and daring to get it right. Here's why it's worth it:\nTablets make for comfortable study. Soon readers will refactor everything they read, as they read it. This will be the \"killer application\" of federated wiki. This mimics the professor who teaches a new classes to learn new material with their own notes becoming lecture notes and then the first draft of their next book. Careful reading becomes publishing.\nI've encouraged Adam to modularize the client code in the process. Sam and I had a go at integrating backbone.js a while back. It didn't work. Our conclusion: we need to build out from new models and think through the proper placement of every line of code as we go.\nThis is the challenge ahead. One client, with clean models, serving all html5 platforms, enabling a new kind of community engaged in a new kind of scholarship. \n. The emit/bind plugin interface has out lived its usefulness. I'll open another issue (#112) where we can discuss improvements. Hopefully if we work that forward together then we won't have a difficult merge between mouse and touch versions.\n. I've looked at a lot of the client code in the last 24 hours. I'm pretty familiar with the control and data flows once again. I'm thinking I should record my understanding in some sort of document that might suggest model objects and model lifetimes. I found creating new pages client-side particularly difficult.\nOne challenge is getting capitalization and punctuation from a link text to the title field of a new page on the disk. (We don't do this yet. We use the down-cased slug as the title.) For example, say I have a link that looks like this in source:\n[[Tim O'Reilly]]\nClicking on this link should make a page in the database under the key:\ntim-o-reilly\nThat has a title field that looks like:\n\"title\": \"Tim O'Reilly\"\nIf you think of new page creation as having six phases, it use to be I needed it in phase 6 but lost it in phase 1. I now have a path from phase 2 to phase 6 so I only have one more phase wall to hurdle. \n. When I asked around about unit test frameworks for client-side javascript the name I got was Jasmine. More recently I noticed Mocha mentioned on issue #100. I assume that would be for unti tests too. \nSinatra tests were developed in RSpec because that is the normal thing for Ruby and in many ways the progenitor of most modern frameworks. Selenium under RSpec is also well understood for integration tests. I hope we can keep that going. \n. Thanks Erkan\n. I'm going to hold off on this merge until Nick has a chance to think through the logic of the changes we made. I'll explain myself here. \nWhy make unclaimed sites writable?\nStephen and I made this choice when we first added authentication. I encouraged it because it preserved the existing behavior and kept our integration tests working. Its also how the original wiki worked: 100% open. \nNick correctly points out that anyone could claim a shared wiki. That doesn't make sense. It is one more way that our federated codebase does not support effective sharing. This doesn't concern me. I'd rather find more ways to get people into their own servers.\nI've used this \"write first and claim later\" workflow to amaze people when I demo. Faced with the protest: \"I don't want to setup a server\" I just say, \"You already have one\". We then go to \"their site\" on my farm and start editing. Two edits into the experience they start wondering how this could possibly happen and what makes it theirs. Then I say, one more step: claim this site by pressing this (G) button right here. Done.\nThis is an instance of the Have-Fun-First pattern for encouraging viral behavior. Here is what the viral loop could look like:\n- read something\n- edit something (privately)\n- edit something (publicly)\n- claim the edits\n- recruit readers (the inner-loop)\n- develop frustration with particular farm\n- move content between farms (see below)\n- launch personally owned site\n- turn personally owned site into farm\n- recruit writers to new farm (the outer-loop)\nI'm suspicious of most viral loops as get rich quick schemes. However, since we can return to the author (or anyone else) the complete contents of the site as one large JSON, we give everyone the opportunity to defect. Git has this property too. GitHub does not.\nWhy touch the filesytem with each request?\nTouching the filesystem with each request is foolish in a high volume environment. And, of course, high volume is where node.js is known to shine.\nHowever, when I first got the farm logic working in sinatra I blew an important demo because I had cached information incorrectly flowing between sites. I've since de-optimized the sinatra server in order to get the logic right first.\nMy suggestion is to write node.js code in the style that confines de-optimizations and follows all the performance patterns one would expect of node.js code, even if it isn't currently optimized.\nNow what?\nMore discussion? Do the pull? What do you think?\n. Nick, I like your plan. Can you pull the mods from Tom's repo and put together a pull request? \nOne thing more you could do: We thought that the claim logic modifications would make the integration tests pass again. Unfortunately we had trouble getting RSpec/Sinatra running on Tom's machine. It was late. Would you see if our work makes the tests pass again. It would make us feel good to know that. Thanks.\n. This commit solves an unpleasant dependency issue for node. Although I see it as a step sideways from the point of view of new site creation, I'm happy to see activity in this area continue.\n. Seems like each plugin could benefit from its own directory. Static html and other metadata could go in files with standard names.\n. It is a good question. We have to be careful to not distribute viruses throughout the federation. We have to jump through some hoops just to pull inert json from remote sites. Pulling active code automatically will require even more care. Some form of sandboxing is required. Here is one that depends on source-to-source transformation:\n- http://code.google.com/p/google-caja/\nAnother suggestion is to include the address of a git repository in plugin metadata. That would leave it to individual server operators to decide how automatic they want to make installation. Perhaps good practice would be to clone your own repo into a DMZ version before browsing too widely when plugin installation is automatic.\n. Hey, thanks for the video. I was scratching my head wondering what a drag-rail might be until I saw the screencast.\n. I can see how it avoids entering a mode when ever you want to see something a little more detailed. If you could also rearrange pages, would you use a similar bar at the top of a pages to drag it left or right?\n. Warning: I did not fully understand the completeness of your thoughts until I had completed writing the following analysis. Please forgive me for going on and on when you had already said most of this. You can now watch me come to understand and maybe even contribue. \nIf a \"mode\" is a state where inputs are given an alternative meaning, then the most evil version of mode is when each mode demands of a user a different input to be released from the mode. Unix is notorious for this: \"ctrl-d\" here, \":wq\" there, \"bye\" somewhere else. Things got better for users when pointing joined typing as a possible input.\nIn the case of the map we have several distinctions to make: are we dragging the map because we want to see past the edge (a temporary change), or, are we dragging the map because we want others to see a different view (a permanent change). The TextEditor uses double-click to enter edit mode, a declaration that changes to be permanent. Loss of focus or command-s is our way out now, though a universal way to 'cancel' would be handy.\nThe TextEditor has a similar conflict as the map: If we're not editing, then drag could mean \"select\", not move the paragraph as it is currently interpreted.\nThe drag bar nicely avoids a third interpretation: we are dragging the map because we want the map to appear somewhere else.  Other possible interpretations of a drag motion include: moving one page within the arrangement of pages, and, moving the collection of pages together.\nAs I write this I'm coming to believe we need (dare I say it) a hierarchy of modes. We start in mode \"reading\" where swipes scroll and taps hyperlink. Then, somehow, we enter mode \"organizing\" where we move pages with respect to each other and adjust map views to see what we want to see. Neither reading or organizing are persistent beyond a session. Editing is the third and deepest mode. While editing we will want some access to gestures that allow us to continue reading and organizing while we edit.\nIf we were to offer a read-only site then we would fix it so that one couldn't get into editing mode. Today we say, anyone can edit but changes don't any further than one has permission to write.\n. I use keynote on osx and ios. I consider both to be good models for manipulation of structured content.  Sometimes recognizing useful analogies between presentation slide and wiki page requires understanding the why behind each of apple's decisions. (If we understand the why, but not copy the what, maybe we can stay out of court.)\n. I'm feeling like Smallest Federated Wiki and SWF are working names. I'd like to reserve Federated Wiki, fed.wiki.org, and fedwiki as important names for the ongoing project.\nHow about we think through what we'd want the experience to be for a fresh install of fedwiki and not make that be a package until we can deliver on that experience. I'd like to make that be our first numbered version, maybe 0.1.0. Of course we'd want to be aligned with \"semantic version numbers\" but I'd have to refresh my memory on how they work.\nHere are some things that have been in the back of my mind:\n- Usability traps that lose content have been fixed. (I'm working on this today.)\n- Sequence numbers in the journal actions. (Max Ogden says I need them based on his CouchDB experience.)\n- Default front page that makes sense to new user. (A fork of fed.wiki.org with links to help)\n- All tests green\n. Agree. I've written a pattern that describes what I would consider proper behavior:\n- http://ward.fed.wiki.org/view/patterns/view/predicted-results\nHere is an interesting example: If A writes a page and B adds to it and I copy B's version, then, when I follow a link in my copy the search path\" would look like this:\n- origin => B => A\nWhen I follow the link the following steps happen:\n- check origin for referenced page, assume it's absent so continue search\n- check B's server for referenced page, perhaps its down so we continue search\n- check A's server for referenced page, if we find it, call it good.\nMaybe the referenced page is only on B's server. And maybe B's gone for good. Then what? We're obligated to show in some meaningful way what's gone on as we poll the federation. And, I would think, we're not out of line offering a blank page upon which I can write.\nAside: in the past we would have asked the origin server to retrieve the remote page acting as a proxy. This creates the (unrealized) opportunity for the origin server to cache all remote pages viewed. With the recent work in this area I chose to go directly to the remote site.\n. I wonder if @nrn has fixed this problem with his work on history: 1170d275ff72bc55c68063cd0644c48ba84cb9af ?\nPoking around a bit myself it seems to be working fine. Reopen with more details if it doesn't work for you with a recent version client.js.\n. The three concerns originally expressed in this issue have been met. Our ids have proven to be satisfactory. We use journal index as the sequence number. We timestamp actions and report that in the hover and elsewhere. All reasons to close this issue. \n. Sweet.\nPrettify seems similar to MathJax in that it expects to see the DOM for the whole page. Does it work when a second or third page with code appears? \nMathJax has an advanced API that anticipates a more dynamic environment. Its on my list of things to dig into. I wonder if Prettify has an advanced API too?\nI'm hesitant to just pull this without at least having a try at dynamically loading the javascript when code is first encountered. Give me a few days to look into this or have a go at it yourself. Again, MathJax is the (less than perfect) example to follow.\n. Oh, cool.\nI bet you could load Prettify with wiki.getScript and then do the div.append in it's callback.\n(I'm off to work right now or I would try it myself.)\n. I put some time into this pull request tonight. The good news is that the commits still merge without conflict.\nI couldn't get results that made sense to me.\n- I created a story item type==code by choosing code from the factory menu.\n- I pasted the 7-line class Voila example from the README documentation linked above\n- It rendered color-coded but it all appeared on 1-line\n- I checked that the <pre> was emitted, it was\n- I console.log'd the prettyPrintOne input and output: 7-lines of input becomes 1-line of  for color-coding\nI'm not sure why it collapsed my lines. Could this be an end-of-line convention problem? Suggestions?\n. Well, my go at this was a few months ago. I wanted to move the responsibility for loading the prettify code. The plugin should load the prettify.js and css so that this network traffic is not incurred by everyone as it would be when loaded as part of the sinatra layout: 1350c4ce3dbf045dbc2364a35180566750e3dfbc.\nDynamic loading can be a little tricky so I wanted to give this adjustment a try. I only remember that I was having configuration problems of my own (see above) before making the loading dynamic so I've set this aside for now. \nI'm eager to see this plugin available. I'm tired of posting code into federated wiki with nothing more than a <pre> tag. \n. I'm closing this request. Work continues on pull request 256.\n. Work has completed on pull request 256.\n. @dvberkel thanks for getting this started. I'm sorry it took so long. \n. Here is a history connection. I created a page and then later deleted it. But, when ever I back up to that history, the page gets recreated again. The specific case I just noticed had the page created in local storage, something that is not specifically identified in the history URL. This means the history can be misleading when logging in and out. Perhaps view should be replaced with origin and local referring to the two places a page might be stored for a given domain.\n. There is a lingering case or two where a server will create an unwanted empty page but this issue does not describe the problem. This one is better closed.\n. Back to 100% passing for Sinatra version.\n. Cool. Sometimes being slow pays off for me. Thanks for the followup.\n. We've avoided adding stray buttons without a more comprehensive design. I'd imagine a button bar appearing when entering editing that would include choices like save, cancel and delete.\nTry clicking outside the edit field to save. \n. The rendering of JSON into DOM objects is largely handled by the emit: methods of plugins. For something simple like a paragraph or image this is handed off to jQuery without further templates. For complex things like equation formatting or data visualization a plugin is free to load more packages and let them have their way with the DOM.\nThere are a few exceptions. \nThe case you mention, the [+] tacked on to each page, is constructed as part of footerElement in the client function buildPage.\nA simplified footer for a simplified page can be generated server-side. This supports robots and curmudgeons that won't run javascript. The simplified footer doesn't include the [+] because there is no javaScript to handle it.\nRegarding controls in general.\nWe're anticipating some generalized button bar scheme that would be developed such that users could move smoothly between touch devices and mouse devices and find the sort of affordances they would expect on each platform. We've been intentionally light on controls so as to not build much of a legacy in advance of that design.\nSuggestions and experiments are welcome. \n. I'd love to see simple instructions for hosting on all relevant services. Maybe you could write up some how-to instructions on our GitHub wiki.\n. Right now the client expects remote servers to accept CORS requests for JSON files organized as Federated Wiki pages. \nThere is currently some further interpretation of the client-to-origin-server uri in order to distinguish drag-and-drop of Federated Wiki pages from one server to another. I'm not happy with this second requirement because it seems to have one server inappropriately depending on particulars of a remote server's conversation with its client and the browser's page history mechanism.\n. This issue suggests ways that federated wiki could interact with other services. All good ideas I'm sure.\nOne unanswered question was how are stories identified at the server level. The answer is that they have a sanitized key, constructed from the page title, that retrieves the whole page in JSON. The items in a page have ids that are used for tracking moves. They are not guaranteed to remain on a given page or even on a given server. It might be possible to conduct a search over some neighborhood for pages that contain paragraphs of a given id. This has not been demonstrated.\nWithout further discussion I will consider this issue closed. \n. I'm having some trouble with this commit. Here's the test conditions:\nI'm in Chrome\nI'm running against the Sinatra server\nI type: localhost:1111 in the location bar\nI get the OpenID form but no Welcome Visitors page\nI also get this javascript error:\nclient.js:253  Uncaught TypeError: Cannot read property 'left' of null\nWhen I try the same thing in Firefox there is no error. Nor is there an error when I run the integration tests. Of course these run against Firefox, not Chrome.\nI'll wait for you to have a look at this before I pull.\n. Pushed a few wiki.log in branch history to see if my buddy Alex and I can figure out what's going on. We think it has something to do with how the Sinatra server sets up initial divs. At least we are suspicious of the code around here\nThis is trace output showing variations between different server/browser combinations:\n```\n Sinatra/Chrome     [\"amost createPage\", Array[0], Array[1], Array[1]]  FAIL \n Express/Chrome     [\"amost createPage\", Array[1], Array[1], Array[1]]\nSinatra/Firefox    [\"amost createPage\", [], [\"\"], [\"welcome-visitors\"]]\n Express/Firefox    [\"amost createPage\", [\"welcome-visitors\"], [\"view\"], [\"welcome-visitors\"]]\n```\nThe Array[1] elements reported by Chrome are exactly those reported as quoted strings in the Firebug console.\nExpress emits a status 302 redirect to the welcome-visitors url while Sinatra relies on the client code reading welcome-visitors out of the html returned on the HTTP / request. \n. Could it be a Mac OSX problem?\nWhat problem does the test for empty string solve in that code?\n. I'll did a little deeper tonight. I seem to have the only failing setup. \n. Thanks for digging in.\nOn Feb 21, 2012, at 6:47 PM, Nick Niemeir wrote:\n\nGuessing because I don't have the broken setup to prove it, but I think the problem is you need a setState() line around 526 for the ruby server in some instances.  But in testing this i've found another problem with the fetch stuff using the new history stuff.  Hopefully get another commit together tomorrow fixing this stuff.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/pull/135#issuecomment-4096855\n. Thanks for taking the time to think this through. \n. This is the second time this has been reported. As I reread this I gather that your complaint is that the dialog box opens, not that ALT-S is an inappropriate key for saving edits. It would be good to know if there are user-interface guidelines for your platform. Do you know? We can summarize here:\n- Mac: Command-S\n- Windows: Alt-S ??\n- Linux: Alt-S ??\n. I believe that CTRL-S was not processed by federated wiki. Rather CTRL-S was processed by your browser to save the browser page. It only appeared that federated wiki was processing CTRL-S because the textarea box lost focus when the browser save dialog came up. Saving your text on loss of focus is a feature of federated wiki. \n\nI've revised the online documentation to make it clear that Linux users should use ALT-S.\nhttp://fed.wiki.org/view/add-text/view/command-s\n. Had you entered a complete and valid url of a site providing OpenID when you got this error?\nThe messages seems to be saying that only http:// was entered.\n. Yes, someday.\n. I would like to avoid modal notifiers, especially alerts. I've described a generalized alternative in a pattern called Predicted Results. The relevant paragraph is as follows:\nReport network failures or other unexpected results in the space allocated for the prediction. Make it easy to discard the unwanted result or even retry the request.\nI would hope to see this pattern supported deeply and generally in the javascript code. Following the pattern, a user would expect to see something appear representing login that contains or becomes the error message. \nIn the absence of this level solution I would rather just let the modules we've assembled report errors the way they choose. I gather from your report the only message was in the server log. Is that true? \n. You may have better luck with Chrome. Drag and drop varies by file type, browser vendor and operating system. It will be a long time before we have the space fully covered. Sorry for the inconvenience. \"Unexpected Item\" is the correct behavior for cases that have not been coded.\n. Bryan Donovan set that up when he set up rspec. I don't know if they are related. Perhaps he had personal information in his own .rvmrc? I use older versions of ruby on older computers for which upgrading is a pain. \n. I have to admire Nick's suggestion since it is more self consistent than the internal and external link markup offered by MediaWiki. MediaWiki uses a vertical bar in one and not the other. I admire it but don't want to do it. Let me explain.\nI suggest we move away from markup as much as possible in the default paragraph type. I've been mixing in a bit of html syntax but I'd like to put a stop to that. To that end I'd like to add a new story element type: html.\nThe html plugin would enforce a subset of html that is compatible with the federated wiki software. That means italic, bold, headings for sure. I'd also like to see a variation of the current TextEdit that understands html and makes sure novices get it right. With the html paragraph type we can then limit the default to not use html tags at all. Angle brackets would be angle brackets again.\n. The link slugs are down-cased and url friendly names of pages. This means you do have case freedom in your references if not tense or plurality. \nThe slug algorithm is not quite what I intended but as we gather more content it becomes harder to change. We should add finalizing this algorithm to our list of things to do before we start numbering releases. (We're not yet to version 0.1) Of special concern here is reasonable behavior with international names.\n. Yes, you can currently write whatever html you want in a paragraph so if you want tags to work like they do in html, you have html. I consider this a dangerous practice but would like to offer an item type == \"html\" where it is allowed.\nFederated wiki has an enhanced linking mechanism which is different than that offered by the html anchor tag. I would like to reserve the [[ ]] syntax for those federated links. This is really what the project is about.\nI should probably write something about this. For now I have a new video which hasn't yet found its way to the video collection: https://vimeo.com/36384739\nI would like the default paragraph type to be plain text, flowed into the available space, suitable for a single paragraph, augmented with [[ ]] and [ ] links. This says, at the paragraph level, words and links are the only things that matter.\nI'm against alternate text for links for a variety of reasons enumerated in an essay here. If one really wants to use alternate form in plain text links then the [ ] link syntax is available, only without the special federated wiki link semantics.\n@kbigler suggests a good method for migrating from our current situation to new paragraph types, say plain and html, which are not quite the mime interpretation of these words, but close.\n. I've written user documentation for federated wiki link syntax and semantics on the How To Wiki pages.\n. We've been developing on ruby 1.9.2 and have the expectation that bundler can assemble a workable complement of gems in that environment. I run sinatra on older macs using ruby 1.8.5 because I can't get 1.9.2 to compile with the older versions of osx. I have to admit that I'm new to bundler with this project and may not use it to best advantage.\n. You're experience is valuable. Thanks.\n. I use the term Smallest Federated Wiki when referring to this project. I use the term Federated Wiki when referring to the communities that I believe will emerge based on work explored first here. SFW is a reasonable shorthand when referring to this project among those familiar with it. This project has, since day one, been done in public on GitHub. You might enjoy reading some of the first commits. \n. I've added the above explanation to the FAQ.\n. Wagn has some very original ideas that have developed nicely. I'm flattered that there is interest in federating our two systems. Wagn is certainly more production ready and might be a great choice for small group issue tracking.\n. I have done an experiment with modeling complex enterprise data, many thousands of pages, in a federated wiki. Different stakeholders can negotiate changes by forking the pages they'd like to see changed. Visualizations would show the impact of respective changes. Competing interests could fork parts of each others proposals until all are happy and the new system allowed to go live.\n. Enough discussion for now I think.\n. More hints:\nSafari (Version 5.1.2) works as expected.\nFirefox (Version 10.0.2) ends up where expected but doesn't scroll smoothly when moving with browser forward/back controls.\n. Yes, active class changes properly in all cases. \n. Ok. I'll set a trap for minX going null and see if I can correlate it with anything I do.\n. Found it. Chrome will sometimes report scrollLeft() of 0 after a scrollLeft(1)\nThis must depend on display scale factor and how numbers might round.\nWork around is to probe with a scrollLeft(2) in findScrollContainer.\nI'll put together a commit.\n. Chrome requires a scrollLeft probe of at least 4 to be detected at the smallest scale. Try it yourself with this in Chrome's console.\nfor(i=1; i<=10; i++){console.log([i, j=$(\"body\").scrollLeft(i).scrollLeft(), j?'':'trouble']);}\n. I don't doubt that the problem is in how the particular browser chooses to interact with the particular window system and ultimately how that gets represented back to javascript through the dom. There are a lot of moving parts here. No surprise that there are issues. I'm glad we have a variety of platforms between us. (I'm wondering how we can write an integration test for this. Doesn't sound easy.)\n. I've been wanting to add this feature to the sinatra server. After playing with it this morning, I'm thinking I'm happier with the information buried in the html as a comment. Since it isn't so easy to click there, I've gone ahead and pulled the one-line commit message for the last ten commits. See 0755160205a97b4f9073551b08f9f3a1a049ee4b.\n. How about we embed this git information in an html comment, not in a visible link?\nIt might take some experimenting to figure out what kind of links are and aren't clickable in show-source. I was waiting to pull this until I had some time to match it in sinatra. But I see there is another commit I don't want to hold up.\n. Can we configure the git repo so that it would be a legitimate static site as is? This would include an index.html at the top level that pulls in enough javascript to get things going.\nCould we keep such a repo configuration and be able to launch either server and have it serve effectively without apache present?\nWould such a configuration be compatible with ruby and/or note package managers? \n. Yes, you can serve content into the federation serving only static .json pages that you prepare by some other means. Here is a GitHub Repo for perl scripts I use to translate Eric Evans' Domain Driven Design patterns from a word file into static .json files.\nYou would need to do more if you want to be an origin server: things like serve up the javascript, css, and starter html. There could be limitations for drag-and-drop of static pages into the factory plugin. Another origin server assumption is the ability to click a page's icon and get to its origin server. No origin, where does that go? It might be possible to configure Apache to do all that is required. A writeup on how to do that would be awesome.\nYou have to do even more if you want to support editing. Maybe supporting editing it no important in some situations.\nIt would be cool if the flat-file repository format were Apache compatible so that one could edit privately and then push static content to a normal web server. \n. A while back I drafted some life-cycle scenarios in Federation Details in the GitHub wiki.\n. Here is a simple path. Say I go to the url:\nhttp://my-server.com/view/foo/your-server.com/bar\nThen I will see two pages, foo from my server and bar from your server. If I edit bar, then bar gets written to my server as mine and has a link in the journal to your bar which is my \"attribution\".\nIssue #70 discusses more ways to find pages distributed on multiple sites.\n. Yes. Here is a federated wiki site that is rebuilt every five minutes with a cron script:\nhttp://sensors.c2.com/\nNotice that it doesn't have a login. Its read only, meant to feed into other people's wikis. However you can still edit it and your changes will be stored in the browser. \n. Code was added to the client to decode those multi-page urls. I'm guessing that there is some configuration detail that was never documented. I'd love for you to figure this out and document it. I'd also forgive you if you launched one of the interactive servers just to keep your project simple.\n. Is that all it takes to make a read-only (publishing) server. I notice that my sensors.c2.com does not handle browser history correctly. That could be from using old javascript. Does everything work for yours? You've chosen a good place to document it. You might add a SHA for the javacript version you've tested. We're a moving target and not mechanically testing code in the GitHub wiki.\nA bigger step would be to support a read-write server in PHP. The strategy here would be to get the integration tests running and then add PHP code until they are all green. This would imply you were supporting the evolving client-server protocol as well as the much simpler federation protocol.\n. 1) This video shows a variety of ways to link. \nhttps://vimeo.com/36384739\nThe drag-and-drop (fragile for now) gets two sites connected. Then fork-on-edit spreads links throughout the reader's site-of-origin.\n2) I'm thinking that inert content (json) will move through the federation faster than active plugins (javascript). Right now we leave it to site operators to install plugins that they have inspected and approved. GitHub clones of this site will be the preferred medium of exchange for now. (@nrn has a mod that links each site to its repo.) When you use custom javascript to render inert content you may have chosen the slow road to sharing.\n2c) I would like to speed the distribution of plugins. I look to Caja or even more advanced security mechanisms to make this safe without having programmers checking the code each step along the way.\n. At this point we've shown that a variety of static page read-only sites can interact well with federated wiki. With respect to ongoing development I suggest we continue our focus on the two server implementations we have. \n. I wonder if dataDash could collect some statistics on how data attributes are used in practice?\nOne advantage of MVC frameworks is that the models give you a sense of what you have to work with when adding features. Perhaps dataDash could serve a similar role by accumulating stats that would be visible in Firebug or Selenium. I'd even be willing to provide an optional string argument as a comment if it would make a dataDash report more useful.\nI'm thinking this would just be a hash of objects holding counts and other things that might be easy (or fast) to collect. For example: \n{\n  \"id\": {\"set\": 145, \"get\": 650, \"on\": [\"div\", \"p\"], \"nb\": \"random key, usually 8 bytes of hex\"},\n  \"site\": { ...\n}\n. Yes. I'd be happy with console.log(dataDash.stats()) too.\n. I haven't been able to get this to run in my Sinatra environment. A bit of this is due to my confusion working with remote branches in git. I think I have that right now. I'm guessing that the client code has taken on a new dependency that I'm not yet meeting from the Sinatra side. The test suite says:\nFinished in 25.74 seconds\n51 examples, 16 failures\nI'm guessing that this requires a simple edit in layout. It would be awesome if you'd make that edit and double check it running the Sinatra version of the integration tests. Thanks.\n. I'm still struggling with this. Tests run green here now but chart and d3 plugins fail with undefineds. Probably need some mechanism to include tests with plugins and run them automatically. I'm going to have to back off on this pull for now until I can think up a validation strategy.\nNow might be a good time for you to write some programmer documentation for the library. I couldn't understand what you were doing with the journal hovering (6ea6a61) so I'm still shy digging in and fixing things myself. \n. I can't say if your on to something with dataDash but I'm sure willing to give it a try. The stats() seems like a big reward and that comes largely from going through a standard interface that is under our control. It might be interesting to float your approach past the jQuery community for comments. Here's a thread that might be related.\nThe lack of plugin test coverage is a train wreck waiting to happen. Accept my apology there. I'd like to see plugin documentation, examples and test cases included in a plugin package. I think I'll ping @stephenjudkins and see if he wants to help me out there where he's been so valuable before.\n. Worked hard on plugin issues Saturday: a2f012f3d2720767bfbeeb141b8c7e55031438dd. Didn't get to writing plugin tests. Have spent today looking into chrome's profiling capabilities. Looking for a way to feel good about this. \n. Wow. That's awesome.\nI've hacked the sensors.c2.com build script to make better image links. I was just getting broken windows because it had site-relative image links. (We've dogged a lot of asset management complexity by using data urls but I haven't used them in this case.)\n. I once thought that if you called up the same page from the same server twice then the first copy should just move to the expected position for the second. You've shown that doing a simpler thing (just having multiple copies) is valuable. \n. I've enjoyed reading these commits but must admit that I lost track of what is going on around line 450. It might just be late. Do the dataDash stats look reasonable?\n. I had some time on saturday to try out this branch. It seemed to work fine with the node server but didn't work at all with the ruby server. Is there something more that ruby must do?\nThe symptom was that every page name turned to undefined. No javascript errors. The wiki.log trace showed undefined in the very first trace line. Have to revert to the main branch. Sorry I couldn't debug this further.\n. Works lots better. Found a couple of things. \n- The colorful icon in a FederatedWiki plugin don't work. Here is what I'm talking about: http://ward.fed.wiki.org/view/federation\n- The specs that go straight to the ruby server are looking for id. Should be simple update.\n. Those federatedWiki citations have two links in one. The blue word goes through the usual search of appropriate servers starting with origin. The little icon however goes straight to that server skipping the search.\nI notice that there are some .data calls in the federatedWiki plugin.\n. Its good to get caught up. Haven't seen anything break with this latest fix. (except the latest commits on master.)\n. I had another go at this merge this morning. I see the same symptoms I described in this comment above: all pages look to be named \"undefined\".\nI checked that the divs that Sinatra generated include data-slug so I know the fix you put into the Sinatra server is working. I've tried every way to be sure Chrome is reloading all the Javascript. It does.\nI checked out the code with these commands:\ngit fetch nrn\ngit checkout nrn/data-test\nFollowed by restarting the server. \nFirefox seems to work just fine in this configuration so there is something complex going on with Chrome. I'm going to have to back out and do some other work but would appreciate suggestions.\n. I put two hours into single-stepping and can't figure out what's wrong here. Sorry.\n. I really appreciate your patience. It is with intense pleasure that I finally merge these commits. Not only have you kept up with my exploratory visualization code that I've been dropping in here without any explanation, you've gently improved the parts where I didn't have time to think it through carefully. DataDash.js and D3.js will make quite a pair. \n. Thanks.\nThere remains a mystery.\nI'll call it the \"all pages go to empty\" mystery. We've seen that it is associated with a particular server, say localhost:1111, but does not affect other sites in the same farm, such as batch.localhost:1111.\nJust now I've seen the mysterious behavior return, without provocation, to the one site on fed.wiki.org that I was planning to demo. No changes. Stops working.\nI tell you this because I found a way to clear the condition, at least for now.\nI went into Chrome's Clear Browsing Data screen and selected:\n- Clear download history\n- Empty the cache\nIt worked. Behavior returns to normal. I chose these because they sounded like the sort of thing that might confuse Chrome if unwanted entries got stored somehow. Previously I'd learned that opening a new tab or even restarting the browser had no corrective effect.\nNext time this happens I will clear one or the other store so as to narrow down the culprit. It might be good to think about what sort of trap code could be added early to the javascript. Recompiling javascript didn't help but reverting from these commits did help. I'm already hooked on the better history and data binding behavior so I'm not considering going back. But where there is mystery, there is often trouble.\n. I just had \"all pages go empty\" on another site. This time I cleared the problem by selecting only \"Empty the cache\". I have not correlated this to any particular event. I was simply calling up a familiar federated wiki site by typing its domain name in Chrome's search/location bar. It came up in the malfunctioning mode.\n. I'm open to suggestions for things to try next time it happens to me.\nI'm guessing its going to happen once or twice a day.\nI tried googling for \"chrome cache clear fixes bug\" and found this note. There might be other posts with more hints. One challenge is having enough of a hint as to what is going on such that one can formulate a good google query.\n- http://groups.google.com/a/googleproductforums.com/forum/#!category-topic/chrome/report-a-problem-and-get-troubleshooting-help/Tta0Xe5AxJI\n. Well, I noticed it again when visiting a farm site on fed.wiki.org that I had not visited since before the pulling our most recent version to that server. The failure scenario must be:\n- visit site before updating client.js\n- update server with new client.js, restart the server\n- visit site after the update, observe failure due to old client.js\nNow, where is the old client code beging cached?\n- I knew to shift-refresh the page. No help.\n- I watched the traffic in Chrome's inspector. Status 200 implied reload.\n- Stopping and starting Chrome made no difference.\n- But, clearing Chrome's cache fixes the problem\nI'm not claiming to know the answer. I'm just reiterating observations from memory. (Real testers keep notes.)\n. If we got rid of the recursive web service calls then we could go back to thin. Is he volunteering to help?\n. After merging this code yesterday we found an issue in drag-and-drop between pages that needs more work. I've moved the merged code to a data-test branch in this repository. I've also restored the unmerged code as master and added a little fix that handles an unwanted extra array wrapped around items that might have been moved. Here is the procedure I used to make this change:\n- https://gist.github.com/1195816\n- http://stackoverflow.com/questions/5916329/cleanup-git-master-branch-and-move-some-commit-to-new-branch\nIt is possible that forks of this repository will have to make some adjustments. We're here to help.\n@nrn and I have chatted this over by phone and will get his hard work integrated as soon as possible, probably after a week of travel and talks for me next week. \n. I found it necessary to tune up some of my local repositories after changes here. Here is what I did to get my local repository back in sync:\ngit branch -m master troubled-master\ngit checkout origin/master -b master\nThis sets the old master aside and creates a new local master that is in sync with the GitHub repo. Sorry for the inconvenience.\n. I've published the script I used to create the Domain Driven Design site: ddd.fed.wiki.org. You may find the approach useful. If you write a more general purpose converter for UseMod wiki then consider distributing it the same way. \n. That is right for now. Simple.\nThere is now a journal action of type == create that might be a nice place to put information about where the data came from. I'm open to suggestions as to what might be useful.\n- Original UseMod wiki url?\n- Url of converter git repo?\n- Original UseMod markup?\n- Initial output from converter? (for ultimate reconstruction from the journal alone.) \n. I'd like to keep the markup extremely simple and then use plugins to do more exotic things like LaTex. Federated wiki has very simple [[ ]] and [ ] link syntax similar to, but much simpler than, Mediawiki. This has been discussed in issue #140.\nI'd like for basic paragraphs to not recognize any html tags. I suggest a html subset be the subject of a new plugin for the cases where html has just-fine solutions.\nFederated wiki wants to do the page formatting itself. If users format whole pages then they will not move through the federation in a frictionless way.\nMy plan is to eventually move my original wiki to the federated form. I look forward to learning from your experience.\n. Cool. I enjoyed reading your code though there are parts I didn't understand not writing a lot of python.\nYou might try chopping your input pages into smaller objects. The drag and drop refactoring works best if every textual paragraph is a story paragraph. I even put heading into their own paragraphs so I can move them freely. With smaller pages you will be happier if you just ignore some page level markup.\n. Ah, I didn't quite get how the splitter worked. Thanks for explaining.\nBe careful with deeply nested outlines. One early decision I made for better or worse was to prescribe a narrow format. Nested outlines are better in a wide format. I've stopped using bullets choosing one line paragraphs instead. They are easy to rearrange and when there would logically be something nested under a line (a sub-bullet) I just link to a new page. Since they come up side by side I have all that context keeping that one expects from a good outline.\nAnother choice would be to translate your outlines to an outline thumbnail leaving room on my narrow pages for the occasional stray paragraph that introduces or summaries the content. When you tap the thumbnail, the outline takes over the whole screen. This is what I do with data. You could make d3 visualizations of your outline too. \n. I've created a GitHub wiki page for collecting examples of this kind of project.\n. I've created a GitHub wiki page for collecting batch conversion examples.\n. We have not yet attempted to have a comprehensive command interface. Our strategy is to do this first for touch-based devices and then back port to the mouse-based devices. Some commands, like delete, are simply absent.\nI have started a federated wiki site with some how-to information. This should be inherited into all new wikis so that it is close at hand from the Welcome Visitors page. If that hasn't worked for you then you can see it here:\n- http://fed.wiki.org/view/welcome-visitors/view/how-to-wiki/view/remove-paragraphs\n. The current page tilte => slug code does show up in a variety of places. This is bad news because it is known to be inadequate in several ways and correcting it will be a conversion struggle. Two things could help:\n1. A clean and simple reference implementation.\n2. Test cases that demonstrate all the features of that implementation\nIt would be a real contribution to this project if you (anyone) were to isolate the extent versions and develop a test suite that demonstrates their features and limitations. This could be the basis of ongoing discussion of a critical issue.\nHere is a template in coffeescript could belong in the /spec directory:\n``` coffeescript\nasSlug = (name) ->\n  name.replace(/\\s/g, '-').replace(/[^A-Za-z0-9-]/g, '').toLowerCase()\nsection = (comment) ->\n  console.log \"\\n\\t#{comment}\\n\"\ntest = (given, expected) ->\n  actual = asSlug given\n  console.log if actual == expected then \"OK\\t#{given}\" else \"YIKES\\t#{given} => #{actual}, not #{expected} as expected\"\nthe following test cases presume to be implementation language agnostic\nperhaps they should be included from a common file\nsection 'case and hyphen insensitive'\ntest 'Welcome Visitors', 'welcome-visitors'\ntest 'welcome visitors', 'welcome-visitors'\ntest 'Welcome-visitors', 'welcome-visitors'\nsection 'numbers and punctuation'\ntest '2012 Report', '2012-report'\ntest 'Ward\\'s Wiki', 'wards-wiki'\nsection 'foreign language'\ntest 'Les Mis\u00e9rables', 'les-mis\u00e9rables'\n```\n. Just pushed this code as 04d6e61. Couldn't help but add a few more cases.\n. @GerryG has tracked down some useful resources with respect to diacritical marks in unicode and how they might be handled in slug formation. \n. Our intention is to provide convenient utility functions to do simple wiki functions like edit text or resolving links. The image type applies these to the caption.\nYour plugin is free to interpret additional fields of the story item which might give you some of the control you desire. You could say, for example:\n{ \"type\": \"wikish\", \"option\": \"paragraph\" ... }\nThe option fields will be preserved through move and edit actions.\nI would be wary of mixing any combination of types since since this creates a combinatorial list of cases to be tested. When plugins come from different implementers it is not clear who is responsible for that testing.\n. (oops. bumped close.)\nI've stored changes as deltas going forward, not backwards. This supports merge-if-possible interactions that will become part of the total federated experience. I know this is hard to do reliably. Perhaps un-merged actions will show red.\nWith a complete history it is still possible to retrieve all versions by playing the actions forward. It might be interesting to try this with the actions as we currently store them. If fork saved the copy that was forked, this means one only has to go back to the last fork. With paragraph content reconstructed before an edit action one could float a quick-diff over the icon [e] in the Journal. That would be cool.\nHow to quick-diff:\nFind the first and last changed character. This defines the span of removed and inserted text. If we classify each of these spans as empty (= 0 chars), short (<20 chars), long (>20 chars), or, complete (= whole text), then case analysis will suggest what sort of mini-report might be floated over the [e] on hover. Hint: long spans can be usefully elided.\n. We've had revision viewing for a long time. My quick-diff suggestion is probably not as desirable as more recent suggestions. So it would seem overdue time to close this issue.\n. I've intended that hover over actions would show you a diff of what changed with that action and that then clicking that action would access the page as it existed when that action completed. There are many usability considerations here that should be consistent within the whole experience including touch-based interfaces. A tall order for sure.\n. All sounds reasonable. Perhaps the previous state page could be instantiated in Local Storage where it could persist.\n. This sounds like a big job. Don't go too far down this path without letting us help. \n. @hallahan implemented this feature pretty much as he described. I will close this issue but do recognize that there is still some functionality one might expect that is missing, specifically more ways to take advantage of the history we have in place.\n. I can imagine many kinds of housecleaning applied to the Journal. For example, if you move a paragraph and then move it back, maybe that doesn't amount to history worth keeping.\nA more agressive cleaning could include removing all but fork actions up to the most recent fork. Link resolution looks at those forks to get hints as to were to find pages so you would want to keep at least one from each site. Even forks can build up in the back and forth of an explicit conversation. Here's a lame attempt at conversation Craig Muth and I had a while back:\nhttp://ward.fed.wiki.org/view/conversation/craig.fed.wiki.org/conversation\nLots of forking going on there. \n. There is not currently much value in saving multiple fork actions from the same remote server. They are identical. One strategy might be to keep only the most recent fork action for all remotes contributing to the wiki page.\nOn the other hand, I've suggested in issue #118 that we should have more information in those forks. I have trouble adding information that I'm not using. I'd love to see some basic implementation that could use remote seq info stored in a fork action to bring a current page up to date with changes on a remote site. \n. A failed drop creates a type == data item with some of the information from the drop interface recorded in the item. Double-click the data item to see what the system/browser have to offer.\n. When there is a right way, and its so economical, might as well do it right. Thanks for showing me how. \n. It sounds like this issue is closed. \n. Be sure to tell us what server you are running and how you have it configured.\nEdits happen in a sequence. It goes like this: something happens on the screen => the dom is changed and events fire => javascript assembles an Action describing that change and sends it to the server => the server changes persistent state and return the action to the client => the client posts the action to the journal.\nWhen you move a paragraph do you see an [m] appear in the journal? \nHas the move been persisted when you refresh the web page?\nHave you looked at the flat-file database in /data/pages? \n. The current behavior (treating local storage as a different and private site) is a compromise based on our limited understanding of what would be desirable as part of a funnel of engagement. I've described the desired high-level behavior in the pattern Have Fun First:\n- http://ward.fed.wiki.org/view/patterns/view/have-fun-first\nThe pattern seems to call for some way to move some or all pages between sites. The design of that mechanism would need to work within the constraints of browser privacy protections. One thought would be a special link that would produce the whole site as a single JSON file.\n. Currently local storage pages are rendered with a yellowish tint in the background. Local storage will be easier to understand when it is made to be more like any other server, at least as much as possible.\n. I'm guessing you would be happy if there were a simple way to see your Local Edits after you log into your server AND a simple way to push some or all of the edits to that server which you thought you were editing in the first place.\nThis could be as simple as not hiding Local Edits when your're loged in AND handling the Command-S key at the wiki page level to initiate a save to the server.\n. There is a changes plugin that shows Local Storage changes and a [[Recent Changes]] page recently added with commit efdcaf718b21db39d1da71cd3827eed6fc32bb16 that shows all changes on a server.\nThere has been discussion suggesting that both of these mechanisms should be merged into a more general capability.\n. I'm guessing we've finished this conversation. \n. This behavior is described in the help pages:\n- http://fed.wiki.org/view/how-to-wiki/view/add-weird-characters\n. Thanks for the tips. We've been cautioned that CORS has security implications though far fewer than JSONP. We use CORS to read from remote sites. I can imagine that the situation is far more complex while writing to remote sites. \n. If you are moving json around by hand you may have missed some punctuation. Google JSON Lint for a friendly checker. You might also look at what is going on with Firebug.\n. Hmm. That is a more esoteric feature and one we don't have integration tests for yet. Are you running the sinatra or the express server?\nI mention this because we use the integration tests to keep the two implementations synchronized.\n. We're talking about the factory code? \n. I'd like to get the meta-factory working together with plugin-bootloaders (coming feature) to recognize when something is dropped that can be read and interpreted. The current case analysis in meta-factory can't grow much bigger without becoming too hard to comprehend. And we've reached this situation when we are handling only a small portion of the drops.\nNick, you're detective extraordinaire for tracking down all three conditions. Let's start with 1 and then document here some cases that work. I know Crome on OSX is what I used in the number 15 video. I've also seen Firefox on OSX work too (with slightly different event.dataTransfer content.)\n. We've had this working well for some time now. We can always handle more drag-and-drop cases but the comments here aren't going to move that forward.\n. Yes.\nYou probably want to interact with item the second argument passed in to emit and bind.\nCommit 655717628eb66dfc6838763543ffc83d6261e808 was the beginning of an experiment along these lines. Here is what the calculator looks like in use: http://ward.fed.wiki.org/view/expense-calculator\n. These are all good ideas.\nOf course federated wiki integrates with everything else on the web through hyperlinks. I've also shown that the simplicity of JSON makes federated wiki easy from the back end. For example, my home sensor network updates an early federated wiki site every 5 minutes. \nhttp://sensors.c2.com/\nIf you are considering undertaking any of these integrations then I'm happy to advise further. \n. Yes, this behavior in the TextEditor is over due for reexamination.\n. Issue #70 discusses incremental search of a neighborhood. This would be titles only. \nFull text search could be delegated to google. \n. Sounds awesome. Much better than drag-and-drop method.\n(I'm too tired to think about code tonight. More later.)\nOn Apr 4, 2012, at 12:49 PM, Nick Niemeir wrote:\n\nSomething I came up with during our discussion today, a bookmarklet that will let you bring the pages you are looking at back to your home server, rebasing you on that root url.\nRebase; for (var i = 0; i < stuff.length; i += 1){x = stuff[i]; if (x == 'view') x = host; if (x == home) x = 'view'; url.push(x);}; document.location = 'http://' + home + url.join('/');})();)\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/issues/175\n. Maybe when you visit a page in someone else's recent changes it just does it without saying much.\n\nRecent changes could explain the behavior and offer the bookmark for other situations.\nOn Apr 4, 2012, at 9:21 PM, Nick Niemeir wrote:\n\nSorry, this doesn't actually implement merging, or streamline localhost (both of which were big topics at today's hangout).\nWhat this does is let you setup a bookmark that when clicked opens the same pages you had open, but with your home server open instead of whatever server you happen to be on.  For example if I was browsing new.fed.wiki.org at say:\nhttp://new.fed.wiki.org/view/welcome-visitors/view/how-to-wiki/fed.wiki.org/add-text/nrn.io/indie-web-camp\nand then click on the browserlet i get redirected to:\nhttp://nrn.io/new.fed.wiki.org/welcome-visitors/new.fed.wiki.org/how-to-wiki/fed.wiki.org/add-text/view/indie-web-camp\nSo now I've got my version of the client javascript, plugins, etc.  And if I'm logged in I can edit things and they get persisted back to my server instead of local storage associated with the other site.  You can see the 'view's in the original url became 'new.fed.wiki.org' in the new one, and 'nrn.io' in the old one becomes 'view' in the new one.  Unrelated server 'fed.wiki.org' stays the same.\nThis seems to make for a pretty nice workflow.  The bookmarklet works well, but i'm thinking about how to put this in the page, i'm not sure what the clean way to do that would be though.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/issues/175#issuecomment-4968315\n. p.s. sounds like you were productive when I wasn't around to explain my thought on everything first. \n\nOn Apr 4, 2012, at 9:21 PM, Nick Niemeir wrote:\n\nSorry, this doesn't actually implement merging, or streamline localhost (both of which were big topics at today's hangout).\nWhat this does is let you setup a bookmark that when clicked opens the same pages you had open, but with your home server open instead of whatever server you happen to be on.  For example if I was browsing new.fed.wiki.org at say:\nhttp://new.fed.wiki.org/view/welcome-visitors/view/how-to-wiki/fed.wiki.org/add-text/nrn.io/indie-web-camp\nand then click on the browserlet i get redirected to:\nhttp://nrn.io/new.fed.wiki.org/welcome-visitors/new.fed.wiki.org/how-to-wiki/fed.wiki.org/add-text/view/indie-web-camp\nSo now I've got my version of the client javascript, plugins, etc.  And if I'm logged in I can edit things and they get persisted back to my server instead of local storage associated with the other site.  You can see the 'view's in the original url became 'new.fed.wiki.org' in the new one, and 'nrn.io' in the old one becomes 'view' in the new one.  Unrelated server 'fed.wiki.org' stays the same.\nThis seems to make for a pretty nice workflow.  The bookmarklet works well, but i'm thinking about how to put this in the page, i'm not sure what the clean way to do that would be though.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/issues/175#issuecomment-4968315\n. On Apr 4, 2012, at 3:38 PM, Joseph Bergin wrote:\nIn writing an essay in SFW just now, I found myself wanting some things. I recognize that these may be hard or even undesirable given federation, but nevertheless:\n(a) Ability to change the title of a page easily. Authors mis-spell things and clean-up is hard if the error isn't caught immediately. This could cause problems with federation, I think. \n\ndoable but would have to leave forwarding \"redirects\" behind.\n\n(b) Ability to merge pages into one. Yes, it is possible with possibly a lot of dragging. If it were possible to select a set of paragraphs for dragging, rather than just one, it would probably be enough -- maybe even better.\n\npossible but has usability considerations. do you have a suggested interface?\n\n(c) Ability to easily split pages into two. Same issue. Currently requires a lot of dragging. But the select-multiple-paragraphs eases this also. \n\nsame as (b)\n\n(d) Ability to easily split a paragraph into two adjacent paragraphs. Merging adjacent paragraphs is easy enough. But now, to split you need to drag the new paragraph in to place as well as adding the text.\n\nthought has been that typing a blank line (\\n\\n) will split a paragraph. blank line at end of paragraph just starts new paragraph. might be obnoxious without undo.\n. I see that the server-side handling of the fork action depends on the forked page being on the public Internet.\nFork could get the page from the client instead. This might be helpful publishing pages from Local Storage too. \nFrom behind a firewall one could still simulate the request stream of a user writing the site from scratch. Yuck. \n. We should be able to pass around files that contain a number of pages. Let us assume that the file format is itself a JSON with { slug : page } as its basic schema. I see two interesting questions:\n1. How are they written?\n2. How are they loaded?\nBatches of pages could be written by bulk converters. That is Joe's use case. They could also be written from a whole site that is to be moved, or some portion of Recent Changes, or some of the Local Edits when someone is ready to share them. Bulk converters can write where ever they want. Writing from a site or especially Local Edits is tricky. Perhaps composing a Data-URL with the desired content and letting the browser pop up a Save As dialog it possible.\nLoading from a drop on a factory sounds expeditions but not exactly consistent with the pluri-potent paragraph notion of factory. Joe is suggestion a smaller drop above but not just a single paragraph or he could just paste it. This also raises the question where does the factory appear? If its on the Welcome Visitors page you could easily overwrite that page with new content. What does this even mean? Merge?\n. Thanks for clarifying. I knew I wasn't exactly echoing what Joe was suggesting.\nWhat I find attractive about the possibility is that it could be a good approach to ad-hoc manipulation of datasets: grab some json, manipulate it in your favorite tool, drop it back into wiki.\nOf course this breaks the revision history because the manipulation is outside the system. Such breaks are inevitable. JSON encourages such manipulations. I find myself in data/pages with my text editor plenty often. This just gives wiki a chance to participate and record that a break has happened.\n. The slug gives us some freedom to change names. Any name that produces the same slug is equivalent. You could edit the capitalization and some punctuation in a title without changing anything else.\nYou could also replace the original page with a forwarding hint: \"This page was renamed to [[Foo Bar]] on April 13th, 2012\"\nWikipedia's article namespace is something like 40% redirects. They handle these automatically when visited. There are lots of knotty little problems here arising from inconsistencies and loops.\nWe can live without automatic redirects and all the nasty issues by keeping the reader in the loop. This is not the same disservice to the user since the referencing page is still onscreen. \n. I don't know the etymology of slug. I learned it on the job. \nI would like to modify the algorithm so there is never a hyphen at the beginning or end of a slug and never two in a row. \nIn my wildest dreams I'd like to be able to change Welcome Visitors to Bienvenue aux Visiteurs and have the slug stay welcome-visitors.\n. Is there an easy way to strip accents from latin characters? Maybe with a regex or something equally common and well supported?\n. I suggest the slug formation conversation move to issue #156. Thanks. \n. I look forward to seeing Joe's code on github. My specific suggestion to him was:\nSave edits after the last fork (right most, numerically higher array index)\nYou should keep the last fork for every unique site so that incremental search will know to look deeply into the federation.\nSave the last so that we know the order of the last touch from each distinct site.\nMore discussion of cleaning history should continue on Issue #160.\n. Try [[Recent Changes]] \n. Alternative implementations of Recent Changes are discussed in Issue #165. Issue #165 discusses other ways that the client can benefit by knowing more about all pages on a variety of servers. \n. 9656890bfa3330443d2cf873ec0ebd6b6f5f347e adds the full action type name on hover. Issue #159 suggests more interesting behavior. Issue #118 talks about timestamps.\n. There are plenty of wikis that work that way. There are videos that explain the motivation for federation.\n. Do you know why the image was not loaded? Was it a user choice or a timeout?\n. Related discussion in Issues #73 and #77. \n. Is there any way to detect this circumstance client-side so that the unwanted store can be suppressed?\n. We have a few cases where communication failures between the client and the server lead to the server inadvertently destroying data. I'm thinking that the most severe damage can be prevented by distinguishing a CREATE from a PUT in these situations. A misinformed PUT destroys data. A misinformed CREATE throws an error.\nI'd like to see us support both operations. I still have aspirations to build a flag chooser (Mentioned in issue #77. I currently use repeated calls on random.png) \n. I tried http://jsonlint.com/ and got a slightly better error message.\nYou can put the two character sequence \\n into a JSON string, but a string can't be split across lines. (This is a Java convention that Javascript adopted.)\n. I was assuming that the server did trap and rescue and that the error message that it printed in the process was confusing. Are you saying that the server failed to respond to unrelated requests after printing the message?\n. I'll accept that.\nOur aspiration for exception handling is described in the Predicted Results pattern, near the bottom.\nThe pattern is mostly concerned with gracefully engaging an ornery network but I'd say an ornery database falls into the same category. Since this is already among the project goals I will close this issue now. Thanks.\n. I've untwisted much of the client code (now in legacy.coffee) and fixed a few bugs too.\nmade refresh small enough to understand\npass item forward to buildPage through the dom\ncollected like methods together, improving comments\nfixed bug that led to out of order fetch resolution\n. I've moved the code to a branch named modules. I've messed up my models branch and can't push it.\n. On Apr 11, 2012, at 12:12 PM, Nick Niemeir wrote:\nYes, please. A fresh pull request would be awesome. Sorry to have done this to you again.\nTesting is getting easier. I didn't notice if you were testing but let's figure out how to do a little more with mocha. Maybe we need jQuery in that page load?\nI'm especially liking mocha now that I have it hooked into our integration tests so I won't forget to check.\n. Do we have any reason to believe that playing the journal will be slow? \nA bigger issue is that the journal is unreliable, as is anything retrieved from a foreign site. Other issues here are discussing what sort of journal revision would be possible and/or appropriate.\nI'm guessing even long journals could be played forward quickly with a modest amount of code. This code already exists on the server otherwise one couldn't create files. It would be interesting to reconstruct a page every time it is fetched and then mark the paragraphs that can't be reconstructed from the journal. \nIf you run a clean site, that is one that doesn't tamper with your own journal entries, then you should expect to play actions forward to merge revised versions from a forked-from source. That is the use-case that motivated the current structure.\nImages and datasets are stored in the page to make the page whole. It would be possible to write a server that split images out of the page and manage them as one often does with static assets.\n. I'll look at this soon. Nick and I had a coding collision last night that needs to be sorted out too. You're working in areas that we've both moved around. That's not a serious problem but the pull might not go in smoothly. Sorry for the inconvenience. \n. Yes, there is a major remodeling in progress.\nNow is a good time to try coding in the new space. Let us know if you find it better or worse. Or if you find it unchanged but have ideas about how the parts you touch could be factors, now's a great time to speak up.\n. @JoeBergin has had his original two requests satisfied. I will close this issue, but not without noting that we've addressed some other issues too.\nRegarding SEO, a page can exist as a server-side rendered page that can be browsed without javascript enabled. This will even work with text-based browsers like Lynx. An easy way to get the html link to a page is to \"Copy Link Address\" from the domain name link at the bottom of every page.\n. I found a list of options in a source file that looks readable. In fact, I bet there is some way to get the server to print this out nicely at the command line.\nThe sinatra server doesn't yet have any command line options. However, if you have data/farm directory then it will create sites on demand and save them under virtual host names in the farm directory. I make new virtual hosts on my laptop by editing /etc/hosts.\n. Allow me to close this issue. Thanks.\n. I'm excited to see this work proceed. Some small points:\n- local-identity must be preserved long-term as it is the claim that is made on a site.\n- NoSql is a natural store. Most access is load and store documents by key. \n- But every now and then we ask for all keys and last modification time.\n- the obvious recurse call is in the handing of 'remote' requests as a proxy\n- the page/status structure gets replicated when you run a farm\n. Cool. I added a couple of paragraphs. Editing works fine. How about a we have a free-for-all up there knowing that it will all go away?\n. Regarding remote requests, I've been tempted to add the short-circuit logic but stopped myself on philosophical grounds, at least until we've exhausted alternatives.\nExperts have confirmed that this is a well known problem.\nI've reverted to Webrick when I run farms. It does not exhibit this behavior.\nNode.js does not exhibit the problem either. \n. I'm not sure we need to support farms on Heroku. Heroku is already a farm.\n. One advantage of not having a farm is that the Heroku instance supports only one user.\nIs there some simple way to identify the owner of the Heroku instance from information in their web requests? If so we could dispense with the claim mechanism which requires setting up (or having) an OpenID from another source. \n. I was hoping that we could abandon OpenID when the user clearly has ownership of the Heroku instance. I guess it is still required to \"share a secret\" between client and server somehow. I'm having trouble thinking of any more convenient way.\n. Thanks for taking persistance on.\nIn our modularization efforts we will want to end up where Local Storage in the browser, flat files on a personal laptop, and recognized document database in the cloud are captured as variations on a theme. The strengths of each are as follows:\n- Browser Local Storage -- Always there and writable.\n- Laptop Flat Files -- Ideal for experimental bulk conversions.\n- Document DB in the Cloud -- Easy to own and always public.\nOnce we have these tiers normalized and robust we can attend to the problem of moving content en mass between tiers. That may be too much to think about now. Better to get robust and then consistent first. \n. Happy to stay with OpenID now. It's working and there is a new generation of web login coming up behind it.\n. Caution: I was experiencing some problem with /global-changes.json where an interaction with the page module caused copies of pages to be stored one level up from where they were meant to be stored. And this was when READING the pages!! That's why I commented out the code until I could think through the path name construction.\n. You'll find awkward code around data_root and page that became that way early in development of rspec tests. On the server side alone there are three paths to storage, not counting your new work:\n- pages in /data/page\n- pages in /data/farm/*/data/page\n- pages in /spec/data/pages\nAlso note that Page is more an abstraction of storage than an abstraction of a page.\nThe mechanisms employed to effect these choices have accreted under various circumstances and are overdue for a rethinking. I mention Local Storage because we have the same sort of accretion present on the client side. When you refactor to add another branch to this choice tree you will face the question of where/how to encode the choice and whether to dig into the artificial complexity that exists there now.\nI wish I could say that we left you a better place to work. It is what it is. Don't feel any need to take on more of this refactoring than necessary either. But if you do go after a deeper refactoring, be aware of all the cases and the ultimate desire to have some alignment of abstractions between client and server.\n(@nrn has rethought some of this in the express implementation too.)\n. There is a little bit of logic that gets turned on or off by the farm logic. For example if you direct example.com and www.example.com to the same server, then as a farm it would be two sites, while without farm they would be one in the same.\n. If someone has been thinking of digging into Harlan's work on CouchDB, reading through this issue and beefing up the ReadMe would be a good way to start.\n. Boy, it is too late in the day to read those docs. One simple solution is to not run farms on Heroku. We're not yet running them on EC2 to avoid these issues there too.\n. Are we there yet? Maybe a step-by-step for those unfamiliar with Heroku. It doen't have to be fancy. Here's what I wrote for EC2: http://ward.c2.com/view/welcome-visitors/view/sfw-on-ec2\n. You were looking for http://sandbox.fed.wiki.org/view/welcome-visitors.\nUse  view source to see what commits are running on that server. You may need to hard-refresh the source view to overcome browser caching. \n. We're exploring new build procedures so that we feel free to modularize the code base. I wish we had updated the ReadMe at least to describe a workable build process that works for both servers and all platforms and coding environments.\nBuilder.pl is a script I wrote to help me through the reorganization. Its mac only so not suitable for the above goals. I will continue to adjust it to meet my needs and then look for a portable way to achieve the desired workflow from a position of experience. \nI'm sorry for the inconvenience. \n. Pull request #195 improves documentation to our current (admittedly modest) standard. The modularization work has created a breath of fresh air in the client-side codebase. I'm enjoying it. Thank you for your patience. \n. Welcome.\nI've read some paper on the darcs' theory but don't remember much. I just read the OT wikipedia article and find I will need some coaching to understand the variations. What's a good way forward?\nI've added move hover and click functionality to the journal so that we can understand by example what we're recording now. I've noticed that there are more cases than I'd realized even with our modest collection of actions. \n. I've pulled this, but still have some confusions.\nI'm suspicious that an edit that causes a fork might have the dates out of order. However, when I try to test this I can't seem to get a fork with a date on it at all. Its late, so it could just be me. \n. It could be a mistake like that. It was late. Have you seen it put timestamps on a fork?\n. Happy to have this on the agenda for the wednesday chat. \n. Wow. It's so neat to see that information surfaced. Nice work.\n. @hallahan and I pair-programmed the client-side version of this and committed it as 46b1755253c54f3d4287f65e119e3b04f5ece9d4.\n. Yes, it could be that simple. We could leave welcome-visitors in default-data/pages for now but copy it on creation instead of on demand if that is more convenient for the server implementation.\nThe rest of those pages should go on fed.wiki.org under the assumption I can keep it running.\nThere might be some value having help pages in default-data/pages so that they are in sync with a release but we don't do that now. \n. The data loss mechanism here is similar to that in issue #181. Same solution (defensive programming) probably applies.\n. IRC is now mentioned in the \"how to participate\" section of the primary ReadMe. \n. Sweet.\nBut I thought that git could handle file renames better than this. Did you use git mv fetch PageHandler?\n. I tried to merge this code today but ran into several problems.\n- won't create new page when referenced in url\n- won't complete creation of new paragraph when text dropped on factory\n- won't create new page when axis label of radar chart clicked\nI thought to look in some of these places after doing a global search for putAction, an api call that this commit renames.\n. Now I'm thinking the page creation logic may have gotten messed up with an earlier refactoring. \n. Ran a git-bisect and found that the creation logic problems are from this commit. \n. An interesting demo for sure. Thanks for putting that together.\nAllow me to think crazy for a moment:\nLet's say there is a page on your own federated wiki server that is engaged in a chat. That means that the javascript that has rendered items of type==chat might connect to the message reflector captured in the item to look for more content. When content comes up the stream, the plugin makes more items in your page to hold it. When you want to send content, you pick any item and edit it, probably by typing a blank line which will be interpreted as starting a new paragraph.\nSome things I like about this is that it isn't strictly chronological. You put your comments in the flow where they belong. You can also edit out the noise and keep the parts of the conversation that you find relevant to the subject: the title of the page. (But what does it mean to drag a chat item to another wiki page with another title?)\n. Happy to have this code in the project. Thanks so much for your efforts. \n. Darn. I didn't see eeaaf95 so I went and did it myself. Then this pull wouldn't close on its own. Now I see that you're way ahead of me and I've cluttered up the commit log. Oh well. Onward.\n. A few loose ends:\n- e0be7d5b726c821127b047040e2cfa215b73aabd -- exit after callback (fixes double-headers on local storage pages)\n- 03c68bef28277e1d6f13d955129a1f1c67d08f5f -- s/fetch/pageHander/ for fork actions in the journal\n. I had a feeling that there was a need for something like this after my own troubles with character encodings. Thanks for tracking this down.\n. I encourage you to publish your converter as a github repo and add it to the list of converters in this project's wiki.\n(Oops. I see you have the repo already. Thanks.)\nAlso, the conversation around issue #176 has turned to batch uploading, something you seem to have a handle on. Please consider some way to share your experience. \n. Hmm. I wonder if this isn't some bundler configuration problem. Removing the line would not show any problems until UTF-8 characters are encountered. If I read the error messages correctly, the server initialization code is having trouble saying \"UTF-8\".\nRuby UTF-8 experts, please help.\n. Ah, yes. I believe you have to run 1.9 in order to say \"UTF-8\". We have very few dependencies on 1.9 but this is one. Sorry to have not caught that for you immediately. Thank you for your persistence.\n. I cherry-picked the 2 small dev tool changes. I'll leave the rest open to discussion.\n. 32ca6b710310b934bbe51810a145d3213280c0c1 removes the unintentional header.\n. Try clicking the flag.\n. Recent Changes (changes plugin) has been working well for some time now. \n. On May 1, 2012, at 6:11 AM, Lev Eliezer Israel wrote:\n\nI'm working to get my head around the expected behavior of the federated wiki ecosystem, so as to decide if it is appropriate for a project I am working on. \nIf I understand https://github.com/WardCunningham/Smallest-Federated-Wiki/wiki/Federation-Details correctly:\nIf I edit a page I own, it is edited in place.\nIf I edit a page I don't own, then it is forked to the place that I own.\nIf edits by non owners create forks, then how can multiple people collaborate on a page on an ongoing basis?\n\nIt is easier to believe this can work if one stops thinking of collaborating on a page and thinks instead of collaborating on an idea.\nHere is an example where the collaborators don't even know the idea until they begin:\nhttp://dorkbotpdx.org/blog/breedx/exquisite_corpse\n. The sandbox works like a traditional wiki because the software does not yet know who owns the site. For this one instance I interfere with the site claiming process so that one can try making a claim. (In the sandbox claims don't last.)\nHowever, federated wiki has scant few mechanisms that support collaborative editing of a shared space. For shared editing of pages one can find excellent software elsewhere. For shared editing of ideas, what ever that means, one would be wise to explore here with us.\n. I'm not able to duplicate this error. There have been changes in how plugins are loaded. Also, factory.js takes special handling which could lead to confusion. In reverse:\n- The server constructs factory.js from the meta-factory.js file. During construction it inserts information about available plugins that the factory interface offers to the user. (this is actually a stub inside the server and not yet fully functional.)\n- The client now looks for plugins in two places: first as a subdirectory of client/plugins, then as a bare file in clients/plugins (the old way). The client expects and handles 404s to get to the later case.\nFrom the client side I can see 404 then 200 for factory.js using both the sinatra and the express servers. Can you see this behavior? \n. How are you starting the server? I'm using:\ncd server/express\nnpm start\nYou might try isolating the commit that causes the problem using git bisect.\n. I've merged these commits. My inappropriate rebase prevents GitHub from closing this pull request automatically. \n. These recently closed suggestions #214, #215 and #216 came at the end of wiki's first year when I was showing wiki around my sponsor's headquarters. The problems they site and in some cases solve have remained as other features grow up around them. Looks mattered, especially clean and consistent looks. That made me overly cautious which I regret. \n. What if any reader could edit (which they can, using local storage) and when they're happy with what they have, bundle all their edits up and send them to a contribution queue for the site owner? What would the site owner do with them? With json in hand the edited pages could be rendered for the site owner as if they were retrieved from a remote site even though no second site exists. \n. Thanks. Good start on author's guide. I thought you were just making up that bit of history about converting to coffeescript until I dug up the commit: 5f7fccc69b14794e7c6907d1e3350d3f22d5e856\n. Do we want to encourage the use of bullet lists? I understand their appeal on the printed page, but maybe we should leave some of those conventions behind.\nWe've made the paragraph (story item, actually) the unit of mobility, not lines or characters. Admittedly they look a little big compared to a bullet item, but that makes them finger-sized for touch-based editing. And, of course, there is the unicode \u2022 for those who like the look.\n. Our goal should be to have a positive influence on how people write, not just to admit every style of writing. It might be interesting to compare federated wiki to twitter, another influential system. Analogous elements:\nMarkup == Abbreviations (contextual, evolving)\nParagraph == Tweet (the mobile entity)\nPage == Hash Tag (the named organizing structure, shared)\nSite == Account (reference to authority, not shared)\nOf course our goal isn't to be twitter, but to recognize that the world is happy to adopt new writing styles when there is utility to be had in doing so.\n. @interstar mentions pulling content from a UseMod wiki. I'm currently feeding my original wiki into the federation through one page of perl that generates json instead of html. It's read only, but it serves the most recent content on the fly and makes it available to sites that might choose to improve it the federated way. Let me know if you'd like to try running this yourself. It's an experiment.\n. These are all good suggestions. I'm sorry we have left them hanging here so long.\nWe do now accept a wiki flavored markdown inspired by github flavored markdown.\nhttp://ward.asia.wiki.org/view/about-markdown-plugin\nWe don't do deep bullets or headings. But we do accept task lists which depend on having a mutable memory behind the markdown. (Github does in some cases but not all.)\nThank you all, and especially @SvenDowideit who offered much good advice early in this project, some of which took me years to appreciate.\n. I figured out what is going on here. Phil authored much of his content using a plugin you don't have. That means your origin, localhost:3000, can't render it as Phil intended. If you click on his flag (favicon) you will reload with his site as origin and reap the benefit of his custom plugins.\nThe default behavior for uninstalled plugins should be to acknowledge the situation, perhaps as follows:\nCan't render WierdoPlugin content.\nSince we can expect json pages to move through the federation faster than the plugins that interpret them we should expect this to be a common occurrence. We could retrieve the plugin from the remote site but don't want to spread viruses that way. We could, however, retrieve inert json from the remote plugin directory that could be useful in explaining the missing plugin. In that case the error message could read:\nCan't render WierdoPlugin content. Learn more ...\nExpanding the plugin format to be a subdirectory opens the door to this sort of behavior. Hopefully every plugin will have a Learn More page that explains how to clone the plugin from some git repo somewhere. (But don't think of making this last step automatic without some sort of javascript sanitizer.)\n. The link that Sven asks for, a link that specifies both a remote site and a page name, exists as a unique kind of paragraph (more correctly, a unique story item type). I think of these paragraphs as citations like those that appear in a bibliography.\nAs a complete item, these citations are manufactured by the factory, through drag-and-drop, not by markup in another paragraph. (Warning: the current drop handler implementation only works with some browsers. Try chrome on osx first.) The factory will go so far as to retrieve the remote page and extract an initial paragraph as the body of the citation. I hope authors will learn to write their initial paragraphs accordingly. For those who don't, you can always double-click the weak citation and revise the words to meet your needs.\nThe sinatra server's recent-changes mechanism creates citations too. You can call up recent-changes on a remote site and start browsing from there. (Hint: click the flag, not the link, if you want to bypass local copies of remote content.)\n. The claim mechanism has served us well for four months. Before claims I was afraid to admit that I was hosting any federated sites for fear that they would be abused, even accidentally. Here's the commit that created the concept: 6a3928d1307ac8c6dca466e09cca3133079f1ac8\nMany aspects of the claim mechanism have been discussed in previous issues. I would like to see the ownership workflow separated into a module supporting variations just as we now have various storage alternatives. Some alternatives we should consider:\n- Its a free farm. Make up a subdomain and claim it.\n- Its a PaaS hosted site. Use your PaaS credentials to access editing features.\n- Its a corporate farm behind a firewall. Engage the corporate LDAP services to identify and authenticate users.\nThe complexity of this issue was brought home to me when I authored the step-by-step how-to for setting up an EC2 instance:\n- http://ward.c2.com/view/welcome-visitors/view/sfw-on-ec2\nThere are a lot of services in play in this scenario, each offering a small increment of assistance. (I'm reminded of how many people I met when I closed on my first commercial real estate deal. There must have been ten of us at the table.)\nContrast this to the early days of the web: if you put some files in public_html then they were on the web. This was easy because it piggy-backed on a lot of time-sharing mechanisms that are long gone. Welcome to the future.\n. If you edit someone else's page, it becomes yours. This logic is coded into the javascript. The javascript must then find some place to store your changes. This happens in the last few lines of PageHandler:\nif wiki.useLocalStorage()\n    pushToLocal(pageElement, action)\n    pageElement.addClass(\"local\")\nelse\n    pushToServer(pageElement, action)\nUnderstand the notion of an origin server: you start browsing at the origin server and load your javascript from there. The javascript useLocalStorage() function bases its decision on you having demonstrated that you have write access to the origin. This is currently done with session state that is set up by OpenID. The origin server's javascript may read from many places but it will only write to the origin or local storage associated with the origin.\nIf you have many browser tabs open, and each tab started at a different origin, you may be confused as to which you own and which you are only visiting. Every week we discover new mechanisms that help. The most recent is a bookmarklet that translates a remote-origin url such that the javascript (and privileges) are loaded from a preferred site.\nI look forward to the day that you can author anywhere and then conveniently sync your browser-stored content to a server of your choosing. The exact mechanisms by which this happens have yet to be worked out. \n. Wonderful. I can't wait to launch some servers.\nA question about philosophy: I wonder if it is desirable to copy pages from default-pages when read? The convention with remote pages is copy-on-write.\nI ask this because I would like to include sample pages with each plugin but I wouldn't expect them to pollute the users own page space.\nCan you suggest how serving pages from the plugin directories should be handled based on abstractions that you've made? I can imagine some generated page, like recent-changes, that enumerates installed-plugins.\nHere are some sample plugin pages: https://github.com/WardCunningham/Smallest-Federated-Wiki/tree/master/client/plugins/metabolism\n. I just did a fresh clone of your master and my master. A recursive diff of these two fresh directories show no changes except in .git. This leads me to believe that you and I are now hopelessly out of sync with respect to git bookkeeping but not out of sync with respect to content.\nI believe one of us has to abandon their current repo to regain sync. Even though it was my error that led us here, I suggest you clone a new copy of my master and move forward from there. I apologize for the inconvenience.\n. Any consensus on whether this has a good feel or not? \n. Probably have to do the whole \"Forgot your password?\" thing too.\n. Ok, I'm beginning to see the logic here. Let's say there is one button: CLAIM, When you push it you get a cookie that makes the site yours and the button disappears, never to return again.\nLet's say that it also gives you some cryptographic credentials in the form of a web page with a link that, when clicked, returns a fresh copy of the cookie. You save-as to a place or two that you consider secure. Should the browser lose the cookie, you just find the saved link and click it.\nNow as Nick has pointed out, if you lose all of this, you still have your content: its' public. You just make a new wiki and fork everything over. \n. I'd like to mention to others here that Nick (@hallahan) and I have put some effort into integrating the design work he showed in our last video chat. My focus has been on keeping the platform simple and open while delivering substantial new capability to users at every level, including those who will make their own sites beautiful.\nAs an open source project we must also be careful to not go too far without open discussion of this work in public. This morning I found myself emailing Nick screen shots I want to share now:\n- http://c2.com/ward/help-plugin.pdf\nThe gist is that I'd like all the css that gives federated wiki functionality to be available to help-text authors when they explain that functionality. Normally authors use captured images. But how can we share help text with captures through the federation when local customization has been encouraged? Answer: replace captures with html using the same (carefully factored) css.\n. f804dbe seems to resolve the placement problem I mentioned in my previous comment. Cool.\n. Thanks for tracking this down. \n. 2 minutes. Pretty quick, eh?\n. I've definitely been thinking in terms of forking whole pages, not forking whole sites. (At one point I considered caching read but unmodified pages from remote sites. I abandoned that idea once cross-site functionality moved from the server to the client.)\n. The current fork button should handle cases where you have a remote page in view from an origin where you can write. The challenge is getting other pages in view as remote pages. Maybe the fork button could do some prep work like get you to an origin server that you own.\n. Once the federation is dense with attributions one will quickly find themselves in remote corners that they quickly fork. Until we reach that critical mass we need more mechanisms to join a disjointed federation. One idea would be to extend the reach of attribution by preloading a search cache with the page names of each site we touch. Then a few keystrokes into an incremental search will call up the pages you intend to fork. Appropriate mechanisms have been discussed in issue #70. Issue #165 suggests other purposes for this hypothetical cache.\n. Yes, yes, there is a connection waiting to be made. But how?\nHere is the scenario: I'm on THEIR site and see a page I want. I click fork. Next thing I want to see is THEIR page on MY site. \nIn this scenario the origin server is THEIR server, not mine. Because the desired page is on the origin server already, it doesn't do anything, at least not yet.\nSo the button on THEIR server has to redirect to MY server. How does it even know what server is \"mine\"?\n@nrn (I think) wrote a bookmarklet that sorta did this, but you had to hack it to know where MY was.\nThere must be some crazy chain of events that will get us from THEIR space to MY space in a reliable and secure (privacy respecting) way.\n. Ah, I get it now.\nI have been thinking of this step as \"provisioning\" a server to hold forked pages, not forking a site. However, I do agree a smooth workflow is important. I've written a pattern on the subject:\n- http://ward.fed.wiki.org/view/welcome-visitors/view/patterns/view/have-fun-first\nIt might be possible with wild-card dns to just create the new site in a random subdomain then let the user rename their site when they begin to understand what is going on. Perhaps we look for a shared cookie identifying the preferred site location before creating a new one for them?\n. Interesting distinction: fork vs. clone.\nSay more about this, especially with respect to information that GitHub manages outside of any particular repository. \nWe have a \"page structured\" implementation of recent-changes (in sinatra) and have been thinking this should be more elemental, probably rendered by a plugin rather than as a page. Perhaps a keen insite into higher-level workflow is exactly the spark required to motivate a re-think of recent-changes.\nSee also: recent changes mentions in issues.\n. I'd like to propose a plugin that sends local edits as a bundle to the origin site. Rather than just accept them, the origin would host them in a subdomain of its choosing. On completion of this transaction, the plugin would add a link to the newly created site on the current page. I'll call this operation \"spawning\".\nFor example, if I were at wiki.example.com and spawned a half-dozen pages from my local edits, they would appear as a new site, say 145.wiki.example.com. If you visited this spawned site and in turn spawned local edits to it, they would appear as an even more deeply nested subdomain, perhaps 1.145.wiki.example.com.\nName generation and page storage would become the origin's responsibility which it would exercise as it saw fit. No guarantees, of course.\nThis implies a hierarchy that is present in name only. With the current farm architecture, site 145.example.com could be deleted while 1.145.example.com remains.\n. I think the local edits \"bundle\" concept solves a number of problems beyond the specific suggestion I make above. I've responded to Don Park's frequently asked question about how one moves content around with a pointer to conversation here.\n. Yes, if CORS and/or IFRAMEs don't open the right doors, maybe http://unhosted.org/ will help us make the \"natural\" workflow possible.\n@moredip's recent commit 0d30bbb56cc099f672f1e590bc01a5948ec0b8cb adds some selectivity to assembling the submission.\n. @nrn, notice how 97f50fea8fd5000a1365151d5c31a4b0e8e3a38f now bundles mocha tests from all installed plugins that have them -- building on the foundation you put in place.\n. I'd like to see slugs handle more accented characters.\nI'd also like to see it not emit leading, trailing or doubled dashes.\nThis change will break some databases.\n. I'm not a fan of calling a wiki page anything but its proper name.\n. You're going to find a lot of things in html that won't have convenient equivalents in our standard paragraphs. I wonder if some thinking might get you into algorithms that actually improve on the pages you scrape. For example, you might discover the general shape of a complex site, find the leveraged paragraphs, and then use these to make what would surely become a familiar visualization that gets readers to the good parts quickly.\nGoogle does this when they offer a few quick links into a website. I wonder how they do it? I know you can offer hints but they do ok without them.\nAlternative link text has been discussed in Issue #140 wherein I cite my short essay on the subject.\n. Let's think for a bit about what will happen once Harlan is done. What happens then? I'm thinking that he might be manufacturing the raw materiel for some amazing mashups that we can't even imagine yet. What are the best decisions we can make today to support those people working in that future?\n(I was thinking of the html tag solution but, yes, that defeats the side-scrolling which does let you look at a lot of content without getting too confused.)\n. Hypothesis: People today use links in a careless way and cover up their bad decisions with alternate link text.\nIf this is so, is there something that Harlan can do today to free people in the future from these bad decisions? Remember, his converter program can look at multiple html pages and infer higher purpose. (Admittedly that inferring will have to be of a mechanical nature that can be programmed today and run in mass.)\nAlso, Harlan can emit multiple paragraphs when he finds one that is hard to convert. One paragraph would be meant to be reused. The second paragraph would be just for navigation and would look fit for that purpose only. \n. It's interesting that the current client is so easily tricked into doing your bidding by authoring \"knowing\" html. This is probably more a flaw than a feature.\nI agree that the client alone should manage most of the look and feel responsibilities with the client side left to storing and delivering useful information plainly encoded.\nMy resistance to enhancing the expressive ability of markup comes from a desire for a radically simplified model of sharing based on the smallest number of concepts, hopefully fresh new concepts at that.\n. Interesting approach. The client could go for the federated wiki page and failing that go to the source.\nPerhaps your server could convert the page on demand or schedule conversions based on links that it has already served.\n. @hallaha showed this during our video chat last wednesday. He and I got together here in Portland Wednesday to walk through the code and get it in shape for a pull request. In doing so we arrived at two important realizations:\n- At the point that one paragraph is closed and another is opened we find ourselves sending two asynchronous requests to the server to process these two events. The results were sometimes corrupted to where a page could not be later displayed without throwing exceptions. We also showed that by inserting sub-second delays we could process these requests in either order and get reliable results. We've left the more logical of these in place. However, we feel that the proper solution would be to serialize the requests via callbacks in the client code, OR, recognize in the client that the two requests are really just one (an add, not an add + edit) and do only one request.\n- Although we were offered interesting alternatives for signaling a paragraph break (blank line vs. single return) and when that signal would be interpreted (on keystroke vs. on save), we agreed that we were allowed to try what we considered the simplest approach especially because the decision could be easily reversed after gaining some experience. This freedom does not apply for decisions that leave a trail in the database. This mod is only a keyboard shortcut to functionality already present.\n. Are you thinking that if I were to hit a series of return keystrokes then I should have created a series of empty paragraphs? \nOne might do this if they wanted a lot of blank space on the page.\nRight now we consider saving an empty paragraph as a request to have the item deleted.\n. Maybe what is required is to have the caret placed at the end of a paragraph when one is double-clicked?\n. Handing the backspace in a consistent way would seem similar to the split-at-the-caret functionality we currently have. The preconditions for special backspace handling would be:\n- The TextEditor is editing an item of type==paragraph.\n- The caret is at postion 0 (beginning)\n- The preceding item is also type==paragraph\nA backspace in this situation would:\n- Concatenate the text from both paragraphs\n- Remove the second paragraph (tell server)\n- Replace the text of the first paragraph with the concatenation (tell the server, if changed)\n- Open the TextEditor on the first paragraph\n- Position the TextEditor caret at the point where the joined texts meet. \nNote that the paragraphs joined by this mechanism need not have been previously split. A reasonable refactoring would be to split a paragraph, drag something into the gap, then join the pieces back together. \nAdmittedly, a common case will be that of hitting return by habit, recognizing that it was unintentional, and then backspacing to return to the desired state.\n. @hallahan's right. The cursor should go to exactly the point between the joined texts. (What was I thinking?)\n. Awesome. Thanks.\n. I'm closing this issue under the assumption that the problem has been solved. If not, it can be reopened with new information or directed to a community more focused on Eventmachine or Windows. \n. Caret-to-end means that I can't delete an unwanted split. I'm working on a merge, adding a few more checks, and reverting this one change. \n. Yes, I think that is what is bugging you. I've reverted this change because I didn't want the other behavior. Have a go at making it right. Thanks. \n. I've found another issue with this commit. It doesn't distinguish that the content is merging is of the same type. That means accidental merges can't recreate the accidentally merged item by simply typing return. \nI've also noticed that selecting a word at the begining of a paragraph and deleting it is incorrectly interpreted as requiring a merge. This should delete only the word, not the word and the split in front of it.\nI've also noticed that items of type \"calculator\" split when returns are entered. This is unwanted behavior for calculator where returns are part of the domain-specific calculator language.\n. More than we could think of in advance but not a lot by any means. \n. Thanks. The ReadMe helped a lot. (By tomorrow I will know much more about browser plugins.)\n. Yikes. Two weeks have gone by and I have yet to try this out. I pulled the code to encourage others to explore. \n. A recent commit to the wiki-client repo captures the intent of this issue. One can now drag a page from a foreign site and drop it anywhere in a browser tab with the desired origin. The page appears in the lineup. A second optional step is to fork the remote page into the origin wiki.\nhttps://github.com/fedwiki/wiki-client/commit/031d72b39fe094cb8049883a992a2a9f98e8f203\nThere is a video that shows this workflow in use.\nhttp://video.fed.wiki.org/23-page-drag-link.html (3 min)\nThis works by adding a drop handler to the web page which is similar to the drop handling of the Factory plugin but stops short of updating anything but the displayed pages.\n@krassif, I apologize that your contribution did not receive the attention it deserved. \n. Curious. I don't remember doing anything special.\n. sorry this pull took so long.\n. Wow. This is expert stuff. I especially like that there is a basic interface:\n- https://github.com/blueimp/jQuery-File-Upload/wiki/Basic-plugin\nI'd like to see this dynamically loaded by the factory plugin, perhaps after someone drops a huge image on a factory. This is a shift in attitude for me in that I've been thinking that big files would be transported in base64 and possibly extracted from the action and diverted to an asset manager. \nOf course I'd want this to be backgrounded following the predicted results pattern:\n- http://ward.fed.wiki.org/view/welcome-visitors/view/patterns/view/predicted-results\n. You defeat federated wiki's server searching algorithm when you open a page in a new tab. You could also be switching origin, but I don't think that is necessary for the behavior you describe. \nIt might be that we are too quick to generate an empty page when we don't find one. I'm open to suggestions as I've found the behavior you describe annoying myself. (A quick way to delete the unwanted page might be enough.)\n. I can override the normal meaning of clicking a link. I don't know that I can override the browser behavior for opening a link in a new tab. Maybe I can. Do you know?\nHere is TEDx talk arguing for overriding normal link semantics:\n- http://www.youtube.com/watch?v=BdwLczSgvcc&feature=plcp\n. I show what I know in the title text rollover for internal links. I could encode that for sure. \nBest regards -- Ward\nSent from my mobile\n503-432-5682\nOn Jun 1, 2012, at 7:40 PM, Tom Carden reply@reply.github.com wrote:\n\nI should say that I am a big fan of the stacking of paths and pages to the right as I navigate. And I really like the way this enables other widgets, such as charts, in the same browser window. I hope to play with this soon.\nPlaying around a little more, I now understand that navigating to /view/welcome-visitors does not load the same html from the server as loading /welcome-visitors.html.\nAnd I understand that because of the on-demand nature of the search that happens when clicking on a link, you can't write the hrefs ahead of time to be the URL that eventually shows up when the link is clicked and the search completes. That's because clicking the link might result in navigating to a page that is sourced from another wiki. (Halting problem is a haunting problem... I never think it's relevant and yet, there it is).\nPerhaps, when viewing e.g. /view/recipes, instead of href=\"pancakes.html\" the link could be href=\"/view/recipies?q=pancakes\". That way if there's no pancakes page on the current wiki, and it finally resolves as /view/recipes/friend.example.com/pancakes, then opening in a new tab would be able to resolve the same connections and show the exact same results?\nThe other loose end in this is that the .html representations would end up truly being for bots only, and never exposed to browsers with javascript active in a normal wiki reading session. I'm not sure if that's a bad thing or not, but it would make my issue #244 (the style of the journal in the html representation) largely irrelevant :)\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/issues/243#issuecomment-6074385\n. Some good news. Earlier I admitted that, \"we are too quick to generate an empty page when we don't find one.\" We are no longer so quick in most situations. In the open-in-new-tab scenario that started this conversation we now generate a 404 page rather than creating an unwanted empty page. Even better, when staying within the current tab, a a ghost page appears offering to create the page, if wanted.\n\nMore good news. The refresh/buildPage logic has more information now that we fetch-ahead sitemaps for sites as they are noticed in viewed content. That means that we could resolve the true destination for many links as they are rendered. Should a browser insist on interpreting the href literally, then it almost certainly end up in a good place.\nIn summary, the obnoxious empty page behavior is gone (except in obscure paths) and an efficient pre-search option is available that doesn't have us re-implementing browser meta-key interpretations.\n. A special purpose link syntax is still a good idea. I'm not sure I fully understood the suggestion @RandomEtc made four months ago. I may not fully understand it now.\n. The server-side rendering is just enough for robots to look at and to not be broken completely when someone turns off javascript.\nI've now observed the journal letters, as you describe, not the journal symbols we now expect. This surprises me because Show-Source shows that the page was rendered client-side.\n. The current behavior starts by being confusing and then, with experience, becomes only annoying.\nHowever, federated wiki assumes there is a writeable store and local storage remains the writable store of last resort.\nWe will find ourselves in a position of great flexibility when we overcome the confusion and annoyance without giving up on local storage. Things that I know will help:\n- When an edit causes a page to be written to local storage (say because one failed to login) the local storage version should be identified in the journal as a fork of the original page. This will provide convenient access to the original instead of hiding it forever.\n- When a page is unwanted, there should be means to immediately and confidently discard it. Currently, for local changes, the indirect method of conjuring up the local-editing page, just to push the indiscriminate discard button, is hardly immediate and only applied confidently for those who find no use for local storage.\n- Federated wiki is quick to pull content closer. There should be some sync-like mechanism to push content to some further (but still writable) store. \nNote also, that any situation that occurs with local storage will also occur with multiple writable sites. Good solutions here will aid those who keep local caches of public sites on their laptops or behind firewalls. These are the true functionality and usability challenges we face.\n. I am relying more on local storage in my current work. I see nothing wrong with any of the suggestions above and will see if I can't make progress on some.\n. This supports the CloudSmith one-page quick-start for EC2 instances by avoiding even a moment where a new instance is unclaimed. \n. I like the approach but agree that this code is not complete enough to be deployed. We have constraints that come from three desirable compatibilities:\n- a server's db must be compatible with the server itself.\n- a server (and its db) must be compatible with every client ever.\n- a client must be compatible with every server ever.\nThe second (every-client) and third (every-server) constraints arise because any random federated wiki client could requests pages from any random federated wiki server.\nA server can apply what ever search algorithms it wants so long as it delivers the proper page when requested and delivers a 404 when that is the correct response. A server stores a more complete version of the page title which can be used in this search.\nIts also possible that we could reliably convert a slug from one algorithm to a slug from another. This is the basis of the permissivity discussion above. For example, if we could show that for slug functions F and G, that if F(x) == F(G(x)) for all x, then we could say G is as or more permissive than F. Intuitively, G permits more characters through than F.\nIt has been suggested that we could allow more permissive slug functions into clients so long as servers that use (or might have used) a less permissive slug function try applying that function and repeating a query before issuing a 404.\nNow I am worried that the repeated-query approach is not sufficient to handle all three constraints enumerated above. Further, I am not sure that our new function is always more permissive. Specifically, the desire to eliminate some redundant hyphens makes it less permissive while permitting international alphabetic characters makes it more permissive. Yikes.\nMy feeling now is that we won't be able to meet every constraint all the time. However, if we could characterize the pages that will suffer, and under what circumstances they do so, well, that would be awesome. Then we can move ahead with confidence.\n(Aside: Have we given up case insensitivity or is that handled properly for all alphabets that have case?)\n. I do hear you, and this is important.\nI've seen some nice things done with the HTML5 content editable mechanism. That seems to be the place to start. I've been meaning to do some research here and offer some direction but have been distracted.\nMy simple understanding is that content editable would make reasonable html. I'd like to separate plain text and html. Right now one can slide html into a plain text paragraph. Better to make that an html paragraph, no?\nThere's lots of opportunity. Suggestions welcome. \n. I've struggled with the interaction of error handling and callbacks. I've complicated my metabolism calculator when I added the capability to grab data from the server in the middle of a calculation. The calculator use to be simple but has grown complex as I've tried to merge callbacks and error handing in an orderly way.\nI'd love some suggestions here, or pointers to code that has handled such integration nicely. \n. http://tw.fed.wiki.org/view/welcome-visitors/view/take-out-challenge\n. Adds come from several operations:\n- pressing [+] to add a factory as the last item of a story, no \"after\"\n- dragging an item from some other page to a specific place defined by \"after\"\n- splitting one paragraph into two by typing return. the second half is \"after\" the first\nThe itemSplicedIn flag appears to deal with inconsistent journals where an \"after\" was expected but not located. Since pages and their journals get passed around there is no guarantee they will make sense. A best effort to reconstruct the story is all that is expected.\nMore test coverage would be much appreciated especially for pathologically corrupted journals. \n. Handling this amendment was easy on my end. The github issue provided great context for this ongoing work. I used git and gitx on my laptop to double-check everything then git push to close the amendment automatically. \n. I'm trying to think through how to make history browsing more interactive. I'd like to \"scrub\" through the actions and have the \"ghost\" version update without having to click each action. The journal is a \"movie\" of sorts after all. What I can't work out is what you do to stop scrubbing and leave the right ghost version up for further interaction. There must be some combination of clicks and hovers that would feel right.\n. Sweet. You are the documented \"take-out\" challenge master:\n32 ++-----\n. Thanks for the contribution. I apologize for letting this plugin atrophy. I hope you can put it to work.\nI'm not sure why you load the script in both emit: and bind: since they tend to be called together. One exception is for very fundamental types that are \"emitted\" as static html text by the server when referenced with an .html sufix. getScript tries to not load a script twice so it shouldn't matter.\n. You inspired me to update the plugin documentation. http://fed.wiki.org/view/choose-plugins/view/about-mathjax-plugin\n. Your experimentation is welcome. Documentation will improve eventually.\n. Thanks for all your work on this. Especially figuring out the factory.json thing. \n. I have noticed that scripts are loaded redundantly but haven't looked into it. Some thoughts on best practice would be appreciated. This often turns into walking the fine line between development convenience and production efficiency.\nI have been burned by my own dependence on network resources when offline. Perhaps plugin scripts should be cached in the server's plugin directory for when network versions are unavailable. Again, best practice here is not obvious.\n(Thanks for your work on the code plugin. It seems to be the canary warning of trouble in the \"script mines\".)\n. I like the plugin a lot. It seems to have done an acceptable job with the 99 Bottles of Beer song (my first thought).\n. I'm not quite sure what sequence produced the error. I've tried a couple of queries against the sinatra version. (I understand that the reported issue is with node/express version.)\nI tried these urls:\n- http://localhost:1111/mumble.json\n- http://localhost:1111/remote/localhost:1111/mumble.json\nBoth returned the oops page as html (not json) but didn't break the server. \n. I hate to be dense, but I'm confused by the phrase \"I added my name, claimed the wiki using Google\". Where are you adding your name?\nI don't doubt that the node server should recover from bad json. There are lots of ways of getting bad json.\nI also know that the \"claim\" mechanism is unusual and leads to much confusion. Are you getting through the claim process to the point that you see a \"logout\" button?\nIf you have an unclaimed wiki, are you able to edit it successfully? Claiming is a mechanism that keeps others from editing your wiki. It is not strictly required but it is expected that everyone will have their own wiki in the federation.\n. Ok, lots to go on now. I'll see if I can duplicate it. Sorry for the inconvenience. \n. d84bc65 seems to fix the problem.\nThe server is expected to proxy to other servers and handle 404s when the remote server returns them. Lots of other things can go wrong, of course, but we'd like to move away from this code anyway.\nCurrent thinking is that the client code should just go to the remote server directly via cross origin resource sharing which all federated wiki servers enable.\n. Pete -- Enjoyed pairing online today. We jumped into the deep end and came up swimming. Thanks loads. -- Ward\n. Many thanks. At one point we had selenium running mocha and scraping its results. This mysteriously broke one day and eventually got commented out with this resignation in favor of green: 9463676356690103889400e9b3c4328d556c0edc.\n. All good points.\nWm Leler speaks on alternatives to Google Maps.\n(Lots to choose from.)\nhttp://www.youtube.com/watch?v=7bIoBeiJOSM\n. I would like to see some mechanism where place and time can be attached to paragraphs in some general way. Of course I would start by just hacking data into existing flat files and then see what I can do with it. Time and place plugins that show all the times and all the places on a page would be a good start.\nAllow me to fantasize. \nA good ui might be to show textual time and place on tap/hover and offer specialized editors on double tap/click.\nPerhaps if a location were specified with respect to a venue, the venue could be another page:\n\"location\": \"[[White House]] Situation Room\"\nThis would lookup White House and find:\n\"location\": \"1600 Pennsylvania Avenue\",\n\"geo\": \"38.9106 / -77.04256\"\nPresumably the geo coordinate would have been refined by dragging the pointer in the map. Just another edit. The editor referring to the situation room might further adjust the coordinate and expect it to stick. Where is the situation room anyway?\nAnother edit feature might be to capture the gps/wifi coordinate off of any devise willing to share. Press the \"Here Now\" button to get place and time.\n. All of the D3 plugins predate the inclusion of wiki pages within the plugin itself. One approach is to fabricate a number of useful instances of a plugin, include them in the plugin pages, and then write a summary page that guides the user to the appropriate version for their own needs. Here a documentation pages offers an index to installed plugins on fed.wiki.org:\nhttp://fed.wiki.org/view/add-plugins/view/choose-plugins\nRecently there has been more experimentation with calculators than with visualizations. One can find data on the server when it doesn't find what it wants already open. Others are good at cascading data through them making modifications along the way. It might be possible to create a catalog of visualization constructor plugins that have needs-specific markup that is easily user editable.\nSo far my priority has been to gain experience with a variety of techniques rather than choose any particular method or try to make the ultimate plugin. \n. Wow. Very impressive pictures. Seems much nicer than, say, google charts.\nIf the drag and drop stuff doesn't work you might just capture his DSL in a TextEditor. You can crib the data-finding logic from one of the existing plugins. (Yes, I know, they are all different and not well documented. I'm feeling my way into this space.)\n. It looks like you are observing the desired behavior of the Journal. The highlight shows which other Actions apply to the same paragraph (in your example, id 821827c99b90cfd1). You might enjoy watching the videos where this is explained.\ndefault-data/pages exists to bootstrap a new server into the federation. \n. The visual metaphor of the Journal is that of footprints left behind by buttons (or other gestures).\n. As we get more sophisticated in managing the asynchronous environment the footprints will \"develop\" throughout a server turnaround. Now they show up when a transaction completes. They could show up at the start of the transaction in a ghostly form that either completes or fails. In the later case they become useful in, say, diagnosing network problems. See Predicted Results for a discussion of the pattern to be employed.\n. Do you have any advice on how date formatting should be handled (and tested) in the javascript? \n. The client code expects to find plugins in two different places. This is just the client probing to see which convention is used for factory.js. It tries the newer convention first.\n. 00e6ba275eb1e2da2d452f4db6795ba28fe06ac1 updates Sinatra server to respond to either factory.js or factory/factory.js requests.\n. Thanks for the mention.\nYes, modern open and distributed software development practices have inspired this work just as the radical modularity and collective ownership that emerged with object-oriented programming inspired my first wiki.\n. I'll close this issue now since there appears to be no more conversation. \n. This code captures the essence of what it means to browse the federated wiki. It's the engine that interprets links in the context provided by the journal and other hints as to the proper neighborhood to find the page.\n@nrn isolated pageHandler from the  legacy code. Now @moredip has purified it by pushing DOM references up to refresh where DOM building is properly done. Kudos to both.\n. The drag logic foolishly tried to save both the from and the to pages on top of each other. I now detect this case and inhibit the second update in 30fb426282be2f4576f80a2e95595b023a423aba making the drag a copy, not a move.\nThe check I've added isn't in the right place. Preferably I would coax jQuery to not show the edit as a move when it is now interpreted as a copy. There is a demo that appears to do this better. I've added a link in a code comment. \n. We've discussed searching nearby servers in issue #70. This commit takes us one step towards that goal by accumulating a list of sites we've seen as we browse around. I like that the list appears in the system bar too.\n. @moredip and I paired on this commit starting with these notes:\n- http://ward.fed.wiki.org/incremental-title-search.html\n. You bring up many important issues. My work on the method plugin has been motivated by a need to document life-cycle analysis (LCA) computations. Unlike Tangle, my formulas are editable. I don't (yet) offer scrubbing of the numbers in methods.  It is a natural extension. Update propagation between methods needs to be fully in place before scrubbing will be effective.\nI've assumed that edits to formulas should be saved, just like edits to text. However, I've thought of scrubbing as more of a reading activity, something someone would do to gain insight into the sensitivities of a formula. I had not considered how new numbers would then be saved through scrubbing. I wonder how both versions of scrubbing could be offered and the distinction made obvious in the interface? Maybe scrubbing (and all other calculations) need to continue working while editing a formula. Its exciting to think about what this would be like.\n. I agree that the network of attributions (the forks that get recorded in the journal) will emerge as a structure as interesting as the link structure we normally associate with hypertext. We'll develop this first to guide incremental search. The initial commits for a \"search neighborhood\" have gone in. See source.\n. Bret Victor and John Resig are both doing inspirational work. I recommend everyone pay attention. I include some of that style of interactivity here mostly to stress the federation and refactoring \"principles\" underlying this project. I want to make sure that we all get to a great place together. Hopefully Bret and John will learn something about federation from us. \n. Regretfully the drop logic has to be specialized for every file type, every browser and every operating system. We'll be seeing unexpected items on drops until the the code is taught to expect every case.\nRather than simply refusing the drop, I capture information Ubuntu 10.04 Nautilus 2.30.1 -> Firefox 14.0.1 has offered. It doesn't look like much. If you're a coder and want to work this problem, I'm happy to help.\n. The case analysis that I mentioned in the previous comment resides in the factory plugin. Scattered through the nested conditions you will find calls on \"punt\". Punt makes the \"Unexpected Items\" that you have encountered. The factory shouldn't punt.\nI'd like to see some cooperation between the factory and the other plugins that it manufactures. For example, the current factory has code for translating .csv files that might be dropped. That should really be a data plugin responsibility. I'm imagining plugins register with the factory to have a chance at making sense of every drop. There is already some cooperation between the factory and other plugins. There needs to be more.\n. It looks like the two systems don't even agree how to represent a list of types. One is a javascript object, the other a javascript array?\n. @rynomad you are very resourceful. Thank you for pointing out this rather circuitous path to posting images. I've hung on to the data-url base64 image encoding because it preserves exactly the sharing semantics of federated wiki. Eventually we must move past it because it can be slooow.\nAnother alternative is available to you if you are willing to write your own json for pages as you are when you run a batch converter. You can use traditional image urls in image items rather than the data-urls. This can be much faster but, of course, you have to store your pictures somewhere other than the page json. \nI did receive your [[How to Wiki]] revision. I'll add it to my developer focused list of Obscure Workarounds. Thanks.\nIf you'd like to help me make drag-and-drop work better, you can find the code that is giving up now in the Factory plugin. A good approach would be to put lots of console.log statements into it and see what Ubuntu/Firefox is sending into wiki. \n. If you have the image drag-and-drop working then send me a pull request. Its easier for me to review if you don't put too much together in one request.\nI'd like to distribute the case analysis across all the plugins that might be created with a drop. I haven't worked how this would happen. Probably drop handlers would be provided by each plugin. These would bid for the right to handle any given drop.\nNote that in most browsers dragging a tab is different than dragging a url out of the location field within some tab.\nWhy don't you fork my ObscureWorkarounds page into a wiki of your own. Then send me the location and I'll add it to sites I watch with the Activity plugin.\n. Yes, we don't yet handle image drags from the browser. I drag images from the browser to the desktop and then from the desktop to wiki. While on the desktop I usually tweak them up a little too. Its a compulsion.\n. I'm liking this plugin a lot, even as it struggles a little to get along with everything else going on in our interface.\nI wonder if we could turn on and off the panning and zooming by entering into an edit mode on double-click. This would let drag mean move-plugin unless you were editing the map, then drag would mean pan.\nThe mapquest javascript seems to only want to put controls up on one map at a time anyway. Let's make it clear which one that should be by clicking into edit mode.\nAnother advantage of the mode would be that journal actions would only need to be recorded on exit.\nThanks for considering all this.\n. Now is a good time for others to reread the conversation in Issue #261 and add pragmatic advice here. \n. We had a lively chat with great participants today. Bit.ly tells us that 60 people clicked the link. 70% from my provocative twitter post. Google only lets 10 in at a time so many failed to join us. We're exploring mechanisms for people to watch but not join. Since a number of our 10 slots were filled by people just listening, that sounds like a good idea. The recording of such hangouts need not be public.\n. Incidentally, my provocative twitter post was:\n\nI've been considering a small wiki's role in a big data world.\nhttp://bit.ly/SFWhangout . Chat about this starts in 5 min.\n\nClearly, we should do more with big data or do more with Whangouts in San Francisco.\n. Adam Yuret offers this advice:\nPeople who might be more participatory not being kept out while people who want to spectate don't have to join.\nI broadcast the lean coffee hangout, I have no idea if anybody spectates the live stream but since people often talk about sensitive material I make the youtube recording private and add people as requested.\nMade a screencast to demo the pratically invisible feature. http://screencast.com/t/oY2OBrrYCUk\n. I have now read the json-patch and corresponding json-pointer specs. Your suggestion makes complete sense now. Thank you.\nHaving story elements with duplicate ids is a problem waiting to happen. So far I have only seen problems when replaying the journal. Many actions are ambiguous in the presence of duplicate ids.\nI'm thinking that ids that would be duplicates should be replaced with new, \"alias\" ids, generated on the fly. The original id could be saved as 'aka': '0948098234' which could be restored should the item move to a location where it is again unique. Algorithms that want to know the original identity of an item can examine item.aka || item.id.\nI would like to see fields of a json object have similar size and lifetime. This is not the case now. However, should the server extract images from json and replace them with static asset references, then our most significant offender will be brought into line.\n. Thanks.\nThere may be more codes than strictly required to identify sites near and far, specific and not. This output may help identify cases were we have been unaware of where the pages are actually found.\n. With these commits I'm experiencing the same CORS problems you were seeing. I'm suspecting the CORS violations are a side-effect of the server trying to report some other error and doing so without with adding CORS headers. I'll be investigating and hope to have some sort of patch this weekend.\n. I've throttled the rate at which we fetch sitemaps and now see no CORS related XHR errors. This confirms my hypothesis. There is nothing wrong with being light on the server. But, when the server can't report its own distress without meeting the CORS expectations of the federation, then something needs to be fixed server-side.\nHere's the commit: 5af47dfa40484509dfa396c25fc9fe04cea6e8b9\n. What would be the better thing to say?\nBest regards -- Ward\nSent from my mobile\n503-432-5682\nOn Sep 12, 2012, at 6:24 PM, Don Park notifications@github.com wrote:\n\n\"~>\" means allow the version to be greater than or equal to 1.3.1 in the last digit only. So 1.4 is \"too new\".\n\u2014\nReply to this email directly or view it on GitHub.\n. One: Fork copies a remote page to the local (origin) server. Its possible to have a conversation by copying a page back and forth. We don't yet merge local changes but don't consider it difficult. We already show that we can playback actions to reconstruct any version.\n\nTwo: I can imagine attaching a privacy attribute to arbitrary story items. A server might elide private items except when requested by a client with write privilege. This would handle most editing situations because the client editing would see all content. However, the forking conversation just mentioned would be a problem. The public correspondent would be editing the elided copy so simply forking it back would lose the private content. A merge would be required, a merge of a different stripe since it wouldn't be action based.\nYou've suggested an interesting thought problem. I'm more interested in knowing if it would work than having it work. I find that in my world I have many wikis, some private, others not. From my view they are federated. The outside view is of public wikis with pages that sometimes reference sites that are not available. \n. Awesome. I was missing search.\n. I notice that node/express can't find the \"reference\" plugin that is in client/plugins/reference/reference.js. I'm guessing that this commit, bb6e854b6e5027facf9ebce741e3be06064ed567, needs to find its way into the express implementation. This looks in the old and the new place for plugin javascript. A later mod revises this to look in the new place first. Just about all plugins that I manage have been moved at this time. \n. Well, I was certainly confused.\nThe code that probes for plugin implementation is located in the client. See the code.\nI was confusing that code with the code that retrieves pages from plugin's pages directories as if they were part of the site itself (which, in a sense, they are.)\n. We don't yet have a simple way to do this.\nIn the mac finder (which also uses Miller Columns) the move/copy distinction is made by holding the option/alt key. Normally one tries various shift keys until the green plus sign appears by the cursor indicating that you found the correct key. I don't know how this translates to a touch interface.\nThe event handlers have been modified to surpress the remove half of a move in situations where it is disastrous such as copying from history. This is a stop-gap measure awaiting more sophisticated interaction with jQuery-UI's Sortable API. This creates the workaround documented in Obscure Workarounds. \np.s. Sorry to have been so slow responding. I somehow missed this issue last weekend.\n. Since there is no more discussion I will go ahead and close this issue. Improved copy/move logic remains in our plans.\n. Many future wikis thank you.\n. http://fed.wiki.org/view/choose-plugins shows how to select plugins.\nhttp://sandbox.fed.wiki.org/ is a good place to experiment\nThis behavior is driven by factory.json files in select plugins. The express server doesn't yet support interpreting these files.\n. Express should be handling the plugins correctly. Thanks @nrn of commit 5ccb2ccfc51786e0fcdb36ba4312a423c22a510c.\n. Hmm. I thought I answered this but I must not have pressed save. \nThe search feature is described in the How To Wiki pages. \nhttp://fed.wiki.org/view/how-to-wiki/view/search-sites/view/list-of-lists-of-sites\nAnyone can create a list of sites (like a bibliography) that will get wiki's search engine working on another corner of the federation.\n. I'm guessing that this conversation is finished.\n. Works great. Thanks.\n. There is already a couch binding for the sinatra server implemented via a \"storage\" abstraction. There is also a storage abstraction emerging on the client side that makes browser local storage and sever side storage look similar.\nIf the client side abstraction were made more plugable, then a direct couch storage plugin would be very interesting. I'm not ready to make couch the only storage mechanism. It would have to be an option.\n. Thanks for getting this unfinished hackathon experiment going again.\nNote to others: The Efficiency plugin does have a bit of documentation and a couple of test cases included in the plugin's documentation pages.\n. I'd already committed the javascript file so bb0c4b6 is not necessary.\n. I'm a little confused as to which commits are new work and which have already been pulled. Are we in agreement as to what the current code looks like? (I can look into this next week if you want. I suspect I've misinterpreted what GitHub has been telling me.)\n. Ok. Our duplication of effort has interfered with GitHub's web based merge. I'll perform a manual merge once I get home. Sorry for the delay. \nhttps://help.github.com/articles/merging-a-pull-request\n. I was not able to reproduce this error. However, it has all the symptoms of a persistent weakness in ruby for which we have applied numerous workarounds. Specifically, if ruby issues a web request to another server in the course of handing a web request, and, the other server turns out to be served by the same ruby instance, deadlock ensues.\nThe ruby webrick and thin servers vary in how they handle \"recursive\" requests. If you are using thin, see if the problem persists when you restart your server using webrick instead. This would be strong evidence that this is the problem. If you are already on webrick, I can think of more things to try.\nAlso note, that many updates follow multiple paths with the server \"following behind\" the client. If you have reason to believe that the client and server disagree, refresh the browser. A better architecture would lead to eventual consistency by hard to break mechanisms, probably some variation of performing the same refresh behind your back.\nAlso note, double clicking a \"reference\" opens the text editor on only the editable text, not the flag or title. This would have been more obvious if the display were not already corrupted.\n. Hmm. I believe the problem is purely server-side and specific to thin. I am working with a ruby expert to find a more comprehensive solution than the band-aids we have in place at the moment.\nNode avoids the problem entirely. I'm glad you got that implementation working. \n. I'm not seeing this error. I am seeing daylight savings time related errors reported by mocha.\n. Daylight savings time test problem fixed with 504927d948d782c93d7f9ef56e24a62fe430fac4.\n. Are you using the version of mocha.js in the repo? Could this be a browser version issue? What are you using? Do you think this is a false alarm or a real error? (As I read mocha documentation it appears to be a built-in variable usage test, not something we asked to have.)\n. Thanks so much. Habits emerge and never get noted. \n. This looks like some kind of sensitivity to a missing title field in the page json. The code is producing the sitemap.json used for search. Title should be optional. I'll look into it.\n. I've made the express server insensitive to missing title fields. It was recreating the slug from the title. I've changed it to remember the file name (slug) across the asynchronous file io so that it can report it without having a valid title. Fix is in 2130066a18d4d988304eb0430656fd18d37a1ed5.\nThis may solve your problem. If not, you may have some file corruption going on that we don't (yet) handle. Re-open this issue if you need more help.\n. The web page footer (status bar) is a good place for information about the origin server and status specific to the client-side javascript loaded from the origin. Currently this consists of:\n- login\n- search\n- neighborhood\nOne thing we should consider, how should added features talk to the user? Currently we just pop up new pages. For search these are ghost-pages that have no permanent (server-side) data behind them. This leads to an odd failure (blank search results) when someone refreshes their display.\nFor example, a help or status response could appear as another ghost-page where there are lots of formatting options and the ability to interact with other pages through drag and drop. Would this be a good way to go? Can the idea be developed further? Or is this piling too much functionality on \"pages\" when more conventional pop-up things should be used?\n(My original wiki was sometimes criticized for not having a fixed top level menu. SFW is currently pretty light in the same way.) \n. There are lots of options. For example, the square brackets themselves could be omitted too. However, the more heuristics that are applied, the more pages it takes to say what is possible. Options are not without their cost. \nHere is the How-To on the subject now:\nhttp://fed.wiki.org/view/how-to-wiki/view/add-links\n. Although this is an interesting idea, I have the same concerns as with Issue #314.\nIn my original wiki one actually removed characters to markup a phrase as a link. My theory was the most important features should have the shortest markup. I made the length of link markup be negative. \nMediawiki implementors told me that their project really took off when they got rid of that convention.\n. It would seem to me that the best solution would be to not offer editing of ghost pages (except for copying items from them.) I would guess you were already getting error messages from the server and your edits only appeared to be saved, no? \n. I would rather find a way to make the journal (history) always attractive. Do you think this is possible? See also comments on Issue #318.\n. I appreciate that you are stressing things. \n. It is possible to remove redundant information from the journal. I suggest this be a destructive delete that could be applied before publishing pages from (say) browser local storage. I would call it \"condensing\" the journal and offer \"light\", \"heavy\" and \"complete\" variations:\nLight condensation would replace adjacent stings of actions with the single action that effects the same change. For example, moving an item up two and then up two more could be replaced with a single move up four. The date of the replacement would be the date of the last action in the condensed sequence. \nHeavy condensation would replace all edits with a string of adds that add the final version of every item in the sequence and with the date of the last edit of the items. This would preserve a sense of the period of authorship but would discard all intermediate results. When forks are present, the heavy condense would preserve just enough forks to meet the attribution requirement of the cc-by-sa license.\nComplete condensation would replace the whole journal with a single create action that creates the whole page at once. The date of the action would be the date of the condense operation. (Forks for attribution would be preserved or could be bundled into the create action in some less visible way.)\nWe don't yet merge changes from various forks. We should do the analysis of each variation to figure out how much we are giving up when we discard history.\n. Until now the index into the journal has been the reference number. When we start messing with the journal we'll have more bookkeeping to do.\n. The journal entry that caused the problem was a \"more\" that referred to a story item that could not be found. The result was an \"undefined\" in the reconstructed story. My solution is to make the move logic more robust in this situation.\n. A search for \"federated wiki\" showed up one fed.wiki.org site in position six. My occasional experiments has shown other hits but none past the front page. I don't know why. I do try to respond to simple http requests server-side with normal anchor tags that make sense without javascript enabled.\n. A google search for:\nsite:fed.wiki.org\ndoes show 90 pages found including some subdomains of fed.wiki.org. I conclude that we are being indexed but remain an obscure corner of the internet.\n. If you are running the sinatra server with flat files for data then you will find your pages in the directory:\ndata/pages\nIf you want to run a farm, then create the directory\ndata/farm\nThen pages for each site are stored under the virtual host name in the directories\ndata/farm/*/pages\nI collect design and implementation notes in my own wiki. You might enjoy reading Where Pages Live.\n. The greenish icon is a fork-action recorded in the page's journal. It provides the attribution required by the CC-BY-SA license. It also guides the client for places to look when it can't find pages, like How To Wiki, on your own site.\nInstallation tips are welcome here:\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/wiki/Hosting-and-Installation-Guide\n. I will close this because it appears to be working for you and discussion has stopped.\n. Yes. Something like that. My vision may be vague but the opportunity remains tangible. HTML5 has opened a door. \nBest regards -- Ward\nSent from my mobile\n503-432-5682\nOn Jun 1, 2013, at 4:11 AM, almereyda notifications@github.com wrote:\n\n@WardCunningham What do you think, where is all this (e.g. semantic, barrier-free, distributed real-time collaboration tools) leading? Thinking in terms of civil society self-organization, the epistemology of knowledge itself and global sustainability & justice.\n\u2014\nReply to this email directly or view it on GitHub.\n. Kyle and I have been discussing SFW's complex interaction with Event Machine for some time. I would appreciate other ruby eventing experts giving Puma a try in wiki farm configurations where \"recursive\" calls still ocurre. \n\nThis is the troublesome request:\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/blob/master/server/sinatra/server.rb#L319\nNotice that there are heuristics that try to short-circuit this call when it is clear that the requested content is from the save server. However, these heuristics don't cover every case.\n. @kyledrake I remain interested in this improvement but have other issues sneaking in front of this one. \n. It is possible to refer to a single page as an html document:\nhttp://ward.fed.wiki.org/reflections.html\nThis has the advantage of returning server-side rendered html with links that can be followed by simple robots that don't interpret javascript. Try:\ncurl http://ward.fed.wiki.org/reflections.html\n@moredip and I have simplified the javascript handling of this case and may have messed it up a little. The last time I tried this with javascript enabled it seemed to paint the page twice. Much of the plugin complexity between emit and bind traces back to handling cases where the dom objects have already been emitted by the .html form.\n. The left and right arrows will scroll left and right. This is different than moving forward and backward in the browser history. We have the notion of an \"active\" window that is used for little except scrolling. See: https://github.com/WardCunningham/Smallest-Federated-Wiki/blob/master/client/lib/active.coffee\nI'm told that horizontal scrolling with the mouse or touch pad works. I've disabled this at the os level and haven't seen it work myself. We've recognized the need for more affordances but don't know what they are.\n. Yes, no doubt we open ourselves for new problem cases by keeping single panes open for so long. I've noticed that active.coffee logic breaks down in some not yet repeatable circumstances. I've suspected resizing but haven't been able to track it down. Reloading the whole window fixes many problems. This is like rebooting your pc only faster. It's a lot faster than reloading the operating system like we use to do with Windows 95. \n. Since discussion has stopped here I will consider the issue closed. \n. Thanks for the improvements. \nRegarding site title, there is some identity information that is initialized from default data:\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/blob/master/default-data/status/local-identity\nI don't think any of this was ever used. I'd be against using it now as it would be just one more thing to be setup before a site goes live. \nThe web page title is updated dynamically from wiki page titles as they are rendered.\n. The github project is still Smallest Federated Wiki or SFW for short. I call the concept Federated Wiki and notice that it googles well, as does fedwiki but not fed wiki. Inside the Federated Wiki context I refer freely to Federated Wiki as just wiki. I find that I use the word wiki to mean Federated Wiki outside that context more and more now. This comes from living with it for 1-1/2 years.\n. Let me summarize the state of this and related work as I understand it. @asolove built the side-scrolling mechanisms that we use today as well as the interface to the browser history that makes the browser back button work and allows us to bookmark and email whole lineups of wiki pages from multiple sites. We were, of course, excited about a tablet-friendly version of the wiki-client code.\nBut the wiki-client code has not been evolution friendly. It suffered from application state scattered throughout the DOM. We have been incrementally introducing parallel data structures (i.e. models) and converting scattered functionality to use them. Experiments like Adam's have informed this refactoring even if the refactoring hasn't made continued tablet support easy for the moment.\n. Prevailing wisdom is that designs should consider touch first, then find an equivalent way to perform operations on the desktop. The design space for federated wiki is open because wiki has chosen to use keystrokes for most special features. Tap behavior has no competition.\nWe have worked with several designers who have been flummoxed by the open-ended goals of federated wiki. My summary: the designers were prepared to produce a performance while federated wiki is a platform for other peoples performances. Designer's head explodes.\nIf you can get past the head exploding and just do something simple like make page scrolling work with reasonable ballistics then you are ready to contribute to tablet federated wiki.\n. Thanks for the hard work. I will review it carefully. Is there anything that was problematic and deserves special attention? \n. @JoshuaKerievsky has demonstrated this failure to me in person. He was attempting to insert a new paragraph before the first paragraph of a wiki page. There is a work-around: type the new paragraph before hitting enter which makes this a conventional split resulting in two, non-empty paragraphs.\nFirst observation: adding text, even just enter, should not make other text disappear. This needs to be fixed.\nSecond observation: his expectation was that splitting a paragraph with enter would give him a new empty paragraph for his new typing. Since we don't save empty paragraphs, the new paragraph would have to be opened for editing. That is, when split creates an empty paragraph, that must be the paragraph that is open for editing whether it is above the enter or below it. Does this sound right?\nThird observation: the three-way split (select a span and then enter) creates more boundary cases. I would have to experiment to know what it does in each of them. For example, if you select all of the text and press enter, does this mean make empty paragraphs above and below the text? I doubt it. @hallahan, thoughts?\n. Thanks. This looks like a simple solution to a complex edge case.\nDo you have any idea where those extra javascript functions came from? Maybe they are something coffeescript thinks it needs?\n. Works great. Can't explain the extra javascript.\n. I'm happy with the user visible behavior with this modification. However, I'm reopening the issue because I've noticed some undesirable side-effects that I would like to add to the record.\n- This leaves spurious empty paragraph items in the story.\n- The journal records the addition of these empties.\n- Retrieving the rev for the empty add will show other text lost (but clearly not lost??)\nThis last point might suggest that the lost text wasn't lost but that some other bug exists in rendering pages.\n. We have a shallow problem and a deep problem.\nThe shallow problem is that we write blank paragraphs into the story. These quietly hide without doing much harm except that they clutter the story for anyone trying to make sense of it as json.\nThe deep problem is that the client and server working together as an editor can't always keep up with the user. Worse, when it falls behind, it throws errors. I'm also suspicious that it could be writing null stories but haven't caught it red-handed yet.\nI suggest that we retreat to the simplest interpretation of splitting paragraphs. The seldom used three-way split should go. Also we should detect the cursor-at-zero case and just do nothing. (This is as if it split off an empty paragraph and then discarded it.)\nMy suggestion would mitigate the deeper problem but not solve it. We can't let our attention diverge from solving deep problems. However, if our current solution is corrupting user data, that is a heck of a way to focus our attention.\n. I have in my notes the following: TextEditor split followed by join fails to rewrite joined item. YIKES.\n. I'm sorry, I didn't mean to suggest that the YIKES issue was related to your changes. I just had it in my notes so I thought it should be considered when working in this area. (The date in my note was from 7 weeks ago. By the way, thanks for those dates.)\n. Oops. Missed this coffeescript update during a busy week. Thanks for the adjustment.\n. I had a little trouble with this commit. It seams that not binds tighter than in. I posted a fix    4840eb2e719f7a47bfe421b9740c9e72649971bf.\nThis raises the question of how we can test this logic reliably. I'd like to collect a set of (continuously evolving) cases from a variety of browser/os combinations. One approach would be to collect info and then make decisions based on the info. We could then run a battery of collected infos agains the decision logic and be sure it makes the right decision in each case. Punt makes the first feeble attempt at collecting info.\n. This is good. Here is how it might work.\n1. Punt gives you better explanation and invites you to participate in a drop survey at, say, drop.fed.wiki.com.\n2. The survey contains a dozen Factory plugins with instructions as to what and how to drop things there. \n3. The user can review the the json of the local storage page before submitting the changes to the drop collector site.\nThis would remain a public reference for anyone hacking the Factory code. \n. This looks useful. Let's make items of type \"paragraph\" instead of type \"data\". Its a little harder to see the metadata but it can be done through the ui. Then the text of the paragraph could even offer an explanation, something like: \nTrouble: We detected the drop but cannot yet make sense of the information provided by your browser and operating system. Try something different. If you know that your drop should have worked and you want to file a bug report you can do so on [https://github.com/WardCunningham/Smallest-Federated-Wiki/issues github]\n(You would read the hidden data by clicking on the journal entry and then double-clicking on the timestamp to see the item in the action that created it.)\n. Commit 9f0bbec656b26fefaeca08465e3e44544d9193ff adds a server-side check for this problem and fails with a non-destructive error.\nThe scenario I describe above does not require a separate browser to elicit the failure. It is sufficient to edit a page from two different tabs. This situation should be detected and reasonable alternatives for going forward presented. One possibility is to fail the edit and then have that save the edit in browser local storage so that the edits can be resolved.\nIssue #342 presents similar needs to save content while resolving failed or destructive operations. It would be great if they (and any more if there are others) could be handled with similar mechanisms. \n. I've added some red warnings that seems related to this error. Have you seen these \"Can't make sense ...\" messages?  Do you recall what you were doing when they first appeared? Did it involve editing from multiple browsers? Multiple tabs? Thanks.\n. I notice that some lost paragraphs are available in the history.\n. We do use browser local storage when we can't write to the origin site. Pages from there are rendered with a yellow border.\nWe don't have a lot of experience with Windows. Perhaps we could skype and look at this together? That would be the way to learn something. I could also just edit the db and make it go away if that is your preference. \n. I'm sorry you've lost a page. Welcome Visitors does seem especially vulnerable to this mistake. Replacing the journal makes it especially harsh.\nI have thought that the proper action would be to merge the journals from where they diverged, possibly from the beginning. There is probably some drag and drop shuffling of the journal that can resolve conflicts, if any.\nI've also thought that an un-delete should be available in a number of situations. This is one. Maybe replaced pages go into a /data/deleted directory.\n. An interesting experiment would be to detect a damaging fork (by looking in the neighborhood sitemap) and then producing the merge as a new, ghost page, instead of completing the fork. This would allow you to play with merging techniques and have some confidence in them before you start writing back to the page store.\nYou might require that two pages have at least some actions in common before you try to merge.\nThis is an area where I've tried to let operational experience guide further development. To that end I make many sites and muddle through with the basic tools all ready in place. Merging has been on my todo list since the beginning. I can't wait to see how well it works.\n. An exotic tool might be an action sorter that would allow one to drag merged actions around and see the impact that these changes have on a result. Actions that fail to apply might show as red. Maybe you could drag one back and forth to find where in the journal it would apply. Crazy things are possible. \n. This issue points out the need to not casually throw information away. Specifically, the within page journal is not enough history to avoid all losses. I am on the verge of implementing a deleted page cache. I mention @nrn and @harlantwood now because they have written server-side storage modules. Here are my thoughts as to how delete should work (copied from here:\nTrash is one deep. Up to server for where it is stored.\nDeleted pages are retrieved with slug suffixed with _old.\nDeleted pages are included in a sitemap. How?\nDeleted pages appear in future pages and twin links.\nDeleted pages show as ghosts that can be forked back into existence.\nDeleted pages have revisions suffixed _old_rev15\nRename will leave old page in trash.\nDangling references will find old page in future page. Do we want to remember the new name?\nRename to blank is like move to trash.\n. All good observations. I worry about the case where 'Alice' is a spammer. We all suffer a little because there are spammers in the world. \n. The simple solution is to not accept submissions. SFW doesn't need them. If you want me to follow your wiki there are many ways to reach me. Send me a link.\nSFW is an editor that saves its work in files. That makes it as compatible with git as any other editor. \n. Really, your connectivity provider blocks port 80? Here is a video where I explain why \"service\" is the highest and most creative kind of internet access. It should be illegal to block port 80.\n. Hmm. I think we're moving to a model where the plugins are managed independently from the core. This counts as a shaky first step. Do you have a suggestion as to how it should be handled? \n. We have code that runs on both the client and the server. \n. Yes, it makes sense for plugins to be at the same level as client & server directories. There are probably other options that we will want to consider. For example, a deployment configuration could be different than development. Also, we should optimize npm based deployment however that works.\n. I've added a try-catch around the plugin server start code. This gets the server going and reports reasonable diagnostics. Client side of plugins are tolerant of server-side failures. \n4dd298b864 (continue serving when plugins fail to start)\n. We might want to eventually come up with a quick way to \"install\" every plugin to make it easier to get a full site up and running. For the moment we're happy to have site operators know what they are serving. So I'll close this issue for now.\n. This commit didn't work for me. I've beefed up the defensive code with f8a27928ccf1f6244be5052d12b7e004b47eb63d.\n. With the addition of server-side plugin modules we've made the install more complex. We are discussing this on Issue #343. Perhaps we can make the plugin launch logic tolerant of incompletely installed plugins?\n. I've added a try-catch around the plugin server start code. This gets the server going and reports reasonable diagnostics. Client side of plugins are tolerant of server-side failures.\n. There are only a couple of plugins that have server-side parts. You should be able to ignore these for now since they don't interfere with launching the http server. Or, as @hallahan suggests, you could just npm install them.\nThe Factory plugin is the first one that is dynamically loaded. It looks like this isn't working for you. If you open chrome's network inspector you should see the client get /plugins/factory/factory.js. If you're not sure what you should see, try using the inspector on http://foobar.fed.wiki.org.\n. I haven't seen this problem. Is there a client/plugins/factory/factory.js file?\n. The gray box comes from the css which has loaded. If I understand your situation, your server has never correctly served a plugin js file. So it does, core js and css but not plugin js. I'd guess you are invoking the server from the wrong place or with wrong arguments or something like that.\n. There was a time that I foolishly did surgery on the factory.js code as it was served. @nrn duplicated this code in the express version. I rewrote the client side to simply request the menu contents, /system/factories.json, in a second server turnaround. The client is happy to work either way. However, It appears you are having trouble with code that we'd like to abandon.\nPerhaps you would like to try implementing /system/factories.json in express? It is easier to test than lots else in SFW since you can call the new code directly with your browser:\nhttp://fed.wiki.org/system/factories.json\nOnce this code works you can delete the special case handling of factorie.js in express just as I have in the sinatra version.\nI'm sorry this has been such a rough path for you. I do appreciate the time you've put into it. I can't wait to see your first pull request. I'm happy to double check your work.\n. I'll let #275 stand for this problem. @rynomad, thanks for good explanation. \n. Even tho it's called wiki, there is no provision in SFW for sharing an individual site. The sharing is all above the level of site. Yes, I understand that this does not meet most people's expectations.\nI host fed.wiki.org for the purpose of testing and improving SFW methods. I put up with login the way it works now because I'm not researching login mechanisms (yet). I would love to see openId replaced with webId but that isn't a high priority. I also see the need for more elaborate provisioning mechanisms developed by people who will run more elaborate farms.\nI encourage you to play with the space you can find in fed.wiki.org and share your insights here. Once you have a sense of what you want to do, you should make a farm under a domain you already own.\n. Incidentally, I make sandbox.fed.wiki.org un-claimable by running a cron job that erases the claim. It also sets the site back to a known-good state. Think of it as raking the sandbox. Good by castles.\n. I use https://github.com/WardCunningham/farm-scripts for that purpose. \n. Or move the script next to the deeper pages directory.\n. I'm sorry it too so long to get to this. After trying it out it occurred to me that we should leave the factory as a factory and use the factory's prompt mechanism to report advice. \n. We are inching towards elegance.\nWe've shown that we can establish independent socket connections between client and server, over the same port even. The logwatch plugin does this by connecting to a single-purpose hook in the server and receiving page view events as they happen. See source.. Thanks to @hallahan for this.\nYou will see a few lines above the logwatch hook where we mention the goal of replacing them with a more general plugin approach. Thanks to @nrn for help here. The ideal would be an even more strongly event-driven server with fit for purpose plugins that could broadcast appropriately aggregated and filtered streams to clients.\nMy own work reaches out past the server to the management of long-running data processing processes under close supervision of the client. This is in support of lab.fed.wiki.org where customized c code parsers are generated on the fly and they stream back parse statistics as they run. \n. Um, you put some javascript in a file and it runs it? I'm not sure what you're asking for?\n. Ah, we are talking about the same thing. There is one server-side hook: server.js. If you put that in a plugin then it will be loaded and the function startServer(app) will be called. App is an object containing the Express application server. This is a wide interface with a large attack surface. We'll narrow that as we learn more about how this hook can be useful.\n. Thanks for this. I especially like the punctuation reduction.\n. Both errors were in the Sinatra server where it handles actions (incremental updates). One is handling Edit, the other Remove. My suspicion is that these are both related to issue #337 but do not yet have a smoking gun.\nWork around is to simply refresh the window/tab so that the client and server are back in sync. Sorry for the inconvenience. \n. Regretfully this is a long standing bug usually excited by hitting two rapid returns thinking a blank line makes a new paragraph. Instead two ajax requests are sent and get confused in the server. It should be impossible to confuse the server no mater how fast one types, or how slow the network. I've postponed working on this waiting to isolate and modularize the client-side storage adapters as separate npm modules.\nOf course servers can get way out of sync when network connectivity is lost. My thought would be to slip into local editing at the first sign of trouble and then let the user push edits back to the server when convenient for both. This is too much logic to code until completing the suggested refactoring.\n. This issue has finally been addressed by https://github.com/fedwiki/wiki-client/pull/50 which saves an edit in local storage when a write to the origin fails. \n. I'm sorry it too me so long to get to this. Thanks for your patience.\n. Oops. Sorry to have ignored this observation. Yes, you are surely right. GitHub has the advantage that it is more reliably up than any individual SFW server. The GitHub wiki has the advantage that it doesn't take a pull request to share experience. But there should be a clear story for how to get going in every environment and we don't have that yet. Here would be one suggestion:\nMain page explains were to find all documentation. \nOther md pages in the code base contain advice for coders.\nGitHub wiki has user curate advice on getting started on diverse platforms.\nFed.wiki.org has how-to documentation.\nProject history and retrospectives are somewhere, maybe ward.fed.wiki.org??\n. Another thing I should explain: I've been experimenting with federated wiki farms that support workgroups that are separated from the internet. This is an outlier usecase for sure. But it does raise to me the question of how one hosting a farm could insert specialized instructions for first-time users that engineer a sensible experience. \n. I see that there has been much conversation. I will catch up soon.\n. I'm sorry it took me so long to get to these pull requests. Thanks for your patience.\n. @paul90 Thanks for all this work. I'm a little behind on the pull requests, especially when I want to study them carefully. I've gone back and forth with event machine and have some other outstanding pull requests that suggest other mechanisms. I encourage others on windows or otherwise to give Paul's work some attention. \n. Uh oh. This is a case of me knowing too much git for my own good. I forced those files into the repo with the -f option on git-add. Oh, but it was late and I wanted to amend the previous commit so I undid the adds, did the amend, and then forgot to force the new files in again. Might be time for some more simplification.\n. Its hard to say what GitHub thought happened. The git log looks clean enough and gitx which I use looks as clean as to be expected. We have tried the rebasing methodology which can make for super clean history but also removes many safety features of git. We learn together.\n. Good catch. I make excuses in #359. \n. This appears to be a case of documentation lagging behind an old fix. (I write #126 here because it is the fastest way I know to navigate to this issue on github.)\n. I'll offer some history but stop short of providing a solution. Sorry.\nFrom early in the project I've felt it important to generate flags for each wiki and that these flags should have a strong family resemblance. (I stopped calling them icons so as to discourage custom designs.) Even as one copies content from other servers, the content refers back to the sites of origin for its flag. Should a site disappear, its flag goes \"gray\".\n@nrn had trouble finding a server-side component that could duplicate the bit manipulation in node that I was doing in ruby. He devised the client-side flag creation mechanism. We were both working against selenium tests at the time and were happy enough to see them working. I have noticed that the flags I generate have a lighter color above a darker color while Nick's are reversed. This was an accident but has remained a subtle clue as to which server created a site initially.\nI've since lost track of which server does what now. I do run both ruby and node servers routinely and haven't noticed any trouble creating flags. I don't doubt there are loose ends as this was early code for both of us. Selenium served to unite the two servers but has been cranky in its own way. I've been relying on Mocha for all the work I do with client-server plugins.\nIf you're thinking of working in this area I would be happy to help or at least share some hopes and dreams as to how we can have more beautiful flags, all built with common code,  and no machine code dependencies. This should start with a pure javascript png gradient generator. \n. Hmm. Thank you for taking the time to look into this. To be clear, you are having trouble with the Sinatra version, right?\nI use Webrick because it avoids some self-reference problems that plague Thin. This could be a limitation in Webrick which is not considered ready for production. I have an outstanding pull request to convert to Puma which promises to be the best of both: production ready and free of self-reference problems.\n. Ah, that could be it. On OS X and Linux it doesn't matter.\n. There could be some spots where we use / in file paths instead of constructing them piecemeal using portable function calls. Ruby goes to some effort to work around this problem but it doesn't handle every case so the portable software author has to know the system conventions AND the extent that ruby accommodates them.\n. I pushed some documentation on the Linkmap plugin. This makes for a good manual test. Don't forget to npm install the Linkmap plugin. Then read this on your own server:\nhttp://localhost:3000/view/about-linkmap-plugin\n. I'm so still a node newbie.\nWhat will I and others have to do when we have pulled this change? npm install? Is that required? Is that enough?\nRegarding refactoring, \nI assume you are talking about those websocket experiments that are taking up space in server.coffee. I wouldn't be hurt if they just disappeared. I think @hallahan and I were the only ones who used them.  LogWatch hints at some useful functionality. We should consider someday how a plugin can listen to the parent server do ing its serverly things. If LogWatch is on the right track here then maybe it should be converted to server-side plugin to show us all how to listen to server events from a plugin.\n. Still a little backed-up here. Will pull this soon.\n. Thank you for exploring this.\nI've been told that the work we've been doing might not ever run on anything less than IE 10. I've used that as an excuse for ignoring this and other compatibility issues. I know that this \"luxury\" can't last. However, I suspect we will run into javascript engine issues for which there are no easy work arounds as we go further back in any line of browser implementations.\n. I'm backed up with a week of travel. I look forward to giving this pull attention on my return.\n. This is a big pull. Thanks.\nI took stringify out of the tests because that doesn't seem consistent either. Let's try comparing Date objects, not the print representation of Date objects. \nThere are parts of this pull that I don't understand. That makes me nervous. It would be great to get a verbal walk-through sometime. Email me if you are willing to spend a half-hour with me.\n. Thanks for the contribution. This will be demonstrated at the 2013 MicroHAMS conference.\n. @nrn mentioned some dependency that prevented using v0.10 in a recent conversation. I struggle but can't remember the details. I mention him here hoping he will remind us all. \n. Wow. This is just too cool to not deserve some mention in the readme files.\n. Awesome. I'd noticed the delay on occasion but had not turned to debugging. Will pull soon. Thanks.\n. I've favored the convenience of plugin authors (like me) by not caching the plugin menu (factories.json) in the server. The client code does cache factories.json content so it won't be retrieved more than once.\nIt looks like ruby on my localhost will generate factories.json in 12 ms. Node takes 750 ms without your modification. Let me pull that and get a better number.\n. You're commit improves fetches of http://localhost:3000/system/factories.json from 750 to 250 ms on my MacBook Air with SSD. The ruby code makes no attempt to be quick but seems quick none the less. See source.\nI'm guessing the \"glob\" package may not be doing us any favors. \n. Thanks for tracking this down. The moral of the story is that if with each port or translation you loose an order of magnitude of performance then pretty soon your code won't work. Said another way: you shouldn't worry about performance, except when you should.\n. @harlantwood added the heroku gem as part of his work running SFW in that environment. I mention Harlan here in hopes that he can assess the best next step for preserving his work as Heroku makes changes.\n. Sounds good to me.\n. I have seen this failure myself once and corrected it for others on several occasions. I'd love to have a repeatable test, maybe a sequence of curls. \nBest regards -- Ward\nSent from my mobile\n503-432-5682\nOn Jun 1, 2013, at 4:04 AM, almereyda notifications@github.com wrote:\n\nIMO still a bug in the write-to-disk-engine, though.\n\u2014\nReply to this email directly or view it on GitHub.\n. I'm assuming this is a transient error that does not always ocurre when deleting paragraphs. If otherwise, please tell us the circumstances that make the error repeatable. \n\nHere is what is probably going on. The client code sends edit actions to the server where they are reinterpreted for the server copy of the page. When the client and server get out of sync the code at line server.rb:259 stumbles. A few lines above the sync loss is specifically detected to avoid a corrupting modification. This particular loss doesn't corrupt so we don't catch one error to report another. \nMuch has been written recently about better architectures for synchronization than we currently employ. We hope to catch up soon. Until then, perhaps a more graceful error handling is in order.\n. I have some experience with OT and was thinking in that direction. If each site has a single owner then the resolution required is simply protecting that owner from his own concurrent writes from, say, different browser windows.\nIn this case we are probably talking about a timing bug caused by careless synchronization client side. We're hoping to improve the client-side models making tackling this synchronization straightforward. \n. This issue, as initially reported,  has finally been addressed by https://github.com/fedwiki/wiki-client/pull/50 which saves an edit in local storage when a write to the origin fails. \n. I hope to see @ozten at the Indie Web Camp this weekend. Maybe we can work through some of these install cases then. Here is the sort of messaging I'd expect to see in the footer using parens to indicate buttons:\n(login) as ward\n(logout) as ward\n(claim this site with your email)\nPresumably the login case would know my full email, ward@c2.com, and enter the persona verification with the full email address from storage. We currently confuse visitors by suggesting that they should login to sites that they cannot own. After we complete the login process (with openid or persona) we report \"this is not your wiki\". Gack.\nI'll have to think through with Austin how this could work when I want to use multiple email addresses interchangeably. Also, another case to consider is when, say, any c2.com login will grant read access but only ward@c2.com grants write access. \n. Austin, Paul -- I really appreciate the effort you have put into Persona. I could see a collision coming with Nick and Christian's work on build process, which is client-side only at the moment. We've always thrown compiled javascript into the repo just to make installs easier. This may have been short sighted. Soon we will fix this by having a separate deployment mechanism. Until then, we won't worry too much about javascript thrashing back and forth with build system variations. Thanks.\n. Thank you both for all your work. When @ozten and I started this on hack night, I suggested we set both OpenID and Ruby implementations aside. Now I'm faced with what to do with all of the sites I'm hosting based on Ruby and OpenID. I'm sure you have left me with the raw material for an excellent solution. Help me think through what this might be.\n. This work merged soothly on my laptop. @nrn helped me figure out how to configure git to remote all outstanding pull requests.\nI'm surprised that the ruby and node versions continue to work for the same site. How can this be? I notice that both versions write identity information into status/open_id.identity. This is my email id when using node/persona. If I delete the file and then log in through ruby I can claim the site again with openId. Then I find an openId domain in the status/open_id.identity file. This makes sense. But, hey, the node/persona version still works. How can this be? Where does it keep my email address? Not in status/open_id.identity, I deleted that.\nI'm trying to assess the work still required to coexist with ruby. @nrn suggests splitting the repos. It would be easy then to become more node-like in everything, including install and plugin distribution. Thoughts?\n. Thank you. I apologize for the small and careful steps when there is so little at stake. It seems now that users of a ruby site can coexist with users of a node site. The owner will need to identify twice if he/she wants to use both.\n. We've created a new repo for the node.js server implementation. It will authenticate with Persona using this code you have so kindly contributed. We've published this as an npm package using directory structure much more like one would expect of a node program. If this works well, we expect it to become the reference implementation for federated wiki. It is my intention to close this pull request once the health of the new repository has been established.\nI have two regrets. First that it has taken me so long to incorporate this code anywhere. And second that we lose the contribution history in the move. This repo has been fattened with generated javascript. The new repo will be developer focused with coffeescript only while the generated javascript will be published in the npm module for simple deployment.\nI look forward to your ongoing contributions to which ever repo you find most useful. Thanks again. \n. If there are no complaints, #398 will replace the client-side javascript with the wiki-client npm module including the persona modifications included in this pull request. This has been the long way around for sure for the node version, which we will also remove from this repo making it exclusively server-side ruby for federated wiki. At this point we'll close this request.\n. I can't wait to try this. Thanks.\n. @ozten just read your wiki pages. They help. Thanks.\n. I thank you for the work you have done. I'm happy to be using Persona for the moment and getting a feel for its many features. Let's follow Austin's lead and move our speculative forward thinking to SFW pages. Immediate coding issues should stay on github in the appropriate repo.\nI'll be speculating in http://own.fed.wiki.org.\n. Thanks for pointing out this capability and offering this config file.\nWhere would be a good place to host a sfw site using this mechanism? How would one install updates without losing the flat-file database?\n. Help me understand where this is useful.\nPages fork automatically when edited. If one wanted to copy lots of content from another site then maybe some even more automatic operation should be provided. I've been waiting to concoct such an operation (fork whole site, maybe?) until I understand how it would be used.\nOh, and thanks for your patience with all your contributions. -- Ward\n. Do you have a clear notion of what is happening with respect to specific requests and replies?\nI could be wrong, but I believe the only redirecting we do is to convert server-site rendered requests to client-side rendered requests when the client has javascript enabled.\nward.fed.wiki.org/ward-cunningham.html ==> ward.fed.wiki.org/view/ward-cunningham\n. Well, this had me stumped for a while. The problem is that those pages aren't served by this server, neither the ruby or node versions. The links above are served by an experiment that used Apache rewrite rules to make it look like a perl script was a full-blown sfw server. You can find that server here:\nhttps://github.com/WardCunningham/c2-sfw-script\nI'll close this issue. An issue on that repo is welcome. But I'm operating at the limit of my Apache config expertise there.\nA quirky work around would be to cite a full-blown sfw server and pass it sfw.c2.com page names like this:\nhttp://fed.wiki.org/sfw.c2.com/welcome-visitors/sfw.c2.com/re-factoring\n. The Reference Plugin could be extended to have a version property in the json. The version is known since we fetch the remote page's json in order to find the text for the reference. Perhaps a shift-click on the reference's flag would be the way one requests the cited version rather than the most recent.\nOnce the reference has been created the author is free to edit the text to be what ever is useful. I'm not sure how this mechanism can be improved without extending our (admittedly simplistic) user interface. Were there some way to naturally indicate a paragraph when the reference is created, then that paragraph's text could be used immediately (so long as it has text). We could also store the id of the paragraph and use that to scroll to and highlight the paragraph on retrieval.\nPerhaps a reference that has been so augmented could be rendered with a double-quote mark in the flag. This would be suggestive of the large gray quotation marks that are popular in blogging software.\nWe have so far resisted operations that cause updates to remote sites. In federated wiki, one only writes to one's own site. This conviction makes some of the suggested use-based queries hard to implement.\n. The node/express implementation was written after the ruby/sinatra version with the goal of meeting the same functional tests. The two implementations differ where behavior wasn't tested. Both support farms, but by different means. Our energy (for the moment) is going into the node/npm version @paul90 mentions. \n. Thanks. I'll close this now as suggested. \n. I'm happy to be moving forward with this organization. I'll follow this with commits that remove code made obsolete with the creation of wiki and wiki-client repos.\n. Thanks. I also would like to learn how to make and manage a \"circle\" that can be invited to the hangout. \n. I've added some notes on CDN caching and fallback to @paul90 refactoring page: \nhttp://ward.fed.wiki.org/code-refactor-nov-2013.html\n. Awesome work Paul.\nI fixed the typo, --data.\nI can't wait to try it.\n. I've tried an install on my mac. It installed smoothly but won't start. Here is what I've tried.\n$ wiki-exp\nenv: node\\r: No such file or directory\n$ wiki-exp --data ~/.wiki\nenv: node\\r: No such file or directory\n$ mkdir foo\n$ wiki-exp --data foo\nenv: node\\r: No such file or directory\n. Updated to version 0.1.1. Works great. Thanks.\n. Wow. This is fantastic analysis. Thank you for the thorough step-by-step. I'm still studying it.\n. Paul, I have granted you awesome power over the fedwiki organization so that you can create the repositories that you suggest. Please email me directly at ward@c2.com regarding rights and responsibilities related to this github mechanism. Thanks.\n. You've set a good lead in the organization and documentation of each repo. I will go through each one and spruce up the documentation. A few of the older ones don't have wiki-readable pages. I'll fix that too.\n. @almereyda -- I considered registering fedwiki.org but didn't. I had to remind myself that my goal was to \"not own\" federated wiki. With that in mind I would be pleased to see quality services provided by others and especially others with an anti-monopolistic bent.\nIf the fedwiki.org site were to suggest a strong association with our github repos then I would rather see it devoted to content related to those repos. Perhaps *.wiki-plugin-method.fedwiki.org would be many sites related to the ongoing development of the Method plugin, for example.\nI'm not opposed to fedwiki.org or any other name variation being only loosely related to fedwiki. If you were to run a hosting service by any such name then you would be in a service business. I would hope that your service would do well by our name. But your relationship with your users would not extend to the volunteers contributing to fedwiki repos.\nThese are my thoughts of the last day or two. I hope they are clear enough to provide some guidance. \n. (This text scraped from localhost. I post it here to expose my work in progress.)\nThis is a mechanically generated list of plugin document updates produced by comparing localhost/pages to clien/plugins/*/pages.\ncd ~/farm/localhost/scripts\nruby docs.rb\nLocal About Pages\nWe look for local about pages and check them against plugin pages, if any.\nAbout Parse Plugin.\nPlugin Pages\nWe check each plugin for expected pages and local copies of any extant page.\nAbout Bars Plugin. Needs about. Can't diff d3-bars.\nAbout Calendar Plugin. Needs pages.\nAbout Chart Plugin. Needs about.\nAbout Efficiency Plugin. Needs about.\nAbout Federatedwiki Plugin. Needs pages.\nAbout Line Plugin. Needs about. Can't diff d3-line.\nAbout Metabolism Plugin. Needs about.\nAbout Parse Plugin. Needs pages.\nAbout Scatter Plugin. Needs pages.\n. Ok, this makes sense. I was beginning to see that but was unsure what the next step for me would be.\nWe have lots of interlocking parts which will become much simpler when reorganized and fully scripted. I thought I'd try a from-scratch build just to see if it would work for me. I have several other copies of repos that are tied together with symbolic links. They make me more nervous than your changes. I'll be happy when I can leave them behind.\n. I got several links wrong. Both fixed now. Thanks.\n. Wow. That PrintWhatYouLike service is an eye-opener. I'd been ignoring print thinking that mobile would replace it. (It still might.) But the service shows how much they can do without any help from federated wiki. Here is the full link to their page printing my page:\nhttp://www.printwhatyoulike.com/print?url=http://code.fed.wiki.org/hyperperl.html\nThey fetch the server-side (no javascript) version of the page and print that. You can see these yourself by turning off javascript in your browser. I notice that Reference plugins don't render server-side. That shouldn't be hard and it is a core plugin.\n. I suppose the proper thing to do would be to take all of the open page and flow them into two column print pages (three column in landscape). There is enough paragraph structure to avoid widowed lines. More graphic elements could float. Images might expand to full column width since print trades resolution for color depth.\nI'm going to have to leave this to someone who knows beautiful printings.\n. I find a common mistake I make is to start editing before I login to my site. This creates an unwanted copy in browser local storage that gets in the way of further editing. I have followed two approaches in this situation.\nVisit Recent Changes => Local Changes where I can delete the local copy. Login. Continue.\nOr, more daring:\nContinue. When convenient, Login. Then \"fork\" the local copy back to the server.\nThe fork-to-server functionality is not uniformly implemented across our servers. It is awesome where it works. (The yellow border just disappears.) I'm looking for ways to more generally move pages around across firewalls based on this technique.\n. The project would benefit from all that you suggest. I would be pleased to see it happen.\n. We did once support a query for a page that had a dataset by name. The idea was that a plugin could look to its left for data and if it didn't find it, then it could ask the server if any page had the data. The Metabolism plugin used this feature.\nhttps://github.com/fedwiki/wiki-gem/blob/master/lib/wiki/server.rb#L155-L167\nhttps://github.com/fedwiki/wiki-plugin-metabolism/blob/master/client/metabolism.coffee#L20-L26\nIf I had already fetched the whole site, like we have considered at the start of this issue, then it would be a client-side extraction to find and attach the desired dataset.\nI wonder what @StevenBlack thinks could be done client-side to parametrize queries and then execute them server-side. The Metabolic Calculator is a query after all.\nhttp://fed.wiki.org/metabolic-equivalent-of-task.html\n. Thank you Steven, JSONSelect is just the sort of generalization that I find useful. The Metabolic Calculator is too application specific. This wiki's general plugin architecture, too general. We spend a lot of time \"wrangling\" information. I think of wrangling as making structures match without doing too much damage.\nhttp://vis.stanford.edu/wrangler/\nI think Henry is describing a world where wrangling is automatic. I like the vision but I don't know how to do it. A reference to LDP would help. I found http://en.wikipedia.org/wiki/Label_Distribution_Protocol but think that is something else.\nMy own pet theory is that there is power in linearizing. The power comes from organizational convenience of a less expressive structure. There are lots of examples here. A favorite of mine is \"scrubbing\" with the mouse over a sequential dataset. Scrubbing works when there is one primary dimension of aggregation and that the aggregated elements have some similarity. I've linearized pages, both the story and the journal. And the lineup too (the sequence of pages on the screen or stored somehow we haven't yet figured out).\nWiki is an editor. I made \"move\" the first and most highly developed edit operation. I imagine this as organizing blobs of json under sentient, motivated, creative control. My thesis, if I have one in this experiment, is that a federation of editors creates a medium, a landscape, within which selfish-blobs thrive.\n. Latour has been mentioned in good light in our happenings so I took some time to investigate. I examined these sources. I did not join the inquiry.\nhttp://modesofexistence.org/\nhttp://modesofexistence.org/tuto-contribution/\nhttps://github.com/medialab/aime-core\nhttp://www.sciencetechnologystudies.org/system/files/v27n1BookReview1.pdf\nThe interactive application appears to be a hypertext organized into three domains were one includes user-generated content. Links carry an attribute, the mode, of which there are provisionally fifteen. I would describe this as a \"schema\" imposed on the hypertext, presumably to the benefit of the project, the inquiry. Latour has been defensive of this schema arguing that though it might be a \"system\" it is one that has surfaced through years of work, not one that is preconceived.\nA new implementation is mentioned in blog posts. Aime-core has three or four contributors working this year in angular, node and Neo4j. This all seems fit for purpose but is neither distributed or designed to outlive the project.\n. I poked around at the leftovers. It remains much as I remembered: promising but hard to find the meat.\n. We currently support \"forking\" a page from local storage to the origin server. This is handy when you forget to log into your origin server and end up with edits in local storage by mistake.\nI would like to provide similar functionality for forking a remote page into the origin server without requiring visibility of the remote server from the origin. This is a departure from the status quo. We currently send the fork action to the origin server and expect it to retrieve the full json for the page from the remote, server to server. This breaks down when trying to \"export\" a page from a private server to a public server.\nI was working very close to this code recently. The choice of approaches comes down to one if-statement in  wiki-client lib/legacy.coffee where the fork-button is handled. github\nThe server bound payloads are very distinct in these two cases. Both send actions, but in one the whole page is sent as an \"item\" while the other case just sends the domain name of the remote site.\nWe GET pages but PUT actions. This is probably premature optimization on my part. There is a low numbered issue here where @SvenDowideit asked if we couldn't make the protocol more RESTful. I've only slowly come to understand the wisdom of his request.\n. Wow. The quality of advice given and ignored in #42 is scary.\n. @almereyda Thanks for the reminder. This issue has been long solved.\n. Tim Berners-Lee, the inventor of the Web and Linked Data initiator, suggested a 5 star deployment scheme for Open Data. http://5stardata.info/\nEarly on I self-assessed decisions I had made in federated wiki and gave myself three stars.\nLater I discussed my interpretation of the five stars with a hangout visitor that was steeped in the theory behind the semantic web. He argued that I really did deserve all five stars. I thought his standards for stars were pretty low. But then I assumed I would be judged by the schema-heavy existing work in the semantic web.\nI'm not quite getting the notation you're using in this issue. Let's continue the discussion in this issue and see if we can tweak federated wiki into a new path to semantic data.\n. This advice comes at a very good time. My own summary of server classes ends with the realization:\n- Servers shouldn't need to participate in browser history by parsing and echoing lineups.\nI've come to this realization in my work with @1337807 on a minimalist federated wiki server in Go where we attempt to stay within core modules and Go centric idioms. Parsing /view/slug/view/slug has been clearly a mis-fit even in this very modern programming environment.\n. Good point. We were a little too attached to regex routing since that is what we had been doing in Sinatra. We do want the added validation provided by regex in the Factory drop logic but that is in the client where it is well supported.\n. Yes, we went down the custom router route. It became more complicated than the rest of the server. I had to stop and ask, what is it about wiki that requires this complexity. My conclusion: nothing. Hence my desire to simplify.\n. Federated Wiki's identity needs are modest. The server must know, is this your wiki? If not, you can't write. \nPerhaps we can assume that the first person to visit a new wiki is in fact the owner. The server could generate a suitable credential and share it with that person's browser. This completes the \"claim\" process. Let's call this the zero-steps to claim solution.\nThe primary advantage of zero-steps is that it just works without any explanation. No entry fields or buttons are required. No identifying information need be offered. No service dependency either.\nServer-side plugins could distribute the credential by a variety of means. Maybe the plugin asks for an email address or a smart phone number and uses that for sharing. This begins to feel like login. However it works, it is a feature of the plugin, not the core wiki client or server code. And it is not the first thing you have to do to start using wiki.\n. Let's imagine that a company with LDAP based SSO wants to host a farm for many conversations. Sites could be \"born claimed\" so that all edits end up in browser local storage until the wiki-plugin-ldap-login creates shares credentials allowing saves to the public site.\n. I've now discussed simplification with @donpdonp at Open-Source Bridge, explored IndyAuth with @aaronpk at Indie Web Camp, and read the @bblfish interview and related posts regarding WebID. I'm convinced we all agree what forces are in play.\nHowever, our goal is much more to innovate as a creative space than to demonstrate a web of trust. As such, we moved login from the header to the footer. Now we should bury it down in some profile pages which our authors may or may not choose to use. This attitude was expressed early in the project history as have fun first. We need more of that.\nhttp://ward.fed.wiki.org/have-fun-first.html\n. I've revived my YAHOO account and used it to claim using the (Y) option. This is just me hanging on to old code rather than investing the effort for a permanent fix. When the big fix happens, I'll have one more test datapoint for automatic conversions.\n. The source code we distribute here is dual licensed, mit & gpl. You are free to use and distribute this code as those licenses allow. Of course you are free to write and distribute your own wiki using what ever license you choose.\nThe federation mechanisms built into this software assume that content is shared under a specific creative commons license. This shouldn't be changed.\n. We made the choice last summer to use npm as the preferred plugin registry. However, anyone operating a site is welcome to come by plugins any way they choose.\nWe ask that all plugins include documentation and that the documentation point to the source repository. This is a courtesy to readers. If one encounters a plugin in the wild they have the means to acquire it as their own.\nBoth the ruby and node servers will reveal their full complement of plugins in response to an api request.\nhttp://fed.wiki.org/system/plugins.json\nIf a plugin author makes their plugin available via npm then we have the automation in place to retrieve that plugin with each install of wiki. A site operator has only to add the plugin's name to the package.json file.\nWe distribute a sample package.json. We would happily receive pull requests to add additional plugin names to this. We do feel some obligation to consider such requests with care as we don't want to become a distributor of malware. All readers here should register a watch on this repo and help vet pull requests for new plugins.\nhttps://github.com/fedwiki/wiki-node/blob/master/package.json\n. The page fetching code in lib/pageHandler applies the simplest logic that will correctly interpret a link in the context it is found. This code has more resources available to it that could be incorporated into optimizations. For example, the ajax requests could be performed in parallel and all results displayed as they arrive. Or the neighborhood's sitemaps could be examined to avoid fetches that will surely fail.\nAuthors share some responsibility for slow links. When they cite a page that is important to their work and don't bother to fork that page into their own wiki they are leaving this task to their reader. When authors fork the reader gets the page the author expects. The reader will be alerted if there is a newer version becomes available. Likewise a reader who has wandered far from the origin would be wise to click the flag of the page they now find interesting. This will reconfigures the reading context and will correspondingly shorten searches.\n. This project was founded on the vision of a proliferation of servers exchanging and caching pages on our behalf. I describe this in this repo's ReadMe three years ago.\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/commit/21f4e7576fe555ae3eb2e1316655bb61236cdb0b#commitcomment-6670041\nWith this comment left this morning I admit that this much hasn't worked out. I have high hopes for IPv6 and even more for NDN and similar overlay networks. However, I can't see how these technologies become anything more than neighborhoods in a more comprehensive federation.\n. I'm understanding Paul's position better. Thank you. I took this issue to be more about offline servers and inconsistently configured sites. But having a zillion similar markups and plugins to render them doesn't sound attractive either. I fear if we choose a subset of html as the \"native\" markup we will forever disappoint those that want something we didn't choose. We had already given up on uniformity of markup by the time we published The Wiki Way. Instead the book suggested how one could add new markup that was useful to a community. Making a new markup is one of the joys, it seems. \n. Is possible to live without markup?\nHere is what we already have in our markup.\nLinks -- must have.\nUnicode -- must have.\nNewline -- makes bulk pastes work.\nHere is what I use from HTML.\nHeadings -- like to have, I only use h3\nItalic -- for separation that doesn't stand out\nBold -- for separation that does stand out\nBreaks -- in rare circumstances that I don't recall\nHere is what I don't miss and wish others wouldn't use.\nBullets -- small paragraphs make better bullets and links handle nesting.\nTables -- this is almost page layout. better to make a csv and drop it in.\nHere are \"figures\" for which I'd love to see great support in plugins.\nImages -- especially drag-and-drop of large and collected images.\nCode -- with pretty printing and generally pre-formatted.\nVideos -- new plugin, not with embed codes.\nMaps -- with geocoded input and temporary scrolling.\nCalendars -- for which there are three or four important use cases.\nThere are also data, computation, and visualization aspects which go beyond any sense of markup but will fit into our editing paradigm as domain specific languages.\n. Here is a strategy forward which is not the simplest, or the easiest, but which is along the lines I've been expecting we would evolve. The initial question was about errors, missing or failing plugin errors I assume. These can be made less ugly by replacing the message with a small faint error icon that must be clicked to see what is wrong. This leaves the page readable. It might even be usefully readable for missing plugins if the text field were rendered in place without transformation. So, if you wrote in markdown, and I didn't have markdown, then I would see your markdown source. I would survive.\nI further suggest that we adopt small translators for wiki focused subsets of popular markups. This list would include wikipedia, latex, markdown and html. This is mostly for the convenience of authors importing content from other wiki. It gets their bold and italic across without tedious editing.\nThe Factory plugin will create items from a menu. This only works well for plugins that expect to be edited with the textEditor. The Factory now organizes this menu using categories that have always been present in the plugins. The choices are format, data, other. If we stick with that theme, here is what it might look like if we add a few more plugins.\ncreole     geocode      map\nmarkdown   calendar     video\nmathjax    reference    code\nhtml       method\nCreole is the one we have never discussed. Creole is like simplified Wikipedia format. Its presence would complete the federated wiki's acknowledgement of other popular markups.\nWe could cherry-pick the features that we choose to implement in these markups to favor their strengths and avoid their weaknesses. The arbitration of what is in and out would be conducted in the repos for those translators. They could all share the same css so that the output look would be consistent.\nYou notice that our plain 'paragraph' format is not on the list. That's because it is the default. Its what we expect to be used when exotic markup features aren't required. Its what we will use in captions and search for to make a synopsis. It means that a mathematician can't slip a mathjax equation into an image caption. I'll take the heat for this limitation.\nAside: Of course a math-oriented site could hack their version of the Image plugin to render the caption with mathjax. But when those pages are viewed from other servers, the latex formulas would render as source. One would have to \"climb the trail\" to find what the original author expected.\nWe should offer these recommended plugins and deprecate the use of html in paragraphs at the same time. By depreciate I mean sanitize and then somehow encourage the conversion to other formats. \n. In issue https://github.com/fedwiki/wiki-client/issues/39 we discuss the multiple advantages of fast and incremental delivery of rendered pages from the server, especially when the content is expected to be read without javascript. Its fair to ask here, what obligation does the server have to render every markup in a readable way?\nHaving recently worked on some minimalist servers I would say, not much. Our current favorite, the node/express implementation, could easily build with the standard translators in the page rendering path. But I wouldn't demand that. Rather, I would say that rendering the various markups I've mentioned above in their (properly escaped) native form should be permissible. This aligns with my suggestion above that the text field of missing plugins should be exposed as the best effort to make content available.\n. @interstar Please describe some of the more exotic features of your personal wiki markup. I looked through your plugin and saw something about transclusion. How has that worked for you? Are you able to make it work as a plugin?\n. @almereyda started this issue with an offhand question:\n\nI wonder how errors are to be handled in the future, i.e. with more understandable explanations and no debug output.\n\nWe've now addressed these issues, first with a help button, and now by a best-effort approach to rendering item text in the absence of a working plugin. I discuss this more fully in a comment associated with the new video plugin.\nhttps://github.com/fedwiki/wiki-client/pull/48#issuecomment-48118966\n. The Plugins plugin renders the result from the server's /system/plugins.json endpoint that goes back to the ruby version. This turned out to be pretty useless since results were not otherwise annotated and didn't know of any that weren't already installed.\nThe Plugmatic plugin is aimed at managing lists of plugins that can be shared throughout the federation and installed and/or updated by npm through the web interface. This promises to be much more useful especially when new plugins are showing up regularly. I have an unsafe version of this working using the Shell plugin and am currently converting these to the new form.\nLet's think about how this scales.\nI imagine one or more sites devoted to curating lists of plugins that work well together. When there are many such sites we'll share rosters so that lists are easily browsed and searched together. Associated information would advise administrators resources required and risks endured supporting the members of any list. \nSuch a scheme has no central authority but does requires a critical mass of participation to keep it working and relevant. I set up a similar scheme for cataloging Transporters but it never included any more than my own Transporters. \nhttp://ward.asia.wiki.org/transporter-roster.html. I've added a section called \"How to Contribute\" to the fed.wiki.org faq. wiki\n. I have suggested that we capture html and markdown in plugins specific to those formats. I would like to start properly escaping paragraph text which will require some conversion process.\nhttps://github.com/fedwiki/wiki-client/issues/46\nIt would be nice to make a data viewer for the Data plugin. I'm more interested in visualizations so I'm not likely to do it. \nA good question might be what would be a more plain way to present the data. \n. I've written up some notes.\nhttp://ward.fed.wiki.org/condensed-journal.html\nI notice that I first add condensing to my todo list two summers ago.\nhttp://ward.fed.wiki.org/view/wants-and-needs_rev36\n. @coevolving, I'd like to correspond by email regarding patterns. I'm at ward@c2.com.\n. Twin pages are identified as 'same' when the last edit timestamps are identical. This happens when pages are published by copying them to a public site as I do with code.fed.wiki.org. You won't see same-twins because you won't see the source site behind my firewall.\nI discuss this publication strategy here.\nhttp://code.fed.wiki.org/view/welcome-visitors/view/exploring-federated-wiki\nThis is the code that bins the twins.\nhttps://github.com/fedwiki/wiki-client/blob/master/lib/refresh.coffee#L161-L163\n. I liked your reorganization of the FAQ and forked it replacing the old copy I had on ward.fed.wiki.org. You may not have noticed this unless you had my site already in your neighborhood. My copy wouldn't otherwise be noticed as a twin.\nYour awareness of my work would be different should I fork my edits back onto fed.wiki.org. (I can do this because I also login to that site.) Since your copy has fed.wiki.org in its journal you will always have fed.wiki.org in your neighborhood when you have viewed your copy of the FAQ.\n. There is room in actions for more metadata. Imagine that there were 'mood' choices in the footer and every action were marked with the current selection. That would be interesting, eh?\nWe could also create a new plugin for comments. Maybe they wouldn't show unless the page was served from your logged-in site. When I fork your page to my site, then I get to read the meta commentary.\nMaybe comments also show when recalling versions of a page. The editing of comments would show in the journal anyway. They might show differently so they would be easy to spot. A lighter gray?\n. Two twins can be merged by interleaving the journal actions starting after the common ancestor. I haven't actually coded this yet because I haven't felt a strong need to merge.\nAn interesting interface would be to shift-click a newer-twin flag and have the journal instantly update. This could be a trial merge that shows up as a ghost page, ready to be 'forked' back into the origin if it looks good.\n. The local storage was for a long time tied to a checkbox in the web page footer. Here is the initial commit.\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/commit/34844c3ce98733f3c8ea4a6fc3ff2dcc926914a3\nWe took the checkbox out when we made login/out more convenient. If you logout then your changes are stored locally. If you log back in you still see the local versions where you have them. You can 'fork' them back to your logged in server. A fork from local to origin storage shows up without color in the journal.\nWe use the term 'fork' to mean a variety of things, pushes, pulls, branches, clones and, yes, forks. Before github, 'forking' an open source project generally meant making a copy so as to gain ownership in different hands. We use the flag as the button icon to suggest staking claim to the page. We call the colored squares flags and consider them an emblem of ownership. \n. Oh, I would make comments move around the page as freely as any other content. I was thinking of Word's comments, not blog comments. \n. Here are some pages that might help with semantic/interpretation questions.\nhttp://ward.fed.wiki.org/view/abstraction-of-method/view/upper-name-hierarchy/view/special-page-names/view/where-numbers-live\n. One editor so  enjoyed fork's gruesome overtones that he (she?) titled an interview, \"Wiki Inventor Sticks a Fork in His Baby\".  The reporter was concerned that I would be upset. I wasn't. wired\n. This is a mock-up of a programmable site scraper that came out of conversations at the recent Indie Web Camp. The idea would be that the federation could keep track of formatting on important sites while end users could exploit this to import specific articles at their convenience.\nhttp://ward.fed.wiki.org/view/bbc-world-service\nMost scrapers that I've written so far just run at the command line. Another alternative would be to make a plugin that could be smarter about specific formats found in a cut-and-paste. If there were a hundred variations, there could be a site that cataloged them, one per page, with instructions for use. The advantage of cut and paste is that it bypasses cross-origin restrictions in the browser.\n. @jasonkgreen summarizes what works with pagekite here:\nhttp://blog.jasongreen.net/2014/08/personal-wikis-a-how-to-including-better-icons/\n. We keep this repo around because there are lots of people watching it. Interesting conversations of a general nature happen in the issues. I'd be happy to see more work done on the ruby version but node has attracted us all away. I'm putting my time into node.\n. This version is deployed and runs fed.wiki.org.  The nice thing about federation is that not everyone needs to be on the same version. I've added a note to the ReadMe explaining the status of this repo.\n. You are right about the button. It shouldn't be more complicated that that.\nCheck-out sites.fed.wiki.org. Let me know when you want to be included. Google will scrape wiki thanks to server-side rendering. \n. I have noticed these omission on occasion too. I believe they can all be traced to some question of semantics which I deferred. I once thought that forks to local storage deserved special treatment and should not be dated. I don't believe that now.\nI once added the code to cite the 'site' of cross-site drags. It was buggy in some complicated way so I decided to discard it until pageHandler was refactored.\nWhen we rely on the cc-by license for permission to mash up content then we should include the 'site' in the journal. I'd like to add a capability to mash up content from the public internet. In this case a type=quotation paragraph could be useful. Imagine dragging quotes from the BBC.\nhttp://ward.fed.wiki.org/bbc-world-service.html\nMy head starts to hurt when I think of where in the edit flow this need for special attribution would be detected. It would be awesome if it just worked though.\n. I have improved some code paths in pageHandler but there are many opportunities for simplification remaining. I have worked slowly here because many cases have order sensitivities due to the compounding of function. Let me mention a few.\n- handles gets and puts\n- handles origin (ajax) and local storage\n- handles explicit and implicit forks\n- handles successful and failed puts\n- handles storage and dom journal updates\nI would like to separate these functions into several layers where they can be inspected for correctness  and more easily accommodate extension. These layers might be:\n- observing and recording history in the journal\n- applying storage policy such as fail-over\n- performing storage operations to an abstract interface\nThe page data could move smoothly up and down this stack. However there is some situational awareness and some exception handing that must move up and down the stack too. I've yet to find the inspiration that would keep the one layer we already have from becoming three layers of equally brittle code. Accumulating experience can help. Auto-quoting the BBC is one more case to think about.\n. I've tested the dating of forks in the following conditions identified by 1) page origin, 2) nature of fork, and, 3) destination of journal update. I found two cases where forks were undated. Pull #54 mentioned above fixes one.\n- origin unowned local => undated (fixed)\n- origin unowned dom => dated\n- remote implicit server => undated\n- remote implicit dom => dated\n- remote explicit server => dated\n- remote explicit dom => dated\nThe other case is more complex. When a remote page is implicitly forked (by editing) the edit action is annotated with fork:site which gets turned into a separate type:fork action by the server. Here we find the code for both ruby and node:\nhttps://github.com/fedwiki/wiki-node-server/blob/master/lib/server.coffee#L451-L453\nhttps://github.com/fedwiki/wiki-gem/blob/master/lib/wiki/server.rb#L235-L240\nThe ruby code dates back to the original project:\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/blob/master/server/sinatra/server.rb#L232-L237\nAll of these server implementations share the problem that the server attempts to fetch the remote page. I will write another issue explaining why we should fix this and the date thing too.\n. On further reflection, a better strategy might be to make ALL implicit forks go to local storage. If they were accidental, then they could be discarded with no harm to the origin server. If they were desired, then an explicit fork from local storage to the origin server will save the page in a single (additional) action. This approach has several practical benefits:\n- avoids immediately overwriting of origin pages (mentioned already)\n- sends the previously fetched page to the origin server (works better with firewalls)\n- allows the client code to date the forks (where we started with this issue)\n- discontinues use of server-side implicit forking (obsolete server logic can be deleted)\n- avoids two rapid-fire ajax requests (probably why we bundled the fork to start with)\nI can think of one disadvantage. There would be one additional action recorded: the second fork that moves the page to its ultimate destination.\n. Page storage abstraction has been consider several months ago, including a survey of points where storage activity is initiated.\nhttp://ward.fed.wiki.org/view/page-storage-abstraction\n. I think the paragraph text is uft-8 safe. If not, its a bug.\nOur slug logic looks for url-safe, file-name-safe alphabetic characters. We could broaden this with care. This issue would be a good place to discuss this.\nWe expect every site to have a welcome-visitors page. This should explain who and what about the site in english and any other languages preferred. \n. It seems you have caught something \"red handed\". Now if we can just figure out what?\nAs I browse for mentions of this error I get the feeling that it is caused by shifting network paths possibly caused by ip4/ip6, cat5/wifi, or vpn/direct alternatives. Any chance any of these could be in play?\nWhen I get an unexpected fork-to-local I have cleared error conditions by reloading the web page. This is a no-risk option with my changes safely stored. When I've had server load based errors a retry without reload also worked.\nThe journal entry for an unexpected fork-to-local will include the error message reported by the failed ajax call. This can be viewed by double-clicking the timestamp on the revision page. I've yet to see the browser offer a better description via ajax than appears in the inspector window.\n. Wow. What a great reminder that there are many layers beneath us. I wonder if WSUV networking folks could shed some light on Mike's particular situation?\n. Perhaps every client should read-ahead and cache a breadth-first traversal rooted from any page you touch. Some priority scheme could balance the available storage space agains many indicators of possible interest such as nature of touch (read or fork), distance from touch (some page rank formula), and even newer versions of any page uncovered in this traversal. The cache could be improved any time you happen to catch a bit of internet while grabbing a latte.\n. There are uses for a user-initiated while site clone. This could come in two versions, one that adds a fork to every journal and one that doesn't. The latter would be most useful when moving sites between servers. For this I currently use scp or rsync. \n. You can type command/alt-I once you are editing the new video. There are also a page of links in the How to Wiki.\n. Use your browser's back button to un-open pages you may have opened.\nUse arrow keys to move between pages. If no page is active, click one to make it active.\nClick a link on any page to replace those to the right with new pages.\n. Yes, agree. This is as far as we have gotten on a rewrite of help:\nhttp://help.fed.wiki.org/online-help.html\n. Thank you for documenting this case in such detail. Last week we had a European contributor judge wiki unusable. I'm suspecting a case where a sequence of edit actions are fired in rapid sequence.\nMost notable is the RETURN handling in the TextEditor where a 500 msec pause was added as a temporary fix to continue with exploratory development.\nhttps://github.com/fedwiki/wiki-client/blob/master/lib/editor.coffee#L12-L29\nOf course my experience editing fed.wiki.org would be very different from someone editing the same code form Eastern Europe. Someone editing against a small share of an amazon server would have an experience somewhere in between.\nTwo years ago I was hoping that by now the reliable way forward would be obvious. That is not yet the case. One strategy is to make the edits more synchronous and put a queue in the loop. Another would be to batch whole pages and submit them in ne group that can be retried until complete. There are more choices.\nWith 76 followers here there are surely a few opinions. Detailed how-tos are even more welcome.\n. I should mention that one approach to increased reliability is to bring the storage half of the editor closer to the interface. This is what I do when I write in a server running on my laptop and then rsync the result to the public site. See http://code.fed.wiki.org/view/welcome-visitors/view/exploring-federated-wiki\n. Good catch. But @paul90, how did you even think to suspect this? I've not seen this behavior mentioned in any localStorage documentation. \nMany thanks to @michaelarthurcaulfield and his students.\n. I'm reminded of little Bobby Tables. http://xkcd.com/327/\n. It would be useful to have a survey of where it works and where it doesn't. We have tried jQuery plugins that enable more touch features but they interfere with others. This is mostly a task of assembling encodings that the industry itself can't keep straight. \n. As I understand Google's position they will stop providing OpenID in a month or two. I've been using Yahoo for new sites but figure that is a dead end too. I'll move a few sites I maintain to new locations by messing with the DNS entries. The rest will exist read-only if I understand this right.\nThe login messaging in the Ruby/Sinatra version has always been confusing. I have no energy for working on Ruby anymore. It is way too brittle from my experience. I'm afraid to touch it other than slipping in new javascript code. It served its purpose launching the federation. Its job is done.\n. I'll be there.\nhttps://plus.google.com/hangouts/_/g32fblr7vpwniyyv6sazxaoswqa\n. Ryan's repurposing of our client code as a plugin drives home the lessons learned from the node community: write and publish small modules that do not presume to know how they will be applied.\nRyan's work has been harder than it might have been had the client side application been developed from day one with this radical modularity as its primary organizing principle. Instead, his willingness to forge ahead informs our ongoing modularization. Thank you Ryan.\n. Ah, I was thrown off by the fact that you were using port 3000, the default for the Node.js version but not what we commonly used with the original ruby version. I still host fed.wiki.org with this code which is not far behind the node.js. There are still a few things that this does that the other doesn't like Submit Changes.\nWe do include this warning in the ReadMe for the repo:\n\n. The node package manager has learned from what worked and didn't work for lots of other package managers. The simplicity of installation was the number one motivation for the shift. The ability to share code between client and server was a close second.\nWe would love to have someone who distributes ruby code every day to take charge of the ruby code and make it equally wonderful. \nIf you poke around deep in the How to Wiki pages you will find pages from other sites. Fork them and then you will find yourself hooked into the federation.\n. Done. Thanks for the reminder.\n. This is it.\n. The ruby version hasn't been maintained and should be avoided. (I still run the ruby version on fed.wiki.org but patch in revised javascript by hand.)\n. The readme begins:\nThis repository exists as both a historical document and \na community of interested parties. This is not where you\nwant to find the current source for Federated Wiki.\nTry the node version which is actively supported on many platforms.\nnpm install -g wiki\nFind source for this and many related packages in the fedwiki org:\nhttps://github.com/fedwiki\nYou are welcome to continue posting here if there is more we can do for you.\n. ",
    "njfrost": "You can reproduce this in Firefox 6.0 by navigating between pages.  Refreshing the page does not reproduce this error.\n1. Edit a document\n2. Click a link or navigate forwards or backwards.\n3. Navigate back to the first page and the edit disappears!\n. ",
    "StevenBlack": "Ultimately class \"action\", as you suggest, is more sensible.  The only immediate side-effects I see is in style.sass, line 29 where '.edit' becomes '.action'.\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/blob/master/server/views/style.sass#L29\nAlso a small adjustment to client.js.\n. Drat!  I didn't mean to include data/pages/smallest-federated-wiki in that commit.  There's no material change however, other than a change and a revert, so the final display content is the same.\n. All events within a delegate container -- be they from new links or not -- bubble to the container and will be processed.  \nTherefore this is a code-ectomy, no server-side code required at all.  This is as it should be, localizing client-side hover highlighting in the client.\nOther upside: one $.delegate() call will handle hover events on potentially hundreds of action links, including those actions added later.\nIt's all-good.\n. Ah, I wasn't aware this was a requirement.\nEvents bubble-up through the DOM so, at the limit, we could use $(\"body\").delegate( \".action\", \"hover\", .... )\nOf course this won't be as efficient as selecting a more specific parent container but that's our catch-all.\nIf journals are smart, as I suspect they eventually will, then we could make them wire themselves, with something like $( this ).delegate( \".action\", \"hover\", .... ) upon creation, or have a potential journal factory do it.\n. Using .delegate() is appropriate because it uses javascript's event bubbling to attach a single point of event handling at the container level, as opposed to attaching event handlers on all the inner DOM elements.  This greatly simplifies situations, like this one, when the number of inner elements is dynamic.\n. Likewise, on my side, for haml and CoffeeScript.\n. Fixed by fa50d48aef82ddecd612f74656b405812f9d0222\n. Fixed by afff6945c766487af4c1\n. Fixed by f08ae5b0b9ea283ab6f4\n. I know.  I agree that some of my recent Pull Requests don't really merit an issue.\nHowever it's often good to have an accompanying Issue so conversation can expand on the Issue, as opposed to speaking to the details of a particular Pull Request that may come from incomplete understanding of the root Issue.\nViewed from a distance, I feel that threaded conversation in an Issue usually speaks to the Issue, whereas conversation in a naked Pull Request speaks to a specific person and their code.  On small collaborations this probably rarely matters.  On larger projects, this can make quite a difference.\nSo when I create an Issue with a Pull Request, I'm really saying two things.  1) Here's an Issue that I perceive, possibly na\u00efvely, and 2) here's a Pull Request, a suggested implementation based on that admittedly uncertain understanding of the related issue.  \nThis way the first implementation can be rejected, but the root-Issue can optionally persist.\n. Whoops, closing this -- this pull request pulls from my master branch.\n. For some reason the image tag I included wasn't showing.  Evernote broke the (once excellent) Skitch app.\n. Some ideas to broaden the scope of discussion:\n- In practice, I find the notion is often flipped: we have all these potentially nestable and compound wikis, how do we present different wiki subsets to different users (access control). \n- Alternately, how can we allow users to select their own wiki aggregations and maintain artefact coherence given the arbitrary namespace composition. (subscription control).\nOrthogonal to the above is feature control. Users have mixed privileges that can vary in surprising ways beyond can user X read and write to wiki Y.  In practice user X is a role that varies from wiki to wiki.  Now add varying wiki-specific peer-review workflow rules, and you're into an interesting set of problems and opportunities.\nIn my admittedly limited but time-tested experimentation, you can do it cleanly with surprisingly terse logic using binary operations on wiki, publish, subscribe, feature, and state (workflow)  integers provided your integer math limitations can handle all the flags you foresee setting and sieving.\nIt's a fascinating problem space.\n. At a meta-level this has much to do with storage granularity, and the mutability of what's fetched.\nHere's a great example that may be new and interesting to some.\nDid you know... jQuery.load() explicitly supports filtering content.  For example, this works great; Only the contents of #container segment of the Ajax response is loaded into the #result DOM element.\n$('#result').load('ajax/test.html #container');\nThis is very handy! \nIn wiki terms, when we talk about \"nesting\" we should also include the ability to nest sub-elements because storage granularity should not unduly limit nesting functionality.\nI apologize if this is obvious.  I'm watching this project from afar, unfortunately.  If this is old news, please disregard :)\nEdit: not suggesting that this should be handled on the client-side.  This should be supported server-side.  But in a pinch the client can do this.  Here I'm using jQuery as an example of the mutability of fetch, which is the central idea.\n. Forgive this utilitarian tangent...\n@WardCunningham have you seen JSONSelect?  This works both client- and server-side via .js library and/or a Node.js module.\n- http://jsonselect.org/\n- https://github.com/lloyd/JSONSelect\nI've used this extensively. I can't overstate how handy mining arbitrary JSON can be, regardless of the implementation layer.  JSONSelect is trivially easy to use too; a masterpiece.\nSo the question becomes vastly interesting if we know the client has keen powers to refine data beyond what the server supports.  Orthogonal to that, it's interesting when both client and server can share the same \"selector\" language.\nSo one way to parameterize a query is to pass \"selectors\" either as direct parameters, or as parameters to a server-side callback (or some server-side post-process) prior to returning the response.\nAlternately, the client can fetch data, and processes the response locally using the very same selector.\n. ",
    "FND": "\nI like the $(\"body\") idea\n\nFWIW, I believe $(document.body) is a little faster (a quick comparison confirms this - though it's barely significant).\n\nresolved by be2e970\n\nThat commit is currently 404; did you perhaps forget to push?\n. ",
    "SvenDowideit": ":)\nI'm pretty much hacking away to try to figure out what it does and\ndoesn't do :)\nthe thing that puzzles me most right now, is the /remote/ stuff, as I\ncan't see any way to use it - I was expecting a [link] syntax to link to\nremote servers\nand then again, I'm thinking - where's search :)\nthus running right back down the twiki/foswiki rabbit hole :)\nOn 27/09/11 17:24, Ward Cunningham wrote:\n\nWow. Lots to look at here. I'm on it.\n\n\n\nProfessional Wiki Innovation and Support\nEnterprise Support Contracts for Foswiki\nSven Dowideit          http://fosiki.com\n. yeah, I worked out what the long term idea might be, and managed to use\nit as a proxy\nbut I can't work out how to get a 'fork' action to happen, or how i\nmight refer to a remote page so that I can view/edit/fork it\nI'm trying more to work out what the code does now, and how to use what\nis there - and the remote / fork bit is still opaque\n(what it will do is a large part of why I'm playing with the code now,\nwhen I probably should be doing something else with my 2 days a week\nwhen the girls (2.5yo twins) are in day care)\nSven\nOn 27/09/11 18:20, Ward Cunningham wrote:\n\nThe /remote/ requests have the local server work as a proxy (soon caching proxy). \n\nRight now it is only used to resolve 'fork' actions in the journal.\nThe hope is that authoritative pages will be forked often and that these\nreferences\nwill be the fabric of the federation.\n\n\nProfessional Wiki Innovation and Support\nEnterprise Support Contracts for Foswiki\nSven Dowideit          http://fosiki.com\n. utf8 - my first bad thought was - what do i8n domains do, but punycode is woeful\nI feel that slugs should really be bidirectional ecodings - and gha-gha doesn't qualify.. there's no filename way to know if that is english, or converted glyph.\nbut that said, I'm struggling to justify my feeling, so meh :)\nso to ignore the hard issue\nTim O'Reilly ==> tim-oreilly\nTim OReilly ==> tim-oreilly\nTim O Reilly ==> tim-o-reilly\nTim O*(Reilly) ==> tim-oreilly\nTim O:Reilly ==> tim-oreilly\nand so on - seems to work well enough, given that each wiki is a personal shared dictionary. \nbut, is that setting us up so that merging, sharing and building a shared vocabulary is a difficult WikiGnome only task?\nWhile my current hope is more along the lines of a distributed google+, with DataWiki and 'legacy' pages, I do wonder how the c2 wiki would have worked without some way for us to have a common vocab (I'm looking forward to free text serendipity linking from a federation)\n(renaming pages, and thus working out how to re-link the federation's links isn't filling me with confidence wrt filenames based on something as fluid as topic name / title)\n. mmm, somehow i 'closed' this :(\nI presume that I clicked on 'comment and close' as i did assume it meant close the window :)\nyup, 4 hour mountain bike ride, maybe i should be recovering\n. I'm especially interested in having a lot less markup than wikis have right now, and maybe diverse markups based on item.type (say, a calculator markup, or even reinventing mathematics in ways suggested by \"Kill Math\".)\nnow that gives me an idea - I'll re-write the star list rendering as a seperate item. I also get the feeling that you're trying to force pages to be broken up, avoiding the double-return mess we have in existing wiki's.\nwhich is another thing - atm, it breaks the writing flow to click 'add factory' rather than just hit return twice - so maybe I'll find a way to get the textarea to detect 'newParagraph' events.\nthe hard issue is how to allow the user to select a paragraph type - prose (what there is now), list (what I'll try out), etc.\n. seeing http://sensors.c2.com/ 's welcome-visitors has an F, and the github version doesn't, and worse, is in an odd state where you can edit the one paragraph, but can't add a factory, tells me that R and F work somehow. can't get the youtube vids working on here atm, so I can't review them.\n. I'm getting hung up on a detail - how does a user of the web interface get to the point where they have something to putAction.\nI've a couple of ideas, but i'm not sure of them\n1. add a [server:page-name] to links so that one can explicitly reference a page elsewhere, thus able to click and load it, and then putAction - thus requiring the resolveLink rendering to have more than one 'link' output\n2. add a serverBrowser dialog that allows users to see what pages exist in the federation, to view them in their session, and make forks from there\n3. much more magic - wherein serendipity linking across the known federation just 'happens'\nthe first is probably the fastest to implement, but will make life a little more complicated later (i think), the second we will need to allow collaboration conversations to occur, and the third is a tad scary scalability wise.\nnote that none of these suggestions require our servers to have a /remote URL - they can all be done using jsonp on the browser side. (what i'll implement next - unless someone gets there first)\n. I've comited a non-finished, but view-functional demo of using jsonp for viewing remote wiki pages - see my latest pull req.\nand of course, it breaks the static server in this form, as the client.coffee assumes a jsonp - i think some code in the ajax error handler might allow it to fall back..\n. I acheived this by moving the (single) paragraph from [[welcome-visitors]] to another topic, and then back - the journal tells me about it, but the resultant story is and array containing a null\n. a path - being a journal of clicks? - interesting (though I can't quite see where i would use it)\nI'm about to write a node.js server (ok, probably next week, when the girls goto playgroup for the day), and I'm going to write it so the server does no html rendering of pages - leaving it all up to the client (cos the code duplication and massive size of the server irritates me :) )\ndoes that mean you'll be adding a toolbar like thing to the client? or where do you view, save and send the 'path item list'?\n. at this point, I'm still thinking of getting rid of /remote - for contacting remote federated wiki's it is actually un-necessary, and adds complexity, and for non-wiki sites its a security risk.\nI think I need to write a more detailed proposal to make sure I'm not missing some important functionality you're intending - the short impl detail is that for federated wiki remotes, we should use jsonp.\nFor non wiki remote access (aka mirroring external resources), we need something a little more secure than a proxy that could be used to bounce hacking attempts\nI was starting to implement PUT, and noticed that actually, you're PATCHing :) all REST-y details I'll work through when i have time.\nat this point, I still beleive that we should have resource-type in the url - \n    GET /page/welcome-visitor\nso that the server knows what to do with \n    POST /receipt/14-10-2011\nwhen I try to add a new shopping receipt (a datawiki application datum that in combination with a search should allow us to  give users the ability to create datasets and schemas that they can then visualise and analyse\nsam - yes, it needs to be relocateable - though really, I much prefer to allow my wiki, which is the only thing on my domain to be in the root :)\n. I was expecting that remote servers would be identified by an alias, that would be 'registered' in the cfg - that way we avoid this issue, and also cater for server moves/consolidations etc.\nits not that different from the tracking we will need for user - I have logins from all over the place.\nthinking a little further about it - references to remote pages are not really that, they are name spaced references to local mirrors of something on a remote server.\nAnd in that context, I recon the extra level of abstraction (namespace->server details) is quite justifiable.\n. I strongly agreed that html output was important between 1990 and 2006, even killing some of my research projects in TWiki for this reason, but in 2011, I would like to ask honestly, can you be specific wrt seems fairly important .. for users who don't have javascript?\nI'm not sure about it for several reasons - \n1. imo it will slow our development & learning down at this early stage to have json->html rendering in every server and client codebase - ok, clearly, its going to slow my efforts (but if there's a clear we must have server side rendered html then I will cope\n2. is the set of non-js users sufficiently non-trivial? (i ask because i have no data - links will help me)\n3. does non-javascript federated wiki actually work as a consistent and useful UI right now (or should we remove it, and bring it back at a later stage)\nyes, I am conflicted, but after 20 years of writing web renderers (eeeek), I've leaned heavily into leveraging client side js.\n. fair enough, but what do you expect a client js heavy federated wiki UI to work like in that environment?\nand as per 3. does it function now, or are there issues.\nI also turn off javascript at times, but i also don't expect the entire web to work..\n(leto - I'm asking for specific datums as applied to federated wiki development now, so that we can make a reasoned decision)\nI'm biased by the fact that http://uwiki.me implements a read only federated wiki using just apache rewrite rules - so there's an index.html, a style.css, client.js and jquery, and a pile of json files in /data/pages... (I'll do more to simplify our codebase for that and the node.js server)\n. mmm, one thing - will Handlebars help with the ruby and C based servers?\nthe thing i may not have emphasised enough is that the pain I had in doing the node.js implementation is because it is not the only backend (and i really want them all to use the same templates/html/css)... (and the static file one, is a really nice example of how far we could go)\neven so - everyone seems to think that having html output is important, so there we go - its easier to design and architect with one goal, and at cross purposes - I'll just have to think harder on how\nif Handlebars works for the C, ruby, Perl (that i might need to write for other reasons), node.js, .NET, and java, then its a very good fit... HAML seems to be mostly ruby :/\n. I've added simple escaping. - \nsorry about the static thing - I mistakenly merged things into that branch to test http://uwiki.me :/\nI'm still not used to git's branch per feature workflow :/\n. goodness me, lets see.\nnon-cross site restricted access to import data from anywhere, into your local browser's persistent datastore. \nI would love it, and so would the blackhats - yes, it's much better than the random code injection, it is still random data injection.\nso Allen - oh, yes please, making a web data -> DOM mechanism would really improve data application developers lives.\nPS:\nif we're talking about pushing browsers into a sanity-land where they actually implement what should have been there in the first place, how about fixing authentication? why are browsers not essentially using my ssh keys (though i don't quite grok why ssh keys are separate from gpg keys ...) \n. yes, exactly, it'll be safer, but still as unsafe as the rest of the web.\nWay too many of us do DSL / data is code like work, so 'no execution of js' does not mean no execution.\nReally, it all comes back to the sad fact that web technology has not worked on webs of trust (gpg style), so that I can use type=application/json , and have some assurance (by checking the signature against corruption and that it comes from a source that I have decided to trust) that its data form someone i know.\nimo we don't really want to end up with a git-like collaboration method - it leads back to single webmaster syndrome, with the added confusion of many duplicate forks with spelling changes. instead, if I sign your public key as 'i trust you', then your changes could be auto-merged...\nI do wish this was rw federated wiki accessible, that way it'd be easer to refactor.\n. Ward - I'm playing with my static-server implementation, and I think I can make it serve jsonp using mod_include (ie, server side includes).\nif thats the case, can we change the definition of a federated server to only talk jsonp? from there, we can later add mirror/read non-federated wiki sources separately..\n. I'm sorry for ignoring this for so long - I need to get some serous foswiki work done before the Foswiki Camp and general assembly at the end of the month - After that, well - I'm somehow expecting that figuring out how to give the girls their first snowy xmas will hit, and then it'll be time to jetlag back to Brisbane :)\nI really don't feel I can put in enough time to work though my fuzzy ideas - but while they're drawing on new paper together (collaboration between young twins is fascinating)\nmy focus on what is currently the static site has more to do with simplifying - if we can use zero code to serve reading, we leverage the nature of http. To add write, all we need to do is write a simple server code to allow PUT (er, or use DAV?)\nThis then makes me feel that we can focus on the meat of the innovation - the interaction and data dynamics that allow users to innovate.\nso when we say 'static' i just think fast and lightweight, not read only.\nbtw, I keep staring at the elephant - what happens if you have millions of edit events on a page? Especially as iirc, we only get ~5M in the browse datastore.\nI was hoping that we would have a federation already, so I could work on our combined, and separate visions, but real life makes quite moments harder :)\n\nmmm, times up.\n. Wikipedia is imo rarely a good example of all the different uses a wiki can have.\ntake for example this 'issue' - it feels pretty awkward to call it, and this discussion an Article, though still valid by Wikipedia's definition\nhttp://dictionary.reference.com/browse/article - says 'a written composition, usually written in prose' (which sounds like a reasonable elevator definition) - which is much less than a 'page' in a federated wiki might be - I'm likely to have an shopping list, expense report, a image and its meta data, or even a definition for a custom item type in my wiki..\nI thought page was a good compromise name for something i've called a 'topic'  forever (with the accompanied awkward feeling) - as the way we're using the Miller columns evokes a UI metaphor of a 'page' of information for me.\n. one possible idea for naming is to put together the full vocabulary - right now, we're missing some of it\nwe have\na wiki contains several pages that are made up of several items, with a possible view being one or more pages\nshould we change it to:\na wiki server (should we call it a library :)) can contain one or more books/magazines/collections/??? (views) made up of one or more articles, which are compositions of paragraphs of different types?\nI'm not totally a native english speaker, but to me Article is to narrow - but I do like what its made me think of:\nI'm interested in how to render Foswiki's DataForm defintions and data sets. in Foswiki, each structured data entry (for example, one persons's contact details) would be stored in a separate topic, which doesn't map well to the initial way most people enter that kind of data - in a wiki table, one person per row. This awkwardness suggests that I've got something backwards in the previous comment.\nSORRY - MORE LATER, KIDS WAKING FROM NAP.\n. as you can see, I'm (sanitizing) a stream of consciousness, but as its been a few hours, rather than editing, I'll continue here :)\nIn my mind, I think I have some added confusions.\nfirst up, I do not make the presumption that what is today a '(wiki) page' is the base container for an addressable thing. I've convinced myself that I should try a refactoring, where 'page' is is a plugin type, like 'paragraph' and 'image' (and 'journal' should be).\nadditionally, imo, each 'item' (which is thus the pure base data type) should be addressable, though its likely that most will not be addressed by users particularly often.\nI got to the point (when I stopped) where I figured that it might make sense to call what Wikipedia strives for an Article specifically because they strive (in the ideal sense) for a static (what i call legacy) document that covers the topic fully and hopefully permanently - but this is none the less only one type of wiki page.\nyup, I can't actually make up my mind, I just thought it might be useful to share my thoughts :/\n. /me looks for the microformat spec, and finds nothing :/\nthis idea is a nice bit of magic.\n. What about using a browser based spreadsheet - or its syntax?\nfor eg: \n- http://visop-dev.com/lib/jquery/jquery.sheet/jquery.sheet.html\n- I'll cry about the death that wikiCalc seems to have come to - though as the 1.0 codebase is still out there...\nin essence, your expense calculator can be implemented as a spreadsheet table/grid - and your expenses example would be a page that contains several of these tables, with some kind of =SET and =GET to access values outside the current table.\noh, yes, I do want that :)\n. cool! You seem to be having altogether too much fun :)\nOn 11/11/11 08:57, Ward Cunningham wrote:\n\nI've ported the awk script which nows runs in javascript on the client side. See http://home.c2.com:1111/view/expense-calculator\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/issues/53#issuecomment-2705810\n\n\n\nProfessional Wiki Innovation and Support\nEnterprise Support Contracts for Foswiki\nSven Dowideit          http://fosiki.com\n. see #84 for a step to allow site-customisation\n. er - its gone already - looks like i'm still too tired to function :/\n. ok, specific proposal:\nin the interests of duplicating things, fed.wiki's will contain 2 data dir's:\n- the default-data, containing the default data\n- the data dir, containing all local data\nmodifications to the default-data will do a 'copy on change', thus causing the server to ignore the file in the default-data dir if it finds it in the data dir.\nboth the default-data and data dirs can contain pages and views dirs and client??, and ?? only the data dir will have a status dir??\n(it seems to me that in many ways the client code (plugins especially) are data that can be customized locally - and from the foswiki/twiki lesson, the sooner customisations are locally but separably trackable, the better.\nstarting a server with no data/status/local-identity will have one created for the user (or even better, will cause the user to tell it what to enter - either by exiting and insisting on parameters with the needed info, interactive questionnaire, or insecure webUI that could also see that user 'claiming' that instance...!\n. @nrn :) wrt handelbars - can I possibly bully you into converting the ruby server to use it too? \nit will make it much easier for us to keep the different dynamic servers in sync if we re-use the same files (I used static html only because the node.js haml impls all fell over with the tiny amount of haml that fed-wiki has.\n. trust me on this: ruby, and the ruby server are trivial to hack on, and Ward does a good job catching newbie (like me too) issues when merging.\nfed.wiki was only the second time i'd written any ruby, and may well continue to be the only place i'm forcing myself to do so.\nyes - I was intending to use the mustache templates on the client side too - cos I still contend that the server processing is redundant (except for non-js use, for which I should make another proposal :))\n. from my experience, lack of strong developer libraries and support are one of the bigger reasons why OpenId didn't take off, and is being replaced by other approaches. (for eg, the perl support - especially openid server, is woeful)\nMozilla's BrowserId feels to me like a better system, but it'll take some time to find out if its flawed too. \nIf I were wishing upon a star, I'd rather we had a federated authentication system based around gpg servers, and for browsers to use gpg for auth, content validation and encryption - damn, more stuff I won't be doing :)\n(I run my own php based openid server so I get to pretend I have control over my identity)\n. argh :)\n4:00am Australian Eastern Standard Time (the Australian timezone UTC+10)\nI am also working towards doing g+ hangouts for foswiki, so I think its a good idea.\nif you can wiggle that by and hour or 2 later, or 4 hours earlier :p \ni'm SvenDowideit on g+ too - I would like to try .\n. I'll see what the girls do at 4pm :) should be able to have a short test at least.\n. yup, wards fed.wiki tells me to nic-off with a simple 'this is not your wiki' message - \n. defer the issue. 404 page and 500 - and 'not your wiki' should all be in the views/ dir as templates. :p\n. I've wondered about integrating data sources that are not managed within fed.wiki, and I wondered if the best approach would be to write proxies foreach 'source' that separately implement the read only portion of the fed.wiki data API (ie. GET json).\nsimilarly, rather than unifying local store and remote store, to treat all remote stores as somewhere all users, including the wiki owner pull into their local store from (and optionally change), and then only the owner (or their authorised delegates) can push to.\nI think the difference to what you're suggesting, is that I'd like to treat the /read/ and non-publish edit interactions of the owner and the guest users in the same way, and then add a publish step, in which the user can publish to any wiki they have commit rights to.\nthis (i think) matches the git workflow more, and in reducing the difference in workflow between users and owners, may simplify supporting it.\nI'm really coming back to the idea that for the javascript enabled user, all interaction is with the local store containing mirrors of data requested from all over the web.\nwhereas i think you're suggesting that the local store is parallel to the remote stores?\n- one side effect of the local store being a copy on change cache, is that it does not have its own favicon, rather , data that have been changed have a new favicon.\nand as far as push to server, or push to new server goes - i don't think a new storage format is needed - imo we need some more UI's:\n1 a graph of remotes and relationships between them (ie, data that they share / forked)\n2 data on remotes & local\n3 ways to see diffs and changelogs, and then to coaless them\n4 ways to mirror / transclude with automatic updating\nthat way, pushing changed data can just drag to a server (including a new one)\nmmm, data, and changesets should be draggable?\n(ok, so we might be saying the same thing)\n. opened #209 to remind us about the express server and other details\n. as usual, I'm playing in one branch, so other fixes are then added to the pull request - clearly I need to get more used to branching\nthe additional commits are :\n1 style changes to show the header, and to make it obvious what the 'current site' is - when I have my screen full of remote wiki pages, and chrome decides it does not need to show the tab's favicon, I need a reminder of where I am\n2 add a close page link - this has bugged me right from the begining - when I want to show someone a story, I need to close some distractions.\n. the header bar is there in fed-wiki, but you can't see it - and it prevents you from clicking the top 20px of the pages, so that commit is to make it more obvious - At this point, I'd remove the header entirely.\n. ok, the remainder is to do with close button (a913e64 and e9a9bcd) on the page and scolling hints for the user (e9a9bcd).\nclose: When I want to view of show a story, I often open some pages (in the middle of the story) that I decide afterwards are a distraction, and want to stop showing them. the well known UI way to do that, is to click the close button on that page. (at the moment i think i need to edit the url and refresh?)\nscrolling: oh my, i never thought to take my hands off the mouse while browsing to use the arrow keys - we still don't give the user feedback that there is a page there to scroll to - thats what the -60 is for. \n. made a video of the close and 60px in action: http://youtu.be/oHL30PgpxnY\n. I'm going to close this, and start a new task and consider a better implementation of the scroll indicator see #214\n. To be honest, I would suggest that queries should not really be a page, and instead be a paragraph, with customisable settings and more\nbut :)\n. oh my.\nWhen I add [[Recent Changes]] to my local express wiki, it then shows new.fed.wiki's changes, but clicking on any recent change that I have a local copy of (like Local Editing) will take me to my version, not the one I would expect.\ncross wiki navigation is still not obvious :/\n. What flag? I see no flags in the UI.\ndoes clicking the flag take me to my wiki's non-existant recent changes? (given that the code hasn't been written)\n. ah, updated - I thought of 'story' as a collection of pages - really doesn't help when I don't remember the terminology\n. see http://youtu.be/lBUhYCHvCcE\n. your plugin is essentially the opposite of what I'm thinking :)\na really nice thing about fed-wiki, is that paragraphs are a basic building block - so from my perspective, it'd be good to have a new paragraph for each type of information - \na bullet-list is a different type of information from a prose block.\nI'm also thinking that headings are a separate element - though I realise that there is going to need to be a way to tie sections together.\nThere are a number of (twiki/foswiki related) reasons for my idea - \nsections, and groups of sections can lend themselves to both structured (data-typed) querying and transclusion, and can push towards giving different views into the wiki - Table of Contents for eg would be a filtering to show only the header typed data, list pargraphs can have much clever-er widgets to allow manipulation and aggregation\nIn this way, importing from other wiki's means detecting these type transitions though.\nout of interest - what wiki engines do you use? (I'm rather too heavily invested in the data-wiki/structured data area)\n. well. this is interesting.\n@interstar I hope we manage to avoid mixing types. thats an issue we butt up against constantly in foswiki - though I think we currently have a kind of mashup in the link rendering - who knows, maybe we'll find a way out of that too.\nWard is (imo) challenging us to cast of the wiki-fu and find new ways to achieve simplicity.\n@WardCunningham I really don't like the idea of mashing rendering into content - so I'm going to try to find a way not to force hardcoded unicode bullets / html or whatever into the source :)\nso - what about this slightly different tack:\nif I want an ordered list to mean something like bullets, rather than sequential paragraphs of prose, I create a list type paragraph that I can drag several paragraphs into? is nesting paragraphs a good or useful idea?\n@interstar we need to separate some of the issues tho:\n1 want to be able to change or select paragraph types in a 'nicer' way\n2 I'd like a more natrul way to start a new paragraph (i'll expand later.)\n. In the first Sunday Hangout, Ward added that he thinks of the Site as like a PDF - which gives us a touchstone wrt the sizing of data, topics and focus of a Site.\nfor example, rather than hosting all of wikipedia on one site, we'd have Waki-Wiki style (sorry, reference to thought experiment dating back to 2005's wikimania) distributed sites that focus on their local topic area / interest.\nas an example, a Chemistry department might have a site that contains mostly local research and learning topics, some of which refer to shared articles like wikipedia, and might have forks of some of those topics.\n. ancient, so closing.\n. ok, and that might be a bug\nhttp://localhost:3000/view/welcome-visitors/thoughtstorms.info/howsfwchangeswiki\nshows Phil's page, but when I then click on any link, it gives me an empty journal with the flag showing its from his wiki.\n(express server - I guess if I'm going to play with this / search / whatever, i'm going to have to make a ruby vm too)\n. very nice.\nadd to that a remember me cookie so i never have to log in again from that computer - or optionally a client side ssl cert >:} and we're more magical than most.\n. at which point you need to have an email address and mail and argh!\n. ",
    "asolove": "Did anything further happen on this issue? I am interested in making the mobile experience better: \n- I've pushed some changes in my master to make the mobile view a bit better by guaranteeing that pages fill the entire viewport. \n- I would like to add a library called iScroll that will let us improve the dragging experience, by making left-right drags snap to pages, and making top-down drags scroll just inside that one page. \n- Editing is a thornier problem, we will need to come up with an alternate UI for triggering edit mode, and decide how drag-drop refactoring can possibly work.\n. Sven: I would prefer not to add a toolbar, it should simply work. When you open a story, the browser's url should change to an emailable url for the current state of your page.\nA few more uses for a path:\n- With a clearly-modeled \"Path\" we can also talk of a \"Path History\" similar to the history of a story. The Path History is a record of an interactive session with the wiki. It says: \"I started out on this story, then went to this one, then moved a few items from one to the other, then opened another story and added something there.\" \n- The Path History is a natural way to model an undo/redo system for allowing people to experiment with changes and then revert them.\n- You could also use it to record and play back an interactive session, either for use in a public presentation, for demonstrating a particular feature of the wiki, or reviewing a colleague's changes before pulling them back into your wiki.\n. Just committed a minor change that supports setting the browser url to a representation of the currently-viewed pages. This provides copy-and-pasting links, and returning to the same place if you press \"back\" after clicking on an external link: https://github.com/asolove/Smallest-Federated-Wiki/commit/833786caeca213a4959957a54cabd5fcb7bf7c1c More to come!\n. @hellekin: I think there are two separate modes. When just browsing the wiki, the url should encode the current state of your view, so you can bookmark it, share it with others, or return to the right place when using the back button. Now, if you decide to record a session, then yes, the url should change to some url where you or other users can view and follow that session. But there's a long way to go before we can do that.\n. I am going to start work tonight on improving the Miller Column UI. Based on the notes in the Wiki, I see that we want to use side-scrolling when displaying more columns than fit horizontally on the page. That should be pretty easy to change. I also suppose that, like the Miller Columns in the Mac Finder, we will want vertical scrolling to be limited to each story, rather than vertically scrolling the page as a whole? I'll build it that way and make a publicly-accessible version so everyone can try it out.\n. I have some rough code at a3e41ed. It demonstrates that side-scrolling is pretty useful, but also hard to get right. Tomorrow I will try adding keyboard shortcuts for scrolling left and right, and perhaps automatically scrolling the page if you hover over a half-shown page near one of the edges.\n. See: https://github.com/balupton/History.js/\nI can take a shot at fixing it to work with remotes tomorrow.\n. Ok, I was a bit overambitious and didn't make much progress on this. The plan is simple enough\n- render data-remote=\"server.com\" and data-name=\"welcome-page\" attributes on all the pages\n- combine the remote and the name to create a unique ID for each page on the dom\n- update all code that was using the ID as the page name to be aware of the data-name attribute and use remotes if needed.\n. It seems fairly important to be able to render static pages, for the benefit of users who don't have javascript. I would like to suggest moving the page rendering into a template format (perhaps Handlebars?) that can be run on both client and server.\n. If you think that this is going to slow down your work, you should feel free to make everything work without server-side rendering and I will be happy to spend the time making the server rendering work.\n1. If we write the template in Handlebars or some other DOM-independent js templating library, we can very easily run it both on the client and server. \n2. About 2% of visitors to Yahoo have javascript disabled: http://developer.yahoo.com/blogs/ydn/posts/2010/10/how-many-users-have-javascript-disabled/\n3. There is an important difference between functioning as an editable wiki (which perhaps should require js) and functioning as a static web page. If I run a wiki that becomes as important as Ward's patterns wiki, and someone links to a page on it, visitors should be able to at least read the linked-to article without javascript, though they won't be able to edit it or use any of the advanced javascript features.\n. The code to do this is quite simple. In the callback for history change events, we just compare the divs on the page to the names in the url and add/remove pages as needed. I can probably send a pull request this weekend unless someone wants to do it before that.\n. da3749d introduces this feature and 0f36a83 has a merge against Sven's commits in your master.\n. This is an excellent idea. In addition, I wonder if this is part of a larger move to give the client more responsibility.\nSven was (rightfully) complaining about having to run the same templates on multiple different versions of the server. I wonder if we could differentiate between two types of servers:\n- embedded servers, like an Arduino, which are only expected to serve up the json API, and not for very many people\n- web servers, which are where public links should point to, and which also serve up html and the client javascript.\nIn this way, you might have a public web server that you maintain for others to access. You have an embedded server that just pushes json data. You log in to the web server and use its client, linking to a remote page on the embedded server, and then pulling that data into the version on your web server.\nThis greatly simplifies the task of building servers that can do interesting things with data, without having to duplicate any code necessary to support the web client.\n. If this is the direction we want to go in, I might suggest stepping back and reorganizing the client code. My recent commits have added features without substantially restructuring things, and it is getting a bit hard to follow. My goal would be to separate the overall miller column UI, the rest API, and the plugins so they know less about each other. It would also be nice to improve localStorage support so that you could bookmark a wiki as an \"app\" on a mobile device, and save all the pages locally so you can read it offline, then push updates when you come back online.\n. Ok, well let me get my server up and running and I'll start actually using it to organize some of my research. Then when we have federation working and I have a large enough set of experience using the wiki to do real work, we'll know a bit more about what it wants to be.\nIs this issue something that should get resolved before then? It should be quite easy.\n. I like the idea of articles containing executable data. We could agree, for example, on a couple of articles that have json content that all wikis are expected to have:\n- About: a listing of metadata about this server. It could be used to identify the server name, author, favicon, and that the server is open to federation.\n- Peers: a listing of other federated servers this wiki knows of, similar to the bibliography at the end of a book. Forking a page from a server automatically adds it to your list of peers.\n- Collaborators: a similar listing, representing servers that this server's author particularly pays attention to. The client could use this information to present a GitHub like interface. When you bring up a page, it lets you know if a trusted collaborator has made updates to their version of the same page, so that you can easily pull them back.\n. This is a very interesting idea. I am taking a crack at something similar to the awk calculator. You enter numbers or simple algebraic expressions and can give them names if you wish. I see it being useful for two purposes: 1. a scratch pad to do math while reading articles. 2. a format for presenting \"arguments\" in the form of calculations that are part of articles.\nSome other inspiration for the future comes from Soulver (http://www.acqualia.com/soulver/) and Bret Victor's Scrubbing Calculator: (http://worrydream.com/ScrubbingCalculator/), of which I am also trying to build a browser version.\n. I really like the way the calculator works. I wonder if we could build \"inline\" editing instead of the way you currently have to edit the whole textarea to make editing less disruptive and to see the changes happening real-time.\n. In the touch client I am pondering how to load in the page data. I am leaning towards populating a collection of pages with just their title and site attributes, then fetching the actual page content only when they are displayed. Does that sound like a reasonable approach? \nI am curious about how we see the search widget fitting with the rest of the UI, specifically\n- Is it part of a page (perhaps the welcome page?) or in the chrome outside any of the pages.\n- When opening a link, do we close all the existing pages as if we are starting over afresh? Or should we open the new page to the right of existing pages, as if we are following along the existing path?\n. @KyleAMathews Right now, all images are stored as base64-encoded strings inside the json for the article. So I don't think pulling everything down is a real option performance-wise. (We should, of course, just allow image uploading whenever we can convince all the server implementations to do so.) As far as the code is concerned, the view is already binding to change events on the model, so it is as simple as model.fetch() and waiting for things to update.\n. I am actively working on the Backbone rewrite here: https://github.com/asolove/Smallest-Federated-Wiki/commits/master Right now, it is capable of displaying things exactly like the current version but does so in an MVC control flow for everything except the plugins. The next step is converting the existing plugins to each have a Backbone view, which will also mean standardizing the way that plugins specify what actions, buttons, etc. are available. If you are interested, definitely feel free to work on something and send back pull requests.\n. This is an excellent idea. Besides providing a good first-time experience and inviting people to create their own federated wiki, these semantics closely conform to what we want to do for the mobile experience of their own site anyways. We should cache the pages they have accessed in localStorage, and then sync edits when they come back. \nI would like to focus on the experience the very first time that someone without an account makes a change. I suspect we can animate the changing favicon, and we should make that favicon a link to a little popup explaining that this is their own edits, and here is how to publish them. I feel like that favicon should have a special treatment, a star or something over it, until they actually publish the changes. \n. I went ahead and started exploring this direction, pushing it to a branch here: https://github.com/asolove/Smallest-Federated-Wiki/tree/touch-client/touch-client\nThis commit does nothing more than render the titles of the requested stories, but this is done dynamically and with what I hope will be meaningful objects.\nThere is pretty good Backbone documentation online for those who want to follow along: http://documentcloud.github.com/backbone/#\n. Kyle, that's interesting, Brunch is almost exactly the stack of libraries I am used to, and it solves the problem of how to build the assets independently of which server people are using. Do you have a sense of how stable Brunch is or how much trouble we would be in if Brunch stopped active development but we wanted to move forward with the same library stack?\n. Sounds good: structured tests and builds would be very helpful. I'm going to try setting this up on my remote branch for those who are interested in watching or contributing to the touch client.\n. I spent some time on this yesterday and got to the point of being able to display and browse a local wiki similarly to the existing client: https://github.com/asolove/Smallest-Federated-Wiki I would appreciate if those interested would go look at the structure of the code there and see if the objects make coherent sense.\nThe net step, it seems to me, will be to rewrite the way editing works. Because the current plugin implementations are not ideal for touch-based clients, I would like to suggest that we rewrite them but I'm not sure how much opposition there is to that idea. Specifically, instead of allowing plugins to arbitrarily bind events, I'd like to expose a handful of hooks they can use (to specify different allowed user actions) and that way we can have a single implementation for how those actions are displayed in the UI. That way we can change the UI in one place and have all plugins change to match.\nFor example, the chart plugin might specify that it has options like: pick dataset, select chart type, add caption. When you enter edit mode on a chart, those options would be displayed in a contextual menu and lead to the right action. The central codebase would also expose reusable components for modal dialogs, contextual menus, text editors, etc. that the plugins could use.\n. @nrn: There is some code here (https://github.com/asolove/Smallest-Federated-Wiki) but it hasn't gotten very far.\n. I'm going to close this issue (which mostly ended up covering necessary refactoring) and start a new discussion on mobile UI.\n. I have some ideas about new site creation and marking non-saved changes from a UI perspective that I am hoping to put into the touch client. They would include allowing the user to create this favicon on the client side using this code, so I am happy to see it here.\n. I want to put in a word for some form of static html. It is necessary for some search engines, and those who do not or are not permitted to use javascript. Of course we don't have to provide the complete plugin in a static html format. I suggest that all plugins maintain an \"alt\" property that represents how they should be rendered as static html. For text modules, this would simply be the text content. For an image it might be the img html tag and its alt tag description. A chart might simply say \"A chart of\" followed by the content of the caption property. This way, servers only have to load the json data and spit out the alt attributes, rather than having to make the plugins or their templates run on all the different servers.\n. I would like to start tackling this issue in an organic way rather than the one-shot refactoring that has now failed twice. \nMy idea is to start by only allowing drag refactoring from a drag rail on the border of each item that appears when it is hovered. \nThen I propose introducing an alternate API while still allowing modules to use the old one and just detecting which they use. Then we can move them over one by one (starting with text) to get a feel for what the new API should be. \n. I create a demo of the first step, the drag rail, this evening. It is done hackily and only works for paragraphs, but is worth playing with. The code is in my drag-rail branch.\nFor the chronically impatient, I uploaded a short screencast.\nThe primary upshot is that you can now click and drag within an item (to select text or move a map) without actually picking the item up and reordering it within the story. The handle also begins to move us toward a UI with more visible affordances for user action. It will be a good place to hang other items, like icons for edit and other plugin-specific features.\n. Yes, I would think so. And whatever pattern we find for entering the edit state or doing other item actions could also be available for changing the page title or other page-level actions.\n. Your comments are very helpful and actually straighten out a few things that were jumbled together in my mind. \nI think a good model for this is a spreadsheet. You can read and move around a spreadsheet without any UI elements in the way. You can click on a cell once to change its properties or drag it around. And you have to click one more time (or double-click, or press enter)  to actually get the line editor to change the cell's contents. \nI haven't played around with Numbers or other spreadsheet programs on the iPad, but I imagine they would be a good place to look for inspiration.\n. Aha, that would be my fault. I will look at it tonight.\n. While looking into this I played with a few versions of Firefox (3.6 is the last version without autoupdating, 10 is the current autoupdate version, 11 is where the developer preview currently is) and found the following issues:\n- There was a FF bug that prevented the page from even loading unless you had Firebug running. \n- The side-scrolling feature of the Miller Columns doesn't seem to work. I will investigate this.\n- In FF9 on Lion the columns look like this for me: http://imgur.com/sq4SB The columns have a scrollbar that looks pretty close to the normal Lion scrollbar, and using the scrollwheel while the mouse is over one of the columns causes it to scroll. Is that not what you are seeing? I would suggest we should also support the up and down arrow keys, scrolling the currently-active column. I will have a patch for this shortly.\nDaan, let me know if you are seeing something other than this and we can look into scrolling in more detail.\n. Actually, please do not merge this in right away. It causes a small display problem and is less efficient than it could be. I will add another commit to the request in the next few days.\n. I've made some changes to the fix. We now only have to find the appropriate element to scroll once. And we no longer mess with the display of the page if it is already scrolled.\n. This is something the new UI should address. I would agree with the idea of a button bar while editing, and setting the tab order correctly to make it easy to save a change.\n. Hmm, I will be curious to see the result. To make it happen, you want to edit the scrollTo function and change the math to base things off the window's center rather than its width. I wrote some of that code and am happy to help if you have other questions.\n. Maybe I don't understand exactly what you mean. I've written some definitely-not-production code that does what I imagine in my head:\n- first page comes up centered.\n- as more pages open, on the right, the whole row scoots left to keep the \"active\" page in the center.\nThe ugly part of the current code is that there is no way to scroll a dom element to a negative index, so sometimes we have to change the positions of individual pages instead of the whole container. A better solution would just guarantee the parent always has a certain amount of left padding so there is always enough room to scroll to a negative offset.\nSee my commit here:\nhttps://github.com/asolove/Smallest-Federated-Wiki/commit/b4c3fcabee359e15a3a5ecedb134137ecb464870#diff-1\n. My sense from using it is that the centered view is not ideal. I reverted this commit locally because I was having trouble. Refactoring and looking at graphs are particularly difficult, because in the most common case (reading the article that is in the right-most column) the right hand side of the screen is empty and so you can see fewer articles at a time.\n. Regarding the map service: I definitely agree with Wm Leler about Leaflet, which is a great client-side API for displaying maps with annotations. But we need an actual service to host the image tiles. For now I will probably use the open data version of Mapquest, which allows free unlimited views of their maps for open-source projects. Hopefully I'll have a demo available tonight.\n. Just a note to myself: Peter Seibel is working on an API on top of d3 that is based on the seminal work The Grammar of Graphics. I would like to explore using his code and allowing users to interactively build different kinds of graphs by dragging in data elements.\n. Thanks for adding the extra documentation! I wasn't sure how to integrate this into the UI. Your suggestion reinforces the idea we had earlier of introducing an explicit edit mode. This is especially necessary on mobile, where people will be tapping and dragging without wanting to actually edit content. I will try adding this in and see how it feels.\n. Ward's comment is exactly right as far as my status. I think the work envisioned originally here is not now, or maybe ever, going to happen. A complete rewrite just ended up being impossible. But with the refactoring ongoing, it's maybe worth looking at incremental steps to make using SFW less painful on tablets. We could start by just adding tablet-specific css to make touch areas larger and see if we can identify ways to improve the tablet experience across the plugin-specific UIs. I am interesting in taking up some of that work but we should probably get consensus on what the real pain points and tablet use cases are.\n. ",
    "hellekin": "Hello, that is an exciting project!\nWould that make sense to fork to localhost if it exists?  I'm thinking of a mono-user case, e.g. running on a personal computer.  You would have a local copy of the wiki, probably sitting in a git repository, and you could then synchronize to wherever the public version sits using git.\n. I love that functionality, asolove!  As I see it, the URL should not grow when recording history: each session should be assigned an id, like any other wiki bit, and each visited id could then be recorded there, under, say, /path/abcdef.  When you want to share it, you can review it to edit the history (you might want to skip some pages, e.g. to provide a linear reading of a hypertext for printing, etc.) and save that selection to a special page (maybe a \"child\" of the original, unedited history.)  You could also maintain a session record, to resume a session at any point (e.g. in the context of a federated classroom.)\n. Just an idea popping up from the top of my head.  Maybe $hostname.local would be better than null, when using zeroconf e.g., in hackathons where people share the same room and can also share their git repositories over the bonjour protocol.  Or on VPNs...\n. Indeed, null is way better then :)  +1 for \"origin\", it's much clearer.\n. ",
    "built": "Two questions. If there's a better place to ask, let me know.\n1. Is there some documentation for those letter buttons at the bottom of an article? I see that they are adding up and creating a history, I just don't get what they are doing, what they are for, or what they mean.\n2. In terms of the full vision of the project, when you fork or pull in content from another server in the federation, will you pull in its history also?\n. Ah, ok. I'd been over that page but hadn't made the connection. Thanks!\n. ",
    "gregwebs": "I think a preferable solution would be to validate parameters on the server side. Thanks for looking into it.\n. To the client it would return an error stating which parameters are missing. Then it would be easy to see on both the client side (in the browser debugger, console.log, or even in a popup if the server is localhost) or server side (in the log) exactly what the problem is without viewing server side source code.\nThis is essentially a problem with weak typing in the routing layer of Sinatra. I have written helpers to both convert a parameter (say to an integer) or send an appropriate error response if it is missing or the conversion fails, but I would like to integrate them back into the routing in a declarative way rather than having to invoke a function on each required param.\n. ",
    "jufo": "Hi\nDid you miss out the \"bundle install\" step in the project directory?\nI also started with the default Ruby installation on Snow Leopard, and went round in circles as a result of trying to follow the installation instructions. There are two issues: the first is that different Ruby versions keep their own gems, so you have to install rvm, use it to install Ruby 1.9.2 and then tell it to use 1.9.2 before doing the bundle stuff. A lesser issue is that (I think) if you use the single-user install of rvm (preferred for development use - you only need the multi-user install if you are setting up a server), you will be working in your own user directory and don't need to use sudo for anything.\nHaving been round in circles on my MacBook Air before finally getting things going, I'll try to do things in a better order on my Mac Mini (where I am typing this). I already have XCode installed (RVM warns you that you need a downloaded copy of XCode, version 3.2.1 build 1613 or later, as the version supplied on DVD with Snow Leopard is buggy).\nFirst, follow the instructions for installing rvm for a single user (I'm just showing $ as the bash prompt):\n$ bash < <(curl -s https://rvm.beginrescueend.com/install/rvm)\nNext, to let you use rvm commands in the shell, execute the following statement to append the loading of the rvm function to your .bash_profile:\n$ echo '[[ -s \"$HOME/.rvm/scripts/rvm\" ]] && . \"$HOME/.rvm/scripts/rvm\" # Load RVM function' >> ~/.bash_profile\nNow start a new Terminal session, and use rvm to install Ruby 1.9.2:\n$ rvm install 1.9.2\nThis takes a little while.\nNow I can start working on setting up the Wiki. I'll start a new Terminal session just to be sure I'm not depending on previous rvm commands (I haven't used rvm before...).\n$ rvm 1.9.2\n$ gem install bundler\n$ cd Projects/\n$ git clone git://github.com/WardCunningham/Smallest-Federated-Wiki.git\n$ cd Smallest-Federated-Wiki/\n$ bundle install\nThis takes time, and is the step where I can see the gem you had trouble with being installed. Here's the output:\nFetching source index for http://rubygems.org/\nInstalling archive-tar-minitar (0.5.2) \nInstalling columnize (0.3.4) \nInstalling daemons (1.1.4) \nInstalling diff-lcs (1.1.2) \nInstalling eventmachine (0.12.10) with native extensions \nInstalling haml (3.1.2) \nInstalling json (1.5.3) with native extensions \nInstalling ruby_core_source (0.1.5) \nInstalling linecache19 (0.5.12) with native extensions \nInstalling rack (1.3.2) \nInstalling rack-test (0.5.6) \nInstalling rspec-core (2.4.0) \nInstalling rspec-expectations (2.4.0) \nInstalling rspec-mocks (2.4.0) \nInstalling rspec (2.4.0) \nInstalling ruby-debug-base19 (0.11.25) with native extensions \nInstalling ruby-debug19 (0.11.6) \nInstalling sass (3.1.5) \nInstalling tilt (1.3.2) \nInstalling sinatra (1.2.6) \nInstalling thin (1.2.11) with native extensions \nUsing bundler (1.0.21) \nYour bundle is complete! Use `bundle show [gemname]` to see where a bundled gem is installed.\nNow (fingers crossed), let's see if it runs:\n```\n$ bundle exec rackup -s thin -p 1111\n\n\nThin web server (v1.2.11 codename Bat-Shit Crazy)\nMaximum connections set to 1024\nListening on 0.0.0.0:1111, CTRL+C to stop\n127.0.0.1 - - [01/Oct/2011 20:40:35] \"GET / HTTP/1.1\" 200 2253 0.0396\n127.0.0.1 - - [01/Oct/2011 20:40:35] \"GET /style.css HTTP/1.1\" 200 1497 0.0742\n127.0.0.1 - - [01/Oct/2011 20:40:35] \"GET /js/jquery-1.6.2.min.js HTTP/1.1\" 200 91555 0.0052\n127.0.0.1 - - [01/Oct/2011 20:40:35] \"GET /js/jquery-ui-1.8.14.custom.min.js HTTP/1.1\" 200 34331 0.0022\n127.0.0.1 - - [01/Oct/2011 20:40:35] \"GET /client.js HTTP/1.1\" 200 17273 0.0016\n127.0.0.1 - - [01/Oct/2011 20:40:36] \"GET /favicon.png HTTP/1.1\" 200 3172 0.0015\n```\n\n\nSuccess! (The output above is from browsing http://localhost:1111).\nCtrl-C to stop the server.\nWhenever you want to run it again, you must have selected Ruby 1.9.2 before using the bundle exec command:\n$ rvm 1.9.2\n$ bundle exec rackup -s thin -p 1111\nHope this helps\nJustin\n. ",
    "samg": "@jufo - Thanks for documenting your setup.  You're right that each version of ruby keeps it's own set of gems, and it's misleading and incorrect how all the install instructions suggest running bundle install, and then running rvm 1.9.2.  It should be the other way around.\nWard's out of town this week, but I know he'd appreciate any improvements you could make to the installation instructions, and documentation.  It'd be great to get a more robust set of instructions pulled into the main repository so others don't hit the same headaches.\nThanks again.\n. +1 to this.  I'd go for it and create a pull request.  I personally don't see any advantage to the non-RESTful routes.\nAlso I know that there's some who believe that putting wiki pages in the site's root directory is bad (http://www.mediawiki.org/wiki/Manual:Wiki_in_site_root_directory).  I've been reprimanded in the past for this in my own wiki implementation.  Another argument in favor of your suggested routes. \n. Sven, I think the main criticism of having wiki pages at the root is that they then can collide with other special urls that point to wiki application functionality. For example if the application provides a list of all pages at /Special:AllPages then you can't create a wiki page there.  I built a toy wiki at one point to demonstrate a wikitext parser we were developing, and the home pages was edited by going to /edit, which meant that there could never be a wiki page about \"edit\". \nRegardless, i think including the resource name in the URL will address this concern.  The main thing is to somehow set up a namespace that allows wiki pages to be created with any name you choose, and doesnt collide with wiki application functions. There maybe other schemes for doing this as well. \n. It seems like this is a pretty good scheme.  I have a few thoughts.  \nWhat's the value of putting view in the url for the local articles?  I can see how always having tuples will make interpreting the url easier, but why view?  Do you anticipate putting other actions (e.g. edit) in the url at some point?  It seems like local (or even l) might make more sense if the intention is to indicate which server the content should be pulled from.  Also show would be a RESTier verb than view if you want to embrace that terminology.\nPutting the remote domain in the url seems to work, though I see one possible issue.  If users wanted to install the wiki in a subdirectory of the webserver this could break your scheme.  For example I install my wiki at www.drasticcode.com/federated-wiki/.  Perhaps the answer is you SFW can't be setup that way.  I know it was difficult to do this in Rails for several years, until they finally baked that feature in.  It seems like many enterprise shops like to structure their sites this way, so if there's a plan to appeal to them it might be worth baking subdirectory install in from the beginning.\n. I think just the ability to close pages, without clicking a previous page's link would be useful to the user.\n. ",
    "m3rabb": "Justin,\nThanks for your detailed post!  I'm not exactly sure where I went wrong the first time, but I was just able to get it working.\nI (more or less) reset my system to its state to before my failed install - uninstalled git, and removed my previous smallest-federated-wiki, .rvm and .gem directors.\nI used Ryan Bigg's guide for setup Ruby via homebrew instead of yours:\nhttp://ryanbigg.com/2011/06/mac-os-x-ruby-rvm-rails-and-you/\nI followed the instructions pretty closely but differed in a few places.\nIn the housekeeping section, I went directly to the homebrew page and used the instructions there: https://github.com/mxcl/homebrew/wiki/installation\nAfter git was loaded, I used the following to re-setup my RSA keys:\nhttp://help.github.com/mac-set-up-git/\nTowards the end of the RVM section, I used .profile instead of .bash_profile. (Differences explained here: http://stackoverflow.com/questions/415403/whats-the-difference-between-bashrc-bash-profile-and-environment)\nI installed Rails, but then stopped short of installing any mysql.\nI then returned to your (Justin's) guide at -> gem install bundler, and followed the rest.\nThanks again!\nMaurice\n. ",
    "leto": "Just as a data point, I disable javascript on my phone for security purposes and to increase battery life. I assume many others do as well.\n. I noticed ALT-S works without the dialog box, closing.\n. Yes, I was on Ubuntu 10.10 when this happened. CTRL-S saves and opens a \"Save as\" dialog, while ALT-S just saves it.\nI agree, the above breakdown for each OS would be useful for new users.\n. It was not intuitive to me that I needed to input a URL prior to clicking \"Claim\". Maybe some client-side JS that prevents an empty submission + notifies?\n. @WardCunningham I can work on that if you point me in the right direction.\n. Yes. I am fine with a no-javascript \"Hey dummy your forgot to input a URL error\". That will surely be more user friendly than a stacktrace from Sinatra.\n. ",
    "allenwb": "Well, I've talked to some Mozilla people about my idea, but it isn't something I've really pushed on. Also, Crockford said he liked the idea. We'd probably need to get broader community support before the browser implementers would really jump on it.\nThe basic idea would be to recognize application/json or application/jsonp as distinct mime types in script tags.\nSo to load a json data file you would say something like:\n <script src=\"whatever\" type=\"application/json\"> </script>\nand the user agent would recognize \"application/json\" as a distinct scripting language which it would process using the JavaScript JSON.parse functionality it already has.  The resulting objects with no cross site restrictions would then be hung off of the DOM node corresponding to the scripting tag.\nI have a writeup that I have never broadly circulated that describes this in more detail and address issues like how to make it work with existing jsonp servers that expose files that would normally not parse with JSON.parse.\n. Note that my proposal is far safer than current jsonp approaches which allow complete arbitrary JavaScript code to be injected and executed. My proposal only loads valid JSON data trees to be loaded.  Such trees are complexly inert.  No execution would be involved.  At worse you get random strings, numbers, and interior tree node objects hanging of specific script nodes. You are going to get that from any form of json data load, so I don't see where your concern lies.  If it is that the data becomes accessible by anyone via dom traversal then stash the object reference in a local var and delete the script node  as soon as it is loaded.  However, you probably aren't really protecting much.  If anybody can already access the data using jsonp and a script node with type=\"text/javascript\".\n. ",
    "nrn": "Very nice.  I actually started on this last night but my solution was going nowhere so I decided to sleep on it... i guess that solved it :)\n. Thanks.  I started off trying to improve Svens existing code, but looking into it decided that the Sinatra to Express port could be done pretty directly.\nThat makes sense.  My initial thought was to not clutter the repo with dependencies, but that does raise the barrier for new installs.  I should have included an ReadMe, the current install process is npm install in the directory, then coffee app.coffee to run it.\nThank you very much, i'll get it set up in there, with dependencies included.\nSounds good, I took the easy way out and was just manually copying /default-data to /data instead of including a way to look in /default-data if it didn't find a page in /data.  It is using the client folder directly, it just needs the extra static html/css files at the moment, should I put those in /server/express while I need them?\nI appreciate your time looking at this stuff, and am excited I can contribute in some way to such a cool project.\n--nick\n. So, I got a bunch of stuff fixed updated and rearranged, and now everything is kind of borked with the last commit...\n. Sorry guys, hope I didn't get things tangled up too bad here.  This has some crap that got added with a bad git commit -a\n. Yeah, sorry about that  This commit looks like mess because it merges everything you guys have done back into my branch in a single step, the other way the difference should be very small, but i'm not sure how to show that in github.\nThe static html and css files I was using got moved to /server/sinatra/views (where they should be regenerated from haml/sass when anything changes)\napp.coffee got moved to server.coffee, paths got cleaned up, and the action handleing got switched over to _ ,since it is now a client dependency as well.\n/server/express/node_modules got added as requested with express and its dependencies installed (pretty big addition)\nadded Stephen's /server/express/Readme.md to mine\nand that's it.\nSorry this got so tangled, and thanks for making room for me.  This pull request should be good (unless you don't want node_modules included), but if it would make it easier going forward I will gladly trash this fork, fork again, and reapply what i've been working on from there.\nIs there an IRC channel or mailing list for SFW?  I feel like I've made a pretty good ass of myself here with poor communication, but I may just not be using the tools at had effectively.\n--nick\n. I would be glad to make it whenever I can, but with my work schedule,\nany time you choose will be just as likely to be good or bad on any\ngiven day :)\nOn Sun, Dec 11, 2011 at 9:26 PM, Ward Cunningham\nreply@reply.github.com\nwrote:\n\nOh, it's not so messed up. We're just facing a dozen button presses instead of one. Live and learn.\nWe're not big enough a community to keep an IRC channel occupied yet. There is talk of a G+ video hangout once or twice a week. What time would be good for you?\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/pull/65#issuecomment-3101953\n. I'm going to go ahead and close this pull request and start from a fresh fork of master.  I've got ruby/rvm/bundle setup now, and starting to dig into rspec and see how many of these I can get passing.\n. No worries.\n\nSounds like a good plan.  I took the version you put up with the fix, that was the only change I had made.\n. Implemented handling of urls by pushing my problems into the client side.  Now if the pages shown in the url disagree with the pages shown in the html when client.js is loaded it will create the pages and append them to '.main'.  I'm not sure if this is an acceptable long term solution, but it solves the problem of not being able to use the haml templates in other back ends for now.  This commit takes the express server to 14 tests passed, 2 failed.\n. I was just re-watching some of the videos on http://wardcunningham.github.com/ which lead me to try an interesting experiment in light of this last commit:\nhttp://nickn.github.com/view/welcome-visitors/view/indie-web-camp/view/why-indie-web\nI'll leave up the repository with default data for a bit just to explore the possibilities.\nhttps://github.com/nickn/nickn.github.com\n. Yeah, that did come out a bit obfuscated.  When client.js loads It takes the current path name, splits it into a list on the '/', ignores every other one (making the bad assumption that it is always \"view\"), and then splices out the empty string from the leading '/'.  It then iterates over the list, and if the sting isn't the id of a page in the dom adds it.\nHmm, just found an example with the remote site syntax, I don't think that this should have any effect stuff served by the ruby server, with or with out remote sites, but i'll get that included soon.\n. Cool, i'm going to put this on the back burner for a bit, but I will revisit it unless someone else gets to it before I do.  It should be a fairly easy port from favicon.coffee to a client side script.  I like both the gradients and Sven's idea of pointing the favicon at a gravitar, I think as long as they are unique it proves a great visual way of quickly telling where resources are and where they came from.\n. Thanks, I'm really happy with how that turned out.\n. I've been looking over all this, and I think i'd like to stick with png.  Most browsers don't seem to support svg favicons, and I think browser support of the favicons is a surprisingly important factor in browseability.\nMy proposal would be this, if the browser requests the favicon for the local site, and gets a 404, it downloads a bit of javascript, renders the gradient to a 2d canvas element, sets that as favicon.png and attempts to upload it back to the server for future use.\nAll the canvas code should be there in the current favicon.coffee, we just need to browserify it a bit, and add the logic to get it when there is a 404 for /favicon.png and and to upload the favicon when it's done.\nIt makes the server side code lighter for all implementations, especially node removing the node-canvas requirement.  While only adding a few lines to the client, and also maintains all the functionality we have already.\n. Right now favicon stuff is working within spec on the express server.  The main problem is that it has a very heavy requirement on the server install for the node-canvas library, which includes cairo and some other stuff.  Which is a pain on linux installs and possibly impossible to install on other operating systems.\nMy theory on this is the same as yours Ward, except I hadn't envisioned a nice picking scheme.  I think this should work fine, if we gave the favicon link an id of favicon:\n$(\"#favicon\").attr('href', ctx.canvas.toDataURL());\nor something along those lines.  I think I can implement this with a few minor changes to client.coffee and a bit of browserification of favicon.coffee.\n. I'm going to move forward on this in the next couple of days if no one has yet.  My plan so far is to make a plugin that gets downloaded when the favicon returns 404, generates an image based on the same criteria we have been using so far, and then uploads it back to the server.  We can add a slick ui later, I just want to get the node-canvas dependency gone on the server side.\n. Thanks, sounds good.\nI've read over it, and I see two possible courses of action.  I think I can shoehorn in the remote site information into what is already there, but my preference would be to remove hidden state from the history api code, and only pushState urls.  This would mean that back/forward wouldn't track which article is active, but playing with it so far that would probably be my preference anyway.\nAny thoughts?  Either way, I'll take a shot at it after this gets merged with what you are working on.\n. this last commit is just what it says, work on /server/express/lib/server.coffee only.  It is the other half of implementing remote site links in node.\n. Awesome, I've been giving this quite a bit of thought for the node server.  I think openID is a great solution.  I have a long weekend of work, hopefully i'll get a stab at this in on monday or tuesday.\n. I'm currently most of the way through this, but I had to admit defeat in my html only approach, and am migrating to using handelbars templates for express serverside.  Should have it up today.\n. I know google works with:\nhttps://www.google.com/accounts/o8/id\nI'm not sure who all offers OpenID, or what the normal way of making this easy is.\n. With OpenID now working on the express server i've launched my test wiki running the express server at:\nhttp://nrn.io\nYou may notice I changed my github username to match my new home on the web.  It feels like it may have been\nan odd thing to do, but it seemed like a good idea at the time.\n. Yeah, I made attempts with haml.js and haml-coffee previously and failed hard at both.  I think it should be pretty easy to use mustache or handlebars on the ruby server, and as long as we don't do anything tricky we can consume the templates with either one.  I unfortunately don't know enough ruby to muck about in the ruby server without breaking everything.\nOne interesting thing with the mustache templates is they are usable in a great variety of frameworks on the server side, and could also be used client side if that became desirable later.\n. Sounds great.  Unfortunately i'm not going to be able to join this week, but next week seems likely.\n. Yeah, i'll see if I can get video working after dinner :)\n. Sorry I wasn't able to make it again this week :/  Hopefully next time.\n. Hmm, we're in here.\nOn Wed, May 16, 2012 at 10:04 AM, GerryG\nreply@reply.github.com\nwrote:\n\nHey, the hangout link seems to no longer work. \u00a0Anything new on this?\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/issues/88#issuecomment-5746189\n. Yeah, I'll work that out soon :)\n. Solved with #90\n. Hmm, \"does not serve the wiki\" reads as \"completely broken\" to me...  It shouldn't be completely broken, just kind of broke, but getting better.\n\nif you cd server/express/bin and then ./server it should run the server in the repo directory with reasonable defaults for localhost:3000.\n. Hmm, probably not.  It should work where ever it runs from.  I'll have a look at this tonight, off to work now.\n. I really like this change, there was no reason to require a global install of coffee-script, and this is a very nice way of getting rid of it.  I pulled from your branch and made a few modifications, and put in another pull request, let me know what you think.\n. Hmm, yeah, we've had a lot of turbulent stuff like that, it seems to be ok, but if you've got a better work flow that doesn't leave those artifact merge commits i'd be glad to hear about it.\n. Something about the redundancy strikes me as off, maybe.  I was thinking it could be more descriptive, but that's a good point that the path really does describe it just fine :) \n. I'm starting to think that just adding the .js back to server/express/bin/server.js may be the way to go.  The name of the file doesn't have to be same as the global command when installed with npm -g, see https://github.com/substack/node-browserify/blob/master/package.json#L7\nFor global commands I do like the \"d\" at the end to denote a daemon, it helps to make sense of the commands.\nI've been waiting on finishing up the package.json with a bin entry and real package versions ranges, etc. until the server/express code stops changing so much.  I'm hoping it will have calmed down some in 2 weeks or so.\n. Yeah, the title was the main thing I was after when I tried to tackle this.  I was looking at getting the inner html of the a tag that called the event that led to the 404... but i'm not sure if that was going to be a fruitful approach or not.\n. Arrgh, renewed discussion here reminds me that I need to implement server side rendering on the express server.\nWe are already using pushState for all of our history interaction, but I agree that there is no reason to change that.  I think the better fail over for federated wiki for browsers that don't support pushState is to just ignore the url, but otherwise function normally.  \nI had imagined that eventually if you went to a /view/something/view/something-else url that you would get a full page with prerendered html, and all the links in the page would be to /view/slug instead of /slug.html, meaning that if you manually open links in new tabs, or browse without javascript enabled, etc, everything would just work as expected.  But if javascript was enabled it would hook itself up to the page when loaded.\n. I've been thinking about exploring a backbone client, and seeing how that fits with our goals, just wondering if there was any movement on this first?\n. The Selenium/RSpec stuff has been a bit of a speed bump in development for me, but I think it's important that the integration tests are the same on both servers, and it's been very nice having those already setup for me to try and work towards, so I've got no desire to change there.\nWhat I really need is a good unit test setup so that I get better test coverage, and so I've got more bugs taken care of before I run the integration tests.  As far as I can tell there isn't really a consensus yet in node as far as testing goes like there is in ruby around RSpec.  I'll probably just pick one and give it a shot soon, and try not to be married to the idea so if it doesn't fit right I can throw it out and try a new one.\n. server/express now has testing with mocha, which seems to be going well.  It also passes 17/18 integration tests.  There is still a lot that is untested, but I think we can call this issue close :)\n. Thanks Tom.  I appreciate the contribution, but I do have a couple of questions though.  I know I've got a lot of bugs still, but in the interest of learning as I go here it would help to know what bugs a commit fixes.\nThe changes to authenticated also worry me.  It's route middleware that could potentially get called a lot, and now requires a hit to the file system where it didn't before.  I'm also not sure we want the default behavior of a wiki that thinks it's unclaimed to be treat everyone like they own it.  The idea now is that an unclaimed wiki can be claimed, but to exploit that on an established wiki you would have to break it in such a way that the wiki thought it wasn't claimed, but it wasn't so broken that you couldn't reclaim it yourself.  With this new setup anything falsy happens in the path.exists call and the wiki is accepting all writes.\nThe setOwner call back stuff is probably a good idea though.  I hadn't run into a problem, but that should be more robust going forward.\nthanks\n--nick\n. Yeah, as you can tell I was up way too late as well :)\nSo instead of having separate claimed logic like the ruby implementation does, I've instead been using the owner variable to show that the wiki is unclaimed if it is falsy.  The whole\nweird dance with owner and setOwner was so that we didn't have to read the file system every time a request came in to see if a wiki was claimed, and if so, by who.  We could cache the claimed status, but I think a falsy owner covers all the use cases I can see.\nGood point on how the ruby works, I had missed that.  So I'll have to champion the idea that current behavior is wrong :)\nEdit: Whoops, I put fake tags around this and it disappeared them.\n(fictional_user_docs)When started, a wiki by default is a read only place.  Any changes a visitor makes are stored only to their local computer, for them to own on their local file system.  Only when someone steps up and claims a wiki as their own can it be edited on the server, and then only by them.(/fictional_user_docs)\nI think that this behavior would improve security, making the default action if a wiki is broken and can't tell who owns it to be read only instead of accept all writes. It would also prevent heartbreak from users in the case where someone starts editing a wiki thinking \"i'll claim this later, for whatever inane reason i'm too lazy right now, even though I'm putting in time, effort, and important data\", and then someone else just comes along and claims it out from under them.\nLooking back up I appear to have mostly been babbling incoherently after \"The idea now is\", sorry.  I think I covered my arguments in a less crazy way this time... I hope.\nI am absolutely glad for \"correctness\" changes, my ideas on best practices in node are still evolving, so that is actually quite helpful :)\nThanks,\n--nick\n. I definitely agree that the do things fist and set it up later workflow amazes people.  However I think a typical scenario will see this demoed virally with the demoers own wiki, making the magic editing locally, and then pushing the demoee's changes up to a farmed server they then own.  The other way still works with an unclaimed site and default-data, but the demo is local until the claim happens.  I think this setup does more to funnel visitors into contributors then an open site, however I'm willing to give up this argument to you're judgement here.\nHowever, I'm still going to argue about how we handle the claimed/owned state internally in the express port :)\nSo I see two options:\n    1) we hit the file system every time to see if the wiki is owned, and if so by who\n    2) we could keep track of it in memory and only hit the file system when needed.\nThis commit moves us to what I see as a bad combination of the two where we track ownership in memory, but then don't trust that owner information as to the claimed status.  While I agree that emphasizing the logic over the performance is key at this stage,I don't think that tracking the owner in memory and persisting to disk is premature optimization.\n. If i'm not overstepping my bounds what I'd like to do is merge in the setOwner cb changes, but remove isClaimed, and instead trust that a truthy owner is the same as the wiki being claimed, and go with your judgment on changing an unclaimed wiki until I can come up with more compelling arguments :)\n. Ohh, good point, that is probably the main problem!  I was thinking the tests were using my view of how an unclaimed wiki worked.  I'll go ahead and do that.\n. Sounds like a good plan.\n. I'm not positive, but i'd guess that it got fixed with Adam's work c3ad12dd867daa086975623f38c1fe94a303e1a6 and your fix 76c1314fe5ee1e49b142bdc83897886f813f073e\n. I think that's a good idea.  Local storage is one of the most confusing parts of SFW when showing it to people.  I think it's time to have a distinct change to the favicon for local pages also.\nOne thing that might help would be to do a create on change instead of create on visit 404, it also feels somehow like page uuids could help, so when you got a page the client could see if you got exactly the one you were looking for, or something else with the same title.  Possibly only allowing things to be saved over the same uuid page?  With an explicit upload required to copy over the entire page.  Just throwing some ideas out here, as I don't have a clear solution to propose :)\nI'm in the middle of looking at the file io on the node serer when I get a spare moment again, but i'll get the don't create over an existing page in there on the next pull request.\n. Thanks, some magic was getting away from me there.\n. Also added a quick testing fix to this pull request.\n. Well, that is some very odd behavior.  Express is doing the same thing as sinatra and appending the divs before it sends out the html.  I'm off to bed but i'll dig into it as soon as I can.\n. I can't seem to reproduce this.  Running the ruby version from your history branch, and chromium from debian testing repo everything looks as it should.  Any ideas on what I could be missing?\n. That was a safety to guard against a trailing / in the url which would cause an empty page to get created on static server implementations.\nIn your logging does the wiki.log 'createPage' ever get called on line 531?  Cause if so that is a bug in any dynamic server implementation (github 404 magic uses it, and it could be used with other static servers).  Every page in the url should be in the dom when the page first loads so I think we're chasing down the wrong section of code here.  Those logs are to be as expected if you are sending the initial contact with the pages in the html but not the url.\n. Guessing because I don't have the broken setup to prove it, but I think the problem is you need a setState() line around 526 for the ruby server in some instances.  But in testing this i've found another problem with the fetch stuff using the new history stuff.  Hopefully get another commit together tomorrow fixing this stuff.\n. Glad to.\nThis last commit makes calls to fetch non destructive to the global wiki.fetchContext, as well as making setActive and setState more explicit, scrollTo is also only called from setActive now.\nThe long and short of this branch is that i'm trying to make it easier to figure out what is going on as we navigate through the browser history as well as making those interactions more robust.  It should  be getting pretty close to pull worthy, i'm hoping this last commit also fixed the error you were seeing with osx/chrome/sinatra.\n. If this is working on your setup I think this branch is ready to ship.  This last commit just changes setState to setUrl (to be a little clearer), and moves the popstate handler down with the rest of the event handlers.  It passes all the tests and seems to work well in use on all the setups I've got available at the moment, as well as poking around a bit with various setups on http://browserling.com/\nLet me know if there is anything else that needs to change in this one.\n. Welcome!  An issue is a perfectly good place to introduce yourself.\nI just gave a concept of this a shot on a  branch here.  The syntax I'm trying is [[Title of Page] optional shown text].  Seems to workout as I'm trying it out.  Let me know what people think.\n. I agree that we don't want to be inventing our own markup language, there are plenty out there already, and they can be dealt with well in the plugin space.  However reading this I'm not clear on what we want the default paragraph type to end up, html backed wysiwyg?\n. Ok, so that's a fairly compelling essay.\n. Hmm.  Is the active class still changing properly?\n. Took a shot at this on my system, but I can't find what would break it in that way.\n. Aargh.  Glad you found it though.\n. That is really odd.  None of the versions of chrome/chromium I have access to on linux/windows has that problem.\n. Very cool idea.  Just got this running on http://nrn.io\n. I'm amazed that github lets me link to your repo, with commit ids that only exist on my fork and it shows the right stuff!\n. Ooo, nice.  I had buried mine back in the comments last night, I'll look into that soon\n. I probably should have done that git stuff in a branch, I'll take a look for an easy way for the developers to get to a link but not users later today.\n. I took a look at this and didn't see any consistently good way to do it.  What is a clickable link depends both on which browser you're using and how you are looking at the source  You can even click on a console.log('url as a string') in chrome which is pretty slick, but no such luck in firebug.\n. I'm not sure that it does everything we need it to, but I've had it in the back of my mind that https://github.com/substack/vm-browserify may work for running plugins we don't trust.\nAs for the static server thing my answer was to add the .json to the static files I was serving.  I haven't updated it in a long time, but I was playing around with running a read only SFW based on github pages.\nhttp://nrn.github.com, https://github.com/nrn/nrn.github.com\n. This also did a drive by of my master, there will be a separate, clean pull request for Express: Cleanup soon.\n. Absolutely.  I wouldn't want to write that out to the dom every time something changed, but we could expose it in the console as dataDash.stats() for example, which the program could then write out to the dom whenever it felt like it calling something like $('body').dataDash({stats: dataDash.stats()});?  Does it sound like that would fit the bill?\n. First draft of collecting stats.  You can get to it from the global context with wiki.dataDash.stats().\n. Wow, we need to fix that script tag problem :)  I've got stats reporting with a new plugin type, \"stats\" temporarily.\n. I've got two failures right now:\n1) viewing journal should highlight a paragraph when hovering over journal entry\n     Failure/Error: first_paragraph.should be_highlighted\n       expected # to be highlighted\n     # ./spec/integration_spec.rb:282:in `block (2 levels) in '\n2) viewing journal should highlight all paragraphs with all the same JSON id\n     Failure/Error: first_paragraph.should be_highlighted\n       expected # to be highlighted\n     # ./spec/integration_spec.rb:294:in `block (2 levels) in '\n. All the tests are green.\n. Whoops, yeah, this is going to take a bit of work to get into shape around the plugins and stuff.  I've just been playing with the dataDash stuff and exploring.  If you want to go this direction i'll get this in shape to pull by the weekend.\n. Awesome.  I think i'm on to something here, but I can't quite put my finger on it, it probably needs some more real world use to flesh it out.  I'll do what I can to get everything working together right.  Any tests you guys write would help a ton.\n. Still not fully tested, but I think we are getting close.\nhttp://dev.nrn.io:3000/view/welcome-visitors/sensors.c2.com/list-of-sensors/sensors.c2.com/daylight-sensor/view/d3-line/view/d3-bars\n. Well, all tests are green, and everything I can find to fix is working.\n. Let me know if you see any blocking issues in pulling this.  As I continue to play with it I believe we're ready for master.\n. Understandable.  I'm feeling pretty good about it at this point, but it is a fairly big change :)  I won't be offended if you want to put this off and grab Stephen's changes now, as long as you give me some idea as to how to make you feel better about it.\n. All tests are green again, and looking good under visual inspection\n. http://dev.nrn.io:3000/view/welcome-visitors/sensors.c2.com/list-of-sensors/sensors.c2.com/daylight-sensor/view/d3-line/sensors.c2.com/shoplight-sensor/view/d3-line/sensors.c2.com/distortion-sensor/view/d3-line\nif my dev server is up this is an example of visualizations finding data magnetically to the left, so far i'm liking it, but that example highlights how bad the page names as ID problem is.  I'm going to go after that next, possibly with temporary random ids, but i'm going to have to test that a bit.\n. It's actually looking at all of the .items that come before the visualization, including on the same page, however it doesn't see any that come after it on the same page.  I think that introduces a lot of edge cases, with multiple data elements and charts on the same page, that I'm not sure how we want to handle, but if you can clearly define the behavior I can figure out how to implement it :)\n. Nice fix on the images, makes the demo much nicer.\n. I'm working through things trying to make the page id unimportant, once I'm sufficiently there I'll try to figure out what it should be instead of the slug.\n. Which I find terribly inconvenient, I was thinking I could just move to using site/slug as the id until earlier today.\n. Well, this seems to be working well without a page id at all, any thoughts?  \nWarning: This is not good to pull at the moment, the sinatra server needs some work to pass data-site instead of id.\n. So, that was a design choice to treat all dataDash get results as an array, which looks kind of ugly when dropped in to replace jQuery's .data method, which only ever returns one item.  Think it should have been 0 on that one, will check in the morning.\nThanks :)  Around 450 is some pretty dense stuff that gets triggered every time we get a popstate event, taking the information from the new url and redoing the pages in the dom to reflect it.  The stats were making sense before this latest round of stuff, will look at them again with the new lack of id, etc, in the morning.\n. Oh, and the stats are looking fairly reasonable to me.\n. Yeah, It's probably a small change, we need the ruby server to add data-slug=\"slug\" to the page div elements instead of setting the id.  But I don't really know my way around haml, or ruby.\n. Giving this a shot\n. So this seems to work, everything looks good and all the integration tests passed.  However I broke a bunch of ruby unit tests that seem to expect specific html that is now different.\n. All systems go, all tests green, looking good poking at it.\n. Hmm, don't seem to have that problem.\n. Oh, wow, didn't even notice those did that.\n. fixed\n. Got the latest stuff in :)  It's still looking good to go to me.  Let me know if there is any thing I can do to put this on the path towards merging into master.\n. Fixes the biggest problem in #168\n. Sorry Ward, took another look at this again after we talked, and I can't reproduce it on any of my setups :/  Let me know if you want to pair and take a shot at this.\n. Well, I'm not very confident without the broken setup to test against, but I took a shot in the dark.  Just some minor improvements trying to make stuff that might be related less fragile.  If you get a chance let me know if that makes it any better, tests are still good.\n. Your understanding and guidance on this branch have been invaluable, thank you!  Man am I glad to have this pull request done.\n. Doh!  Sorry.  Well, the mystery of what i'm going to work on now is solved...\n. Aargh.  I spent the afternoon looking at this, with no real leads.\n. Aha! I got one to do it for me.  Well, I found our problem.  For some reason the sinatra server seems to randomly serve up a very old version of client.js, not sure what the most direct way to a solution is though.  But I got a fresh reload that came up *.fed.wiki.org/view//view//view, and the client.js didn't even have dataDash in it anywhere, even though the html was the modern version, and dataDash was downloaded.\n. Sorry, I mean when trying to access sinatra server you end up with a very old version of client.js.  It's more then likely not actually coming from the server.\n. The broken one seems to get 200 ok size: from cache\nWhereas the working one seems to get 304 not modified size: 159b\n. Yeah, something along those lines fit my experience as well.  It seems important to note, that the last modified on the client.js my broken instance is running is \"Wed, 14 Mar 2012 21:43:44 GMT\"\n. Well, my ruby guru says we should get off webrick for production, and recommends the heroku platform as a service.\n. Well, he volunteered to answer questions :)\nIn this space I started to write about how this was probably just some http header stuff that we could probably fix with a little bit of research and inspection, but then I looked at the sinatra book on deployment http://sinatra-book.gittr.com/#deployment, it doesn't even mention anything other than heroku.\nLet me know if there is anything I can help with here, I'm really not sure what to do.\n. So picking and choosing the behavior I like that seems to play well together from these discussions leads to this:\n- History before a fork disappears when forked (except previous forks), fork saves either the entire state of the page as it exists when forked or the diff from the last fork, owner of a wiki can collapse everything back to the last fork into one action (as if it was forked from itself, hiding iterations which were thought better of).  I think it's always important to keep the fork actions, removing them removes the citation of the person from the page, but what minor actions they did in what order, especially if they undid them, seem to be irrelevant once the conversation moves on.\n- Hovering over an action shows the change set from the previous action.  There needs to be a good way to do this between two different open versions of the same page.  Clicking on an action displays the page as it was then, editing it in that mode forks from that point.  This is possibly paragraph specific?\nI believe this solves several important problems.  It's surprisingly hard to collaborate if you can't tell what someone has improved since the last version you worked on without manually looking over the entire article.  If you could do this by paragraph you could take the improvements that work well for you and leave the ones that don't in an easy way.\nIt also helps a lot with the problem of embarrassment in publishing.  Working in git you have to accept the idea that the stupid mistakes you make will be visible to anyone who cares to look no matter how fast you correct them, the only saving grace is that there is a separate publishing step, and any mistakes you make between those publishing steps are hidden from prying eyes.  Losing focus on a paragraph doesn't seem to be long enough of a publish step to gather your thoughts, and leaves a large amount of embarrassment to get over, as pretty much all of your editing is visible forever.\nThis would also reduce the noise in the journal, making it more meaningful, hopefully exposing the information in a way people can understand why they should care about it.\n. Should all be in /server/express/ReadMe.md, probably need to point that out somewhere else though :)\n. How'd the install go?  Anything wrong with the install docs other then not being noticeable enough?\n. I am running in small production right now in dev mode, so that I get any error messages that happen, I had forgotten about this.  It seems to be perfectly fine for moderate use, but if you are planning on getting a ton of traffic in you might want to take a look at other options for storing session data, like https://github.com/visionmedia/connect-redis .  I'm not going to worry about it until we are approaching v0.1.0, but if someone else wants to do the due diligence on session stores I'm not going to argue :)\n. It's like the simplest passable thing to temporarily store session data.  The biggest problem for me is that it loses session data every time i restart the server process.  I just don't want the complexity of dealing with something else when things are changing this much.\n. My first guess would be that your edits are in local storage.  If you claim a server and then edit things while not logged in it would give the symptoms you are describing.  You can check this at view/local-editing.\n. I'm taking a look at this, and I always seem to be getting the destination url instead of the origin url...  Anyone know anything about this stuff?\n. I think we appear to have yet another instance of all browsers/os/servers/etc. handle it differently.  In chromium i'm getting the url that i'm headed to instead of from on both servers.  In iceweasel i'm getting a failed CORS request with express and it works with the ruby.\n. This is also interacting poorly with the difference between www.example.com and example.com.. blargh.  Fix coming after lunch.\n. The problem is multi parted.\n1) server/express isn't allowing cross origin requests where it should\n2) meta-factory isn't detecting federatedWiki plugin drops in every case it should.\n3) Some browsers provide different information to event.dataTransfer when you drop a link in.\nNo idea how i'm going to fix 3 though.\n. I've got the easy fix up.  I'm not sure what the general case should be for handling urls, especially with www. in front.\n. Thanks :)  Sounds like a good plan.  Case 1 should be fixed, testing using iceweasel on Debian testing, and should bring the express in line with sinatra on this feature.\n. And just to make everything easy on you the node server uses a completely different set of unit tests :/  sorry.  The good news is the commands for the general rspec stuff are in the main readme, and the express stuff is in the server/express readme.\n. Sorry, this doesn't actually implement merging, or streamline localhost (both of which were big topics at today's hangout).\nWhat this does is let you setup a bookmark that when clicked opens the same pages you had open, but with your home server open instead of whatever server you happen to be on.  For example if I was browsing new.fed.wiki.org at say:\nhttp://new.fed.wiki.org/view/welcome-visitors/view/how-to-wiki/fed.wiki.org/add-text/nrn.io/indie-web-camp\nand then click on the browserlet i get redirected to:\nhttp://nrn.io/new.fed.wiki.org/welcome-visitors/new.fed.wiki.org/how-to-wiki/fed.wiki.org/add-text/view/indie-web-camp\nSo now I've got my version of the client javascript, plugins, etc.  And if I'm logged in I can edit things and they get persisted back to my server instead of local storage associated with the other site.  You can see the 'view's in the original url became 'new.fed.wiki.org' in the new one, and 'nrn.io' in the old one becomes 'view' in the new one.  Unrelated server 'fed.wiki.org' stays the same.\nThis seems to make for a pretty nice workflow.  The bookmarklet works well, but i'm thinking about how to put this in the page, i'm not sure what the clean way to do that would be though.\n. Sorry, I haven't implemented the recent changes special page on the express server yet.\n. The thought for that use case is that whoever is most invested in the collaboration happening will start a farm instead of a single wiki, allowing everyone they want to work on it to create their own wiki dynamically on their server, if they don't have one.\n. lol, I hadn't considered that case.  So the express server is setup to download the favicon plugin and generate a new favicon if it receives a 404 error when trying to retrieve the favicon, it then pushes this favicon back to the server, and if you have permission to write to the server it saves it for you.  I'm up for suggestions on how to deal with this, but still provide the desired functionality of creating a favicon when none actually exists.\n. I'm continuing to work on this branch, slowly moving parts out of legacy and adding some tests.  The test setup we have seems to be working very well, using Mocha to run the tests, Expect to assert things, and I just added Sinon for spys, stubs, and mochs.\n. Doh.  I'm off to jury duty today :/  But if there is anything I can do to help tomorrow or thursday let me know.\n. I actually could have, I didn't know ahead of time, but they had wifi, tables, and power.  I took the time to catch up on some reading though.\nWard, do you want me to take a stab at merging models and modules, or wait till we have a moment to work together on this?\n. No worries Ward, this reorganization stuff does not play well with git.\nI'm having problems figuring out how to test some stuff (like the history/location code), but most things I feel like I'm getting meaningful tests in on.  Getting jQuery on the test page how we really want it is part of what we are wedged up on :)\nVery nice!\n. Big merge resolved!  Everything is looking good so far, but this needs some shaking out for sure.  One test fails because we haven't gotten the main wiki api free yet, and I figured it was better to leave it broken instead of faking it.\n. ./bin/server.js --help\nwill print out all the command line options.\n./bin/server.js -f\nWill start the server in farm mode, with otherwise default options.\n. Well, at least it would be doable as single server.  That may be the desirable pattern anyway for a service like heroku, having lots of people deploying small free single servers.\n. Very cool, I'm looking forward to implementing the couch stuff on node when this is finished, now that I'll  have a reference for all the hard parts :)\n. Yeah.  Unfortunately we discovered that it was useful to have sets of pages up with multiples of the same page, and be able to navigate them normally.\n. Moved wiki.fetchContext to fetch.context, rearranged fetch so that the logic as to what to call buildPage with is all in the main function, and added tests.  \nI've been cheating some with a faked up wiki object in these tests, hoping to quit when we get the main api into it's own module.\n. The perl is an interesting bonus, not the heart of the build process.  I'll try and get some better docs written up for this tomorrow, but the the straightforward way is:\nnpm install in the client directory then you can build the client with npm start and\nthe test client with npm test, no need for global installs.  I haven't been having any bigger problems debugging in the browser than before.\nThis has a lot of advantages, from being able to easily use modules meant for node on the client, to having all of our javascript only need one script tag (much faster to load), and instead of having everything share one name space we can clearly see how the parts contact each other.  The workflow isn't perfect, but I think it's on the right track.\nEverything was moved into legacy.coffee, which we are then splitting up.  As i'm imagining it, as that progresses client.coffee will be the main entry point into the client again, instead of just a reference to legacy, which is going away.\n. Took a crack at updating /client/ReadMe.md to reflect the new build process.  I was also thinking the perl script can probably be made to use /client/node_modules/.bin/browserify instead of the global one, but I don't have a mac to test it on.\n. I'm kind of thinking that we may want to do away with default data fall back completely, and instead when a site is created we 'prime' its data/pages with any defaults we want (for the main project that would probably be just the welcome-visitors page with relevant forks in it.)  This would simplify a lot of things as we move forward towards supporting multiple databases/storage mechanisms.\n. That would be tempting, there is a node irc server, however I've only got the one server and I use it for lots of development stuff, so it's down a lot.\n. It got a bit tangled with me stashing, pulling, and then reapplying changes, so maybe I didn't.  It is supposed to auto detect those things though :/\n. Whoops, sorry Ward, I'll go through this stuff tomorrow.\n. Hmm, I went off in a weird direction after this stuff that didn't pan out, and it looks like when I trashed it I threw out a bunch of stuff that I thought was in this commit...  I'll get this straightened up and resubmit.\n. Hmm, never seen that error.  Node will implicitly compile the coffee script as long as you start with an entry point that is in javascript and requires coffee-script.  For us that means either launch from the command line at /bin/server.js or require('./index.js\") from another node program.  Any .js files with the same name as a .coffee file though will take precedence over it.\n. My apologies, this one actually seems to drag and drop.\n. Good stuff Sven, no idea why it didn't occur to me to use npm start to start the server!\n. I like it.  Some part of me thinks that the local storage \"bundle\" shouldn't just be for one federated wiki site, but for all of your local storage edits, along the lines of treating your machines local storage as another server.  This however would be fraught with technical problems (cross domain local storage through iframes?), and could easily lead to people publishing things they didn't intend to.\n. The unhosted stuff looks really interesting.  I hadn't thought of turning it into a CORS problem so we don't have to muck around with circumventing the same domain lock on local storage.\nNice, the modularity of the plugins is really coming along.\n. I think Adam has a good point, it would also allow us to review some of the great discussions in the future, and possibly release short clips from the discussion as public youtube videos if we wanted.\nSince we were talking about it this week I upgraded the rebase bookmarklet, https://gist.github.com/3669857\n. I think the best option right now is to hop on the Grunt bandwagon.  It is an automated build tool with a lot of features that may be complex, but in return it offers a whole lot of convenience. The biggest reason to go this way is that there are a lot of heavy front end projects going this way, which helps a lot with having developers that are familiar with the tool. It should also be easy to put a couple of commands in the readme for people who don't want to get involved with the build process to make it work.\nGrunt can be found here: http://gruntjs.com/\nA good example of a grunt file doing similar stuff to what we would want, by some friends of mine:\nhttps://github.com/wearefractal/boiler/blob/master/build/grunt.coffee\n. So I sat down to work on this and grew more and more anxious that we were going to make our problem worse switching to grunt as a build process without first solving some of our redundancy and scatteredness.  Especially with the multiple build systems/test frameworks. I settled on the temporary measure of adding the plugins to the builder.pl script #311.  I'm not sure what a clean build process for this complex project would look like, so I'm looking for ideas on how to do this right (with or without grunt).\n. Thanks. Everything felt pretty good.\nLots of stuff changed name spaces, or went from several possible references (local variable, property on wiki object and property on a module) down to one. I attempted to check things, but it would have been very easy to miss one of those renames.\n. Let me know if you need anything working on this, I'd be glad to help.\n. Ok, I'll fix this real quick then.\n. Added the ability to specify options in a default config file, specified config file, or by wiki_ env var.\n. Thanks, exactly what I was looking for. Got it sorted out for now, but this is starting to look in sore need of some refactoring.\n. Nope, just an npm update.\nCool, I'll go ahead and pull those out. I think that we can just pass around a reference to the server and then emit events out of it for plugins to listen to.\n. You added a bunch of .coffee~ files in this.\n. The node server uses bouncy in front to route requests, and for some reason bouncy has caught on node v0.8.x. As far as I know the server should work in node 0.10.x when not using the farm.\nConsidering better ways to route the incoming requests, if anyone has any ideas. I want to keep the clean separation such that servers can be spun up and managed by an outside force, and the automatic farming is just one possible way to do that.\n. Awesome. I knew he'd be working on it, didn't notice it was fixed!\n. Working on refactoring so that modules/tests don't rely on globals, and can be tested individually and with testling ci.\n. Nice, I like how this is shaping up!\n. Awesome guys. I'm super excited to see this land soon. I've been off in the woods camping with JavaScript nerds for a while, will hopefully get to really dig into this on Wednesday.\n. Absolutely Christian, thanks for your grunt expertise.\nThanks Paul, glad to hear that. I was hoping it would replace those two and builder.pl, but wanted to make sure before I removed them. \nLet us know if we missed anything Ward.\n. I think pagesInDom above or locsInDom below make a good argument for dataDash operating on arrays of elements, it makes for a natural interface when most selectors return more then one item anyway.  It does have the odd problem of requiring you to get single items out of the array when you want just one, however you have the ability to do powerful things to ranges of selectors, and not worry what happens if there are different elements in the dom then you expected, you can just always get the first or last item, even if you get more then you expected you get a sane result.\n. Maybe switch to mikeal/request to make this section easier to follow?\n. This is kind of a crazy line :)\n. How about req.isAuthenticated() and req.session.email ?\n. ",
    "notbenh": "Thanks Ward for the very detailed reply. I think that it's very\ninteresting move toward a js only core. This would make it insanely\nportable. Is this the intent or is it just a happy accident? Though if\nthat's the direction, then I wonder why having Sinatra on the backend\nat all? It seems like overkill if the intent is to just ship a single\npage app, wouldn't that be doable by just a simple flat html file?\nThen if I understand thing correctly, with that move then the app\nwould then use local storage rather then the JSON file scheme that is\ncurrently set up with Sinatra. Then the wiki cross talk would be done\nvia JSON packed snapshots from local store. All possible via JS, it\njust needs to get to the browser.\nDoes it sound like I have a good understanding on where you want to\nsee SFW heading? Either way this is a great excuse for me to play with\ncoffeescript.\n. ",
    "BryanDonovan": "p.s., I've created another branch where I've added a subdirectory named 'sfw' under server/, and namespaced all the classes with 'Sfw::'.  See https://github.com/BryanDonovan/Smallest-Federated-Wiki/tree/sfw-namespace/server\nLet me know if you think I should proceed with that direction.  It's more typing, but I think it clarifies what is home-grown and what is a 3rd party library, and might help prevent future namespace collisions.\n. ",
    "ngzax": "Ward-\nThanks for starting this... its really interesting. Just to clarify: there is currently no syntax to define an external link to a federated page so that you can see them in the \"Miller columns\" view, right?\nI was able to drag a page from one federated wiki to another using Chrome, but all I got in the target page was the text of the initial page. It had no link knowledge of where it came from. Is that the proper behavior?\nThanks,\nDaryl \n. Ok, thanks, Ward. I was able to create the paragraph of type \"federatedWiki\" but it only displays a portion of the included page and when its clicked I a new \"page\" appears to the right, but it is blank.\nI am really excited and looking forward to see where this project goes, thanks again!\n. Ward- \nYes, I am using Sinatra. Using your example link, I now have the remote wiki linking working and it doesn't come up with a blank page. \nThanks again for the help!\nDaryl\n. No apologies necessary. This is an exciting project that I will be watching closely. I intend to set up a \"federation\" with my programming Team once it matures and stabilizes a bit more and will continue to give you feedback on my progress.\n. ",
    "KyleAMathews": "Sounds perfect!\nIt makes sense as well to do as I'm doing on my GTD app and group results by server. The screengrab below shows a sample search. We could do the same here but group by server with the origin server always first.\nAnother part of my design that might have application here is the metadata I'm adding to the results. The yellow stuff are tags and the green are projects. I'm not familiar enough with this project to know but there might be metadata which would be helpful to add to the results.\n\n. Sounds great. When I have some free time again, I'll put together a little prototype to see how things come together.\nOne quick question -- is there a list anywhere of publicly available servers? That'd be helpful for testing this.\n. This little thing called paid work has been getting in the way :) Nasty business, making money.\nThere are two things I could use help with.\nOne, how to get and maintain a record of what pages are available on each server the client is connected to. You mentioned above the possibility of creating a url that would return that information. That'd work great as would converting to using Backbone and creating on page load models for each wiki page. Using Backbone would be especially ideal as the autocomplete widget could just watch for events on the page collection and keep itself updated that way.\nTwo, is how to open a page once someone selects it from the autocomplete widget. I'm assuming there's a function of some sort that's called?\n. @WardCunningham looks good!\n@asolove I don't see any reason not to load everything. Most pages are quite small so loading everything in one JSON object wouldn't take that long and the interface feels much more responsive when the wiki pages load instantly vs. having to wait for a roundtrip to the server. Also, the code is a lot simpler when you're not doing lazy loading.\n. @asolove fair enough.\n. @WardCunningham My Saturday is looking nice and empty so I'm going to tackle getting this built. Has anything changed since https://github.com/WardCunningham/Smallest-Federated-Wiki/issues/70#issuecomment-3610219 that'll affect what I need to do?\n. Didn't get as far as I hoped Saturday. But I did get the development environment setup and read through the code so hopefully later this week I can jump on this again.\n. Nope, the code is pretty straightforward and it looks like it won't be too bad to do as you suggest and fetch the slugs.json file from each server you connect to and store that locally.\nThings would definitely be easier with Backbone -- is anyone actively working on that? Backbone is a perfect fit for SFW so the transition would be smooth.\n. Ah, found it -- the backbone stuff is all under the touch directory. I knew you'd said you were working on Backbone-ifying things but couldn't find anything before when I went looking.\nCool, I'll take a gander through your code then.\n. One option for rebuilding the frontend is http://brunch.io/\nIt's a integrated toolset for building rich HTML5 frontends. I've been using it for 4-5 months now and really like it. It makes it really easy to cleanly organize your code and includes integrated js testing via Mocha and minifying of js/css.\n. I've used it for ~5 months with zero problems. So stability is fine. There's 14 contributors / 54 forks / 637 watchers at the moment so that's a decently wide base.\nI haven't made the jump to making changes to the Brunch code itself as it's met my needs perfectly so far but my understanding is the core Brunch code is fairly slender + it's written in coffeescript so it'd be pretty easy for this community to modify. That and the 1.0 release has moved to a plugin architecture so it'd be easy to add custom build steps for SFW. \n. Twitter is actually getting rid of their hash-bang urls - http://ngriffith.com/2012/02/20/hash-banging-the-intertubes/ arguing that browsers that still don't support PushState can just reload each page.\nI think it'd be better to just go straight to PushState. Hash-bang urls are quite ugly and un-webby. It's not too hard to have the server be able to create a simple html page for the robots that then Javascript decorates for the humans after loading.\nSee http://danwebb.net/2011/5/28/it-is-about-the-hashbangs for more discussion on Hash-Bang vs. PushState\n. Fair enough. Clicking outside the edit field does work. Missed that before.\n. Also, for future reference, I've built a nice auto-expanding editing widget for my GTD app I could contribute when it comes to that point. It's based on the ideas/code from this article - http://www.alistapart.com/articles/expanding-text-areas-made-elegant/\n. What do you mean by \"farm\" mode? Do mean creating a cluster of Express server instances?\n. ",
    "samrose": "Thanks Ward, permissions fixed it indeed.\n. ",
    "martypdx": "Yes, exactly. I've got it working in concept, I just need to extract out the random color/angle logic from the favicon.coffee Nick created, then have a favicon.png.coffee and a favicon.svg.coffee with some logic to select which to use to create.\nCan we put the favicon color data in the status local-identity.json? Seems logical but I dunno.\nI did the template in jade because I have a designer that makes it easy to test, but we can switch it to haml if you like (assuming haml supports arbitrary tags).\nI need to do some more testing, but it looks we could just ignore the file extension on the web requests and ask for http://mysite/favicon and then it sends the mime type and the browser figures it out. There's a lot of references to favicon.png scatter all over the place that would need to be cleaned up.\nBTW - One other options is to have the favicon on the wiki pages be a div tag and set it's style via css and it could be either png image or css defined gradient. This might be cleaner anyway so we can use a css class rather than have all those hard-coded favicon.png's floating around.\nI'll check something into my branch soon and you can take a look...\n. So the server get for /favicon returns the image in /status, but is totally decouple from favicon image creation and doesn't know/care how it got there? \n. You might want to grab something like http://jscolor.com/ and let people choose there own colors.\nHere's a library for creating .png from canvas: http://www.nihilogic.dk/labs/canvas2image/ (you may not even need that, just use canvas.toDataURL() and post that in a json object to the server)\nThe approach in the existing code doesn't take advantage of the built in gradient methods. Much easier to just do:\nvar canvas = $('#myCanvas')[0];\nconsole.log(canvas)\nvar context = canvas.getContext('2d');\nvar gradient = context.createLinearGradient(0,0,32,32); //control direction here.\ngradient.addColorStop(0,   'yellow'); //name, hex, rgb, rgba - all ok \ngradient.addColorStop(1,   'red'); \ncontext.fillStyle = gradient\ncontext.fillRect(0, 0, 32, 32);\n. ",
    "GerryG": "Very good conclusions, Ward.  For SFW, putting development convenience first makes a lot of sense as long as you don't loose sight of secure deployment issues in the design.  In other words, it is fine to have a \"lax security mode\" of exchanging plugin code with your development network, but there is no way I will be allowed to deploy code this way in production mode.\nSFW isn't in production mode, and it may never be run that way, but I fully expect that related derivatives will be, and they will need a secure way to share plugins.\nIn the end, this is mainly an issue of social engineering and process architectures.  The difference between development and production is how the code is \"blessed\".   The way we talk about this with respect to Wagn is to introduce the idea of conformance to an API or specification and that will probably include both passing acceptance tests and code review (by the development communities).  As is common practice in open source communities, there would be common code repositories and different levels of review and testing are indicated by promotion to different repositories.  The communities define the terms used to describe the level of confidence, \"stable\", \"development\", \"production\", \"alpha\", \"beta\"; the labels are totally arbitrary, but they make it easy to know what code you should be looking at when.\n. Is this still happening?  I guess that would have been a couple hours ago if it happened?\n. Hey, the hangout link seems to no longer work.  Anything new on this?\n. The link takes me back to this page: https://plus.google.com/hangouts and I can't figure out how I can still get to the hangout.\n. The link that seems to work has this extra underscore in it.  Not sure where that came from in the first place.\nhttps://plus.google.com/hangouts/_/extras/talk.google.com/fedwiki\n. Now it seems to be broken altogether.  Anyone else having the same problems?\n. Wish I had the time to join you on these chats.  I'm now working at Tribune Technologies (since last summer).  Trying to get back into Wagn work too, working on creating 'live-edit' views and maybe using Dojo's \"coweb\" framework for synchronization: http://dojofoundation.org/projects/opencoweb \n. Followed the link on GUID's and I'm thinking that this page is more important for wider standards compliance: http://en.wikipedia.org/wiki/Universally_unique_identifier\nThere are a number of 'versions' that could be used productively in the SFW context.\n. I think it woud be productive to add a version that can generate guaranteed unique id's in our context.  If you segment the UUID into bits that uniquely identify the origin (identified by a base URL whose binding can be changes) and the data item (identified by an Id that is controlled by the origin, so card_id for a Wagn for example).  Then all we need is a federation way to generate and register origin UUIDs.\nI suggest that we keep this origin UUID thing decentrallized.\n. Wagn uses [[<link>|<link text]] and <link> can be either an internal link (card name) or  URI.  Early Wagn had [URI][<link text>] maybe just for external, but I think a unified form works best.  I think this is borrowed from somewhere else.  Let's try to use something that's already invented :)\n. I can offer a number of promising directions to look for something a bit more \"ready to deploy\" as well as experiment/build with.\nI work on Wagn (http://wagn.org/ and at github) which is a fully developed Wiki with  advanced features that make issue tracking type apps pretty straightforward to implement.  As for trackability, I have something I'm working on as a plug-in to wagn (we call them \"Packs\" in keeping with our model of \"Cards\", our objects that then compose pages).  It is part of the Metacurrency project.  I had it working on a much earlier version of Wagn and we have been making Wagn more modular and creating the Pack API with extensions like this in mind as we build.  Long story short, I'm almost back around to having my Pack with the new API.\nWhat that pack does is create a cyptologically signed audit trail of \"Metacurrency Flows\", in other words a sequence of signed transactions.  Metacurrency will also involve an open language to declare the transactional rules of a currency system.  So the idea is that you would declare the currencies and flows of issue tracking, and any subscriber to these flows can reconstruct the \"state of play\" in the currency system.  That is count and display open, closed, in process issues as well as tracking and analyzing all of this through time.\n. And, I agree with your motivations for looking at SFW, I'm here because I want to be able to connect Wagn as a Federated Wiki, and share content as this network develops.\nI want to make sure that when Wagns Federate in this sense they get the maximum benefit of the fact that they share a lot of structure and convention, but I also want other Federated implementations to optionally share this structure and convention as well if they can interpret the features.\n. Well, now Ward has merged your code, so it shows the commit in his repo.  I was wondering what it looked like.\n. The real issue here is playing well with other apps in URL space, and there are some key considerations for that.\n1)  Don't use  more of the URL space than absolutely necessary.  Your move of functional stuff to one base url '/system' would serve that.  In many application frameworks, each \"controller\" gets a name in the base URL space, and Wagn originally had several controller prefixes.  We have moved towards a unified namespace where: /mount_point/card_name.format is used with a resource API (REST).\n2)  Did I mention REST?  That makes things easier for remote apps as well as rich clients.\nAll the important work is in how things are externally named.  User friendly names can be changed, so references have to understand that.  With Wagn, we haven't had to address that yet because we haven't worked on \"WagNet\" much at all.  Wagn updates all the internal references when a card is renamed.  We would either use the \"card_id\" which is stable on each Wagn, so you would just identify the Wagn instance, and that plus the card_id would identify the card.  SFW might just require that a participating Wiki have one of these, not how the id's are constructed.\nOr you could go the way of tracking the name changes and having the reference include a timestamp or version identifier so it could find it in any case and even fix the reference with the new name.\n. In this way of thinking, things like 'changes' would be a view, and the .json extension would request the SFW APIs in most cases.\n. I think that's what he's asking.  What are the functions of an \"origin\" server, obviously that is more than just serving the json.\nCorrect me if this is wrong, but I would think serving the json is what is necessary to drag content from your page to a \"factory\", which is how you take content into SFW.\nIt would be really helpful to have a clear picture of some typical \"life-cycles\" of a page as it gets dragged and dropped, changed and then have those changes incorporated back to the original page.  I'm getting that this isn't totally defined, and some of it is intentionally open until we see what needs show up.  When does it get identities of different kinds (page names, \"slug\"s and so forth)?  What is tracked in the journal and how?\n. Note a side benefit from having me make Wagn::Cardname into a more general gem:  we already have internationalization in our work queue, and when we do this, we will certainly be making the changes in this new gem, or the Cardname class if we haven't made a gem yet.  For example, plural/singular is part of the name folding and that clearly needs to be more general that what we have so far.\nIt almost requires the objects (Pages in SFW, Cards for Wagn) have full alias capability in the naming library.\n. I extracted it to a new repo and took out the Wagny parts.  I'll add test cases and such as I have some more time.\nhttps://github.com/GerryG/namelogic/blob/master/lib/name_logic.rb\nSuggestions about what to name it or its methods or components are welcome.\n. Now it has some tests.  They are all taken directly from Wagn tests, and I haven't looked at how good the coverage is yet.\nhttps://github.com/GerryG/namelogic/commit/f640c695110164ecd49fc41fc95b9642a0a7969d\n. Oh, I think the point I've been dancing around be not stating clearly is about keys and equivalence classes. It is perfectly ok if different Wikis in the federation have a different representation of the key (slug), but the way the namespace is structured in terms of equivalence classes has to be mappable. For all names that map to the same key, they must map to the same key in all of the wikis. I'm pretty sure this will always mean that if you take the key from a foreign system, and convert it to a key in your system, and then reverse that process and re-encode your key into the foreign system, it will end up on the same key.\nIn fact, I think the latter is a lessor condition that may be enough for good interoperability. If I were a better mathematician I might be able to prove they are the same condition.\n. Now I have to find some time to document this and create a configuration variation that matches SFW rather than Wagn.\n. Do you have a preference for a name segment separator for SFW?  '/' is out since you already use that.  A double character like '--' seems like a reasonable options, but I don't like it that much.\nEven if you don't think you'll need that, I'd reserve a syntactic character for it now.\n. What I like for keeping histories, is just to define a flow protocol for the changes, then any number of bots and engines can filter and record all or part of the history as needed to validate and track the data that is 'of interest'.  No server has to make a contract about keeping intact histories or any of that, all we want is to make sure that the history goes out in a transparent and complete way to the parties creating this history.\nThis is in keeping with Ward's stated philosophy on SFW, to keep and hold data by virtue of having copied it and linked it into a larger structure.  If it is useful for the origin servers to serve out all or part of this history, we want to support that, and also support having all this history happen in other servers that are also monitoring the transactions flows.  This is exactly the same kind of model we are talking about for Metacurrency protocols.\n. Read that, but I'm still not getting the node packages correctly.  Or is it actually there, I could have missed it.\n. D'oh, just about the first thing.  I must have jumped past it.\n. Really easy.  I was sort of tripped up by having already done node and npm for etherpad-lite a while ago.  That's why I missed the npm install command.\nI think formatting can help, make each important step a bullet point or subsection heading so sub-steps are harder to miss.\n. I found the file right off, finding the details in the file is where I failed.\n. There are a lot of rspec tutorials, but I suspect the tricky part here is the selenium stuff that runs against a javascript simulation engine, often a browser driven by the test libraries.  They are pretty easy to run once you have the tools installed.\n. I was looking around for diff tools, and noticed this: http://richardbondi.net/blog/javascript-diff-combines-scripts-to-fill-the-gap/  which takes two diff libraries, one for line diff and another for word-diff so they work together.\nI also was looking into what Wagn does for diff, and it actually has custom ruby code for html based diff: https://github.com/GerryG/wagn/blob/master/lib/diff.rb\n. On the topic of (a), I really think it needs some deeper thought to get this right.  You really do want to be able to change names, but that is clearly a trickier issue in a Federated Wiki.  Wagn has a powerful rename that fixes all the references, and Wagn has a lot of them, but that isn't going to work in the Federated situation because you can't possible fix all of the external references.  You might not be able to enumerate them, so you certainly won't be able to update all of them.\nI'm starting to think through the idea that a title is content just like any other content.  This kind of thinking means the chunks of content, whether paragraphs of titles, heading and whatnot, need to have some sort independent identity, something link purple numbers.  Then you could create a purple number for when the original title is created.  External references could be by purple number,  so changing the name of a page would not require updating any references.\nI think we maybe want a bit of formalism around \"binding content\" where you either create a new name (and its purple number) and bind content to it (a list of paragraphs, which could be formally the list of purple numbers it contains), or you rebind an existing name (purple number) to an updated list of paragraphs.\nNow we can begin to consider change histories a little differently.  Mapping onto the git terminology, any changes to bindings, are like \"tree\" changes, and changes to the paragraphs and names are changes to blob objects bound to the trees.\n. Does 'slug' actually mean something?  If not, maybe you should just call it a key?\nWagn does exactly the same thing with cardnames, we map all names to a key, so each key represents an equivalence class.\nI'd like to change the to_slug code so that it is more in line with Wagn keys.  There are a couple of things that are different.  We fold all non-key characters into the space equivalent and strip and compress them.  You just strip the non-key characters.  I think it is better to leave a space.  We also do underscore processing, the reverse of camelcase so that MyName maps to my_name, so My_Name, my_name, MyName are all the same, while you would have MyName the same as myname.  I have a pretty strong preference for the former not just because it is the way Wagn does it.\nWagn also singularizes, but that is much more of a judgement call.  I thought we could generalize that idea to the idea of \"without inflections\" which could have a multi-lingual equivalent processing.  This is all complex to get right, so probably not wanted for SFW.\n. But fundamentally the \"giving a purple number to a name\" is an important concept.  I think it is worth exploring how adopting such an idea would help or hurt SFW.  I see a lot of good things coming from the idea of tracking changes from paragraph identifiers that are not permanently tied to the name and content.  Calling them purple number is optional, but I like the idea of using ideas that are well explored and extending them as needed rather than inventing something new.\nIn terms of the redirect issue, loops and other nastiness.  I would not put alias links in the content, I would make them bindings and require that they map to content, not make them 'content that is a reference'.  Wagn already has a Pointer card type for references, but I would never try to use that for namespace things like aliases and equivalences.  We are already thinking about that for multi-lingual support and generalizing the 'inflections' concept.  Base characters with accent marks can be handled much like case folding, but irregular plurals and other morphological variations may require \"extra index entries to the same name\".\nWhat I'm really talking about here is \"hard links\" vs. \"soft links\".  I think we want hard links.\n. That is worth looking about for.  I'm not aware of anything, and I will see if I can find anything.\nDon't you worry about collisions given you will have so many identical slugs?  I guess I'm thinking that an id that is completely independent of the name has some advantages, but it would be similarly duplicated in a move to a new instance, so maybe it isn't really a different problem.\n. I have more work to do before I can suggest moving the the code I split out of Wagn, but it does already have a lot of properties that you want.  Having only single internal \"space equivalent\" is there.\nYou also want to think about a couple of other classes of character.  We have two characters that are in cardnames and become part of the key, '*' and '+'.  The first we use in the initial position to indicate a \"system card\" and keep them separate from the content cards in the namespace.  '+' is our \"name segment\" character.  Then there is another handful of characters that are just banned in names so they can be used in URL syntax outside of names.  Github is right now crashing my browser tabs on my repo right now, so I can't look this up in the code.  There are like four of them, I think '~', '.', '/' and one more maybe.  You probably want to ban these as well for similar reasons.\n. The -- convention, or I would recommend at least two characters that are not word characters, but are carried in the key (slug).  One for a name separator (/ or -- for you, + for Wagn).  I think it is best to ban '/' so it can be used at the next higher semantic level above.\nAnd I totally agree about guessability.   I think, though, when you are folding the different forms of a word into one key, that almost necessarily means a one to many relationship of keys to objects (pages, cards, sub-pages, whatever the objects being indexed are in the model).  Maybe that's disambiguation pages, but more likely it is contextual.\nIn Wagn, take for example the key 'tag', you may want that to be a cardtype where the name is \"Tag\", and  you may also have cards, \"+*tags\".  Tag and foo+tags both reference the same card in the Wagn world (foo+tags has three cards, foo, tags and foo+tags are each a card), but the context is different and it should be able hold onto a different inflection or capitalization, etc. in each context.  It isn't a use case for disambiguation.\n. I've done a little exploring on the issue of diacriitcals and such.  This link was useful: http://ahinea.com/en/tech/accented-translate.html\nI gather it is straightforward to \"decompose\" such characters into the base character and the accent mark as a second \"combining character\".  Then you would just filter the marks out of the key.  You would want to delete it, and not make it inter a whitespace equivalent as Wagn does with most special characters.  I think SFW just deletes them already.\n. For ruby: http://www.jroller.com/obie/tags/unicode\ndef to_ascii_iconv\n    converter = Iconv.new('ASCII//IGNORE//TRANSLIT', 'UTF-8') \n    converter.iconv(self).unpack('U_').select{ |cp| cp < 127 }.pack('U_')\n  end\n. Good idea.  Looking at the comments here, the part that actually belongs here relates to sub-page structure, and how sub-page elements will be identified and tracked (roughly equivalent to having purple numbers).  I think that is key to being able to track where things move as apposed to how things change, something like the blob/tree change objects in git.\n. That's what I was thinking.  This is client side, so the browser has to know that it is set for not retrieving images, so when you get a missing icon, you have to test that global state as well.\nIt seems odd that the status code would be 404 when the browser never fetched an image.  This could also be highly browser dependent, so you'll have to be sure to test many of them as well.  \nThe browser should never have sent a request in this case, right?  The setting to not fetch images should not send the request at all.  Many browsers do indicate that images were not fetched with the same icon as 404, so internally they may treat it as if the request failed with 404, but it seems to me that you would have to be able to tell the difference in javascript.\n. Too bad they hold you in-communicado when in the courthouse.  Can you use a portable computer?  I think Ethan is coding today while traveling to Portland.  Git works great disconnected :)\n. There was talk of a command line option for that.  I think that was the Sinatra server though, and that the feature was missing in the express server.\nKyle has the description of the behavior correct.  It's supposed to create server instances automatically.\n. Hmmm, the server side does seem to be the place you need to fix this.  I'm not sure the client can really know if two domain names are on the same server.  It could compare IP addresses, but that still isn't a guarantee, since a server may be handling many IP addresses.\nRemind me why we care about running on thin server?\n. I'm not sure I understand, Ward.  What would \"the user clearly has ownership\" mean?  It has to mean the possession of some sort of authentication token, typically a login or session.\nShare a secret isn't exactly the protocol.  The server needs to know who to trust, and it can validate you by knowing a couple of public facts.  Using the identity of the user and trusted public key registrars, it can establish a shared secret between the server an whoever possesses the private key for the public identity involved.\nHow does the server find out the identity of trusted a trusted user?  The command-line OPEN_ID pattern seems like one reasonable pattern, but you could also store the identity in the claim operation, now it's in the database, right?\n. Not to lean too heavily on this metaphor, but git practice has a distinct fork and clone that are related.  Fork is a higher level operation of the github (or similar) repository.\nWhether or not you can limit the scope of an SFW fork to less that a whole site, you do want it to be more than one page.  I'm sure we will want to treat sets of changes to a group of pages as units similar to a commit.  We will want that to effectively handle merges, automated and otherwise.\n. Making a clone is just making a copy, then you can introduce changes into copies (with the additional source control layer of branches) and synchronize those between the clones.  This operation is all about moving the data around, but not much about merging of different lines of change except in the sense of organizing ones own work (on branches).\nForking is a paradigm where you are (potentially) creating a distinct line of change that may or may not eventually get merged back to the source.  The pull request is how this folds back in, and an analogous action in SFW would probably be a big help in all of this.  It would also be a potential unit of change for diffs and histories.  Histories no longer need to be represented linearly either, we could have network graphs just like github.\n. You may also find something like a branch to be useful.  The idea of distinct lines of change organized in a single repository is very important.  With this feature, I can keep distinct lines of change separated within a single server.  You may represent these as different instances, factory-fashion, for API and access.\nI'm thinking about the dynamics of using git and how you do merging in two contexts, one is 'local' in the sense that you are merging versions of things that are all present in the clone.  The other context is the pull request, and this is external in the sense that we expect that the originator of the request doesn't have update permission to the target.  Even if he does, there is likely some form of role separation so that the pull requests become available to the larger community.  In other words, Ward might post a pull request to his own SFW repo just so that the rest of us can comment.  Or Ward might delegate his authority to Nick, but Nick still posts pull requests for the same reason.\nAt the lower levels of data/change models, they are composed of the same APIs and update mechanisms, but there is a higher level that we represent through a UI.  I think that just like git defines a bare (metal) api, and you can attach different user tools (porcelain), that SFW should do the same.\nThat will also make it easier for people to contribute tools at different levels.  Someone could create some modules to clone and fork existing SFW content sets that connects to the factory features (through some plugins, no-doubt), and that can develop in complementary cycles with other work around the UI for viewing changes and doing merges.\n. I really like where this discussion is heading.  I just started a new contract, and don't have as much time for my OS work.\nI can share this etherpad page where we are working out the future of Wagn URLs and the idea of multiple \"namespaces\" in Wagn.  This is a completely different take from SFW on this part of the problem (names), but I think this discussion is about something more general.  http://etherpad.openstewardship.net/wagn-names\nWhen I have this namespace feature in Wagn, I would probably want to implement the returning bundles not by pushing them to another domain, but with this namespace feature.  In other words, I can create a namespace independent from and attached to my main namespace and take the bundles back there.\nThe posted edit bundles could be much like a pull-request.  As with a pull request,  you don't have to save the whole bundle, just a pointer to the source commit in the source repo.  After the changes are included, we would have the same kind of network graph of commits that github displays as the \"Network\" feature.\nThe pull-requests, the posting of bundles of edits, are not really represented in the change graph because the bundles, like a pull-request, are composed of many separate edits and saves.  The low-level change tracking code is tracking those edits, saves and merges, and the people, not the system are resolving the higher level issues of what changes to take or not and who they come from (organizationally and personally).  A bundle of edits may come from a whole team of editors and they may have posted edits to the \"lead editor\" who posts the whole bundle.  The low level change tracking will record the minute detail, but it is agnostic to the social processes that generate the bundles.\nMy main point is that the higher level events (bundles of edits or pull requests) are almost purely documentation, providing human indexing and pointers to navigate the lower levels.  The lower level substrate makes the referencing reliable and helps us with the human level tasks of managing change.\n. I'll try to make the next one.  I was on a contract for a bit, did a bit of RoR from scratch generation and Extjs javascript on that, and I've been working on Wagn Json rendering and widgets a bit.  Using jstree and jsgrid (jquery plugins) More here: http://wagn.org/Json_and_Xml_Renderer.\n. ",
    "dvberkel": "I would like to join in but I am at UTC+1. Seeing that I am only an aspiring coder you should go ahead with 10pm pacific.\n. I was experiencing some problems with server/express. I will check out your branch to see if I my problems are resolved by your commits. Otherwise I will open another issue.\n. One of the problems I was experiencing was that the data directory was searched in the wrong place, i.e. on directory higher then the default. That is fixed.\nBut the express server still does not serve the wiki. Is this to be expected?\n. That works fine indeed. I was executing the bin/server command from the server/express directory. This serves up one miller column without the welcome message.\nShould the README.md be changed to instruct people to execute from the server/express/bin directory?\n. Ok, thanks. Have a nice day at work\n. I have a question regarding the cloning of wiki in relation to the plugins available on the original wiki. Will the plugins be cloned as well?\nWhen you are federating a wiki on a specific subject e.g electrical engineering or mathematics,  it is not unlikely that you will develop a custom plugin to render some data relating to the subject. In a sense the wiki relies on the plugin. When I clone your wiki I would like to benefit from the plugin you created.\nAre plugins cloned automatically? Is it a mechanism which should receive some attention in this discussion?\n. Both suggestions are interesting albeit a bit technical. I am not sure that all intended user that would want to host their own wiki are up to the task of configuring the plugins in the suggested ways.\nLike you said, we have to be alert on the possibility of spreading ill-intent viruses (I think that virus memes should be encouraged), but I think it should possible to manage plugins without a deep understanding of any technology like google-caja or git.\nIdeally one would enable users to manage plugins (and maybe more) from within the wiki. I am thinking out load, when a wiki is cloned all \"custom\" plugins will be cloned as well, but signaled to be dirty. For example be prefixing the file-name with \"dirty-\".  We then could add a menu only visible for the owner of the wiki with a command to clean up dirty plugins.\n. Your example is clear and demonstrates an important point (for me) the central idea of a federated wiki, i.e. you own the data. In particular when you clone a page and the original goes away, you are still presented with a stub which can be expanded to reflect your idea.\nBut a notification what is going on, as described in the predicted results page would be welcome\n. Great\n. I made a mistake in the description. Horizontal scrolling is working just as you described. It is the side-scrolling that is not working. Just as you mentioned in your second point. \n. the README does not document a function prettyPrintOne. This function accepts a piece of code and returns the decorated code, which is amendable for colorization via css.\nIt is this function that I used in the emit. I.e.\nemit: (div, item) -> div.append \"<pre class='prettyprint'>#{prettyPrintOne(item.text)}</pre>\"\nI am pretty confident that this will work correctly when the code is first encounter. (Or else I have an opportunity to learn an intricate detail of the plugin system.)\nTake your time to review the code. In the mean time I will look into the Mathjax plugin.\n. I met a busy schedule the past few weeks so I have not found time to work on this. I will see if I can squeeze in some time to finish the code plugin because there is not a lot that have to be done. \n. @WardCunningham I am sorry that I could not find the time to work on it some more.\n. ",
    "hallahan": "So the hangout is the same time but on IRC instead? Bummer google messed up a good thing.\n. Thank you, that makes sense. This should be a good start for little plug-in experiments. Once I get more of a feel for things, I'll be down for doing more pertinent things along the lines of the general plan. \nBeing light on buttons at this point sounds wise to me. \nI have set up a NodeJS Federated wiki at: http://theoutpost.io/\nI have been using Rackspace; the server is far from reliable. I am considering moving to something a bit more managed like Joyent. It is frustrating dealing with these cloud services, because you never truly have control. I also will be setting up a server from home perhaps with a Dynamic DNS service, but Comcast's upstream bandwidth is not very good. I also may attempt to convince my university to let me plug in a box in their network. Upstream bandwidth limitation is an irritating issue to deal with when trying to take a decentralized approach to serving data.\nOn Feb 13, 2012, at 5:05 PM, Ward Cunningham wrote:\n\nThe rendering of JSON into DOM objects is largely handled by the emit: methods of plugins. For something simple like a paragraph or image this is handed off to jQuery without further templates. For complex things like equation formatting or data visualization a plugin is free to load more packages and let them have their way with the DOM.\nThere are a few exceptions. \nThe case you mention, the [+] tacked on to each page, is constructed as part of footerElement in the client function buildPage.\nA simplified footer for a simplified page can be generated server-side. This supports robots and curmudgeons that won't run javascript. The simplified footer doesn't include the [+] because there is no javaScript to handle it.\nRegarding controls in general.\nWe're anticipating some generalized button bar scheme that would be developed such that users could move smoothly between touch devices and mouse devices and find the sort of affordances they would expect on each platform. We've been intentionally light on controls so as to not build much of a legacy in advance of that design.\nSuggestions and experiments are welcome.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/issues/129#issuecomment-3953053\n. I have added a Hosting and Installation Guide for the wiki. Hopefully people find it useful!\n. I would like to clarify what i meant with issue #179 dealing with reverting to a past state. Clicking on a button in the history panel would only change state in the client's view--not the server or local storage. Thinking about git's workflow, it is common for someone to check out a tag or some commit that is not the head of a branch. Then, the user will create a new branch from that state and make modifications based on that. Of course this is too complex for a use case outside of the domain of coding.\n\nSo, instead, we could have something with the same effect without creating a new branch.  We could just get to a past state and make changes. Those changes will just be the latest entry in the journal. \nI agree that showing a dif would be useful, but it still does not enable the user the ability to make edits based on a former state. Perhaps mouse-overs could show a diff instead of just highlighting what changed? Then, a click would revert state.\nAnother option would be that clicking on a history button will show a dif, and double clicking on that dif will allow you to edit based on the green, later dif entity. I think that this option could be confusing for the user, however.\nI think that this issue is worth working on, because showing a history at the bottom of the page implies that the user may view and control the single branch of states through time.\n. I have implemented the date feature for actions submitted to the journal as requested. This will be a good piece of info for a mouse over. In addition, I think it can be 1 of 2 things we can do to solve the ambiguity of the journal UI.\nFirstly, the date and time should be shown either on the top or the bottom of the story. This will give an indicator that we may be viewing a state that is not the latest.\nSecond, the buttons in the journal should be clickable. The active state should be indented to show that it has been pressed. This way one can casually click through different states to see how things have changed through time.\nYour suggestion would then make sense to save in local storage the button that has been clicked last. By treating this widget as a panel of buttons, we could then select difs between any pair of states by using the shift key to have two states simultaneously selected. Also, making this work for a mobile device wouldn't be hard. We could just have another button for dif mode. Then, you can just tap two buttons in the timeline to get the comparison.\nI'll implement some of this and submit a pull request. I like your concept of showing history with a widget like this, because it takes up very little screen real estate. Once the letters are replaced with icons, I think this will be good stuff. All other representations of history and version control I have seen so far are usually a long row in a table of some form--usually requiring an entire view just for that functionality.\n. I like this bookmarklet approach, and I agree, having a button in the bottom panel makes sense. Perhaps one could include a text box to input your home url?\nI am not clear on the original method to fork pages from one server to another. Is this done only manually now, or is there some way for the user to take an entire view from one server and put it on his own with the [f] added?\n. Let me know if you would like me to refactor things around changes made. nrn is splitting up client.coffee into several files, right?\n. Sorry about that, I forgot to put in gitignore. I will switch to the getTime method, I didn't know about it. \nThe reason why I want to have the date/time on the top of every page is specific. With this in place, pressing buttons in the history will move the page to versions in the past. We need a label to show the time of what is there so that we can tell which revision is in the view. The button widget will be an indicator, but showing the time will also help make things obvious. I am also thinking some sort of color indicator with the font might be a good idea to show that we are viewing something that is not the latest revision. Also, consider how often wikis (i.e. wikipedia) are cited. In a citation, the date is usually needed, and so it should be seen up front on a publication.\n. Also, when I attempt to use builder.pl, I get this:\n./builder.pl client.coffee\n./builder.pl: line 1: syntax error near unexpected token $old,'\n./builder.pl: line 1:my ($old, $new);'\nor without args\n./builder.pl: line 1: syntax error near unexpected token $old,'\n./builder.pl: line 1:my ($old, $new);'\nPerl is a foreign language for me. What is going on here?\n. Ok, I did a npm install in the client directory to get browserify. To get browserify into your path, you also have to \nnpm install -g browserify\nso that it is in your path. Finally, it seems to be working by typing:\nbrowserify client.coffee -o client.js\nStill, the output we get does not produce valid javascript. it still tries to require coffeescript. This is frustrating.\n. It sounds worth a try. It would be cleaner.\n. I think I did this right. It's the same request, but I updated the latest commit from my forked copy.\n. Has this been resolved with the regeneration of the browserify js?\n. Check it out in action:\nhttp://theoutpost.io:3001/\nI have gotten the basic functionality working. Now you can click on Journal buttons and see the view in its past state. It's pretty exciting.\nBullet points #1 and #2 are not yet done. I would like to do a pull request, but github won't let me till you make a branch called \"revision\". This is certainly not ready for master branch yet.\nhttps://github.com/hallahan/Smallest-Federated-Wiki/tree/revision\n. You could go a step further and use your server also as the IRC server. Who needs freenode anyways? : ) This is all about independence and decentralization, right?\nhttps://help.ubuntu.com/11.04/serverguide/C/irc-server.html\n. This idea will tread uncharted territory with UI design. Though the idea is crazy, it has occurred to me that this can be done in a fairly short span of time in a sane manner. I have already implemented a NodeJS app that you have seen, and it is fully functional. The code is a bit sloppy, and there is a bug or 2 lurking, but it has been live for about a month and has not crashed. \nStill, I think it should be rewritten in coffeescript. The quality of the code does not have the deliberate, zen-like approach that SFW is taking. That being said, I started to muck about with trying to integrate the server code I have written with SFW, and it really is overwhelming. I think a more sane approach in engineering this would be to keep the two projects separate for the time being. Instead, functionality can be implemented as a plug-in for SFW. The real time stuff can stay with The Outpost's Broadcast.\nThe way that I deal with data is that every keystroke of the user broadcasts the contents of the textarea back to the server via socket.io. Then, the server sends that out to everyone connected except the sender. When the user presses enter, that message is persisted to the server's persistent data store (JSON files similar to SFW's model).\nMy suggestion is to have Broadcast do a HTTP POST to SFW whenever a message is persisted. I have a feeling that this can be an extremely robust approach. If Broadcast fails, it doesn't matter for SFW. It also walks down the path of using multiple NodeJS processes and possibly separating the interests of real-time and history between two completely separate machines.\nThe concept of non-linear conversation i think is a very important milestone to get to. Because multiple people may be adding to a conversation in any given location of a view, we would have to think of some new way to bind the identity of the speaker to the message. All chat UI up to this point simply attaches a label to a block of text. This approach doesn't work well with non-linear conversation.\nI have been playing around with SFW on my site, and I have a list of issues I have come across in casual usage:\nhttp://theoutpost.io:3000/view/welcome-visitors/view/sfw-issues\n. Thanks for the node pointers. Yeah, I'm not sure what is going on with the Express problem, but it's not a very important issue. I'm going to close this.\n. I recloned your repo, and the same problem remains. Looking in the debug console, I find this:\n```\nError 1: GET http://localhost:3000/plugins/factory/factory.js?_=1335911864870 500 (Internal Server Error)\nError 2: GET http://localhost:3000/plugins/factory/factory.js?_=1335911864871 500 (Internal Server Error)\n```\nwiki.log says: \"ajax error\"\nLooking into the objects the log gives  us, I see as response text:\nresponseText: \"ReferenceError: utils is not defined\u21b5    at next (/home/og/node/Smallest-Federated-Wiki/server/express/node_modules/express/node_modules/connect/lib/http.js:172:48)\u21b5    at next (/home/og/node/Smallest-Federated-Wiki/server/express/node_modules/express/node_modules/connect/lib/http.js:205:9)\u21b5    at next (/home/og/node/Smallest-Federated-Wiki/server/express/node_modules/express/node_modules/connect/lib/http.js:205:9)\u21b5    at Object.oncomplete (/home/og/node/Smallest-Federated-Wiki/server/express/node_modules/express/node_modules/connect/lib/middleware/static.js:150:11)\"\nsetRequestHeader: function (a,b){if(!s){var c=a.toLowerCase();a=m[c]=m[c]||a,l[a]=b}return this}\n. I ended up doing a fresh install of the latest Ubuntu 12.04, and it seems to be fine now. I think i just had a screwy environment.\n. got better at git, made new pull request\n. I'm looking at that file, and I think that the animation of \"page\" elements would need to be reimplemented for centering. The number of open pages would need to be counted. Then, calculating the x,y offsets in absolute mode based on the sum of the pages should be able to make it happen. Not a simple tweak, unless you can think of one?\n. I like the \"never have to log in again from that computer\" very much. We can finally actually make use of cookies for what they were intended for!\n. Ah ha! Very good! Why have a magic word when you can have a magic key! \n. Great, yeah that makes sense. The button group is what right orients now. \n. How about something in between both ideas? If you are forking a page to your site, the logic will be most simple if you are viewing the foreign page in your site. A foreign site should not be meddling with your site's affairs. However, it is annoying having to type in that url of the remote page in another tab so that it can be seen on your site.\nMaybe you can come up with some sort of UI where a foreign page on a foreign site can be loaded on another site?\nSo, say you are at sillywiki.theoutpost.io viewing a page and you want to fork it to wiki.harlantwood.com. Maybe somehow redirect the browser to load that specific page at another site?\nAnother crazy idea I am having is that local storage could be synchroized with your own site?\nI do agree that something is missing in the logic of forking when you are dealing with a foreign user browsing a native page at your native site.\n. Interesting, it feels almost like a backwards IPv4. \nA number would be an anonymous POST, while a name could be used if done by the owner of a wiki in the farm?\nThis gets me thinking about treating URL in a new way. As it stands, the string to the right of the TLD corresponds to a path for a single given resource on the domain. Subdomains can mean anything, but they tend to be a different server.\nI've been thinking about how HTML has this strength of letting loose a set of requests for more resources. It would be really cool if you could get a graph of resources for a given element rather than a single resource. The graph may correspond to history as well as different locations.\nWhat if we crafted a schema for the left side of a domain that provided a set of resources that the browser could recurse with a given granularity? \nThe JSON objects that we have for a given view has a journal, but that journal is going to get very big, and it probably is inconsistent. Could the left side of the domain be a schema to set of a graph of requests down the internet to construct a history?\n. I think that the fabric of the notion of history and origin should be dynamic and non-determinate. An unknown chain of requests used to construct a history will provide this quality, and I think the result will provide a truth that cannot be manipulated by individuals or organizations.\n. This does make sense as being potentially useful, but you will notice that this feature is rarely used in Wikipedia. I have played with your converter, and I do think you should keep working on it. It will be incredibly useful for getting content off of all of those nasty sites with ads everywhere and just a nugget of content.\nI would like to suggest two things:\n1. HTML will never ever go away, so don't fight it. It is bloated, but so is the English language. If you don't feel like storing link data in HTML makes sense, have you considered JSON?\n2. If it can be done client-side, it should be done client-side. Sure, if your converter is doing the work, Ruby makes sense, but it does not make sense if the user is making a decision about the link.\n. After playing around with the new functionality, the noop when we press\nenter and the caret is at position 0 doesn't seem right. I think this would\nwork the best:\nAt position 0:\n  Enter -> New Paragraph as if caret was at end\n  Backspace -> Edit end of preceding item.\nCould some of you try out the new functionality and agree or disagree with\nthis suggestion?\n. Nah, I see no reason to have a bunch of white space. I'm talking about if the caret is at 0 and we have text in the box after it. This is useful if I double click on a paragraph and I want to make a new paragraph after it.\n. I agree, however, a backspace should still go to the last item... I have found a strange bug. The flag is this weird parallel line glyph in Chrome in Windows 7. On mouse over, it is a rectangle. It's interesting to see. Boot up Windows and have a look. It is more fork like, but less country like... I like using the symbols for icons, but I think we need to load in an explicit font so that things are the same everywhere.\n. That last step of positioning the caret at the end of the text after a backspace concatenation may be weird. This will mean that the caret will jump from where it was to the end. Maybe just concatenate but leave the caret where it is? I'll have to implement this and try out both.\n. done, see pull request.\n. The backspace at the beginning of a text area allows you to delete unwanted splits. What goes wrong when the caret is at the end of text areas by default? \n. I think I know what you are talking about. Perhaps I could have the caret go to the end only on double click so that splits will have the caret at the beginning to facilitate an easy rejoin.\n. Wow, lots of problems. I will begin tomorrow resolving this. I will carefully play with my fix.\n. This looks really good. How would it work with git as a backend? Would we have a bare repository and then check things out upon request? It is a beautiful system, and exploring using it in this way sounds compelling. Does it perform well enough to be treated like a database?\n. I just assumed that git has whatever http server it uses on top of a file system where the git repos live. If that is the case, does git really provide any functionality with serving content? Do you have any info on how this actually works? Git may be a useful tool for deployment, and github may be a useful service to use to serve content, but I am wondering if there is anything special we actually get from git in this usage model.\n. Wrong checkout, my bad. Closing issue.\n. no prob.\n. I plan on giving docco a whirl today.\n. You are correct, the NodeJS server has not been tried on Heroku. An effort in this direction would be appreciated. For it to work on Heroku, you would need to port Couch support, because the Node implementation stores data as .json files on the filesystem. Heroku does not allow this.\nThe Node server does work well, and this is the one I use primarily. It is easy to set up on your local computer, and it is also easy to install NodeJS on an AWS EC2 instance as well as Rackspace. The nice things about these services is that you actually have root access to your server, so you can install and run anything you please.\n. Here's a good looking Ruby WebSocket Lib:\nhttps://github.com/igrigorik/em-websocket\n. A really good looking project using Sinatra, WebSocket for file uploads:\nhttps://github.com/thirtysixthspan/waterunderice\n. And the CSS just for that page has a thick border. All of the other pages do not... Is Node still being maintained?\n. local storage bug. reset browser and it works now.\n. Unfortunately that version specification is with the sinatra-websocket gem. I'm thinking if we are to get this to work, we have to actually pull in websocket-sinatra into the project or somehow modify that gemfile.\nThe good news is that I took a look at SockJS, and it works rather well with the Express project. It turns out we can have our cake and eat it too regarding the use of web-sockets and being backwards compatible with browsers that do not support it. SockJS falls back on XHR but uses the same WebSocket standard API. So far, I'm liking it more that socket.io. There is also a Ruby server implementation that is currently under development. Right now I'm leaning towards updating the Express with the search functionalities and going with that. \n. In the node, http://localhost:3000/plugins/reference/reference.js works \nfine. The plugins are served like any static asset in the client \ndirectory. Does your Sinatra server respond to some sort of alternative \npath other than this? Resolving the path to the plugin is client side \nI'm thinking.\nAlso, the socktest plugin does indeed work, and it has a newer style \npath. What exactly does the reference plugin do, and how do I use it? \nNote that the logwatch plugin we made uses the newer path style, so I \ndon't know if this is really the issue?\nOn 09/16/2012 05:06 PM, Ward Cunningham wrote:\n\nI notice that node/express can't find the \"reference\" plugin that is \nin client/plugins/reference/reference.js. I'm guessing that this \ncommit, bb6e854 \nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/commit/bb6e854b6e5027facf9ebce741e3be06064ed567, \nneeds to find its way into the express implementation. This looks in \nthe old and the new place for plugin javascript. A later mod revises \nthis to look in the new place first. Just about all plugins that I \nmanage have been moved at this time.\n\u2014\nReply to this email directly or view it on GitHub \nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/pull/287#issuecomment-8601913. \n. Whoops, I didn't have dependencies with this checkout. In process of merging two checkouts to avoid noise in git log. Please ignore.\n. Argh, when I took out code I was working on, it was fine. Strange, I just was serving a specific static path with some stuff that was unrelated.\n. I am pretty sure this is a new bug regarding eating up paragraphs when the caret is at 0 and enter is hit. I will do some debugging and see what is going on.\n\nI am unclear about Second observation. Are you saying that enter should push the contents after the caret to a paragraph below, outside the textarea, and the text area should remain in place, empty?\nI will first try to see if I can do a simple fix to this bug. Some errors are being thrown in the console.\n. You probably have an older version of the Coffeescript compiler or some other Node package. I have a brand new aws server and everything was installed yesterday and today. \n. The question is, what is the ' ' really doing, and how do we have the proper effect without doing this? Can you clarify bullet # 3? Other text is lost or not?\n. I just checked the commit 83605ebe30c3d9ceb1202bf330d3256b3af8321d which is right before aba35ec50d72c6ee359a720720bac789ef65b0cb. So, this bug has been here before I made the ' ' fix. The problem is here as well. So, I think this would be a server issue, because it is fine before refreshing the page.\n. Well, as it stands, the core crashes if the server plugins are not installed. I would either put server code in the server/plugins dir or take the plugins out of the client dir for clarity's sake.\nPerhaps we could have an install script that insures that npm install is run for client, server, and the plug-ins?\n. Indeed, so maybe just take the plugins up one dir level?\n. So, basically, you want to do npm install in the client directory, as well as the plugin directories that are failing. Your specific error to this issue is solved when you npm install in client.\n. Also, if you are trying to run this on windows, you will have to do some extra work. I was unable to make the ws library work on a windows host. I do not have Visual Studio 2008 installed, which I think is required for it to compile properly.\n. The 'ws' module is short for web socket. This shouldn't have anything to do with factory as far as I know.\nJust so you know, the the coffeescript is precompiled to js under the hood when you start node. Client side coffee is explicitly compiled, but server side, node does it when it starts up. Sometimes, I do expliclty compile it to help me out with debugging, but it's not needed.\n. I deleted this, because it is no longer used. This bind happens in plugin.coffee\n. ",
    "harlantwood": "@WardCunningham, you suggested last week creating a circle with the people who'd been on hangouts who you recognized and inviting only those.  I've never done this, but it seems like a good plan if it's possible.\n. @WardCunningham, you asked in #176 in this comment:\n\nIs there an easy way to strip accents from latin characters? Maybe with a regex or something equally common and well supported?\n\nRails has a String#parameterize method, which creates a slug very much like what SFW uses (except that it allows underscores), including calling transliterate (both methods are part of the activesupport gem).  Some of the docs and code from transliterate:\n```\nReplaces non-ASCII characters with an ASCII approximation, or if none\nexists, a replacement character which defaults to \"?\".\n\ntransliterate(\"\u00c6r\u00f8sk\u00f8bing\")\n# => \"AEroskobing\"\n\ndef transliterate(string, replacement = \"?\")\n  I18n.transliterate(ActiveSupport::Multibyte::Unicode.normalize(\n    ActiveSupport::Multibyte::Unicode.tidy_bytes(string), :c),\n      :replacement => replacement)\nend\n```\nAnd parameterize:\n```\nReplaces special characters in a string so that it may be used as part of a 'pretty' URL.\n\n==== Examples\n\nclass Person\ndef to_param\n\"#{id}-#{name.parameterize}\"\nend\nend\n\n@person = Person.find(1)\n# => #\n\n<%= link_to(@person.name, person_path(@person)) %>\n# => Donald E. Knuth\ndef parameterize(string, sep = '-')\n  # replace accented chars with their ascii equivalents\n  parameterized_string = transliterate(string)\n  # Turn unwanted chars into the separator\n  parameterized_string.gsub!(/[^a-z0-9-_]+/i, sep)\n  unless sep.nil? || sep.empty?\n    re_sep = Regexp.escape(sep)\n    # No more than one of the separator in a row.\n    parameterized_string.gsub!(/#{re_sep}{2,}/, sep)\n    # Remove leading/trailing separator.\n    parameterized_string.gsub!(/^#{re_sep}|#{re_sep}$/i, '')\n  end\n  parameterized_string.downcase\nend\n```\n-- Both from https://github.com/rails/rails/blob/master/activesupport/lib/active_support/inflector/transliterate.rb\n. In the hangout today we discussed whether non-ascii characters should be allowed in slugs, eg:\nwww.example.com/\u062a\u0645\u0627\u0633-\u0628\u0627-\u0645\u0627\nwww.example.com/j\u00f8rgen-tuinman\nI took this approach tonight in the slug generation on my 'Open Your Project' site:\nhttps://github.com/harlantwood/software_zero/blob/c140caf64498c81a2d905afbafe4b3b9fc89f4a6/spec/lib/ruby_extensions/string_spec.rb\n...I also brought in @WardCunningham's bilingual slug test idea, as posted above.  Naturally the intention is to merge this code and the fedwiki slug code into one common place (gem?) when the dust settles.\nI am also doing some additional processing of URLs when they used to generate slugs -- just using the path part.\nThe code itself is here: \nhttps://github.com/harlantwood/software_zero/blob/c140caf64498c81a2d905afbafe4b3b9fc89f4a6/lib/ruby_extensions/string.rb#L10\n. Since I use slug generation in multiple contexts, I have moved the versions I find most useful into a new gem I created: \nhttps://rubygems.org/gems/superstring\nThis has 2 \"permissivity\" settings (see #248 for permissivity discussion):\n\"page\" (permissive) -- \nhttps://github.com/harlantwood/superstring/blob/ad82347273b6a5b844b413f70ee50e6ee3094568/spec/superstring_spec.rb#L73\nand \"subdomain\" -- \nhttps://github.com/harlantwood/superstring/blob/ad82347273b6a5b844b413f70ee50e6ee3094568/spec/superstring_spec.rb#L97\nIf there is interest in using this gem in SFW, I am very open to adding other options as well.  Probably the issues in #248 need to be sorted out, regardless of whether it's in an external gem or not.  I mention this only in the hope that the work I've done on the gem might inspire further clarity around slugs in the SFW community.\n. It's a little to half-baked for me to want to add it to the List-of-Batch-Import-Examples, but I will mention here that I wrote a ruby script to convert markdown files on my local machine to HTML, and upload them to SFW instance(s).  \nThe interesting features apropos this discussion is uploading to SFW by HTTP PUT'ing the JSON to a create action.   \nHere is a sample page generated by the script: http://enlightenedstructure.harlan.fed.wiki.org/view/software-zero\nThis was a spike.  I have shifted my efforts away from processing local markdown -- my intention is to instead start with arbitrary HTML (which may be generated from my markdown or any other source), and strip it down to plain text, or super basic HTML, for insertion into SFW.\nI mention this script mostly for the \"sample code\" of creating a page via HTTP.  In case you want to actually use or extend it, be aware that there are many places the script falls short -- those I can see are enumerated in the README:\n- Will only upload pages once. If a page by that name already exists, the script will warn you of a conflict, but will not overwrite the existing page.\n- No index page is currently generated.  The pages will exist, but only to those that know their paths.\n- Does not handle images\n- Does not attempt any conversion of links to wikilinks\n. I got a version of SFW up and running on Heroku:\nhttp://oyp-htw-sfw.herokuapp.com/\nwith minimal changes:\nhttps://github.com/harlantwood/Smallest-Federated-Wiki/commits/heroku\n-- Note that this is a test only; none of the major work described above has been done yet.  Wiki changes will appear to save correctly, but due to the Heroku \"ephemeral filesystem\", all changes will be blown away on application restart.\n. Some notes on the recursive calls:\n- Proxying remote servers work fine, eg http://oyp-htw-sfw.herokuapp.com/view/welcome-visitors/craig.fed.wiki.org/questions-and-ideas\n- \"Normal\" local calls using view as the site also work fine, eg: http://oyp-htw-sfw.herokuapp.com/view/welcome-visitors/view/smallest-federated-wiki\n- The problematic calls are those that specify the local server address, eg: h t t p : / / oyp-htw-sfw.herokuapp.com/view/welcome-visitors/oyp-htw-sfw.herokuapp.com/smallest-federated-wiki  (hitting this URL will crash the app)\nOf course, in the example above you could substitute view for the last oyp-htw-sfw.herokuapp.com -- but not so in a farm situation, eg:  http://john.wiki.me/view/johns-page/bob.wiki.me/bobs-page -- if wiki.me was running on a thin server, this would crash.  \nSo the solution seems to be: when the farm page is available to the server locally, don't try to make a remote request for it.\n. @WardCunningham I can understand the philosophical objections.  I'm using Passenger to run a farm, and that also handles recursive calls just fine. \n@GerryG thin is what Heroku uses, so we only care if we want to be Heroku deployable.\n. I took a crack at unrolling the recursive calls (before reading your responses above).  Here is the new code:  https://github.com/harlantwood/Smallest-Federated-Wiki/compare/480f83c...5238d8f  (Note that the state of current code is to get feedback only -- if we went this direction I would expect to DRY it up, etc.)\nBecause you can't access the Heroku file system directly to add the data/farm directory, I added a FARM_MODE environment variable which indicates the same thing.\nI added a domain name to the heroku app so I could try out subdomains in farm mode.  Recursive calls now work:\nGiven that x.forkthis.net and y.forkthis.net exist in the data/farm directory (ie they have been visited)\nThen recursive calls like http://x.forkthis.net/view/welcome-visitors/y.forkthis.net/welcome-visitors will work as expected.\nThe only URLs that would still be problematic would be those referencing subdomains that do not yet exist in the data/farm dir -- eg h t t p : / / x.forkthis.net/view/welcome-visitors/brand-new-subdomin.forkthis.net/welcome-visitors (will crash)\nQuestion: is there ever a case where we create a subdirectory in the data/farm directory which the server does not own?  eg do we ever cache remote servers' pages in our data/farm directory?\n@WardCunningham and @nrn, I can see the advantages of only supporting non-farm mode on Heroku.  On the other hand, if we did go in the direction of unrolling recursive calls, there is a significant performance gain, both speed for the end user, and removing the load of the server making HTTP calls to itself.\n. Interesting idea @WardCunningham about the claim mechanism on Heroku.  The idiomatic way to interact with your Heroku app is through environment variables, so from the command line the app owner could say:\nheroku config:set OPEN_ID=http://myidentitysite.me/\n. I did a spike on storing and retrieving page data in CouchDB\n- The commit: 0c67cce\n- Working version on Heroku: http://1.sfw.forkthis.net/\nThe Couch document ID is currently the absolute path of the file, eg \n/app/data/farm/1.sfw.forkthis.net/pages/new-pages\n...which should become the path relative to the app root, eg\ndata/farm/1.sfw.forkthis.net/pages/new-pages\nThe Couch document contains just a \"data\" key containing the file as a string:\n1.9.2-p290 :041 > puts $couch.get(\"/app/data/farm/1.sfw.forkthis.net/pages/new-pages\")['data']\n{\n  \"title\": \"new pages\",\n  \"story\": [\n    {\n      \"type\": \"paragraph\",\n      \"id\": \"cfe1dde740b6185e\",\n      \"text\": \"I am a new page\"\n    }\n  ],\n  \"journal\": [\n  ...\nNext steps:\n- Uploading favicons as \"attachments\" to Couch \"documents\"\n- Save \"identity\" to Couch\n- Refactor code to allow saving to either the filesystem, or to Couch\n. @WardCunningham and @GerryG, I don't think we need to change the current OpenID strategy for Heroku -- I think the current code will work fine.  Even the one-line change I suggested to the OpenID::Store above I now believe to be unnecessary.\n. Spike 2 went smoothly: favicons are now persisted in Couch:\n- The code: https://github.com/harlantwood/Smallest-Federated-Wiki/compare/0c67cce...3ef64de\n- Live on Heroku: http://any.subdomain.forkthis.net/favicon.png and http://any.subdomain.forkthis.net/random.png\nI manually base64 encode the favicons, and store them in Couch with the \"path\" as the key, exactly the same way as other documents -- happily, I avoided using the Couch \"attachment\" feature entirely.  Live example from the console:\n```\n1.9.2-p290 :063 > ap $couch.get(\"/app/data/farm/z.forkthis.net/status/favicon.png\")\n<CouchRest::Document:0x7ff0134c3168\nattr_reader :_attributes = {\n     \"_id\" => \"/app/data/farm/z.forkthis.net/status/favicon.png\",\n    \"_rev\" => \"1-62345e227a77187c779e9c44d57cfe51\",\n    \"data\" => \"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAGDElEQVR4nK2W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\\na8kdqtuH+bL9H1M2VJkTgpNBAAAAAElFTkSuQmCC\\n\"\n}\n...\n\n``\n. @WardCunningham the 3 tiers make perfect sense, although I am a complete n00b on local storage, in this app and in general.  Some thoughts on syncing between cloud and desktop:\n- The server could offer a zip file of all pages in a subdomain, or all pages in an entire farm\n- We could consider a version control system like git (which we could talk to eg through the github api from the server) for robust 2-way syncing, merge conflict handling, etc \n. Persisting identity claims in Couch should be easy.  The last remaining juicy item is the Couch implementation of/recent-changes.jsonand/global-changes.json`.\nI had originally thought of making a new db for each subdomain in the farm, but I think the way that couch wants it done is a single db for the whole farm, and a \"view\" to allow us to pull the data for a particular subdomain; or to pull the top n changes for each subdomain, as we now do in ruby from the filesystem.  \nSo, next steps:\n- Couch spike # 3: /recent-changes.json and /global-changes.json\n- Save \"identity\" files to Couch\n- Refactor code to allow saving to either the filesystem, or to Couch\n. Thanks Ward for expanding on the current state of page storage.  Question:  do we need the \"non-farm\" storage location at all?  It seems like a \"non-farm\" SFW instance could be defined simply as a server at which only one domain points, eg mydomain.com.  All of the pages would be stored at:\n/data/farm/mydomain.com/data/page\nThe site would be redefined as a farm if I simply pointed the DNS for *.mydomain.com at the same server:\n/data/farm/x.mydomain.com/data/page\n/data/farm/y.mydomain.com/data/page\n/data/farm/z.mydomain.com/data/page\n-- or is there something I am missing here?  How concerned would we be about breaking compatibility for existing sites with a change like this BTW?\n. The 3rd spike is complete.  Everything is now persisted to Couch.  We are using couch \"views\" to request the recent-changes.json for the current site in a farm, or the default site in non-farm mode.\nIf you want to browse recent commits, they are here: https://github.com/harlantwood/Smallest-Federated-Wiki/commits/couchdb\nlocal-identity and openid.identity are persisted to Couch.  I turned off the OpenID gem writing to the filesystem at all, as it gets confused and crashes when Heroku erases the open ID data directory on application restart.  Current version:\ndef openid_consumer\n  @openid_consumer ||= OpenID::Consumer.new(session, nil)\nend\nThis is called \"stateless mode\" by the OpenID gem.  They say: \"Stateless mode may be slower, put more load on the OpenID provider, and trusts the provider to keep you safe from replay attacks.\" (from https://github.com/openid/ruby-openid/blob/master/lib/openid/consumer.rb)\n. Next step: brainstorm and create a simple architecture for swapping in either filesystem-based or couchdb-based persistence.  \nIdeas so far:  the Page and Server classes could have a @store variable, of type FederatedWiki::Store::File or FederatedWiki::Store::CouchDB.  The store would have a few methods:\n@store.put_page  # pages contain special metadata like a timestamp\n@store.put_blob  # for favicons\n@store.put       # for arbitrary strings, eg local-identity\n...and analogous get methods.  Something like that.  The interface should fall out of the similar parts of the current file store and CouchDB store.  Any ideas very welome.\n. I created a pull request for the CouchDB storage: #204.  This is the bulk of the code that will get us Heroku-able.  I'll create another pull request or two for the Heroku-specific code once the Couch code is happily integrated.\nThat's great @nrm that you want to Couch up the node server too!  I found Couch very pleasant to work with.\n. With the Couch work merged into master, I am tackling the last major item needed for Heroku:  handling of recursive calls with the thin web server.  I've done this in a minimally invasive way: only if you set the ENV variable \nSINGLE_THREADED_SERVER=true\nthen we look for sites locally on the current server.  The code:\nhttps://github.com/harlantwood/Smallest-Federated-Wiki/compare/4d1fc53...670298d0\nNote that this diff includes some refactoring as well.\n@WardCunningham, take a look at the minor README changes in this diff, and let me know what you see that still needs beefing up -- I certainly want these changes to be usable and accessible to the community.  When the Heroku work is all merged to master, I will also add a heroku section to the \"Hosting and Installation Guide\" wiki page.\n. I'm still investigating one other pathway before creating a pull request -- it seems that there are ways to handle async requests on thin using EventMachine.  From the Heroku docs:\n\nThe herokuapp.com routing stack can be used for async or multi-threaded apps that wish to handle more than one connection simultaneously. Ruby webservers such as Goliath, Thin (with a suitable web framework such as Async Sinatra), or your own custom EventMachine web process are some examples. \n\nThis is a bit of a deep rabbithole, not sure if it's worth it or not...\n. Hm.  I was running farms with no problem on EC2 under Passenger.  I added good installation docs to the \"Hosting and Installation Guide\"  wiki page for this setup.\nI would like to get farms up and running on Heroku, at least for my own purposes -- hopefully I can do it cleanly and non-intrusively enough that you're happy to pull the changes into master too.\nAfter spending a little time with EventMachine, async_sinatra, and friends, I think I'm going to leave this powerful but complex territory for another day, and create a pull request based on the diff I referenced above.\n. Created pull request #221, Heroku support.\n. I wrote up instructions in this page: https://github.com/WardCunningham/Smallest-Federated-Wiki/wiki/Hosting-and-Installation-Guide -- search for \"Using Heroku\".  \nI think we are there!  Please let me know any issues you encounter, happy to help.\n. Side note:  there is one test that fails on my (OSX) machine -- it was failing prior to cutting my branch as well:\n1) testing javascript with mocha should run with no failures\n  Failure/Error: failures.should be('0'), trouble\n    []\n  # ./spec/integration_spec.rb:305:in `block (2 levels) in <top (required)>'\n. Good to chat tonight.  I incorporated the changes that came up, plus a few other minor improvements.  Tests pass (except for the unrelated ./spec/integration_spec.rb:305), and smoke tests on both file storage and couch seem good.  I'd like to get in more testing (probably this weekend), but it's also fine to pull these changes now if you'd like.  I'll try to respond quickly if bugs show up.\n. I'll do some testing on this tomorrow and get it wrapped up by the end of the weekend.\n. I created two test apps:\n- http://filestore.sfw.remixfreeip.org/ \n  - uses FileStore\n  - deployed on EC2 Sinatra Passenger\n  - code: 'stores' branch\n- http://couchstore.forkthis.net/ \n  - uses CouchStore\n  - deployed on Heroku\n  - code: 'stores' branch plus minimal Heroku commit 8d513b4\nBoth seem to work well in every smoke test I can think of:\n- Creating pages\n- Dragging and dropping content between pages\n- Dragging and dropping a page from one site into the other\n- Uploading images\n- Creating favicons\n- Claiming sites\n- \"Recent changes\" pages\nConclusion: ready to pull.\n. Fixed a bug that effected the couch version.\n. Thanks Ward.  It's great to be working on a project I believe in so strongly.\n. Some useful commands for setting up your heroku app: \nheroku config:set STORE_TYPE=CouchStore\nheroku config:set COUCHDB_URL=xxx  # Your couchdb URL, eg from https://addons.heroku.com/cloudant\nIf you want farm mode:\nheroku config:set FARM_MODE=true\nheroku config:set FARM_DOMAINS=my-sfw-farm.org\n. > Can you suggest how serving pages from the plugin directories should be handled based on abstractions that you've made? I can imagine some generated page, like recent-changes, that enumerates installed-plugins.\nAnything that is checked into the git project we can safely serve up from the file system.  It's only \"dynamic\" files that have to live in the Store layers.\n\nI wonder if it is desirable to copy pages from default-pages when read? \n\nIt seems useful for the \"home page\" of each site, less so for other pages, especially if we have another system for plugin-related pages (eg some way to get what we now get with the default pages d3-*).  The home page copying could happen on site creation instead of on read.\n. <happy developer>\n. Here's what I have in mind for Open Your Project -- when I ask OYP to scrape:\nhttp://enlightenedstructure.org/\nit creates a SFW instance at:\nhttp://enlightenedstructure.harlan-t-wood.openyourproject.org/\nWhen Nicholas comes along and wants to fork content on this site, we ask him for a username (or get it from his cookies if he's done this already), and create his fork at:\nhttp://enlightenedstructure.nicholas-hallahan.openyourproject.org/\nI guess ideally this new fork would open up in a new panel to the right, perhaps with some messaging indicating that it's your fork, you can claim it and edit it, etc.\n. Great discussion.  I want to add to throw in my (mostly orthogonal) latest thinking on forking, cloning, etc.  As I've been working more with representing pages and sites visually...\n\n...the visuals have affected how I'm thinking about forking, and also about curation.  Suppose there was a huge sea of CC-licensed pages available in Federated Wiki space.  When I encounter pages I like, or possibly even whole sites I like, I want to be able to easily clone them into my space, to show my interest in the content (ie add to my curated collection), and of course for possible later remixing.  Ideally I'd like to be able to do this visually, by dragging a page circle or a site circle, perhaps from one SFW pane to another.  \nNote that all of the above could happen outside of SFW, in tools built on top of it, and/or within SFW, probably as a plugin.\n. Rails has a good solution (and gem) for the transliteration in ruby -- not sure on the JS side.\n. Done on the ruby server side: 141aa1d\n. Interesting.  I'll tell you my use case -- maybe there is a solution that would suit both our desires.  \nAs I demoed on the call last week, I am developing an external app which scrapes an arbitrary (CC) website and converts it to a SFW instance.  I am currently working on converting HTML links to wiki links.  Consider two links to the same page:\n```\nPlease check out our API\nSee also our lovely Geek Stuff\n```\nThe solution I had in mind is to convert these HTML links to wiki links thus:\n```\n[[API]]\n[[API|Geek Stuff]]\n```\n. Thanks for referring me to the previous discussion.  Next time I'll search the history before starting a duplicate issue ; )\nTo get right down to the meat of it:\n- My intention is to scrape a lot of creative commons licensed sites into SFW instances\n- It sounds like your intention is to have SFW used only by people creating content directly within a SFW instance\n\nYou're going to find a lot of things in html that won't have convenient equivalents in our standard paragraphs. \n\nTrue!  What I'm hoping to achieve is a good representation of the content -- not necessarily the parts of the HTML that are about presentation.  I am currently reducing HTML to literally plain text -- I do want to restore links, and inline images, and probably a few other things.\nI want to keep links with the same \"text\" and the same \"target\" as the original content, and can't see how to do this without a feature like the one described above.  Oh, well, there is one option.  I could just have HTML links like:\nCheck out <a href=\"/view/about-us\">our team</a>\ninstead of:\n[[about us|our team]]\nThey wouldn't be wikilinks, so they wouldn't open up a new panel to the right; they would replace the whole page with the new page, no matter how many panels were open.  But on the up side, the links would work with all SFW servers, not just my fork ; )\nIt's possible that SFW is not the right target technology for what I'm trying to do.  But the drag and drop remixing is just so compelling...  Imagine the SFW remixing capabilities, combined with a huge body of CC licensed content, in a topic area of genuine interest to you.  \nObviously you have a very strong position on this one.  I will keep experimenting and communicating, and trust that we can find solutions that meet all our objectives.\n. Hm, I'm a bit suspicious of trying to improve other people's content in an automated way.  Authors who have carefully crafted link text that differs from the linked-to-page-name will surely disagree that any changes to the link text are indeed  improvements.  My thinking has been to assume that the original content is sound, and try to reproduce it as faithfully as possible, while converting it to an easilty remixable (and ultimately fork/diff/merge-able) format like SFW.\n. > you will notice that this feature is rarely used in Wikipedia. \nInteresting point.  I am starting to come around to the idea that using the page name as the link text is a \"best practice\", although I still don't want to force content into that form, out of respect for the original authors. \n\nI have played with your converter, and I do think you should keep working on it. \n\nGreat!  Always a pleasure to have someone use your software ; )\nLast night I was playing with generating links on the sever side to replicate the client-side links -- in order to get the side scrolling working.  I made some progress (see lines 77-80 of the forker), but still no side scrolling.\n\nIf it can be done client-side, it should be done client-side. \n\nI think that's the key.  One possible strategy -- on the server side, when scraping:\n<a href=\"/content/recipes.html\">Chapter 24, Recipes</a>\nconvert the tag to:\n<a slug=\"content-recipes\">Chapter 24, Recipes</a>    # 'content-recipes' is the path converted to a slug\nor \n<a slug=\"recipes\">Chapter 24, Recipes</a>        # 'recipes' is the page title, converted to a slug -- this is harder but better\nthen on the client side, when we see <a> tags with a slug but no href, we convert them to internal links in the usual pattern:\n<a class=\"internal\" href=\"/recipes.html\" data-page-name=\"recipes\" title=\"origin\">Chapter 24, Recipes</a>\n. The more I think about the server/client solution above the less I like it.  It feels ugly to pass around munged broken <a> tags.\nAttempt # 2 -- on the server side, when scraping:\n<a href=\"/content/recipes.html\">Chapter 24, Recipes</a>\nsimply add a class to the tag, to mark it for later client-side action:\n<a href=\"/content/recipes.html\" class=\"fedwiki-internal\">Chapter 24, Recipes</a>\nthen on the client side, convert to internal links in the usual pattern:\n<a class=\"internal\" href=\"/recipes.html\" data-page-name=\"recipes\" title=\"origin\">Chapter 24, Recipes</a>\n. When I've been crawling wikipedia pages, I end up with lots of \"internal wiki links\" that don't work -- because I have not crawled all the pages that are referenced.  So I am now thinking to keep the original link intact, and also pass a hint to the client of the probable slug name, in case such an internal page exists.\nSo, attempt # 3 -- on the server side, when scraping:\n<a href=\"http://my-cookbook.com/content/recipes.html\">Chapter 24, Recipes</a>\nsimply add a hint to the tag, to mark it for possible client-side rewriting:\n<a href=\"http://my-cookbook.com/content/recipes.html\" fedwiki-slug-hint=\"recipes\">Chapter 24, Recipes</a>\nthen on the client side, if we know of such a page, convert to internal links in the usual pattern:\n<a class=\"internal\" data-page-name=\"recipes\" title=\"origin\">Chapter 24, Recipes</a>\nand otherwise leave the link unchanged, so it still points to the original resource.\n. Nice, I like the idea of offline crawling of the pages we link to but have not yet crawled.\n. It looks like the stacktrace is coming from this line:\nset :versions, `git log -10 --oneline` || \"no git log\"\nMaybe git can't be found in your path or some such?  You could try:\nset :versions, `git log -10 --oneline` rescue \"no git log\"\n. I was originally thinking of storing the images all in one dir, with the image filename being the MD5 of the image data.  This would make the images content addressable, such that if there are 100 forks of the same page with an image, we only store the image once.  \nMy latest thought is that content addressability should be a separate layer -- eg I would love to see us add a GitStore (or GithubStore) as well as the current FileStore and CouchStore.  If you chose the git backend, you would get this content addressable deduplication for free.\n. We could have a git repo on the local file system.  Even more compelling from my point of view (largely because it would work with cloud-based hosts like Heroku) is just using the Github API.  So you would push to the github repo backing the given site, using their HTTP API, from your SFW server.  \nThen the cool part: when you want to access the images, you can just link to the \"raw\" version of the image on github, eg: \n<img src=\"https://github.com/harlantwood/open_your_project/raw/master/doc/images/collections-of-pages-circle-pack-viz.png\" />\nNote that we could do the same for JSON, eg:\nhttps://raw.github.com/WardCunningham/Smallest-Federated-Wiki/master/default-data/pages/welcome-visitors\nSo github could serve a lot of our dynamic content over it's highly optimized pipeline.  We might want to check their TOS, and even check in with them directly before doing this, to make sure they're cool with it.  If so, it could be awesome.\n. Again, if there are 100 forks of the same page with an image, in 100 different farm instances, even though we will make 100 \"copies\" of the image in the SFW backend, the git repo (or any other content addressable storage layer) will only make one copy of the image.\n. I have begun work on a GithubStore in another project --\nhttps://github.com/harlantwood/software_zero/blob/3137bf56106393627c20008417e9724ab86c677b/lib/stores/github_store.rb\n-- so far the #get_text & #put_text methods are implemented.  \nThis uses the excellent github_api gem, which closely mirrors the Github HTTP API.\nBecause the Github API is very low level, we need to create repos, branches, trees, etc.\n. Hey Tom, great to have your energy and enthusiasm on the project.  I did the work to get SFW on Heroku, including (primarily) abstracting the storage of page and other data, so it can live in the filesystem or CouchDB.  This is (emergently) architected to allow easy plugging in of other \"stores\".  The next store that I'm currently excited about is a GithubStore ( update: see this comment ) -- partly so that we get content addressable de-duplication for free, and partly to provide a kind of backdoor compatibility to Gollum wiki and Github Pages (both git backed).  But I digress.\nI have been using the Couch servers on Heroku with no problem, and I don't know of bugs.  I encourage you to use it as a basis for a Node version.  The closer we can keep the code bases the better.  I'm happy to learn from and draw from your efforts on the Node side.  \nI'm now using Foreman and a Procfile to boot up and manage env vars on Heroku.  I'm cranking on getting a demo out, so this is currently just in a branch of my repo:\nhttps://github.com/harlantwood/Smallest-Federated-Wiki/blob/oyp/Procfile\nhttps://github.com/harlantwood/Smallest-Federated-Wiki/blob/oyp/.env.example\nPlease consider this strategy for the Node server on Heroku -- it's worked wonderfully for me.  I don't know how we can have two Procfiles!  But maybe you can.  Maybe we can point Heroku to a procfile on deploy.\nTesting:  the test for Couch are thin and mocky.  I would love to see integration specs that test against a real Couch instance.  Couchrest, the excellent gem we use to talk to couch from ruby, has specs that talk to a real couch DB (as you'd expect).  If I was trying to implement more rigorous testing, I'd start by looking there.\n. In a separate commit (93ad271), I added multilingual support: \nwww.example.com/les-mis\u00e9rables\nwww.example.com/\u062a\u0645\u0627\u0633-\u0628\u0627-\u0645\u0627\nwww.example.com/\u0192\u00e5\u00f8\nWhich is optional, but I advocate strongly for.  Otherwise the examples above are damaged or disappear entirely.\n. I agree that the less+more permissive changes are an issue.  Note that all of the changes are in the area of the slug specs that was marked as \"problematic\".\nWhen I run the tests, these are the pages that would break if they had been stored already on existing servers:\n'Welcome  Visitors'\n'  Welcome Visitors'\n'Welcome Visitors    '\n'Pride & Prejudice'\n' - - - - '\n'   '\n'Pride & Prejudice' is concerning because it is a legitimate title (old servers would have saved this as 'pride--prejudice').\nStrings like '  Welcome Visitors' (--welcome-visitors) are unlikely, but could have been passed in as titles from converter scripts reading from other sources.\nThe ways forward as I see them:\n1) We could write a converter script to upgrade the slugs on the filesystems (and possibly CouchDBs) on servers\n2) We could accept that a few servers may break on a few pages\n3) We could leave the slug generation allowing multiple hyphens, as it does now on existing servers\nI vote for 2 or 3.\n(Aside answered in babc03c.)\n. Does anyone know if JS supports POSIX character classes in regexps offhand, such as [:alnum:]?  \nDoes anyone want to take on the Coffee side of this upgrade, once we finalize the details of the way forward?\n. I'm happy to help as needed with thinking about (or reviewing the implementation of) the way these features interface with the \"stores\" in the ruby server.  It looks to me like these features are mostly at a higher level however, i.e. simple Store.put_text(...) and Store.get_page(...) (etc) calls will probably work fine, across all Stores.  Feel free to ping me anytime, I'm happy to brainstorm architecture or review code as is helpful.\n. Hi @rynomad... as it happens, I've just split out my ruby code abstracting stores into a gem called polystore -- which includes an (ugly but working) github store.  \nOf course, this only handles get and put operations, not all the git operations you list above -- notably not merge, which is where things get really interesting.\nIt's a great topic, and totally in line with my larger mission to bring the fork/diff/merge information ecosystem to all kinds of creative works.\n. Yes, @paul90 is correct, just removing heroku from the Gemfile should do the trick, without sacrificing anything.\n. ",
    "almereyda": "I have been trying to attend one or two meetings. But the hangout link was somewhat down Wednesdays, 7pm CEST (what I calculated must be something like 10am PSST).\nAlso #fedwiki seems a little abandoned these days : at meeting times I didn't find activity. I should definately idle there a bit to see what's going on.\nAny plans to revive this? Else the thread could be closed.\n. Merci for the clarification :+1: .\n. I wanted to propose a hashtag for a twitter conversation paralelly to the hangout.\nIt could be #wikiwebchat - refering to http://mappingmashups.net/geowebchat/ .\nOr like http://blog.rebellen.info/saetchmo-echochamber-livestream/ .\nThis broadcast thing could also be an interesting intermediate (and more private) starting point for now. I'm proposing this mainly to have a persistent chat record, as the hangout appearantly doesn't store everything. https://github.com/WardCunningham/Smallest-Federated-Wiki/wiki/Notes-from-Weekly-Google-Hangouts could also be deleted then.\n\nUntil Wiki gets concurrent real-time edits , etc.pp. A summary would already be finished at the end of each session.\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/issues/384#issuecomment-28675343\n\nClose this issue in favour of https://github.com/WardCunningham/Smallest-Federated-Wiki/issues/277 ? (nicer title + shorter)\n. Minutes of this week are to be found at http://yala.fed.wiki.org/view/hangout-131127\n. today's minutes at http://yala.fed.wiki.org/view/hangout-131204\n. Topic : Extraction of data out of and into the wiki.\nraw minutes to be found at : http://yala.fed.wiki.org/view/hangout-131211\nnamespaces, reflections on wikipedia in general, some NDN tidbits and general Federated Wiki workflow suggestions.\n. New notes : http://yala.fed.wiki.org/view/hangout-140115\nAnd as a side discussion: For practical reasons, wouldn't it be good to get rid of those /view/ compartiments in the URIs?\n. http://yala.fed.wiki.org/view/hangout-140212\n. Since my last update, we have collected little notes this month:\n- http://yala.fed.wiki.org/view/hangout-140507\n- http://yala.fed.wiki.org/view/hangout-140514\n- http://yala.fed.wiki.org/view/hangout-140521\n. After updating the last minutes some minutes ago, I would have loved to scroll back in time.\nAs I found it quite nice the last time, Hangouts On Air is nice : Fishbowl discussion of 10 people in the hangout (leaving 2 places empty) and an uncountable number of watchers via YouTube?\nWould the current community and esp. Ward like to try this out once?\n. Little notes, this time on : http://yala.fed.wiki.org/view/hangout-140528\n. In some way, \"let's video chat\" can also mean \"let's chat with videos\" so I'm posting these as inspiration:\nWhy do we continue to develop camp? about a darcs like patch based version control. [via]\nDifftimeline - Demonstration of a git repository visualization tool. [via]\n. Well, @coevolving has notes from this week, freely forkable via http://fed.coevolving.com/view/digest-2014-06-18, found at https://github.com/WardCunningham/Smallest-Federated-Wiki/issues/423\n. Appearantly [1] I'm meeting with one of their developers this week.\nAny questions towards them?\n[1] https://gist.github.com/nrn/3669857\n. Is this here similiar to https://gist.github.com/nrn/3669857 ?\nOr is there somewhere a centralized collection of Bookmarklets / Best-Practices / Third-Party addons etc. ?\n. The idea with OT is still somewhat current : https://github.com/WardCunningham/Smallest-Federated-Wiki/issues/384.\nIntroducing a granular versioning as darcs messes with the Journal, which decreases compatibility, if not handled right. Although https://github.com/WardCunningham/Smallest-Federated-Wiki/issues/318 warns about this, still I kept on thinking if it wouldn't be wise to allow a more granular versioning backend.\ndarcs is interesting, because it handles commit dependencies differently. there's an impressive video at http://projects.haskell.org/camp/unique .\nI'm imagining to fork / copy certain patches instead of whole paragraphs. Maybe someone forked my site and made a clever addition. I want this to return to my page. But only this halfplace.\nIf I'd hover the left side of a paragraph, I'd move it in total. But if I hover above the text, portions of it that belong to certain commits are highlighted and can be dragged, too.\nWe imagine this page is already opened via the federation. So dragging works basically.\nNow if I took one of those darcs/camp partial commits/patches, I could hover them above any other paragraph and if the diff 'snaps in', I could let it fall. Otherwise a new paragraph is being created.\n. Wow, Paul, thank's for the mention of substance. It seems that's one part of the (self-hosted infrastructure) puzzle I've been waiting for.\n\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/issues/186 also talks about Data Schemes for the Journal.\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/issues/278 talks about storing JSON changes in JSON.\n. Great and inspirational summary. Happy to discuss that further.\nOn 5 March 2014 09:05, Ryan Bennett notifications@github.com wrote:\n\nThanks Paul!\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/WardCunningham/Smallest-Federated-Wiki/issues/194#issuecomment-36718529\n.\n. I just realized, that we maybe don't even need character-per-character Operational Transformation as in @hallahan's Broadcast, but paragraphwise like Gingko with something PubSubHubBub could already be enough.\nOn the other hand, that implies the availability of Transclusion within a distributed patch based real-time version control..\n. So heavyweight Socket.IO on the one side, RSS dialect PubSubHubBub on the other. Maybe the simple compromise is a de facto standard like Webhooks, in fact a remixed RESTful API, that might need to be rewired to work in distributed environments? People are still waiting for fork notifications.\n. Another intermediary possibility for bridgin the Operational Transformation and Distributed Versioning world could be Webmentions, if we think of the more important need to provide Fork notifications over real-time concurrency.\n. I believe https://github.com/hallahan/broadcast can still play a role either as operational-transform-real-time-reference-implementation https://github.com/WardCunningham/Smallest-Federated-Wiki/issues/384 or as a persistent chat https://github.com/WardCunningham/Smallest-Federated-Wiki/issues/88#issuecomment-28675438 .\n. the hangout is now announced via twitter and the FAQ 10 to 15 minutes in advance:\n- https://twitter.com/WardCunningham\n- http://fed.wiki.org/view/frequently-asked-questions\n\n\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/pull/401\n. Thanks for this immediate response.\nI have to admit that I didn't understand completely yet the way everything works here.\nNow I understand that by creating this farm folder I will disconnect from fed.wiki.org and the others and thus maintaining privacy?\nI'll try it out.\nThe documentation for now seems quite widespread and not so condensed, but this could depend on the exerimentality of this project.\n. Update : Well, I got it running so far and the server automatically created the directory according to the vHost that I've used to access the wiki.\nBut still : on the main page appears the \"new.fed.wiki.org\" greenish icon in the bottom left, which by my understanding signalizes a connection to this server by the JS client. Is it because my welcome-visitors page contains wiki-links to pages that I don't have stored locally?\nStill entering the field ... \nBtw. : where should I post installation tips for CentOS 6.2?\n. This topic can remain closed, just for a quick follow-up sidenote, to document the aftermath :\nMy hosting description (that I've now started to write in my SFW fork - and unfortunately won't be able to 'pull' back via this webinterface) would describe a Sinatra setup together with nginx and Unicorn on CentOS (as mentionned above), including Basic HTTP Authentication.\nI've finally also found all the federation details hidden in the /data/pages/ article (JSON) files.\nWell, maybe a little OT, but now I'm searching for some kind of \"config\" to only let the originally claimed, OpenID-User upload any changes. Until now it also works for anonymous users whose changes will be immediately publicly viewable after \"Submit Changes\". Which isn't really a problem, as I've got another auth layer in front, but just doesn't feel \"right\". Should I open a new issue for that?\nStill big ups for rolling out this concept. Of idea-mind-something-sharing. Post-Wiki.\n. What my second paragraph is talking about is a misunderstanding about the concept how github manages wikis as markdown git repositories : an intuitive commit, merge, etc. webinterface is yet to come. i believe i was already mixing the ideas behind federated wikis with the distributed version control as it is implemented here on github.\nyes, i could write locally and push changes to the .wiki git path ending, directly into the \"online\" branch, but i wouldn't get the comfortable way of forking and \"merge requesting\" once i'm finished writing my versions. multiple .wiki-n branches would be amazing, too.\nthinking of this as a collaborative content production and review system.\nmaybe that's what Editorially is about?\nalso I do believe federated wikis are already an answer to the serious \"Epistemological Critique of Wikipedia\" mentionned by Pierre Levy four days ago about a superior Master's Thesis.\n. @WardCunningham What do you think, where is all this (e.g. semantic, barrier-free, distributed real-time collaboration tools) leading? Thinking in terms of civil society self-organization, the epistemology of knowledge itself and global sustainability & justice.\n. thanks.\n. This is also similiar to https://github.com/WardCunningham/Smallest-Federated-Wiki/issues/384 .\nIs it resolved yet? Then this issue could be closed to do some 'house cleaning'.\n. If you are satisfied with my latest posts/issues I'd offer my help to volunteer with the documentation.\nThat would mean I'd revisit https://github.com/WardCunningham/Smallest-Federated-Wiki/wiki/_pages, turn it into a specification / reference repository and tagged the issues in the repositories according to their topics.\nMaybe it'd be clever to do so after it's been decided how the project repositories are organized in the future.\nPreparations have already been done in https://github.com/WardCunningham/Smallest-Federated-Wiki/issues/402.\nAlso if the codebase would have been organized in an organization I wouldn't have had to fork the whole project just to issue a little change as in https://github.com/WardCunningham/Smallest-Federated-Wiki/issues/88\n\nMain page explains were to find all documentation.\nOther md pages in the code base contain advice for coders.\nGitHub wiki has user curate advice on getting started on diverse platforms.\nFed.wiki.org has how-to documentation.\nProject history and retrospectives are somewhere, maybe ward.fed.wiki.org??\n\nI have the intuition that this is a little too cluttered, to be frank. Maybe one can simplify this even further? I mean, I'm working on the wiki for months now and I'm still finding new links to important pages that explain an issue, an idea or a specification on places that I didn't think of before. But I admint the red line is also constantly changing as the project develops (with the different repos, etc.) and the used, existing systems don't inherit the same flexibility as the Wiki does.\nStill, for me it is not clear where to find specifications / recommendations / conventions / references. I'm always looking in the GitHub wiki for this, but don't find everything I'm interested into.\nAlso, there's another documentation that's hidden in the code's comments to be made alive with docco. https://github.com/WardCunningham/Smallest-Federated-Wiki/issues/240 is an example of someone not finding it. Maybe one regenerates those docs with each npm build and put's them on a gh-page? That could also be a place - the place? - for a \"end-user friendly\" landing page - i.e. wiki.github.io (or a custom domain name)?\nThis could also take the project history and retrospectives, if appropriate.\nThis would also be possible with the organization.\nMy target is only to simplify the documentation. Please correct me if my ambitions head in the wrong directions, as I'm always not sure if I demand too much, imagine without knowing the right way or express interests that your experiencies have proven disadvantageous.\n(Last post for today. Happy.)\n. resolved : somehow the App got confused while writing many edits (create, edit, shuffle and remove paragraphs) and therefore left a very uncanny\n315868\n    }\n  ]\n}\nafter the last closing ]bracket, therefore leading to unvalidated JSON\n. IMO still a bug in the write-to-disk-engine, though.\n. I'm sorry I'm not yet into CI.\nEhm, and sorry for the white night spam tonight. Thoughts rambling.\n. Are you talking more about http://en.wikipedia.org/wiki/Operational_transformation or https://neil.fraser.name/writing/sync/ ?\nI'm sure your idea is going into this direction. Also this error didn't come up again, as I'm favourizing the node server in the last times / deploy situations. Will try out raspi soon.\n. https://github.com/hallahan/broadcast might be an interesting candidate to start thinking about implementing OT, maybe first of all as a plugin:\nhttps://github.com/WardCunningham/wiki-client/issues/5\nIt's been done with https://github.com/learnboost/socket.io\nhttp://ward.fed.wiki.org/view/welcome-visitors/view/federated-wiki/view/related-sites proposes indirectly https://github.com/mozilla/togetherjs/\nhttps://github.com/share/ShareJS appears to be a fairly simple implementation.\n\narchitectural note : (on not claimed/unclaimable permissively open wikis or those allowing collective authorship by invitation {through federation}\n\nAnd you know why real-time collaboration is so amazingly different? Because I've just filed an issue at https://github.com/hallahan/broadcast/issues/1 and meet the author in persona just a few moments afterwards at http://broadcast.theoutpost.io/ . You can check our chat history there.\n. https://github.com/WardCunningham/Smallest-Federated-Wiki/issues/210#issuecomment-5438946 has an argument against shared pages.\n\nIt is easier to believe this can work if one stops thinking of collaborating on a page and thinks instead of collaborating on an idea.\n\nI remember I've read something similiar also in one of the fed.wikis.\n. Issues that seem to be related:\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/issues/224\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/issues/285\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/issues/318\n. Thanks for your explanations. Keep on rockin'!\nDue to the documentation I was aware of the fact, that both versions differ in functionality.\nActually I was not aware of the new repo. Obviously the current development has been moved there, as I have understood by comparing the commit histories of both.\nAs I would not want to mess with the .coffee files directly, which I might understand a little by now, an explanation or at least a config-rename-template.json would appear helpful.\n@paul90 After rethinking the case, I think a dedicated configuration via a config.json, followed by an automatic farm folder structure creation appears more robust. On the other hand flat-file data storage invites to use \"intelligent\" flat \"folder management\" as data-specific tasks are already carried out directly in the file system.\n[EDIT] We might want to close the issue here and move the discussion over to https://github.com/WardCunningham/wiki/issues .\n. Thanks in return.\nStill, any hint anyone could give me on how to write a config.json would be very appreciated.\nWell, I could also try to understand the code to find out what it's looking for ...\n. > As well as creating another 35(?) plug-in repositories (and npm packages).\n:+1: I had to smile a little. :facepunch: :package: \n. Another idea could be to finally create a #fedwiki GitHub Organization and move all concerning repos there.\n. Great work! I was not really aware of the fact that this was a one person task.\nDespite the last but one line carries a little typo : --data.\n. I inexpicably like what I'm seeing here. Finally the GitHub organization arrived.\nAs the issue descriptions propose indirectly and the domain is still available, I propose to register fedwiki.org. I could but wouldn't do so myself without acceptance of the core maintainers.\nOne could then run a new public Wikifarm there, building on the modularized Node version deferring from the new codebase emerging instead of the Rails stack. Until the code'd be ready, a simple pre-templated GitHub Pages landing page would suffice.\n. > If you were to run a hosting service by any such name then you would be in a service business. I would hope that your service would do well by our name. But your relationship with your users would not extend to the volunteers contributing to fedwiki repos.\nI have to think further about this.\n. claphands\nAm 26.01.2014 09:41 schrieb \"Paul Rodwell\" notifications@github.com:\n\nClosed #403https://github.com/WardCunningham/Smallest-Federated-Wiki/issues/403\n.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/WardCunningham/Smallest-Federated-Wiki/issues/403\n.\n. A small update here, as this issue popped up in a conversation between @rynomad and me, also concerning WIK DVCS:\n\nI am currently looking into reasonable hosting (EU, independent) and wait for my funds to rise to pay for the domain. Once these conditions are reached, I will be happy to announce ... yeah, well, I leave the surprise.\n. Coming from https://github.com/fedwiki/wiki-client/issues/74 :\n\n(Maybe a separate subproject could capture all user-reported issues?)\n\nI am about to start a federated wiki community initiative. It will include a simple landing page and knowledge base at http://federated.wiki + a community forum at http://socio.federated.wiki .\nThe Discourse is not set up properly, and I only have a simple Jade-Bootstrap template that I will build upon when I have time.\n. Gone live. Closing.\n. In some discussions we've also come across the situation of the need to fork a set of pages, or even a whole wiki as above, where the first would only represent a subset of the latter.\nThe ReadWriteWeb / LinkedDataPlatform could hold a key with its WebID implementation of federated identity, xwiki even has it already. But that's all 5-star data future.\nI've just had little workflow imagination in my mind, that I'd like to share for discussion:\n\nPreassumptions:\n- Federated Wiki is a federated, multimodal data storage.\n  - This data can be explored by the wiki client.\n  - Its implicit dimensions are:\n    - the journal (top down linear time, refactored. page scope)\n    - the story (URI read linear from left to right. interaction scope)\n    - the sitemap (a wiki's concrete coordinates within the flat name space)\n    - the federation (connected wikis in front (newer) or behind (older) the current one, current implementation encodes this as coloured borders)\n\nNow I've had the idea, how to represent forking a certain set of pages within a workflow:\n- Currently, when we're dropping a link on the factory, only the last page at the end of the URI gets resolved by the server.\n- Couldn't we hack it that way, that the story (i.e. pages opened next to each other) is preserved and clicking on such a link would open all child elements to the right, thus recreating the story?\n  - Those pages should already having been forked, if available. If the original source (i.e. another domain in the URI) is not available, the current federation is queried for possible 'second hand', 'best guess' solutions, that should be marked as such.\n    - A new drop-story-factory could be hold responsible for that.\n    - If applicable, the nested wiki approach seems interesting for that scenario, too.\n. Now that Xanadu arrived, I see another use for Nested Wikis.\nSometimes I want to group some paragraphs together, maybe for referencing, moving around together or other purposes. That interface is quite easy to imagine: Connected paragraphs would have a coloured bar on their left side, or another visual connection.\nThe ~~Nested Wikis~~ data plugin could then hold that data, which paragraphs belong to each other. And I could decide how I relinearize my story/journal/page with the client. Then, in the next step, the Nested Model would also allow to store cached copies of full paragraphs, additional metadata, etc., once we move into the direction of Transclusion.\n~~Nested Wikis~~ The data plugin could therefore be a Data Storage for plugins that need different content types, sourcing from different factories within the Nested Wiki, available for their main factory's summary view.\n\nInspiration here coming from: Rizzoma, Gingko and Wagn, loosely from Netention [Demo] and Fluid Views [ scroll, switch top-left ].\n\n\nEdit from Feb 15 15 : Updated naming.\n. > Note to self: Thinking alot about plugins today, I will further invest how the ideas from The Object Network with their Dynamic UI models and Mozilla Brick's Web Components inform this discussion. @bblfish also mentionned something about self-building UIs in Paris.\n\n\nUpdate: TangleDown is another candidate for (easy?) inclusion as a factory.\n\n\nThere are some predefined X-Tags to get an impression.\n\nHaving said that, it would be nice to also have a federation of versioned logic, in this case the plugins. What else could we think of?\n. I would like to see Nested Wikis as kinds of branches. That might again involve thinking about a pseudo three dimensional interface metaphor.\ndubbing\n- For me, the name space each wiki opens is part of an epistemological graph which is browsable and crying for inference with others. Not that we still try to produce a neutral point-of-view, but instead can show differences over time of any given name.\n- Once we start creating content on wikis as a comment to previous wikis, we want to have stable links, because we will rely on other's wikis content. Nesting several wikis, i.e. with help of the data plugin, can be one way to secure the availability of side-strings of our conversation.\n  It should be possible to keep a shadow copy of a whole domain. yala.fed.wiki.org could then be accessed as usual by inserting it next to the URL slug of a given page, but would be preferred locally, if a shadow copy exists (i.e. a data paragraph that specifies the originating domain of its nested wiki). They are not forks in the regular sense!. The client could still show updates on the remote domain\nreturning\n- We want to improve what Mike Caulfield started to hack on in his wiki journal into 3D interfaces alike this Master's thesis, some Conditional Design and acko.net\n\n_Edit _15 02 15: Formulated sentences out of the former stub.\n. Rereading this conversation, some comments emerged.\n\n\n@WardCunningham wrote\n16 Mar 14 I'm feeling a need for moving whole sites around from platform service to platform service. I do this now for a few sites using rsync. Does a wiki of wikis make this just another drag-and-drop refactoring?\n\nFor two production wikifarms I'm using private git repositories for this task. Including occasional string replacements in newly checked-out branches, if domains don't fit.\nThere may be other git-based storage engines worth the elaboration.\n- https://github.com/mapbox/hubdb a GitHub-powered database. Also see https://github.com/mapbox/hubdb/tree/db\n- https://github.com/nkallen/gitdb a GitHub-like REST API for git repositories. Similiar approaches should also be found within GitLab, GitBucket et al.\n\n\n@rynomad wrote\n17 Mar 14 I've got a very rough outline of a linked data model that might be relevant to this conversation.\nhttp://rosewiki.org/view/wik-dvcs/view/wik-data-model/view/wik-json-schema\n\nThis URI remains a perfect example of why we need more fine-grained forking of pages / stories / wikis. It only became available again after I persuaded @rynomad to release its data folder and republished it on http://wiki-allmende.rhcloud.com.\nBecause this is working material for me, which I reused in the last-but-one line of Support the DAT ecosystem for example.\n\n\n@StevenBlack wrote\n18 Mar 14how can we allow users to select their own wiki aggregations and maintain artefact coherence given the arbitrary namespace composition\n\nIt is especially the term namespace composition which strikes me here, why the fold of differently aligned namespaces becomes the geometric function we apply on the topologic relations implied by a wiki's sitemap and linkmap.\n\n9 Jun 14 not suggesting that this should be handled on the client-side. This should be supported server-side. But in a pinch the client can do this.\n\n@WardCunningham wrote\n\n14 Jun 14 This project was founded on the vision of a proliferation of servers exchanging and caching pages on our behalf. I describe this in this repo's ReadMe three years ago.\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/commit/21f4e7576fe555ae3eb2e1316655bb61236cdb0b#commitcomment-6670041\nWith this comment left this morning I admit that this much hasn't worked out. I have high hopes for IPv6 and even more for NDN and similar overlay networks. However, I can't see how these technologies become anything more than neighborhoods in a more comprehensive federation.\n\nMaybe one day wiki-client will share components that can be reused by wiki-node-server, i.e. the push and pull logic, so in fact we don't care where a client of a remote wiki API is located; inside a user's browser or running as a sub-process of any arbitrary wikifarm.\n@bblfish Do I read LDP right for the instance that WebID delegation offers help here to authorize such requests, wherever which client is acting upon whose behalf?\n\n\n@almereyda wrote (me! ;))\n9 Jun 14 it would be nice to also have a federation of versioned logic, in this case the plugins\n@WardCunningham wrote\n14 Jun 14 We made the choice last summer to use npm as the preferred plugin registry\n\nTwo aspects of package-managing plugins come to my mind:\n- There is substack/hyperboot which calls itself an offline webapp bootloader. But what it does is signing versioned releases of a certain piece of software. A package repository built on top of an exposed REST API that gives access to such releases may help in getting independent from NPM. Ideally built with wiki itself.\n- As it comes into play, lazy loading of wiki-plugin-* packages via NPM within the browser could come in handy, if no such parallel package repository and no wiki in the current search scope offered a certain module for cross-referencing or copying to the local client. Is that even possible?\n\n\n@bblfish wrote\n10 Jun 14 But here we are not that far from the web as a distributed publishing mechanism.\n@WardCunningham wrote\n10 Jun 14 Wiki is an editor. ... My thesis, if I have one in this experiment, is that a federation of editors creates a medium, a landscape, within which selfish-blobs thrive.\n\nHere you write about the Thought Soup again, if I don't misunderstand.\nAs long as we aggree that the web platform itself is the modulary compound we're commonly designing, then our scope switches quickly from implementing open and standardized ecosystems to educate and go by example in self-dogfooding of already existing federation interfaces.\n\n\n@bblfish wrote\n10 Jun 14 I'll soon put online a new rww-play server which will show how one can use this for distributed Social Networks.\n\nFYI, @allmende has taken custody of this task to be released within the @ecobytes infrastructure.\n. Did anyone interested in Xanadu recently play with Bruno Latour's Inquiry into Modes of Existence?\n. Unfortunately their blog indicates the premature end of the software development.\n. @rynomad's former rosewiki.org is again rehosted, since a recent shut down of the OpenShift v2.0 free tier, at http://rosewiki.federated.wiki/view/ryan-bennett/view/recent-changes. If one takes the federation aspect thoroughly, secure (TLS, SPDY) authentication/authorization (OpenID Connect, WebID) of background tasks of that like might become a topic, too.\n. schema-heavy Yes, let's start there. Slowly.\nBecause from my view on the Semantic Web, it's ontology design is still quite an arguably criticisable experts system.\nNice anecdote, thanks for providing the 5 star context to TBL and sorry again for the improvised triple notation. Sure Turtle and N3 would make that more clear (to the experts :wink: ).\nWhat is a non-experts web anyway? The Social Web(TM*)?\nI doubt the term will not be coopted again, like so many before.\n* not to be taken seriously; no pun intended.\n. @coevolving is talking of Graphs for knowledge representation, which makes me think about a thin JSON-LD layer for federated wiki again:\nAs its data model is already purely JSON-based, it should be simple to write a very minimal vocabulary, in JSON-LD's language a @context, which explains the keys of a given dataset, here a fedwiki page, a sitemap or an export. Further, a dedicated plugin could take care of injecting neccessary links, like @ids and @contexts, if a site owner decides to let it do so.\nhttp://search.fed.wiki.org:3030/ gives us an idea about which items we may want to identify and publish (as seperate/# URLs) in a linked data format and thus have to be covered by a vocabulary definition of their meaning, itself expressed in RDF. Because then, by reusing existing terms and expressions, we understand wiki is not the place to publish everything from anew, but aggregating into the federation from a bigger web of data, in the same time contributing back to it without violating its autonomous architecture.\nIn a future with practical uses of the linked data publishing format we can imagine site owners to publish very diverse data via their loosely coupled wiki interfaces. Then we are all ready to contribute our part to the Linked Open Data cloud.\n. @elf-pavlik, can you comment about Persona's fate @WardCunningham is mentionning above?\nI explicitly ask you, as you are promoting Persona everywhere possible.\nIs it time for the transition to WebID already, @bblfish?\nIn general, I'm not so inclined in having Y-A-L-S: Yet another login system. In the end, right now, I favour Open ID Connect to anything else. There's also Node modules for both clients and providers.\n. @paul90 Let's just do it. We have the author of the piece at hands, if something goes wrong or doesn't work.\n. Quick via mobile/mail : we'd rather love to go for node. Maybe Ward can\nhave his rewrite there ;)\nAm 04.06.2014 17:36 schrieb \"Henry Story\" notifications@github.com:\n\nThe WebID specs are pretty stable now. You can find them here:\nhttp://www.w3.org/2005/Incubator/webid/spec/\nI have implementations in scala and have previously done some in Java. I\nam not so sure about the ruby tools. You may want to ask on\nhttp://www.w3.org/community/webid/\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/issues/415#issuecomment-45106916\n.\n. @bblfish There's two things now. As GitHub issues have limited moderation possibilities, I will fork out your complete rewrite topic quite soon and answer to that in a comprehensive manner. Short answer: We're already exploring this issue, but still lack a suitable place for discussion. Time will tell ;) .\n\nThere is a nice collection of authentication strategies by a Ruby project.\nLike databank or remotestorage provide interesting approaches for the storage layer, passport seems to provide something similiar for authentication in Node. There is an old, exhaustive overview of an implementation of WebID in Node available, too.\nI'd also be interested in knowing if WebID also means to upgrade our leveldb layer to levelgraph, or something suitable @elf-pavlik, to add some quads for improved overall Semantics.\n. There is another Web Identity Proposal, interestingly based on Telehash, that derives from the Web Payments Community Group and is called Identity Credentials.\n. You might enjoy the read of @bblfish's interview especially regarding philosophical implications of a federated web.\n. Hey @paul90, just as a sidenote : I always like your verbose and explanatory narratives. :tulip: \n. Then we need advice from @bblfish again.\nAm 14.06.2014 00:23 schrieb \"Ryan Bennett\" notifications@github.com:\n\nAh, I see I missed what was really going wrong vis a vis plugin\nfetching... For what it's worth, the pattern I use for federating page\nfetches works just as well fetching plugin JS and CSS (I fetch the JS and\nturn it into an Object URL and pass it to JQuery/css tag as appropriate).\nThat said, security is a definite concern when fetching JS and injecting it\ninto a page, but luckily NDN has signature and verification built in, so in\nthis case we'd need to come up with some sort of cert/web-of-trust\nmanagment strategy.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/issues/417#issuecomment-46066409\n.\n. Yes, for sure. How blatantly overseen. wik : wiki index keys. No, or maybe\nyes? So in fact the repository should already be distributed, I think. So a\nplugin repository page would then just be special places in the flat\nnamespace that are peer authentified or are offered for peer authentication\n- a little like OTR : I can only connect if I trust a site. So I will have\n  to think before. Hmm, so automatic would be better.\n\nI also like the idea of a centralized, trusted registry, but as GitHub\nshows, it's a somewhat diverging movement to the proposed decentralization.\nThen, I'd love to find better solutions. Have we had a look in\nhttp://cjdns.info/ already? How does it compare to he NDN/Telehash couple\nfor routing and authentication?\nAs Ryan mentionned; in fact everything should be encrypted and signed. Down\nto every commit to the journal, if you ask me. As the factory items remain\nthe factors (sic) for any refactoring; be it plugins. [ < Does that sense\nmake any sense in English language? Sometimes I have to reassure myself].\nSlightly OT: Therefore a metadata provider for a wiki page is also just a\nfactory with special content.\nOn 14 June 2014 02:06, phil jones notifications@github.com wrote:\n\nI suppose a stop-gap to fully automatic federated plugin sharing would be\nto have a standard package-repository for plugins (like npm) where we could\nall contribute them. Then any SFW owner who found they'd pulled a paragraph\nfrom another wiki in an unknown format would have a standard place to look\nfor the plugin.\nBTW: that wikish plugin is here :\nhttps://github.com/interstar/ThoughtStorms/tree/master/plugins\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/issues/417#issuecomment-46072245\n.\n. @paul90 wrote in the third comment above two-and-a-half years ago:\nThis is really an example of the lack of plugin discovery.\n\nThis is nowadays approachable with:\n\nhttps://github.com/WardCunningham/wiki-plugin-plugmatic\nhttp://fed.wiki/about-plugmatic-plugin.html\nhttps://github.com/WardCunningham/wiki-plugin-plugins\nhttp://fed.wiki/about-plugins-plugin.html\n. In a two iterations process, it should be possible to integrate a MarkDown flavour that supports tables, one should carefully select, into one of the editors I've mentionned for\nMarkDown parsing as a factory plugin.\nThen integrate the dialect.\n\nWould you be interested in collaborating on this one? As I'm not a real developer, this would be something like my first project; where I would like to have a little supervision or feedback.\n. Japp. Another quick hack would be to use plain old html (in one line))\nAm 10.06.2014 21:28 schrieb \"Paul Rodwell\" notifications@github.com:\n\nIf you are going to want to anything with the data other than simple\npresent it as a table then a alternative solution would be to extend the\ndata plugin, to provide an alternative presentation and a way of\ncreating/editing the data conten\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/issues/419#issuecomment-45660083\n.\n. Raw HTML is already available. Just make sure it is being typed within one\nParagraph. That's also the way how lists are being done these days, for\nexample.\nAm 10.06.2014 22:28 schrieb \"David Ing\" notifications@github.com:\nI would be happy with entering raw HTML, markdown, or even reusing a\nspreadsheet function if that were available. A grid of data is a relatively\nstandard way to present data.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/issues/419#issuecomment-45667108\n.\n. Thanks for the notes, @coevolving! That's a big deal for me.\n\n\nBelow some comments to the text; as we don't have annotations yet:\nI have to add to the discussion, that we have good experiences with Docker and Etherpad. As Wiki leverages on a much more simpler build process (sudo npm install -g wiki) it should be fairly easy to dockerize it. We're also playing with Hypothes.is or similiar tools that emerged from the Open Annotation W3C circles.\nIf you want to spare the time to install it, have a look at sites that run etherpad-lite. If you want to see how others integrate it into their environments, check the Third party web services that have support for Etherpad-Lite.\nBut right now I'm not so sure why conversations need a real time component - wrapping and versioning the paragraphs ends up messy again, I think.\nPlease also be aware that there are already some ideas, if not for comments, on Operational Transformation within Wiki https://github.com/WardCunningham/Smallest-Federated-Wiki/issues/194.\nOne last word to the mouse click behaviour : please let's consider freezing wiki pages, so I can easily select and copy parts of a paragraph without refactoring it. Then annotation would be possible. I admit the ease of dragging and dropping paragraphs is seductive, but it's not that often happening.\nAlso, sometimes, when I would like to copy a paragraph, I can only move it. The implied fork of a paragraph from a Wiki I don't own is also not correctly represented. A reload would only show the  result. That doesn't feel too natural to me, esp. between different wikis. A Ctrl keyboard shortcut for local wikis could be enough. A non moving paragraph at a remote page + a copy at the cursor during the event of a drag could be of great help.\n. Have you tried @interstar's http://project.thoughtstorms.info Quick Paste Converter?\nI believe it should still work, but I haven't tested it yet.\nThe resulting output can than be saved in Wiki's data folder and will be loaded if the page that decodes to its filename is being called.\n. What about wiki farms (--farm) here?\nI'm running one locally serving ~6 local wikis which are occasionally synced (either by file system/db sync, in this setup supposedly even by forking between the LAN and WAN instances run by a farm, or sooner or later mediated via browser) to public facing wikis.\n. IndieAuth OpenID delegation, which I tested on a fresh jon.fed.wiki.org domain, also didn't work.\nMaybe we can rename this issue and just call it \"OpenID claiming no longer working\" and add a checklist for\n- [ ] Google and\n- [ ] IndieAuth plus\nwhichever else doesn't work? We could then link to the provider-related issues from here.\n\nUntil then this issue is a duplicate of #415 \n. I'd happy to have a look at this task, as we already have Docker containers for Federated Wiki. We just need to design them in their preferred way.\nDo we already know about documentation about how to integrate applications into Sandstorm, especially regarding its autoupdate feature?\n@pierreozoux, this could be a thing for you, too.\n. We'll first have to go through the packaging tutorial step by step.\nTheir design guide lives in a how it works document.\nParallely we can already think of dedicating a systemd equipped VM, for @ecobytes preferably Debian 8, with a Sandstorm instance for publishing the result.\nIn a better world, their Dockerfile would already abstract the environment variables a bit better and we could use it instead. A common pattern is to use a docker-entrypoint.sh, for example. Unfortunately Ubuntu is an often used base image, which results in overall bigger images for containers.\n. ",
    "paul90": "\n@WardCunningham, you suggested last week creating a circle with the people who'd been on hangouts who you recognized and inviting only those. I've never done this, but it seems like a good plan if it's possible.\n\nThis would also seem to be a good idea, as the event will appear in the upcoming event list, and could be created well ahead of time.\nWhile it is not yet possible to create a reoccurring hangout using events. It is possible using google+ calandar, by creating a reoccurring event and using \"add video call\" to add a hangout - sadly you need email addresses to invite other, and it does not appear in the G+ event list. However, it does appear to use the same URL for each hangout.\nAt least one other project, I know, is using Hangouts on Air, to record and make available on YouTube their weekly discussions. See TiddlyWiki Hangouts.\n. Trying to start the hangout, as both Ward and Nick are away, and I am just getting an error.\n\n. Maybe some of ideas used over on substance provide some ideas/pointers, see substance internals.\n\nMaybe someone forked my site and made a clever addition.\n\nThere remains a discovery problem. How to know that somebody has forked a page from my site? While it will show up if you already have their site in your neighbourhood, if you don't the wiki currently has no mechanism to inform you which pages have been forked, and to where.\n. That should be http://rosewiki.org/view/wiki-version-control\n. > If I understand your situation, your server has never correctly served a plugin js file.\nNot with the express server (on Windows) - the sinatra server (also on Windows) has a different problem (see issue #349 ), but otherwise appears to work.\nLooking at the express server code, it looks as if the request for /plugins/factory/factory.js is handled at line 353 in server.coffee  - and if so this looks different to how the sinatra server is handling it (the code referencing meta-factory.js there is commented out, at least in server.rb)!\n. This may take some time - I'm tempted to write up the instructions for getting the Sinatra server version running on Windows first, as that seem to be the simplest route for those of us on Windows to get a working version.\nI'm going to have to revisit getting ws to install, I notice that there have been some updates on that over at einaros/ws since I last looked at this.\n. A quick update, not sure what my original problem with getting ws to compile was - it compiled with just some warnings about 'size_t' to 'unsigned int' and no definition for inline function 'v8::Persistent v8::Persistent::New(v8::Handle)' - I suspect I was having an environment problem similar to that some have raised over at einaros/ws (having system32 missing from the path).\nAs well as running npm install run in the client, client\\plugins, sever\\express directories it also needs to be run in client\\plugins\\linkmap and client\\plugins\\parse.\nWill add this to the windows readme over at #358 \n. Regarding the workgroup separated from the internet while it is something of an outlier, needing to be self contained. I can see this just being one end of a spectrum of connectivity (both internet and federation) - though how many points along that spectrum will see use?\n. issue closed to move commit to new branch, to remove block on master\n. That's not a problem.\n. Notes on installing and launching the Node.js Express server.\n. Not sure what happened with the pull request, there should having only been the one commit, and not all those old ones repeated.\nLooking at it this morning, it seems as if the pointers where different between github and my local - Git for Windows said they were sync'ed, but they very probably were not... (the references for upstream master (your GitHub) and origin master (my GitHub fork) were pointing to different places although they should have been the same - the origin master was getting left behind somehow). Using the command line to push the changes to GitHub seems to sort things out. Goodness knows how that happened :(\nGuess I will have to use the command line and stop using Git for Windows...\n. All about learning, and those we share the journey with.\nClosing the issue as it is now resolved.\n. I will probably have a brief look, in the first instance to see if this problem is restricted to running the ruby server on windows.\nGiven that the node server seems to behave correctly - the flag displayed is the one saved on disk, and the favicon plugin is not called. I'm getting a strong feeling that this may well be something about with running the server on windows.\n. We, it wasn't what I thought it might be. Not quite sure what is going on, but have just spotted that I'm getting the error message Image corrupt or truncated: http://localhost:1111/favicon.png in the firebug console, and if I access it directly I get much the same message in the browser.\nDeleting the favicon.png file will generate a new flag, that is saved to disk, but next time it is fetched from the server the problem returns.\nUsing a test page to download favicon.png from the server, all it contains is \u2030PNG - so something to do with reading the file??? Nothing obvious yet.\n. Yes, this is with the Sinatra version, on Windows.\nDigging a bit deeper, it has to be in server/sinatra/stores/file.rb - just tested it with CouchDB and it is not a problem. \nDoesn't the file read for blob data needs to be in binary mode? The PNG format contains a EOF character near the start, I expect that is causing the read to stop, and explains why we just get the first few characters.\nI will do the code changes, and test it out.\n. There are a number of case in the forums related to reading binary files - this issue only causes a problem on windows, and is solved by opening the file in 'binary' mode. \nThis only seems to cause a problem on Windows, so would explain why this has not cropped up before.\nNot seen any problems with / in paths yet - but I seem to be able to use them rather than \\ without problems in Windows 7. Problem with hopping between OS for years. But, will keep an eye open for any.\n. Unfortunately it been a problem for a very long time, with different browsers working in different ways. Though thankfully nowhere as bad as it was some years ago.\nIt would be good to support all the currently supported browsers - though it has been a few years since I took to good a look. It will however include IE 9 ( IE 10 has not yet been released for Win 7 - which is still out shipping Win 8).\nIn part this is behind adding in the Modernizr library - for checking feature support (not browser version), and being able to add in support where it is needed.\nNot sure how far back we may want, or need, to go back in any of the browser lines. Hopefully not too far, and definitely not back to versions that are no longer supported.\n. Currently 25 mocha tests fail with IE 9, but the same results with the master branch - though some of that may be because it is rendering the test page using 'quirks' mode.\nSome of the other errors look to be a know mocha issues, so will updating the mocha version.\n. Code to resolve this was rolled into #368\n. replaced by #368\n. The full error is:\nstream.js:81\n      throw er; // Unhandled stream error in pipe.\n            ^\nmodule not found: \"./lib/wiki\" from file F:\\GitHub\\Smallest-Federated-Wiki\\client\\client.coffee\nIf I alter client.coffee to window.wiki = require('./lib/wiki.coffee'), or if I use browserify.cmd on windows I get the following - though it seems to generate a slightly different error each time it is run :(\nF:\\GitHub\\Smallest-Federated-Wiki\\client\\node_modules\\browserify\\node_modules\\module-deps\\node_modules\\detective\\node_modules\\esprima\\esprima.js:3859\n            throw e;\n                  ^\nError: Line 8: Unexpected token :\n    at throwError (F:\\GitHub\\Smallest-Federated-Wiki\\client\\node_modules\\browserify\\node_modules\\module-deps\\node_modules\\detective\\node_modules\\esprima\\esprima.js:1156:21)\n    at throwUnexpected (F:\\GitHub\\Smallest-Federated-Wiki\\client\\node_modules\\browserify\\node_modules\\module-deps\\node_modules\\detective\\node_modules\\esprima\\esprima.js:1213:9)\n    at parsePrimaryExpression (F:\\GitHub\\Smallest-Federated-Wiki\\client\\node_modules\\browserify\\node_modules\\module-deps\\node_modules\\detective\\node_modules\\esprima\\esprima.js:1559:16)\n    at parseLeftHandSideExpressionAllowCall (F:\\GitHub\\Smallest-Federated-Wiki\\client\\node_modules\\browserify\\node_modules\\module-deps\\node_modules\\detective\\node_modules\\esprima\\esprima.js:1636:61)\n    at parsePostfixExpression (F:\\GitHub\\Smallest-Federated-Wiki\\client\\node_modules\\browserify\\node_modules\\module-deps\\node_modules\\detective\\node_modules\\esprima\\esprima.js:1695:20)\n    at parseUnaryExpression (F:\\GitHub\\Smallest-Federated-Wiki\\client\\node_modules\\browserify\\node_modules\\module-deps\\node_modules\\detective\\node_modules\\esprima\\esprima.js:1775:16)\n    at parseMultiplicativeExpression (F:\\GitHub\\Smallest-Federated-Wiki\\client\\node_modules\\browserify\\node_modules\\module-deps\\node_modules\\detective\\node_modules\\esprima\\esprima.js:1781:20)\n    at parseAdditiveExpression (F:\\GitHub\\Smallest-Federated-Wiki\\client\\node_modules\\browserify\\node_modules\\module-deps\\node_modules\\detective\\node_modules\\esprima\\esprima.js:1798:20)\n    at parseShiftExpression (F:\\GitHub\\Smallest-Federated-Wiki\\client\\node_modules\\browserify\\node_modules\\module-deps\\node_modules\\detective\\node_modules\\esprima\\esprima.js:1815:20)\n    at parseRelationalExpression (F:\\GitHub\\Smallest-Federated-Wiki\\client\\node_modules\\browserify\\node_modules\\module-deps\\node_modules\\detective\\node_modules\\esprima\\esprima.js:1836:16)\nI have no idea if it is a fault in browserify (or esprima) itself causing this, or something in the code here.\nStepping back to the suggested version of browserify resolves these problems.\nAlso for others using Windows, note that npm start and npm test don't work - you will need to extract the commands, substituting \\ for / and expanding the directory wildcard in test.\n. The only errors I am currently seeing in the Mocha tests are with Internet Explorer 9 & 10\n- \"should render a plugin\" - string does not match as the attributes in the link are in a different order (title coming first rather than last).\n  Error: expected <p>blah <a title=\"view\" class=\"internal\" href=\"/link.html\" data-page-name=\"link\">Link</a> asdf</p> to equal <p>blah <a class=\"internal\" href=\"/link.html\" data-page-name=\"link\" title=\"view\">Link</a> asdf</p> it would be possible to change this test to look at the DOM object, but...\n- report plugin - advancing \n  \"handles weeks\", \n  \"handles weeks with offsets (noon > now)\", \n  \"handles years with offsets (march < now)\", and \n  \"handles election day (election > now)\"\n  with the original test these all fail as IE uses a different toString format day of the week month day hour: minute:second time zone year ! Changed the test to use JSON.stringify to resolve.\n. branch brought up to date with the latest commits to master\n. The wiki has been updated, so others will not walk into that problem.\nSeems somewhat strange though - SFW works with node v0.10.3 on Windows. (v0.10.5 is now the latest)\nDo you know which of the 0.10.x version node-latest.tar.gz gave you?\n. Looks as if there has been a recent update over at substack/bouncy to get around the 0.10.x issue.\n. I have been been doing a bit more digging. Not only does the glob do calls like the one above, treating files as directories, it is also making stat call on non-existent files, on OpenShift these calls seem to take about 500ms!\nWhile I will create issue with glob about this, I am tempted to replace the glob.\n. Looks as if this is expected glob behavior - though to me it looks to be a fault. I will try to put together a test script as Isaac requests - though not sure how soon.\nI'm wondering if there is really a need to generate /system/factory.json each time a user creates a paragraph. It would seem that paragraph creation is way more frequent that adding/removing a plugin.\nMaybe it would make sense to generate a static /system/factory.json file as and when a plugin is added/removed/modified - and/or at server startup, as a catch-all - and server that rather than re-generating it each time it is needed.\n. Well, on my Windows machine it goes from 2.75s to 1.25s (but no SSD) - whilst on OpenShift from something like 13s down to 3s (though it is quite variable - probably to be expected for a free PAAS). \nLooking at the ruby code, I think I may have spotted something else, looks as if it brings it down to 39ms (on Windows). Bit too late to do any proper testing now, so more in the morning.\n. Thanks for pointing to the ruby code. Even ignoring the different behaviour of ruby dir.glob and node.js glob the search was not the same, ruby using client/plugins/*/factory.json and node client/plugins/**/factory.json. Using globstar is not needed as factory.json, if one exists, will the plugin's directory. So, the node glob is needlessly searching the entire directory below client/plugins/, rather than just one level below.\nThis makes a significant performance improvement, down to 145ms (on OpenShift), so the delay is barely noticeable. glob is still performing stat's on non-existent files, but no longer such a problem. \nThis change is in pull request #378 \n. There is an old Heroku blog on Upgrading to the Heroku Toolbelt. It should be noted that the gem was sunset on December 1, 2012.\nI am fairly sure that we are not using the gem to programatically access the Heroku API. So, it should just be a matter of removing the Heroku gem from the gemfile, and for those wanting to use Heroku to install the Heroku Toolbelt.\nI look briefly at this some months ago, but didn't get the time to test and update the documentation.\nOne other thing I spotted, which probably mainly impacts those of us using Windows, is that if the gemfile.lock contains any references to x86-mingw32 it gets discarded, which causes some other issues with version incompatibility. The simple solution, at the time, was to edit the lockfile to remove all reference to x86-mingw32, two lines in the specs section, and one platform.\n. Not sure why two old commits are showing here (69704b6 and b5e73e4) as they had previously been merged into the master branch here. In any case, the only change here is to server/express/lib/server.coffee\n. Just had a quick look, on Windows 7, the server crashes when I try to login (with either FF21, or Chrome).\nF:\\GitHub\\Smallest-Federated-Wiki\\server\\express\\lib\\persona_auth.coffee:58\n                  setOwner(verified.email, function() {\n                  ^\nReferenceError: setOwner is not defined\n    at IncomingMessage.<anonymous> (F:\\GitHub\\Smallest-Federated-Wiki\\server\\express\\lib\\persona_auth.coffee:58:19)\n    at IncomingMessage.EventEmitter.emit (events.js:117:20)\n    at _stream_readable.js:883:14\n    at process._tickDomainCallback (node.js:459:13)\nalso notice that if I use IE 10 that the login button does not get displayed.\nand, if I try editting an unclaimed site - which should work - I am getting browser display corrupted with Error on /page/welcome-visitors/action: Forbidden, and rejecting /page/welcome-visitors/action in the log. This looks a bit like a repeat of the problem we had earlier with the auth state being indeterminate. Should have some time to look at that later.\n. > This looks a bit like a repeat of the problem we had earlier with the auth state being indeterminate.\nWhile it looks, in server.coffee, as if like the app.get ///([a-z0-9-]+)\\.html$///, (req, res, next) -> section needs owner: owner adding to the info data structure, and I guess console.log 'req.session', req.session as well; that is not the solution to the problem.\n. No, that's not it. Forgot to say that regenerating client.js was one of the things I tried. So, still getting\n```\n[ 'welcome-visitors' ]\n[ 'isAuthenticated? owner=',\n  '',\n  { cookie:\n     { path: '/',\n       _expires: null,\n       originalMaxAge: null,\n       httpOnly: true } },\n  undefined ]\nreq.session { cookie:\n   { path: '/',\n     _expires: null,\n     originalMaxAge: null,\n     httpOnly: true } }\n[ 'isAuthenticated? owner=',\n  '',\n  { cookie:\n     { path: '/',\n       _expires: null,\n       originalMaxAge: null,\n       httpOnly: true } },\n  undefined ]\nF:\\GitHub\\Smallest-Federated-Wiki\\server\\express\\lib\\persona_auth.coffee:58\n                  setOwner(verified.email, function() {\n                  ^\nReferenceError: setOwner is not defined\n    at IncomingMessage. (F:\\GitHub\\Smallest-Federated-Wiki\\server\\express\\lib\\persona_auth.coffee:58:19)\n    at IncomingMessage.EventEmitter.emit (events.js:117:20)\n    at _stream_readable.js:883:14\n    at process._tickDomainCallback (node.js:459:13)\n```\nI also notice that the Mocha tests no longer appear to work - the browser window stays empty.\nThe problem with not being able to edit an unclaimed site is still there. \nHowever, if I forget to clear down ownership, by removing open_id.identity: -\n- if I am not logged into persona there is no problem with making changes and them getting saved into the browsers local store.\n- if try and login, I get the OOPS - this is not your wiki. The same as currently with OpenID. \n- if I am already logged in (from the failed login and claim above) I also get the OOPS error - this would cause problems in a wiki farm, preventing you from moving between wikis you own and those owned by others while logged in. Also raises a question about what happens when you go to an unclaimed wiki in the farm - probably need a separate claim button. \n. The site is already claimed, by you, so the main cases that I am reporting problems with can't be recreated there...\nGiven that the last commit, be97430, just contained updated client.js and testclient.js - and the problems appear to be server side - is there still some of the server code not pushed to github???\nOh, and if you are starting with a clean install, so an empty data directory, the server code now blocks the creating of favicon.png - but it also fails to clone welcome-vistors from the default data - this all points to problems in the server code. This is just with viewing an new, unclaimed, site - not got to login, and claiming, yet...\n. Not sure what happened to the post I added late last night, some thoughts about some of the problem edge cases, but...\nI think it may be heading towards a far larger discussion about the security model. It is probably best to take elsewhere so we don't overload this thread with the bigger picture.\nOh, and I forgot to say, it is good to see a Persona implementation here - it should have a permanence that some OpenID providers don't have, but that is another problem...\n. Starting from a fresh site - so no data directory\nI am getting Error on /favicon.png: Forbidden at the top of the browser window, and the following in the logs\nreq.session { cookie:\n   { path: '/',\n     _expires: null,\n     originalMaxAge: null,\n     httpOnly: true } }\n[ 'isAuthenticated? owner=',\n  '',\n  { cookie:\n     { path: '/',\n       _expires: null,\n       originalMaxAge: null,\n       httpOnly: true } },\n  undefined ]\nError: ENOENT, stat 'F:\\GitHub\\Smallest-Federated-Wiki\\data\\status\\favicon.png'\n[ 'isAuthenticated? owner=',\n  '',\n  { cookie:\n     { path: '/',\n       _expires: null,\n       originalMaxAge: null,\n       httpOnly: true } },\n  undefined ]\nrejecting /favicon.png\nThe data\\pages directory is created, but nothing else.\nIf I try and do a edit of the unclaimed site, I am still getting the Error on /page/welcome-vistors/action: Forbidden message, and in the logs\n[ 'isAuthenticated? owner=',\n  '',\n  { cookie:\n     { path: '/',\n       _expires: null,\n       originalMaxAge: null,\n       httpOnly: true } },\n  undefined ]\nrejecting /page/welcome-visitors/action\nIf I sign in, the favicon file is saved, also as the site is now claimed the edits appear to get handled \ncorrectly - to the site, if authenticated; and to local data is not.\nSigning out appears to have little effect - other than changing the button back to \"Sign in...\" - as any subsequent edits are still getting saved back to the server, with the following in the log\n[ 'isAuthenticated? owner=',\n  'xyz',\n  { cookie:\n     { path: '/',\n       _expires: null,\n       originalMaxAge: null,\n       httpOnly: true },\n    email: 'xyz' },\n  true ]\n[ { type: 'edit',\n    id: '63ad2e58eecdd9e5',\n    item:\n     { type: 'paragraph',\n       id: '63ad2e58eecdd9e5',\n       prompt: 'Create a page about yourself. Start by making a link to that page right here. Double-click the gray box below. T\nhat opens an editor. Type your name enclosed in double square brackets. Then press Command/ALT-S to save.',\n       text: 'Test - signed out' },\n    date: 1372240300855 } ]\n[ 'saved' ]\n[ 'isAuthenticated? owner=',\n  'xyz',\n  { cookie:\n     { path: '/',\n       _expires: null,\n       originalMaxAge: null,\n       httpOnly: true },\n    email: 'xyz' },\n  true ]\n[ { item: { type: 'paragraph', id: '1412cd66b4611005', text: '' },\n    id: '1412cd66b4611005',\n    type: 'add',\n    after: '63ad2e58eecdd9e5',\n    date: 1372240301372 } ]\n[ 'saved' ]\n[ 'isAuthenticated? owner=',\n  'xyz',\n  { cookie:\n     { path: '/',\n       _expires: null,\n       originalMaxAge: null,\n       httpOnly: true },\n    email: 'xyz' },\n  true ]\n[ { type: 'remove', id: '1412cd66b4611005', date: 1372240302174 } ]\n[ 'saved' ]\nLooking at the browser state, I don't see the 'sign out' doing anything - the same cookies are there to be set with the next request...\nOn a more cosmetic note, it would be nice to put a vertical-alignment on the sign in image, middle seems to work best. Also not sure about having the email address there, but imagine that is just for testing...\n. @ozten what I am doing for manual testing, though I guess it could be automated, is to go back to a pristine state - so there is no data directory - so nothing from the OpenID state can interfere. So, when I first connect to the site I would expect, because the favicon does not exist, for the client to create one and save it back to the site (the ruby server is different at this point, as there the favicon is server created).\nFor the favicon, I don't think this has anything to do with the session management, but rather that the code no longer caters for an unclaimed site - think the changes to authenticated  in server.coffee is very probably the cause.\nIn many parts of the code the isAuthenticated is really should be asking if the user isAuthorized, which with the current security model will depend on if the site is owned (if not, then we don't care who the user is) and only if it is owned asking if the user isAuthenticated and the owner.\n. @ozten - good to meet with you on Friday.\nIt would be good if somebody else could try this new scheme out, as some of what I am seeing does not appear to be consistent - so there is something I am not spotting.\nLooking at this again - starting with a new site, creating the favicon and claiming all appears to be OK.\nAm getting some very strange problems else were - having signed in, with persona, edits are still going to local storage - waiting does not seem to resolve this, you mentioned a delay? - only way this seems to resolve itself is by reloading the page. Looking at both the OpenID implementations (node.js and ruby/express) I see they are doing a redirect to the site index as the last step of the authentication.\nSimilarly, sign out still does not really appear to sign me out fully - most of the time the server appears to get it, but the client still thinks I am signed in! So, I get rejecting/forbidden messages in the browser, and server log. Reloading the page seem to resolve this most of the time, but not always. It looks as if the local session cookie is not consistently getting updated...  Again looking at the OpenID implementations they are doing a redirect to the site index on sign out.\nI had a quick look at your unclaimed site - see the testing page - I'm not sure that the sign out behaves the same as on my local test site, as it still thinks I am signed in even after reloading the page.\nFor both the sign in and out, I think it best to redirect to the site index page as the last step - this will become more important going forward as more complex authorization models are added, and will ensure that the user will see the correct content.\n. Not too sure what has happened, as long as the site has already been claimed there doesn't appear to be a problem - logout now works correctly.\nHowever, if the site is not claimed, the page load falls into a loop - have not yet managed to workout what is causing this. The following is grabbed from firebug, showing the network calls.\n\nWill probably get a chance to look at this later.\n. This looks a bit like Problem with Persona and Safari Web-browser on Windows (users unexpectedly and incorrectly logged out!), though of course here the users is not yet logged in!\nLooking at the Persona issues, there seems to a number of other issues that may be related - but it looks as if putting the page refresh is being done in the wrong place.\nWhile looking at this I notice that this is not working with IE10 - the sign in button does not get displayed, unless I have the developer tools running! Most strange.\nMore later....\n. Still appears to be looping with a clean install :(\nAdding an alert to onlogout shows it is getting called, and causing the loop. Still can't see where this is getting called from... Adding a test to see if the owner is not null, while stopping the loop also prevents the logout from working....\n. Getting more confused - if I try debugging this with FF, and Firebug, stepping through the code - the page does not get reloaded, but the \"Sign In\" button does not get displayed either. If I remove the breakpoint in Firebug, then it goes back to looping through the page reload.\nI am getting exactly the same with Chrome, but not with IE10 which loads the page but doesn't display the \"Sign In\" button.\nFor the unclaimed site, owner = '' when the call wiki.persona(owner); is made in static.html - that looks right.\nAs I am seeing different results depending on if I am running the dev tools, or not, I'm beginning to wonder if this could be a timing issue - or something wrong with the generated javascript?\nLooking at the loggedInUser docs, it would seem that '' might be getting treated as undefined? Which would trigger a logout...\n. Sorry, ignore that - looks as if fetching from upstream is no longer fetching the tracking branches for the pull requests - not sure why I didn't spot that earlier - looks as if the last merge into my branch is from the 2nd.\nLooks as if the recent Github for Windows update has changed the config  - will sort this in the morning and the pull request branches are no longer getting fetched...\n. Just a quick update - managed to sort my git config issue.\nThe latest patch fixes the looping problem with a fresh site.\nThe only remaining issue I am seeing is with IE10 - the problem is that onmatch is not getting called, so the sign in button remains hidden - seem to be some related browserid issues - looks to be tied up with p3p issues.\n. Probably need to go into the MDN wiki and split onready out of onmatch, didn't spot that event either.\nLooking good - but, it would be a good idea to merge the latest changes to master into this branch - already done on my testing branch. The only conflict is the generated js, so run the grunt script to regenerate...\n. Looking back through the comments, I'm sure I commented on the vertical alignment of the Persona sign-in button. It would be good to tweak this, so it aligns better with the search box. So just a one-line css change.\n. > But, hey, the node/persona version still works. How can this be?\nI have noticed that if you delete status/open_id.identity while the server is still running it looks very much as if the contents has been cached somewhere - as the server carries on as if it is still there. So, if you have both servers running it might be possible to have them both think you are the owner with different credentials.\nNot too sure about splitting the repos, as the client code is shared. Though splitting the plugins out into their own repos will probably help encourage the development of more.\n. Looks very nice. You should also be able to remove both build.bat and build-test.bat.\n. I had meant to pull together the various security thoughts here, or using the fed wiki, so that the switch to Persona could remain focused. Unfortunately, I got sidetracked so that didn't happen.\n. This sounds like this could be an extension to the reference plug-in.\nWould need to add the ability to let a site know that some content is being referenced, something similar would be nice when a page is forked. Might need to be moderated to prevent linking to spam.\nUsing the selected text will probably only work if this is being done within a neighbourhood - so not across browser tabs/windows - but could always use copy & paste to work around that. \n. Too minor? Never. If anything it highlights a known problem that the documentation is not all it might be, and there are differences between the two server implementations that are not documented.\nYou might want to look at the recent configuration changes over in at WardCunningham/wiki, where most of the current development is happening. There is now support for a configuration file, like local-identity.\nThe node/express server does not currently automatically enable farm mode if the farm folder exists. Maybe it should? For the time being this has to be done via the configuration. \nAs far as I remember the -u '[DOMAIN]' parameter is only used as part of the authentication process, at least in the node/express code.\n. Rather than repeat myself, please see https://github.com/WardCunningham/wiki/pull/23#issuecomment-24465370\n. The refactoring of the node version is now available, as an installable package. See paul90/wiki-exp.\nThis must still be considered a work in progress, the code for both the client and server is contained in the paul90/refactor branch of my forks of the wiki-client and wiki repositories.\nThose wishing to try this new version can install it globally with npm:\n$ npm install -g wiki-exp\n$ wiki-exp --data ~/.wiki\nN.B. A data path should be specified when starting the wiki, see WardCunningham/wiki#41.\n. Thanks, it didn't start out that way. \n. @WardCunningham can you try again - looks like a line ending issue. I have changed the line endings in bin/server.js and republished, which should resolve the problem.\n. That's good.\nThere are a few recent updates that have not yet made it across. I \nshould get to them soon.\n\nWard Cunningham mailto:notifications@github.com\n17 December 2013 16:08\nUpdated to version 0.1.1. Works great. Thanks.\n\u2014\nReply to this email directly or view it on GitHub \nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/issues/403#issuecomment-30763908.\n. I think the following steps are needed to complete this.\n\nRather than creating new repositories, we will transfer the existing GitHub repositories to the fedwiki organization. \nGitHub will redirect to the new location - as long as we don't create a new repo with the same name - see \nhttps://github.com/blog/1508-repository-redirects-are-here, and https://help.github.com/articles/renaming-a-repository\nI propose to create an issue in each repository with the relevant checklist. \nFor each wiki-plugin-*\n\n[ ] Update package.json to include new repository and issues url\n[ ] Update ReadMe.md\n[ ] increment patch version\n[ ] move to fedwiki org.\n[ ] remove github wiki, if not already done.\n[x] add ward as an owner of the package\n[ ] republish to npm\n\nfor wiki-exp\n\n[ ] review the documentation, and update\n[ ] move to fedwiki org, as wiki, or to wiki-npm so like wiki-gem???\n\nThis will replace the existing wiki npm package, so need to get all the docs to reflect this, including details on how to contribute (both to the existing components, and also writing new plugins).\nA few more steps that are needed once wiki and wiki-client steps are completed.\n- [ ] ensure package.json is updated, using npm rather than git to install the wiki-server and wiki-client packages, as well as new repository and issues url\n- [ ] publish to npm - this is the last step, as it will be published as wiki.\nfor wiki\n\n[ ] review documentation, and update\n[ ] move to fedwiki org, and rename as wiki-server\n[ ] merge refactor branch into master\n[ ] Update package.json to include new repository and issues url\n[ ] publish to npm, as wiki-server\n\nfor wiki-client\n\n[ ] review documentation, and update\n[ ] move to fedwiki org\n[ ] merge refactor branch into master\n[ ] Update package.json to include new repository and issues url\n[ ] publish to npm\n\nThose who have forked the wiki and wiki-client repos on GitHub will need to update their upstream remote to use the new URL - though as we transfer/rename the repos they will get redirected. See https://help.github.com/articles/how-to-transfer-a-repository#redirects-and-git-remotes\nHow best to handle issues? As the many of the components (client and plugins) are used by both server \nimplementations, I suggest that we central issues repository. The Smallest Federated Wiki repository would seem to be a good place.\n. A long list of referenced issues, one for each plug-in. So now just a matter of working through them all...\n. All the plug-ins are now migrated over to the fedwiki organization. I have created an issue for those without an about page (but may have missed some), but documentation is best described as minimal. \n. Some very early notes on working with the new repositories, see http://wiki-paul90.rhcloud.com/working-with-the-refactored-node-code.html\nAs long as they make sense I would like to push on the final step...\n. I should have said, those notes were written from the context that the refactor was complete, and the new/updated packages had all be published.\nThanks for your comments Ward. I have added some notes on using the 'wiki-exp' package, which grabs the refactored packages from github, until the updated 'wiki' and 'wiki-client' and the new 'wiki-server' packages are published.\n. Yesterday (25th Jan), Ward and myself worked through the final stages of getting the refactored code merged in, and the the associated npm modules published.\nN.B. Before updating to use the latest version you should ensure you have a backup of your data directory. For those unsure where it is, when you run the wiki the parameters are displayed db points to where your pages are stored. If the directory pointed to by db is within a node_modules directory, if you don't backup your data it will be lost when you update the software.\nFor those who installed the npm version, using npm install -g wiki, simply need to run npm update -g wiki to upgrade to the new version.\nThe latest version will, by default, use ~/.wiki to store data. If your data has been lost in the upgrade, you will need to restore the data you backed-up above into this folder.\nThose working with the git repos will find some notes over in fedwiki/wiki-node.\n. Looks as if this is a stylesheet related problem.\nA quick temporary hack is to drop into developer tools and edit the node <link href=\"/style.css\" rel=\"stylesheet\" type=\"text/css\"> in the header to read <link href=\"/style.css\" rel=\"stylesheet\" type=\"text/css\" media=\"screen\">\nThis could also be applied to server/sinatra/views/layout.haml, and the equivalent template file in the node version.\nIt would also be nice to have a print stylesheet to provide some better formatting when pages are printed.\n. We talked briefly about this during today's hangout.\nI have a print stylesheet that will start each new page on a new piece of paper, also hiding some of the page elements (twins and journal), and the screen footer.\nI think I will publish this, on in the node version, in the next few days. On the basis that that having something will foster some discussion, and should be useful.\nThe output is currently single column, but it should be possible to flow into multiple columns and vary this on paper size and orientation (using media queries).\nSome other articles that are of interest:\n- CSS Design: Going to Print Eric Meyer (2002),\n- Building Books with CSS3 Nellie McKesson (2012),\n- Improving Link Display for Print Aaron Gustafson (2005)\n. Interesting idea\nWhy limit it to being a read-only copy? Having the wiki data embedded is potentially only a view, it could be stored as pages in a sub-site (which would provide an interesting option of those not wanting to, or unable, to run a farm).\n. That issue you are thinking of is #42 \nThinking about that example. Another possible solution is if you are only ever intending to write to your local private copy. Then the public facing copy could be a static view. Advantage it would only need a dumb server, populated with the content the client expects (pre-rendered html, raw page json, sitemap, etc\u2026) BUT the current URL presents a issue as with the client rewriting it, it does not point directly to something. See #412 \n. Currently the OpenShift quickstart does not include the code to config federated wiki so that it starts in farm mode. While in theory this should be simple, I think I have spotted a problem in how the farm starts, it does not appear to pick-up the configuration correctly. So, will take a little investigation.\nOnce that is sorted, given the way OpenShift works, each of the alias will need to be manually created so that its proxy knows where to route requests. So, creating a new wiki within the farm requires a manual step.\n. Farm support is now available in paul90/wiki-openshift-quickstart, ~~at v0.2.3~~ v0.2.5\n. I believe, from a number of comments in support, this problem is caused by OpenShift using an old version of npm. Though it does appear to be intermittent, or maybe the gears are hosted on different machines!? I currently have one gear that installs without a problem, and another that doesn't.\nThe quick solution is to use npm shrinkwrap, see https://gist.github.com/paul90/5a6a02e1151055a2580a. Just add this into the root of the repo, commit and push up to your gear.\nThe longer term solution is either to add the shrinkwrap file, to go back to building a more up to date version of node (or just npm) in the gear, or see what happens to the request to update the version of node on OpenShift.\n. It might be an idea to add your vote to https://www.openshift.com/content/please-update-npm-version as this represents the best solution.\n. The OpenShift quickstart now includes the shrinkwrap file, so the above steps no longer need to be taken.\n. Not sure the shrinkwrap is the answer, just doing some testing and it fail installing glob - needing a later version of npm, or probably more correctly semver.\nLooks as if we will need to build a later version of node as part of the gear - not ideal.\n. I have reverted to building the wiki OpenShift quickstart off the custom node version project.\nThis is now over at https://github.com/paul90/wiki-openshift-quickstart - as v0.2.5 - which uses node.js 0.10.28 and includes configuration to enable farm mode (off by default).\nI have deleted the gist mentioned above, as it is no longer required.\n. @coevolving,\n(1) In normal mode, self.url = \"http://\" + process.env.OPENSHIFT_APP_DNS; is used as the return address from the Persona login. This will only be correct if you are not using aliases. So, in your case accessing the site as http://fed-coevolving.rhcloud.com.\nThere are three possible solutions, either: -\n1. (a) above - which is not idea, or \n2. edit server.js so that self.url = 'http://fed.coevolving.com', or\n3. use farm mode, this will make fed.coevolving.com and fed-coevolving.rhcloud.com two sites - though content, or code, could be added to redirect people to the former from the later this would probably be a useful feature to add N.B. each site has its own data sub-directory so you will need to move any content to fed.coevolving.com's sub-directory (pages and status).\n(2). Strange, this is in the home directory of the repo you push up to OpenShift? Alternatively just create the file, using a texteditor. I tend to put the date in the file, just so I can see at a glance when I last performed an update.\nAnd, I still have to reflect Nick's changes into the OpenShift version, while this will alter server.js to bring in the changes that were made in fedwiki/wiki-node\n. > Over at issue #414, there's a peril at claiming the web site using a Gmail address right after installation on Openshift (e.g. http://fed-coevolving.rhcloud.com/ ) and then setting up the CName and alias (e.g. http://fed.coevolving.com/ ).\nThis is more an issue with how the return address to be used when signing in with Persona is configured. The value is set when the server is started, so causing problems when multiple routes can be used to reach the server. This is avoided in farm mode, as node bouncy is used, and each access route gets its own configuration.\n\nIs it time for the transition to WebID already, @bblfish?\n\nThis is something I was talking about, with Austin, when we added Persona to the node server. In conjunction with WebAccessControl, for those wanting to control access to content. Can that really be a year ago, where does the time go!? Sadly I never got around to writing up my thoughts.\n. > Perhaps we can assume that the first person to visit a new wiki is in fact the owner. The server could generate a suitable credential and share it with that person's browser. This completes the \"claim\" process. Let's call this the zero-steps to claim solution.\n\nThe primary advantage of zero-steps is that it just works without any explanation. No entry fields or buttons are required. No identifying information need be offered. No service dependency either.\n\nInteresting idea, not sure how this fits with the idea of leaving some site unclaimed in a farm. But, maybe adding the role of farm manager with the ability to configure such things. Or how you recover from the loss of the shared credential.\nOther than we were discussing how to restrict read access, @bblfish that sounds almost identical to a conversation I was having with @ozten, while we were adding Persona support a year ago.\nAs @WardCunningham commented at the time, probably the biggest barrier for WebID is the requirement for server certs, especially for the wildcard cert that a farm would require. A great shame that all the noise about the problems with the CA trust model has not really delivered.\nMaybe part of the answer with farms is to single sign-on to the farm, so offering a way of removing the need for a wildcard certificate. Or, to switch from using sub-domains to a url structure. But, while ward.fed.wiki.org could easily become fed.wiki.org/ward, what about 623633.ward.fed.wiki.org? Maybe not such a good idea!?\nAll that aside, we probably need to pull the security out into a module as a first step. There is currently an inbuilt assumption that only the site owner will authenticate, anybody else gets an error message, and that being authenticated which in the current model means being the site owner also means you are authorized to edit.\n. > Inspired by the broken federation between two already existing Wikis, as seen here and there\nThis is really an example of the lack of plugin discovery. @interstar (Phil Jones) has written the wikish plugin,\n```\nWikish 0.1\nPhil Jones\nWikish is a wiki markup, derived from UseMod's and adapted for my earlier personal wiki \nsoftware : SdiDesk.\nThis is the beginning of my adaptation of it for Smallest Federated Wiki, so that I can \nport my existing wikis to SFW\n```\nAs this plugin exists on thoughtstorms.info, but not fed.wiki.org you see the error TypeError: Can't find plugin for 'wikish' once for each wikish item on the page.\nWhat is required is for the client to recognize when the origin server does not have a plugin that is required to render a page (do we also need to worry about plugin version?) had have some mechanism to reach out and find it. A question is this something best done by the client or the server. Having the server build a library/cache of none core plugins used on pages it is serving would be an attractive idea, and protects against an original server becoming unavailable. Though this would only be for client side only plugins.\nOh, and the page load with the errors is slow, as the request are made sequentially and you are using  Ward's server, the requests of the missing plugin get made serially (speed is determined by the speed of the origin server) - so it only gets loaded once - but there is currently on error handling/recognition so the same request is being made repeatedly. Something that needs fixing.   \n\nnot even mentioning ageing wikis\n\nOr just broken in a different and interesting way - a couple of 500 and 404 errors, one of those 500 errors being for the stylesheet. As long is able to server the page json, the way to view the site is to use a different server as the origin, so something like http://wiki-paul90.rhcloud.com/theoutpost.io/the-outpost\nAlso worth reaching out to @hallahan so he know there is an issue with his server.\n. Something feels wrong, I wonder...\nRather than have different plugins, and content types, to provide editing with different markup. Wouldn't it be better to use a single content type paragraph for text content, with a common markup (lets say a sanitized sub-set of HTML) and, have editor plugins that provide an author selectable editor with their preferred markup?\nThis would mean the having a separate sanitized HTML content type might not be needed, as we could have a raw editor for that. We would have to provide a framework to support this, and allow the selection of which editor the user prefers, but then 'Editor' plugins could then be developed to support the different markups.\nAlso connect with #419 , fedwiki/wiki-client#7 , and very probably others...\n. No, nothing to do with content being static. I am suggesting that for 'text' content rather than creating a different container for each different authoring markup that a different approach of having, user selectable, editors which perform a round trip from a common markup to the markup that the user is editing in, and back.\nThere currently seem to be three, maybe four, different markups being talked about within the community. Imagine a page that ends up with text content marked up with a number of different markups and the effect that has on anybody wishing to contribute.\nFor the current notes on plugin development, see http://plugins.fed.wiki.org/view/make-a-new-plugin/view/make-plugin-script\n. If you are going to want to anything with the data other than simple present it as a table then a alternative solution would be to extend the data plugin, to provide an alternative presentation and a way of creating/editing the data conten\n. The server port is set using --port or -p\nThe Persona audience is set using --url, which you will need to set as this defaults to http://localhost\n. Not sure, as I don't use pm2, but I think you will want something like\npm2 start path_to_fedwiki/index.js -- -p 80 --url http://site.i.want.example.com\npm2 parameters will need to come before the --\n. Just tried pm2, as far as I can see you need to remove the app from the process list before starting it with different parameters, using pm2 delete, but it appears to work as expected.\n. The node version should be considered to be the current version of the server. The ruby/sinatra version is currently not being actively maintained.\nCurrently the simplest way to get a server up and running is Deploying a wiki.\n. There looks to be a know, long running bug, with Chrome - see Issue 166593, might be related.\n. Which browser are you using? The screen shot looks like Chrome.\nAs much as I hate to suggest it, have you tried a different browser? As all the reports seem to indicate that this problem is only seen with Chrome.\n. will also freeze with other localStorage keywords\n. Not seen it mentioned anywhere either, but there are some hints. At least one question points to DOM Storage guide\n\nNote: Although the values can be set and read using the standard JavaScript property access method, using the getItem and setItem methods is recommended.\n\nI also noticed that trying to follow the link to 'clear' also empties localstorage, which is a bit of a give away. Although the error in the Firefox console points to JSON.parse.\n. Of course localStorage[\"clear\"] is the same as localStorage.clear which clears localStorage, which is a wee bit of a problem.\nOther than the recommendation quoted above, most of the JavaScript texts seem to freely mix the use of setItem/getItem, square brackets, and the dot notation with not a word of warning of the trap that is awaiting.\n. sandstorm.io was mentioned in yesterday's hangout\n. Favourable, though I'm not sure how much depth it has been looked at yet, or if anybody has it up and running yet - from a cursory glance it looks interesting.\n. @opn I don't think so, I only ever wilderness camped...\n. From memory, you will at the very least need to create a custom version of the wiki-node package - sandstorm appears to put the package at root which causes path.join to fail with not a string error. \nAlso you will need to move the data directory into /var - as that appears to be the only place that is mounted r/w.\nNot too sure if farm mode, with dynamic creation of wiki's, will work. While it is documented how to point a single domain at a grain, I very much doubt that it will work with a wildcard.\nAs far as I can see sites running on Sandstorm are only accessable over HTTPS - so you will need to proxy Sandstorm to make it available without HTTPS.\n. should not be hard coded - there is already a parameter for this, use \"#{argv.u}\"\n. the original version of this, with OpenID, should have more correctly been called authorized\n. in the original the user was also authorized if the site was unclaimed.\n. ",
    "andyburnett": "I would support that idea a lot!\nCheers\nAndy\n\u1427\nOn Mon, Nov 18, 2013 at 3:49 AM, Paul Rodwell notifications@github.comwrote:\n\n@WardCunningham https://github.com/WardCunningham, you suggested last\nweek creating a circle with the people who'd been on hangouts who you\nrecognized and inviting only those. I've never done this, but it seems like\na good plan if it's possible.\nThis would also seem to be a good idea, as the event will appear in the\nupcoming event list, and could be created well ahead of time.\nWhile it is not yet possible to create a reoccurring hangout using events.\nIt is possible using google+ calandar, by creating a reoccurring event and\nusing \"add video call\" to add a hangout - sadly you need email addresses to\ninvite other, and it does not appear in the G+ event list. However, it does\nappear to use the same URL for each hangout.\nAt least one other project, I know, is using Hangouts on Air, to record\nand make available on YouTube their weekly discussions. See TiddlyWiki\nHangouts http://www.youtube.com/user/jermolene.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/WardCunningham/Smallest-Federated-Wiki/issues/88#issuecomment-28682427\n.\n. \n",
    "rynomad": "Sorry for having gone AWOL for so long... I started my first class during this summer semester and joined yet another band, but I'm still hacking away at NDN + wiki when time allows :) hopefully be able to make it back with regularity once I settle into my new schedule\n. For anyone still interested in this problem space, I've been doing some thinking on version control in Wiki:\nhttp://rosewiki.org/view/wiki-version-control\nTL;DR It looks like we're building a version control system in Federated Wiki: Let's abstract it and put some deep thought into what we need it to do (and what we don't need it to do)\n. Thanks Paul!\n. I'm a novice developer (I know little more than Ruby parts of speech) but I'm running into this problem as well and plan on tackling it once I get a better grasp on both the language and the workings of SFW. \nIn the meantime, I found a workaround to get proper JSON images into my own wikis: \ncreate an html file with a set of images you'd like to import and use html-json.rb (https://github.com/JoeBergin/Html-to-JSON-batch-converter). Copy the resulting JSON file into pages folder in SFW. create a link to it in your wiki and you should get a bunch of image objects that you can drag into the relevant pages.\nI've also submitted a brief description of this workaround to the [[How to Wiki]] page on new.fed.wiki.org, though perhaps it would be better served as a screen cast detailing the process? I'd be happy to make one\nI'm running Ubuntu 12.04 + Firefox and a lot of the drag-n-drop/plugin functionalities don't work quite right at the moment, so until I progress further away from padawan and closer to jedi master so as to fix the underlying code I'm just figuring out how to do things manually... I'd be happy to document these workarounds as I find them; Thoughts/requests?\nSorry if I've rambled a bit outside the scope of the OP. Really love this project btw, I'm finding a lot of cool use cases. Thanks to everybody for their work.\n. Thanks for the pointer! I've found the problem and a possible solution...\nProblem: upon drop, the factory checks for dt.types: \"text/uri-list\" or \"text/x-moz-url\" (found in safari and firefox respectively) and then starts treating the datatransfer like a federated-wiki url (I think). When that doesn't work, it punts.\nPossible solution: add another condition in the if statement above to check if dt.types 'Files' exists. It does, so it passes on into file reader and the image is uploaded. This is working swell for my setup, though I wonder if it wouldn't be better/cleaner for drop logic to be handled by switch/case rather than nested if statements, especially as we start handling more objects and dealing with more browser/OS combos. Newbie thought, I'm sure.\nNote: I still can't get drag and drop to work between tabs in firefox: tooled around with it for a while, not sure if this is another browser/OS issue or if it's just not part of the code yet. It seems to almost work, but even though there\u2019s a 'Files' type when I drag from say, facebook, dt.file reads null. Though I do get a url to the file, I don't know how to turn that url into a file that I can feed filereader. ( xmlhttprequest looked promising, but I'm really in over my head at this point... maybe if I sleep on it).\nIf my solution sounds like it won't break something, let me know and I'll learn enough about git to do a pull request. \nPS Thanks for the link to Obscure workarounds. Would it be appropriate to submit future hacks there, or should I just post on the issue threads here on github? I don't want to be spamming your wiki, though at the same time one of my favorite things about this project is that it really seems like it can be developed from the inside out, organically.\nCheers.\n. Done and done. Interesting concept for the drop logic, waaay over my head, but good thought-food.\nthanks for the heads up on tab vs url. What I've been trying to do is drag an image from some website (eg the logo from wikipedia). I tried dragging the url but of course that's just another unexpected item. though that point did help me wrap my head around the federated link workaround, so thanks!\nI followed your advice and forked ObscureWorkarounds, but it'll be a minute before I can send you a location; my setup is a bit of a hack running off my laptop. I'll be in touch once I get my dedicated server/DynDNS all squared away.\n. Looks good! If you have anything hosted with this I can take a look at it in android and let you know how it feels from there\n. Yup, this short. sorry if my English verbosity led you to believe otherwise... the only thing that was going wrong was that the drop was being passed into the code to generate a reference first thing, and then since it was an image it would fail and punt, so I just made it check that there were no files in the data transfer before passing it down. I suppose it could be done with a new clause that checks for the specific combination first and then executes accordingly.\nUltimately it seems like it would make sense to have some sort of browser/UI-check at the beginning of client.js, passing the result into a simple variable and then using that throughout the code in various top level conditionals. It doesn't make much sense now, but it might make things simpler down the road. Holistic solutions to specific problems, yeah? \nInterestingly enough, the 'drag url into factory to create reference' doesn't work in the first place on UB/FF so I really don't know if I'm breaking something for other browsers, my hunch would be that a location field drag is unlikely to contain a 'Files' type in any scenario, so we should be good. I'm going to work on tweaking the code  to actually make those references for me (in my browser the location field drag is nothing but plain text so it doesn't get handed to the reference-maker in the first place). While I'm at it I'll explore browser/OS checking and do some reading on how to code for widest compatibility.\nI edited the javascript, which I now realize to be a faux pas... I basically was teaching myself js as I worked the problem and using coffeescript almost as comments more than as code. I'll do some research into coffee and re-render the solution/a better solution thusly.\n. same fix via coffeescript, still looking into alternatives for browserchecks etc.\n. I keep trying to look into best practices for browser compatibility, and keep getting distracted by new ideas for other things (ccnx, tent.io, and meshnets oh-my!). I like the idea of collecting error data from every failed attempt, either manually or in the form of code that automatically pulls the browser/version/OS data from visitors, or half-n-half (detailed coding info + a users description of error would be very nice). \n((This is where I practice google-fu for a moment))\nI found this Browserdetect script http://www.quirksmode.org/js/detect.html\nThough in the first paragraph the author advises against using it to code for compatibility: http://www.quirksmode.org/js/support.html \nWhat we run into is less about the browser supporting something we want to display and more about anticipating how the browser is going to present something the user wants to insert, so I'm on the fence about how much salt to take all this with, but in the interest of collecting data I'd say put the browserdetect inside of the punt definition, capturing the results as we do with \"Types\", and turn \"unexpected Item\" into a hardlink to a page on one of your wiki's. On said page, instruct user to move the punt over, type a short description of what they were trying to do, and [[Submit Changes]] If the user flow works as it does in my hand, every time someone encounters this error you'll get a submission that gives you a lot more information than what we're currently getting, and you'll probably find that by staying within the context of the wiki, more failures will be recorded than there are now, since folks won't have to come all the way over to github to tell us about it.\nWhew, a little long winded on this one. I've been procrastinating far too long on this, as well as on some content writing for a little syndicate I'm trying to form on my SFW farm. I'm going to get to the latter, and then I'll try and test the former for you. If I find the user flow and the collected data to be as described, I'll post a link and you can decide whether you'd like a pull request.\n. Update: Got rudimentary browser-detect working inside of the punt, and unexpected item is now [[Unexpected Item?]].  though the factory plugin is now... less than elegant; I just tossed the entire script at the top of the factory code because I'm having trouble calling it from a separate file, so until I exercise some more google-fu/trial-and-error I don't want to commit it as it stands. \nIn the meantime, does this look like a solid data collection result? ('capable' refers to css3, I'll try to find a way to make that more explicit for commit)\n\n\nI'd be happy to author a drop survey as well, though I'd like your input as to what sort of cases we could test... my imagination is coming up a few short of a dozen.\n. OK, that seems straightforward enough. I'll try and have a polished commit by tomorrow.\n. github wont let me delete my comment entirely... still having trouble getting browserdetect as a separate script/ knowing where to put it (I'm thinking in client/lib?) but stackoverflow and I are becoming friends. thanks for your patience.\n. Confirm lost paragraph in your problem scenario. Doesn't seem as likely to happen in practice even with two users visiting the page at the same time because they wouldn't both be logged in, but still troublesome. would handling the different actions as seperate put items help in any fashion? or maybe farm.page_get/put real quick before executing the move to make sure that the server is modifying the 100% current version of the page rather than the cached version which, in case above doesn't have the new paragraph and so put's down a new page w/o the intermediate edit? If there was a way to push a page back to all connected clients immediately after an edit, that could be mighty handy but perhaps that's just a naive notion from a newb. \n. +1 to merge from diverge. The idea seems to mirror the way git handles the issue, and when I'm describing SFW to my geekier friends, my 'in-a-nutshell' description is generally \"it's like git meets wiki.\"  a data/deleted directory would be a good stop-gap though, and handy to have in any case a la \"deleting me softly: so we never have to say sorry\"\n. further consideration: what if two wikis have pages named the same but about completely different content. Example:\nI run a development wiki for my spinoff of SFW, and I have a page about Sinatra, giving new developers some background on working with the framework. I take a break and browse the growing federation and find that someone has a page about Frank Sinatra (let's just assume that they left off 'Frank' in the name) I like Frank and I happen to know of a good youtube video of one of his better performances, so I want to add that to this other page. If SFW automatically merges these pages, you get a (reversible) mess of content that was never supposed to mix, and no way of pushing that content to another federated wiki.\nPotential solution: Rather than auto-forking any and all edits to origin while logged in, require that one actually presses the 'fork' button. in all other cases, save into local storage and treat as if not logged in... allowing one to be logged into one's own wiki while simultaneously making edits in local editing and 'submitting changes' to other wikis. Of course, in the case that I find another wiki page about the Sinatra framework, and I want to incorporate that content, 'merge' should still be a function of the fork button. But it seems SFW should allow the user to explicitly define what they want to do.\nI'd be happy to take a stab at this: any pointers as to where in the server/clientside code I should go first?\n. Been putting some thought into this, and poking around the code, and have come to a few preliminary conclusions:\ncoding for merge is far beyond my current skill level, though I fully intend to break your program a half dozen times trying to make it work anyway. \nI really like the idea of merge as a ghost page... anything that lessens spontaneous-user-panic is a plus, and accidentally overwriting/making large-scale changes to your hand crafted page with an inadvertent or misunderstood double-click is up there on my list of panic-inducers.\nLikewise, I've been creating many wiki's on my laptop farm and doing some exploration and use-simulations, and more and more I'm running into scenarios where 'Alice' is logged in to her wiki, and wants to contribute something/fix a typo/ join the discussion on 'Bob's wiki, and the end result she's seeking is not to have that content on her wiki, but rather to enrich bob's wiki... The Submit changes mechanism seems impossible to access while she is logged in. Her options are to either logout, make and submit her changes, and log back in. Or to fork bob's page, and manually inform Bob to check out her version/ hope he's got the activity plugin pointed at her. so he can fork back. Neither of these are as smooth or as effective as the local editing + submit changes mechanism already in place. (sidenote: Imagine a cumulative 'review/submit changes' button... You spend an hour posting to 10 of your friends wiki's and on one page you review all those changes and submit with one click!)\n^^A little long-winded, and delayed. Apologies on both counts. I'm looking at this project  with what might perhaps be an overly specific envisioning of a federation/syndicate I'd like to spawn, so take all this with a grain of salt. That said, as opposed to merge-logic, calling the local-edit function while logged in sounds like something that might fit within my current progression of learn-by-breaking-things-until-they-work-better, so I think I'll give it a shot and report back how I like it.\nLast Thing - In reference to Un-delete... if you append a number to the end of a page's title slug, it seems to become completely invisible to wiki internal linking, might be a quick way to implement an undelete key, make it hide in plain site, until we iron out a design for a more structured data-store.\n. Indeed. Spam is massively unpleasant. I wonder if we could implement some sort of \"accept submission\" config for wiki owners. i.e. \nallow all submissions (current default), or\nallow submissions only from other wiki owners, or \nallow submissions from a select few, or\ndon't allow submissions.\nalternatively, it would be interesting if there were some way to 'report spammers' within the federation, some sort of distributed blacklist...with  each submission the server weighs your report card against the owners 'benefit of the doubt' setting to determine whether your submission will be accepted.\noh, and world peace would be cool too.\n. also, and this may be a naive, inane, unthinkable idea, but in theory, could we use git itself to handle these logistics? fetch, clone, pull, merge, push(with permission). There seem to be a few rubygems that deal with git. What's more, what about using git repo's as one of the available stores?\n. @harlantwood thanks for those repo links, very good brainfood. I also liked the 'enlightened structure' site; that link turned into quite a rabbit hole.\n@WardCunningham fair enough, I suppose I did kinda get off on a tangent with an overly specific use-scenario, thanks for reeling me back in. And thanks for shedding light on git compatibility. Since you mention it, I finally have my SFW farm up and running, and I recall promising you a link to an 'obscure workarounds' fork:\nhttp://nei.ghbor.net:2013/view/obscure-workarounds\n^gotta use port 2013 since I'm hosting this behind a residential broadband connection that blocks port 80. \n. aaaaand, once again I give myself the bonehead award. http://nei.ghbor.net is now live on 80. I'll save the song and dance but suffice it to say the real issue wasn't the ISP, it was PEBKAC. good video though, I agree wholeheartedly.\n. Probably, were you trying to drag/drop something into the factory? the JSON above looks like a factory punt (basically the last resort if the factory doesn't know what to make of something you put in it) The number indicated that it didn't think you were trying to drop a file, and it didn't think you were trying to make a reference to another wiki (pretty much the two things factory drag/drop has been coded for). What was the action that caused this? And what browser/OS are you using? As stated in #275, drop logic has to yet to be coded for all combos of browser/OS.\nTypes: says plaintext, which makes me think you were trying to drag and drop a paragraph of text from a non SFW source... which could probably be coded for without too much trouble, but as a pragmatic usability tip for now I recommend copy/paste into a new paragraph vs click and drag.\ndisclaimer: I'm not an authority around here... I've just had some adventures with similar surprises. Best of luck.\n. No problem, I've been working on a number of other projects myself. Thanks for pulling.\n. turns out browserify is now js only by default, which means:\nslight modification to 'npm start' command\nrequire('./module.coffee') must specify .coffee extension if it's to work correctly\nthis commit doesn't touch the test command or the test code because I am unfamiliar with mocha, though I tried similarly modifying the npm test command but was met with:\n```\n\nsfw@0.0.1a test /home/rynomad/backup/dev/Smallest-Federated-Wiki/client\n./node_modules/.bin/browserify -c 'coffee -sc' testclient.coffee ./plugins/*/test.coffee > test/testclient.js\n\nerror running source transform command: coffee -sc\n```\nI think it might have something to do with the mixture of js and coffee in the test dir, but again, none of this is my area of expertise, though client.js now compiles and runs successfully through latest browserify.\ncheers.\n. thanks, and sorry 'bout that. not rightly sure where those come from or why, \n. quick note: when merging upstream changes from the recent grunt pull request, git complains a lot. luckily, it's only about the compiled js, not the .coffee. running grunt build seems to sort everything out.\n. I've been pondering this very scenario in thinking about wiki revision control:\nIf we have semantic linking between pages and the story items they contain (including other pages), then one could see having a group of 'collection' pages that contain categorized pages relevant to a subject area... as one or more of these referenced pages change, the updates could bubble up to the parent and propagate to other peers that maintain a version of that same parent page, which means you could effectively subscribe to updates for a page without needing to keep a copy of that page on hand.\nI've got a very rough outline of a linked data model that might be relevant to this conversation. \nhttp://rosewiki.org/view/wik-dvcs/view/wik-data-model/view/wik-json-schema\nTL;DR: every story Item and page is accessible via URI, page stories consist of an array of story item or page uri's. URI's are versioned on update and are constructed as a combination of 'type', id, publisher id, and timestamp. A major implication of this model is that we would need to rework how links are rendered, as page slugs would no longer be the primary key for looking up a page. \n. Regarding HTTP RecursiveGet vs Te-NDN recursiveGet:\nLet's say you are following a link from a page with many authors recorded in the journal, and the page you wish to find resides on only one of those servers. As currently implimented with HTTP, the client iterates through the domains present in the journal (stored in the 'localContext' var), moving on to the next only upon timeout of the previous request. If you are unlucky enough that the server you're looking for is at the bottom of that list, and the list is long, the link will take a significant amount of time to load.\nNDN issues one request to your server, and trusts that the server-mesh NDN network will route the request properly to wherever the data resides. Caching at each node here is a bonus, as is the fact that the network doesn't care what your localContext is; which means that if a link CAN resolve, it WILL resolve.\nThe tradeoff here is keeping more state on the server, and a bit more work to get the federated network in place when you initialize wiki (a large wiki with lot's of peers/pages might take a while to reboot) \nTelehash as a transport is generally both reliable and speedy, though there is the encryption overhead to take into account.\n. Ah, I see I missed what was really going wrong vis a vis plugin fetching... For what it's worth, the pattern I use for federating page fetches works just as well fetching plugin JS and CSS (I fetch the JS and turn it into an Object URL and pass it to JQuery/css tag as appropriate). That said, security is a definite concern when fetching JS and injecting it into a page, but luckily NDN has signature and verification built in, so in this case we'd need to come up with some sort of cert/web-of-trust managment strategy.\n. Cool! A few additional notes as I re-familiarize myself with the changes:\n- I initially stated that Gruntfile.js should be the first stop when looking at the difference between client.js and client.chromeExtension.js. Manifest.json should be the first stop for examining the operation of the extension, specifically background and content_scripts section\n  - background contains the steward.js file (and a jquery file for ajax), These scripts are \"always-on\" and ready to handle forks from any arbitrarily visited wiki, and (potentially) publishing to a wiki owned by the user\n  - content_scripts contains two patterns to inject scripts. \n    - The first pattern (file:///.wik) injects bootStrap.js, a simple script which scrapes the text of the .wik (.json) file, parses it, and sends it to the steward for storage. Upon recieving an ack from the extension, bootStrap creates and clicks on a link to the extension, including a location.hash value to view whatever page was just imported. This write action is currently haphazard. There is no check for an existing page, so you always overwrite any existing page\n    - The second pattern (http:///) injects checkwiki.js, which checks the title tag of each web page visited for class wiki. upon finding it, checkwiki changes the class to 'wiki-extension-enabled' so as to alert the client.js from the website that it should outsource fetching/saving semantics to the extension and register it to the neighborhood.\n      - This is a bit of a terrifying level of access for the features we want, but it's the simplest way I can think of at the moment to ensure that we can detect whether a site is a wiki without keeping a global roster of wikis in the manifest of the chrome extension, or requiring some sort of unified host pattern in order for a wiki site to be compatible with extension.\n- Having had some time to let the current approach stew in the back of my mind, and recognizing that I'm currently 65 commits behind fedwiki/master and should probably start fresh, it occurs to me to map out explicitly the contexts which will occur for someone who has the extension and is browsing wiki's. Nothing written below should be considered set in stone, but merely an exploration and reflection upon work thus far.\n  1. Direct to Extension: User clicks a chrome-extension button which takes them to the chrome-extension:// url of the extension. Extension acts as \"origin server\". in effect, and operates offline, just like a wiki server running on localhost.\n  2. Open a .wik file: User get's an email with a .wik file, opens it with browser. First approach injected wiki-client into the file://* tab. New approach imports and redirects to 1)\n  3. Wiki that User doesn't own: Without extension, fork to local. With extension, fork to extension. Should we register extension from the neighborhood in this case? It seems useful to be able to pull up one's own pages in the context of a remote wiki.\n  4. Wiki that User does own: fork from local turns into fork from extension. Should fork from remote duplicate pages on server and extension? It seems reasonable that the extension sitemap can be a superset of sitemaps that are stored on public facing sites, replicating the local-edit semantics often used with localhost wiki's. Also, for offline reading/editing, it seems handy to have extension stay synced with sites I own (but not necessarily have those sites sync with the extension).\n- Some thoughts for case 3 and 4 above: In either case, to get the webpage version of client.js to talk to the extensions steward.js requires multiple levels of message passing (first a custom event to get from client.js to checkwiki.js, second a chrome.runtime.sendMessage to get to steward. and then back again) which get's messy fast, notably when managing callbacks to handle responses in the form of custom message events between checkwiki.js and client.js.\n  - One possible solution would be to enforce an \"always browse wikis from extension\" policy, whereby a semantic similar to 2) above would simply note that a wiki was being visited, grab the lineup order, and convert it into a hash url to feed into a chrome-extension redirect. This has the advantage of simplicity in code, but there may be good reasons not to enforce the extension as the only gateway to all wikis. That said, given the ludicrous permission of http:/// in manifest.json, redirecting to the extension could be a good way of insuring that no site tries to contaminate our app without our knowing, which would be possible if the steward is simply listening to messages from any site that identifies itself as a wiki.\nI've tried to remove as much of the high level cruft from manifest.json and client/chrome/ but there's undoubtedly still some confusing bit rot in lib/ \nT-minus 1.5 hours till Hangout. See you there :)\n. Yesterdays discussion was fruitful. There were a few digressions into the larger context of fedwiki's future, as well as a few practical considerations having to do with this specific issue/feature/evolution. Documented here are a few big ideas in question/possible answer format.\nIf a user has write-access to several wiki's, each within the local context, how does the client code decide which to push edits/forks to? \nCurrently, one only has two choices: the origin server (if logged in) or localStorage. One must manually open a new tab for any wiki in order to make edits to a new origin. A common pattern is to host a wiki on localhost to edit and refine pages until they are \"ready for the world.\" Similarly, one may log out of their own server to edit from local storage and then log in and fork. A wiki extension has the potential to unify/replace these patterns by functioning similar to a localhost server, while potentially being able to establish credentials with several public-facing wikis simultaneously. We discussed the notion that a sensible default action might be to always save data to whatever location is \"closest to the screen\" (localStorage/extension/app) and pair this semantic with a more explicit push/publish button which would ask which of your write-enabled wikis (if more than one) you would like to publish to. This has the twofold advantage of being able to preserve the localhost privacy of ideas in progress as well as support multiple publish points with a minimum of confusion.\nIs there a way to simplify the sometimes circuitous paths one must take to link wiki's together, Or to switch between viewing a remote wiki and editing one's own? Currently, if I email someone a link to my wiki, and they begin reading from my origin, notice that they'd like to fork my pages to theirs or make a new page based on some spontaneous inspiration, they must open a new tab, log in to their wiki, drag links/urls. One potential benefit of an extension/app is to unify the browsing/editing experience across multiple origins. That is, the extension becomes the default handler for all wikis, local or remote, and handles it all within a single tab.\nThese and other discussions of usability, as well as the code complexity referenced in the previous comment, suggest a more in-depth exploration of \"wiki as an installed app\" semantics. Note that we say here exploration so as to remind the reader that the federated wiki they know and love will continue to be able to function as a traditional webapp as this exploration is made. Ideally, the two use patterns would co-exist (a good analogy might be that of an installed email client paired with webmail, configured such that you can use whichever you have access to at a given moment, and many users may prefer to use only the webmail client, with the extension/app being more oriented towards power users).\nI will likely spend a little time here and there exploring some more of the chrome/firefox extension/app ecology over this week, and hope to have another session next Sunday.\n. For those interested, the new pass at the chrome extension is located in my 'chrome' branch of wiki-client:\nhttps://github.com/rynomad/wiki-client/tree/chrome\nIn a nutshell:\n- Packaged app\n- ducktype detection of wiki HTML structure forwards to viewing that wiki as a remote page from app origin\n- history persisted using localStorage, launching wiki brings up the last used lineup\n- very minimal code changes, able to redirect require calls via packageChrome task specific options in Gruntfile\n- all standard plugins are bundled, until we find our holy grail of package distribution\n. ah yes! Though the reason you didn't find it is because I forgot to include it...\n- in chromes url bar, go to chrome://extensions\n- toggle \"developer mode\" to ON\n- click \"load unpacked extension\"\n- navigate to the /client/\n- start browsing\n- when you want it off, disable from the chrome://extensions page\nIt's worth mentioning that there's no publish ability as of yet, and other things you would expect from a local server are lacking (sitemaps, favicons). Definitely a minimum viable build, not good for much more than storing pages for offline reading/private journaling at the moment, but I'm liking the app-centered approach a lot. Much simpler.\n. I haven't looked much into PouchDB yet, but I do have some experience with levelUP on-top of indexedDB, by way of level-js. It works quite well.\nbetween the ability to store large amounts of data and webRTCs data channels for peer to peer browser connections, I'm hopeful that the packaged app environment can be nearly as feature complete as a public facing server. Chrome even allows apps to run a background page, akin to having a wiki tab always open. If we put page storage and serving in that background page, it could be accessible to the federation as long as the browser was running. \nMy pie in the sky idea here is that eventually the notion of running a large wiki farm can be supplemented with the action of running a wiki 'hub' that helps app users connect directly to each other. \n. ",
    "jhulten": "I like the potential offline aspects of this as well. The only issue I could see is that local storage is per domain, so even if I currated a bunch of pages they would not have access to each other via the browser. With the remote links we could provide a central location for the application cache and then link out from there. We would have to address the points from last weeks hangout about site identification and page identification tho.\n. Well if we had a list of all the slugs we could have a sync function that pull the changes that are needed into localStorage:\njavascript\nlocalStorage.setItem(\"name\", \"Hello World!\"); //saves to the database, key/value\ndocument.write(localStorage.getItem(\"name\")); //Hello World!\nlocalStorage.removeItem(\"name\"); //deletes the matching item from the database\n. You know, if we had every object separate (see my comments in #118) the whole site could be captured in one array. The question is where to put the journal. I think it might need to be in each object with a timestamp that can be sorted...\n. I guess part of my thinking on partitioning data below the page level is around forking below the page level (as opposed to copying) and allowing for incremental transfer. It would also allow for data in pages to be addressable in potentially interesting ways. This increases some complexity in that there is more than one type of object, but simplifies it by treating all objects equally.\nI guess this is a question of how fluid the standard is. If the first version is set in stone, thats fine. If it does everything you want as the lead, Ward, okay. I see two elements to this project... One is the federation and the other is the presentation and user experience.\n. Thinking about the client side vs. SEO... When I go to http://twitter.com/jhulten (which can be reached statically) I get the static content and I am redirected to http://twitter.com/#!/jhulten which I presume uses something like backbone.js to route the application.\nThis allows the page to be indexed without all the javascript. Its allowed to be, if you will, stupid. What if we did something similar. http://wiki/page (no trailing .html) would return the static page with just enough javascript to redirect to http://wiki/#page\nThe bonus in regards to this issue is the ability to pick a client based on the browser while still having indexable content.\n. An example of the existing paragraph JSON is below:\njson\n        {\n            \"id\": \"7b56f22a4b9ee974\",\n            \"type\": \"paragraph\",\n            \"text\": \"Welcome to the federated wiki. This page was first drafted \n                     Sunday, June 26th, 2011, at indie-web-camp. You are welcome \n                     to copy this page to any server you own and revise its welcoming\n                     message as you see fit. You can assume this has happened many \n                     times already.\"\n        }\nI don't know if it is useful to expand to a larger format that isn't informative to the application at hand. I could see a modification if we created a wiki specific ID. Then we could do something like:\njson\n        {\n            \"id\": \"deadbeef13-7b56f22a4b9ee974\",\n            \"type\": \"paragraph\",\n            \"text\": ...\n        }\nwhere deadbeef13 is the site id.\nI could also see changing it from \"text\" to \"data\" and every type would have their specific use of \"data\":\njson\n        {\n            \"id\": \"deadbeef13-7b56f22a4b9ee974\",\n            \"type\": \"paragraph\",\n            \"data\": \"Welcome to the federated wiki.\"\n        }\nThis would also allow for stories to be handled the same way:\njson\n        {\n            \"id\": \"deadbeef13-974deadbeef\",\n            \"type\": \"story\",\n            \"data\": [\n                \"deadbeef13-7b56f22a4b9ee974\",\n                \"deadbeef13-7b56f22a4b9ee972\"\n            ]\n        }\nIf we wanted to include the whole doc in one request we could attach an \"assets\" key (or some other name)...\njson\n        {\n            \"id\": \"deadbeef13-974deadbeef\",\n            \"type\": \"story\",\n            \"data\": [\n                \"deadbeef13-7b56f22a4b9ee974\",\n                \"deadbeef13-7b56f22a4b9ee972\"\n            ],\n            \"assets\": [\n                    {\n                        \"id\": \"deadbeef13-7b56f22a4b9ee974\",\n                        \"type\": \"paragraph\",\n                        \"data\": \"Welcome to the federated wiki.\"\n                    },\n                    {\n                        \"id\": \"deadbeef13-7b56f22a4b9ee972\",\n                        \"type\": \"image\",\n                        \"data\": \"\\<BASE64 ENCODED\\>\"\n                    }\n            ]\n        }\nThen you can take the \"type\" field and use mime-types instead (making them up as appropriate for text/markdown or wiki/story).\njson\n        {\n            \"id\": \"deadbeef13-974deadbeef\",\n            \"type\": \"wiki/story\",\n            \"data\": [\n                \"deadbeef13-7b56f22a4b9ee974\",\n                \"deadbeef13-7b56f22a4b9ee972\"\n            ],\n            \"assets\": [\n                    {\n                        \"id\": \"deadbeef13-7b56f22a4b9ee974\",\n                        \"type\": \"text/plain\",\n                        \"data\": \"Welcome to the federated wiki.\"\n                    },\n                    {\n                        \"id\": \"deadbeef13-7b56f22a4b9ee972\",\n                        \"type\": \"image/png\",\n                        \"data\": \"\\<BASE64 ENCODED\\>\"\n                    }\n            ]\n        }\nStill thinking about the journal.\n. You know one of the ways to keep the page id unique would be to use the site id (if we had one) as a seed. If the slug is welcome-visitors then the id would be SHA256(\"#{site_id}:welcome-visitors\")\nOf course this implies that we think we need a page id (I do).\n. If course if you want to be able to rename a page you would either have to fork internally or add something to the hash seed.\n. I realize this is one URL category. I had something else but it has slipped out of my head...\n. Any wagers on if the code is expecting the .json extension?\n\nJeffrey Hulten\nPrincipal Consultant at Automated Labs, LLC\njeffh@automatedlabs.com  206-853-5216\nSkype: jeffhulten\nOn Mar 16, 2012, at 4:41 PM, Ward Cunningham wrote:\n\nCode was added to the client to decode those multi-page urls. I'm guessing that there is some configuration detail that was never documented. I'd love for you to figure this out and document it. I'd also forgive you if you launched one of the interactive servers just to keep your project simple.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/issues/151#issuecomment-4550381\n. \n",
    "pmuellr": "I was hoping the pull request would be auto-referenced here, but it wasn't.  \nPull request for this issue: https://github.com/WardCunningham/Smallest-Federated-Wiki/pull/97\n. I should note that I have NOT run any tests on this, besides running the express server from the command line, which still works as well as it did before.  I'm currently not set up to run the actual tests.\n. This looks good, thx for updating the doc as well.\nNote that you included a \"drive-by\" fix for server/Wikiduino/ReadMe.md, in commit 6bf16f9 .  No problem for me, but not sure how \"clean\" you're trying to keep your commits/pulls.\n. On Sun, Jan 15, 2012 at 20:16, Nick Niemeir <\nreply@reply.github.com\n\nwrote:\nHmm, yeah, we've had a lot of turbulent stuff like that, it seems to be\nok, but if you've got a better work flow that doesn't leave those artifact\nmerge commits i'd be glad to hear about it.\n\nNot a problem at all; I do these kind of \"drive-by\" fixes all the time for\nmy own projects, but I've also worked on some projects that like to keep\ntheir commit histories squeaky clean.\n\nPatrick Mueller\nhttp://muellerware.org\n. Typically, to keep things squeaky clean, I create a new branch for everything that I want lumped together.  Only make changes in that branch for things you want lumped together.  For example, for every \"bug\" I work on, I create a new branch called issue-17 (where 17 is the bug number).  Do all my work in that branch, when I'm done, go to the \"master\" branch (or whatever), and do a \"git merge issue-17\", then push to GitHub, then pull request.  \nFor a days-long sort of fix, I'd work out of issue-17 and push it to my GitHub fork every night, then when I'm done merge it into master and do the pull request.  I think if you're working out of a fork, merging to master probably isn't even needed (or desired) in most cases; just leave your work in your issue-17 branch and issue the pull request from there.\nYou can work on multiple bugs/new features simultaneously this way.  Sort of loosely based on a development flow that I've used in the past: http://nvie.com/posts/a-successful-git-branching-model/\nOne nice thing about this is, if you start making changes for issue-17 but you've forgotten to create the branch, you can create the branch then, and all the non-committed changes go with you.  That is if you were starting from a clean branch already.\nI'm less familiar with \"fixing\" a set of commits in a branch using --rebase, et al.  I try to keep it from getting to that point :-)  If I had to, I'd go look at one of the online Git books like http://book.git-scm.com/ \nIn the end, doing \"drive by\" fixes like this isn't really a big issue, I don't think.  Sometimes it's too much work to be squeaky clean, when all you really need to do is be pretty clean.  I wouldn't try to \"fix\" the commit with rebases unless you're familiar with the processes.  OTOH, good opportunity to get familar with the processes.\n. For the server renaming, I've opened issue #99.\n. ",
    "thomaslee": "Hey Nick,\nQuick response before I crawl into bed -- I've already stayed up far too late :)\nAs you've identified, there's room for improvement on the performance of authenticated() -- we can cache the \"claimed\" state in memory, for example.\nwrt these specific changes, we were just trying to address what I think might just be a bit of a misunderstanding: if I understood Ward correctly (and I'm sure he'll correct me if I'm lying!) an \"unclaimed\" Wiki should be editable by anyone. The ruby/sinatra code base reflects this (see server.rb:84 and server.rb:208 as at b24872c2...).\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/blob/master/server/sinatra/server.rb#L84\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/blob/master/server/sinatra/server.rb#L208\nOnce a Wiki has been claimed, the openid filesystem \"tag\" gets created. The logic in master for the express server seems to be overlooking the \"unclaimed\" case.\nHope I'm making sense there!\nI don't understand most of what you were saying from \"The idea now is ...\" onwards, sorry -- it must be getting too late.\nAnd yup, the setOwner thing was more a simple \"correctness\" thing. Wasn't causing issues, wasn't likely to cause issues, but theoretically it could have caused issues. :)\nIn any case, thanks for taking a look.\nCheers,\nTom\n. ",
    "elf-pavlik": "hi, any progress here? =)\n. from: http://identity.mozilla.com/post/78873831485/transitioning-persona-to-community-ownership\n\nThere are no plans to decommission Persona. If it fits your needs, please use it. We will support you.\n\nI see no reason why not to support Mozilla Persona, WebID and OpenID Connect leaving people choice which to use. IMO Persona with its graceful fallback to centralized IdP as minimum requires someone to have email address, while WebID and OpenID Connect require visitor to already have an account at some IdP (not sure here if http://webfist.org/ can help in OpenID Connect case)\nWhen it comes to work on yet another auth, i would like to see first in depth analysis of those three i mention above and clear explanation how it would build on experience with designing/using them!\nFor WebID please note distinction between WebID and WebID+TLS In http://www.w3.org/2005/Incubator/webid/spec/ you can also find link to notes on Identity Interoperability\nsome work on WebID for node - https://github.com/magnetik/node-webid\n. One more development I need to investigate further - Identity Credentials & HTTP Signatures Manu Sporny who leads its development participated in earlier days of WebID development and had reasons not to use it in work on Web Payments\n. very relevant! read, also discussing Mozilla Persona and WebID: http://manu.sporny.org/2014/credential-based-login/\n. ",
    "grignaak": "Ward, what's the output you're getting?\nI just tried this in firebug on stackoverflow, and the end-of-lines (both unix & windows) were converted to <br/> tags\n```\nprettyPrintOne(\"public class Hello {\\r\\npublic static void main(String... args) {\\r\\n}\\r\\n}\")\n\"public class Hello {public static void main(String... args) {}}\"\nprettyPrintOne(\"public class Hello {\\npublic static void main(String... args) {\\n}\\n}\")\n\"public class Hello {public static void main(String... args) {}}\"\n```\n. Oh, and by-the-way, you can add line-numbers to the text by calling prettyPrintOne(html, null, true)\n. I've dug into the code of prettify.js. It's the minified code from the download site which was uploaded in June 2011.\nThey've added more sophisticated line-handling since then. You can snatch a later version from StackOverflow's cdn: http://cdn.sstatic.net/js/prettify-full.js\nThe CSS is the same. The other JS files are extensions that the user must manually add to the html page, and then call prettyPrintOne(html, \"java\"), so they can probably be removed.\n. I've made my own pull request to fix the issue. Please try it out.\n. I've been using this on my laptop for a week now, it seems great.\nAlthough, I wonder if there is a better way for plugins to specify required resources. I noticed (and haven't entered an issue for it, yet) that for every time the current plugins or their factories appear on the page, a network call goes out to fetch its resources\u2014even if the resource has already been loaded. This is especially poignant when working offline because wiki.getScript adds a no-cache hash query to the script file. (Have you considered using AMD?)\nThat aside, I think the code plugin is ready.\n. ",
    "lhitchon": "Thanks. That's the same fix I applied locally. I was going to separate it into a pull request later today, but now I won't need to .\n. ",
    "kbigler": "I have yet to learn about your underlying models and representations, so let me ask a naive question.\nIf you have a paragraph-type concept maybe you could have paragraph types like \"prototype-1\" and have in the environment a specified current-default paragraph type which might for now be \"prototype-1\" (not \"default\").  Then perhaps without too much effort you can as needed change the current-default paragraph type and simultaneously deprecate some paragraph types but retain them to keep existing content working.  Then when a paragraph of a deprecate type is edited, warn the editor but make it their responsibility to correct as needed to conform to the syntax requirements for the new type.  This would keep things moving forward while permitting different markup practices to co-exist in the environment and not get in each others way.\nOf course I understand that might violate keep-it-simple but have no sense of the weight of the issues involved.\n. > The link slugs are down-cased and url friendly names of pages\nAh, ok, so I take it link-slug just refers to what is generated by the [[...]] syntax which also determines the file name created for the page.  So not something used directly (when editing), except as needed for any links from outside.\n\nMediaWiki uses a vertical bar in one and not the other\n\nI had googled up MediaWiki but could not grasp the syntax charts, also had no idea which subset you had implemented so seemed not worth pursuing. Is there currently a way without Nick's branch to give different shown text to an internal link, perhaps using the vertical bar you mention?\n. > I'm against alternate text for links for a variety of reasons enumerated in an essay here [http://www.iana.org/domains/example/] [http://example.com/?if-there-really-was-an-essay-you-will-not-learn-its-title-here].\nThe problem is somewhat mitigated by the fact that the link id (url or internal) does not have to be completely hidden, e.g., it might be made readily visible via a float-over, although I'm really not in love with float-overs in their familiar and often dysfunctional form.\nSecondly, regarding the title of the article, the problem is worse for external links for which the title is not available at all unless the site is live, whereas with SFW internal links the link identifier is the title.\nSo it would appear to be more of a problem to allow alternate text for external links than for internal links, if some natural process or hint would lead the viewer fairly directly to the link superceded by the alternate text.\nAs things stand you can give alternate text for an external URL and it is still not clear to me whether in the current implementation you can for an internal link.\nThere is always that question about whether tools that risk shoot-in-the-foot behavior should be withheld. My usual response is to allow it with the combination of making the usage not entirely obvious and providing appropriate warnings.  The idea would be that people who can discover the method will have the opportunity to see the warnings.\n. I have used neither bundler nor ruby before, and am newbie here to perhaps an unusual extent.  There is so much I can't speak to that I may have little idea of the implications of what I suggest.  So I have to qualify what I say here as merely my best guess (as far as knowing all the implications for your project) and then go ahead and say it.\nI submitted the issue thinking you might be collecting evidence that would permit the ruby version number to be moved forward.  Such movement forward may be motivated by the fact that without moving to 1.9.3 using OSX Lion will be problematic in many cases.  It may be workable but may require having multiple parallel versions of gcc in a way that at the very least requires a lot of side-stepping learning curve for the less-sophisticated server maintainer (me).  If the server is to be dedicated to this project then the ruby-1.9.2-compatible gcc could be installed and there'd be no issues, but that was not my situation and is likely not to be the typical situation especially since new Macs ship with Lion.\nSince I moved to 1.9.3 with no hitch it might suggest 1.9.3 might be just as inter-developable (term?) with 1.8.5 as was 1.9.2.\nWhat I know nothing about is whether 1.9.3 has any problems with OS's that would run 1.9.2.  I could try 1.9.3 on my Snow Leopard desktop machine if you like.\n. Thanks for all the input.  Here's what I'm thinking.\nI think SFW is currently a blank slate in relation to \"tracking\" as I'm using the term.  Working in blank-slate scenarios is something I inherently take to.  But it also creates the opportunity to discover what the very minimum structure is that will satisfy a certain class of tracking needs.  The organization I have foremost in mind does not need any more structure than necessary influencing native styles of working.  And this experiment should [imperative] reveal some things that are more broadly applicable.\nOne way to approach this is to use this as my initial project for learning SFW.  Toward that end please offer advice about how best to use what you have here, including the github issue tracking structure.  It could be that we just keep this issue open for a while and it becomes the forum in which I present scope and design ideas and get feedback on that and also get help with the process of learning SFW as needed.\nI would almost undoubtedly end up implementing plug-ins and I'd suppose that now and then the ensuing dialog would lead to a consensus for some base changes to SFW.  I'd be glad to work on those as well as on other things not specifically pertaining to \"my project\" but which no doubt relate to my interests since I have general interest in SFW.\nI'm not interested in maintaining a separate branch in the forseeeable future, or if possible, ever.  I want to stay within a bounds of simplicity that allow for the most ease and convergence, aka sanity when you want to have a life.  (I have a day job, too.)\nSo I propose to present design ideas right here as long as they remain basic, and spawn another github issue whenever one of them gets involved.  If that seems reasonable I'll start as soon as I can digest some of the SFW basics.  I plan to start with something ever so simple so that the entire scope can be readily grasped including how it fits in with SFW, plug-ins, and federation.  No doubt I'll have to edge toward an understanding of the implications.\nIf it seems better to close this issue and open other issues as needed, let me know.\nI will also want to be asking for guidance on how to learn SFW technically and that may present itself rather quickly as a candidate for a separate issue.  Maybe that process would lead to development of the SFW github wiki pages, which I could contribute to.  At the moment I have no idea how everyone else here doing significant development managed to get \"in sync\".\nBut for the moment there is already enough material I haven't yet digested.  That may slow my start.\n. ",
    "interstar": "So, suppose I have a bunch of static json files on a server. How do I now reference them from a live SFW page in such a way that they get pulled into the federation?\n. OK. But what does my \"static\" server have to be? If, for example, we're talking about static files served on a basic hosting service, the path to them might not be as simple as \"domain.com/pagename\" ... it's going to be more like \"domain.com/long/convoluted/path/pagename\" Can we handle that?\n. Also ... is it possible with a purely static file host to let people download a client and browse the static files via it, even if they can't edit? Ie. to launch a client looking at a static welcome page?\n. Hmmm ... I'm still not sure I'm following. I've put a bunch of converted static json files on my server.\nYou can now access them through the right sort of url eg. http://sfw.thoughtstorms.info/HomePage\nBut if I try calling a URL on my local dev server of the form : http://localhost:3000/view/welcome-visitors/sfw.thoughtstorms.info/homepage then nothing appears and I get the following error from the server : \nremoteGet error: { [Error: getaddrinfo ENOENT] code: 'ENOTFOUND', errno: 'ENOTFOUND', syscall: 'getaddrinfo' }\nShould this be possible?\n. Ah ...that was it jhulten. I've changed my host-script to remove \".json\" from the end of the request.\nWard I'm happy to document this. Currently added to the end of https://github.com/WardCunningham/Smallest-Federated-Wiki/wiki/Hosting-and-Installation-Guide\nI'm wondering how much further it might be worth developing the PHP into a simplified SFW server. I don't have root on this host so setting up something else is a bit more complex.\n. So that way of serving static pages works ... but  ...\n1) How do you make a link from inside a page on one SFW to this static server? If you make an ordinary [ link ] it opens as an external page.\n2) This highlights the issue of plugin federation. For example, http://fed.wiki.org/view/welcome-visitors/sfw.thoughtstorms.info/homepage seems like it's working. But you won't see anything because I put all my paragraphs into a different type.\nThis seems a serious problem. Federation is in conflict with extensibility through plugins. What's the thinking about this?\na) some kind of graceful degradation (eg. unrecognised paragraphs are treated as paragraphs? or raw source-code is shown?)\nb) Users are given the option to fork a local copy with an alternative paragraph type.\nc) some mechanism for getting the plugin from the other wiki and installing it locally? (I recognise the security implications of this)\nd) Asking the remote server to render the page as HTML?\n. Thanks. \nMy main concern was generating a correct id and a correct journal entry to represent the creation of the page. \nAs far as I can see from your Perl script the id is just a random 16 digit hex number (not a hash of anything)? And you don't need the journal. \nIs that right?\n. Ward. As far as I can see, SFW doesn't have a markup language. Is this a deliberate policy? I realise it's a can of worms but is the intention to handle it with plugins at the level of individual paragraphs? With special types? Or I see in your last answer you were suggesting putting it into the journal import event. Or are you encouraging people to move away from markup and towards HTML to ensure compatibility \nWhat's been your thinking about this, so far?\n. OK. That makes sense.\nI've started doing my conversion. So far it's a script to import flat files into SFW. And a plugin to render the markup language (derived from UseMod)\nHere : https://github.com/interstar/ThoughtStorms\n. Hi Ward,\nthanks for the read and comment.\nThe code is basically a port of your Perl. Perhaps the biggest change is the \"splitter\" function which does indeed carve the page up into separate paragraphs. \nHowever it's a function that takes its criteria of when to split as an argument that is, itself another function. \nThe main reason I did this is that I like to use nested bullet lists, (starting with '*'). I want to keep these lists together within a single paragraph whatever the criteria for splitting in the the non-list part of the code. (I may eventually make some kind of collapsable / outliner plugin type for them)\nThe slightly convoluted criteria function I'm currently using is mainly for the splitter to track when we are in \"list mode\" and when we are in normal mode.\nTo make this clearer I've now uploaded an example input file and the corresponding output from the code in the \"example\" subdirectory.\n. Ah ...so MemoryStore is a cache like Redis?\nYeah, I'm probably not going to have traffic that makes this important at the moment.\n. Yes. Sorry ... it seems to running w/out noticable issues (with my fairly low traffic)\n. Ah ... that seems to be it Nick. The changes persisted when the machine was switched off and when the server was restarted but weren't in the data/pages in the file system.\nWhen I look at view/local-editing I see that the pages I changed are listed there.\nNow, I've logged in again but, of course, all my changes have now disappeared. This does make sense (and is quite clever) but can I recover the changes I was making yesterday? Or have they gone for good?\n. I think the behaviour is definitely a good one. But it would be nice to have some kind of visual warning (maybe recolour the page) that alerts the user that it's just their personal (in browser store) version of the wiki that's being updated.\n. I'm re-opening this because it's REALLY annoying in practice. \nOn my screens (netbooks, sunlit rooms), the \"yellowish\" tinge is unnoticeable. I've had three occasions now when I've somehow been logged out without noticing it. I make a bunch of changes (often small ones, scattered throughout many pages because I'm in wiki gardening mode). I then see that I'm in not logged in, and there seems to be nothing I can do about it.\nIf I log in, I'll immediately lose all these changes. And if I don't log in, I can't update the real wiki. The only option seems to be to try to remember all the changes I made and laboriously edit and copy the changed paragraphs, one by one, to a text file, then log in and go back and recreate them again.\nI think I'd far prefer it, if local storage is to be the same as any other server, that you have to log in to use it. That would at least make clear what I'm doing at any point. Alternatively could we just turn the background bright fuchsia or something?\n. Ah ... now I understand one of the reasons why I'm hitting this problem so often. I have my wiki available at both : http://www.thoughtstorms.info and http://thoughtstorms.info  Even when I'm logged in to one, if I open the other, I'm not considered to be logged in. \n. Yes. If I log in, it could ask \"do you want to upload your local changes to the server\"\nAlternatively, a way of exporting all the locally made changes as a single \"patch\" style file which could be imported to a server. \nWhat I ended up doing was opening the json view of each page I changed, copying it into a file and then scp-ing it up to my server.\nThat was better than paragraph by paragraph, but a single file would be better still. \nBTW, is there any kind of RecentChanges ?\n. Ah ... thanks.\n. No, I'm not moving it by hand. I'm trying to do what you do in video 15, drag the URL of the public wiki into the factory area of a page on my private wiki.\n. There are no javascript errors on the console. I'm assuming it's the server that's baulking.\n. Ward, Nick\nthanks for the responses. \nFor the record I'm running the express server and testing in Chromium on Ubuntu 11.10.\nIf I try the same thing in Firefox I get a grey box that says 1 X 1 in large letters and \"Unexpected Item\" underneath in small letters.\nBTW : the reason I haven't really dived in to start looking at the code is that I'm not familiar with your integration testing tools. Until I get my head around how to do that I don't suppose it's worth making changes and sending them to you. \nIs there a quick how-to / or startup tutorial anywhere for how you test. Ruby / RSpec / Sinatra etc. are a bit of a mystery to me.\n. The other thought I have is that dragging and dropping across different browsers could be a hard problem to solve. But in the meantime it would be great to have a text format that produces the same effect.\nFor example, why not be able to write [[other.federated.wiki/ForeignPage]] to make an \"in federation\" link to another wiki?  (ie. resolves to a url like http://my.wiki/view/mypage/other.federated.wiki/foreignpage)\nOr to have \"federatedWiki\" as another option in the factory menu when I create a paragraph?\n. Ah cool. Thanks.\n. Cool Nick, works for me. Even pictures come across.\nBTW : I used this ( http://userjs.up.seesaa.net/js/bookmarklet.html ) to make the bookmarklet.\nBut I guess you could just have it automatically created as a button along the bottom of the page, alongside the \"logout\".\nNo Ward! You were sadly missed. \n. Didn't the rise of the web teach us to be more relaxed about broken links? (Compared to, say, the hypertext researchers of the 80s?) Yes, writers shouldn't break links. And redirection (manual or automatic) is nice-to-have. But the underlying technology shouldn't stop people doing what they want just because of this worry.\nI'd bet 99% of the \"I want to change the name of this page\" scenarios are because someone discovers a spelling error or feels that a title is too clunky within the first few hours of creating the page, well before anyone else is likely to have linked to or forked it.\n. Rather than splitting paragraphs, what would be really useful to me would be a \"clone paragraph\" function which copied everything in a paragraph but gave it a new id. That would also help with splitting because you could then edit down the two copies to be the split parts of the original.\nOne of the things I'm thinking of using SFW for is for checklists where you might want to start by cloning a master copy of the list into a specific page, and then update the list there to reflect the status of which items have been done. \n. Cool @JoeBergin I'm watching your repo. My SFW related scripts are here : https://github.com/interstar/ThoughtStorms\n. @JoeBergin I have something similar. A private wiki-like thing I wrote a few years ago. One of my near term plans is to export pages from that to an SFW running as a local server, and then syncing what can be public of it, up to my public SFW.\nI think this is yet another use-case for a general purpose git-like \"pull\" from one SFW instance to another. \nOne step towards that might be a json-merge script that does something like the following :\n1) You type \nsfwmerge source destination\n2) If a paragraph id is in source but not in destination it gets copied to destination and slotted in as near to the right place as the script can find.\n3) If a paragraph id is in destination but not in source, it may (subject to a command-line flag) be removed\n4) If the same paragraph id is in both we raise a \"conflict\" flag and do a unix-style diff merge of the text of the two paragraphs into one. (I'd suggest having the two as different paras, but then you have to decide who gets the id)\nDoes this sound reasonable, everyone? @WardCunningham ?\n. I just did a very quick and dirty repurposing of tlrobinson's \"jsondiff\".\nTry it at http://project.thoughtstorms.info ... in the form put in two domains for old and new ... eg \"http://fed.wiki.org\" and \"http://thoughtstorms.info\" ... then a slug like \"smallest-federated-wiki\" into the page input.\nNot sure how useful this is in it's current state ... but we could potentially adapt this into a useful tool.\n. Yep @GerryG I used that snowtide diff (referred to in your first link) to make an in-browser diff tool for a job I was in a few years ago where we didn't have source-control!!!! It was invaluable. You can't believe how much time that library saved me and how grateful I am to snowtide. \nFor our purposes I'm sure we're going to need a custom diff that not only works with json but does something sensible from our perspective. (Eg. we're a lot more interested in diffs in the story than the journal so perhaps the tool should focus on that.) \n. Hmmm .. in my personal notebook, SdiDesk, I had hierarchies of \"subpages\" and sub-sub-pages etc. separated by / . \nWhen I converted this to SFW I turned / into double hyphen -- to distinguish from ordinary. As far as I can see we don't have any other non-alphabetic separator except hyphen so having a way to distinguish different uses of it ie. a word separator vs a \"larger aggregation\" separator, is useful.\nWhat's the problem with doubled hyphens? And if we get rid of them, can we have something else to distinguish \"pseudo-spaces\" from \"bundles of pages\"?\n. @GerryG  I think an id that is related to, and logically derivable from a name is an essential part of what makes wiki so good. \nIt means that page-names can be \"guessable\". When I'm authoring a link, I guess what's the most obvious name for an idea or thing and most of the time, I'm right. Even better, sometimes someone links to a non-existing page, and later on, someone else via a different route, fills that page in. \"Magically\" the link now works. If you replace page-names with arbitrary identifiers, that never happens.\n. @nrn Is there anything I can do to help with Recent Changes on Express? How far along is it?\n. @KyleAMathews \nI understand there's a mode which means that whenever a someone comes in on a new subdomain a new wiki is created for that subdomain.  Eg. if I'm runnning SFW on mywiki.com and someone comes in on steve.mywiki.com then there's a new \"steve\" wiki created. It's one running instance of the express server (I think ???) but multiple collections of pages.\n. Thanks @nrn @WardCunningham \n. Does that mean it will stop jumping back to the first open copy of a page if I try to open the page again further along my meanderings?\nI rather liked that behaviour.\n. @SvenDowideit  I have simple nested bullet-lists in my Wikish plugin : https://github.com/interstar/ThoughtStorms/tree/master/plugins (Also italics, bold, horizontal lines etc.) \nI'd really like to get this into the official distro if possible. Or at least into the official catalogue of available plugins. \n. @SvenDowideit I agree with the SFW model of putting things into separate paragraphs to make it easier to manipulate them. And also with using paragraph types to capture the logical structure. With a couple of provisos : I'm pretty sure we're going to find the existing model of one paragraph, one type becomes a bottle-neck very soon.\n1) We are going to want to give multiple types to individual paragraphs. That's true whether we think of paragraph types as conferring logical meaning or styling / visualisation hints. I can think of two ways to do that : either like CSS where you just add a number of classes to the same object. Or through a more formal class-inheritance model where we can define specialist subtypes that inherit or mixin functionality from more general types.\n2) The great joy of wiki was how it made marking-up a document so much easier than either raw HTML or using a GUI. While the SFW may need less visual markup (given that it has more tightly constrained layouts) we surely don't want to lose that fluency of expression. And if paragraph types are our way of marking up documents then we need an equivalently flexible and fluent way of applying types to paragraphs.\nAt the moment, in SFW, selecting a paragraph type is fairly clunky, ie. we do it from a single unstructured list in the factory. And it's impossible to change or modify a type after the paragraph has been created. \nI don't have the right solution, but I think the original wiki intuition of using lightweight text markup is still a good one. Ie. an asterisk in the first character of a line can MAKE the paragraph become a bullet-point in a list. We can parse the paragraph at save time and turn the markup into explicit type. Similarly, have ==  markup create a heading, which is itself a logical type (and available for TOC creation etc.)\nFor the record, my initial use-case of SFW was that I wanted to port a large number of pages from UseMod and its derivatives. Hence it made sense to adopt the UseMod markup language. I'm happily evolving onwards from that, while wanting to keep that import channel open. (I'm hoping to encourage other UseMod users to make the same jump.)\n. Ah. OK. Thanks. It's because I'm on express.\nIs anyone working on the express version at the moment?\n. Thanks Ward. Haven't seen this since updating.\n. Ah ... OK. I didn't know I could do it on the Local Changes page. Thanks.\n. I suppose a stop-gap to fully automatic federated plugin sharing would be to have a standard package-repository for plugins (like npm) where we could all contribute them. Then any SFW owner who found they'd pulled a paragraph from another wiki in an unknown format would have a standard place to look for the plugin. \nBTW: that wikish plugin is here : https://github.com/interstar/ThoughtStorms/tree/master/plugins\n. @paul90 's suggestion seems to be based on the idea that content is static. But from the beginning @WardCunningham has had plugins that render live data (ie. would need to be executed at read-time, not merely at authoring-time)  So I'm not sure how that would work. And anyway, it wouldn't resolve the problem because people will want to edit forked content too. So if you have special editors you'll still want to migrate them along with the data. \nUnless a special editor is a one-shot thing, which wouldn't be particularly useful. For example, I have a (currently unreleased) \"network-diagram\" drawing plugin that renders a high-level representation of a bunch of connected nodes. At the moment I'm drawing the diagram with Canvas, but I've played with Rafael.js and SVG. Or maybe I'll port to D3 at some point. Or even WebGL. Whichever, I want to keep the model in its abstract form both to keep the flexibility to change the rendering technology and so that the data can be edited. \n. BTW : I didn't know about the decision to use NPM for distributing plugins. Makes sense.\nI'll certainly package my plugins in this form. Can anyone point me to some existing instructions for how to do this?\n. @WardCunningham  The problem with using paragraphs for bullet items is that we don't have anything else for larger scale aggregation.\nVery often I'll have short lists of three or four items with a title that obviously need to be kept together as a unit, but within a larger document. If I make them n separate paragraphs then there's nothing that DOES keep them together when I'm trying to refactor either within or between pages.\nMost of the time, I'll put the entire list in a single paragraph and use bullets to maintain the structure, because the pain of having to edit them through \"normal\" cut-and-paste is far less than the pain of trying to keep them together while dragging each item around separately.\nIn fact, my trend is towards making fewer and larger internally structured paragraphs precisely for this reason. Although dragging and dropping in SFW is cute, it sets a very hard unit-size. And often I find that I want edit \"across the borders\" of this unit (eg. take the end sentence from one para and the beginning two sentences from the next and inject them into a paragraph higher up the page.). \nAlthough when I originally converted my wiki to SFW, I made every paragraph an SFW paragraph, for new writing today I'm more likely to use SFW paragraphs as \"sections\" and edit their internal structure in the traditional way. \n. @WardCunningham \nTo be honest, I don't even remember why I was doing that Transcluder. I think it was to do with having some UseMods that hadn't been converted yet. But I don't think it's something I'm particularly committed to. My aim is to migrate all my public facing wikis to SFW. \nI don't really have very exotic requirements from a text plugin. But I do want (nested) bullet lists. And I do want (simple) tables such I use here : http://thoughtstorms.info/view/netocracy (Preferably using my double-comma-as-separator markup)\nI use these markup idioms too heavily to want to abandon them. Particularly as there's no other way to represent different levels of nested structure in SFW. \n. Yeah. QuickPaste is basically a solution for geeks converting one-off\ndocuments, who don't want to have to copy and paste a paragraph at a time.\nIf you want to write a script to do the conversion and post it\nautomatically somewhere you could grab some of the code from\nhttps://github.com/interstar/ThoughtStorms/blob/master/scripts/SFWTools.py\nand modify it to your purpose. Though I think that's never going to be part\nof an end-user solution. Just a convenience tool.\nPhil\nOn 9 July 2014 17:30, David Ing notifications@github.com wrote:\n\n@almereyda https://github.com/almereyda I just tried the Quick Paste\nConverter, and see that it converts from plaintext or wikish to JSON. This\nwould suggest the next step would be to FTP the file to the server ...\nwhich isn't a good path for non-programmers.\n@WardCunningham https://github.com/WardCunningham had said that he\nmight consider writing a quick translator of some sort. As an alternative\nto an input being HTML with breaks in it, perhaps translation from the\nDokuwiki markup would be better.\nI was wondering why Etherpad Lite would specifically choose Dokuwiki\nformatting over any other wiki creole, but then found an EtherDoku project\nat http://sourceforge.net/projects/etherdoku/ (last updated 2013-04-18),\nand then \"This is a guide to building your own hybrid etherpad + wiki\" at\nhttp://canidu.com/etherwiki-howto.html ... not to mention an integration\nplug-in at https://www.dokuwiki.org/plugin:etherpadlite .\nThe combined use of Etherpad with any wiki is probably worth discussing.\nSo far, I've been positioning federated wiki as a multiple-perspectives\ninquiring system tool, whereas traditional wiki is an inductive consensual\ninquiring systems tool. (For those unfamiliar with inquiring systems, see\nhttp://coevolving.com/blogs/index.php/archive/the-meta-design-of-dialogues-as-inquiring-systems/\n).\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/issues/424#issuecomment-48529973\n.\n. \n",
    "hart404": "I must admit, I was thinking that previous forks should be removed but I'm convinced by your attribution remark. Yes, I vote for all forks to be maintained in the journal.\n. ",
    "jc00ke": "You're welcome! I'm looking forward to using SFW soon.\n. ",
    "JoeBergin": "More on this. If I produce something like <center> *** </center> in a json paragraph, using a batch translator to create the json, the literal text shows on the page. However if you double click the paragraph, make no changes, and then save it, it is interpreted correctly (centered). The same is true for pre tags. You see the tags initially, but after a save, pre formatted text. \n. For (a) Ability to change the title of a page:\nSuppose it were only possible to do this prior to the first fork. Linda Rising once told me about the importance of naming her patterns correctly. Once the names were \"adopted\" by the community, she lost the ability to come up with better names. But before adoption... Her solution is to keep alias names for as long as possible while developing the patterns - through the shepherding process at least. I don't foresee such a solution here, though. But treating forking as formal \"adoption\" might work. \nFor the other three \"wants\": If it were easy to split a paragraph, the others would be much easier, since you can use ordinary text selection to grab a bunch of paragraphs from one page and enter them in to a single paragraph on another. \nTo do a split, suppose that the text factory noticed a specific non-standard markup, say <split/> and replaced that with two paragraphs generated from the current one at the point of the \"tag\".  Yes, I realize I may be naive here, but, hey - take a risk. At least this would be more \"intention revealing\" then double newlines. \n. Yes, that was exactly my main use-case. \nOn Apr 5, 2012, at 8:25 AM, phil jones wrote:\n\nDidn't the rise of the web teach us to be more relaxed about broken links? (Compared to, say, the hypertext researchers of the 80s?) Yes, writers shouldn't break links. And redirection (manual or automatic) is nice-to-have. But the underlying technology shouldn't stop people doing what they want just because of this worry.\nI'd bet 99% of the \"I want to change the name of this page\" scenarios are because someone discovers a spelling error or feels that a title is too clunky within the first few hours of creating the page, well before anyone else is likely to have linked to or forked it.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/issues/176#issuecomment-4975481\n. But after a \"few hours\" or a couple of days, links within the site may have proliferated. So changing a page name may need to look around a bit and update \"stuff\". \n. Yes, I think that would work as well for me as my earlier suggestion. A bit more general, and the same amount of work (more or less) for my use-case. \n. I just built (and am testing) a \"page renaming\" ruby script. It only works on the local server (not over the web). You give it the title of an existing page and a new name. It does this: If the page to be changed is a fork, it does nothing. Otherwise, it changes the title of that page (and its filename) and changes all references in local pages to the original to reference the new page. However, it does this only in the story, not the journal. It writes changed pages to a new directory, preserving the originals. \n\nDoes this sound like reasonable behavior, or stupid? Does is sound like desirable behavior? I'll make it available for test. Send me an email. \nHow do I tell (can I tell) if a page \"has been\" forked, as opposed to \"is a\" fork? Sorry, but there is lots I don't yet understand and don't want to act like the proverbial bull in the china shop. I can eventually post these scripts on github, but not before test and consideration. \n. All my scripts have been uploaded to https://github.com/JoeBergin/Batch-SFW-Scripts. Use with caution. \n. Thank you, Ward, for getting me going on this and providing the basic script framework I based this on. \n. Ward suggested writing a batch upload script using the current API. I have documents on my home system (not a visible server) that I'd like to publish elsewhere (joe.fed.wiki.org, say) without access to anything but a \"claimed\" SFW site. I don't really know where to begin - or even where to find API docs. \nThe use-case would be (a) convert some dusty docs to json, wiki format, in private and refine it there. I assume multiple, inter-linked, pages. (b)Push it to a federated site. (c) Let the world enjoy, troll, whatever.\n. Speculating here, since I don't feel comfortable with the code, but would it be hard to build a JSON factory, that lets you drop a json file,  or otherwise correctly formatted json, into a box and the story of the dropped stuff is incorporated into the current page? I assume at the point at which the factory was opened when the drop was made. \n. I'm still thinking about refactoring. I am thinking merging, not replacing. Once a factory (currently) is opened (with +), it can be moved to any break point before use. Then, to copy part of one page, I can grab the json of the existing one (source), edit it and then drop the result onto the json factory (in dest). Yes, the factory's result could be several paragraphs. This could count as one edit or several in the journal. The old page (source) isn't changed. \n. I'm about to upload an offline page splitter to https://github.com/JoeBergin/Batch-SFW-Scripts. Use with caution. It preserves journals and the original split file. \n. The renamPage batch/offline script will now replace the original page with a forwarding hint. \n. Running Express server, if I do that from my welcome page it points back to Ward's server. if I do it from a locally created page, it points to an empty page. I think it is chasing the fork in the welcome page. \n. I think that 1 is a bit dangerous, unless there is a confirmation first. I can see myself clicking accidentally and losing the recent version. Of course, if you could also go forward (undo) it wouldn't be a problem - unless you modify an older page. \nNote that you can now view the json for a page and copy from it, so you can recover the old version as it is. Time stamps would be nice, but whose time? Universal, local, ...\n. To be honest, I missed the context (within a string). I should have been paring, I guess. It was a manual edit that produced the error. But still, the server should trap and rescue. Well-formed inputs and all that. \n. Actually I was saying that the server shouldn't crash. Such a paragraph could be (a) ignored, (b) replaced with some user friendly message, (c) other, but not crashing. I assume the server normally runs as a daemon so it will restart quickly, but not crashing is better, certainly. \n. And I'll be more careful too. \nThanks and Happy Easter.\n. (3) This is related to the Refactoring issue #176 as well as this one. Suppose there were a UI (keyboard, gesture) action that would let you open a factory anywhere on a page (between paragraphs) rather than just at the bottom. Splitting paragraphs would then become easy. The journal could reflect where the factory opened, etc, so a minimal change to other stuff. \nI think it would aid page creation in general. \nThe gesture might be a double click, or a press-hold in a space between paragraphs. OR such a gesture could decorate the page with (+) icons in various places in which a new paragraph might appear (between existing paragraphs). \n. ",
    "Philippe-Arnaud": "The image was not loaded because of the browser setting which was set to \"DO NOT load images automatically\" in the settings window.\nYou can set the same environment for testing with Chrome by selecting the \"Do not show images\" located at chrome://settings/content under the images section.\n. ",
    "EliezerIsrael": "Re: 1) \nThis is also important for external indexing and seo.\nQuestion: What's ideal behavior on this?\n- Do we preface titles with an overall wiki name? \n- If one article is open, then it's reasonably obvious - the title body is the title of that article.\n- If there are more than one articles open, does the title reflect every article?  Is there a limit?\nIt looks to me like client/lib/state.coffee would be the place to set the title, in the pushState and popState functions.  Might be a good, relatively isolated, place for me to jump into the code. \n. Looks like these two as well, for the initial page:\nserver/sinatra/views/layout.haml\nserver/express/views/static.html\n. Experiments show that multiple users can edit the same page.\nhttp://sandbox.fed.wiki.org/view/welcome-visitors/view/lev-israel#\nIn that case isn't the above referenced wiki page incorrect?\nAnd if multiple users can edit the same page, what is the concept of ownership for, and when does a fork occur?\n:confused:\n. So ... the spec is correct, and the behavior that I see in sandbox is a bug?\n. On the philosophical/design point - I'm still digesting...  \nThe exquisite corpse example is fun, and I understand the evolutionary parallels.\nStill - isn't collaboration a value, alongside evolution?  To say that each version must be subsumed in order for positive growth to occur...doesn't sit well with me.  I'd love to see some form of communicating up or sideways, not just down.\n. I'm willing to go down the rabbit hole. :)\nThanks for the clarification on sandbox.  I won't use that instance to test out collaboration concepts anymore.\nI'm now editing http://ddd.fed.wiki.org/view/welcome-visitors/view/domain-driven-design.  I found that it let me edit, but has neither saved my edits, nor (as best I can tell) forked to a page. \nWhat should I expect to happen?  A copy of the edited page on lev.fed.wiki.org? \nPerhaps part of the issue is that I can't seem to log in on ddd.fed - it tells me \"this isn't your wiki!\"  I didn't realize that login was a claim to ownership, I thought it was a claim to identity. \n. I'm curious about that syntax question as well...\n. Okay - I may be just displaying my ignorance here, but I need to learn, so I'll just go ahead.\nIf I don't own a page, but I want to edit it, I understand that this makes a new copy of the page on my wiki.  \nSo - \n1) How does the page I don't own know who I am, or where my wiki is?\n2) Does this happen currently, is it part of the roadmap, or am I just off base here? \n. And of course, if I own a bunch of different wikis, hosed on different servers - where does that page I start editing live?\nI feel like I'm missing something obvious here. \n. Gotcha.  That helps. \nSo the \"login\" concept I'm looking for - where I identify who I am, and where my home wiki is - is currently determined by where I start my browsing session.\n. ",
    "finneycanhelp": "Thank you.\n. ",
    "donpdonp": "Im not sure what the way forward is but something should emerge with enough studying of OT and an understanding of how the wiki works/should-work. An infographic of the indivisible pieces of the wiki would be a nice start.\n. \"~>\" means allow the version to be greater than or equal to 1.3.1 in the last digit only. So 1.4 is \"too new\". \n. \">= 1.3.1\" should take care of it.\n. The minimal implementation I see is, on page load, check for an auth token for this federated wiki. If it exists, check the request headers for a matching cookie. If match, the visitor is the owner, otherwise visitor is a guest. If no auth token is recorded for this wiki, add a cookie with the token to the response.\n. ",
    "caquilino": "I love this feature. You could have chat revision history too. It'd be like a more granular Wiki \"Talk\" page. \nEtherPad and ApacheWave do real-time web chat and revision history too. I don't know the technical details. But maybe they could provide inspiration for how to do it in SFW. I don't program. I just advocate for Federated Wiki.   \nWordpress also has real-time chat and federated features: OStatus and other plugins.\n. ",
    "eungjun-yi": "It seems that this causes an error as follows:\n$ bundle exec rackup -s thin -p 1111\n/var/lib/gems/1.8/gems/rake-0.9.2.2/lib/rake/ext/module.rb:36:in `const_missing': uninitialized constant Encoding (NameError)\n    from /home/nori/src/Smallest-Federated-Wiki/server/sinatra/server.rb:10\n    from /home/nori/src/Smallest-Federated-Wiki/server/sinatra/config.ru:2:in `require'\n    from /home/nori/src/Smallest-Federated-Wiki/server/sinatra/config.ru:2\n    from /var/lib/gems/1.8/gems/rack-1.4.1/lib/rack/builder.rb:51:in `instance_eval'\n    from /var/lib/gems/1.8/gems/rack-1.4.1/lib/rack/builder.rb:51:in `initialize'\n    from /home/nori/src/Smallest-Federated-Wiki/server/sinatra/config.ru:1:in `new'\n    from /home/nori/src/Smallest-Federated-Wiki/server/sinatra/config.ru:1\nIt works fine after removing the line added by this pull request.\n. Now it works for me.\nI have removed ruby-1.8 and ruby-1.8-dev, reinstalled bundler, installed bundles: \n$ sudo apt-get remove ruby-1.8 ruby-1.8-dev\n...\n$ gem install bundler                           # It failed,\n...\n$ sudo gem install linecache19 -v '0.5.12'      # so I installed linecache19 using gem,\n...\n$ gem install bundler                           # and continue...\n...\n$ bundle install\nThen the wiki runs without error.\n```\n$ bundle exec rackup -s thin -p 1111\n\n\nThin web server (v1.3.1 codename Triple Espresso)\nMaximum connections set to 1024\nListening on 0.0.0.0:1111, CTRL+C to stop\n```\n\n\nI guess I should make sure my ruby version is enough to run this wiki, even if I executed rvm use 1.9.2.  \n$ ruby --version\nruby 1.9.3p0 (2011-10-30 revision 33570) [x86_64-linux]\n. ",
    "creatinglake": "As I not a programmer, I hope you all don't mind me chiming in.  I am very excited to see this project underway and hope to integrate it into a social networking platform in the future.  \nSo, from my perspective, Wikipedia is an inspiration and should be considered closely when determining what features will exist in Smallest-Federated Wiki.  I love the innovations you are bringing to the wiki, in fact I think they have profound implications.  That being said, I think having different types of blocks/entities besides paragraphs would only expand possibilities of expression through the wiki.  I am not suggesting having too many, but think allowing bullet lists, for example, just makes use more flexible and useful.  I see bullet lists in Ward's videos so maybe i am missing something here. \nAlso, why not integrate a standard editor that allows many types of word document type modifications within each block/entity?   Also, about the editor, and this is very important from the layman's perspective, I suggest having a WYSIWYG editor.  Mark-up languages are a barrier to entry for many people.  It may seem simple to you all, but it is not the language most people write in.  Although I think this model has worked for Wikipediai because it is a type of quality censorship, it seems that a federated wiki should be all about people to people sharing of anything with anyone.\nI guess what I am suggesting is a federated MediaWiki with an integrated WYSIWYG editor like http://ckeditor.com/.  \nNow that I am thinking about it, if there was a federated protocol written for MediaWiki, could all existing MediaWiki installations be upgraded and immediately able to operate as a federation?  Or is JSON allowing this drag and drop magic and not able to work with MediaWiki code?  \nI would really appreciate any feedback on these comments/ideas.  Thanks you all for your work on this project and am thankful to see that it is being develop. I hope I can help spread the word once it is ready for the masses.   \n. Thanks for being so open Ward.  I will be following this project closely and hope to integrate it with elgg, a open source social networking platform, at some point.\n. ",
    "davescruggs": "Well, I just tried it out, and I'm running into a gcc error:\nCOMPLIATION ERROR: error executing \"gcc -shared    -O3 -fno-omit-frame-pointer -ggdb -Wextra -Wno-unused-parameter -Wno-parentheses -Wno-long-long -Wno-missing-field-initializers -Werror=pointer-arith -Werror=write-strings -Werror=declaration-after-statement -Werror=implicit-function-declaration -L.  -I C:/Ruby193/include/ruby-1.9.1 -I C:/Ruby193/include/ruby-1.9.1/i386-mingw32 -I C:/Ruby193/include -LC:/Ruby193/lib -o \\\"C:/Users/David/.ruby_inline/ruby-1.9.1/Inline_String_9a6686966a50705d94442f198bc2c408.so\\\" \\\"C:/Users/David/.ruby_inline/ruby-1.9.1/Inline_String_9a6686966a50705d94442f198bc2c408.c\\\"   -Wl,--enable-auto-import -LC:/Ruby193/lib -lmsvcrt-ruby191 2> nul\": pid 6208 exit 1\nRenamed C:/Users/David/.ruby_inline/ruby-1.9.1/Inline_String_9a6686966a50705d94442f198bc2c408.c to C:/Users/David/.ruby_inline/ruby-1.9.1/Inline_String_9a6686966a50705d94442f198bc2c408.c.bad\nC:/Users/David/Documents/Projects/Smallest-Federated-Wiki/server/sinatra/server.rb:26:in `': No such file or directory - git log -10 --oneline (Errno::ENOENT)\n    from C:/Users/David/Documents/Projects/Smallest-Federated-Wiki/server/sinatra/server.rb:26:inclass:Controller'\n    from C:/Users/David/Documents/Projects/Smallest-Federated-Wiki/server/sinatra/server.rb:21:in <top (required)>'\n    from C:/Users/David/Documents/Projects/Smallest-Federated-Wiki/server/sinatra/config.ru:2:inrequire'\n    from C:/Users/David/Documents/Projects/Smallest-Federated-Wiki/server/sinatra/config.ru:2:in block in <main>'\n    from C:/Ruby193/lib/ruby/gems/1.9.1/gems/rack-1.4.1/lib/rack/builder.rb:51:ininstance_eval'\n    from C:/Ruby193/lib/ruby/gems/1.9.1/gems/rack-1.4.1/lib/rack/builder.rb:51:in initialize'\n    from C:/Users/David/Documents/Projects/Smallest-Federated-Wiki/server/sinatra/config.ru:1:innew'\n    from C:/Users/David/Documents/Projects/Smallest-Federated-Wiki/server/sinatra/config.ru:1:in <main>'\n    from C:/Ruby193/lib/ruby/gems/1.9.1/gems/rack-1.4.1/lib/rack/builder.rb:40:ineval'\n    from C:/Ruby193/lib/ruby/gems/1.9.1/gems/rack-1.4.1/lib/rack/builder.rb:40:in parse_file'\n    from C:/Ruby193/lib/ruby/gems/1.9.1/gems/rack-1.4.1/lib/rack/server.rb:200:inapp'\n    from C:/Ruby193/lib/ruby/gems/1.9.1/gems/rack-1.4.1/lib/rack/server.rb:301:in wrapped_app'\n    from C:/Ruby193/lib/ruby/gems/1.9.1/gems/rack-1.4.1/lib/rack/server.rb:252:instart'\n    from C:/Ruby193/lib/ruby/gems/1.9.1/gems/rack-1.4.1/lib/rack/server.rb:137:in start'\n    from C:/Ruby193/lib/ruby/gems/1.9.1/gems/rack-1.4.1/bin/rackup:4:in'\n    from C:/Ruby193/bin/rackup:23:in load'\n    from C:/Ruby193/bin/rackup:23:in'\nI'll poke around to see what I can figure out. Is there a place I should look to see if someone has tried this out on Windows?\n. I tried adding {program files}\\git\\bin and {program files}\\git\\cmd to my path, and I still got the error.  I didn't post some of the other things I tried.  Cmd.exe and bash both had issues (git bash had more). \nI can give your suggestion a try--and I will. \n. ",
    "krassif": "Just initiated a pull request on the initial implementation for Google Chrome. Please review.\n. Sure. Glad I could help. It is always easy to start with Chrome.\nCouple of general thoughts:\n- The other sharing options out there, though not collaborative in terms of the Federated Wiki, seem to be easier to add data to. The browser extension can help a lot making the Wiki single click and competitive.\n- There may be a need for more sophisticated HTML parsing that to certain extend preserves the original page look and feel. The right place for it would be the client branch (the browser extension is optional in my view).\nExtension related:\n- The extension script runs sandboxed, and the communication with the page script (client branch) itself must go through the DOM. Had to establish communication channel for the Peer Belt's extension not long ago.\n- The Chrome/Safari extensions need a certificate as part of the build/packaging process. Github may not be the right place for the certificate files. Thoughts on where to host these? \n- There is a simple shell script that automates the Chrome extension build. Once the certificate location is known, will add the script to the project.\n. Implemented extension wide content cache to handle new Wiki tab case. If no Wiki tab is available when the user clicks Add to Wiki, the extension opens the Wiki page in a new tab. While the page in the new tab is loading, it cannot handle external requests. The extension caches the content instead of shooting in the dark. Later, when the Wiki page is ready, the page itself initiates a content pull request. If there is content waiting, it is being transfered to the Wiki page for processing.\n. Thanks. Let me know how the review goes and what the next steps may be.\nCheers, -K\n. ",
    "RandomEtc": "Thanks - I'm up and running with the Sinatra server on Heroku. After I've kicked the tyres a bit and worked out what goes where I'll try the Node server locally and see what it would take to get it running on Heroku as well. I imagine one issue might be that the repo is already set up for a Rack-based Ruby app (the presence of config.ru triggers this on Heroku I believe) - it might be possible to manually set the Heroku build pack so that node.js takes precedence. Time for tinkering begins ... now :)\n. A couple more things I've noted as I start to read the coffeescript, which I leave here for myself or a future node-positive contributor to tackle:\n- as noted above, and in server.coffee, a new page factory will need to be implemented and selected when running against a database\n- the owner and favicon methods in server.coffee also use the filesystem and need abstracting to use the db if available\n- the express session should use a database too (I favor https://github.com/visionmedia/connect-redis for this, there's a free redis addon for Heroku) \n- alternatively since it is small (just user id?) you could store the entire session in an encrypted cookie and not require a database for the session at all https://github.com/caolan/cookie-sessions\n- regardless of session store, the session secret should be configurable to avoid spoofing a user's session\n. Thanks for the timely response. Do you think my defeat of the algorithm means that there should be a better algorithm, or that I've misunderstood what the algorithm is doing for me and I should try harder to work within its limits? :)\nI'm still catching up on the project history; reading https://github.com/WardCunningham/Smallest-Federated-Wiki/wiki/Federation-Details now and also slowly starting to read the code. I'll also go back and watch more of your videos, because I think I missed something somewhere about the underlying principles. I welcome your guidance on how best to learn about the federated side of things and how content is chosen for display.\n. You can certainly catch the click event on all a tags and prevent the link from being followed, in jQuery:\njavascript\n$('a').click(function(e){ \n  e.preventDefault(); \n})\nThis also catches command-click (which you can detect by looking at e.metaKey on mac, not sure on windows). However I'm not sure if it's then possible to trigger another action that would result in a new tab. If not then preventing it would be a bad thing, usability-wise.\nA better solution could be to write the href of the a tag to give the new tab enough information to behave correctly, but I'm not sure what the initial tab has to hand that the new tab doesn't (regarding the requirements for resolving a page across the various federated sources). Yet. I will keep reading around!\n(update: no need for return false in the jquery click handler)\n. I should say that I am a big fan of the stacking of paths and pages to the right as I navigate. And I really like the way this enables other widgets, such as charts, in the same browser window. I hope to play with this soon.\nPlaying around a little more, I now understand that navigating to /view/welcome-visitors does not load the same html from the server as loading /welcome-visitors.html.\nAnd I understand that because of the on-demand nature of the search that happens when clicking on a link, you can't write the hrefs ahead of time to be the URL that eventually shows up when the link is clicked and the search completes. That's because clicking the link might result in navigating to a page that is sourced from another wiki. (Halting problem is a haunting problem... I never think it's relevant and yet, there it is).\nPerhaps, when viewing e.g. /view/recipes, instead of href=\"pancakes.html\" the link could be href=\"/view/recipies?q=pancakes\". That way if there's no pancakes page on the current wiki, and it finally resolves as /view/recipes/friend.example.com/pancakes, then opening in a new tab would be able to resolve the same connections and show the exact same results?\nThe other loose end in this is that the .html representations would end up truly being for bots only, and never exposed to browsers with javascript active in a normal wiki reading session. I'm not sure if that's a bad thing or not, but it would make my issue #244 (the style of the journal in the html representation) largely irrelevant :)\n. ... and confusion fades to annoyance :) ...\nThanks for continuing to clarify the various things I'm stumbling into. Much appreciated!\nIf this issue is to be left open I would like it to be more actionable for you and other contributors, but I think there are several action items (the ones you outline above already warrant individual action items). I defer to you on whether you want to create separate issues for all these or not:\n- as you say, saving a new version of page into local storage should add the \"fork\" action to the local version's journal, so that the original page can be found again\n- when viewing content that's served from local storage, perhaps the URL should be different (/view/foo could be /local/foo). This /local/... URL could be made to work even when logged in (and server content could continue to take precedence at /view... URLs). When logged-in the /local version could offer ways to merge/copy/push the content back to the server, and the /view version could likewise offer ways to merge/copy/pull the content from local storage.\n- if you're viewing a page from local storage (ie you're not logged in) the client UI should invite you to log in so you can save that page to the current wiki (something like \"Edits to this page will be saved to your browser's local storage. If you're the owner of this wiki, log in to save edits on the server.\")\n- the client UI should indicate if edits will be saved to the browser's local storage (and invite you to log in) (this could perhaps go in the footer, or use similar yellow styling to the pages which are served from local storage)\n- if you're logged in and there's a page in local storage that matches the current page, the client UI should give you a way to access that content, and a way to merge the content into the current page (even if the pages have diverged?)\n- the client UI (perhaps the footer) should have a visible link to admin pages such as /view/local-editing (should such pages be locked to avoid newbie editing/clobbering?)\nI confess at this point I am only really thinking through the divide between local storage and \"my\" wiki, and not whether the same terminology and UI makes sense when considering other federated sources. I also have only just come to understand the nature of claim/log-in, and have some suggestions on that front too:\n- once a wiki is claimed, the \"login\" UI should be changed to only accept log-in from the the original claimant\n- the \"oops this is not your wiki\" page should explain about federated wikis and offer links and guidance on how to get your own (this is shown if you claim a wiki with one OpenID and then logout and attempt to log in with a different one)\nIs it core to your thinking on federated wikis that each wiki is run by a single author? Or are there ways to share a single wiki? (I'm aware of the farm terminology, and the general goal that it should be quick/easy to get a new federated wiki for yourself, but that's about it.)\n(Sorry for long issues/comments on a Sunday, please let me know if you'd prefer another channel or means of discussion... I am pretty close to forking and implementing some of this but that may take a little longer!)\n. ",
    "6e441f9c": "Why not mark the link as a ink to the remote origin server where data lies anyway? So \"Open in a new tab\" would open the link on its own native origin server, as would \"Open link here\" functionality bypassing JS present in some browsers. \n. Well, I am not sure what exactly he meant. What I would suggest is the following: \nAfter rendering the new page start resolving the links as if they were clicked and cache the result. For each succesfully resolved link script will find out what page from what wiki would be requested. Then you could alter the acnhor element's href attribute to point to an URL that would instruct your wiki to pull that precise page from that remote wiki.\nOf course, if all this is just considered lower priority than, for example, search - then it is another matter. And, probably, merging back forked pages is a more critical problem to solve..\n. ",
    "dtsato": "Another small refactoring, to test attaching commits to the original issue :)\n. This was related to Issue #253 (not sure why Github didn't ask me to attach the code to the previous issue)\n. ",
    "hng": "I've tried calling MathJax only from emit, but it doesn't work. This is the only solution I could come up with that works properly. The only way to call MathJax (without errors) from the plugin seems to be the getScript callback.\nAlthough this is just what I think happens, because the code has almost zero comments and no api documentation. It should be easy to setup some Rake task that compiles documentation for all code (e.g. yard (http://yardoc.org/) for the ruby code and codo (https://github.com/netzpirat/codo) for coffeescript). That would make contributing much easier! :)\n. ",
    "eventi": "Steps to reproduce, I added my name, claimed the wiki using Google, clicked the new link, and it dies.\nThe problem is when the oops page is returned to the remoteGet function (server.coffee Line 156) \nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/blob/master/server/express/lib/server.coffee#L156\nIt's being passed to JSON.parse even though it's not valid JSON\nI put a few console.log() into the server.coffee to demonstrate, and here's the response data passed to JSON.parse.\n``````\ngetopts.path: [[/eugene-ventimiglia.json]]\nresponsedata: [[\n\n\n\n      Page not found\n    \n\n\n]]```\n``````\n. I'm sure it's my lousy description, not your density :)\nI followed the instructions on the \"Welcome Visitors\" page, edited the first factory to be [[Eugene Ventimiglia]]\nI can edit the Eugene Ventimiglia page after creating it, but when I \"claim\" the page and click the link to edit the page again, the server crashes.  It does successfully authenticate (I see the logout button).\n. ",
    "moredip": "Apologies for the noise of the merge commits in here - still trying to get my github workflow figured out when I'm playing the role of contributor rather than maintainer :)\n. ",
    "saper": "It seems like those multiple icons are hard coded in the default-data/pages files. \n. Maybe I should rephrase it not to look completety stupid:\nto the inane user, journal icons look like \"action\" icons - icons that do change something. In a way, they represent past actions and they do something (they present past revisions). But it is contra-intuitive, since we do the actions on the text (much better than editing via extra buttons).\nMaybe original letters (as shown in the video) were \"less promising\"?\nMaybe it would be good to experiment with the design (for example a narrow, high block instead of the square will probably not suggest it being an action).\nHighlighting is completely unintuitive to me - although I think I understand how a set of JSON actions represents a revision, I currently don't get how non-consecutive actions get highlighted together.\n. ",
    "StephanEggermont": "On OS-X 10.6.8 with Safari 5.1.7, it looks more like \n   {\n      \"type\": \"data\",\n      \"id\": \"114a82cd2a8683f2\",\n      \"text\": \"Unexpected Item\",\n      \"data\": {\n        \"number\": 4,\n        \"url\": \"file://localhost/Users/stephan/Desktop/DSC_1577.jpg\",\n        \"types\": [\n          \"dyn.ah62d4rv4gk86rexmsv4u\",\n          \"dyn.ah62d4rv4gu8yc6durvwwaznwmuuha2pxsvw0e55bsmwca7d3sbwu\",\n          \"CorePasteboardFlavorType 0xC4706431\",\n          \"com.apple.finder.globalAnchorPointFlavor\",\n          \"text/uri-list\",\n          \"com.apple.finder.pasteboard.bnch\",\n          \"CorePasteboardFlavorType 0x6F726769\",\n          \"AppleCoreDragItemBounds\",\n          \"CorePasteboardFlavorType 0xC4697475\",\n          \"public.file-url\",\n          \"dyn.ah62d4rv4gk86rexuqu2u\",\n          \"Files\",\n          \"dyn.ah62d4rv4gk8086xhre\",\n          \"dyn.ah62d4rv4gu8y6y4grf0gn5xbrzw1gydcr7u1e3cytf2gn\",\n          \"CorePasteboardFlavorType 0x6675726C\",\n          \"CorePasteboardFlavorType 0xC46C6E6B\",\n          \"dyn.ah62d4rv4gk86rexqr3zu\",\n          \"CorePasteboardFlavorType 0xC469746E\",\n          \"dyn.ah62d4rv4gk86rexmsv1a\"\n        ]\n      }\n    },\nThe chrome version works though.\n. ",
    "zacharyharris": "It can be foolish to try to speak into matters way beyond one's level of understanding, but if I say ridiculous stuff here, please just apply Cunningham's law.\nAs a thought experiment, say that an arbitrary story item with a \"private=true\" attribute gets served to unauthenticated users as a \"type=private\" item with the same \"id\" as the original item, and contains no members other than these two required ones (so none of the original \"meaty\" content is served). Say then, for example, that \"type=private\" items are rendered with a \"censored\" icon. Now we are leaking the fact that private material has been placed in certain locations, though not leaking the private content itself,... but let's say for this thought experiment that leakage of the existence of private material is acceptable.\nNow, consider how various actions on the client interact with a \"type=private\" item:\n1. move: I see no problem here because the original \"id\" is intact.\n2. edit: Disallowed. Does nothing.\n3. add: You don't add \"type=private\" items; you add regular items and mark them a \"private=true\" attribute.\n4. remove: No problem. Remove the item with this \"id\".\n5. fork: I don't think there is a problem here.\nWardCunningham wrote:\n\nI find that in my world I have many wikis, some private, others not. From my view they are federated. The outside \nview is of public wikis with pages that sometimes reference sites that are not available.\n\nHmm. That bears some interesting resemblances to the personal wiki infrastructure I just set up, although in my world it is just two wikis---one totally private and one that is public-readable except for masked transclusions from the private one. If you happen to have any further public references that comment on the structure or interaction regarding your \"world of wikis\", I'd be interested.\n. ",
    "spencermountain": "thanks!\nyou're the best!\n. ",
    "mkelleyharris": "These two commits represent new work that you have not yet pulled.  You'll get real calcs:\nAdd initial Efficiency calc strategy function using two gray-scale bins.  f4f143d\nAdd a new calc algorithm for Efficiency Plugin. Clustering. Not calle\u2026   2971886\nI think it would be safe to take all three commits. The later two also include updates to the efficiency.js file.\nSorry for the confusion. I'd issued a pull request for the first efficiency.js file. Then when I did two new commits, I tried to issue additional pull requests but new commits were associated with the early pull request. I'll need to learn how to separate pull requests.  Let me know if I should reissue pull requests.\n. Thanks for the update Ward. I do think the two added commits are useful\nprogress.\nOn Fri, Nov 2, 2012 at 10:31 AM, Ward Cunningham\nnotifications@github.comwrote:\n\nOk. Our duplication of effort has interfered with GitHub's web based\nmerge. I'll perform a manual merge once I get home. Sorry for the delay.\nhttps://help.github.com/articles/merging-a-pull-request\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/WardCunningham/Smallest-Federated-Wiki/pull/304#issuecomment-10022369.\n. I agree with your solution of not offerring editing of ghost pages. That's simple and clean.\n\nI didn't see obvious error indicators in the GUI itself.\n. I sure support you following your intuition on this issue.  One possible hybrid is to show the last n lines of the history, with a more button, if people want to seem more.  If most pages have a small number of edits, this won't come up often.  On one of my pages I had over 80 edits and the history started taking up significan real estate ...\n. Your idea of offering \"light\", \"heavy\" and \"complete\" variations sounds very complete and powerful for an author.  One extra possibility would be to provide a version number with each edit so that people could reference a specific version. The version could be a increasing number for that page. Or it could be a SHA1 hash of the page, or just a date-time stamp.\n. Got it. Thanks.  http://ward.fed.wiki.org/reflections.html, does get the person there. It does currently repaint twice.  It might not be obvious to people how to construct such an address to which the want to refer.  People are probably trained to copy the URL.  Might help to provide a \"Get Link\" button.\n. Sorry about the false alarm/request. I was apparently in some odd use case, whereby the pages were not responding, and back button appeared to be revisiting prior history of the active page, when I really wanted it just to scroll. I'll be on the look out for a repeatable example. It might be just a corner case bug, and not a big deal.  I was in content mode tonight, and I noticed this confusion I'd seen numerous times before, and sent in the issue. I'll try to charecterize it better.\n. Powerful idea.  Possible tree & leaf relationships.   At the very least it\ncould be a hierarchical indexing strategy.  One master wiki that holds\nother wikis.  It could be a candidate as the default behavior within a farm\nof wikis.\nOn Sun, Mar 16, 2014 at 12:21 PM, Ward Cunningham\nnotifications@github.comwrote:\n\nWhat would be the purpose of nesting one wiki within the pages of another?\nI tried this yesterday by inserting the entire cave.fed.wiki.org content\nas a data item in another wiki. To my surprise this worked just fine. The\nData plugin written many months ago even scrubbed through the page names.\n[image: screen shot 2014-03-16 at 9 39 20 am]https://f.cloud.github.com/assets/12127/2431392/87e48d2e-ad2c-11e3-91c2-757f38de118d.png\nThe thumbnail says 1x171 to indicate the data is organized as 171 rows by\n1 column, the pages of the embedded wiki.\nWhat more can we do with this?\nThis question is important to me now as I consider refactoring the core\njavascript and especially its relationship with plugins and storage\nmediums. Here a plugin on one page has become the storage medium of a whole\nadditional wiki.\nWhat might work:\nImagine that upon viewing the Cave Wiki as Data page its embedded wiki\nbecomes just one more read-only site in your neighborhood. I'm probably\nonly 10 lines of javascript from making this much a reality. Then what?\nOf course there are large object issues that we already face with images.\nIf it helps, imagine wiki had an asset manager that allowed it to hold\nlarge photos and even movies. How is a wiki of wikis of wikis ...\nnecessarily different?\nI'm feeling a need for moving whole sites around from platform service to\nplatform service. I do this now for a few sites using rsync. Does a wiki of\nwikis make this just another drag-and-drop refactoring?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/WardCunningham/Smallest-Federated-Wiki/issues/409\n.\n. This general idea may support aggregating multiple wikis of data. e.g. USA\nsustainability data could be aggregated from separate states' wikis. This\nallows divide and conquer for people's projects.\n\nAlso dashboards could aggregate data and subscribe to updates from the leaf\nwikis.\nRecursive nesting ...\nOn Mon, Mar 17, 2014 at 2:02 PM, Ryan Bennett notifications@github.comwrote:\n\nI've been pondering this very scenario in thinking about wiki revision\ncontrol:\nIf we have semantic linking between pages and the story items they contain\n(including other pages), then one could see having a group of 'collection'\npages that contain categorized pages relevant to a subject area... as one\nor more of these referenced pages change, the updates could bubble up to\nthe parent and propagate to other peers that maintain a version of that\nsame parent page, which means you could effectively subscribe to updates\nfor a page without needing to keep a copy of that page on hand.\nI've got a very rough outline of a linked data model that might be\nrelevant to this conversation.\nhttp://rosewiki.org/view/wik-dvcs/view/wik-data-model/view/wik-json-schema\nTL;DR: every story Item and page is accessible via URI, page stories\nconsist of an array of story item or page uri's. URI's are versioned on\nupdate and are constructed as a combination of 'type', id, publisher id,\nand timestamp. A major implication of this model is that we would need to\nrework how links are rendered, as page slugs would no longer be the primary\nkey for looking up a page.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/WardCunningham/Smallest-Federated-Wiki/issues/409#issuecomment-37856471\n.\n. > This reminds me again why one should not build on other people's services.\n:-)  Quite a pattern observation in the age of the cloud and many web\nservices.\nMaybe like any dependency, the Dependency Inversion principle could be\napplied. With SFW dependency on an authentication interface, the concrete\ninstances can be swapped out,  Of course, I know you know this.   Just\nbrainstorming out loud ...\n\nOn Sat, May 31, 2014 at 5:51 PM, Ward Cunningham notifications@github.com\nwrote:\n\nIf one attempts to claim a site with the (G) button for OpenId\nauthentication then this process fails as follows:\n- Google authentication page comes up.\n- User agrees to authenticate\n- Google quits with 404 explaining that site is not already registered.\nTheir exact message is:\nOpenID auth request contains an unregistered domain: http://foo.fed.wiki.org\nI interpret this to mean that they are only authorizing logins to sites\nthat have already been claimed. This is good for a while but they say that\nthey are withdrawing even this.\nhttps://developers.google.com/accounts/docs/OpenID\nThis reminds me again why one should not build on other people's services.\nThe (Y) choice seems to still work so those willing to make Yahoo accounts\ncan claim sites with their OpenID.\nWith Persona facing a similar fate it may be time to write our own login\nsystem.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/issues/415.\n. I'd welcome some hints on how to login to my account: http://kelley.fed.wiki.org/view/welcome-visitors, to add content.  I see this when I log on with Google (As expected from this thread. Page originally claimed with Google.):\n\"OpenID 2.0 for Google Accounts has gone away\"\n\nThen I tried my Yahoo account, and got:\n\"This is not your wiki\"\nThank you.\n. ",
    "dhemery": "I was using thin.  I switched to webrick, and I think I had the same problem. (It's possible that I didn't clear my browser history after switching to webrick, so I don't know whether my browser history contaminated my exploration of the problem.)\nRather than explore the problem further, I've switched to the express server, and no longer have the problem.\n. ",
    "andyl": "Are multi-threaded servers overkill?  Jruby / Rubinius dependencies add complexity.\n. ",
    "kyledrake": "This improvement is good for both MRI and the threaded versions. MRI will be able to do concurrent IO work with this change. This does not affect JRuby/Rubinius compatibility, and I think that should be treated as a separate issue.\nSwitching to Puma removes the EventMachine dependency (Puma is pure-ruby with a cross-platform C or Java based HTTP parser) which actually reduces the complexity by taking a lot of code out. EventMachine occasionally has compatibility issues too, it didn't work with Ruby 2.0 for a while IIRC.\n. ",
    "jasonkgreen": "This issue seems to have been quiet for some time.  Does anyone know the current status of the tablet UI work?\n. Aha,  The port 3000 of the original instance is shifted to port 80 (I think) when it goes through pagekite.  It may cause a hostname change as well. Is there a way to get SFW to run over port 80?\n. I had to use pm2 to  start it b/c of some node flakiness. Would you end up with something like\npm2 start something.js -p 80 --url http://site.i.want.example.com\n?\n. Paul -- It didn't throw an error when I passed the parameters, but I still couldn't get Persona login to work. Not sure why\nJon -- It's only login and editing that won't work remotely.  I can fork from it without problem.  My Persona difficulties mean that I have to be logged in from the VM localhost to make changes.  This is a smaller problem in the grand scheme of things, \nI'm now getting an error about alternate addresses, which is more than I was getting.  The --port flag doesn't seem to affect the port SFW is running on.\n. Got it.  Actually it's \npm2 stop [id of wiki process]\npm2 delete [id of wiki process]\npm2 start ./index.js -- -p 3000 --url http://mysubdomain.pagekite.me\nYou need to tell persona what port SFW is running on, rather than the port it's forwarded to via pagekite.\nThanks for all the help.\n. ",
    "thoka": "Is there some collection of problems facing using fedwiki on a tablet ?\nOne, that I think of, is the problem of missing modifiers for actions.\nSince tablets are multi touch devices, this could be \"simulated\" by having a floating modifier palette on top of fedwiki, which allows to alter the action of the other finger.\n. ",
    "orcmid": "When I went through the little squares at the bottom of the orcmid.fed.wiki.org Welcome page, I did get to a page that had a large number of Can't Make Sense entries.\nCLUE: The errors I am seeing arise when I am shown as already-logged in.  When I was not shown as Logged in just now, I could edit the Welcome page just fine.  Then when I logged in (using OpenId), I got an error immediately.  Logging Out cleared it up.\n[I am also operating on a Windows 7 Ultimate system running Internet Explorer 10 Preview.]\nThis does seem specific to the Welcome Page.  I followed the instructions on customizing it.\nCuriosity: Is browser-local storage being used for this?  Am I seeing a sync problem that I don't understand?\nTabs: Hmm, there might be other tabs open at times.  There wasn't in my last experiment where I saw the Logged In issue so clearly.\n. [Trying direct reply]\nGood idea.  \nYes, I do see the Yellow Border.  Bummer.  I have a lot of yellow border on those pages.  Interesting.\nI'm \"orcmid\" on Skype.  I'm going to make a late lunch and return to my desk by 4:15pm PST.  \nI'd rather not make orcmid.fed.wiki.org go away just yet.  If you think there is something I should do differently when starting over, let's talk about that.\n- Dennis\n-----Original Message-----\nFrom: Ward Cunningham [mailto:notifications@github.com] \nSent: Thursday, January 24, 2013 14:05\nTo: WardCunningham/Smallest-Federated-Wiki\nCc: orcmid\nSubject: Re: [Smallest-Federated-Wiki] orcmid.fed.wiki.org Problem with Errors and Failure of Changes (#341)\nWe do use browser local storage when we can't write to the origin site. Pages from there are rendered with a yellow border.\nWe don't have a lot of experience with Windows. Perhaps we could skype and look at this together? That would be the way to learn something. I could also just edit the db and make it go away if that is your preference. \n\u2014\nReply to this email directly or view it on GitHub https://github.com/WardCunningham/Smallest-Federated-Wiki/issues/341#issuecomment-12676347 . \nhttps://github.com/notifications/beacon/Jshd8sI44GVrKZBvymxqKG_WGOVcinwYxF6LVRG5ZnUXMa9YHmv82nPtVOHpEwtF.gif \n. @WardCunningham - The problems that I have do seem to be cured by \"forking\" the browser-held pages, and being logged-in.  I also confirmed that this cleans up well from IE10 Preview in Windows 7 as well as the IE10 that is bundled in Windows 8.\nI did receive a JSON Parser error by deleting all of the text in a paragraph in order to remove the paragraph from  a page.\nThanks for the Skype call.\n. ",
    "sergueif": "Yep, same situation. Demo to friend. edit his page -> auto-fork -> overwrite my own copy with all history lost. +1\n. ",
    "cScarlson": "Added an update to issue #345, since Ward's try-catch implementation -- still some errors however.\n. This try-catch worked for me (author).\nStill throws errors for the plugins: linkmap, parse. Which I need more information about as there still seems to be a rather crucial error:\n\nHere's the TypeError:\n\nps. Ward, thanks for jumping on that so quickly.\n. Hey guys, thanks for the pointers.\nThe problem is that factory.js holds/maintains an http statusText of 'pending' (0B/13B loaded). Might this happen because the .coffee is being compiled? - (@hallahan, is this what you're referring to above?).\nI'll continue to trouble shoot this issue off & on.\n. There is a client/.../factory.js.\nThe current behavior is that the factory div (gray container) displays, however, none of the links appear inside of it, after about 6 minutes chrome cancels the request/GET fails.\n. ",
    "holtzermann17": "BTW: this is potentially a copy of #275?\n. AFAIKT, it wasn't caused by me, but by another user - so if that info isn't\nlogged, I can't be of much further help.\n. OK, got it sorted out.  After ruby build.rb, I had to do:\nmv ../pages/recent-farm-activity ../farm/metameso.org/pages/\n. OK, looking better (i.e. functional) in that case.  Cheers!\n. How about the case of relatively simpler server-side plugins/hooks?\n. Caveat: programming-wise I am coming from an Emacs and Drupal background -- two places where hooks rule (as explained at those links... and especially for Drupal).  If the right way to extend RoR/SFW is not via hooks, that's OK with me - the question is just a matter of feeling my way around a bit (as someone with no RoR experience).\n. ",
    "davidascher": "STR is unclear - some paragraphs edit seem to trigger it, some don't.\n. ",
    "andreypopp": "just FYI, there is also commonjs-everywhere module which does the same as browserify but with coffee support out-of-the-box. Personally I don't the like the idea of specifying the .coffee extension in requirements.\n. ",
    "jimcam": "v0.10.3\nOn Apr 24, 2013, at 3:33 AM, Paul Rodwell notifications@github.com wrote:\n\nThe wiki has been updated, so others will not walk into that problem.\nSeems somewhat strange though - SFW works with node v0.10.3 on Windows. (v0.10.5 is now the latest)\nDo you know which of the 0.10.x version node-latest.tar.gz gave you?\n\u2014\nReply to this email directly or view it on GitHub.\n. Verified.\n. \n",
    "ozten": "@nm - I'd love any help and feedback. I'll get this patch cleaned up, based on the TODOs.\n. Forced pushed an update:\n- Finished Persona based middleware\n- Page uses session state to properly display signin/signout UI\n. Hi nm. I think we're in pretty good shape. I've done all the TODOs I had written for myself.\nWhat do you think? To try it out.... Remove your data/status/open_id.identity file, so your wiki is unclaimed.\n. Sorry about that. I'll try a fresh deployment, to try to reproduce the crash.\n\nWhile it looks, in server.coffee, as if like the app.get ///([a-z0-9-]+).html$///, (req, res, next) -> section needs owner: owner adding to the info data structure, and I guess console.log 'req.session', req.session as well; that is not the solution to the problem.\n\nI don't think owner should be in that line of code.\nMaybe my coffee-script is wrong, here is what I'm trying to do:\nIn server.coffee where owner is in scope... create a function which you can call and it returns that owner value.\nAt runtime for each request... pass this function as an argument to authenticate_session and to verify_assertion\nThe persona_auth module exposes two functions. Each function, called when processing a request, passes this getOwner function, so that the code can find out what owner is set too in server.coffeee.\nEvery request would hit the authentication middleware which will call getOwner() which is passed in.\n. Finished deploying on dotCloud. I didn't reproduce any of these problems, I'll keep digging.\nYou can see it here:\nhttp://aokwiki-ozten.dotcloud.com/view/welcome-visitors\n. @paul90 +1 thanks for keeping this pull request focused and small.\n. Thanks for the tip.\nI deleted the favicon.png and my identity under data.\nI now see the two problems your reporting. Investigating.\n. This is in much better shape. Took me a while to debug, sorry about that.\nThis is ready for review again.\n. @nm @paul90 - I'm happy to join a Google Hangout or call where we can jam on this. ozten AT mozilla DOT com is my email address for coordinating.\n@paul90 thank you so much for testing this branch. Do sessions work as you expect now, I didn't see pages getting saved to the backend after logging out. I'm so new to this project, that I don't quite know what to look for with manual testing.\n. I've deployed an OpenID wiki in addition to my Persona enabled http://wiki.aok.io site.\nThe problem with the favicon could be a timing bug and related to how Persona does session management.\nI've started a page that explains how Persona Sessions work.\n. Here is an unclaimed wiki running the latest code.\n. Paul and I jammed on this for a bit today via Google Hangout.\nOne note: If you want to convert your various wikis to Persona in bulk, the easiest thing to do would be to update all the data/status/open_id.identity files with your email address. That way they will stay claimed.\n. Thanks for pointing out a few issues, which I've fixed in be3824b.\n- login or logout events now redirect to homepage\n- owner and current user are really two different concepts. Passing req.session.email as user to the page\n- Turns out there were two logic paths to static.html, I had only updated one of them\nTested locally and updated mine http://wiki.aok.io and yours http://cleanwiki-ozten.dotcloud.com\n. I didn't test a clean install. these are bugs in my coffee script code, not in Persona proper.\nOn a clean install, the page refreshes because\n    var owner = '';\nThat should either be null or a valid email address. I'll isolate the bug and fix. Sorry about that.\n. You're seeing this issue with my latest patch? This should be fixed now.\nThe login/logout events are geneated by navigator.id.watch when the Persona session and the wiki session don't agree. loggedInUser must be either null or a valid email address.\n. Okay, I was using the wrong event callback. After switching to onready to show the auth UI, everything is dandy in IE 10.\nSorry about that. I think this patch is unblocked again, for review.\n. Merged from master and tweaked CSS to align Search and Sign in button.\n. How are we feeling about this pull request? Anything I can do?\n. I've submitted d6f5519c9973a8649f5b949e294251f0a05a9e63 which renames the status file to persona.identity.\nI think not doing this earlier was a mistake.\nNow you can run a wiki on Ruby with OpenID, then switch it to Node.js with Persona. The data directory can happily coexist with open_id.identity.\n. This is great news!\nA new focused repo makes a lot of sense.\nCongrats on the new repo!\n. I've started two pages for some thoughts on Persona Sessions and \nVisitors\n. Is the protocol, hostname or port number different when you are using Pagekite?\nThe Persona config values must match what is in your address bar, otherwise an \"audience mismatch\" will happen.\n. Good call! This bit of information lives server side and we need to access it in JavaScript. What do you think would be better? How about: 436e5a9ea4ae78\n. This fixed one of the failing unit tests.\n. This fixed the second failing unit test.\n. I see, that makes a lot of sense.\nI've created a Google Hangout and invited you and @nrn.\n. This could be written better, but I don't really know Coffee, so this is the best I could do.\nWhen I had this as a one liner, it was the source of the refresh bug. Maybe land the patch with this and clean up in  a refactor?\n. ",
    "christiansmith": "Great working with you, @nrn! Hope we get to do it again. Thanks for your time.\n. ",
    "pstuifzand": "It works at the moment for docker build github.com/pstuifzand/Smallest-Federated-Wiki. This is quite a bit easier than following the installation instructions from the Readme. At the moment it only creates a new server. It keeps the changes, but you should start from the later image not from the Dockerfile itself if you want to stop and restart the server.\n. I have to say that I'm still trying out Docker and learning about it. I thought it was really cool to see that it was so easy to create a Dockerfile for SFW. I was as easy as writing down the installation instructions.\nI don't know about hosting. I used it locally on my home development machine. I guess that in the future it becomes easier to host it online.\nAt the moment the following seems to be how to save the flat-file database.\nStarting docker run with the -v <dir> flag will make  a volume, which means that it will persist between starts of the container. In the newer releases there is also a \"VOLUME\" keyword for the Dockerfile which builds this into the image.\nRebuilding the image from GitHub will create an updated image and starting this new image with -volumes-from  will use the volumes from an earlier container, as shown in this example: http://docs.docker.io/en/latest/examples/couchdb_data_volumes/\n. Not really. Docker is a great tool, but not quite there yet for me. I'd like to run it on Debian, but I would need to upgrade kernels to make the best use of it.\nI only installed the wiki locally on my computer (running Ubuntu) to try it out a bit. I don't have a specific use for it.\n. ",
    "hackervera": "derp making a feature branch, commited extra commit\n. Basically an easy way to \"download\" page for local backup\n. Visiting my link above is reproducible on multiple devices. All of them are chrome though. I haven't tested with Firefox yet. I'll do some investigation this weekend\n. ",
    "pragmar": "@almereyda A config gets spit out by the server wrapped in an array when the server starts, looks something like this:\n{ _: [],\n    autoseed: false,\n    test: false,\n    help: false,\n    h: false,\n    '$0': 'node /development/wiki/node_modules/wiki/index.js',\n    client: '/development/wiki/node_modules/wiki/node_modules/wiki-client/client',\n    db: '/home/user/.wiki/pages',\n    status: '/home/user/.wiki/status',\n    url: 'http://localhost:3000',\n    id: '/home/user/.wiki/status/persona.identity',\n    uploadLimit: '5mb',\n    neighbors: '',\n    database: { type: './page' },\n    root: '/development/wiki/node_modules/wiki/node_modules/wiki-server',\n    packageDir: '/development/wiki/node_modules/wiki/node_modules',\n    data: '/home/user/.wiki',\n    debug: true \n}\nReadme says to place it in the root directory, but root is not the root in the config (wiki-server), it's the same directory that holds the base wiki directory. In the above example, /development/node_modules.\n. ",
    "espinielli": "Print stylesheed would be great!\nBert Bos did cover print media in his book Cascading Style Sheets \u2013 designing for the Web, this article of his on how to print a book with CSS could be of inspiration too (even if in the end he relied on a commercial product to do so).\n. I'll have a look at those and am eager to try something out and provide feedback: thanks!\nAs another reference, the \"Cascading Style Sheets\" book has been written using CSS.\n. ",
    "bblfish": "Since I was summoned, I'll just post a few thoughts on the notion of merging information.\nI'd just start with the following point: merging pages ( literals ) and data have very different properties\n- Merging  of pages can be done only with diff tools and only if differences are very minimal\n- Merging data is much easier - that is what RDF is designed to do\nSo when we build applications in LDP we are merging data fetched from different parts of the web. What the user decided to merge or unmerge will depend on his point of view. That is merging of data is subjective, and decides a point of view on the world: which possible world we think we are living in, or which possible world we would like to explore. \nStill one can imagine an LDP version of a wiki or of text, where pages publish metadata about which version they were copied from, and what pages are copies of it. If one puts together a notification mechanism ( see eg: Friending on the Social Web ) then one could   make it easy to keep such distributed histories of pages. A user could then agree with another diff and incorporate it, or even just point to the other page ( it's a question of trust which way you go ). But here we are not that far from the web as a distributed publishing mechanism.\nAs pages become more semantic ( data oriented  if you want ) it becomes easier to cross reference, and verify logical cohesion  a set of information mechanically, or at least to point up potential logical conflicts. That can of course go much further than textual conflicts, and is also another level of engineering altogether.\n. The main LDP spec is quite solid, though I think one really only needs to learn about the BasicContainer https://dvcs.w3.org/hg/ldpwg/raw-file/tip/ldp.html\nThe LDP Primer is still work in progress and if you stick again only to the BasicContainer you have the key feature. https://dvcs.w3.org/hg/ldpwg/raw-file/bblfish/ldp-primer/ldp-primer.html\nYou can see it in action, together with WebAccessControl and WebID+TLS with curl on rww-play https://github.com/read-write-web/rww-play/wiki/Curl-Interactions \nI'll soon put online a new rww-play server which will show how one can use this for distributed Social Networks. It's quite complimentary to wikis: just an additional tool in the helping re-decentralise the web.\n. The WebID specs are pretty stable now. You can find them here:\n    http://www.w3.org/2005/Incubator/webid/spec/\nI have implementations in scala and have previously done some in Java. I am not so sure about the ruby tools. You may want to ask on http://www.w3.org/community/webid/\n. Aside: if you guys are also thinking of doing a re-write and are even considering other languages, and if node.js is of interest, because then one can develop in the same language on the client and server, then I'd suggest also looking at Scala since with http://www.scala-js.org/ you can then program with a language that has all the advantages and elegeance of Ruby, with additional type safety, ( code that compiles is mostly correct ) and can be run both on the client and the server. Plus you get mega powerful frameworks like playframework.com . And then of course I could work directly with you. :-)\nBut anyway, this has got nothing to do with WebID authentication, WebAccessControl, etc.. LDP can run on any platform, and the semantic web is programming language agnostic - though having very good multithreading and asynchronous libs is a mega useful for distributed systems. \n. @almereyda concerning Y-A-L-S the advantage of WebID+TLS is that you can in fact make a request over TLS for a X509 certificate when a user clicks a button. If the user does not have one, the Chrome certificate selector won't show up, and so you can switch to the other less secure identity and authentication systems. These can all be tied together in the authorization section of the code. I need to do a demo showing this on https://github.com/read-write-web/rww-play ( you can find good demos on the wiki of how WebAccessControl can work with WebID there ( in the curl demos ) )\n. Yes, @elf-pavlik the main interest is that Persona has died. Otherwise we have 4 short criticisms of WebID and then a long and complex description of something that does not exist, but that when it exists will be done so that Identity Providers have a big advantage. The criticism of WebID is in a few short phrases:\n\"WebID+TLS, unfortunately, relies on the client-certificate technology built into most browsers which is confusing to non-technologists and puts too much of a burden, such as requiring the use of an RDF TURTLE processor as well as the ability to hook into the TLS stream, onto websites adopting the technology. WebID+TLS also doesn\u2019t do much to protect against pervasive monitoring and tracking of your behavior online by companies that would like to sell that behavior to the highest bidder.\"\n1. Client Side certificates are in most commercial desktop browsers extremely intuitive and easy to use - except Firefox, but that's their problem. On linux everybody copies Firefox. There are some screenshots https://www.w3.org/wiki/Foaf%2Bssl/Clients/CertSelection\n2. Nothing in WebID requires Turtle. So that that can't be a major problem. There are Turtle parsers for every language. JSON-LD Parsers could be made available too, but that's not quite a standard yet, and we don't have anything against that. We just wanted to avoid the other criticism that would be leveled against us, and that is that this is too complex to have many parsers.\u00a0\n3. And the monitoring and tracking issue is complete nonsense. If you identify to a web site with a global identity - whatever it be - a web site can share who did what. They want to use an e-mail, could that not be sold behind your back?\n4. That WebID has not evolved: Well it is so simple, there is not much to do in making it simpler. It has evolved as other standards have evolved allowing Web Access Control, Read/Write functionality with LDP, etc...\n. @WardCunningham the advantage of WebID is that you could be more flexible in who has write access. You could:\n- allow your friends to write\n- allow friends of your friends to write ( foaf )\n- allow anyone to write, but have their writes be moderated by your foaf\n- and of course this is not limited to foaf, but could be much more flexible such as allowing groups of people you met at a conference, where the conference published the members of the group on their web site to have access\nWebID because it ties easily into distributed declarative social networks makes that relatively easy to do. One could of course also use other authentication methods such as OpenID to complement that. \nI'll be developing a blogging tool on rww-play to demonstrate that. Then it is quite clear that adding a wiki would be a very useful feature too... I'll keep you posted.\n. ",
    "egonelbre": "Just noticed the Go problem you were having, you can parse /view/alpha/view/beta with\nfunc handler(rw http.ResponseWriter, req *http.Request) {\n    tokens := strings.Split(req.URL.Path, \"/\")[1:]\n    // tokens[0] == \"view\"\n    // tokens[1] == \"alpha\"\n    // tokens[2] == \"view\"\n    // tokens[3] == \"beta\"\n}\nreq.URL.Path contains the pathname as a regular string so you can split/parse it as needed.\n. There are plenty of different routers that provide different features (e.g. (http://www.gorillatoolkit.org/pkg/mux, https://github.com/bmizerany/pat, and even more https://github.com/vishr/go-http-routing-benchmark).\nOf course, implementing your own router is trivial https://play.golang.org/p/lXRFobL4GJ (note, code untested and can be made nicer).\n. ",
    "coevolving": "@paul90 Thanks for the assessment.  Since I'm initially looking as the Fed Wiki primarily as a pioneering demonstration, I'm going to move the CNAME from the collaborative domain to my personal domain, and set it up as an independent wiki.  \nI'll keep track on progress on the farm feature for OpenShift, and install another instance for collaboration, later.  I expect that many of my collaborators won't be savvy in web technologies, and the farm will be useful some month down the road.\n. @paul90 Thanks for the diagnosis.  \nI'm really a complete Github newbie, so when you say \"Just add this into the root of the repo, commit and push up to your gear\", I'll have to learn a little more to parse each of those three steps.\nToday, I'm on a train and then in a conference for two days.  I will try to time-slice in some learning and give this a tryo.\n. @paul90 I followed the instructions at https://github.com/paul90/wiki-openshift-quickstart/blob/master/README.md , and everything successfully installed at http://fed-coevolving.rhcloud.com.  I then claimed the site with my Gmail address there.  \n(1) I set up the CNAME from coevolving.com and the alias on Openshift for http://fed.coevolving.com/ .  When I try to \"Sign in with your Email\" with the same Gmail address that did the claim above, I get a message \"Login Failure.  It looks as if you are accessing the site using an alternative address.  Please check that you are using the correct address to access this site.\"  \nDoes this mean that (a) I should use http://fed-coevolving.rhcloud.com for editing and direct readers to http://fed.coevolving.com/ , or (b) I shouldn't do the claim until after the CNAME and alias are set up?  My read is that option (b) ensures greater portability, should I decide to move off OpenShift some day.  \n(2) Following through the instructions to \"Updating the Federated Wiki modules\" doesn't seem to set the marker:\n-- begin paste --\n$ touch .openshift/marker/update\ntouch: cannot touch `.openshift/marker/update': No such file or directory\n-- end paste --\nIs this something that only works after there's a change in the upstream repo, or am I doing something wrong?  (Yes, I'm still a Git newbie).  \nI apologize for the slow implementation of your speedy and diligent updates.  After stops to Oxford-Nottingham-Coventry-Stratford-upon-Avon, I'm back home in Toronto with a stable Internet connection.\n. @paul90 Thanks for the clarifications.  I have followed these, and everything seems to now work.\n(1) To resolve the alias issue, I edited server.js to hard-code self.url.  I first thought I might try farm mode, but then reverted because I don't understand all of the configurations required there yet.  I'll work on content for a while, and implement farm mode on a different domain, later.\n(2) In looking beyond copy-and-pasting, I figured out that the instructions read:\ntouch .openshift/marker/update\n... where they should read ...\ntouch .openshift/markers/update \nThis is a good way for me to be learning what's really happening with Git.  It's easier to modify your instructions than for me to try to figure out from scratch.  This interactions has been very helpful.\n. I've clarified some instructions for novices less familiar with OpenShift and Git at http://fed.coevolving.com/view/wiki-openshift-quickstart.  (Since I was doing this again, I had to refresh my own memory!)\n. @WardCunningham Over at issue #414, there's a peril at claiming the web site using a Gmail address right after installation on Openshift (e.g. http://fed-coevolving.rhcloud.com/ ) and then setting up the CName and alias (e.g. http://fed.coevolving.com/ ).  Mozilla Persona doesn't recognize the Gmail address across both URLs, so that when I try to \"Sign in with your Email\", I get a response of \"Login Failure.  It looks as if you are accessing the site using an alternative address.  Please check that you are using the correct address to access this site.\"  \nI use OpenID quite frequently for logging in, but don't usually use my Gmail address.  The first OpenID I used was for http://daviding.wordpress.com , since I have multiple e-mail addresses for different web personas.  This has me signed up at http://en.gravatar.com/daviding .\nSo, is the challenge not with \"other people's services\" but the implementation?  Is the Gravatar approach (supported by Wordpress) a better (or at least alternative) target that e-mail addresses?\n. For a novice, there's quite a difference between the C2 wiki and the current instantiation of Federated Wiki.  \nOn the C2 wiki, if I want to contribute, I press the \"Edit Text\" button, and I'm on my way.  Of course, the edits happen on the server, so this is simpler.\nIf I go to http://sandbox.fed.wiki.org/view/welcome-visitors , I can \"Double Click to Edit\", which seems to make the changes.  As a novice, my assumption is since I'm working in a browser, the changes are saved on the server.  (This could be an artifact of the history of browsers before HTML5, but only if I had \"saved\" the document on my personal computer would I presume that the contents are saved there).  \nThere's further confusion, when -- as a novice -- I then notice at the bottom of the page \"OpenID ... Claim\" ... and don't know that means.  I try my preferred open ID -- a wordpress.com site -- which works.  I logout.  I try the \"G\", and login with my Gmail address -- which works.  I logout.\nDoes a solution that \"all edits end up in browser local storage until the wiki-plugin-ldap-login creates shares credentials allowing saves to the public site\" do the intuitive thing for novices?  On the C2 wiki, effort put into writing content is preserved.  On the current Federated Wiki, how long does the content stay in my local browser?  If I spend 3 hours editing content before logging in, what happens when I return to the site a week later?\nOne of the fastest ways to turn off a novice will be for him or her to put some work into writing content, and then have it lost.  If the web site is explicitly labelled as \"Sandbox\", a good portion of the user community will understand that Sandboxes are not permanent places to write content.  However, when we talk about pages not named \"Sandbox\", what is the behaviour of least astonishment?  \nShould the statement on \"Welcome Visitors\" that says \"You will need your own site to participate\" be clearer?  I'm not the average user, as I already manage web domains, and was able to set up an OpenShift account with the help of many in this community.  As I look forward to bringing on a Service Systems Thinking community in a federation, I'm wondering myself about how the learning curve could be eased for them.\n. On the Hangout yesterday, we discussed the challenge with using OpenID, with Google's support waning.\nOf all of the choices of OpenID providers at http://openid.net/get-an-openid/ , I had stated my preference towards using Wordpress as my OpenID provider, as they have a strong participation in the open source community, as well as a commercial platform -- so their longevity is promising.  \nDuring an installation of a Wordpress blog last night (using a CPanel installer), I noticed a checkbox option for \"Clef\".  Never having seen that, I went over to https://getclef.com/ .  \nI'm not currently inclined towards using Clef, at the moment.  I did stop to wonder about the progressiveness of the Wordpress community, as Clef is showing up there first.\nFor novices coming onto Federated Wiki for the first time, perhaps is Google isn't supporting OpenID, it may be better to support Wordpress as a provider.  It's free to sign up for Wordpress, and I don't see any indication that they're going to drop support soon.\n. As I look at https://github.com/fedwiki , the updates show dates that clearly say the code is evolving.\nFrom a content perspective, any novice would go to http://fed.wiki.org/view/welcome-visitors as a logical place to start.  There's no mention there of the foundation code.  Should there be?  If not on the main page, the next logical places would be http://fed.wiki.org/how-to-wiki.html or http://fed.wiki.org/frequently-asked-questions.html .\nOn the one side, novices should appreciate that fedwiki is not a dead project.  On the other hand, we don't necessarily want to scare off people who might be interested in contributing content, but not quite up to the intimidation of learning Github.\n. For the 2014-06-25 video chat, we were playing with an Etherpad to figure out how it might be used in relation to a federated wiki (in addition to the usual weekly discussion).  Digest is posted at http://fed.coevolving.com/view/welcome-visitors/view/digests-from-sfw-meetings/view/digest-2014-06-25 .\nThere are multiple ways to export from Etherpad.  The Dokuwiki export uses asterisks ( * ) for bulleted lists, that pasting into fed wiki runs together all of the bullets rather than putting them on separate lines.  The HTML export uses the break tag instead of paragraph tag, so pasting that into fed wiki gives a single paragraph rather than multiple paragraphs.  That isn't so conducive to moving small sections of text out.\n. I would be happy with entering raw HTML, markdown, or even reusing a spreadsheet function if that were available.  A grid of data is a relatively standard way to present data.\n. @almereyda Thanks for pointing out raw HTML already works.  There wasn't enough whitespace between columns, so I changed -- table --  to --  table border=\"1\" --, which works at the bottom of http://fed.coevolving.com/view/welcome-visitors/view/small-target-areas-1968 .\nWriting in one line is ugly, but if it gets to be too much, I could use an editor like BlueGriffon and then copy in.\n. One way of pre-empting the filling up of journal with unwanted revisions could be to recognize the difference between major edits and minor edits.\nOn Wikipedia, after the \"Edit Summary\", there is a checkbox for \"This is a minor edit\".\nOn Drupal, in the \"Revision Information\", there is a \"Create New Revision\" checkbox.  I normally do use that option when I'm making substantive changes, but not when I'm correcting spelling mistakes.\nThere may already be a workaround in federated wiki:  I could take the browser offline to do all of my edits, and then reconnect the browser when I feel the cumulative edits should go up to the server.  However, I don't normally put my browser into offline status, and would then need to think about what that does to the other work I'm doing (since I multitask, and having part of a browser offline doesn't feel right).\nSo, I'm now seeing the long series of icons in the journal as a symptom, rather than the real problem.  The real problem may be tight coupling of the client and the server that conceptually shouldn't be there.  It's common for people to write long documents on their workstations (e.g. in Word or LibreOffice) and then upload the content when they feel it's \"ready\".  I have worked in simultaneous edit using Google Docs, but only when I have synchronous alternative communcations (e.g. telephone) with the other person.  Federated wiki seems asynchronous to me, rather than synchronous.\n. Responding to https://github.com/WardCunningham/Smallest-Federated-Wiki/issues/423#issuecomment-46088483 ...\n\npages are published by copying them to a public site as I do with code.fed.wiki.org. You won't see same-twins because you won't see the source site behind my firewall.\nI discuss this publication strategy here. http://code.fed.wiki.org/view/welcome-visitors/view/exploring-federated-wiki\n\nOn your \"Exploring Federated Wiki\" page is \"Production Methods\".\n\nI'm authoring these pages on a private instance of federated wiki running on my laptop. This has the environment always with me.  [....]\nIn a better world I would author in browser local storage or on the live site depending on what was conveniently available. Sync could be automatic or dependent on my approval for publication.\n\nYes, I would like to see that \"better world\"!  How might we implement that?  Would it be a possible to have a button so that edits are in \"local browser only\" or \"browser connected to server\"?  Is this a simple matter of programming now, or is an architectural change required?\n. Responding to https://github.com/WardCunningham/Smallest-Federated-Wiki/issues/423#issuecomment-46088951 \n\nI liked your reorganization of the FAQ and forked it replacing the old copy I had on ward.fed.wiki.org. You may not have noticed this unless you had my site already in your neighborhood. My copy wouldn't otherwise be noticed as a twin.\n\nThanks.  Yes, the FAQ page didn't show up in my \"Recent Changes\", but the \"newer\" flag to ward.fed.wiki.org did show up.  The procedure of \"forking\" a page does work!  I've made some additional small changes.\nTo be precise, is the label of \"forking a page\" actually correct?  When I press the flag button, am I not just branching the page, to curate a copy for myself?  The action of curation doesn't change the content, it just brings a copy over from ward.fed.wiki.org to fed.coevolving.com .  The fork technically occurs when I edit the curated copy.  I can't observe on your view, about whether the \"newer\" flag comes up after I do a curation, or after I do an edit.\n. Responding to https://github.com/WardCunningham/Smallest-Federated-Wiki/issues/423#issuecomment-46089254 , there are two separate ideas.\n\nThere is room in actions for more metadata. Imagine that there were 'mood' choices in the footer and every action were marked with the current selection. That would be interesting, eh?\n\nThe revision metadata would require an entry field that would show up only during editing.  A productive way for readers to surface that revision metadata would be to mouseover the icon for more details.  Thus, instead of only \"edit 43 hours ago\", the mouseover could show \"edit 43 hours ago, Added new diagram and fixed grammatical error\".\nThe \"mood\" choice reminds me of Christopher Alexander's patterns, where the number of asterisks reflected his confidence in the maturity of the pattern.  Unfortunately, even one-asterisk patterns were published in hardcopy, so the path for continuing evolution was cut off.\n\nWe could also create a new plugin for comments. Maybe they wouldn't show unless the page was served from your logged-in site. When I fork your page to my site, then I get to read the meta commentary.\n\nAs a blogger immersed in Wordpress, \"comments\" appended to the bottom of a post connotes either concurrence with the original content, or clarification questions that are easily resolved.  A conversation has back-and-forth threading that is more challenging to represent.  (Observe how I'm responding within the limits of Markdown in this response)!\nSome people might like a comments plugin, but I would put it as a lower priority.  The federation of wikis allows for richer interaction ... although currently there's a barrier to entry, as each author needs his own wiki (or a friend with a wiki farm) to participate.  The barrier to entry may not be a bad thing, as I believe that federated wiki targets the 5% of people who are inclined to write content, while still satisfying the 95% who want to read.\n. Responding to https://github.com/WardCunningham/Smallest-Federated-Wiki/issues/423#issuecomment-46091663 ...\n\nIf you logout then your changes are stored locally. If you log back in you still see the local versions where you have them. You can 'fork' them back to your logged in server. A fork from local to origin storage shows up without color in the journal.  \n\nWell, that's something really missing in the \"How to Wiki\" page!  I may tackle that when some semantic/interpretation questions I have are resolved.\n. Responding to https://github.com/WardCunningham/Smallest-Federated-Wiki/issues/423#issuecomment-46091663  ...\n\nWe use the term 'fork' to mean a variety of things, pushes, pulls, branches, clones and, yes, forks. Before github, 'forking' an open source project generally meant making a copy so as to gain ownership in different hands. We use the flag as the button icon to suggest staking claim to the page. We call the colored squares flags and consider them an emblem of ownership.\n\nAs a newcomer (with an author's orientation overriding a developer's orientation), I would like to suggest that we consider using a different label than \"fork\" in specific places.  The mouseover on the flag icon says \"fork this page\".  To a non-technical person, that mouseover label is not meaningful, and is intimidating.  If I was sitting with you at a dinner table, and asked you to fork something, I might have to watch out in fear of getting stabbed.\nIn the \"How to Wiki\" page, there's a friendlier \"How to Copy\" label.  However, in primary school (not to mention the legalese that comes with every copyrighted book, movie and software we experience everyday), copying is not an encouraged activity.  \nI notice that \"curating\" is popular term on Pinterest, that is less threatening. \"Reshare\" or \"share this\" doesn't quite get the idea of taking something from someone else and bringing into a personal collection.  Information is a non-appropriable asset, so when I have it, it doesn't mean that you can't have it too.\nOther terms that may be just as clumsy (via a thesaurus lookup) are \"replicate\" and \"clone\".  It would be nicer if we had a friendlier term like \"borrow\", as we do with libraries.\n. On the weekly call, talking helped to clarity what was meant by \"conversations\" and \"comments\".  http://fed.coevolving.com/view/digest-2014-06-18\n. @almereyda I just tried the Quick Paste Converter, and see that it converts from plaintext or wikish to JSON.  This would suggest the next step would be to FTP the file to the server ... which isn't a good path for non-programmers.  \n@WardCunningham had said that he might consider writing a quick translator of some sort.  As an alternative to an input being HTML with breaks in it, perhaps translation from the Dokuwiki markup would be better.  \nI was wondering why Etherpad Lite would specifically choose Dokuwiki formatting over any other wiki creole, but then found an EtherDoku project at http://sourceforge.net/projects/etherdoku/ (last updated 2013-04-18), and then \"This is a guide to building your own hybrid etherpad + wiki\" at http://canidu.com/etherwiki-howto.html ... not to mention an integration plug-in at https://www.dokuwiki.org/plugin:etherpadlite .\nThe combined use of Etherpad with any wiki is probably worth discussing.  So far, I've been positioning federated wiki as a multiple-perspectives inquiring system tool, whereas traditional wiki is an inductive consensual inquiring systems tool.  (For those unfamiliar with inquiring systems, see http://coevolving.com/blogs/index.php/archive/the-meta-design-of-dialogues-as-inquiring-systems/ ).\n. Looking at Slidewiki, the approaches seem to be to start from (i) a Powerpoint pptx file; (ii) a deck.js file; or (iii) an empty deck.  For maximum compatibility going forward, I thought that deck.js would be a logical choice, and started investigating that.\nA lot of the deck.js content seems to be about building the text in slides point-by-point.  That is, unfortunately, not the way I do presentations, as I prefer drawings -- or at least blocks of text arranged in columns that are easier to read -- to lots of text on pages.  \nThis led me to reading more about SVG and ways of presenting that.  The conventional way to do that, for some years, has been to use Inkscape, and create each \"slide\" as a separate layer using JessyInk.  The JessyInk plugin is well-supported, but doesn't seem to have taken many presenters away from LibreOffice / OpenOffice Impress.  \nOne advantage of drawing in Impress is that shapes can easily be copied and pasted into LibreOffice / OpenOffice Draw, and then exported as SVG.\nIn the pattern language work that I'm doing, I find that the style has generally been a lot of text, which only the most motivated literate writers and readers prefer.  I'm think these days about big animal pictures, and how representations other than blocks of text would help interpretation.  Slidewiki helps to make text slides more exchangeable, but diagrams with SVG would require copying and pasting content in from another drawing package like Inkscape (when the drawings get more complex than the browser-based SVG-Edit could handle).\n. @paul90 On yesterday's hangout, was there an assessment or evaluation of federated wiki with sandstorm.io?  Favourable or unfavourable?  (Sorry, I couldn't be in two places at one time).\n@WardCunningham Thanks for the reminder to avoid the Ruby version.  It changes the options for implementation.\n. ",
    "axelson": "Do you have any notes if you don't want to run the node version or use the deploy to heroku button?\nThe Heroku section of the installation guide seems to be out of date (cloudant:oxygen no longer exists).\nhttps://github.com/WardCunningham/Smallest-Federated-Wiki/wiki/Hosting-and-Installation-Guide\n. Okay, maybe that should be mentioned briefly in the main README, or when running the rails version. Also we need to update the procfile: https://github.com/WardCunningham/Smallest-Federated-Wiki/blob/master/Procfile\n. I would argue that it should be more obvious that there are two separate source code repositories. I had no idea that the node version was so completely separate. For reference here is the link:\nhttps://github.com/fedwiki/wiki-node\nGood thing I haven't dug into this code, since that effort would have been wasted since this project is no longer actively developed.\n. @WardCunningham Thanks! Definitely agree that with federation not everyone needs to be on the same version. But it is nice for people to be on the same page in regards to the current status of the project.\n. Okay, thanks for the info! I think it would make sense to include some of this info in one of the introductory pages or videos.\n. I think that the formatting page should say how to do the formatting, is it using something like markdown? It's not clear. Also it's not entirely clear how linking to a page works, but I believe this link will take you to the main federated wiki formatting page:\nhttp://fed.wiki.org/view/welcome-visitors/view/how-to-wiki/view/add-formatting\n. ",
    "cpjobling": "Thanks for all your comments. I took @paul90's advice and established a new wiki at wiki.cpjobling.me hosted on a DigitalOcean droplet as per Mike Caulfield's instructions Deploying a wiki. Compared to a Heroku deployment, $5 a month is not free but it's not that expensive either, and a small VM (512MB RAM/20 GB Disk) should be ample for a personal wiki farm for quite a while.\n. ",
    "michaelarthurcaulfield": "Actually, at this point I was on T-Mobile 3g. I'll see if I can collect errors on ethernet and wifi connections as well. \n. Also, weirdly the error caught in journal looks like this:\n{\n  \"type\": \"edit\",\n  \"id\": \"fa75743835d1a9ac\",\n  \"item\": {\n    \"type\": \"paragraph\",\n    \"id\": \"fa75743835d1a9ac\",\n    \"text\": \"[[Importing Course Items]]\"\n  },\n  \"date\": 1413330027435,\n  \"error\": {\n    \"type\": \"error\",\n    \"msg\": \"\"\n  }\n}\n. An interesting thing to me is that the JSON from the working one looks perfectly correct. It's journal is cleaner. And it has LESS entries which means it can't be dropped requests. \nIn my limited understanding, it almost feels like entries in the journal are being generated by actions the browser does not recognize as significant. Server gets sent a weird malformed edit, browser ignores it. Is that possible? I'm out of my depth here, obvi.\n. OK, weirder and weirder. I go to the affected page Idea Mining. I create a Idea Mining Scratch page. i drag and drop the journal from Idea Mining onto Idea Mining scratch to do a journal merge, and BAM! the full page I wrote appears, missing paragraphs back, things out of order put in order. \nEven with the extra gunk in the journal, running the journal still reproduces the page. I hope that's a hopeful sign. It also gives us a laborious but possible way to get back lost work.\n. New instance. My Welcome Visitors page is getting log heavy. I create a clean one in a separate site by dragging elements onto a fresh one. Page looks like this:\n\nI drag it to my other site, page redraws like so: \n\nSome data missing, all of the story data out of order, with newer elements I've added on top. \nDragging the journal  onto a scratch page in this instance does not replace lost data (as it did last time) but does restore the correct order of items on the page. Adding to this issue because falls in broad category of \"Journal is right page is wrong\" errors. \n. I will have more time to do this later, but would just mention in addition to \"how-to's\" people really need \"why-fors\".\nWhy are the pages narrow? \nWhy are there multiple pages on a screen?\nWhy can't I format text in paragraph blocks?\nWhy can't the avatars be pictures of faces? \nWhy is there no commenting architecture? \nWhy can't I get automatic updates on a page I forked (instead of having to refork each time)? \nIt took me a while to understand some of these issues: narrow multiple pages allow refactoring, comparisons, chained data structures, clicking links while keeping context. Formatting text reduces reusuability. Commenting architectures allow people to talk about pages instead of fixing them or producing other valuable versions. A page you save to your site is one you stand by and have looked at -- automatic forking undermines this authority. \nPeople need to know these things I think as much as the how-tos.\n. ",
    "svdoever": "Perfect! Missed that in reading the documentation i'm afraid!\n. ",
    "garthk": "I've claimed my npm installed localhost using Persona, and myname.fed.wiki.org using one of the supported mechanisms \u2014 none of which are Persona. Can I log the fedwiki SPWA into both at the same time so I can edit both?\nHow do I install a plugin on a farmed wiki? I suspect the latter is impossible unless I own the farm, but it's not called out in Add Plugins. If I install a plugin on localhost and then browse to a neighbourhood page (notwithstanding the above), can I use the plugin on that page? Will the content be visible only to me? \nHow do I build my neighbourhood out from a fresh localhost? Seems I need to add /name.fed.wiki.org/some-article-stub to my URL, and wait for the SPWA to reload. There's also supposed to be some easier way using a plugin that isn't present on localhost by default; I lost the mention. \n. ",
    "judell": "Thanks Ryan. Is there an install recipe I didn't find?\n. I think this opens the door to use of alternative local stores, including the wildly interesting and capable PouchDB, which in turn opens the way to CouchDB sync. \n. ",
    "rbeesley": "Researching this more, it would seem that this project is more or less abandoned as all current work has been moved to the new projects: https://github.com/fedwiki. This is not clear while reading this repo and it adds to confusion.\n. Sorry, I suppose the port change might have led to some confusion. I was reading documentation and watching videos, and I saw :1000, :1111, and :3000. 3000 seemed like what was being used in the latest content, so that's what I used when starting the server.\nI'm up and running on the Node.js version today and experimenting with what I'm finding. Install was a breeze unlike when I first gave this code a go. That could very well be my lack of experience with Ruby, although my Node.js experience isn't much better.\n. ",
    "opn": "Yes. I looked at this in some detail yesterday - though not to the extent of downloading and running code. I'm pretty keen on this direction at first site. It certainly fits my coding style and my security preferences and offers a great scope with regard to giving students their own work and coding sandboxes. I contacted them yesterday about getting involved at Chaos communications Camp - they seem interested. @paul90 you want to come camping ?\n. ",
    "timothyfcook": "Wow, I was just browsing Fedwiki install instructions and Sandstorm and thinking the same thing and came here to create an issue.\nLo and behold...\nThis sort of one-button install is exactly what FedWiki needs. Maybe a bounty is in order...\n. https://www.bountysource.com/issues/23026315-potential-for-federated-wiki-on-sandstorm-io\n. Awesome. I know a lot of people interested in FedWiki, but don't have the time or expertise to get it stood up.\n. ",
    "pierreozoux": "@almereyda thanks for the ping.\nI don't integrate images on IndieHosters until a user asks me to. #lean :)\n. ",
    "cliveb": "Same page viewd from a Mac at same time, correctly reflects changes made.\n\n. "
}
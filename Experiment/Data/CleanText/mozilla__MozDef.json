{
    "jeffbryner": "fixed with https://github.com/jeffbryner/MozDef/commit/38e9d0dc25d2b3ec63a4a336f2ef54f4e8656366\n. delete works for me, you on chrome again?\n. web hook added\n. doh..thanks!\n. thanks!\n. Could have a sample syslog file that heka reads in and just stamps with the current date/time. i.e. if you don't tell heka to pick out a time from a log line it just sets the current time.\n. Ahh, I think you got caught in the midst of converting this to it's own lib. It's now at: https://github.com/gdestuynder/mozdef_lib and exists in this repo as a reference. Github won't let me merge this here, can you submit it at Guillaume's repository?  Sorry also for the hard to reproduce state of mozdef. It really is in POC phases and under heavy development to get to parity functionality with our existing SIEM. We are working to make install easier, have a 1.0 release, etc and appreciate your help!\n. MozDef aims to be agnostic about log shippers, so logstash, nxlog, beaver, heka, rsyslog, fluentd, etc are all shippers we want to support. We use heka internally because it's a mozilla product, but logstash is supported out of the box by pointing logstash to a mozdef front-end http instance. \n. heka is deprecated, closing this in favor of ongoing fluentd work tracked in other issues.. Ahh, good point. I'll add it to the requirements\n. OK, thanks. The gotcha I ran into with Kitnirc was that it seems like only the github site supported SSL. Haven't looked recently to see if pip installed that version or not.\n. example error: \nCaused by: org.elasticsearch.ElasticsearchIllegalArgumentException: failed to parse ip [2a02:d28:666::69], not full ip address (4 dots)\n    at org.elasticsearch.index.mapper.ip.IpFieldMapper.ipToLong(IpFieldMapper.java:85)\n    at org.elasticsearch.index.mapper.ip.IpFieldMapper.innerParseCreateField(IpFieldMapper.java:295)\n    at org.elasticsearch.index.mapper.core.NumberFieldMapper.parseCreateField(NumberFieldMapper.java:215)\n    at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:408)\n    ... 14 more\n. Fixed with ipfixup plugin to separate ipv4 from ipv6 addresses\n. closing this as wontfix, CEF is too cumbersome to parse in lua\n. closing as ongoing work.. tab added with rudimentary list\n. also useful is curl -XGET http://nodename:9200/_nodes/hot_threads | grep 'out of ' which gets you a text (not json) list of what the nodes are up to like so: \n16.7% (83.5ms out of 500ms) cpu usage by thread 'elasticsearch[nodename][search][T#4]'\n    9.4% (47ms out of 500ms) cpu usage by thread 'elasticsearch[nodename][bulk][T#3]'\nThe uwsgi stats are only useful for the workers attached to the nginx socket and don't seem to report that accurately at least according to experiments with uwsgitop.\nI'd like the about page in the meteor UI to have a snapshot of key stats including the existing cluster health and these key stats to start with: \nFor each ES Node:\n-OS CPU%\n-OS load average\n-JVM Memory%\nFor each mozdef front end: \n-OS load average\n-MQ eventtask publish_eps\n-MQ eventtask deliver_eps\n-MQ eventtask messages_ready\n-MQ eventtask messages_unacknowledged \nTo me the best way to do this is to have a python job that grabs these stats from elastic search indexes, APIs, etc, sanitizes them and drops them into mongo where the meteor UI will auto-refresh as they are updated.\n. added theories panel to add, list and delete theories. Need edit, probably via modal edit window.\n. attacker/ogre display is functional. dc.js filtering is POC.\n. Cleaned up the three.js rendering, session controls to work better in meteor which allows for bootstrap modal dialogs, etc. which will make the UI easier.\n. last seen pie chart with filtering, select # attackers displayed is complete\n. added sticky nameplates to the attackers with links to detail screens of their alerts, events, links to kibana,etc. Hook up categorization and this is done.\n. This is covered in the plugins interface for the REST API delivered in release v1.7. Banhammer, FXA queues implemented as plugins.\n. I think this is long been fixed.. The incidents/references tab is a place for URLS now, but it's pretty basic. I'll set about adding recognition to the url scheme for just pasting in a link and having it figure out the destination to add some decoration/features/tooltips.\n. Modifying this a bit to be a dashboard of incident/veris tag metrics/counts and to have a go at getting it in release 1.2\n. Demo site is up and operational for this purpose\n. screenshots in the docs now \n. releasing 1.X versions every month with github issues tied to the releases for folks to vote/work on.\n. Supported via rt_flow.py message queue plugin.\n. closing to clean up pull requests. No longer needed, duplicate of simultaneous changes in front end processing.\n. done via the supervisord and celery functions\n. Also, the undo/redo buffers don't seem to clear when moving from one incident to another. i.e. you can mistakenly 'redo' into a different incident\n. Closing this as an overall issue, from my perspective it's handled by allowing an administrator to setup index templates and thus choose which fields are analyzed, not analyzed, indexed, etc. \n. closing this, will add .raw as needed.\n. updated\n. Closing this as there are sample events sent as part of the demo system put in place with docker for the BlackHat release.\n. closing this as the demo scripts take care of adding random data and event types to the demo/sample instances.\n. added panel and list of notes\n. Implemented initially as a filter for the last X alerts which are sent to the client. Next, distinguish searching mongo collection from filtering results in meteor.\n. Closing this and opening a general alert search (not filtering on screen).\n. Closed\n. exposed geoip in underlying events via revised objKeyValue ui helper.\n. pushing this to release v1.1 to get additional feedback. Should it by ack'd first? Should ack'er be the only one to push it to an incident? \n. Yeah..something is up on the persona/mozilla IDP end.\n. This was actually related to autopublish being still enabled. Fixed\n. renamed functions/forms/rest, etc. \n. I believe this has been fixed, please re-open if I'm mistaken.\n. reworked ip dropdown menu to not use bootstrap to avoid getElementByID errors, will still eventually need to upgrade so keeping this open. \n. need to increase the uwsgi timeout in nginx.conf from default 60s to 2m using: uwsgi_read_timeout 2m; to accomodate slow lookups. \n. added default confidence of 65, closed.\n. closed\n. Nothing stopping this from happening within multiple ES queries, not sure a state machine is needed? Such a machine would need to witness and track all events, but an es query can drill down without needing to track state?\n. The current searchMongoAlerts function in collectAttackers does this in that it searches alerts for ones that match current attackers and appends alerts to an existing attacker.\n. I don't think it's the storage (table, session), something is triggering the refresh. Probably need a publish/subscribe or a 'lastrefreshdate' sorta thing to check.\n. No, it doesn't need to happen for each screen change. Just needs to check for new dashboards once every x minutes/seconds? \n. I've added a function to summarize a python dictionary by the most common terms and it's in use in most of the alerts, but not all. I'll see about getting that into all alerts for this release.\n. OK, I think I've got them all converted.\n. fqdn isn't sent currently in the syslog message. If we change the syslog-ng config to send fqdn, then I can use it no problem.\n. mozdef is just echoing what is sent in the syslog message on this one. It would need to change at the source which I believe is justdave's fail2ban for SIP?\n. I think there's a typo: logger -t  -p  \n. doh..it ate part of the comment: loglegel should be loglevel\n. Along the same lines is failure ok? or does it need to be failures?\n. I like the bot reply bit, allows further integration with the bot as well like querying an IP for attacker counts, etc. \n. yes, that's a good idea. I dunno how you'd do the -n? I guess it would just be find with a sort on utcepoch descending with a limit of n? \n. bumping this to release v1.2 since it's mostly bot/irc oriented and likely mozilla-specific\n. bumping this to release v1.3\n. Clearing this from the release timeline for the moment\n. https://github.com/nate-strauser/meteor-accounts-saml\nhttps://groups.google.com/forum/#!topic/meteor-talk/1eD6eVbuJpM\n. fixed via auth0, nginx proxy implementation https://wiki.mozilla.org/User:Gdestuynder/test/OpenID_Connect_Guide#Implement_OpenID_Connect_.28OIDC.29_for_my_web_application_.28RP.29. Will achieve by implementing a plug-in system for the block request from the UI. \nThe REST API will re-use the same plugin system as the events input to allow plug ins to register for REST endpoints and they'll be passed a copy of the request that's sent.\nAdditionally the REST api will present an endpoint to list plugins so the UI can present checkboxes if needed (like in this case) to allow the user to choose actions. \nAll this will result in the 'block ip' UI form allowing the user to specify an IP/CIDR along with a timeframe, reference and to choose which blocking services should receive the request.\n. Sorry for the confusion. Machine learning isn't 'built in' at the moment, but the plug in architecture gives you the opportunity to do so. \nI'm currently experimenting with netflow and the 'esworker.py' plugins to do some profiling of network traffic for example. Having events and plug-ins available to python enables you to use some of the common frameworks for machine learning.\nIs there a specific functionality you are looking for? \n. Hey, good point. I guess there is a race condition between the initial entry and how fast one could click out of an incident. \nWhat is this google you keep talking about ;-]\n. Is this on a local install or the demo site? \n. One critical part of the persona authentication is that the browser is the one in control. So the url you use to access must match the one in the settings.js file. \nFor example if settings.js has: \nmozdef = {\n  rootURL: \"http://localhost\",\n  port: \"3000\",\n  rootAPI: \"http://localhost:8081\",\n  kibanaURL: \"http://localhost:9090\",\n  enableBlockIP: false\n}\nthen you would access the mozdef UI using http://localhost:3000 in order for persona to send an auth token that matches. If you accessed via http://127.0.0.1:3000 or http://hostname:3000 it won't match what persona will use and though the web server will seem to be working, you will never be able to authenticate. \nDoes this help? \n. Any luck?\n. https://github.com/jeffbryner/MozDef/commit/519ebb39726f09b2b01db25a2234ef69d18a1d24 closes this.\n. Do you think compliance items is a mozilla-only add-on or should it be considered a core feature of mozdef? \n. Yeah, it does.. just happened to me the other day. If you click around in the tabs really fast it seems to trigger\n. Hey, thanks for taking it for a spin. \nSorry about the confusion, let me see if I can clarify and use this opportunity to clean up the docs. \nLet me know if there are parts of the docker how-to that could be changed to be easier to understand. \nAs for the login. meteor ships with a bunch of options for how to login but unfortunately all of them require some additional tweaking. I'm noticing the docs that worked for meteor .91 don't quite get local accounts working for current meteor 1.0.3 so let me do some tweaking of the code and docs.\n. OK, I made some changes to the code and especially to the docker configuration to allow for easier transition to local accounts. \nSpecifically, https://github.com/jeffbryner/MozDef/blob/master/docker/Dockerfile#L65  is a docker command to copy a css file into the working meteor that allows the local accounts UI controls to display. \nThe end result is that you get a choice of using persona or local accounts without having to reconfigure any meteor code. \nDo you mind trying it out? I rebuilt the http://demo.mozdef.com:3000 site using it and it works fine there, you can login with local or persona and access the system.\n. Closing this as 'works for me', feel free to re-open if this doesn't satisfy the issue.\n. Implementing this as a deadman alert: i.e. a search that should return records and alert if it doesn't.\n. Looks like it's \nmozdef.js:\n//init myo if present: \n        //see if we have a myo armband\n        try{\n            myMyo=Myo.create();\n        }catch(e){\n            debugLog('No myo found..you really should get one.')\n        } \nIn each template:\n        //setup myo gestures.\n        if (myMyo){\n            myMyo.on('fist', function(edge){\n                if(!edge) return;\n                console.log('Hello Myo!');\n                this.vibrate();\n            });\n        };\ntemplate destroyed to clear event mappings:\n    if (myMyo){\n        myMyo.events=[]\n    }\n. I've got this working for pan/zoom and rotate in the attackers screen, but lack of wss support makes this fail when mozdef is running in https: \nhttps://github.com/thalmiclabs/myo.js/issues/1\nHolding this until wss is supported.\n. No api key yet, delaying to release v1.9\n. Still no api key..delaying.\n. This will add two options to the .conf file: \nenable the feature:\nautocategorize = True\nset the mapping (read in by json.loads to form a dictionary)\nalert category: attacker category\ncategorymapping = [{\"bruteforce\":\"bruteforcer\"},{\"someAlertCategory\":\"skiddie\"}]\n. Nope, mongo is the only incident store\n. Probably this is meteor trying to download the modules that are listed in it's .meteor/packages file. \nIf it's the first time (and it always is with docker) meteor needs to load up the packages not included in the app/client/js.\nI've gone back and forth about whether to include everything locally, or whether to dynamically download it. \nIncluding everything locally means all those .js libraries need to be in this github repo and will age as updates occur that aren't referenced here. \nIncluding dynamically means you get fresh updates (bugfixes, etc) as the libraries are updated. \nIs this installation just a trial/demo? Do you intend to run mozdef in a closed network? Or are you just needing to get it going to take it for a spin? \n. Ahh, I see. \nWell for production deployment there are some options with meteor: \nhttp://docs.meteor.com/#/full/deploying\nBasically you can run meteor on a workstation to download the files, test any changes, etc. When you are happy, you can bundle it into a tarball and transfer that to the closed environment. The only gotcha here is if the architecture is different since the underlying bcrypt library is arch-dependent due to the crypto in use. So, developing on mac, deploying to RedHat for example would require some re-compilation on the server. A VM for build/deploy bundling would be a good option if this is an issue in your environment.\nDeploying in production then is just untaring the tarball and restarting whatever web service you have fronting meteor. \nI will say though, that there are other features that anticipate internet access (whois, dshield integration come to mind) so if a proxy environment is an option that may be worth considering? Nothing stopping a closed environment, just some more hoops to run through.\n. FWIW I've got a prototype of a function to retrieve a dict key by dotted string path::\npython\ndef get_dict_path(input_dict, path_string):\n    \"\"\"\n        Gets data from a dictionary using a dotted accessor-string\n        http://stackoverflow.com/a/7534478\n        path_string can be key.subkey.subkey.subkey\n    \"\"\"\n    return_data = input_dict\n    for chunk in path_string.split('.'):\n        return_data = return_data.get(chunk, {})\n    return return_data\n. Labeled breaking since this will break existing alerts that assume they are being aggregated at the details.subfield level by default\n. Thanks. I think I'm going to also use this opportunity to fix up some names: Aggreg->Aggregation for example just to make sure it's consistent.\n. http://docs.cymon.io/\n. Yup, that sounds reasonable. \n. I'm into _source.details.uri\n. nice! would sourceuri then include 'referer' ?\n. Some patterns: \n``` python\nfrom elasticsearch import Elasticsearch\nfrom elasticsearch import helpers\nfrom elasticsearch_dsl import Search, Q\nfrom elasticsearch_dsl.connections import connections\nfrom datetime import datetime\nes=connections.create_connection(hosts=['esServerName.Somewhere.com'])\ncreate event to send\ndef genEvent():\n    anevent={\n          \"category\": \"jabtest\",\n          \"processid\": \"0\",\n          \"receivedtimestamp\": datetime.now(),\n          \"severity\": \"7\",\n          \"utctimestamp\": datetime.utcnow(),\n          \"timestamp\": datetime.utcnow()\n        }\n    return anevent\nactions=[]\nfor j in range(0, 10):\n    a=genEvent()\n    action = {\n        \"_index\": \"events\",\n        \"_type\": \"event\",\n        \"_source\": genEvent()\n        }\n    actions.append(action)\nprint(actions)\nif len(actions) > 0:\n    print(helpers.bulk(es, actions))\n```\n. Parallel bulk: \nhttps://github.com/elastic/elasticsearch-py/blob/master/elasticsearch/helpers/init.py#L199\n``` python\nparallel bulk:\nfrom elasticsearch.helpers import parallel_bulk\nactions=[]\nfor j in range(0, 10):\n    a=genEvent()\n    action = {\n        \"_index\": \"events\",\n        \"_type\": \"event\",\n        \"_source\": genEvent()\n        }\nactions.append(action)\n\nfor ok,result in parallel_bulk(es,actions,thread_count=4, chunk_size=500):\n    print(ok,result)\n```\n. Punting to the end of march release to publish the feb release.\n. closing this as this was removed when updating to modern meteor.. Sorry for the late reply. The docker container is only meant as a trial, so start a screen session first and you'll get persistence.\nYeah, it's a lot to run under docker so patience is key for the UI components to come online. \n. Doh, doc error. It should be index.conf (default, you can make it anything which may be where the confusion came from). \nHere's an example: \ncat index.conf\n[options]\nkibanaurl = http://servername:9090\nesservers=http://servername:9200,http://server2name:9200\nDoes that help? \n. closing this due to inactivity\n. Hrm.. good point. \nThose (currently) come from Veris/Verizon: https://github.com/vz-risk/veris/blob/master/verisc-labels.json\nShould we alter ours on import? Convince them to change theirs? Alternate? \n. \"it would be interesting to get Verizon's take on it\"\nMaybe I/we should reach out to https://github.com/whbaker for his take \n. That does help, thanks Wade. I should have given you some context. \nWe recently published the levels we use: https://wiki.mozilla.org/Security/Standard_Levels\nBasically telling ourselves to only use: low, medium, high, maximum since it offers a readily understandable range. Guillaume is rightly pointing out that an immediate discrepancy within Mozilla is MozDef's use of Veris since the levels don't match. \nThat's good info that STIX is a likely synchronizing point. I'm not exactly a fan FWIW and find it cumbersome. MozDef for example uses VERIS to allow one to tag incidents which given the schema is straightforward. \nNot to turn this into a STIX discussion, but tagging doesn't seem like a primary use case for STIX when compared to VERIS? [reference: http://stixproject.github.io/about/#veris ]\n. Does your docker build step report a working python? \ni.e. step: https://github.com/mozilla/MozDef/blob/master/docker/Dockerfile#L50  will install a python environment. \nThe '/home/mozdef/envs/mozdef/ only contains a directory called bot.' is normal for the docker install since it does the more linuxy thing of putting it /opt/MozDef\n. OK. For reference, here's what I get when pinging the REST api endpoint from my docker install: \nroot@mozdef:/var/log/supervisor# curl http://localhost:8081/status\n{\"status\": \"ok\"}\nYou should still get a response with just the root: \nroot@mozdef:/var/log/supervisor# curl http://localhost:8081\n<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\n<html>\n    <head>\n        <title>Error: 404 Not Found</title>\n<..snip..>\n. closing this due to inactivity\n. Gonna use pytest given that it runs unittests and does some things nose2 doesn't seem to allow and we apparently like it: \nhttp://mathieu.agopian.info/presentations/2015_06_djangocon_europe/\n. Duplicate or re-statement of #1027 . why do we only care about 7 days? \n. OK\n. 'cept this section of the docs is talking about the event processing plugins, rather than the alert plugins so it's correct to reference the location of esworker.py\nProbably should be separate sections for event plugins, alert plugins and rest plugins since they operate the same way ( registering for things, receiving messages) but live in different spaces since they modify different parts of the system (events/alerts/rest calls).\n. \"Get fluentd to start passing a customendpoint field set to True.\"\nThe 'customendpoint' marker is a feature from the http injestion and related esworker that pulls from rabbitmq. It serves simply as a prompt to send the event through plugins first before attempting to normalize the event (make field names the same, etc). \nWe can probably start with the assumption that the SQS worker should just send all events through plugins and avoid this step. \nreference: \nhttps://github.com/mozilla/MozDef/blob/a09e83c5cc0ca5e6bba107a66a32719145d4f013/mq/esworker.sqs.py#L361-L365\n. Hey, \nThanks for taking it for a spin. You can either reconfigure it to use that location or cp the file into the mq directory. \nJeff.\n. It's whatever format can be discerned from the javascript library http://momentjs.com/. Nothing stopping one from writing a pcap parser if you just want some sample data, but you might be better off with https://github.com/aol/moloch which is written specifically to handle pcaps.\n. The kombu object didn't play well with the SQS queue. I don't remember specifically what the issue was, (I think it was around base64 encoding) but I couldn't get it to work using the underlying kombu library. So this allows us to work around that, keeping the same basic structure without solving kombu issues.. 900+ items in the veris tag catalog and it's tough to come up with a selection mechanism if you aren't familiar with the framework. So this was a way to add searching and tagging without drop down lists, etc. \nThe initial 'space'  as a selection is dumb, removed that and added 'category' as the session default (whatever you type will stick with you from incident to incident). Added some tooltip as well. Let me know if that helps?. I don't think we specify python versions (just 2.x) anywhere in the code, so should really be mostly a deployment issue for any virtual environments, etc. . details.action instead of details.path and the registration key should probably be lower case?. @scriptonist  You are getting the same thing I got: \"make: g++: Command not found\"\nThe docker container is missing a dependency to build bcrypt. You can find the fix in this pull request: https://github.com/mozilla/MozDef/pull/454. Neither of these are suitable for a logo. I'd recommend just using a generic mozilla one like this instead: \nhttps://www.mozilla.org/favicon.ico. Ahh.. yeah, old cruft. That and this https://github.com/mozilla/MozDef/blob/master/meteor/public/images/concrete.jpg can probably be removed. . I think we are at the initiative phase of the brand guidelines: \nhttps://mana.mozilla.org/wiki/pages/viewpage.action?pageId=61249802#BrandandIdentityGuide(orHowtoSuccessfullyBrandYourEarlyStageProject)-InitiativeWordmarkLockupExamples\nand as such should endevour to either match those styles or replace the firefox graphic with the dino graphic that is non-initiative specific but generally approved for general graphic use. . This will likely report all cloudtrail events as errant ;-]. Hmm.. something is missing. Tried the build with multiple containers and error'd out also in bcrypt: \ngyp ERR! build error\ngyp ERR! stack Error: not found: make. I think you are proposing a feature request for Firefox? MozDef is not related to firefox, but is a separate project.. I can't get the single-build to work at all. I'd suggest accepting these fixes for the multiple build and work the single as a separate concern.. Looks good, that's the faster of the two functions anyway (dateutil.tz.tzlocal seems super slower), but like you say it's only on startup rather than per event.. looks like type is actually severity? i.e. info/notice/warning/etc? . Ahh, thanks. I've got a pet peeve against 'type' as well, mostly because there is usually a better description but also as phrozyn mentions _type is already a thing for ES and is really easy to misunderstand. Dunno if we need to go as far as \"re-writing all the events that are already in MozDef and came from Bro\" since they've already been alerted on, but we should settle on what to call it\nhttp://words.bighugelabs.com/type\nhttps://www.merriam-webster.com/thesaurus/type\nvariety?\nsubclass?\ndivision?\nbranch?\nfamily?\nothers? ;-]\n. This seems to replace python with a shell script that just does the same exact thing? \nhttps://github.com/elastic/elasticsearch-py/blob/3be98b0a09e6d0820efdc20966f086ea6b8b3396/elasticsearch/client/indices.py#L325. Looks good to me!. Thanks, yeah this has been long overdue while we refactored the backend code and all the docker container(s), removing persona logins, etc. \nWe should probably remove it until it's up and functional again. . FYI: I ran this template through a 6x docker image and got complaints about mappings for anything other than: \"possible values are [object, string, long, double, boolean, date, binary]\", so for float, short, byte, etc. . Looks like the travis ES version might not be right? \n$ ES_VERSION=2.4.4; curl -O https://download.elastic.co/elasticsearch/release/org/elasticsearch. According to: https://www.elastic.co/guide/en/elasticsearch/guide/current/_deep_dive_on_doc_values.html \n\n\"Doc values are enabled by default for all fields except analyzed strings. That means all numerics, geo_points, dates, IPs and not_analyzed strings.\n Because doc values are on by default, you have the option to aggregate and sort on most fields in your dataset. But what if you know you will never aggregate, sort or script on a certain field?\"\n\nWe don't sort or script, so the only reason to keep doc values enabled would be aggregations. Do we aggregate on all fields?\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/norms.html\nI think in the previous template, norms were also turned off since we don't score.\nKeyword as a datatype makes sense since most fields are a single value except summary which is a phrase.\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/index-options.html\nI think because we don't score we can also make use of different index options? . Sure, but I doubt we are going to aggregate on all the cloudtrail fields for example. So if we are trying to avoid sparse indexes: \nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/removal-of-types.html#_why_are_mapping_types_being_removed\nit would seem removing doc_values everywhere and adding only where we need them would be a way to help. . This catches a negative string, but I don't think it'll work for a negative number? something like -86000. \ud83d\ude22 but looks like no progress on https://github.com/thalmiclabs/myo.js/issues/1 so it's too much trouble to set up for the utility of it. Can you say some more about which docker setup you are using? We have the single instance docker (everything on one container) and the multi-instance (docker-compose).. I did some fixup on it and I think it's ready to merge. . Good question, probably for another thread. Several ideas about automation, existing observatory inventory, etc come to mind. . What problem are we solving for? . The organizing principle for indexes is retention period, rather than data type. i.e. things that only matter for a day are in events-YYYYMMDD. Things that need to be retained longer like alerts in monthly indexes. The type of data (they are all security events) doesn't matter so much or else we'd end up with dozens of indexes and exploding complexity. \nIf we are worried about historical data increasing the size we can remove this: \nhttps://github.com/Phrozyn/MozDef/blob/6aad873903557083022c2bb1635aa27d011077a7/cron/healthAndStatus.py#L137\nand just limit it to creating a static doc ID for current ala: \nhttps://github.com/Phrozyn/MozDef/blob/3362fad060734b88e889ee8be39d488d66cc3052/cron/healthAndStatus.py#L141\nthen you only have one document per host per day but I'm not sure it's worth even that change to save what's probably minimal space?\n. Ah, ok thanks. This is about data retention for health/status/event stats events, which makes sense. i.e. rather than keep 100s of millions of events online longer; keep smaller sets of data online longer for greater historical trending. \nFYI: https://github.com/mozilla/MozDef/blob/master/cron/eventStats.py#L87 should also get a PR then to allow it to post into the same index. It's the thing that summarizes counts of events per category every X minutes for historical trending of incoming events. . No worries, thanks for explaining the effort!. There are alerts and visualizations that use the string version especially for aggregations. I don't see a need to do this since it just reduces the flexibility and leaves us open to issues in ES where they misinterpret an IP (happened before, especially in aggregations). \nLets discuss before further work.. Might not be alerts, but there is definitely other code using these fields. Again, I don't see a need but I do see lots of regression testing that would be necessary.. Hey, caught up with Tristan on this. My apologies for not realizing this is part of the normalization effort, I should have looked for context before commenting. \nhttps://github.com/mozilla/MozDef/search?q=ipaddress&unscoped_q=ipaddress may be a useful start at all the places that may be assuming string/ipv4. In particular we should be cautious about offering up an ipv6 to a routine that is not expecting it. . The root menu anchor (the thing saying 'kibana' should take you to kibana. does it not? . The end result should be like: https://cloudymozdef.security.allizom.org:9090/_plugin/kibana\nIdeally we pass this as OPTIONS_METEOR_KIBANAURL so meteor doesn't have to guess and it's just set as an environment variable. \nAny way to derive this from the bits already in cloudformation? . Ahh, we could do a thing where if the URL isn't set (i.e. the ENV is empty), or if it's set to some special value OPTIONS_METEOR_KIBANAURL=figureitout then we construct it via https://docs.meteor.com/api/core.html#Meteor-absoluteUrl or some other javascript magic. I propose we adopt the latter.. a special kibanaurl variable/keyword that tells meteor to set the url relative to whatever dns the instance lands on: \ni.e. OPTIONS_METEOR_KIBANAURL=http://relative:9090/app/kibana\nwould parse via new URL(OPTIONS_METEOR_KIBANAURL) as hostname==relative, and Meteor can do a call to Meteor.absoluteUrl() to get the real hostname and substitute it.. This is an issue with the docker overlay file system driver on mac: \nhttps://github.com/docker/hub-feedback/issues/727#ref-pullrequest-364363543\nThe workaround/fix is to use the aufs driver: \n\n. We disable websockets here: \nhttps://github.com/mozilla/MozDef/blob/master/meteor/server/mozdef.js#L16\nIt may be that you are seeing just one or two before that setting takes effect? . Actually it looks like you ran into the docker filesystem bug: \nhttps://github.com/mozilla/MozDef/issues/907#issuecomment-434416866\nYou might need to change the underlying docker setup in order to properly build. It's intermittant, but it shows up in a tar file extract. The issue above has the link to the docker tracking bug. . Yes this is true, a build will require more than the default memory settings for docker.\nSorry about this, you are running into these just as we are updating docs and re-working a bunch of the docker stuffs. I'll add a bit here: https://github.com/mozilla/MozDef/blob/master/docs/source/demo.rst. @Phrozyn can you help with this? Might be a good opportunity to share the updated syslog configs.. The sample/demo events may be a helpful reference: \nhttps://github.com/mozilla/MozDef/blob/master/examples/demo/sampleevents/events-benign.json\nYou can use curl to post those to the loginput endpoint ( http://localhost:8080/events/) as is done in the sampledata docker container if you just want to test connectivity. \nhttps://github.com/mozilla/MozDef/blob/master/examples/demo/sampleData2MozDef.py is the program in the sampledata container that does the lifting of those particular json files into mozdef, but it gets fancy to use current dates, randomize ip addresses etc. A simple post of json should suffice.. @darakian I finally got what you are trying to do and I think the issue is that netcat doesn't know HTTP post. \nIf you want to just post some json to mozdef do something like: \ncurl -v --header \"Content-Type: application/json\"   --request POST   --data '{\"tags\": [\"test\"],\"summary\": \"just a test\"}'   http://localhost:8080/events\nwhere http://localhost:8080 is whatever is running the 'loginput' service. \nThe 'data' curl option is what gets posted as json to MozDef. . Cool, I'll add a bit to the docs. . I'm wondering if https://github.com/mozilla/MozDef/blob/95be0e40eb2f45a43ab4ac77a9fa13117fb4824e/cron/auth02mozdef.py#L317 ever exists? . Looks like the details.request/response exist for api calls, but not for other events which is why this is usually being set to true. . lets just rm -rf the errant license? since all of mozdef is already covered by https://github.com/mozilla/MozDef/blob/master/LICENSE. On 2nd thought since people probably arrive there via the package, I pushed a branch/pr to fix . That's a good catch. I ran into the same thing and made a change to the template here: \nhttps://github.com/mozilla/MozDef/pull/974/commits/46b1e849febb958f33c6e3555323c24addd01563\n@pwnbus any idea why the tests require this, but the alerts don't?. Excellent, newcomer points of view are just the ticket for accurate docs! ;-] (protip; check timestamps if you know you have data but they aren't alerting. We do our best to get everything to UTC, but if source doesn't have timezone and not operating in UTC it can cause 'missing' alerts). I wonder if we need a schedule stanza at all? What if we auto scheduled any .py file that lives in the /alerts directory to run? Mostly they all run every minute currently (since they take<1sec 99% of the time).\nThis would make it easy to deploy an alert.. simply copy the file into a directory. \nThoughts? . Good point @Phrozyn about turning off an alert. @darakian true, a UI for checking/unchecking alerts would be more functional. Usually thresholds are set in the alert so the frequency of the alert running doesn't matter. Alerts also mark events as 'alerted' when they match so there isn't a concern about overlapping time, etc. . Interesting . Looks like we are expecting 'facility' to become 'source'  here: \nhttps://github.com/mozilla/MozDef/blob/master/mq/esworker_eventtask.py#L63\nAnd since we ignore it when it's explicitly sent we set the default here: \nhttps://github.com/mozilla/MozDef/blob/9e2b1ac236dafe597c5ea025f973451737717e6d/mozdef_util/mozdef_util/event.py#L36\nGood catch! Should be a simple change to allow source to be explicitly set. . this syncs up the logincounts data viz with the normalization standard we've applied to auth0/duo to have category:authentication with details.username and details.success as fields signaling auth success/failures.. Why remove?. It does look for failed logins? It starts by gathering a list of all failed logins, then looks for success logins and creates a data stream counting each category.. ExistsMatch('details.sourceipaddress') means that the fields details.sourceipaddress has to exist in the json data structure (it can be any value.. but it has to be present) before it will match. \nSo for it to match you'd have to change: \ncurl -v --header \"Content-Type: application/json\" --request POST --data '{\"tags\": [\"test\"],\"category\": \"helloworld\"}' http://localhost:8080/events\nto\ncurl -v --header \"Content-Type: application/json\" --request POST --data '{\"tags\": [\"test\"],\"category\": \"helloworld\",\"details\":{\"sourceipaddress\":\"1.2.3.4\"}}' http://localhost:8080/events\nor remove that from the selection criteria. \nHope that helps!. Oh, I should add that doing an aggregated search also introduces an 'exists' criteria in the search as well behind the scenes, so even if you remove it from the search_query.add_must, it'll get added back in to satisfy the aggregation here: https://github.com/mozilla/MozDef/blob/master/alerts/lib/alerttask.py#L280. If you don't really care about aggregation and just want it to fire on witnessing one event you can use this pattern that the cloudtrail disabled uses: https://github.com/mozilla/MozDef/blob/master/alerts/cloudtrail_logging_disabled.py#L16\nThen you'll get an alert for each event that matches. \nAnd yes if you change the code for an alert; you'd need to restart the alerts process for it to take effect.. Looks ok, do you have it scheduled to run ala: \nhttps://github.com/mozilla/MozDef/blob/a4ebe1105b301132b5268d6e3bc70a6f95e85fc9/docs/source/alert_development_guide.rst#scheduling-your-alert\nAs for restarting. If you are doing active development; I'd just docker exec -it  bash and do restarts from there. . hrm.. lemme spin up a local docker version and see what I run into. I fired up the docker images and got it to work. I don't think there's anything wrong with your code, it's just strangeness with docker that we are dealing with and how to best use it to develop alerts. \n\nThe trick for me was making an overload yml file to let me manually start the alerts, something like : \n```\n\nversion: '3.7'\nservices:\n  alerts:\n    image: mozdef/mozdef_alerts\n    build:\n      context: ../../\n      dockerfile: docker/compose/mozdef_alerts/Dockerfile\n      cache_from:\n        - mozdef/mozdef_alerts\n        - mozdef_alerts:latest\n    restart: always\n    command: bash -c 'python -i'\n    stdin_open: true\n    tty: true\n    depends_on:\n      - base\n      - elasticsearch\n      - rabbitmq\n      - bootstrap\n    networks:\n      - default\n```\nthen docker exec -it into the container and run: \nsource /opt/mozdef/envs/python/bin/activate && celery -A celeryconfig worker --loglevel=info --beat\nmanually when the alert and config file was ready. I used your curl command above to send an event and when a minute passed it ran and created the alert. \nLemme play around with the best way to offer an 'alert creation docker image/overlay' file and I'll add one to the project. \nYeah, a web UI to at least schedule alerts would be a welcome addition. Yup client code goes in meteor/client but usually involves some server side code for the collection, etc. . You can set environment variables that will take precedent over the settings. You can see the set of environment variables here: \nhttps://github.com/mozilla/MozDef/blob/683311f1baaad969c584c70ca49a8a7da8d5e8ef/meteor/imports/settings.js#L13\nand set them via Docker if you like\nhttps://github.com/mozilla/MozDef/blob/9906caf4892594dd477b179a34668c0f0080e7d1/docker/compose/mozdef_meteor/Dockerfile#L10\nor  via a compose file or a .env file: \nhttps://github.com/mozilla/MozDef/blob/9906caf4892594dd477b179a34668c0f0080e7d1/docker/compose/docker-compose-cloudy-mozdef.yml#L6\n. Oh, sorry. So Meteor is programmed to look at the environment first and use that if it's set. \nYou can set the environment via Docker in a couple different ways: \n1) When you build a container you can set ENV lines in the Dockerfile directly\n2) When you use docker compose, you can specify a .env file to use for the 'service' presented by the container. \nSo for your case, I'd say create a .env file and modify the docker-compose. The .env file is just: \nVARIABLE=value\nSo if you wanted to set the kibana location you'd include a file, say meteor.env with \nOPTIONS_METEOR_KIBANAURL=https://somewhere. Good question, @gene1wood do we have that file in a repo anywhere or a reference of what's in it?. Great question. Assuming this is an aggregated alert you get event data burped up to the alert in three ways: \na count\na reminder of the aggregation value\na subset of the events that matched\nHere's an example of the ssh bruteforcing alert using that to include some counts in the summary field: \nhttps://github.com/mozilla/MozDef/blob/master/alerts/bruteforce_ssh.py#L44\nYou could do similar to pull out, say, the first summary from an event that tripped the alert like so: \nsummary = ('Alert on: {0} '.format(aggreg['events'][0]['summary']))\ni.e. you are using the aggreg['events'] list and just selecting the first one to grab the 'summary' field and copying it into the alert summary. . They would be anything that matched the query parameters set in the alert. So in this case: \nhttps://github.com/mozilla/MozDef/blob/master/alerts/bruteforce_ssh.py#L18\nAny events matching that ('failed' in the summary, from the details.program:'sshd', with any of 'login', 'invalid', 'ldap_count_entries', 'publickey', 'keyboard' also in the summary) within the timeperiod of the alert (run every minute, looking back 10 minutes). \nSo you can pull out individual fields, aggregate counts (the mostCommon routine for example) of any field in that record set.\n. Yeah, it's an elastic search document convention to hold the source of the doc in _source.. closing this to open a cleaner PR. It's actually a typo here: https://github.com/mozilla/MozDef/blob/master/meteor/client/incidentEdit.html#L415. Yup the mq workers are responsible for pulling json from rabbit, running through the plugins and depositing in ES. \nIf you can curl the ES endpoint and you get the version/tagline json response then the ES instance is working. So then it's a matter of ensuring the mq workers know where to point and can reach the endpoint.\nSounds like you can curl the ES endpoint. If it were me, next up I'd see if you can also curl from the mqworker container? i.e. get a shell (docker exec -it  bash ) ? If so, is the environment variable right in the container? . This is part of the feature flagging and you can turn off all the visualizations (globe/attackers are the same data) or just the globe, just the attackers, etc. Currently the features are all on by default, but you can turn them off as you wish. https://github.com/mozilla/MozDef/issues/936. It's not that they don't work, they may just not have any data? i.e. attackers are the combination of events->alerts->attackers. If you run the sample data demo events you'll get some created in just a couple minutes. \nBut out of the box the whole system doesn't have data, so if we use that criteria then we disable alerts, kibana, etc? . If it helps, it creates attackers based on hitcount and ipv4 address. i.e. if someone from the same IP block creates X alerts it becomes an attacker: \nhttps://github.com/mozilla/MozDef/blob/master/cron/collectAttackers.py#L154. ah, that'd would explain it. There's specific logic here https://github.com/mozilla/MozDef/blob/master/cron/collectAttackers.py#L171 to not create attackers out of 'friendly' ip address ranges. . Ah, good point. You could accomplish that currently by using a custom plugin to do something similar to this: https://github.com/mozilla/MozDef/blob/master/mq/plugins/ipFixup.py#L129 if the actual 'source ip' comes in via a header or similar method.. Can I ask why this instead of removing _type? (since as I understand it, it will always be static)? does type==category?. _type currently for alerts is always 'alert' and I don't see a place where that's different? \nhttps://github.com/mozilla/MozDef/blob/4190c8d5c5fb79485f56d6cca86e0227daf5336e/alerts/lib/alerttask.py#L377. this doesn't work for a generator\n. I ended up implementing something similar just to avoid the repeat removeAt and lower()\n. Not sure I know what this is doing or how to deploy it?\n. not sure about this. If the index is missing we should create it, but not create the alias to the old/previous.\n. I'd like to leave the default to look for a .conf file the same name as the prog. We can tell it to use a consolidated .conf file in the .sh script the cronjob runs.\n. no way to get to the awesome colorify function? Should we refactor it?\n. Maybe this? https://github.com/dslackw/colored\n. Yup, I think it's safe to get rid of. Not sure how to figure out the file version string anyway?. plugins should be able to change a message such that the subsequent plugin gets the new message. This enables things like picking out or correcting an IP in one plugin, geo coding it in another.. OK, as long as each plugin gets the output of the last plugin it sounds like you are doing the same thing as originally. If there is an error caught by the try/catch, where does that get reported so someone know there is a problem?. @pwnbus is this a wildcard registration?. Maybe I'm not understanding, but it doesn't seem like it would need to run on all alerts, just geoIP? Unless I'm missing something, it also doesn't match the registration needing to be a list? https://github.com/mozilla/MozDef/blob/ed2f6e02c254a6eb8e60c375a126fb64c27e0a35/alerts/alertWorker.py#L32. Thanks for the IRL explanation. I missed the part where * is added as a matching value as well. As we discussed, seems like a trade off of a refined registration system vs config file based matching that'll probably play out as we add more types of alerts.. The only other way I know how to do it is via something like: \nhttps://github.com/jeffbryner/ldapChangeMonitor/blob/master/ldapChangeMonitor.py#L244\nIt would be interesting to profile.. Yes, this is because it's easier to have ES default to use all strings, unanalyzed than to attempt to pick the correct types per field (lots of numeric options) and we never do queries by ranges of process ID anyway.. . I'd say we want it to be a unique default so we can still tell if there are events that do not contain the required fields.. @tristanweir these design discussions happened here: https://github.com/mozilla-iam/sso-dashboard/issues/130 The todo here is to sync what we are sending with the UI elements that were implemented as the UI that was implemented is for low and medium risk alerts.. I'd suggest UNKNOWN in caps as it should be obvious and easy to search.. Might consider always having the remove match * for oldindex, since all we want is an atomic set the alias to be X. \ni.e. we don't really care what it is now, but we know what we want it to be, so we don't have to worry about what it currently may be.. Is this expected to be a list? Seems like it would be a single value?. doh, added. Does it break something to update? . Where to we aggregate on the summary field? Seems like a memory killer according to: \nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/fielddata.html. ES aggregation or python aggregation? ala: https://github.com/mozilla/MozDef/blob/master/alerts/lib/alerttask.py#L243. Yeah, IIRC we did it that way because the ES aggregation made it difficult to get to the actual records themselves and we only deal with small data sets; so it's easier to grab the raw record set and let python aggregate. . the comment is more about the todo/ like limit to some max/min of the asn/attacker but I'll fix it up.. I'd think manage would be about managing mozdef itself. Lets leave it here for now, can always change it later.. I had to change it since I mapped my meteor development directory into the container...reverting.. Sure, added some that should exercise the get('string.string') a bit. Yeah, I don't think we'd create attackers based on DNS, so I think it's safe to remove.. can remove since attackers don't have dns. probably needs a new option. The option is just to limit any list to X entries.. the way mongo works, since you've defined an item as 'fqdn' in fqdnCursor you'd reference the address as fqdn['address']. s/IP/DNS. s/iplimit/fqdnlimit. The format is Template..event so for this it should be Template.blockFQDNform.rendered since you want it to autofill a block in the fqdn form.. wouldn't it be the same logic/script, just a different -c conf file for each queue you are targeting? . shouldn't this call main() instead of doing what main() is doing?. Then the same access key/secret key/region has to be valid for each queue?. I'm not sure of a good way to require 2.7 via yum? . Since those build the meteor package to run with node, I added an option to the live reload overlay to trigger whether or not that step happens (will happen by default). \nhttps://github.com/mozilla/MozDef/pull/770/commits/efc8d2f95f0e0869f4f360ff42b8652c5abcce29. Probably could be? Maybe there are benefits of leaving it to not require docker in order to run, though?. great question, I think the packages file is the declarative set of intended packages where the versions is the resultant set of everything pulled in including dependencies. Dunno about the version mismatch; that's interesting but doesn't seem to affect functionality.. good catch! missed the 'cut' part of copy/paste ;-]. looks like a typo or copy/paste misfire.. that repo doesn't even exist.. ah.. should we just rm it? . Probably rm the file and move to env variables is the right choice rather than having a one off file/source that we copy around. i.e it's not just a dict of settings, it's code now to allow for ENV variables to override defaults.. OK, added some sanity checking for bool environment variables and cleaned up the way this all interfaces with the accounts system.. My intention is to have this be the way to disable features system wide (eventually). i..e either passing in a common env variable or having all components connect to the resulting mongo features table to see if they should be enabled. So for now I think it makes sense to leave it generic.. I fought that for a while. The issue is the subscription to the features table is ready before records show up (this is universal to all collections and is just the async way meteor works). Since the 'menu' template is reactive it removes the features once the data shows up; but initially it's empty so everything shows. \nI'll keep looking for more elegant ways to avoid showing the menu until we have records; but I don't know of one currently. . Just had a brainstorm and added a utility function the template can use to know when records are ready. It now delays painting until it knows the features.. Yeah, good point. I didn't realize the rest docker container had it's own .conf. For whatever reason it wasn't working for me without this change, but I'd guess it's something in my local docker setup. I'll revert this. . yeah, I left it as the convention followed by other plugins. I think the stoplist should eventually be pulled from that config file. we can, but I'll bet we add it right back in the next revision depending on what sort of data shows up. In some cases it seems like auth0 is sending \" \" as a username for example.. I wonder if this is going to traverse into really complex sub structures like are present in cloudtrail? (lists of dicts of lists, etc)\nhttps://github.com/mozilla/MozDef/blob/master/mozdef_util/mozdef_util/utilities/dict2List.py is an example of some of the complexity involved in this. . Not impossible, but if we wanted to be consistent we would have to change the standard so things like sourceipv4address become source_ip_v4_address, destination_port, source_port, etc. Probably a discussion for another issue/pr.. hrm.. tabs/spaces misfire.. It would fit the pattern better if the thing that makes up the watchlist is actually a plugin that could be custom to one's environment, but gets called via the rest interface. i.e. like the blocklist, or logincounts, etc. The endpoint is generic but the plugin has the business logic/tech integration/etc.. oh, this is the 'get' endpoint.. /me reads more. Is watchlisted ever not False?. This is a bit concerning: \nhttps://github.com/tgs/requests-jwt/blob/master/README.rst#this-package-is-not-actively-maintained-and-has-never-had-a-security-audit. rather than set these fallbacks at the end of a series of if/elses, I find it better to set defaults up front so you never run into a scenario where you have an undefined variable. I'd advocate for continuing the series of setting defaults just after line 63 to include user_data, source_data and anything else that should have a default, then just override the default in the if/then series based on what you find.. I think you want watchlist instead of watchitem unless you supply a term watchitem/term.. s/ip//. Still probably a good idea to pull the getWatchlist() function out into a plugin. That would give folks who don't want the UI a way to still use the watchlist feature fed by something else.. good catch! hence the guide ;-]. I think it was trying to add some threading to work around timing issues in the kitnirc client. If we aren't using that anymore you can probably ditch it. ",
    "netantho": "I fixed the drag and drop, the delete button (not commited yet), I still need to work on the saving part.\n. Nop, didn't work on firefox either:\n\"click .tagdelete\": function(e){\n-            //console.log( e);\n-            //console.log(e.target.parentNode.firstChild.data);\n-            tagtext=e.target.parentNode.firstChild.data\n+            tagtext=e.target.parentNode.firstChild.wholeText;\n             incidents.update(Session.get('incidentID'),{\n                 $pull:{tags:tagtext}\n-            })\n+            });\n. Hi there,\nSure, you can send us a pull request if you want.\nSomeone created a Docker-MozDef project, I don't know if works yet, I still have to get docker running correctly on my machine. Maybe it can help you to install the deps.\nPlease open an issue if you have problems while installing the deps so that we can improve our docs.\nPlease note that the project is in a early proof-of-concept stage and therefore we'll not do a lot of time-consuming support of now.\nThanks\n. I agree with you on not having generated files in the git repo ;-). Did I miss some of them in our .gitignore?\n. Sounds good, it would be great if you put the command you use in your pull request's description.\nThanks!\n. I'm working on pep8 integration and fixes\n. We can find examples in official elasticsearch projects:\n-  https://github.com/elasticsearch/curator/tree/master/test_curator\n- https://github.com/elasticsearch/elasticsearch-py/tree/master/test_elasticsearch\n. I would be super cool to build docker on travis and have functional tests run on new commits.\nLike ship some logs to mozdef, check that the transformations steps work, get them querying elasticsearch.\n. Having Kibana dashboards importation working depends on this because you need to have the index to be able to import a dashboard with a reference to this index.\n. I would prefer having a .sh or .py file that send logs to loginput in ES json format, so that we don't have something shipper-dependent and it's really to have a reproducible dev environment easily installable.\nI need this to have Kibana dashboards working in a docker container.\n. @jeffbryner Can you own this?\n. I think the Persona package doesn't work anymore. @jeffbryner Can you confirm?\nIf it doesn't, do we still want to use it (which would mean patch it at this point)?\n. Also, provide only the Logstash Style dashboard by default in this repo\n. To get the dashboard list using curl:\ncurl -XGET 'http://mozdef.example.com:9200/kibana-int/dashboard/_search?pretty'\n. I'm working on it in my branch, screenshot\n. Oops, thanks for the report :-)\nIf you want a quick install of MozDef, I would recommend the docker config, see http://mozdef.readthedocs.org/en/latest/installation.html#docker\nWe still lack sample data to have something usable for a demo, issue #19 .\n. @2xyo We prefer using heka internally to ship logs. Its lua sandbox + lpeg (kind of special regex implentation) can ship > 100,000 events/sec and allow nice scripting.\nThis task is rather to create a simple server with very minimal dependencies, ideally just python, that can be used by anyone who wants to send logs to MozDef (developer, IT, etc.) and wants something to emulate MozDef's receiver with debugging features.\n. We have heka config examples ;-)\n. Just found a solution with http://linuxfr.org/news/logiciels-pour-survivre-avec-docker\nTested, it works! I need to translate it into English in the docs tho.\n. Thanks!\n. I think we now have enough examples. I'm closing it.\n. I'm changing the priority on this issue, will start with just python, no webui, see #147 \n. @jeffbryner Do we still want to add software specific stats or the health dashboard + marvel + rabbit web interface seem enough? (and having links to them seems like a better approach)\nSome notes on how to get logs:\nelasticsearch:\ncurl \"http://mozdefes.example.com:9200/_nodes/stats?all&pretty\"\n[...]\n      \"os\" : {\n        \"timestamp\" : 1398466574532,\n        \"uptime_in_millis\" : 345757,\n        \"load_average\" : [ 2.02, 2.31, 2.31 ],\n        \"cpu\" : {\n          \"sys\" : 3,\n          \"user\" : 50,\n          \"idle\" : 36,\n          \"usage\" : 53,\n          \"stolen\" : 0\n        },\n        \"mem\" : {\n          \"free_in_bytes\" : 145899520,\n          \"used_in_bytes\" : 12345417728,\n          \"free_percent\" : 15,\n          \"used_percent\" : 84,\n          \"actual_free_in_bytes\" : 1987534848,\n          \"actual_used_in_bytes\" : 10503782400\n        },\n        \"swap\" : {\n          \"used_in_bytes\" : 36409344,\n          \"free_in_bytes\" : 2111066112\n        }\n[...]\n          \"heap_used_in_bytes\" : 7359083928,\n          \"heap_used_percent\" : 76,\n          \"heap_committed_in_bytes\" : 9646243840,\n          \"heap_max_in_bytes\" : 9646243840,\n          \"non_heap_used_in_bytes\" : 59530192,\n          \"non_heap_committed_in_bytes\" : 89894912,\nRabbitmq:\n```\ncurl -i -u guest:guest http://mozdef.example.com:15672/api/vhosts\n[...]\n[\n   {\n      \"message_stats\":{\n         \"ack\":172659846,\n         \"ack_details\":{\n            \"rate\":683.4\n         },\n         \"deliver\":172670110,\n         \"deliver_details\":{\n            \"rate\":677.2\n         },\n         \"deliver_get\":325662471,\n         \"deliver_get_details\":{\n            \"rate\":1430.0\n         },\n         \"deliver_no_ack\":152992361,\n         \"deliver_no_ack_details\":{\n            \"rate\":752.8\n         },\n         \"publish\":325649618,\n         \"publish_details\":{\n            \"rate\":1382.0\n         },\n         \"redeliver\":15552,\n         \"redeliver_details\":{\n            \"rate\":0.0\n         }\n      },\n      \"messages\":140,\n      \"messages_details\":{\n         \"rate\":-44.0\n      },\n      \"messages_ready\":0,\n      \"messages_ready_details\":{\n         \"rate\":0.0\n      },\n      \"messages_unacknowledged\":140,\n      \"messages_unacknowledged_details\":{\n         \"rate\":-44.0\n      },\n      \"recv_oct\":116340360463,\n      \"recv_oct_details\":{\n         \"rate\":1008884.6\n      },\n      \"send_oct\":97961129567,\n      \"send_oct_details\":{\n         \"rate\":981831.4\n      },\n      \"name\":\"/\",\n      \"tracing\":false\n   }\n]\ncurl -i -u guest:guest http://mozdef.example.com:15672/api/queues\n[\n   {\n      \"memory\":14120,\n      \"message_stats\":{\n         \"deliver_get\":27,\n         \"deliver_get_details\":{\n            \"rate\":0.0\n         },\n         \"deliver_no_ack\":27,\n         \"deliver_no_ack_details\":{\n            \"rate\":0.0\n         },\n         \"publish\":27,\n         \"publish_details\":{\n            \"rate\":0.0\n         }\n      },\n      \"messages\":0,\n      \"messages_details\":{\n         \"rate\":0.0\n      },\n      \"messages_ready\":0,\n      \"messages_ready_details\":{\n         \"rate\":0.0\n      },\n      \"messages_unacknowledged\":0,\n      \"messages_unacknowledged_details\":{\n         \"rate\":0.0\n      },\n      \"idle_since\":\"2014-04-25 15:16:03\",\n      \"policy\":\"\",\n      \"exclusive_consumer_tag\":\"\",\n      \"consumers\":1,\n      \"backing_queue_status\":{\n         \"q1\":0,\n         \"q2\":0,\n         \"delta\":[\n            \"delta\",\n            \"undefined\",\n            0,\n            \"undefined\"\n         ],\n         \"q3\":0,\n         \"q4\":0,\n         \"len\":0,\n         \"pending_acks\":0,\n         \"target_ram_count\":\"infinity\",\n         \"ram_msg_count\":0,\n         \"ram_ack_count\":0,\n         \"next_seq_id\":0,\n         \"persistent_count\":0,\n         \"avg_ingress_rate\":0.0,\n         \"avg_egress_rate\":0.0,\n         \"avg_ack_ingress_rate\":0.0,\n         \"avg_ack_egress_rate\":0.0\n      },\n      \"status\":\"running\",\n      \"name\":\"amq.gen-OiWTDQZpfY5_MATiK36z8g\",\n      \"vhost\":\"/\",\n      \"durable\":false,\n      \"auto_delete\":false,\n      \"arguments\":{\n  },\n  \"node\":\"rabbit@mozdef1\"\n\n},\n   {\n      \"message_stats\":{\n         \"deliver_get\":153247831,\n         \"deliver_get_details\":{\n            \"rate\":351.0\n         },\n         \"deliver_no_ack\":153247831,\n         \"deliver_no_ack_details\":{\n            \"rate\":351.0\n         },\n         \"publish\":153247751,\n         \"publish_details\":{\n            \"rate\":701.2\n         }\n      },\n      \"messages\":0,\n      \"messages_details\":{\n         \"rate\":83.0\n      },\n      \"messages_ready\":0,\n      \"messages_ready_details\":{\n         \"rate\":83.0\n      },\n      \"messages_unacknowledged\":0,\n      \"messages_unacknowledged_details\":{\n         \"rate\":0.0\n      },\n      \"policy\":\"\",\n      \"exclusive_consumer_tag\":\"\",\n      \"consumers\":1,\n      \"memory\":1115624,\n      \"backing_queue_status\":{\n         \"q1\":0,\n         \"q2\":0,\n         \"delta\":[\n            \"delta\",\n            \"undefined\",\n            0,\n            \"undefined\"\n         ],\n         \"q3\":0,\n         \"q4\":0,\n         \"len\":0,\n         \"pending_acks\":0,\n         \"target_ram_count\":\"infinity\",\n         \"ram_msg_count\":0,\n         \"ram_ack_count\":0,\n         \"next_seq_id\":38621730,\n         \"persistent_count\":0,\n         \"avg_ingress_rate\":320.03417874961303,\n         \"avg_egress_rate\":320.03417874961303,\n         \"avg_ack_ingress_rate\":0.0,\n         \"avg_ack_egress_rate\":0.0\n      },\n      \"status\":\"running\",\n      \"name\":\"events\",\n      \"vhost\":\"/\",\n      \"durable\":false,\n      \"auto_delete\":false,\n      \"arguments\":{\n  },\n  \"node\":\"rabbit@mozdef1\"\n\n},\n   {\n      \"message_stats\":{\n         \"ack\":172915211,\n         \"ack_details\":{\n            \"rate\":701.4\n         },\n         \"deliver\":172925518,\n         \"deliver_details\":{\n            \"rate\":701.0\n         },\n         \"deliver_get\":172925518,\n         \"deliver_get_details\":{\n            \"rate\":701.0\n         },\n         \"publish\":172909487,\n         \"publish_details\":{\n            \"rate\":767.6\n         },\n         \"redeliver\":15552,\n         \"redeliver_details\":{\n            \"rate\":0.0\n         }\n      },\n      \"messages\":140,\n      \"messages_details\":{\n         \"rate\":1.2\n      },\n      \"messages_ready\":0,\n      \"messages_ready_details\":{\n         \"rate\":0.0\n      },\n      \"messages_unacknowledged\":140,\n      \"messages_unacknowledged_details\":{\n         \"rate\":1.2\n      },\n      \"policy\":\"\",\n      \"exclusive_consumer_tag\":\"\",\n      \"consumers\":10,\n      \"memory\":2921928,\n      \"backing_queue_status\":{\n         \"q1\":0,\n         \"q2\":0,\n         \"delta\":[\n            \"delta\",\n            \"undefined\",\n            0,\n            \"undefined\"\n         ],\n         \"q3\":0,\n         \"q4\":0,\n         \"len\":0,\n         \"pending_acks\":140,\n         \"target_ram_count\":\"infinity\",\n         \"ram_msg_count\":0,\n         \"ram_ack_count\":140,\n         \"next_seq_id\":1566154905,\n         \"persistent_count\":140,\n         \"avg_ingress_rate\":671.9935184851178,\n         \"avg_egress_rate\":671.9935184851178,\n         \"avg_ack_ingress_rate\":671.9935184851178,\n         \"avg_ack_egress_rate\":668.3962297616287\n      },\n      \"status\":\"running\",\n      \"name\":\"eventtask\",\n      \"vhost\":\"/\",\n      \"durable\":true,\n      \"auto_delete\":false,\n      \"arguments\":{\n  },\n  \"node\":\"rabbit@mozdef1\"\n\n}\n]\n```\nFor meteor I didn't find anything simple to use yet, I would like to display the server console.\n. uswgi:\n- unix socket and json format http://uwsgi-docs.readthedocs.org/en/latest/StatsServer.html\n- snmp http://uwsgi-docs.readthedocs.org/en/latest/SNMP.html\n. - each script used to process logs before the log is indexed in ArcSight should report the number of events/s it handles.\n. Would be nice also if the workers could adapt to too many errors when pushing data to elasticsearch\n. We should also be able to have alerts when we have less than X events or more than Y events in the last Z mins for specified searches. Example: no logs from a datacenter, from a service\nBonus point if optional integration with nagios\n. This pull request is now ready\n. @jeffbryner FYI a few other python scripts are using pika: https://github.com/jeffbryner/MozDef/search?q=pika&ref=cmdform\n. I'm putting priority high now since the docker config is broken :-(\n. The problem seems to appear when the same field gets edited in the two windows in a same three seconds window. Not sure if we will won't fix or find a solution yet. We feel we're kind of limited by the technology on this one.\n. docker 1.0 is out, see #115 \n. I closing this for now since TTL is not working as expected in ES and we don't recommend using it in MozDef\n. @BjornArnelid Seems good, do you have other commits to push or is your pull request ready?\nThe best source of installation so far is the Docker config, I'm regularly testing it.\nMy mid-term plan is to factorize config files and then create installation scripts used both by the docker config and in the documentation (so that we could actually test the documentation).\nThanks!\n. Yep, we'll have init scripts (both for RHEL and Debian/Ubuntu).\nFor now I guess the best way is to look at the supervisor config.\n. Sorry for the late reply. I reviewed your pull request, there's only one small fix to make and it should be good to merge ;-)\nThanks!\n. Perfect, thanks! :-)\n. @jeffbryner seems ready now, please review.\n. Please wait before merging, some debugging lines to clean and some potential bugs to fix.\n. @jeffbryner this PR is now ready for review\n. @jeffbryner ready for review\n. POC at http://anthony-verez.fr/globe-mozdef/\nCode at https://github.com/netantho/webgl-globe/tree/netantho-mozdef/globe-mozdef\nNeed integration once we have attackers models implemented in MozDef\n. It works.\n. Actually looks like it just takes a long time to load async (before persona is fetched, \"sign in\" is not showed)\n. Is it fixed now? Thanks for the report.\n. @jvehent @jeffbryner LGTM, tested in qa env\n. Do we want to save them in a Meteor session?\n. LGTM\n. A variant for the bot instead of using an ID would be:\nmozdef | NOTICE blablabla\u00a0\nmozdef |\u00a0NOTICE totototo\nmozdef |\u00a0NOTICE tatata\naverez | !alert last\nmozdef | URL for NOTICE tatata\naverez | !alert -1\nmozdef | URL for NOTICE totototo\n. @jeffbryner Do you like my last proposal?\n. I was thinking enabling this confirm dialog when trying to exit between the system detect a new change (modified field) and when the change is saved in the database.\n. Oh, you're right\n. So, the builtin json module in python is an older version of simplejson. They share the same API. Simplejson has a C extension compiled if you use pip. With this, it's 10 times faster to decode json strings. Simplejson is already in our requirements.txt file.\n. Arf, missed that. I'll fix it in the next PR.\n. Should be On Yum-based systems: (only one colon) because :: is used to create a block for a command\nCan you add a commit to fix that?\n. ",
    "BjornArnelid": "I still need to get all the dependencies right on my machine. But i did run pylint anyway since it may give you some useful results. Is there a good place to send / attach the results?\nIf you want me to, I can try to fix some of the easier warnings as soon as i get my environment up and running?\n. Though, is it really desirable to source control the generated files together with the project? \nThank you for the reply!\nMaybe it would be better to share it in some other way? gist perhaps?\nI don't expect time consuming support, I just ran across the project and thought it would be interesting to look into for fun.\n. No, it just sounded like you wanted me to commit lint result files to the repository, i will send a pull request with corrections as son as i am finished :)\n. Just a note, i created a shell script that runs common static code analysis for python on the project.\nIt will run the suites that is installed an simply skip any that is not installed.\nHopefully that makes it easier to run code analysis, and helps standardize on how to run static code analysis in the project. \n. I submitted the change to gdestuynder/mozdef_lib so I am closing this pull request.\n. No problem hope it helps! :)\nFirst off, i do not really need to have MozDef up and running, i am only looking at MozDef because i find the project interesting and want to know how it all works \"under the hood\". And hopefully i am helping out somewhat by reporting things and sending pull requests as i dig around.\nAlso, i did try installing with docker but it failed for me, i am not really sure why though so i wont report it until i know weather it really is a bug and not just a support issue. \n. I can do it if you want, maybe ill find some other dependency missing?\n. Why is netaddr and pygeoip added twice in the requirements list?\n. Never mind, its already fixed :)\n. Nothing more to push, so ready for merging. \nSounds like a good idea. Maybe for starting the service as well? As it is now i kind of have to dig through the install guide to figure out what services to start if i want to launch MozDef.. \n. ",
    "gdestuynder": "Note: unit testing of alerts would also be a good thing\n. one thing i find useful is eps per event type for ex in AS dashboard we have:\nSCL3 auditd 2000 eps current, 10434eps max  2mi total events, 1mb/s\nSCL3 cef 3300eps current 6544eps max, 500k total events, 500k/s\nPHX1 ....\netc.\nmakes it easier to find where spikes come from\n. This is a notification mechanism for FxA\n. hmm i would think that the best is to pass any string to the colorifier maybe\nso that its consistent\n. tested on mozdefqa1\n. @pwnbus I won't reopen the issue but I would still strongly recommend against having any code that can modify configuration at runtime without user interaction as this goes against the user's or operator's expectations.\nIt also makes it impossible to use configuration management tools.. NOTE! existing checkouts will need to run:\ngit submodule foreach git pull origin master\ngit submodule update\n. @ameihm0912 im sure you might have some weight in this as well\notherwise, feel free to assign to me (if not hopefully i dont forget about this - planning to just add the doc references and normalization this quarter as part of other work things)\n.  _source.details.source_uri\n _source.details.destination_uri\nalthough it seems like this might be too complex (since nobody will read what the diff is)\nI wonder if we're ok just matching \"any kind\" of URI and triage the alerts that way. \nthis is otherwise akin to source ip address vs destination ip address (which is a better-understood concept). I'm not actually set either way right now. Might have to think about it a little more.\n. As per \"sourceipaddress\":\n_source.details.sourceuri\n_source.details.destinationuri\n\"actually\" ;)\n. I think so - makes sense to me at least\n. @Phrozyn That's generally exactly one of the problems in the security community. See openssl advisories for example. Nobody knows what high or critical means to them and there's chatter about it every time.\nThat's why we use standardized levels with a complete description of each. Of course best would be if everyone standardized on something - but  at least it helps within our own projects.\nIn this case it sounds like it would be interesting to get Verizon's take on it, regardless of the outcome.\nNote that in code, we do conversions anyway, all the time - no choice :) Ex: vuln2bugs normalizes CVSS and some other stuff into the Mozilla standard levels (surprisingly easier to communicate than CVSS also ;-)\n. Yeah I'm in fact quite happy with the VERIS tagging in MozDef - it's just that it could use standardization with our other tooling.\nAlso, this is great info and thanks!\nHere's some more info/context:\nThe CVSS example is an example of conversion from \"standard X [here CVSS]\" to Mozilla's standard levels. Here's the actual conversion for this case: https://github.com/gdestuynder/vuln2bugs/blob/master/vuln2bugs.json.inc#L45\nThe STIX levels are interesting if they gain traction (https://stixproject.github.io/data-model/1.2/stixVocabs/ImpactQualificationVocab-1.0/) however in my experience, the advantage of using more generic terms (low-medium-high-maximum for ex - notice how it's not \"critical\" but maximum in particular) helps a lot when attempting to apply the same levels with the same meaning to different the different metrics we use (i.e. not just incident impact).\nThe big advantage of being generic, is that when one come across a 'HIGH' anywhere that follows our levels, they know immediately and exactly what that means - even if it's a \"work effort\" and not a \"risk impact\" or if it's anything else really.\nHere's an attempt to translate STIX to Mozilla standard levels as another example:\n| Mozilla Standard Level | STIX |\n| --- | --- |\n| UNKNOWN | Unknown |\n| LOW | Insignificant |\n| MEDIUM | Distracting |\n| HIGH | Painful, Damaging |\n| MAXIMUM | Catastrophic |\nOne thing I immediately notice is that both STIX's painful and damaging have.. almost the same definition (\"somewhat serious\" vs \"serious in the long term\").\n. @gdbassett our level ratings are documented at https://wiki.mozilla.org/Security/Standard_Levels#Risk_levels_definition_and_nomenclature (also \"yes\").\nIn particular they're not tied to specific financial impact as these are company dependent (and Mozilla's rather special in that case). You can find our process for tying that together here: https://wiki.mozilla.org/Security/Risk_management/Rapid_Risk_Assessment\nIn particular though, we have a scoring \"engine\" that takes multiple likelihood factors and a single impact factor (RRA). We're calling the multiple likelihood factors \"data points\" which are normalized to our low-max scale - and we're calculating risk from them on a resulting 0-100 scale.\nThat risk score (0-100) is then mapped back to a normalized level as per the standard levels (so you get both the quick/simple \"how risky do we estimate this is right now?\" and a more granular 0-100 score)\nI think that's might be in fact implementing the same idea you're mentioning.\nIn other words, the veris score from incidents become a low-max data point which is part of the formula resulting in a 0-100 + low-max score per service.\nWhile we haven't documented the risk calculation part it'll come up eventually - we're still playing with the data at this point though :) (also this part has more/mostly code attached to it)\n. randomish-comment:\nwhile toUTC is good to convert dates, to set the default date the program run as I would  consider using os.environ['TZ']='UTC'\nThe reason for that is that it correctly detects timezone as its already doing what toUTC does and then some, and has been in use for a few decades\nHere's an example of implementation that still uses toUTC() and is backward compatible:\nhttps://github.com/gdestuynder/rra2json/blob/master/rra2json.py#L49\n. @pwnbus @Phrozyn @mpurzynski  if one of you could review :)\n. Note: this code is currently live on https://mozdefqa2.private.scl3.mozilla.com/ with 10s refresh for testing.\nReal world scenarios use 15min refresh\nPlease do perform additional testing to ensure that there are no corner cases. If we find some i'll have a look :). erf, hub cli. .travis.yml                                        |  15 +-\n Makefile                                           | 133 ++++---\n alerts/alert_worker.py                             |   3 +-\n alerts/auditd_commands.py                          |   2 +-\n alerts/auditd_sftp.py                              |   2 +-\n alerts/bruteforce_ssh.py                           |   2 +-\n alerts/bugzilla_auth_bruteforce.py                 |   2 +-\n alerts/cloudtrail_deadman.py                       |   2 +-\n alerts/cloudtrail_logging_disabled.py              |   2 +-\n alerts/confluence_shell.py                         |   2 +-\n alerts/deadman.py                                  |   2 +-\n alerts/duo_authfail.py                             |   2 +-\n alerts/duo_fail_open.py                            |   2 +-\n alerts/feedback_events.py                          |   2 +-\n alerts/fxa_alerts.py                               |   2 +-\n alerts/generic_alert_loader.py                     |   2 +-\n alerts/geomodel.py                                 |   2 +-\n alerts/honeycomb.py                                |   2 +-\n alerts/http_auth_bruteforce.py                     |   2 +-\n alerts/http_errors.py                              |   2 +-\n alerts/ldap_add.py                                 |   2 +-\n alerts/ldap_delete.py                              |   2 +-\n alerts/ldap_group.py                               |   2 +-\n alerts/ldap_lockout.py                             |   2 +-\n alerts/lib/alert_plugin_set.py                     |   5 +-\n alerts/lib/alerttask.py                            |   7 +-\n alerts/multiple_intel_hits.py                      |   2 +-\n alerts/old_events.py                               |   4 +-\n alerts/open_port_violation.py                      |   2 +-\n alerts/plugins/dashboard_geomodel.py               |   5 +-\n alerts/promisc_audit.py                            |   2 +-\n alerts/promisc_kernel.py                           |   2 +-\n alerts/proxy_drop_executable.py                    |   2 +-\n alerts/proxy_drop_non_standard_port.py             |   2 +-\n alerts/session_opened_sensitive_user.py            |   2 +-\n alerts/sqs_queues_deadman.py                       |   2 +-\n alerts/ssh_access_signreleng.py                    |   2 +-\n alerts/ssh_bruteforce_bro.py                       |   2 +-\n alerts/ssh_ioc.py                                  |   2 +-\n alerts/ssh_key.py                                  |   2 +-\n alerts/ssh_lateral.py                              |   2 +-\n alerts/ssh_password_auth_violation.py              |   2 +-\n alerts/ssl_blacklist_hit.py                        |   2 +-\n alerts/trace_audit.py                              |   2 +-\n alerts/unauth_ssh.py                               |   2 +-\n alerts/vpn_duo_auth_failures.py                    |   2 +-\n alerts/write_audit.py                              |   2 +-\n bot/mozdefbot.py                                   |   5 +-\n bot/mozdefbot_slack.py                             |   3 +-\n cloudy_mozdef/Makefile                             |  56 +++\n cloudy_mozdef/cloudformation/base-iam.yml          |  28 ++\n cloudy_mozdef/cloudformation/mozdef-efs.yml        | 207 +++++++++++\n cloudy_mozdef/cloudformation/mozdef-es.yml         |  94 +++++\n cloudy_mozdef/cloudformation/mozdef-instance.yml   | 230 ++++++++++++\n cloudy_mozdef/cloudformation/mozdef-mq.yml         | 175 +++++++++\n cloudy_mozdef/cloudformation/mozdef-parent.yml     | 177 +++++++++\n .../cloudformation/mozdef-security-group.yml       |  67 ++++\n cloudy_mozdef/dmake                                |  40 ++\n cloudy_mozdef/packer/packer.json                   |  36 ++\n cron/collectAttackers.py                           |   7 +-\n cron/correlateUserMacAddress.py                    |   7 +-\n cron/createFDQNBlockList.py                        |   3 +-\n cron/createIPBlockList.py                          |   3 +-\n cron/esCacheMaint.py                               |   5 +-\n cron/eventStats.py                                 |   7 +-\n cron/google2mozdef.py                              |   3 +-\n cron/healthAndStatus.py                            |   5 +-\n cron/healthToMongo.py                              |  12 +-\n cron/okta2mozdef.py                                |   5 +-\n cron/pruneIndexes.py                               |   5 +-\n cron/rotateIndexes.py                              |   5 +-\n cron/sqs_queue_status.py                           |  13 +-\n cron/syncAlertsToMongo.py                          |   7 +-\n cron/update_geolite_db.py                          |   5 +-\n cron/update_ip_list.py                             |   3 +-\n cron/verify_event_fields.py                        |   7 +-\n docker/builder/Dockerfile                          |  22 ++\n docker/builder/Makefile                            |   5 +\n docker/builder/requirements.txt                    |   4 +\n docker/compose/docker-compose-cloudy-mozdef.yml    | 178 +++++++++\n docker/compose/docker-compose-cron.yml             |   4 +-\n docker/compose/docker-compose-norebuild.yml        |  35 ++\n docker/compose/docker-compose-rebuild.yml          |  67 ++++\n docker/compose/docker-compose-rest.yml             |   4 +-\n docker/compose/docker-compose-tests.yml            |   4 +-\n docker/compose/docker-compose.yml                  |  55 +--\n docker/compose/live-reload.override.yml            |   4 +-\n docker/compose/mozdef_base/Dockerfile              |  16 +-\n docker/compose/mozdef_meteor/Dockerfile            |  24 +-\n docker/compose/tester/Dockerfile                   |   2 +-\n docker/conf/initial_setup.py                       |  12 +-\n docs/source/cloud_deployment.rst                   |  11 +\n docs/source/demo.rst                               |  10 +\n docs/source/images/MozDefCloudArchitecture.png     | Bin 0 -> 70286 bytes\n docs/source/images/MozDefCloudArchitecture.xml     |   2 +\n docs/source/index.rst                              |   1 +\n docs/source/installation.rst                       | 401 +++++++++++----------\n examples/demo/sampleData2MozDef.py                 |  20 +-\n lib/query_models/__init__.py                       |  13 -\n meteor/.meteor/packages                            |  24 +-\n meteor/.meteor/release                             |   2 +-\n meteor/.meteor/versions                            |  47 +--\n meteor/client/incidentEdit.html                    |  57 ++-\n meteor/client/incidents.js                         |  44 +--\n meteor/client/investigationEdit.html               |  23 +-\n meteor/client/investigations.js                    |  71 ++--\n meteor/client/layout.js                            |  36 ++\n meteor/client/main.js                              |   2 +-\n meteor/client/mozdef.js                            |  42 +--\n meteor/client/mozdefhealth.js                      |   3 +\n meteor/client/verisTags.html                       |  13 +-\n meteor/imports/collections.js                      |   5 +-\n meteor/imports/helpers.js                          |  17 +-\n meteor/imports/settings.js                         |  23 +-\n meteor/package-lock.json                           |  23 +-\n meteor/package.json                                |   7 +-\n meteor/public/css/mozdef.css                       |  13 +-\n meteor/server/main.js                              |   3 +-\n meteor/server/mozdef.js                            |  12 +-\n mozdef_util/.editorconfig                          |  21 ++\n mozdef_util/.github/ISSUE_TEMPLATE.md              |  15 +\n mozdef_util/.gitignore                             | 102 ++++++\n mozdef_util/.travis.yml                            |  16 +\n mozdef_util/CONTRIBUTING.rst                       | 128 +++++++\n mozdef_util/HISTORY.rst                            |   8 +\n mozdef_util/LICENSE                                | 340 +++++++++++++++++\n mozdef_util/MANIFEST.in                            |  10 +\n mozdef_util/Makefile                               |  88 +++++\n mozdef_util/README.rst                             |  24 ++\n mozdef_util/docs/Makefile                          |  20 +\n mozdef_util/docs/conf.py                           | 160 ++++++++\n mozdef_util/docs/contributing.rst                  |   1 +\n mozdef_util/docs/history.rst                       |   1 +\n mozdef_util/docs/index.rst                         |  19 +\n mozdef_util/docs/installation.rst                  |  51 +++\n mozdef_util/docs/make.bat                          |  36 ++\n mozdef_util/docs/readme.rst                        |   1 +\n mozdef_util/docs/usage.rst                         |   7 +\n mozdef_util/mozdef_util/__init__.py                |   7 +\n {lib => mozdef_util/mozdef_util}/bulk_queue.py     |   0\n .../mozdef_util}/elasticsearch_client.py           |   0\n {lib => mozdef_util/mozdef_util}/event.py          |   0\n {lib => mozdef_util/mozdef_util}/geo_ip.py         |   0\n {lib => mozdef_util/mozdef_util}/plugin_set.py     |   0\n mozdef_util/mozdef_util/query_models/__init__.py   |  13 +\n .../query_models/aggregated_results.py             |   0\n .../mozdef_util}/query_models/aggregation.py       |   0\n .../mozdef_util}/query_models/boolean_match.py     |   0\n .../mozdef_util}/query_models/exists_match.py      |   0\n .../mozdef_util}/query_models/less_than_match.py   |   0\n .../mozdef_util}/query_models/phrase_match.py      |   0\n .../query_models/query_string_match.py             |   0\n .../mozdef_util}/query_models/range_match.py       |   0\n .../mozdef_util}/query_models/search_query.py      |   2 +-\n .../mozdef_util}/query_models/simple_results.py    |   0\n .../mozdef_util}/query_models/term_match.py        |   0\n .../mozdef_util}/query_models/terms_match.py       |   0\n .../mozdef_util}/query_models/wildcard_match.py    |   0\n {lib => mozdef_util/mozdef_util}/state.py          |   0\n .../mozdef_util}/utilities/__init__.py             |   0\n .../mozdef_util}/utilities/dict2List.py            |   0\n .../mozdef_util}/utilities/dot_dict.py             |   0\n .../mozdef_util}/utilities/is_cef.py               |   0\n .../mozdef_util}/utilities/key_exists.py           |   0\n .../mozdef_util}/utilities/logger.py               |   0\n .../mozdef_util}/utilities/remove_at.py            |   0\n .../mozdef_util}/utilities/toUTC.py                |   0\n .../mozdef_util}/utilities/to_unicode.py           |   0\n mozdef_util/requirements_dev.txt                   |  14 +\n mozdef_util/setup.cfg                              |  22 ++\n mozdef_util/setup.py                               |  61 ++++\n mozdef_util/tox.ini                                |  21 ++\n mq/esworker_cloudtrail.py                          |  11 +-\n mq/esworker_eventtask.py                           |  15 +-\n mq/esworker_papertrail.py                          |   3 +-\n mq/esworker_sns_sqs.py                             |   7 +-\n mq/esworker_sqs.py                                 |  15 +-\n mq/lib/plugins.py                                  |   5 +-\n mq/lib/sqs.py                                      |   2 +-\n mq/plugins/broFixup.py                             |   2 +-\n mq/plugins/cloudtrail.py                           |   3 +-\n mq/plugins/complianceitems.py                      |   3 +-\n mq/plugins/fluentdSqsFixup.py                      |   3 +-\n mq/plugins/geoip.py                                |   3 +-\n mq/plugins/guardDuty.py                            |   7 +-\n mq/plugins/parse_sshd.py                           |   3 +-\n mq/plugins/suricataFixup.py                        |   2 +-\n mq/plugins/vulnerability.py                        |   3 +-\n requirements.txt                                   |  17 +-\n rest/index.py                                      |   9 +-\n tests/docker-compose.yml                           |  15 +-\n tests/lib/query_models/query_test_suite.py         |   3 +-\n tests/lib/query_models/test_aggregation.py         |   3 +-\n tests/lib/query_models/test_exists_match.py        |   3 +-\n tests/lib/query_models/test_less_than_match.py     |   3 +-\n tests/lib/query_models/test_phrase_match.py        |   3 +-\n tests/lib/query_models/test_query_string_match.py  |   3 +-\n tests/lib/query_models/test_range_match.py         |   3 +-\n tests/lib/query_models/test_search_query.py        |   3 +-\n tests/lib/query_models/test_term_match.py          |   3 +-\n tests/lib/query_models/test_terms_match.py         |   3 +-\n tests/lib/query_models/test_wildcard_match.py      |   3 +-\n tests/lib/test_bulk_queue.py                       |   5 +-\n tests/lib/test_elasticsearch_client.py             |   5 +-\n tests/lib/test_event.py                            |   5 +-\n tests/lib/test_geo_ip.py                           |   3 +-\n tests/lib/test_plugin_set.py                       |   3 +-\n tests/lib/test_state.py                            |   3 +-\n tests/lib/utilities/test_dot_dict.py               |   3 +-\n tests/lib/utilities/test_key_exists.py             |   3 +-\n tests/lib/utilities/test_toUTC.py                  |   9 +-\n tests/loginput/loginput_test_suite.py              |   3 +-\n tests/mq/plugins/test_broFixup.py                  |   3 +-\n tests/mq/plugins/test_suricataFixup.py             |   3 +-\n tests/mq/test_esworker_sns_sqs.py                  |   5 +-\n tests/rest/rest_test_suite.py                      |   3 +-\n tests/suite_helper.py                              |   5 +-\n tests/unit_test_suite.py                           |   4 +-\n 218 files changed, 3393 insertions(+), 854 deletions(-)\n(git diff --stat master..infosec_workweek). technical note: we used to have a dependency for this so that containers would build before tests, but this bothered some other people as docker does not detect if images were already built very quickly \nsome would say, its not a bug, its a feature ;-)\nthings i though of to make this better: detect dockerfile changes instead of relying on docker build, though this will likely not easily detect if other files on the filesystem changed (main reason why docker build is slow to detect changes). fixing travis.. :). @gene1wood  is the env variable from a file better than just a file? in both case there's a file, so i'm wondering. Is this so that you can import it from another directory than the example?\nIf so i think i wouldn't include and example because it makes it likely to be wrongly committed with a secret. @gene1wood I'm thinking we could just document the fact that you should have it in the env and not provide the file - i think a lot of people just copy paste in their env from their secret manager, while if its in a .sh file they'll leave it in the repo anyway and forget (just like the json file)\nthat way you can still make a file in the repo, but its on you instead of being the default\nim preapproving as this does not change behavior from the json file in this case, and i leave it up to you to choose, though the above would be my own choice. we have to either modify upstream IRC code to integrate it in the irc module class, or to make it a separate class/lib that the module can import.\nany preference?\n. @Phrozyn \nshouldnt it be:\n3.  /etc/init.d/nginx start or systemctl start nginx\n$ /etc/init.d/nginx start\n$ systemctl start nginx\ninstead?\n(mainly \"service\" is only available on some distros anyway like RH/CentOS, but also just to be consistent in the doc - theres another instance above for elastisearch)\n. that would mean each of these feature potentially lives in their directory, such as:\ncron/auth02mozdef/__init__.py\n  cron/auth02mozdef/DotDict.py\netc.\nNot really against it though since these are currently self-contained programs its hard to say how it should be done (else i'd want a standard \"shell\" for the main code and run things as class/methods and have test cases)\n. probably should use:\nif [[ $? -eq 0 ]]; then\n...\nbecause it's more reliable and probably an important check\nalternative:\ncurl -s -XGET mozdef.private.scl3.mozilla.com:9200 || {\n   echo \"curl connection failed\"\n   exit 127\n}\n. but otherwise, really, you want that for each command above instead (so that you know all commands worked). likewise id just add a quick error check:\ncurl -XPOST mozdef.private.scl3.mozilla.com:9200/_aliases -d' { \"actions\": [{ \"add\" : { \"index\" : \"events-'${index_new_date}'\", \"alias\" : \"events\" } }, { \"remove\" : { \"index\" : \"events-'${index_current_date}'\", \"alias\" : \"events\" } } ]}' || {\necho \"ERROR: POST failed - failed to roll over events alias\" \nexit 127\n}\n. (or with just warning if failure is acceptable). same. you might want to just add a contributor file at the root (common practice since its more discoverable than commits and make people happy to be recognized). that seems right. true!. it seems to boot just fine, we also removed it in cloudymozdef\nif not, i would use wait-for-it instead of sleep, which is docker's recommended way to do it as well. that said, in my tests i found no issue with not sleeping so far. note: the py.test 10s wait is equally annoying during testing. i added a curl command for now. yeah that probably doesnt work with curl. i changed this to a bash command so it works with all and still no deps. ```suggestion\nOIDC_CLIENT_SECRET is set in an environment variable by running \"source aws_parameters.sh\"\n. clarity.suggestion\necho \"Will use the following environment variables:\"\ncat $dmake_env_file\n```. im just assuming this doesnt work in the env file, or that we fear people would hard code it?. in docker compose i use a double file so that just like the other pattern we have in your previous PR one can use a file, or not, at their convenience, but there is no \"default in file\"\nthat said without compose its kind of complicated to make this reliable because you need an existing 2nd file  (and even with compose i had to do a bit of magic with make)\nI'd say just ship it as is. this sort is the bug that took forever to figure out.. this is the new parameter to avoid duplicating when paginating, but it also needs a 1 millisecond bump (just like the previous api needed a 1 second bump.. except now its millisecs). Added default offsets for all categories - only authentication really supports it, but this avoid running into compat issues with the file. this is the new offset logic so that we save the offset sent by duo when we're done processing this batch of logs, only authentication supports this right now. ",
    "pwnbus": "We've added unit tests for alerts and for other key components of MozDef in https://github.com/mozilla/MozDef/pull/399  We will be enabling travisci very soon.. We've fixed this issue in https://github.com/mozilla/MozDef/pull/399 and verified that we are only using setConfig in the example demo python script, so I believe this issue is \"resolved\". This has been fixed in https://github.com/mozilla/MozDef/pull/399. @ajomadlabs We have docker containers that you can build (we don't push them to a registry yet) that will stand up a local instance. Would that work for ya?. @ajomadlabs How would you get started with Docker or how would you get started with MozDef using docker?. For docker, they have some good docs for folks starting out, I'd recommend looking at https://docs.docker.com/get-started/#setup \nFor running MozDef in docker, simply run \nmake simple-build\nmake simple-run\nAnd if you want to shutdown the container:\nmake simple-stop\nOnce those commands finish (should take ~10 minutes to build), simply visit \"http://127.0.0.1\" for the web ui, and \"http://127.0.0.1:9090/app/kibana\" for the kibana interface.. FYI the docker stuff is quite recent for us, so we haven't properly added documentation for it, so let me know if you run into any problems!. @ajomadlabs Not a problem! Take however long you need. If you get stuck, let me know!. @jeffbryner @Phrozyn and I talked about this, and thought maybe it's best to bring up during our next infosec meeting real quick to get people's thoughts?\n. Modified to reflect 3 months\n. Closing PR since this change is no longer needed.. r+\n. PLEASE DONT MERGE YET, just curious about reviewers, then I'll work on moving all callers of toUTC to use this utility function.\n. Closing this PR due to changes coming down the pipe that remove the need for this patch.. We've added support for alert plugins, so this issue is resolved. Closing PR due to changes coming down the pipe that removes most of these changes. r+\n. r+. @cglewis We apologize for the dead air here. We were actively doing development on a private project, but have just recently merged these changes into this public repo (this repo will be the one used going forward FYI).\nAs part of the merging of the code bases, it looks like we've encountered a merge conflict for these changes. Since you're operating on a fork, we have a couple options here:\n1. You can fix the merge changes yourself on your fork and then I'll merge this PR\n2. I can create a new branch, and cherry-pick your changes (so that we retain the commit history), and will resolve the merge conflicts myself within the branch. I will then close out this PR, and reopen a new one pointing to the new branch.\nI realize this PR was quite old and was just sitting here, so in my opinion, option 2 would be best and easiest for all.. I've created https://github.com/mozilla/MozDef/pull/401 which resolves the merge conflict, so I'm closing this PR. \nThanks for the contribution!. r+\n. Closing issue since this cron script has been converted to a worker.. Closing PR since it doesn't actually fix the problem.... r+. r+. There will be more changes to come, since this dockerfile uses outdated versions of ES and other services.. We'd have to update our requirements file to use dependencies that support python3. Sad to see we didn't even get to running our unit tests to see.. I've pinned the unit tests down to what we are running in prod (which is 2.7.11), but that was for consistency purposes.\nIt would only affect our deployment process.. IMO, I think this text should be nuked for a couple reasons:\n\n\nThis info can get outdated, real quick and can be a pain to keep accurate. Do we have to on January 1st of every year, have to modify each and every file to update the year? \n\n\nEvery file we edit, we need to make sure our name is in the contributors list? More developer work, for what benefit?\n\n\nThis doesn't provide any additional info that git history can't provide, while git history provides even more info that this\n\n\nIf let's say DeveloperA writes a block of code in a file, so DeveloperA appends his/her name to the file. DeveloperB comes around, and removes that block of code as part of a feature request or improvements. Does DeveloperB then remove DeveloperA from the contributors list in the file? If yes, then that means DeveloperB must identify who wrote each line of code in each file modified and make sure the contributors list at the top is accurate (which git history would be used). If DeveloperB doesn't remove DeveloperA from the list, this doesn't accurately reflect the current state of the file. DeveloperA no longer has any code in the file, so should their name remain in the file?. Also implemented https://github.com/mozilla/MozDef/pull/586 to clean up remaining lists in html/js files.. @jeffbryner Fixed the details.path in https://github.com/mozilla/MozDef/pull/417/commits/9e0b613f3c28831a56ebf845423b0f0eb29a5931 and fixed the lowercase registration term in https://github.com/mozilla/MozDef/pull/417/commits/4b665d8771ac57e51ae7b47bc0a566cfed211f5c. I'm going to manually override the fact that the tests need to pass for this PR, since I did not have travis setup correctly to build on forked branch PRs. In the future, when a PR is created from a forked branch, the tests should run.. Yeah of course! I'll unassign myself. In order to assign you to this item, we'll need to add you as a contributor.. Not a problem! Any contribution is a good contribution :)     If you run into any problems, feel free to let me know and we can work through them!. @scriptonist Run the make command from the project root, so don't 'cd docker'. @scriptonist The makefile is located here https://github.com/mozilla/MozDef/blob/master/Makefile  Have you updated your local copy of mozdef recently? \"git pull origin master\"? Do you see other files that are in the project root as well (requirements.txt, README.md etc?)\n\n\n. Ohh, I'm sorry, it's not make build, you need to run \"make single-build\" and \"make single-run\".. Once you run make single-run once, you can do \"make single-rebuild\" which runs a build, stops the current container, and reruns the updated container. This is helpful for debugging/developing.. It appears as though the docs are outdated, so I apologize for that. I'll create an issue to update those, all of this docker stuff is fairly recent, so we'll work through on ironing out any issues :). We finally got this merged and rolled out, thank you for the PR!. Another possible idea for a logo for these 2 locations would be http://blog.seanmartell.com/wp-content/uploads/2012/03/logo_0061_62.png. An example of the header logo change https://imgur.com/a/QirTv. Maybe we can use this logo (or whatever one we decide on), in the readthedocs pages as well. @jeffbryner That sounds good to me, I figured since there was already an image in the repo called 'logo.png', that this was the preferred logo to use.. @Phrozyn @jeffbryner I updated the favicon with the mozilla.org one that you pointed towards. \nI've also removed the firefox logo image on the header on the left (next to the MozDef 'home' button), but kept the mozilla tab image on the right. I figured we would address updating the mozilla tab as part of a larger effort to update the look and feel.\nThoughts?. @jeffbryner Originally this was meant as a quick update of some of the images in mozdef (including adding a favicon), but this is turning into a lot larger work item.\nI'm gonna close this PR and we can start the initiative phase of rebranding sometime.. Can we close this PR in favor of https://github.com/mozilla/MozDef/pull/445 ? If you were trying to create a PR for the unit test functions, that's fine, then my comments from the 445 PR are relevant here.\nWhichever you decide is fine with me!. @jeffbryner Yeah, I don't think we're gonna enable it in prod yet, but more just use it as reporting for now that we can start to tackle the larger issues, and work our way down. Hopefully eventually we'll be able to enable it in prod, but right now it'll be noisy every time it runs.. https://github.com/mozilla/MozDef/pull/463 implements the change. Closing issue.. Merging even though our status checks are in limbo (I have a fix for this), but since this is only a documentation change, this doesn't carry any risk.. Wahoo! Your first contribution!. This is a result of https://github.com/mozilla/MozDef/commit/77936b10cd18367229ad122c019eb51f3a7a252c adding unnecessary packages that meteor container doesn't need.. I believe this requires more testing, it worked on tcp, but appears to not work on udp?. @scriptonist Since we haven't heard anything from ya for a month, we're gonna close this PR. \nFeel free to address the comments and commit the fixes, and we can either reopen this PR, or create a new one :). Since I needed some of the changes in https://github.com/mozilla/MozDef/pull/464  I merged those into here, so the changes that actually occur, are from https://github.com/mozilla/MozDef/pull/470/commits/e252eff6a6fd00615560c8a033e736fad9f079a2 on up.. We've assigned this task to this sprint (that ends 9/27). I'll update this ticket once progress is made.. Implemented in https://github.com/mozilla/MozDef/pull/483. Closing issue since this is now running in production.. @jeffbryner any update on this? I believe we need to apply these changes to the docker/Dockerfile as well?. This is setting us up to specify or not specify a list of plugins to enable. This allows us to turn on or off certain plugins if we so choose.. @jeffbryner I think I may have found a solution that can pull the local system timezone, so that we don't have to have 'UTC' hardcoded in the toUTC function. Thoughts?. I did some benchmarking, and the faster of the implementations depends on if you import the library or not. Since we're only importing and running the function once, no matter how many times toUTC is actually called, this performance should have a very minimal affect.. Let's use 'UNKNOWN' as the place holder text. I'll update this.. @Phrozyn I updated the config option name to be \"listen_host\", since I think that's a little more descriptive as to what that value is used for.. Closing this PR since @mpurzynski implemented the updated alerts as generic_alerts.. @andrewkrug I decided to wrap your meteor URL changes into this PR, since I had to modify the same files to update the download location for kibana.. Implemented in https://github.com/mozilla/MozDef/pull/513  Closing issue since that PR has since been merged.. Closing issue. Thanks for pointing that out!. Closing this since it's implemented in https://github.com/mozilla/MozDef/pull/515. @Phrozyn Thoughts?. @edmorley I decided to include the IP address in every message, at the end. We also filter out the city now, if it is unknown.\nThis means for your example, the modified summary would be 'Did you recently login from Norway (1.2.3.4)?' We will also be adding the date from which it happened to the beginning of the message, which should further help in identifying the legitimacy. \nThanks for the recommendation!. This has been pushed to production, so you should start to see the IP show up in the summary, as well as the city excluded if it is 'Unknown'. We haven't quite wrapped up the \"showing the date in the summary\" part yet, but hopefully that can get resolved soon.. @Phrozyn Agreed, I haven't created it yet because I was looking for a thumbs up on changing it from a generic plugin, to an alert specific dashboard plugin.. This could use support for ICMP type messages.. I still need to verify this in qa.. Verified this is working as expected in QA. All set to review.. @Phrozyn Can you resolve the conflict please?. @Phrozyn Changing this to debug means that we will no longer get the cron email when this task completed successfully.. @yashmehrotra Sorry about the failed unit tests (the last round), I've pushed a fix to master, if you update your branch with specifically https://github.com/mozilla/MozDef/pull/579/commits/4f37cb39bf56abb75886637d32b96c2e5b74f58e , the tests should pass.. I've tested this change with merging with master and everything passes (https://travis-ci.org/mozilla/MozDef/jobs/325204550) so this is safe to merge.. Very nicely done!. Thanks for the patch!. Closing this PR since this has been implemented in other PRs using es_upgrade_version_5 as the destination branch.. Closing PR since we've decided to break out ES5 changes from ES6.. @jeffbryner Yeah, this PR isn't complete, as there are still a couple things that need to get figured out/fixed. Also, FYI this is for ES5, not ES6 (yet).. Closing PR since https://github.com/mozilla/MozDef/pull/625 is the preferred one now. @neomh We currently aren't publishing the docker container to dockerhub or any external registry. \nWe've created a few make commands that can help in building and starting MozDef in docker, but that requires ~10 minutes to build and you gotta pull mozdef down via source in order to do that (which I understand is not ideal). See https://mozdef.readthedocs.io/en/latest/installation.html#docker for more details.\nI'm gonna close this issue, since I've created https://github.com/mozilla/MozDef/issues/617 to keep track of the work to actually do the work.\n. As per initial question from https://github.com/mozilla/MozDef/issues/616. Closing due to separating ES5 from ES6 changes.. @jeffbryner I had made the change in the destination branch, but that prevents travis from running our tests with the updated ES version, I'll cherry pick that commit into this branch, thanks for the call out!. Closing PR since this was implemented in https://github.com/mozilla/MozDef/pull/638. Replaces https://github.com/mozilla/MozDef/pull/627 since we're breaking out ES5 from ES6 changes.. @jeffbryner After having a meeting about it, this is what we propose as the changes to the mapping file, thoughts?. Closing PR since it might take us a bit of time to circle back to ES5 -> ES6 upgrade.. Thanks for the submission, I'm going to point this PR at a virtualenv specific branch \"virtualenv_path_change\", instead of merging directly into master. There are other changes that we are preparing that go along with this which will also be merged into \"virtualenv_path_change\" branch.. Closing since this is implemented in https://github.com/mozilla/MozDef/pull/653. Closing this issue since we implemented this behavior in other alerts.. The tests are failing as expected, because we changed the attributes of the alert. You need to modify https://github.com/mozilla/MozDef/blob/master/tests/alerts/test_old_events.py#L14\nThe failing tests also provide some helpful output as to why the tests fail -> https://travis-ci.org/mozilla/MozDef/builds/367907548#L907. Closing PR since this was implemented in another method.. Updated in https://github.com/mozilla/MozDef/pull/984. Closing issue since this is no longer an issue in master.. Closing this PR in favor of https://github.com/mozilla/MozDef/pull/725. These changes seem to already be in place in master:\nhttps://github.com/mozilla/MozDef/blob/master/cron/auth02mozdef.py#L350\nhttps://github.com/mozilla/MozDef/blob/master/requirements.txt#L35\nhttps://github.com/mozilla/MozDef/blob/master/requirements.txt#L25\nMy guess is that your fork isn't update to date?. Closing PR since these changes are no longer needed (they were already added to the codebase). I love this idea, and we could even extend this to include username if the PTR is from a VPN ip.. @gene1wood This looks good, no, you shouldn't need to add this to the rebuild docker-compose, because we only want it to run in cloudy mozdef environment.. I think it overall looks good, to answer a couple of your questions @gene1wood \n1. My guess is that was meant to describe a problem in any of our documentation\n2. I'm not sure there's a way, besides unless git log to get the commit hash or something like that. @Phrozyn mind taking a look here since you were the original creation of that template I believe?. Decided to fix in https://github.com/mozilla/MozDef/pull/891 so closing PR. We decided to remove this field from the default mapping instead (https://github.com/mozilla/MozDef/pull/894), so this PR is no longer needed.. Added in https://github.com/mozilla/MozDef/pull/1043 . This requires creating a file for each individual visualization, as well as creating a file for the dashboard.. I'm just stating what work has been done as a result in case folks wanted to add/update/improve a dashboard.\nSorry for the confusion, I suppose the PR is somewhat self descriptive :). Very nice!. The new preferred way is to make build-tests, however that still appears to build all of the containers, we should fix that so it only builds the tester, rabbitmq, and elasticsearch.. @tristanweir .flake8 is the config for pep8 essentially, so we want that part of version control. Slowly but surely we dwindle that exception list down. I created https://github.com/mozilla/MozDef/pull/1095 which removes about 100mb of space per container.. \nThis styles the commands to look like the following. @mpurzynski I added hostname_from_ip which I think satisfies your requirements here.. @mpurzynski Adherence to coding standards promotes openness, readability, and simplicity. We've been lax on these before but it's worth the time investment to make this consistent across the project, no matter if it's tests or the implementation.\nDespite the length of the code, it only took 5 minutes to update this code to conform to PEP8 standards, which I've done for you this one time at https://github.com/mpurzynski/MozDef/pull/1.\nLet me know if you want and we can figure out a way to make your dev environment as easy to manage for PEP8 as possible (e.g. auto correction, linting). For example, sublime text can be configured to report on pep8 failures when you're typing, as well as autofix certain issues when you save the file.. If you're using the docker environment, you can also send syslog events to \"docker host ip:514\".. @jeffbryner The tests require that field to be set when alerts are INFO level or lower. The reason this field exists, is to determine whether or not the bot should receive this alert.\nWe can change the default to be, notify_mozdefbot = False, but then we have to specify this field for every alert that is WARNING or higher.. @jeffbryner @darakian  https://github.com/mozilla/MozDef/pull/975/files#diff-fc564a2df4483f88fb7ac9a26d9be17dR32 is the regression that this is now causing those templated tests to fail. I'll have a PR up shortly that will fix that.. Fixed in https://github.com/mozilla/MozDef/pull/981. This has been merged, so I'm closing the issue.\n@darakian In order to enable the alert, you need to modify https://github.com/mozilla/MozDef/blob/master/alerts/lib/config.py#L13 . If you're using docker, you can temporarily enable it by modifying the corresponding config.py file in docker -> https://github.com/mozilla/MozDef/blob/master/docker/compose/mozdef_alerts/files/config.py. @darakian Yes, there needs to be one line per alert that you want to enable.\nCurrently, the only supported way is to modify this file https://github.com/mozilla/MozDef/blob/master/docker/compose/mozdef_alerts/files/config.py on disk before building/running, we don't support passing in that list via environment variables or anything (I just created https://github.com/mozilla/MozDef/issues/982 to add that support).\n. @darakian I've got some docs internal that describe what each one is used for, let me try and add them to the repo. @darakian I created https://github.com/mozilla/MozDef/pull/988 which I think adds what I originally had on hand, with the expectation that these will get improved in the future.. @ryandeivert since I haven't heard from ya in a bit regarding this PR and it's changes, I'm thinking later on this week, I'll modify this PR to point to a different branch, and then I'll commit my changes on-top of yours, and then I'll have a PR to merge that branch into master.\nThis way your changes still get merged, and my comments can get addressed :). Very nice!. I think we can also delete cron/oui.txt as well?. Great idea! One thing worth noting, all our PR status' will fail as a result of a bug in one of our dependencies, we're actively trying to work through this, and will get resolved as soon as https://github.com/mozilla/MozDef/pull/1032 is merged (should be done in the next couple hours at most hopefully). Should be all set if you merge master into this branch!. @darakian FYI it would be 'foo' in aggreg['events'][0]['_source']['tags']. Closed issue for now since we're no longer receiving these mapping problems.. This issue has been recently fixed (https://github.com/mozilla/MozDef/pull/1032), and was due to a dependency of ours not version locking a dependency.. Yeah, you'll need to git pull and then rebuild the containers unfortunately. . When writing unit tests, this makes it easier to show the entire path (with ../ expanded into the directory name).\nGoes from\n$SRC/tests/alerts/../../alerts/lib/../deadman_generic.json\nto\n$SRC/alerts/deadman_generic.json. Also, great change, the last thing we want is folks debugging test's internal logic to find out why things aren't working!. @darakian We have https://github.com/mozilla/MozDef/blob/master/tests/alerts/alert_test_suite.py#L115 already in place that should do this logic, but the assert error text could probably be updated to be specific as to what we're checking for.. @darakian Huh, interesting. On my local dev environment, I just added an expected_alert to a test, and it failed accordingly. Do you have a working example on your end where that isn't happening?. Makes the messages look something like: \n. @Phrozyn I updated the PR to use the preferred new rabbitmq repo so it should be all set to rereview!. This will fail as a result of the mozdef_util changes, but once those are initially approved, I'll push out a new version of mozdef_util which will fix the build failures.. Instead of running raw docker-compose commands, we did our best to abstract that away into make file targets, so you can simply run make build and make run instead.. Ugly, ugly whitespace.\n. A couple notes here:\n1. I decided to use UTC as the default timezone, I couldn't figure out how to grab the local timezone using tzlocal that didn't cause stuff to break, but the assumption I had was to assume we wanted UTC? I will probably circle back to this to see if I can use tzlocal.\n2. I didn't move any of the cron scripts or anything that have this method redefined yet, I'm more or less looking for reviewers to see if this is how we want to tackle this.\n. I need to switch from 2 spaces to 4 for this file....\n. Although, now that I think about #1, do we want to have the timezone be dynamic? Maybe we should make a hard rule that any timestamps in mozdef, will always try and be UTC?\nIn this case, I can modify the code to remove the second parameter.\n. I removed a couple methods to detect if a variable is a digit, and to return the number of digits. I believe we don't need to support numbers like 1+2i do we?\n. More of a in the future type of thing, but I think we should get to a point where we have one class per file. I don't think this is a blocker or anything for now\n. Agreed, I think eventually they could have certain functions/classes exposed to the cron jobs, or like you said, some sort of \"shell\" that you overwrite a \"run\" or \"execute\" function to perform the custom code.\n. Good find, this was due to the rest config, I fixed that and the tests in https://github.com/mozilla/MozDef/pull/402/commits/a695692382f34cd60e707d2ddf7f871998aebcdc and https://github.com/mozilla/MozDef/pull/402/commits/bf465b0480138c5bb65eaa159ffd2d686bcfb0d8. This will eventually get readded, but I just wanted to update the installation of our dependencies, so this will be the next step.. We should write to a temp file, then move the temp file into place.. Added in https://github.com/mozilla/MozDef/pull/433/commits/53828fafd8b1a338eefb1a193a204b246af1eefe. The create_timestamp_from_now function is not defined anywhere. This is a duplicate function definition.. Add utctimestamp to this. We lost the indentation here, which needs to be there since we want to call 'main' only if we're running this python script.. esworker has been renamed to \"esworker_eventtask.py\", not \"esworker-eventtask.py\". Should 'syslog-worker' be changed to reflect the new 'eventtask' naming scheme?. Using the old lookup method actually prints a deprecation warning. So this is to update our use of the library to use a nondeprecated function.. This comment is no longer true. If we want to keep these commented out, can we remove them to avoid any confusion?. Same here, remove commented out code?. I think unit tests here for each 'logtype' would really help clarify the transformations that we're doing. It will also help lock in the functionality so if we ever need to go in here to modify stuff, we can ensure we aren't unexpectedly breaking anything.. If we're replacing the cp line that this variable is referenced in, then we can delete this line all together.. Do we need to make these changes to https://github.com/mozilla/MozDef/blob/master/docker/Dockerfile as well? Since currently, we're supporting both a single container as well as multiple?. Looks like a find replace error here. This is different, and worth noting. The plugins have the ability to modify a message, so we need to copy the original message so the callers of the plugins, have access to it.\nThoughts?. Plugins still have the ability to do that, there are a couple examples in the tests. This is specifically if a plugin fails, that we won't have a half modified event, and have reference to the original in an esworker for example.. @jeffbryner The sendEventToPlugins method this was written based off of, returns the modified message, so this is how callers of the plugins, can handle the message after a plugin has modified it.. @jeffbryner I originally misspoke, there isn't a test for it specifically, but subsequent plugins will receive the updated message. I'll circle back though and will add a test for it.. I decided to remove the deepcopy, so the expectation is that both the message that is passed to the function, and the value returned will be the modified message. This allows us to return None in a plugin, and that will indicate that we should drop this message.. @jeffbryner It is, the idea here was that our plugin will potentially want to run on all alerts, and that way, we can be more specific as to what fields we specifically want to match on, and throw to SSO. Ex: we want to match on a category of 'X', but not necessarily if any keys or values have 'X'.. What I was imaging was in the future, instead of having to modify the actual plugin code, we would only need to modify the configuration file in order to add more alerts that we will be sending. This way, the plugin will be more or less generic, and other folks can possibly use the plugin in their environment.\nFor the registration, I'm using the new plugin set class from https://github.com/mozilla/MozDef/pull/476  which does support both lists and strings as the value (https://github.com/mozilla/MozDef/pull/476/files#diff-028a2df2c7685ba77c2542250d06e793R72 is a specific test for it, which calls https://github.com/mozilla/MozDef/pull/476/files#diff-90debaea9cca010ebab44a2e8a9652b5 if you're curious).\n. Instead of mocking the method, we could just simply require this test to run on a system that is using UTC. This will however, fail on other systems, but maybe it gives us higher confidence that this determination of the timezone is working correctly.. I do question if there's a better way to do this. I didn't want that get_localzone() function called every time toUTC is called however...maybe a check for if it's None, set it?. Don't you want to assert result == event?. Same comment, what are you asserting here?. Do we want to use actual hostnames here, or should we replace mozilla specific values with generic ones?. Leftover comments. Does fluentd set the customendpoint to an empty string? Does it make more sense to instead of using a ' ', populate it with some value?. Is this intended to be commented out?. We should verify the summary is set correctly here. Can we delete this?. These values are the required fields that we have listed in the docs. Do we want to make 'source', 'timestamp', and details, all required fields? . While we do this logic in save_object as well, there are cases in our code base (the mq/esworker_sns_sqs.py for example), where we call the save_event with the body being a string, and not json.\nWe can either do this parsing logic in save_event, or in the Event class init function, but unfortunately I think we're going to have to have these checks in both save_object, and save_event.. While logically, one would think processid is an int, we treat it in many areas of the codebase as a string, which is why I'm using the default string here.. I wasn't 100% sure here if we wanted to set the severity to 'INFO', or a legitimate default value, or just use the 'None' placeholder here.. The original intention here was to keep this generic so other alerts could fit right in, but my thought was, we can make this more generic to fit whatever next alerts we care about.. I don't think any events will have 'None' as the value, so we should be able to search for these events. I'm fine with switching the placeholder text to be whatever, if folks think 'None' isn't unique enough to search on.. I've updated this minor change since I have a script that was comparing the summary strings from the web ui. The hope is this script will be used in the future to compare and update the message types.. @jeffbryner You recently modified this, however we aren't actually running the web ui using that version. https://github.com/mozilla/MozDef/blob/master/meteor/.meteor/release is the file that determines what version of meteor we will use (if we have a more recent version of meteor and try to build, we actually will download the version specified in that file).\nI decided to revert it back to what we're running. If we choose to update meteor, we'll have to modify that file and then test things on qa as well.. I moved this list of dependencies and stored them in https://github.com/mozilla/MozDef/pull/538/files#diff-d7366c300fa0f1d70a1278125050d8d7R10, so that we only need to do \"meteor npm install\". If/when we need to update the node libraries we use, we'll only need to modify the file, and not change any of the deployment.. We also need css for ERROR. I also modified the summary so that we stuff the IP address at the end of the message. This resolves https://github.com/mozilla/MozDef/issues/536. In order to update, we'd have to update node as well (in production too). Since this task requires a bit more qa'ing and hand holding for prod, I think it would be best to have it in a separate PR.. That config option isn't used anywhere in the file. You'll need an else statement here.\nOr 'else' we'll be creating the alerts index, then will be trying to create the same index.. You'll also need to modify https://github.com/mozilla/MozDef/blob/master/tests/lib/test_elasticsearch_client.py#L444 to use the 'index_config' parameter name.. You'll want to swap these lines with each other https://github.com/mozilla/MozDef/blob/72bd27d6bae4a585d65e0ffd0de17f7679b424d3/cron/rotateIndexes.py#L82-L83 and fix the indentation so they are both lined up.\nYou'll also want to swap https://github.com/mozilla/MozDef/blob/72bd27d6bae4a585d65e0ffd0de17f7679b424d3/cron/rotateIndexes.py#L85-L86 as well. TL;DR: log before we perform the action, so if the action goes wrong, the log statement will be accurate.. We need to modify alerts/ssh_password_auth_violation.py to remove _type as well.. Since we replaced '_type' with category in the alert, we should have a negative test with a bad category.. Do we need to remove lines like https://github.com/mozilla/MozDef/blob/master/tests/alerts/test_geomodel.py#L14  from all of our alert unit tests?. I wasn't sure where to put it, but looks like we have a couple alerts that still use the _type field:\n$ grep -r '_type' ./alerts/ (with some filtering)\n./alerts//amo_failed_logins.py:            TermMatch('_type', 'addons'),\n./alerts//cloudtrail_logging_disabled.py:            TermMatch('_type', 'cloudtrail'),\n./alerts//ssh_password_auth_violation.py:            TermMatch('_type', 'event'),\n./alerts//ssh_ioc.py:            TermMatch('_type', 'event'),\n./alerts//host_scanner_alerts.py:            TermMatch('_type', 'cef'), \nI'm also not sure what to do about these entries, thoughts?\n./alerts//lib/alerttask.py:                'documenttype': e['_type'],\n./alerts//lib/alerttask.py:                    'type': alertResultES['_type'],\n./alerts//lib/alerttask.py:                self.es.save_event(index=event['_index'], doc_type=event['_type'], body=event['_source'], doc_id=event['_id']). In order to get our aggregation tests working properly, we had to add the fielddata to true. Thoughts?. Good point, fixed in https://github.com/mozilla/MozDef/pull/575/commits/167a3d6374b779cb9d33a60f1dc3c6f80faa61e6. @jeffbryner We're aggregating based on summary in a lot of our generic alerts.. Ahh, you're right, we don't do any ES aggregation in the alerts, I forgot that the aggregation in the alerts is python based. I think I can fix that test to use another keyname and will remove the fielddata as well from the mapping.. We no longer keep contributor lists in each file, so you'll want to remove lines 5 - 9. Do we really want to match on *? (Meaning, every event?). I'd disagree, I think the list of contributors by commit is sufficient here, as it stays updated automatically.. I think this statement should be:\nif ('program' in message['details'] and message['details']['program'] == 'sshd') or message['processname'] == 'sshd':\n. We should have a test for a session closed example. This change was merged into master, so I cherry-picked it here since we're technically dependent on it.. Same with this, this change was merged into master, so I cherry-picked it here since we're technically dependent on it.. I took the full default config file, and did our custom settings at the very end, so next time if we need to update, we know exactly what we changed (the stuff in the Mozdef Custom Settings) at the end.. This file is specific for the docker container where everything runs in one container, the file you pointed to is used for docker with multiple containers.. Sense is no longer present in kibana, they removed the plugin entirely. They offer the same functionality built into kibana.. For our specific config, I believe this is the case. Since this is the default, I used that value here.. Now instead of having multiple if statements when searching for 'details.', we can use this utility function to make things cleaner.. It's preferred to use our elasticsearch client to get this list https://github.com/mozilla/MozDef/blob/master/lib/elasticsearch_client.py#L48. Need to remove this.. This library isn't used so we can remove it. This library isn't used so we can remove it. This library isn't used so we can remove it. We also have a helper to create loggers, so you can simply \"from utilities.logger import logger\" and you no longer need this line, and the loggerTimeStamp and initLogger functions defined in this file.. Does this mean we're switching from blocking folks based on /24 and now a /32? If so, we should update the comment on line 157 \"limit attackers to /24\". Weird indentation. Weird indentation. These changes in this file aren't necessary, we overwrite this file as part of the docker containers (https://github.com/mozilla/MozDef/blob/master/docker/compose/mozdef_meteor/files/settings.js and https://github.com/mozilla/MozDef/blob/master/docker/conf/settings.js). This indentation is not correct, line 104 - 109 should be directly under the 's' in self from line 103.. It seems weird that the ip blocklist url would live under the Alerts tab. IMO it seems like it could be a new top level tab, or some sort of \"Manage\" tab. Thoughts?. Unnecessary debug line, can we delete to clean it up?. Can we remove this line if it's not necessary?. The space between '(' and str makes this block look weird, can we delete the space, and shift the other lines over so that it's aligned?. This class is somewhat unit tested, so is there any chance we can add a few tests for this function to https://github.com/mozilla/MozDef/blob/master/tests/lib/utilities/test_dot_dict.py. We don't actually use this \"netaddr\" library in this file, can we remove it?. What about if we moved this function into lib, and then took the examples in the comments and made unit tests for them?. This line will error out, \"self\" is not defined.. Unused import. Since most of this file isn't relevant to the mozdefstate index, can we instead just represent the options we care about (replicas/shards) in a dictionary object within the cron script?\nExample:\nmapping = {\n    \"settings\": {\n        \"number_of_shards\": 1,\n         \"number_of_replicas\": 1\n    }\n}\nes.create_index(index, mapping). I don't think we want this to run indefinitely like this, maybe we should have some sort of counter to keep track of how many times we've slept for, waiting for the index to get created? And after maybe the 3rd time, the script just exists with an error message saying the index isn't created?. Very nicely done!. Understood, since the mapping should be only a couple fields, would it be better to put that dictionary inside the cron script, instead of a separate file?. We need the ability to specify an account owner, per region, per sqs queue. What we have right now, will not cover all of the queues that we care about (as we are restricting our \"search\" to only one region for one account).. I'd argue that it'd be better to modify a config file to add a new queue, instead of having to append to crontab to run another python file.. Would it be better to represent the config file in a json format? So that you can do something like:\nqueues = [\n    {\n        \"queue_name\" : \"ExampleQueue\",\n        \"access_key\": \"12345\",\n        \"secret_key\": \"abcdefg\",\n        \"region\": \"us-west1\",\n    },\n    {\n        \"queue_name\" : \"AnotherExampleQueue\",\n        \"access_key\": \"678910\",\n        \"secret_key\": \"hijklmnop\",\n        \"region\": \"us-west2\",\n    }\n]. Since most of these fields aren't used in the state index, can we remove the unnecessary ones?. The preferred way of doing this type of logic is to do:\nwhile not es.index_exists(index):\n   .... A more pythonic way to do this is:\nif es.index_exists(index):\n    .... Any chance we can remove these commented out lines of code if we aren't going to use them?. I wish bro would include the ports that were identified as being scanned in the original message so we could surface that.. This function isn't called anywhere, can we delete it?. This function isn't called anywhere, can we delete it?. This function isn't called anywhere, can we delete it?. Can we delete these commented out lines?. Just noticed this, but do we want to make this severity CRITICAL?. This line has a syntax error, there is no method called \"key()\"\nWhat you want instead, is to do something like this:\nif 'details' in message:\nThe same goes for when you verify if 'dhost' is in message['details']. This if statement works just fine:\nif 'details' in message.keys():\nThe proposed syntax works the same as above, just seems to be more pythonic. With that being said, the typo fix will be sufficient. Works for me. At this point in the docs, we don't have mozdef git repo cloned, so this command will fail.. This line isn't needed, since we're changing directory 3 lines down.. I think we still want to run uwsgi to start the rest api and loginput service.. Same here, I think we still want to run \"uwsgi --ini loginput.ini\". Indentation is off. Same here, I think tabs vs spaces issue perhaps?. Hard-coded paths here. Same here, hardcoded paths. And here.. Can we delete the commented out line here or is there some value in keeping it around? I've got commented out lines in other docker-compose files to make it easy to expose ports for example, so if it's not leftover cruft, we can keep it around.. Are these 2 commented lines still needed?. Is there any worry here that centos7 will update to python3, and thus we'd run into breaking changes? Can we make sure we install python27 via yum?\nAlso, we can delete the $PYTHON_VERSION environment variable at the top of this file.. Can we delete these commented out lines?. Same here, can we nuke these commented out lines?. Do we need to use supervisor here since we're only running one command? Can this be done all via the command statement in docker-compose?. This won't be consistent every time, the only reason we have resolver specified in prod is for auth0 integration, so I don't think we need this line?. What's the difference between this file and meteor/.meteor/versions? For example, we have 2 versions specified for blaze-html-templates.. Misspelled destination :). uwsgi and celery are part of requirements.txt. so you shouldn't need to explicitly pip install them.. This block doesn't have anything to do with Docker, does it make more sense to name it \"Manual Installation\"?. The sockets need to be world readable?. Why do we need git installed?. We already have git installed in the beginning of the file. We shouldn't need to do this, the cron container should be responsible for creating that directory I would think.. I thought we want to make these lines all under one RUN command, so that just one RUN command caches all these results (the filesize of the container is smaller as a result).. Can we point this to the mozdef repo instead of the fork?. Same here, point to the main mozilla/mozdef repo. This will throw a syntax error, we don't need the options parameter here.. We don't actually require moto anywhere, do we need that line?. Webtest and freezegun are only needed for development purposes (running tests), since this is still specified in tests/requirements_tests.txt, can we remove it from this?. Ok, since it's a unit test dependency, maybe it's better to move it to tests/requirements_tests.txt?. @gdestuynder I couldn't find a make command to build and push to docker hub, so I'm curious if I'm doing this right.\nBasically, the TL;DR is that we have a container named mozdef_mq_worker, that is a generic mq worker container, that we can use to create services such as \"mozdef_mq_eventtask\", and mozdef_mq_cloudtrail\" which uses the mozdef_mq_worker container.. If you want to strip any of the general comments, I'm all for it.. I think this needs to be a bit more strict, I believe this false positives on the following details.destionation:\nhttp://mirrors.cat.pdx.edu/centos/7.3.1611/os/x86_64/repodata/repomd.xml. This needs to get updated to be as part of some of the work week changes:\nfrom mozdef_util.query_models import SearchQuery, TermMatch, QueryStringMatch, PhraseMatch, ExistsMatch. Ahhh, no I did not.. I think this tells flake8 to only run pep8 checks on tests directory, which we want to run it on the entire repo './'. We unfortunately want to keep the sleep 90 command, basically it's a poor man's way to wait for elasticsearch service to get booted up and all ready.. This can be improved by running a command that every couple of seconds, queries ES to see if it's ready, and if it is, continue on with the python alert_worker.py -c alert_worker.conf command (similar to https://github.com/scoringengine/scoringengine/blob/master/docker/wait-for-port.sh), but that's an improvement that can be made down the line. Welp, disregard previous post, because I had typed it up before you posted about 'wait-for-it', so similar idea.. This container is dependent on the rabbitmq container/service being up, which has in the past, caused a timing related problem. I don't know if the curl command will work on port 5672 (amqp), or if we need to readd the sleep/write a script to monitor the port?. Remove commented out code. Since this alert isn't really specific to nagios, it's more of a \"alert if any host in a list of configs, connects to some other host\", should we name it more generically?. It's worth mentioning here that this file doesn't actually get used anymore in docker meteor environment (we write to the wrong location). This never resulted in errors because we have docker settings in the real settings.js file (for example, we point to the rest api container).. I think we have a couple options:\n1. Convert that file over to an environment variable file (similar to how it's done in cloudy mozdef) and then include that .env file in docker-compose.\n2) Sync entire contents of https://github.com/mozilla/MozDef/blob/master/meteor/imports/settings.js with the file in the meteor container, however, that settings file now contains meteor specific logic such as https://github.com/mozilla/MozDef/blob/master/meteor/imports/settings.js#L31-L33, so it would be preferable to decouple https://github.com/mozilla/MozDef/blob/master/meteor/imports/settings.js#L10-L18 from the settings file in meteor, so we only overwrite the settings dict and nothing else. (We have a full version of that file with all of the meteor stuff in production, so we could use this functionality regardless). I think if we remove the file, then we're now saying \"in order for docker to run, the meteor/imports/settings.js file must be using docker environment specific values\" (which is what it currently is doing unless you use environment vars like cloudy mozdef) but I think it's preferable to keep the two configurations not dependent on each other.. Agreed!. Since we're doing it in python now, would it make sense to break this out so that sourcemustnotmatch is an array of ips to exclude? or do we need the ability to do \"complex\" 'and' with 'or' statements?. To align with the other environment variables above, should we make this OPTIONS_METEOR_REMOVE_FEATURES?. One weird thing that I've noticed, is if you turn off a few features via the environment variable, and then rebuild meteor, everytime you refresh the page (or originally load it), the disabled features links originally appear, and then disappear. \nIs there a way to fix this?. Works for me!. I don't know how I feel about us modifying main config files to work in docker (there were a couple other ones recently as well).\nI think we should keep these files as they would be for a non docker environment, and then make local modifications if we need it to work in docker, or have docker overwrite these files accordingly.. There are unused imports here, for example:\nrequests\nElasticsearchInvalidIndex\nTermsMatch\nlogger\ninitLogger. You reference a config file here, but there isn't one included as part of the PR?. If we're not using the config file anywhere, nor are we using the 'ignoredusers' key from a config file, can we just remove the logic to load a config file entirely?. I think that's fine, we can cross that bridge when we get there.. I could be the minority here, but I actually prefer to have the key names contain the '_' to separate words like this, it seems more legible for me. . Sounds good. This should be alerts/files/config.py. You don't need to add this file.. We want to get away from using relative paths here, so we should instead do something like:\nos.path.join(os.path.dirname(__file__), 'github_mapping.yml')\n. Any chance we can move this into mq/lib instead?. We also have this function defined in mq/esworker_cloudtrail.py, can move that definition as well and reference the shared one?. Format is a bit off here. This comment wasn't addressed, so I'm not sure why you marked it as resolved.. We no longer need to modify the path to import mozdef_util, we can simply just do\nfrom mozdef_util.utilities.toUTC import toUTC. Works for me. I don't think these statements will ever get triggered, because by the time this plugin runs, all the keys should be lowercased.. This does not work as the index hash \"AWdcCjsPo9lh6BmjcvcD\" is specific to each kibana installation. (meaning this won't always be the hash of the index pattern name). We don't need to define this route, meteor handles the opening of the url so this is never actually called.. Is this comment still relevant?. There's some inconsistencies for this variable name. You define it here with a capital 'W', but then later on, you reference it using the lowercase version. I think we should use the lowercase variable name throughout.. This 'sendMessageToPlugins' line isn't needed, as we don't actually have a plugin that registers for \"getwatchlist\". I believe you can remove the line entirely.. Instead of using global variables here, you can use an instance variable to keep track of the watchterm. For example self.watchterm = 'someusername'. Instead of creating the logger using this, we can actually use https://github.com/mozilla/MozDef/blob/master/mozdef_util/mozdef_util/utilities/logger.py . As part of this, can we remove all of the cron/correlateUserMacAddress scripts as well as https://github.com/mozilla/MozDef/blob/master/cron/oui.txt?. I'm specifically referring to https://github.com/mozilla/MozDef/pull/906/files#diff-c9dff09105e9694576c5a91b26bcd704R549 and https://github.com/mozilla/MozDef/pull/906/files#diff-c9dff09105e9694576c5a91b26bcd704R554 (both of which are in index.py). . I added some logic to wait 3 times for the index to be ready at  https://github.com/mozilla/MozDef/pull/1042/commits/18f9b7e9e47b95ee8d718ab87f68c40a4257d4bd. The source command is no longer needed as I've added the virtualenvironment path to the base container.. Same here, the source command is no longer needed.. The correct file to modify is actually 'alerts/lib/config.py', not 'alerts/files/config.py'.. You shouldn't need to source once you're in the container anymore.. I combined the IRC and slack Readme into this bot/Readme.md in https://github.com/mozilla/MozDef/pull/1018/commits/ce377adb53addc9d902af988a8fe32851e0714c5. Fixed in https://github.com/mozilla/MozDef/pull/1018/commits/e05243983cb9167303a19e85a3c88f74da8e2612. Fixed in https://github.com/mozilla/MozDef/pull/1018/commits/ffb0a3ca5e9c0525e89a8745c064f7273a5a4036. Fixed in https://github.com/mozilla/MozDef/pull/1018/commits/458723466763d1f51f2879147d4c1c255c4ae221. Fixed in https://github.com/mozilla/MozDef/pull/1018/commits/f0f30519a71750afa8dea1db5de8f67f622be039. Fixed in https://github.com/mozilla/MozDef/pull/1018/commits/458723466763d1f51f2879147d4c1c255c4ae221. Fixed in https://github.com/mozilla/MozDef/pull/1018/commits/458723466763d1f51f2879147d4c1c255c4ae221. This was a copy/pasta from the original IRC mozdefbot, so could you explain this a little more of what you're asking please? I'm honestly not entirely sure what @wraps is doing here.. Fixed in https://github.com/mozilla/MozDef/pull/1018/commits/46b9e7e0650f7032997e07369c724098375fa02a. This is actually how it's currently setup, we tokenize the entire slack message into a command + parameters (https://github.com/mozilla/MozDef/pull/1018/files#diff-46fed7650379c8a8432b6efca3b11a91R53 and https://github.com/mozilla/MozDef/pull/1018/files#diff-46fed7650379c8a8432b6efca3b11a91R54). We then identify which plugin (if any) is associated with the command, and simply pass the plugin the parameters (https://github.com/mozilla/MozDef/pull/1018/files#diff-46fed7650379c8a8432b6efca3b11a91R69).\nIs this the functionality you were requesting, or am I misunderstanding the comment?. The post_attachment function is more of an internal function (which I suppose I can rename with an underscore to make that more obvious. The callers of this function (post_info_message, post_welcome_message etc at https://github.com/mozilla/MozDef/pull/1018/files#diff-46fed7650379c8a8432b6efca3b11a91R145) set a default value of None here, so we're doing this \"up the chain\".. I renamed the post_attachment function to match the private function naming convention at https://github.com/mozilla/MozDef/pull/1018/commits/6d20570292dfebcfbb18f16ae9f98146f36d6be3. Implemented in https://github.com/mozilla/MozDef/pull/1018/commits/30346b03e5adc484378fe4953bb59b8ca8bf3cb8. I moved this into mozdef_util at https://github.com/mozilla/MozDef/pull/1018/commits/5f67c2d9eb99ab1a74e8420b6eb08334e9cbf241 . The build will fail because we had to bump the version of mozdef-util in requirements.txt, but I haven't built and pushed a new version of mozdef-util yet, so if you could take a look at the previously mentioned changes and give a thumbs up or down, and I'll then push a new version of mozdef-util to pypi, which should cause the build to then start passing.. Oh, upon second thought, do you mean to have each plugin be the one to tokenize the parameters? So if the slack message is \"!ipinfo 1.2.3.4 5.6.7.8\", the parameter we would pass to the plugin would be \"1.2.3.4 5.6.7.8\" (instead of ['1.2.3.4', '5.6.7.8'] with how it's currently implemented)?. I ended up removing the run_async file and implemented it like you recommended.. Instead of copy/pasting the entire docker-compose.yml file, we can use \"inheritance\" (docker overrides) here, so that we only have to redefine the env_file location. This could look like:\n```\n\nversion: '3.7'\nservices:\n  nginx:\n    env_file:\n      - $ENV_FILE\n  mongodb:\n    env_file:\n      - $ENV_FILE\n  kibana:\n    env_file:\n      - $ENV_FILE\n  elasticsearch:\n    env_file:\n      - $ENV_FILE\netc\n``` . I'm not a big fan of writing a file to location within the git repo, so could we change this so the environment file would get referenced directly? This way, we could store the environment file in a location outside of the mozdef project directory. It could look something like (with the addition of using a docker override):\n.PHONY: run-env-mozdef\nrun-env-mozdef: ## Run the MozDef containers with a user specified env file (relative or absolute paths). Run with make 'run-env-mozdef -e ENV=my.env'\n    ENV_FILE=$(abspath $(ENV)) docker-compose -f docker/compose/docker-compose.yml -f docker/compose/docker-compose-user-env.yml -p $(NAME) up -d. Yeah, it apparently can handle global variables like that, I was hoping we'd be able to specify the .env file as part of the docker-compose up command so that we wouldn't need to have a docker/compose/docker-compose-user-env.yml file and could bake it into all of the other make commands , but I don't think we're able to, so I think this is the next best option.. You can also remove the volumes and networks blocks in this file as well, since those are already handled by the \"parent\" docker-compose.yml file. The docker-compose pull I don't think is needed, as it can be controlled via https://github.com/mozilla/MozDef/blob/master/Makefile#L11\nSo I think you can run something like make -e BUILD_MODE=pull build run-env-mozdef. Do you have a specific version that we can pin to?. Remove debugging statement. Could we customize the assert error message here, just so when this assert fails, it's a bit verbose as to what this is actually checking? You can do something similar to https://github.com/mozilla/MozDef/blob/master/tests/alerts/alert_test_suite.py#L190 , where you specify , \"A custom error message\". Oh, right, yes. I think in what I was testing with, I had the ENV_FILE setting on the same line as the docker-compose line, so that docker-compose would run with that environment variable set, this also works though!. I think this change actually makes it more difficult to read. Since I'm assuming these changes were automatically done via some tool, can you configure that tool to ignore line length edits like these in the future please?. We can use the mozdef_util version of logger (https://github.com/mozilla/MozDef/blob/master/mozdef_util/mozdef_util/utilities/logger.py)\nlike from mozdef_util.utilities.logger import logger. We can remove all of this logic (line 30 - 35) when we import the logger.. Do we still need this line?. This comment isn't correct, because each index is pulled from elasticsearch, not a .conf file.. We can actually do if 'events' in index which I think is a bit more clear as to what we're trying to accomplish here.. Instead of printing, can we use the logger (logger.debug('sometext'))?. We don't have to cast this to a string, as it's already one when it's initially set.. Same comment as previous, we can do if odate in index. I think we want to move this try except block to where it's only around the specific logic to close the index. Right now, if we receive an ES connection error, we'll get an exception like Unhandled exception while closing '', terminating\n. Instead of creating this index_to_close variable, can we just use the index variable instead? This way, we also don't have to do index_to_close = '' earlier up the file.. Can we use the logger here?. I don't like how we're modifying the options.index_age config value here. I think this is due to the comparison logic to determine if an index is old enough to be closed.\nI think if we instead took the the index name from ES, converted that string events-201904 to a date, and then compared that date to date(options.index_age) and if it was older, we call the close index function.. This is a syntax error, you need a semicolon at the end of the line.. Syntax error, missing a trailing semi colon. . I think these tests can be improved to verify more of the open/close logic. All we are doing here is checking that we can run 2 functions, and then that the second function, returns True.\nFor example, what if we closed the events index, and then tried to save an event. When you try writing to a closed index, elasticsearch throws an exception, so we can tell pytest to expect an exception like https://github.com/mozilla/MozDef/blob/master/tests/mozdef_util/query_models/test_search_query.py#L291. We can improve these tests by closing an index, opening the index, and verifying that we can write an event.. What I'm saying is, you aren't actually testing the functionality that an index does get closed, you're testing that self.es_client.index_close('test_index') returns a True value.\nFor example, if this is what the implementation looks like of index_close, your tests will pass, which is not what we want:\ndef index_close(index_name):\n    return True\nSo, to improve these tests, we want to have the tests verify that self.es_client.index_close('test_index') actually closes an index (that that function is doing what we want it to do). \nOne way to verify if an index actually gets closed, is to try and write to it and expecting an elasticsearch closed index write error to get thrown.\nAnother idea might be to query elasticsearch and have them tell us if the test_index is now closed.. Do we have some sample events that match these regex, so we can add some additional tests to https://github.com/mozilla/MozDef/blob/master/tests/mq/plugins/test_parse_sshd.py? . ",
    "jstevensen": "Looks good.\n. ",
    "2xyo": "Why not use logstash for retrieving logs? \nThis solution has the advantage of offering many formats as input and it's easy to check if logs are compliant (with GROK). \n. Ok, perfect! Thank you for the clarification and for making me discover heka :)\n. Oups, I didn't see the PR #121 ... \n. ",
    "mpurzynski": "We should also monitor rabbitMQ queues and ES.\nMessages going in but not out? Alarm.\nMessages not going in? Alarm.\nES not receiving messages of type \"X\"? RabbitMQ may have died. Alarm.\n. I bet that the CIF framework can do all the parsing and automatic update for you. It already has all the parts and can output in multiple formats.\n. Cross correlation between multiple alert types would be nice to have. Let's say:\n1. Bro notices an Intel match for a suspicious DNS domain name being resolved\n2. The same host that just send the DNS query and got a response connected to the IP\n3. Alert is created.\n. Yeah a state machine isn\u2019t needed - the less state we keep the better for us, something that Bro teaches you in no time.\nCross correlation is what we want here, I won\u2019t be playing smart about how to do it.\nOn Jul 21, 2014, at 11:48 PM, jeffbryner notifications@github.com wrote:\n\nNothing stopping this from happening within multiple ES queries, not sure a state machine is needed? Such a machine would need to witness and track all events, but an es query can drill down without needing to track state?\n\u2014\nReply to this email directly or view it on GitHub.\n. r+\n. r+\n. 61 unit tests added.. Interesting idea, please open me a bug or a Trello card because changing the type field requires changes to fluentd configuration on 11 clusters (ansible++), changes to the BroFixup.py code and re-writing all the events that are already in MozDef and came from Bro.. Nope, this is the type of a Bro log - conn, ssl, x509, smb_files, notice (bro notice) and so on - over 20 of them.. https://github.com/mozilla/MozDef/blob/master/mq/plugins/broFixup.py <-- type is here, so we will need to change that plugin, unit tests, ansible fluentd configuration and that leaves us with inconsistent state of Bro events i.e. using the old field name. Will reopen with a new design, some heavy changes in the syslog-ng configuration file and Bro's parsing need more broFixup.py updates. Are you referring to some specific worker, like the SQS/SNS or the idea of having the \"source\" field?\n\nIt is not true that the source field is not consumed, it is. The steps to reproduce is something that some of our workers do around 2500 times per second.\nA quick summary of values of the source field shows me 24 different values and, among them some unknowns.\nThe source field is a crucial component of our Zeek, Suricata and possibly other data feeds. It tells you, what is the log file that data came from. There are examples of how to understand it (we do not do it for syslog, maybe we should).\nRather than your proposal of throwing the baby out with the bathwater, how about we fix workers that should be setting it?. Furthermore, the Github plugin also uses the source field, again (and it's the case with Zeek and Suricata as well) - according to the original idea\n13 different values in the past 7 days for GitHub, so a total of over 37 different values across all of our data sources.\nI would say, it's rather used and used heavily.. I did not, waiting for a review first.\nOn Tue, Aug 28, 2018 at 11:39 AM, A Smith notifications@github.com wrote:\n\n@mpurzynski https://github.com/mpurzynski Just a quick question before\nmerging, did you stage the removal of the original port scan alert?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/mozilla/MozDef/pull/739#issuecomment-416696160, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACZ4ZkBfqqizfHoJ3V9ZD313z15oI8jNks5uVY5_gaJpZM4WIhD_\n.\n. Changes made, code pushed, thanks for the review!. If the sourceipaddress field is present together with the cluster_client_ip field then the later should be used and the former overwritten. Please note this is very different than the x-forwarded-for header, that can easily be spoofed.. This requires alert plugins to be read-write, something I'm working on right now (literally). This alert plugin would query infoblox and maybe some other databases.. Per docker best practices I ran it 10 times in a loop and it worked.. Oops, forgot to push the file with unit tests. You're welcome. 60 LoC, 3000 lines of tests.. I refuse to spend several days \"fixing\" unit test so intentions are what pep wants them to be. Checks like this on 3000 lines of a unit test full of JSON serve no purpose.. Per request I moved the mapping file to the plugin sub-directory.. Ansible changes merged. @Phrozyn thanks, good catch. An example configuration file added.. Thanks for reporting a very interesting software design problem. I'd like to clarify some claims here, as the description is not very accurate.\nfor any required fields that are missing from the event, set them to the value \"UNKNOWN\" or default values like datetime.now() or the current hostname\n\nThe UNKNOWN value is set by either plugins or the generic code to help with data sources like cloudtrail, that do not follow any logic, pattern or definition and have fields that either show-up or vanish. Unfortunately, for some data sources (like cloudtrail) a field a.b can be there on day 1 and be gone (for the same event) on day 2 and appear again, as a differrent type, on day 3. And then go away completely.\nThis creates problems with \"vanishing fields\" during the IR or alerts writing.\nThe current hostname is set for the mozdefhostname field, according to the standard.\nThe datetime.now() is stored in the receivedtimestamp field.\n\nEstablish 3 workers, sqs, papertrail and amqp(eventtask) which exclusively fetch inbound events but do no transformations\n\nThere is no way 3 workers can handle the load of events that we have. Did you mean three worker types or classes?\n\nThis would replace the current plugin based categorization process which is fuzzy and allows a message to trigger multiple plugins\n\nThe current process is fuzzy on purpose and it is by design that a single message can be acted upon by multiple plugins.\nTo show why this is useful, let me give you two examples\n\nThe ipfixup.py plugin seeks for anything that might be an IP address, in multiple places, and when it finds it, does some standarization and moves data around (like moving the IP from the cluster_client_ip to the sourceipaddress field, among others)\nthe geoip plugin acts on data produced by the ipfixup plugin and attaches intels about geolocation\nThe benefit here is that if a source ABC starts sending an IP address, or moves it around, we get the Geo intel \"for free\" - without constant monitoring of every data source and missing crucial data in the time period between the source change and us pushing the fix to production.\n\nI will not comment on the design proposed, but will ask a different question instead - why?\nThe design proposed means basically rewriting most of MozDef's code from scratch. That takes people and time away from real-world problems and it creates a project that might be interesting from the computer-science studies point of view, without benefiting us much.\nI do not see the value proposition here.\n. Let's do the following sequence, which makes replacing the database atomic.\nsave the new db into a .tmp file\nmv new old\nThis way it's atomic (on XFS at least) and even if the server crashes hard in a middle of a write, the old DB will be accessible after a reboot.. Changed to 'nsm'. Removed. Changed to compare event on input with event on the output. Should be the same, those tests are to make sure non-bro messages do not match.. Changed to compare event on input with event on the output. Should be the same, those tests are to make sure non-bro messages do not match.. Changed.. Removed.. It can populate with customendpoint = bro. Not really, fixed. Deleted. Done. timestamp - is it always a copy of the UTC timestamp, or it can be anything that the source sets? Could it be a local (for the source) time in a local timezone?\nsource - if we can come up with some smart heuristics\ndetails - source creates it, otherwise it's not present. I'm not sure we want to have it set to null, for example, because it's kind of like a pointer to a JSON dict. Let's keep it as string. UNDEFINED so we can query it later?. Done, about to push that. This is an interesting challenge. We could sample ports or maybe even print the most popular ones? I need to do some further research and reach out to @JustinAzoff. Q1 - I need to rewrite Bro's port scanning detection for this.. We do need this capability - it's just easier to express subnets this way. I mean, I know by heart what 10.1.2.3.5/13 range is, if we want to go this way.\nWhat we need here is\n1. a way to express a long list of whitelisted IP addresses - /32\n2. a way to express arbitrary subnets\nIf that can be done cleanly with Python, I'm happy to write it.. Done. Done. We found out that isinstance() is painfully slow and been trying to migrate away from it, as far as I know.. Actually, this block of code will never be reached if the customendpoint does not exist, see line 67. https://github.com/ambv/black is what I started using for everything. Do you want me to stop splitting lines and rather keep them long? I guess I can configure that https://black.readthedocs.io/en/stable/the_black_code_style.html#how-black-wraps-lines. done. ",
    "Phrozyn": "This is part of an effort we have regarding streamlining mozdef.\nI don't have details just yet as we are working on designing this.\n. I think I just reproduced this. It does change tab, but I can see the old tab's fields at the bottom of the new tab window, I got this to happen with 3 different tabs, ending with the last window having 2 other tab's fields stacked at the bottom.. \n. At this time the only mozdef files with this included are as follows:\n./cron/backupSnapshot.py\n./cron/compromisedCreds2fxa.py\n./cron/backupDiscover.py\n./cron/google2mozdef.py\n./examples/demo/sampleData2MozDef.py\n. hi yashmehrotra, \nYou are absolutely correct. I made a mistake on installation of mongo which is why it didn't start. \nI just caught it after testing out what you were saying.\nThanks! This will be closed.\n. They are all the same to me.  Except that using different terminology\nindicates you are talking about a different metric when discussing them.\nIt's all apples to oranges though.  One company used p1 for high priority\nitems while another used p3 for high,  p0 being the lowest.\nSame thing for severity imho.\nIt's relative depending on who is consuming the informative.\nI'm sure verizon has coding around their framework as many other\norganizations using it do. You could be one voice vs. many in that\nargument.\nJust my two cents.\nAlicia\nOn Mar 8, 2016 4:43 PM, \"Jeff Bryner\" notifications@github.com wrote:\n\nHrm.. good point.\nThose (currently) come from Veris/Verizon:\nhttps://github.com/vz-risk/veris/blob/master/verisc-labels.json\nShould we alter ours on import? Convince them to change theirs? Alternate?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla/MozDef/issues/335#issuecomment-194040242.\n. Hi Guillaume!\n\nThanks for this,  being mainly operational and coming from a place where\ndev time is hard to come by,  this really helps me understand it.\nDoing conversions just makes sense.\nI've never really encountered anyone having issues with levels,  That is\nwhy I responded in the fashion I did.\nCarry on!\nAlicia\nOn Mar 10, 2016 3:06 PM, \"Guillaume Destuynder\" notifications@github.com\nwrote:\n\n@Phrozyn https://github.com/Phrozyn That's generally exactly one of the\nproblems in the security community. See openssl advisories for example.\nNobody knows what high or critical means to them and there's chatter about\nit every time.\nThat's why we use standardized levels with a complete description of each.\nOf course best would be if everyone standardized on something - but at\nleast it helps within our own projects.\nIn this case it sounds like it would be interesting to get Verizon's take\non it, regardless of the outcome.\nNote that in code, we do conversions anyway, all the time - no choice :)\nEx: vuln2bugs normalizes CVSS and some other stuff into the Mozilla\nstandard levels (surprisingly easier to communicate than CVSS also ;-)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla/MozDef/issues/335#issuecomment-195091237.\n. Corrected from \"investigation Summary\"\nto \n\"Investigation Summary\"\n. Issue closed.. This should close issue #349\n. @ajomadlabs  Of course you can!\n\nI'll work up some screenshots to post to this thread and explain later this evening.. @ajomadlabs \nThis is what the page looks like as far as layout now. This can fill up over the course of a year, and if you ever wanted to go back and reference an investigation or incident, there's no way to actively search for it. \n\nThe code for MozDef UI lies in the meteor directory here:\nhttps://github.com/mozilla/MozDef/tree/master/meteor\nThe investigations and incidents code is here:\nhttps://github.com/mozilla/MozDef/blob/master/meteor/app/client/investigations.js\nhttps://github.com/mozilla/MozDef/blob/master/meteor/app/client/investigationTable.html\nhttps://github.com/mozilla/MozDef/blob/master/meteor/app/client/incidents.js\nhttps://github.com/mozilla/MozDef/blob/master/meteor/app/client/incidentTable.html\nUltimately, we'd like a search feature that will allow us to:\n\nsearch for events based on a word or IP stored in the event (summary, or date stored in any tab within the event)\nfind events entered by a specific user\nfind events between certain dates.\nfind events that match a specific phase/status (closed, holding, identification, escalation, evidence)\n\nLet me know if this information helps, or if you need more!. @ajomadlabs Have you forked the repo?\nWe've assigned you as a collaborator.\nYou'd have to accept the collaboration request before we can assign I think.. I've assigned you to the issue!\nThanks for the help! Let us know if you have any further questions :). +r - requested testing on mozdefqa1 before committing.\n. Notes taken from an offline discussion:\n- We should be deliberate about the block time value\n- Distinguish first offense/second offense?\n- Possible signals: if the IP is from AWS => shorter block\n- Incorporate into roadmap for MozDef\n. I had to raise the number of IPs blacklisted in MozDef due to going over 2000, then over 3000.\nWe should implement this for now, maybe change it to 3 months of aging instead of 7 days, until we have more time to really make something beautiful out of this.\n. I also noticed supervisord.alerts.conf is not included in the public repo. I will add this as it doesn't contain any sensitive data.\n. fixes #359\n. closes #359 \n. I can't answer that, I do know there are some timestamps with a date and time with the offset added at the end, I don't know if this is transforming those dates or if those are somehow transformed by the shipper or something else or if they are kept as a separate field.\n. acknowledged, @jeffbryner \nI'll work on this.\n. Once the above is done, a single systemd script can be used to start mozdef uwsgi processes.\nHowever, we can still have single scripts to stop and start each one in order to be able to reload after config changes.\nRerference for doing this is here: \nhttps://www.reddit.com/r/systemd/comments/3spd5k/start_multiple_instances_with_one_service_file/\n. I've added a single startup script for the mozdefalertplugins for now with commit 8781118\nThis is so that mozdefalertplugins can be started until this \"master init\" is completed.\n. all uwsgi services now have a startup script. This was resolved with merge #389\nI took a slightly different route and may change this in the future.\nAt this time every uwsgi worker can be started/stopped/restarted individually\nAll of the mq workers can be started/stopped/restarted by issuing systemctl start/stop/restart mozdefmqw*\nmozdefweb, alertplugins, loginput, and restapi are stopped/started/restarted by \n\nsystemctl start/stop/restart mozdefloginput\nsystemctl start/stop/restart mozdefrestapi\nsystemctl start/stop/restart mozdefweb\nsystemctl start/stop/restart mozdefalertplugins\n. Going to re-request after I back out that Plugin registration change as that is going to be revised.\n. I'm sorry I didn't get to this in time! Looks good though!\n. Hello FengJie!\nLet us know if there are any other questions we can help with!\nI wanted to add that there are init scripts located in the initscript directory of the repo. \nYou can put these in your /etc/init.d/ directory and use them to start mozdef.\n. @Lfengjie \nWe found the issue in the code and have pushed patch #378 to resolve this issue\nThank you for bringing this to our attention!\n\nSome major revisions will be forthcoming in the coming months, stay tuned!\n. closes #377 \n. Thanks for the commit! \nI'll test these changes out and commit once we've done that.\n. Hello FengJie, \nI'm currently testing the docker version of MozDef and will  update you as soon as I have concluded my investigation.\nI have been able to duplicate the issue you mentioned.\nCan you tell me what the specs are for the docker system you are running it on?\n. Hello! \nThank you for the update, I'll consider this issue closed then.\nThank you!\n. mongod conf is in original commit!. closes #366. fixes issue #336. Ready for Review. Closing as this was for the older release which is deprecated.. Thanks for the PR! . This was taken care of in PR #576\n. Tooltips were added to the incident and investigative timeline fields in this PR #572  that has been merged.. I would agree with you, but this won't get set with any priority, removing them doesn't add any value aside from a few bytes in decreased filesize.\nPersonally, I find it interesting to see how many people have touched/modified a particular script when I open it to do the same.\nAt this point I don't think it serves really any other purpose than to give credit. I'll let others weigh in.. @claudijd   Thanks for the offer, I'll bring it up in our mozdef weekly meeting and see how the team feels, and I'll update you.. I'd like to drop the old mozilla logo in the grey tab on the upper right, and add this instead:\nhttps://www.mozilla.org/media/img/mozorg/mozilla-256.4720741d4108.jpg\nThough we can keep the grey tab (I can photoshop this into a vector graphic)) and just add the new mozilla logo there.\nDue to the rebranding of projects, we really can't have a mozdef logo, everything is supposed to conform to a particular design as defined here: https://mozilla.ninja/logosystem\nThere is a favicon example as well which is just the M as @jeffbryner pointed out earlier.. I will rework this to alleviate conflicts, closing. hi @bsrdjan \nThat could get tricky imho. Mainly because a user may load a site with bad code that looks for the api availability and exploits it in some way. I don't think it's a horrible idea, but I think it would need much further analysis to determine the possible issues it could cause and whether the value add would be worth it.. Closing this PR, replaced by #475. Nevermind, this is not working the way it should. Closing.. Merge only AFTER the ansible-eis changes are merged in PR 341. Handled in PR #495 closing this one. This is waiting on further discussion per @pwnbus . In Alerts dir. Closing PR, seems bugged - there is no esworker.cloudtrail.py in the changes I'm merging.. I'm going to close this PR, as I think this can be handled with what we have, just using a different call to the ES client than what we are using now.\nOr we'll just use curator and not need any modifications to this at all.. This is not complete yet due to unit test issues, please do not review or merge yet.. Closing in favor of #519 . Won't merge till @jeffbryner has a chance to review.. I approve! \nI think we can likely improve on this in the future using the generic idea in an abstracted framework possibly. We can talk more on that another time.. Please add the ansible-eis PR when you get a chance before merging this!. Added in #558 . Removing this PR, going to combine it with 547. Closing as this pulled in an additional change  that should not go with the nginx redirect. Will resubmit.. Not Ready Yet, this seems to have broken the dashboard link - will revise. Adding to 547. Closes #542 . I feel like the cloudtrail commit will need reworked due to the many items that it currently isn't handling which is causing the field type exceptions. Am I misunderstanding?. Just curious why we are changing this to debug, I've not heard anything in regards to there being a problem with the output we have now.\n. Makes sense! +R. solves Issue #396 . Many thanks for the improvements! Looks great :). Something just got really messed up in the PR lol - going to close and resubmit.. I'm not positive this should be merged and pushed yet until we actually split the indexes, as _type is relevant up to the point we split the indexes, removing it prematurely will cause our data to be indexed differently, and any dashboards /saved searches currently in use with _type defined will break.\nRemoving _type from search params is ok though, that won't break anything.\n. That link doesn't mention doc_values, I think our inverted indexes are going to have sparsity as it is mainly because we are \"indexing\" all fields. I don't think doc_values really has any bearing on that other than the \"metadata\" it's keeping for scoring/aggregation/sorting\nAnyhow, since there are supposedly improvements in the next version, let's just get off 2.x with what we have (minimal changes) and see where that puts us. If we find we have problems we can look at refining how we classify and assign these features to our data before moving to 6.x. This will not cause a significant increase in alerts as originally thought. \n. commit #666 fixes this\nMore work will need to be done in the area of keyboard/interactive due to the way mfa logs those events when authing.. I'm not sure how this alert ever got added since it's failing in an unrelated area to the change.. ahh makes sense, when I looked at the output, it looked like it was failing for something completely different.. I know I responded to this in email, but got to thinking about it later on. The reason the  clearing of cache didn't \"help\" when I ran it was due to not many people running complex searches. However, as Kibana is used more and more for complex searches clearing the cache could definitely help during those cycles. . This is to remove non-security event data from the events index and reduces the mapping requirement for the index by a tiny margin.\nUpon daily events index creation, this data is no longer dynamically created as it is statically mapped into it's own index.\nThis also extends the ability to add further metrics and time series the data without creating overhead by having to search through a large data set like events.. This is not an effort to save space, I apologize if that is how my response was interpreted.\nI was speaking more to the advantage of having health data in it's own index for review of trending without creating overhead from having to search a larger data set for it.  This is to help with any troubleshooting  of health issues and provide trending so we can optimally plan for things before the problems present themselves. \nThis is specifically for historical data on the health of the cluster and letting events be used for security work, while health data is used for ops work, and doesn't touch security event data.\nAlso to address the other assumption of splitting events up into further tiny indexes, that's not something we are planning on doing, although there is an argument for moving nsm data into it's own index (namely shard size so we  can prevent from having to add more servers to accommodate larger shards)\nIn addition to the above, the addition of the index_exists to the elasticsearch_client can optimize some of the other crons we have where we look for specific indexes upon rotation by eliminating the return of a large dataset, we can pick a specific index.\n. Yeah I plan on tackling that next, it'd be nice to see the trending for everything :)\nThanks, Jeff! Sorry my initial post wasn't more informative.. @pwnbus can you validate the changes and remove the request changes flag if it looks good?\nThis must be merged before the sqs stats PR.. Closing this PR since it's more efficient to just open a new one from a new branch. I can't seem to locate the commit that added all the merged commits into this PR.. @jeffbryner FYI, there are no alerts that actually use this field, they all use sourceipaddress or destinationipaddress. As mentioned in my OP ES now supports both versions of IP addresses (link for reference: https://www.elastic.co/blog/indexing-ipv6-addresses-in-elasticsearch), thus the issue seen previously should no longer be a problem. We can handle what version an IP is within the code without having to explode our mapping. . I completely agree on that point. I'll add further test cases to the OP.. I do understand the concern, and do plan on ensuring everything is fully tested and scoped before any of this is implemented. I was using vscode to search for occurrances of \"ipv\" in our repo.\nSome items that use functions for ipv4 use the ipaddress field to validate what type of IP it is, the code that did transform and populate the ipv4/ipv6 address fields was changed in my PR and tested. However, I have much more testing to do to ensure there aren't weird anomalies presented as a result of this. For example I have not looked through the cron scripts I mentioned in my OP as things to review.\n. Going to close this PR for now, until Kibana adds a method for Aggregating on ipv6 IPs (they only aggregate on IPv4) we can't remove the text fields as it will cause some problems.. @mpurzynski Just a quick question before merging, did you stage the removal of the original port scan alert?. Initial commit is for mq plugins that define details.hostname. Validated that all events that trigger these alerts have hostname populated, while not all of them have details.hostname populated\n. No further edits needed. . I kind of like the idea of two separate alerts, but the proxy doesn't support non-std ports (outside of 80/443) at all. This could be quite noisy as individuals set up systems that are configured to use the proxy might have things that need to have flows opened and the proxy bypassed. However, it can also alert us to nefarious programs trying to phone home. It's a double-edged sword. I definitely think this would be one of those alerts MOC should handle as a first response. \nThat said, it would be nice to have a runbook set up and ready for them once we see what the results of this are.. Just a note as I stated in PR 748 that this can result in high false positives due to the nature of the proxy. Maybe to start we put this in INFO level and then raise if we find that it's not as FP heavy as I suspect it might be.. I agree with @pwnbus' comment above, I think we should dig into this a little further and determine whether further normalization is needed for the data we are trying to overwrite here.. Still need to add the supervisord instructions to this before I can call it done.. Closing this as PR #758 already  covered these changes.. Closing, merging master into my branch didn't work as expected. WIll open a new one.. This is odd that this was in /lib, it was always in meteor under meteor/lib/\nGood catch!. hi @darakian,\nI wanted to let you know you can also use syslog-ng to format events into json format and post to the rabbitmq  eventtask queue, or send to loginput - either way.\nIf you would like more information on either of these options I would be happy to provide some information.. That would make it easy but difficult to turn off a particular alert\nwithout having to manually remove it from the directory. That's not really\nideal.\nOn Fri, Nov 30, 2018, 11:57 AM Jeff Bryner notifications@github.com wrote:\n\nI wonder if we need a schedule stanza at all? What if we auto scheduled\nany .py file that lives in the /alerts directory to run? Mostly they all\nrun every minute currently (since they take<1sec 99% of the time).\nThis would make it easy to deploy an alert.. simply copy the file into a\ndirectory.\nThoughts?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/mozilla/MozDef/issues/982#issuecomment-443286046, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AEEf0H6aztZ_gmU_P1fI_WYe2K-e2eTrks5u0XFtgaJpZM4Y2Zbs\n.\n. What does this do?\n\nAlso you have some conflicts to work out. Closing, not sure why it is saying there's a merge conflict :(. > Why remove?\nIt doesn't work. Brandon and I discussed and decided to remove for now until the trello task to make it work gets assigned.\n. Current: (I just noticed the doc links are missing here)\n\nAfter this commit: (we can change the colors to whatever, this is just an idea) The dc chart has a css file we can modify if we want to change the ring and chart colors.\n\n. This is awesome!. Fails on: No package rabbitmq-server-3.7.11 available. This would need to be pulled from rabbitmq specific repos and not EPEL.. > Can I ask why this instead of removing _type? (since as I understand it, it will always be static)? does type==category?\nSince we have different _type's of alerts, this will now be cast into \"type\" rather than \"_type\" since \"_type\" will become _doc and is no longer going to be used for filtering. We felt if we preserved the _type as type we'd still be able to use it for filtering. It's a sub-category essentially.. > _type currently for alerts is always 'alert' and I don't see a place where that's different?\n\nMozDef/alerts/lib/alerttask.py\nLine 377 in 4190c8d\n def createAlertDict(self, summary, category, tags, events, severity='NOTICE', url=None, ircchannel=None):\n\nYou're right, I just looked again (earlier I had seen auditd and alert) and there is only one _type.\nI guess my session had timed out and it mixed results in a weird way. I'll modify.. This documentation is expressly for centos :)\nAnyone running a different distro should be familiar with their startup commands, the debian flavored installation documentation is being written, but hasn't been committed yet.\n. Also, aside from that - that's not what was changing in that commit. Line 597 corresponds with line 580 (prior to modification)\nAlso those lines are added in line 595 of subsequent commits:\n1. /etc/init.d/nginx start or systemctl start nginx\n. Yes, this disables it, because node doesn't recognize our CA.\nThis will be resolved soon as node was working on allowing users to point to their ca of choice.\nThis was fixed using a different method in the new release. This PR will be closed without merging.. That's a good point. Thank you for the advice. While this PR won't be merged, we will consider this update.. I don't think dashboard should be in here twice.\n. Same as previous comment, I don't think dashboard should be in the url twice.. enableClientAccountCreation is the toggle for enabling meteor's accounts-password.\nauthenticationType is for telling us which is set.\nNeed to determine which type to have set as default.. The date on the copyright establishes how far back the claim is established to be, \nIt could look like copyright = 2014, 2017, Mozilla \nor just copyright = 2014, Mozilla\nEither way . oh! thanks for catching this! I had it changed, but guess I reverted when I backed out a commit. . yeah, typo, will fix. ok, will fix. should this be python/pythonpython?. same question as above - should this be python/pythonpython?. should this be python/pythonpython?. should this be python/pythonpython?. should this be python/pythonpython?. should this be python/pythonpython?. should this be python/pythonpython?. should this be python/pythonpython?. How does this  affect how it runs considering all crons are in /opt/mozdef/envs/mozdef/cron and not /opt/mozdef/envs/python/mozdef/cron?. should this be python/pythonpython?. should this be python/pythonpython?. should this be python/pythonpython?. should this be python/pythonpython?. should this be python/pythonpython?. should this be python/pythonpython?. should this be python/pythonpython?. should this be python/pythonpython?. should this be python/pythonpython?. should this be python/pythonpython?. should this be python/pythonpython?. should this be python/pythonpython?. should this be python/pythonpython?. should this be python/pythonpython?. should this be python/pythonpython?. should this be python/pythonpython?. should this be python/pythonpython?. should this be python/pythonpython?. should this be python/pythonpython?. should this be python/pythonpython?. should this be python/pythonpython?. should this be python/pythonpython?. should this be python/pythonpython?. should this be python/pythonpython?. should this be python/pythonpython?. should this be python/pythonpython?. should this be python/pythonpython?. should this be python/pythonpython?. also there's a leading slash that shouldn't be there on the second line.. I agree with @jeffbryner . In the abstraction layer we have 3 variables.\nHowever, we have two aliases being used: events, events-previous\nI changed the abstraction layer to only take in index, oldindex, and alias.\nI couldn't reuse that function if it was taking in oldindex, newindex, previndex, alias, index, etc.\nSo I separated by an if statement the two calls (one calls for \"events\" alias update, and the other for \"events-previous\" update.\nThe casting is to populate what the abstraction layer expects with the correct values.\nI do think this can be done more elegantly, but I think that would require a rewrite.\n. Definitely agreed, I'll add comments in a little bit.. That makes sense, I always try to be as explicit as possible, but am ok with not being explicit where it's not needed.. That's actually an artifact I should have removed, we plugged in the events-previous into the config file, which I forgot to PR as well. \nI will resolve this. Thanks for catching it!. Done. Done!. Done!. Fixed! Thank you. Why is mqack being removed?. there should be a two space indentation after the parent key name (s= space):\n              \"apiVersion\" : {\n              ss\"type\" : \"string\",\n              ss\"index\" : \"not_analyzed\",\n              ss\"doc_values\" : true\n              },. Extra comment line here that should be removed to be consistent with the rest of the commit. Extra comment line here that should be removed to be consistent with the rest of the commit. Thanks @pwnbus!. done. ahh ok, makes sense :) . I think we should remove the match on _type since we are moving away from using this as a term we match on to align with the migration.. Once we split the indexes we can remove all remaining calls to _type - this PR is only for removing the _type from the search parameters that trigger events. Low hanging fruit so to speak.. Good call, I have submitted a commit to resolve.. Not yet, not until we actually split the indexes.. This particular alert never fires, it will be part of our discussion on Wed.. These have notes on them in the spreadsheet as to why they aren't modified, many of them don't trigger as they are.. as for alerttask.py -  should be removed when we split the indexes.. This can be removed, we no longer use okta. Per https://www.elastic.co/guide/en/elasticsearch/reference/5.3/breaking_50_mapping_changes.html#_floating_points_use_literal_float_literal_instead_of_literal_double_literal\nfloat is used instead of double, which leads me to wonder if we should remove double and simply allow float to take care of this? I haven't checked if we actually have any \"double\" fields yet.. Upon reviewing supported numeric types, it looks like double is still a thing, but it's a 64bit floating point number, while float is only 32 bit. Will look through our field types and see if we use double anywhere. We do not store any \"double\" field types. I really don't like the idea of enabling fielddata, We'll still be able to search on the summary if it's set to Text based on any token within the field, but I don't know that we do any aggregations now based on single words in summary only searches for strings within the summary.. Per https://www.elastic.co/guide/en/elasticsearch/reference/current/text.html they recommend using keyword field for sorting/aggregation  - not text fields. I can't imagine aggregating on summary would be very beneficial if it returned every token in the field in an aggregation (like returning new and york rather than new york)\n. We should probably talk about whether we want doc_values here or keep it as is. The question is would we ever want to sort/aggregate on a boolean field.. Per our discussion, this  is ok if it never fires (this is good) and I have modified _type to be category instead.. This is deprecated as this system is no longer on our network. thus the removal.. This is deprecated as we are no longer using CrowdStrike for this.. This is deprecated as this solution was modified and lives on in ssh_key.py, thus removing. I think you're right. I'll fix. I suppose all of this is not needed for docker, but does sense still work if we remove the path to it?. You have these custom settings, but these settings are duplicated on lines 22 and 23 setting Xms and Xms to 2g, while these set it to 512m.\nWhich ones are to be the standard for a docker container and we should comment out the ones that are not.. curious why this line is here, and not in  docker/compose/nginx/files/nginx.conf - just seems a bit duplicative?. oh nice! that's great news :). I understand that, but what I'm saying is all four lines are live settings - I feel two of them should be commented out to prevent any conflict should something go awry.. ahh ok! I missed that. Thanks!. This should be set to 30s \nWith as much indexing as we do setting it to 1s would create way too much overhead for our cluster.. Should change the default refresh interval here too to 30s. ahh okies :) ty. I have to check that we change these ports on deployment.. We do change these on deployment, this change is fine.. Yeah, this is what I was  wanting, a Ban Manager tab, with both a whitelist and blacklist menu item so we can manage both.\nThis can definitely be added later though :)\n. This won't capture auditd commands that are categorized as execve, I know that nmap commands come in as execve and not category auditd, so this might need a little more work.. Best practices are that you use a well defined mapping for an index, and not dynamic mapping as dynamic mapping actually creates a stressor, by removing this particular mapping from events and adding it to it's own index, there are a few less entries that es has to determine on it's own.\nSince this mapping should never change, it makes perfect sense to include the mapping and take the brunt of that work off of es.. I had wondered about this myself, I agree with you. I'll modify.. Many thanks :). This change has been committed.. Right now this only has mozdefstats fields, this mapping needs changed to include the mozdefhealth fields as well.\nI'll add the additional fields and we'll see just how big it will be.\nAlso please note the sqs queues I'm adding will also be additional mappings. Let's decide once I have the finished mapping fleshed out.. I've updated the mapping, and we should probably talk about this more once you've reviewed the sqs status PR, as any changes there may indicate a need for changes here - depending on whether we are ok with the queues being mapped dynamically to a field rather than a json blob.. You're absolutely right.. Yeah this is a concern, I was sort of modeling this off of how we currently do healthAndStatus, but what I noticed is that even healthAndStatus only seems to pull data and insert an event from a single host at a time, that data is not merged together in the result as I was trying to do here.\nSQS queues are not host based, and I think that's where I'm finding some issue with how to extract this data and present it without it causing some logic problems.\nI did want to avoid a separate config for each queue, but doing this will require a better schema structure, and if everything is in one event, then the event can possibly have an infinite amount of fields in it's mapping (around 3 or 4 for each queue).\nAlso to speak to @jeffbryner's point of accesskey/secretkey for all of them does cause problems, because we don't have the same key in multiple accounts. \nSo with that said, I think moving back to the one config per queue addresses most of these problems, and if one queue fails, it doesn't fail for all of them in this manner.\nI can add ownership information for each queue as well to address that need.\nI do feel there is a more elegant way to do this, but not with crons, I'm thinking more of a microservice implementation that would run on supervisord to kick off these checks and feed into ES using celery maybe. Could possibly move to that kind of structure for all of our crons and get out of using cron and simply add the queue name, keys, region, and owner to a list in a celery config, or use existing esworker configs to grab the required data and populate the values needed for the connection. \nSo instead of having a cron directory for these items, maybe we just have a plugin that cycles through the mq configs and pulls this data using celery workers?. Going to try the json config and see if I can make that work.. This is what is in place in the other scripts, I didn't deviate from what we have standardized on.\nI would think if we want to change it, we should change it across all scripts in a separate PR.. Same as with the other comment goes for this one:\nThis is what is in place in the other scripts, I didn't deviate from what we have standardized on.\nI would think if we want to change it, we should change it across all scripts in a separate PR.. Found a bug here, this filename doesn't match the one committed, will push an update for this after lunch. Seems I committed 2 files for mapping, going to remove one in favor of the other. \nmozdefStateMappingTemplate.json will be removed\nmozdefStateDefaultMappingTemplate.json will be retained.. :) thanks! I'll correct.. Interesting, I did not know this.. it's a typo, should have been message.keys() which is in other places in this script, with that said, does that mean that those other fixups that contain message.keys() are not working? I've used message.keys() in other places and it does work in that regard. . I just checked other events where message.keys() is used to transform a field, and the other locations that this is present does appear to be functional, so I think just correcting the typo is all that's needed here.. Actually we do, it's on line 57 of this doc.. agreed. I've pointed to the systemd unit files to start these services.\nI'll put additional information on how to start them manually, but I think most will want to use a script in order to be able to have a better time at stopping the process rather than digging through documentation to find out how to do it in uwsgi.\n. Now on line 177 per the update. Maybe, I'll see if I can figure out what's going on between my ide and me, will correct shortly.. I know what is causing this. They are set to spaces in the IDE, and are indented correctly there, but because I submitted the commit from my windows machine git is trying to correct for crlf I think that may be throwing things off.\n. I just pulled it down on the mac, and indentation is correct there too for these files.\nWhatever github is showing is incorrect.. looks fine in vscode, not sure what github is doing to my commit :( \nGoing to correct online rather than in my branch since it's showing correct there.. This is a docker embedded construct for  the nginx to be able to query DNS for the other containers, it's always this ip.  This has no relation to the resolver we have in the production nginx config. We add this because we have our networks defined.\nhttps://docs.docker.com/v17.09/engine/userguide/networking/configure-dns/. Everything from line 13 to line 255 can be removed. \nThese are published via collections.js in the imports directory. \nI've tested the removal in my own environment, and then tested that all the functionality still exists.\nThis will get rid of the \"ignoring publish.. \" error messages when starting meteor.. not for the source install that we do. \nIf it's using a distribution based package you can lock it with yum versionlock. Just a nitpick, before these declarations you have no spaces around the \"=\" but now you do.\nI know it doesn't break the code, but wanted to call it out as we are trying to standardize the format. Any particular reason for or against the spaces around the \"=\"?\nI think pep8 says to remove them, but I think it's prettier with them. Thoughts?. It does, I have tested it on real cloudtrail events.  Thanks for the inquiry!. Looks like this is missing a closing bracket.. You have an extra colon in this line after hostname.. You have an extra colon in this line after hostname.. We spoke of this on slack, and came to a solution which has been committed to revert this.. good point.. I've replaced with the plaintext version of \"events-weekly\"\nAlso asked you to automagically populate the id of index-patterns so we don't have to deal with hashes, this should resolve the issue :) \nThanks!. hmm, I think  you are right, originally I was using this section to open the url, but later moved it.\nI'll update :) ty!. ha, no, will revise. these are two different variables. index.py uses python based WatchList variable for it's database results. The other is node based variables, which do slightly different things. the other watchlist you see in this file that is lowercased, is for the db collection, which I do not want to get confused with the list we create from that collection.. I think I understand what you mean now, but I am unable to wrap my head around it just yet.\nI don't think get watchlist is where we would do this, ultimately we should publish generic alerts and/or possibly make an abstraction layer to allow other sources to add to the watchlist collection, but that really doesn't work well as this is right now. We would really need to dig into what the requirements for that would be. It could get pretty unmanageable if not done right. This was not a part of the original requirement, thus should be a future iteration.. This can definitely be improved, I removed the logic that determined if an item is watchlisted, that's handled earlier in this code. So we could essentially remove the check for is false and simply input the data.. previous comment still stands, one collects all current entries from mongodb to validate if the item should be deleted, while the WatchList variable is a list of only the watched terms rather than full mongodb content.\nWould this work better for you if I renamed the WatchList to WatchTerms or something? In my mind line 38 of the plugin watchlist.py is where this var is referenced:  self.name = \"WatchList\". Thanks! Good advice!. the newest version was released about a month ago, and was updated 4 months ago. I can reach out to the maintainer and ask if that banner is old cruft from when it wasn't maintained.. Will try that, thank you!. So the maintainer says he only added that to say he's not actively using this project anymore, and isn't working with jwt anymore, but he would provide minor fixes if needed.\nThis particular package is doing a very small piece of what we need (taking a secret and providing a hash of it in conjunction with \"requests\" lib.\nI'd have to see what else is out there before ripping it out  to replace this functionality.. I've modified this in a way that these two vars no longer imitate one another. added to a trello card to track. Per our conversation in the meeting today, we will move forward with this, and I'll work with Andrew to implement an even better solution utilizing auth0 authorization mechanism for machine to machine communication.. Should line 94 and 95 also have if statements to be consistent with the rest of the code here?. Also if these fields do not exist, what will be put in their place?. ack will revise.. Will do. Thanks, was part of debugging :). Edited, leftover artifacts from the maint cron.. I feel like it's evident, but I'm ok with changing this to what you feel is more appropriate.. I like your approach, sounds cleaner. Will modify. The first test closes the test index that L#127 creates.\nThe next test creates a test index on L#137 and then closes it on L#139, and then opens the one we closed on L#140.\nAre you just asking for a negative test case for saving to a closed index? That will never happen in production as we only save to the current events alias, which is never closed. We only close  \"actual index names\" not aliases. In fact in 6.x you can't close an alias at all, you MUST cite the index name.  We should never, ever, be writing to any indexes that are not tagged with the current events alias.\nPlease help me find the value in this request or explain how it will benefit us moving forward.. fixed in last commit.. fixed in last commit.. still need to do. To explain, it says don't close any indexes unless they are 30 days or older. That's how that logic is working. . So I should have two tries?\nOne that gives us an exception for when it cannot connect, and one for when it can't connect to close?. sure! Thanks for the suggestion :). when I was testing it required it, because odate_month is actually turned into an int later on in order to increment it to 30+ days. What this does is say if any of the indexes do not have \"events\" in the name, then they won't be closed. Basically, any indexes that are not named \"events-something\" are never rotated, and also never closed.\nSo basically, changing this to if events is in the index, then we are changing the way the logic works, which is fine, it's much more specific if we change it, and makes sense.\nThanks!. Added! Let me know if this is enough or if you think we need more.. I understand now, thank you for clarifying.. Agreed!. Done :). Take a look at what I added and let me know if that works for you.. Done!. Added source here, because I was not sure if \"type\" could be used when stating self.type = type in that fashion.\nI felt maybe we could go  back and change it to add casting this value into a type field, but for now added it as source. pinging pwnbus for insight.. in my head I feel like maybe I should remove documenttype from this stanza, and add this later in the code as simply 'type' so that there is no confusion between _type: _doc and type: . This block appears unused as it is. We have no doc_types of \"cef\". ",
    "jvehent": "Aside from a couple of nitpicks, this looks r+ to me.\n. lgtm\n. @ameihm0912: r?\n. debug prints ?\n. need &limit=1000000\n. debug print?\n. I'd call that a complianceitem just to be consistent with the naming in MIG.\nA compliance check is the subpart of a compliance item that contains the check type, value, etc...\n. nit: a comment about what this does would be useful\n. ",
    "526avijitgupta": "@jeffbryner There already seems to be a publish and subscribe in place for kibanadashboards .  Does the REST call really need to be for each screen change ?\n. @jeffbryner I am facing difficulty in reproducing this, does it still seem to occur ?\n. @jeffbryner Good catch.\nIt would make sense to break the code into different files. We'd just have to care about loading the files in the required order. And maybe that's the initial reason that we had actually concatenated different files together. \n@darkprince304 Need your confirmation on this. \n. ",
    "justdave": "Yeah, we wrote that log message as part of our fail2ban rule.  Wording is easy to change. :-)\n. ``` diff\nIndex: modules/asterisk/files/fail2ban/logger.conf\n===================================================================\n--- modules/asterisk/files/fail2ban/logger.conf (revision 91340)\n+++ modules/asterisk/files/fail2ban/logger.conf (working copy)\n@@ -31,7 +31,7 @@\n #            unix timestamp of the ban time\n # Values:  CMD\n #\n-actionban = /bin/logger -t  -p  -- The IP  has been banned for  seconds after  failed attempts against \n+actionban = /bin/logger -t  -p  --  failed attempts against  by , banned for s\n# Option:  actionunban\n # Notes.:  command executed when unbanning an IP. Take care that the\n```\nSending        modules/asterisk/files/fail2ban/logger.conf\nTransmitting file data .\nCommitted revision 91774.\n. Doh.  Nice catch.\nSending        logger.conf\nTransmitting file data .\nCommitted revision 91988.\n. sigh...  thanks! :)\nSending        logger.conf\nTransmitting file data .\nCommitted revision 92020.\n. ",
    "hankyzz": "Sorry \uff0cI just Saw \u3002I tried all !   I use docker to install in the Ubuntu System\uff1a\"from debian testing\" and\" from Ubuntu latest\" \uff0cI also install on a local system without the docker \uff08 on the Centos6.5_64 system and Ubuntu 14.04_64 system\uff09,but all the things that I have tried didn't work! \n. ",
    "poyw911": "Hey jeff, this is Di. \nI was just googling about the problems i sent you through the emails, and found this guy was having similar issues. I have my settings.js information matches /etc/nginx/nginx.conf and all the exports. But I am still having the same problem: \"sorry, the reuqested URL 'http://localhost:8081/' casued an error: Not Found: '/' \" for rest and loginput uwsgi. \nLet me know if you already had a solution in mind. \nThanks!\n. ",
    "yashmehrotra": "@poyw911 What is the output for http://localhost:8081/status. Because the 404 you are getting maybe bottle's 404 response.\n. @jeffbryner I would like to work on this one. One question I needed to ask was, apart from mongodb, do I need to store the incident details anywhere else ?\n. Fixed in https://github.com/jeffbryner/MozDef/pull/299. @jeffbryner Kindly close it.\n. @jeffbryner The new documentation is tested and its working. Its merge ready from my side. Do you have any suggestions ?\n. I also took the liberty of adding a Reset Filters button since it was getting a bit difficult to manage 4 filters.\nThis is how it looks:\n\n. @jeffbryner Hi, have you tested this one out, any feedback/suggestion ?\n. @technion You can try the full Manual Installation, it's tested now, and should work. Those docs were a bit old so I have updated them with #326\n. When you have installed it, you can go to MozDef/meteor/ and then run mongodb by $ meteor mongo. BTW, when you start MeteorJS, the mongodb automatically starts with it. So all you have to do is:\n$ cd <path_to_MozDef>/meteor/\n$ meteor\n. @claudijd @pwnbus @Phrozyn \nI made a PR (#578) regarding this issue.\nKindly check it out.. @pwnbus Thanks \ud83d\ude04 . This should be\nif 'ircchannel' in alert and alert['ircchannel'] != '' and alert['ircchannel'] != None):\nas you are checking that key's value in the alert dict. ",
    "ameihm0912": "Looks good, will also make required changes in mozdef_client for this\n. Sounds good to me. I'm guessing this would indicate the \"requested\" URL in an event from the perspective of the server, and not a requested URL from a client perspective? Not sure if we need to distinguish explicitly, it might be useful to have a constraint around how it's used documented, just to avoid things like the field being used by an application making a request using the URL?\n. @gdestuynder looks good to me.\n. @mpurzynski one comment here actually, we should probably change this from \"scan\" to sensitive user. ",
    "kknd22": "Jeff,\n  Thanks for the quick response. The current effort was to install the software in the closed environment (no internet connection, by policy) and perform evaluation. The final product need to support both open and close environment. \n  To reproduce the issue, on my local machine, I was able to run with internet first, successfully then shut down the connection to the internet. It looks like that it will be useful to have a switch to let the app either use the cached *.js or down load on the fly? Is there a quick workaround?\nThanks\n-Chris\n. Thanks Jeff, I will give it a try.\n. ",
    "h2ko": "I have a fix for this that retains the functionality of the original, but you can manually call a dot path as an option:\nhttps://github.com/h2ko/MozDef/blob/master/alerts/lib/alerttask.py\n. ",
    "peteydevops": "I ended up figuring out the issue. My apologies for the bring up the issue. Seems that I had some inconsistencies with my docker image files. I got everything to work by completely uninstalling docker.io and removing all of docker files within /var/lib/docker. \nAfter completely removing these files I re-installed docker.io on debain system. Not sure if this might help anyone else in case this issues arises again. \n. ",
    "DarkPrince304": "Agreed. I'll fix this and send in a PR soon.\nOn Feb 1, 2016 10:17 AM, \"Avijit Gupta\" notifications@github.com wrote:\n\n@jeffbryner https://github.com/jeffbryner Good catch.\nIt would make sense to break the code into different files. We'd just have\nto care about loading the files in the required order. And maybe that's the\ninitial reason that we had actually concatenated different files together.\n@darkprince304 https://github.com/darkprince304 Need your confirmation\non this.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla/MozDef/issues/324#issuecomment-177762679.\n. \n",
    "technion": "Thanks for this clarification. I'll close this issue because you've answered my question, however, you may want to review documentation.\nDocker is listed first in the installation guide, with no such warnings. The \"full\" installation is listed as \"Alpha phase\" and comes with a warning about not being tested.\n. ",
    "whbaker": "Hello everyone. First of all, thanks for working with and referencing VERIS. It hasn't received much love lately because many of us who used to work with it a lot have left Verizon. But since it is open and no longer a Verizon-only thing, I've been meaning to pick it up again at some point soon. Stuff like this can help shape it. \nRe your question on impact levels, I'll try to clarify. That impact scale is intended for use in a post-incident scenario to provide a qualitative rating of \"pain\" to the business. VERIS also included fields to record a quantitative estimate. I'm not sure about the context in which you're using it above, but I see mention of CVSS. My sense is that it's probably not optimal for something like that. \nIf you're trying to figure out how well it maps to a different scale - maximum,high,medium,low for instance - I suggest reviewing the expanded labels:\n\n\"overall_rating\": {\n      \"Catastrophic\": \"Catastrophic: A business-ending event (don't choose this if the victim will continue operations)\",\n      \"Damaging\": \"Damaging: Real and serious effect on the \\\"bottom line\\\" and/or long-term ability to generate revenue\",\n      \"Distracting\": \"Distracting: Limited \\\"hard costs\\\", but impact felt through having to deal with the incident rather than conducting normal duties\",\n      \"Insignificant\": \"Insignificant: Impact absorbed by normal activities\",\n      \"Painful\": \"Painful: Limited \\\"hard costs\\\", but impact felt through having to deal with the incident rather than conducting normal duties\",\n      \"Unknown\": \"Unknown\"\n    }\n\nHopefully that will help you figure out whether it'll fit. \nOn the point of standardization, I couldn't agree more. In fact, one of the reasons I haven't done a lot with VERIS lately is that many VERIS elements have been incorporated into STIX as part of the data model or recognized vocabs. That's not to say they can't both exist, but if STIX has the momentum, I'd rather influence and incorporate rather than \"compete.\" And FWIW, STIX does leverage the VERIS impact rating for it's Incident:Impact_Assessment construct. https://stixproject.github.io/data-model/1.2/stixVocabs/ImpactQualificationVocab-1.0/\nHope that helps. Let me know if that opens more questions than I answered.\n. ",
    "gdbassett": "I'm on the DBIR team and have been looking at what can be done to improve VERIS.\nWhen it comes to impact (or likelihood) ratings, I'd recommend focusing on cross-correlation.  I've found that everyone has 'names' for levels that work for them that they become rather attached to for whatever reason.  The easiest solution is to have clear mappings rather than ask people to change their scores.  The easiest way is to map all to a 0-100 scale, but direct mappings work as well.\nFor VERIS, I'd like to make sure that enumerations remain as objective as possible, preferably linked to business KPEs.  Is Guillaume's mapping above an accurate mapping of the business impacts associated with 'low/med/high/maximum'?\n. ",
    "keldwud": "I'll build a new instance and report back with the messages from the python section\n. ",
    "claudijd": "@jeffbryner @Phrozyn r?\n. well, it was worth a try.  Maybe when this is a priority, we can tackle this effort.. This is going to require a lot of edits to pass, closing for the time being.. I do wonder if we should though, as Python revs which undoubtably changes behavior, we should have that pinned for our unit-tests or it could pass in our unit-tests on Python X and fail on Python Y.\nI think travis is running 2.7.11, as per .travis.yml. I'm going send a PR to add .12 and .13 to travis.  This will let us know that they all work, regardless of deployment choice.. I'm going to resolve this, as deployment details like what Python you want to use in Mozilla Prod aren't really relevant in the source repo.  We can now say it works on .11 - .13 and we're validating that against our unit-tests. . example: https://github.com/mozilla/MozDef/blob/master/alerts/confluence_shell.py#L9. @Phrozyn Seeing who touched a file in git is most conventially done with a git blame action in modern SCMs, which will tell you who modified each and every line and in what change set.  As to your priority comment, this is probably ~10min worth of work and IMO just good code hygiene, I'd be willing to send the PR if the effort/prioritization against other priorities was a hurdle here.\ngrep -R Contributors ./ | wc -l\n241. This alert will trigger on events generated from pentest-automation...\nhttps://github.com/mozilla/pentest-automation/blob/master/policy/external_ssh_policy.rb\nhttps://github.com/mozilla/pentest-automation/blob/master/policy/external_ssh_policy.yaml\nThis is very similar to the open ports alert strategy, if you're looking for something to compare it to.\nI set to warning for the time being to give it some time to run in prod.. FWIW, there is a specific term, and this is:\nKey: details.proxyaction\nValue: TCP_DENIED/-\nThat said, I agree that deny only is not enough to alert, this is why we're adding these alerts instead, which will present more IR value than deny events alone, which represent mostly misconfigs rather than malicious behavior and we want to focus on malicious behavior.\nhttps://github.com/mozilla/MozDef/pull/748\nhttps://github.com/mozilla/MozDef/pull/750. Another question here, do I need to produce an FQDN white-list and upload in the same process as we do for Mozilla ranges or would we want that white-list to live closer to MozDef?. I made a couple adjustments to this branch to add essential fields to the example event defaults as well as adding some test cases that stress the conditionals in the aggregation logic of the alert.  It's still not passing, but I think it's really close.. @Phrozyn yes, totally (RE: unit-tests), just wanted to get the concept up for consideration.  If all seems good and we have it where we want it, then I'll add the tests to make it so.. I think I want to change the name of this alert, because I have a few other ideas for suspicious proxy drop behavior that I want to capture.  I think repeated drops is an interesting thing, especially when we add the executable context, but in addition to that I think it would be interesting to have yet another alert that suggests CONNECT proxy attempt abuse.\nFor example:\nTCP_DENIED's can come from HTTP/HTTPS traffic, but their destinations will have slightly different attributes (80's will have URLs, 443's will have host/port), what if we had an additional alert that triggered on denied events for non-std ports. \nTCPDENIED for say example.com:444\nI recall an old trick I used to use to get IRC bots to push through a connect proxy to call home and although we would obviously block that, it would be nice to have a more upfront alert suggesting this is deny behavior we should be looking closer at to detect exfil/cnc behavior.. @Phrozyn the proxy does support non-std ports by function, just not by policy.  So these will be drops in #750 and I included an example to demonstrate this.\nIn this particular PR, the query logic restricts to events with executable extensions that are already being denied, this is bubbling up behavior to make us aware of that fact, which would otherwise go unnoticed in the regular deny noise, which makes up nearly all traffic.. For the purposes of clarity, a run book for this alert now exists on mana.. I don't believe high-false positives are a concern here.  Namely because we have a not-statement on the query syntax on \"443\", which is the only port that should legitimately using the proxy.  Additionally, we have a context to restrict this to denies, such that it's really focused on catching malicious behavior more so than misconfig behavior.  This is actually a common tactic with IRC bots phone-home behavior.\nI also verified that non-std ports on the connect proxy are logged:\n\n. For the purposes of clarity, a run book for this alert now exists on mana.. @pwnbus I want to hold on this, I have some tweaks add more port exceptions, or we should move to a config. I have reviewed 7 days of history on this, and this doesn't seem like a false positive concern.. My assumption here is that maybe the recent changes to mozdef inserted a requirement for an authenticated image and thus this is why I can't pull the necessary base images to build locally?. Points of feedback here:\nInitial build time seems significantly longer than before (this may impede the speed of local iterations on a single test, I'm not sure, defer to @pwnbus on those deets).  It seems it's building all containers, rather than those needed for test.. It appears as though the intent is to do this:\nmake build-tests\nmake run-tests\nHowever, make build-tests appears to still build all the things.. I switched to storage-driver aufs and noted it in the guide wip, though expect possible breakage if Docker doesn't fix the issue we're working around before deprecation.\n\n. $ docker --version\nDocker version 18.06.0-ce, build 0ffa825\n$ docker-compose --version\ndocker-compose version 1.22.0, build f46880f. After reviewing the error, it seems my docker env/image was bloated.\nI did note some errors/warnings in the make build-test which where...\nMaybe you want: rm -rf /var/cache/yum, to also free up space taken by orphaned data from disabled or removed repos\nSo it's possible we could do some additional checks in this process to shed some image size bloat after the system is updated.. I was able to get this to build, though, I suspect this is not an isolated issue and could come back to bite some users, so I'm leaving it open until perhaps we address the /var/cache/yum bloat feedback.. docker image prune && docker image prune -a made some room.  I suspect this was made worse by running make build-test which previously built all containers and may have bloated the base image enough to be an issue.. Going to pair with pwnbus on this and make sure it works with make strategy. Removed. Removed. changed to fqdnlimit as value, but not sure if I need to worry about namespacing $limit as key. fixed. fixed, went with FQDN for consistency, but the entry point for the user will have FQDN/DNS in the UI, so it's not like a \"what is FQDN?\" type response.. fixed. fixed. I wonder if we need some more logic here to cover a case where the data within summary is not what we expect.\nSay summary contained \"foo\", would this stack trace mozdef or handle it gracefully?. @pwnbus on alert logic that might produce \"N\" sources of malicious activity, do we need to worry about producing a summary below a specific length to avoid run-off?. My guess is that this default event needs summary data if the alert and aggregation is consuming this information.\nWe should probably also consider providing two alternative examples with one from say IP A and another from IP B to ensure that our aggregated source IPs are properly being served.\nAdditionally, as we noted in testing, there's a weird quote chars being presented in the summary from honeycomb and we should carry these forward in our test cases.. I think this threshold needs to be set higher, but leaving it 1:1 until we can have some discussion as to what the threshold should be.. This search query may also require some exception logic to include tuning out destinations we know to be potentially problematic until they can be resolved.  (Example: centos repo URLs). I'm pretty sure this regex doesn't work, needs to be verified as proper syntax. Can we add a requirement that the logging priority be critical?  Honeycomb does emit other severity levels, but criticals are the only ones we're really interested in for the sake of alerting.. This is actually the condition I was thinking about (though not a stack trace, certainly non-desirable)...\n\n\n\narray = [1,2,3]\narray.append(re.search('foo', 'bar'))\narray\n[1, 2, 3, None]. This is to work around an issue where by CentOS will ignore provided repositories.  We will not alert on those cases to try and get a better signal to noise.  There is not much concern around an attacker knowing this, as the traffic is being dropped anyways.. I think 'continue' is more appropriate in this context\n\n\n\nhttps://www.tutorialspoint.com/python/python_loop_control.htm. This is being commented out for the time being for TS purposes.. This is a temporary short circuit this until main is fixed.. Yeah, I think warning for a few weeks and triggering examples of it and seeing it live will help increase confidence before making critical. @pwnbus please note that there is something in this logic which does not cause an exception but prevents a match.  Initially, I thought it was somehow AND'ing instead of OR'ing, but that does not appear to be the case based on a test where I left only \"exe\" in the config exceptions.  Something else is happening here.. This hack is implemented to show we can make the unit-tests pass without fixing the query search.  The concern however is that by not fixing this we can run into a situation where a given source IP has multiple deny events within an aggregated search and the ones we want are discarded (or not sampled) and then we further weed those events in the onAggregation such that a exe target failure would basically not fire in some circumstances.  Not to mention, it's horribly unperformant to do this in python and not in Lucene/ES (at least in concept, I'm not sure how much more overhead this presents).. I want to expand this list out, once we have a functional alert in test.. I'd like to add some negative examples to exercise the logic of what happens when it doesn't end in .exe as well as what happens when we have multiple aggregated destinations.. Discussed with pwnbus, this is fine.. Fixed in ade1c5b . @pwnbus opinions on stripping this from new alerts? Feels more like template documentation, which serves as clutter in finished alerts.. Did you note the comment on line 25?  I pretty much say this much and validate later to prevent false positives.. ok, will do. Needs updating to reflect new alert test strategy. We should provide less examples here.  I think two is enough.. ",
    "ajomadlabs": "Can I also be part of this ?  I am really interested to take up this. Could you explain more on this feature.. @Phrozyn Thank you . @Phrozyn Can you assign me this issue ?. @Phrozyn I have forked the repo and accepted the collaboration request. @Phrozyn Thanks. . @Phrozyn Can I know where I should start from . @Phrozyn Can you provide me with docs on how I could run this locally.. @pwnbus So how should I start, any specific guidelines. @pwnbus Can I get some guidance in both, starting with Docker as well as MozDef using a Docker. @pwnbus I am bit slow as I am just catching up with Docker.. ",
    "VitorFalcao": "@Phrozyn Is this enhancement still needed? If so, I would like to fix this issue! I am just starting with this project, so could you give me some pointers? Thank you. ",
    "gene1wood": "Or we could enable plugin authors to be explicit in their registration declaration about their intent to match field names or tag values.\n. With the current message format as interpreted by the mozdef sqs worker, here are ways to search for your events\n- details.tags:foo\n- details.hostname:ip-172-31-14-43.us-west-2.compute.internal\nAnd here's what the resulting event in MozDef looks like with the details within details.\n{\n  \"_index\": \"events-20161005\",\n  \"_type\": \"event\",\n  \"_id\": \"ECBd7QjIT-KRTfpF0X-3fQ\",\n  \"_score\": null,\n  \"_source\": {\n    \"receivedtimestamp\": \"2016-10-05T21:41:26.887228+00:00\",\n    \"tags\": [\n      \"infosec_mozdef_events\"\n    ],\n    \"mozdefhostname\": \"mozdef1.private.scl3.mozilla.com\",\n    \"utctimestamp\": \"2016-10-05T21:41:26.908187+00:00\",\n    \"details\": {\n      \"category\": \"event\",\n      \"processid\": 11724,\n      \"severity\": \"INFO\",\n      \"tags\": [\n        \"foo\"\n      ],\n      \"timestamp\": \"2016-10-05T21:41:26.626639+00:00\",\n      \"hostname\": \"ip-172-31-14-43.us-west-2.compute.internal\",\n      \"summary\": \"This is my summary\",\n      \"processname\": \"/path/to/my_script.py\",\n      \"details\": {\n        \"bar\": \"baz\"\n      }\n    }\n  },\n  \"sort\": [\n    1475703686908\n  ]\n}\n. The notion that the first camel case word is the verb as well as which verbs are readonly comes from reviewing a list of all AWS actions. Adopting #1096 would resolve this.. Should now be possible. I made this against master accidentally. Ignore this PR. See #830. Fixed in #977 . The substack template was actually left in place, though unused at\nhttps://github.com/mozilla/MozDef/blob/master/cloudy_mozdef/cloudformation/mozdef-mq.yml\n831 is where reference to this template was removed from the parent. This may require setting up a service linked role. If so leverage how we're doing it for ES. This is resolved with #880 #879 #876 #868 #867 #866 and #840. This is resolved in the cloudtrail es worker but not the sqs or snstosqs workers. As we don't require those two workers for cloudtrail (though we will need sqs for GuardDuty), I think further work on this doesn't block getting MozDef working in AWS (with only CloudTrail ingestion).\nI'm going to remove the cloud label on this as I'm going to work on this in a general sense across all workers.. Resolved in #867. Duplicate of #848 . @andrewkrug As there is a contributing file in mozdef are you thinking of changes to it, or an additional one specific to the cloud portion? (or something else). Fixed in 1780aef700f0a16b72546d9bdba0be2309dd7dec. This fixed #862 . @pwnbus would you think that I should also add a new entry in this docker-compose-rebuild.yml file for the eq_cloudtrail container?. This was a side effect of the bug fixed in #928. So, if we take out \"Operating system\" and MozDef component version, little is left in this template.\n\nAsking people if this is a bug or feature request\nTrying to cue them to search for an existing issue first\nGiving them a list of MozDef components, though I don't think we do anything with this\nAsking them to give steps to reproduce, expected results and actual results\nAsking for custom configuration and log messages\n\nCan I propose that we actually just remove the template entirely? It seems like we're pre-optimizing to handle problems with the level of detail provided in issues (unless there's been a historical problem with people submitting vague issues that require a lot of back and forth to gather details about).\n@Phrozyn  ?. Phrozyn suggested calling out the challenges I'm facing with the template\n\nThe two and a half pages of content in the blank issue when opening it makes me reluctant to file an issue because of the time commitment needed to read through and fill out those pages. In response I've found myself deleting the template, unread and filing a 4 or 5 line issue.\nBeing asked to test if the latest release, and master branch are affected too. is confusing for feature requests and as a user, a barrier to entry in reporting a bug, not having an environment to stand up and test these in. Considering I'm in infosec, and I don't have a way to easily test this, a user outside the team, to which this template is targeted would find this a disincentive to report a bug.\nI don't know what a - Documentation Report is. Pwnbus suggested it was likely a bug report for documentation.\nWhen asked to Mention any settings you have changed/added/removed I'm unsure what settings would be germane or where to find them. This likely stems from my lack of understanding of the configuration system for MozDef in general. Again, if it's hard for me and I'm in the team, the user this is targeted for may have trouble with this step.\nI didn't understand how to determine the version of MozDef components as asked for in Mention the OS you are running MozDef from and versions of all MozDef components you are running and it sounds like there is no way to determine that.\nThe 4th level deep heading style seems odd\nMentioning that You can also paste gist.github.com links for larger files feels unnecessary. This seems like another instance of pre-optimizing for problems we don't have (of users either pasting large blocks of text into issues or not providing the data because they don't know where to put it)\nshow exactly how to reproduce the problem makes me feel like I'm at risk of not providing the level of specificity that this template demands. As a user that this is targeted at, we should be more welcoming.. This should revert #867 . Oh, interesting. On our cloudy mozdef deployment it links tohttp://localhost:9090/app/kibana# instead of the actual kibana URL. Looks like we may have missed that localization piece.\n\nI'll close this issue and open on specific to the cloud deployment not using the environment variable. Not sure why I dind't notice that the drop down already had a link (just not the right one). @pwnbus When you say that this requires creating those two files, do you mean those two files were created in #1043 or the creation of those files is work still required to have a default CloudTrail dashboard?. So #926 doesn't fix this because it sets the meteor link to kibana, not to the OIDC reverse proxy container URL which fronts the AWS ES Kibana URL, but instead the raw AWS ES Kibana URL.\nThe result is that the URL isn't reachable because you're supposed to contact it via the reverse proxy.\nSo ways we could approach fixing this\n Do some javascript trickery to allow us to have a relative URL with a port number and set the kibana URL to a relative URL with port\n Change the 2 mozilla.oidc.reverseproxy docker containers into a single one and add support for mozilla.oidc.accessproxy to accept not a single path/backend pair but an arbitrary number of pairs such that each path would be proxied back to a different backend. Then with these 2 containers consolidated into one, the kibana URL becomes a simple relative URL with no special port number\n* Have the FQDN that the site is accessed at, passed in to the instance (see #871 ) so we can construct the kibana URL by combining that value plus a port plus a kibana path\n. >  Any way to derive this from the bits already in cloudformation?\nUnfortunately, not at the moment. #871 proposes making the DNS name of the deployment available in the stack and would solve this.. @gdestuynder The driver for the env variable instead of json file is so we can include the template location on the command line. Without this (or some other solution), when you use the makefile to upload to a development area in S3, that location isn't then consumed when you create or update the stack because the parameter isn't passed in.\n\nis the env variable from a file better than just a file? in both case there's a file,\n\nThis isn't intended to address the problem we'd talked about the other day of the secrets being in the CWD of the git repo. So no, an env file isn't better than a json file in regards to the downsides of having the secrets in the git repo.\n\nIf so i think i wouldn't include and example because it makes it likely to be wrongly committed with a secret\n\nI included an aws_parameters.example.sh file and updated .gitignore to ensure aws_parameters.sh doesn't get committed. Does that work or should we do something more or different?\n. Agreed, I'll open an issue so we can move away from a file model and to something safer/better.. @andrewkrug writes \"further it looks like the cron job is able to query the health data and place it into mongo\". Yup, this is working, this was a side effect of the bug fixed in #928 . Yes, previously when we had the OIDC login issue, the errors were continuous and repeated, but now it does only happen like once. Cool.. Travis failed because of a network timeout to centos yum\nCould not retrieve mirrorlist http://mirrorlist.centos.org/?release=7&arch=x86_64&repo=os&infra=container error was\n12: Timeout on http://mirrorlist.centos.org/?release=7&arch=x86_64&repo=os&infra=container: (28, 'Operation too slow. Less than 1000 bytes/sec transferred the last 30 seconds'). Indeed, it's generated by the CloudFormation template. > the elastic search server and kibana server are external\nCorrect, in our MozDef for AWS deployment we use AWS ElasticSearch which is external\n\nOpenID seems to be used for accounts and a few options are set.\n\nYes, accessing Kibana, hosted by AWS with the ElasticSearch service, is done through our OIDC Proxy (which also fronts the MozDef UI)\n\nThe only major thing I'm not seeing is if I can override a tls cert file.\n\nTell me more, which cert do you want to override and what do you want to change it to?. I've used the new make target to deploy the new templates and then created a new stack using the new launch button link.. I'll submit another one of these.. Fixed in #1042. Here's the CSS line causing the problem\nhttps://github.com/mozilla/MozDef/blob/c6866d5f5a9c8af8bf77c439db116d5309d59e2c/meteor/public/css/mozdef.css#L197. > Did you mean three worker types or classes?\nYes, 3 .py files which would manifest whatever number of running workers that is needed for load.\n. @jeffbryner What do you think about defaulting the visualizations that don't work out of the box to disabled in feature flagging? If that's a good idea should I open a new issue on that or is this issue good for it?. @jeffbryner Ah sorry I'd misunderstood. @pwnbus had said \"the logic to search and label an attacker is currently pretty specific to mozilla I believe, so we\u2019d have to figure out a way to have folks customize that per environment\"\nIf indeed the globe does work out of the box (as soon as data comes in), I'll work with the MozDef team to figure out why @darkian isn't getting any data vis in the globe in his deployment.. Agreed!. Whoops, missing a trailing \\. Doh! Sorry, that was what I was using to debug and I left it in. I'll just remove it. Thanks for catching that.. We could either pass OIDC_CLIENT_SECRET to docker by appending it to the temporary file containing the environment variables or via this command line argument.\nI guess it comes down to whether we prefer the secret to be on disk or on the command line. On disk, if something goes wrong and the temp file doesn't get deleted it's left here. On the command line and it's visible to other processes.\nI would say it's probably better to put it in the environment temp file. What do you think @gdestuynder ?. I've put in a change to move the OIDC_CLIENT_SECRET into the env file, assuming that's better.. suggestion\nMozDef for AWS is new and we'd love your feedback.  Try filing GitHub issues here in the repository or connect with us. It shouldn't need 3 public subnets, it should work with anything 2 or greater (maybe even 1). Might be worth calling out what's missing for support in other regions, or link to a GitHub issue on support in other regions.. A note : I was hoping to do this (in #871) by passing in a zone and having CloudFormation handle the route53 record. I may switch to that model down the road.. I've looked and compared this line that you've added the comment to but can't tell what changed. Was there a missing typo that was re-added? What is this line change about?. Is it not possible to use the existing /opt/mozdef/docker/compose/cloudy_mozdef.env pathway which makes the ESURL value available instead of creating config.py?\nFor example instead of creating config.py here in the user-data of the instance, change the config.py on disk to something like\nES = {\n    'servers': [os.environ.get('OPTIONS_ESURL')]. Do we want this PR in master or should this go in a reinvent branch?. Funny indent. I was thinking of making it conditional (if you provide a zone as a parameter, use it, otherwise expect the user to deal with names on their own). @andrewkrug What was the typo you were referring to in the attempted comment?. @andrewkrug Should I change this from reinvent to guardduty or from reinvent to master?. Talked to andrew on the phone: master. Talked to andrew on the phone, this refers to the reserved word relative. Done. Whoops, this line isn't checking the right key. I'd worry about this not being enough to ensure that the index you just created will be available below. You might want to instead do a try except loop over some period of time with your queries. Looks good. Good call, fixed in b85ed2a . ",
    "Lfengjie": "Hey,\nThank you for your reply, I will try what you said. I decide to download v1.20 MozDef. I think it will be more stable. But when I try it, it still has some problem about jdk, I think it is the problem about network., I'll try it again.\n. I'm sorry I didn't see your reply!\nI found the problem. It seem that MozDef will connect the network when I start it. So sometimes It can not be reached. The network I used blocking some website.\n. Thank you for you reply! \n. ",
    "cglewis": "@pwnbus thanks for following up. option 2 sounds good. let me know if you need me to do anything.  thanks!. ",
    "shikhar-scs": "Is this issue still up ? (if yes) Is anyone working on it?. ",
    "tristanweir": "Thanks @pwnbus for extensive work to bring this back to open source!. We actually just planned dealing with \"Unknown\" cities this for our upcoming sprint, although the UX might be slightly different than surfacing the IP.. > We don't sort or script, so the only reason to keep doc values enabled would be aggregations. Do we aggregate on all fields?\nOur theory is that we don't want to hamstring ourselves by excluding any fields from doc_values unless we know we won't aggregate on them. In other words, keep the default of being able to aggregate on everything, and exclude things sparingly. \nAccording to ES documentation,\n\nyou can disable doc values in order to save disk space\n\nDisk is not traditionally a limiter in our environment, although it could be in other deployments. Ultimately, I'd rather keep the option to aggregate available as long as disk capacity is available.. Closing because we don't want to get rid of okta, even though we no longer use it internally. The fix to cron mac address correlation will be programmed in early 2019.. Heh, just came to see how to add this.. Fixed. Appreciate the suggestion.. We pose it as a Y/N question, but only offer a single button.  Would it be better to present it as \"Your account recently logged in from {0}, {1}. [Get Help]\"\nAlso, is it possible to pass a date/time to the message as well?. Based on the assignment statements, we are functionally passing es.update_alias(oldindex, newindex, index). I get clarifying alias = index, since we are basically passing the date string (eg events-YYYYMMDD) but why the other reassignment?. My guess is that we are going to lose the thread on the distinction between previndex & oldindex after a while. We should either comment clearly or refactor to have todayindex, yesterdayindex, twodaysagoindex. Does it meet expectations to hardcode in \"events-previous\" vs \"events-YYYYMMDD-1\". ",
    "scriptonist": "Hey can I take this up ?.. @pwnbus Thank for assigning me this issue. I'm a beginner and i'm very new to open source contributions. I will try my best to resolve this issue. I will  keep you updated with my progress.Please help me out if get stuck.. @pwnbus I'm not able to build the image. when Irun cd docker && sudo make build I'm getting following error\nmake: *** No rule to make targetbuild'.  Stop.. @pwnbus same error when running from project root. @pwnbus I'm getting some npm errors when running the build\n```\nnode-pre-gyp ERR! Tried to download(404): https://github.com/kelektiv/node.bcrypt.js/releases/download/v1.0.3/bcrypt_lib-v1.0.3-node-v46-linux-x64.tar.gz \nnode-pre-gyp ERR! Pre-built binaries not found for bcrypt@1.0.3 and node@4.7.0 (node-v46 ABI) (falling back to source compile with node-gyp) \nmake: Entering directory/node_modules/bcrypt/build'\n  CXX(target) Release/obj.target/bcrypt_lib/src/blowfish.o\nmake: g++: Command not found\nmake: *** [Release/obj.target/bcrypt_lib/src/blowfish.o] Error 127\nmake: Leaving directory /node_modules/bcrypt/build'\ngyp ERR! build error \ngyp ERR! stack Error:make` failed with exit code: 2\ngyp ERR! stack     at ChildProcess.onExit (/usr/lib/node_modules/npm/node_modules/node-gyp/lib/build.js:276:23)\ngyp ERR! stack     at emitTwo (events.js:87:13)\ngyp ERR! stack     at ChildProcess.emit (events.js:172:7)\ngyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:211:12)\ngyp ERR! System Linux 4.4.0-92-generic\ngyp ERR! command \"/usr/bin/node\" \"/usr/lib/node_modules/npm/node_modules/node-gyp/bin/node-gyp.js\" \"build\" \"--fallback-to-build\" \"--module=/node_modules/bcrypt/lib/binding/bcrypt_lib.node\" \"--module_name=bcrypt_lib\" \"--module_path=/node_modules/bcrypt/lib/binding\"\ngyp ERR! cwd /node_modules/bcrypt\ngyp ERR! node -v v4.7.0\ngyp ERR! node-gyp -v v3.4.0\ngyp ERR! not ok \nnode-pre-gyp ERR! build error \nnode-pre-gyp ERR! stack Error: Failed to execute '/usr/bin/node /usr/lib/node_modules/npm/node_modules/node-gyp/bin/node-gyp.js build --fallback-to-build --module=/node_modules/bcrypt/lib/binding/bcrypt_lib.node --module_name=bcrypt_lib --module_path=/node_modules/bcrypt/lib/binding' (1)\nnode-pre-gyp ERR! stack     at ChildProcess. (/node_modules/bcrypt/node_modules/node-pre-gyp/lib/util/compile.js:83:29)\nnode-pre-gyp ERR! stack     at emitTwo (events.js:87:13)\nnode-pre-gyp ERR! stack     at ChildProcess.emit (events.js:172:7)\nnode-pre-gyp ERR! stack     at maybeClose (internal/child_process.js:854:16)\nnode-pre-gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:222:5)\nnode-pre-gyp ERR! System Linux 4.4.0-92-generic\nnode-pre-gyp ERR! command \"/usr/bin/node\" \"/node_modules/bcrypt/node_modules/.bin/node-pre-gyp\" \"install\" \"--fallback-to-build\"\nnode-pre-gyp ERR! cwd /node_modules/bcrypt\nnode-pre-gyp ERR! node -v v4.7.0\nnode-pre-gyp ERR! node-pre-gyp -v v0.6.36\nnode-pre-gyp ERR! not ok \nFailed to execute '/usr/bin/node /usr/lib/node_modules/npm/node_modules/node-gyp/bin/node-gyp.js build --fallback-to-build --module=/node_modules/bcrypt/lib/binding/bcrypt_lib.node --module_name=bcrypt_lib --module_path=/node_modules/bcrypt/lib/binding' (1)\nnpm ERR! Linux 4.4.0-92-generic\nnpm ERR! argv \"/usr/bin/node\" \"/usr/bin/npm\" \"install\" \"source-map-support@0.4.2\" \"semver@5.3.0\" \"fibers@1.0.13\" \"amdefine@1.0.0\" \"underscore@1.8.3\" \"bcrypt\"\nnpm ERR! node v4.7.0\nnpm ERR! npm  v2.15.11\nnpm ERR! code ELIFECYCLE\nnpm ERR! bcrypt@1.0.3 install: node-pre-gyp install --fallback-to-build\nnpm ERR! Exit status 1\nnpm ERR! \nnpm ERR! Failed at the bcrypt@1.0.3 install script 'node-pre-gyp install --fallback-to-build'.\nnpm ERR! This is most likely a problem with the bcrypt package,\nnpm ERR! not with npm itself.\nnpm ERR! Tell the author that this fails on your system:\nnpm ERR!     node-pre-gyp install --fallback-to-build\nnpm ERR! You can get information on how to open an issue for this project with:\nnpm ERR!     npm bugs bcrypt\nnpm ERR! Or if that isn't available, you can get their info via:\nnpm ERR! \nnpm ERR!     npm owner ls bcrypt\nnpm ERR! There is likely additional logging output above.\nnpm ERR! Please include the following file with any support request:\nnpm ERR!     /npm-debug.log\n```. @pwnbus Sorry! I totally forgot about this. I believe now all looks good and submitted a PR.Please review and suggest fixes if any. . My bad ! Pushed the wrong branch !. Let me correct that ASAP. ",
    "bsrdjan": "Thank you both for the prompt and helpful input. Yes, it is actually the feature request for Firefox and will try to post the request there. . ",
    "andrewkrug": "In compose you specifically have to bind UDP to get this to work with UDP output.  In my case I'm using 5140/udp to avoid conflicts with port < 1024 on my system and side step potential issues with SELinux on Fedora / RHEL.\ndocker run -d --name fluentd \\\n  -p 5140:5140/udp -v /etc/fluentd:/fluentd/etc -e FLUENTD_CONF=fluentd.conf \\\n  fluentd-krug\n^^ My docker run command is there.. ```\n            alert_dict = {\n                'alert_code': '63f675d8896f4fb2b3caa204c8c2761e', # This is simply uuid.uuid4().hex that makes it so users don't get an alert repeatedly if the condition recurs.  You can store this as a static string in the alert definition.\n                'user_id': self.userinfo.get('user_id'),  #The users auth0 user-id example: ad|Mozilla-LDAP|akrug\n                'risk': 'medium', #These map to Mozilla risk levels (low, medium, mask-risk )\n                'summary': 'Your version of Firefox is older than the current stable release.', #Text that we show in the alert bar.\n                'description': 'Running the latest version of your browser is key to keeping your '\n                               'computer secure and your private data private. Older browsers may '\n                               'have known security vulnerabilities that attackers can exploit to '\n                               'steal your data or load malware, which can put you and Mozilla at risk. ', #Text that will be shown in the alert center.\n                'date': str(datetime.date.today()), #Date of infringement.\n                'url': 'https://www.mozilla.org/firefox/', #There is a button in the URI this is where it links.\n                'url_title': 'Download',  #Title for the button in the UI\n                'duplicate': False #If the alert should not duplicate if the condition recurs prior to user acknowledgement.  False means do not duplicate for the same alert_code.\n}\n```. I've added the above additional context at the request of @pwnbus .  . @pwnbus please let me know if you need me to help with the AWS IAM policies for the MozDef user. :+1. Thanks @pwnbus . Do we really need operating system?  Can't we just make a support statement of \"mozdef is only supported on RHEL/CentOS 7.x\". This is likely because you haven't built the containers yet.  \nmake build as a first step.  Then run make run-tests .\nSince everything inherits from the base container it's gotta be on the system first.. From slack:\n`make build-tests` should only build the required containers to run unit tests in them, which is the tester, elasticsearch and rabbitmq\n`make run-tests` should only run the tester, elasticsearch and rabbitmq (I haven\u2019t confirmed wether this is the case or not. REPOSITORY                            TAG                 IMAGE ID            CREATED             SIZE\nmozdef/mozdef_syslog                  latest              bf70e10d9263        2 days ago          226MB\nmozdef/mozdef_meteor                  latest              515ee2f1258b        2 days ago          2.55GB\nmozdef/mozdef_rest                    latest              7a9025fc6e8e        2 days ago          688MB\nmozdef/mozdef_mq_worker               latest              ef4161529da6        2 days ago          688MB\nmozdef/mozdef_loginput                latest              a83e3a7f847c        2 days ago          688MB\nmozdef/mozdef_cron                    latest              c6bf7ca9efff        2 days ago          719MB\nmozdef/mozdef_alertplugins            latest              245b5d4e3ec3        2 days ago          688MB\nmozdef/mozdef_alerts                  latest              f3b611ae0ed3        2 days ago          688MB\nmozdef/mozdef_bootstrap               latest              4c1555d9c6c7        2 days ago          688MB\nmozdef/mozdef_mongodb                 latest              c6a57607fd92        2 days ago          496MB\nmozdef/mozdef_rabbitmq                latest              30cf10e1b48b        2 days ago          261MB\nmozdef/mozdef_base                    latest              b2d98a783320        2 days ago          688MB\nmozillaiam/mozilla.oidc.accessproxy   latest              2f9dcea1bcec        13 days ago         860MB. Complete in https://github.com/mozilla/MozDef/commit/995d730c9c934aa0549e9d19d54ee32e5035b5c1. @hyandell it's most definitely an error.  Likely generated by Cookie Cutter.  If you'd like to PR a change to MozDef with an MPL 2.0 license we would very much appreciate setting things correct.. thanks @jeffbryner . This duplicates #974 and #975 by @jeffbryner .  If there's no complaint I'd just merge this all at once since it supports each other.. What specific exception are you trying to catch here?  This will bury literally everything.   \nGenerally I like to use \nexcept Exception as e:\n   logger.error({}.format(e))\nAs a minimum so you can know why.. CentOS packaging syntax isn't going to bump python2.7 to python3 until CentOS 8.  This would violate the rules of RedHat.  I think we should leave this as is.. We will need it.  We're going to wrap the SQS tests and CloudTrail tests in moto mocks.. Looks like these were added by @zsck probably good to remove them here.. This was also added by @zsck whilst fixing imports.  We're not using this yet.  OK to revert.. Feel free.. Same ... no reason not to just update to the proper url.. If meteor fails a step it makes it REALLY REALLY hard to troubleshoot.  This is why we made a conscious choice to move these all to individual lines.. Let's drop the second instance.. For building dev containers where python eggs come from git+ssh:// instead of warehouse or a directory on disk.. Are these workers not covered in the test suite?. #wontfix. I'm going to defer addressing this one.. We should remove my comment line here.  It actually is treated as a string in CF when you do this.  Comments need to be separate lines.. We should switch this back to master.  I missed this.  Need to come up with a better build / deploy for launching a stack off a branch based build.. Can you just fix this in the github editor?. We also need to template out that file to activate different alerts.  I'd suggest merging this as is and filing an issue to change the way this works entirely to use a different config system.. I think assuming that roue53 exists in the customer acct is a big leap.  . We can add conditional logic to support a less redundant deployment.. AMI replication.. You should use isinstance(normalized['user'], dict) instead of type based comparisons if possible. . Do you know why?. Fixed typo :p. Can you add a message that makes this reflect what we talked about yesterday?  It should notify the caller that this is being built using source \"from github\" and not local to set expectations.. ",
    "edmorley": "Hi! To clarify - in this case whilst the city was unknown, the country was also wrong, so fixing the city wouldn't help on it's own (unless the intended fix is something different).. That sounds perfect - thank you \ud83d\udc4d . Amazing! Thank you :-). ",
    "cknowles-moz": "With increased looking at TCP_DENIED, is there a way to setup some sort of \"exception list\" - thinking here of the known bug around centos mirrors popping into existence... it's a known issue that's being worked on, but it causes a significant amount of TCP_DENIED noise in the proxies.. ",
    "caggle": "I have basically used \"proxy_drop_executable.py\" alert (and its corresponding unit test) as a template for this one, they are very similar.\nOne potential caveat I thought of was, if this alert would catch on CONNECTs, as most of these domains in question are available over HTTPS only.. I encountered the same issue while trying to build, after I got past the 'tar' bug on overlayfs (by making docker use the aufs, as was suggested on the web and in Slack).\n@claudijd how did you get it to still build?. I am unable to get past this issue, even after running a multitude of docker rm and docker prune commands. Strange...I am stuck.\nI am following the alert development guide also. Getting this error on the make build-tests step.. OK, I was able to figure this out. For reference in case others run into this problem, I simply increase the disk image size for Docker disk image:\n. I think you mean, if the event summary is completely different than what expect, i.e. no originatingIP, but simply a random string like \"foo\". Still I don't think it would stack trace (not from regex at least), but we should probably but a validation the next line in case there is no match (i.e. len(offendingIP) == 0). Agree.. I have included a condition from L48.. Added another condition in https://github.com/mozilla/MozDef/pull/747/commits/81738efeffecad9d288589a63905f37e4285e87e. Done in https://github.com/mozilla/MozDef/pull/747/commits/aff07d46fcd5496e0d36676b111ef3bd146757a5. From memory, making it critical is the eventual goal but we'd like to get it more \"productionized\" first by possibly installing more instances/more services.. ",
    "Baron-Severin": "Forgot to read contributing guidelines.  I'll reopen after following the requested steps.. ",
    "darakian": "Never mind. I ran make run before make build. Oh interesting. That's good to know about. Thanks!. No worries. I just wanted to report it so that more people don't waste time with this issue.. From what I'm reading Rsyslog has just introduced a module called omhttp for outputting http formatted logs. A json template is needed and I intended to create one for my own needs which I can share.\nAt the moment I'm still uncertain of how to format input into MozDef. . @jeffbryner. The lack of http support would do it I guess. Thanks!. Sure thing. I'm still just trying to come to grips with how to use MozDef. I'm not getting any alerts at all right now, so I'll probably make a PR for an extra page of documentation once I wrap my head around it.. So, I'm trying to get a basic alert to work for logs coming in from rsyslog. I've modified the default alert to\nsearch_query.add_must([\n            TermMatch('category', 'rsyslog'),\n            ExistsMatch('hostname'),\n        ])\nFull source below.\nI read this to mean \"put an alert on mozdefhost/alerts\" if there exist logs with the category=='rsyslog' and something non-null for a hostname (and with timestamps within 20 minutes of \"now\" for now being the time runtime of the query).\nI can check the Kibana instance (this is all a MozDef demo docker thing) and such logs exist. In fact I've extended the time range to be 2000 minutes and I still don't see anything on the alerts page. I feel like I'm missing something basic here.\nEdit:\nPerhaps I'm simply impatient and the alert generation takes some decent amount of time?\n```\n!/usr/bin/env python\nThis Source Code Form is subject to the terms of the Mozilla Public\nLicense, v. 2.0. If a copy of the MPL was not distributed with this\nfile, You can obtain one at http://mozilla.org/MPL/2.0/.\nCopyright (c) 2014 Mozilla Corporation\nfrom lib.alerttask import AlertTask\nfrom mozdef_util.query_models import SearchQuery, TermMatch, QueryStringMatch, ExistsMatch, PhraseMatch, WildcardMatch\nclass AlertRsyslog(AlertTask):\n    def main(self):\n        # Create a query to look back the last 20 minutes\n        search_query = SearchQuery(minutes=2000)\n    # Add search terms to our query\n    search_query.add_must([\n        TermMatch('category', 'rsyslog'),\n        ExistsMatch('hostname'),\n    ])\n\n    self.filtersManual(search_query)\n    # Search aggregations on field 'hostname'\n    # keep X samples of events at most\n    self.searchEventsAggregated('hostname', samplesLimit=10)\n    # alert when >= X matching events in an aggregation\n    self.walkAggregations(threshold=1)\n\n# Set alert properties\ndef onAggregation(self, aggreg):\n    # aggreg['count']: number of items in the aggregation, ex: number of failed login attempts\n    # aggreg['value']: value of the aggregation field, ex: toto@example.com\n    # aggreg['events']: list of events in the aggregation\n    category = 'rsyslogcategory'\n    tags = ['hello', 'world', 'rsyslog']\n    severity = 'WARNING'\n    summary = \"My first alert!\"\n\n    # Create the alert object based on these properties\n    return self.createAlertDict(summary, category, tags, aggreg['events'], severity)\n\n```. @pwnbus does that mean there needs to be one line for each alert that I want in the config.py file? Bruteforce_ssh and unauth_ssh both seem to get loaded whenever I build the containers. Is there some way to set desired alerts before building/running the container?. Got it and got it working! Many thanks for the help :). By 'environment variable' do you actually mean shell variables or could this also mean something like a config file?\nEdit:\nI've made a basic branch. Let me know if you agree with the direction and I'll put the work in on it.\nhttps://github.com/darakian/MozDef/commits/add-base-schedule-file. @jeffbryner That certainly could work for my purposes. It would be nice to set some queries to run at a lower rate if they become complex or if only a certain amount of activity is expected over a given time frame ex. 10 logins an hour is normal, 11 an hour implies certain doom.\nAnother option would be to have the schedules be configurable via a web page and/or http endpoints. Default everything on/off and running on the minute. Let users configure from there. Maybe keep the two example alerts as the only ones turned on so as not to overwhelm new users.. Ah yes, I forgot about the thresholds in the py files. Perhaps those should be user configurable/override-able via a gui/end points?. @pwnbus Where would I find the docs on the differences between PhraseMatch/Term[s]Match/etc...?. @pwnbus thanks!. Glad to help. :). Sorry, I've commented the sourceip line out. Updated code to reflect that.. Ahhh I see. So if I aggregate on category that should remove the exists requirement? Also do I need to restart mozdef to propagate the change?. Current alert form is below. Also, is there a better way to restart the alerts service with the docker containers than make stop; make run? Still nothing.\n```\n!/usr/bin/env python\nThis Source Code Form is subject to the terms of the Mozilla Public\nLicense, v. 2.0. If a copy of the MPL was not distributed with this\nfile, You can obtain one at http://mozilla.org/MPL/2.0/.\nCopyright (c) 2014 Mozilla Corporation\nfrom lib.alerttask import AlertTask\nfrom mozdef_util.query_models import SearchQuery, TermMatch, QueryStringMatch, ExistsMatch, PhraseMatch, WildcardMatch\nclass AlertHelloworld(AlertTask):\n    def main(self):\n        # Create a query to look back the last 20 minutes\n        search_query = SearchQuery(minutes=20)\n    # Add search terms to our query\n    search_query.add_must([\n        TermMatch('category', 'helloworld'),\n    #    ExistsMatch('details.sourceipaddress'),\n    ])\n\n    self.filtersManual(search_query)\n    # Search aggregations on field 'sourceipaddress'\n    # keep X samples of events at most\n    self.searchEventsAggregated('category', samplesLimit=10)\n    # alert when >= X matching events in an aggregation\n    self.walkAggregations(threshold=1)\n\n# Set alert properties\ndef onAggregation(self, aggreg):\n    # aggreg['count']: number of items in the aggregation, ex: number of failed login attempts\n    # aggreg['value']: value of the aggregation field, ex: toto@example.com\n    # aggreg['events']: list of events in the aggregation\n    category = 'hellocategory'\n    tags = ['hello', 'world']\n    severity = 'WARNING'\n    summary = \"My first alert!\"\n\n    # Create the alert object based on these properties\n    return self.createAlertDict(summary, category, tags, aggreg['events'], severity)\n\n. Ya I've scheduled it. Here's my full alerts schedule with some pseudoonomization\nALERTS = {\n    'foo1.AlertFoo1': {'schedule': crontab(minute='/1')},\n    'foo2.AlertFoo2': {'schedule': crontab(minute='/1')},\n    'foo3.AlertFoo3': {'schedule': crontab(minute='/1')},\n    'foo4.AlertFoo4': {'schedule': crontab(minute='/1')},\n    'foo5.AlertFoo5': {'schedule': crontab(minute='/1')},\n    'helloworld.AlertHelloworld': {'schedule': crontab(minute='/1')},\n}\n```. Cool. Is it possible that one of my alerts might have an error and somehow kill the alerting process?\nEdit:\nIf I wanted to make a web UI alert creation page. Where in the codebase should that be done? meteor/client/*?. Just so I'm clear this fix would create a new yml file in docker/compose/mozdef_alerts/ then docker exec -it (get a shell?) into the mozdef_alerts container? Sorry, I'm still fairly new to docker.\nLooking at docker ps it seems that my alerts container is in a reboot loop.\n#        mozdef/mozdef_alerts          \"bash -c 'while ! ti\u2026\"   2 days ago          Restarting (1) 29 seconds ago\nFound a missing end quote in one of my alerts which was causing the reboot loop. Fixing this does not enable alerts for me, but I made a PR to try-except on alerts importing anyway.\nEdit:\nThis may have fixed my alerting issue after all, though it took well over a minute for the alerting to come back. Thank you much for the help @jeffbryner and sorry for sending you down a rabbit hole.. Ya, no worries and no rush.. @pwnbus Ready to be reviewed.. I'm not sure I understand. In the first link I see that Meteor is doing a self check to see if it should be using default values or not which I guess are set in the Dockerfile linked second. I'm not sure where PORT 3000 comes from though and I don't see it in the first link. As for the cloudy env file; I can't find that anywhere.. An env file certainly seems the maintainable approach. Looking at the use of \nenv_file:\n      - cloudy_mozdef.env\nIt seems that each docker container is being passed the same env file for overrides. Given your description of an env file I'm wondering if this is because there are reused variables or if there are somehow sections to the cloudy_mozdef.env file.\nMy mental model for the moment is that I have my build system insert lines\nenv_file:\n      - clear_mozdef.env\nfor each service (those I care about) and to configure my env as desired. One immediate question is; how do I know what I can override? Is the cloudy_mozdef.env file available anywhere for reference?. So, as I read this file the elastic search server and kibana server are external via this configuration. \n${ESURL} defining elastic search and \n${KibanaURL} defining kibana\nOpenID seems to be used for accounts  and a few options are set.\nThe only major thing I'm not seeing is if I can override a tls cert file.\nThank you much for the link!. Long term it might be desirable to change this to a cert associated with my company domain. The relevant containers are probably meteor and log input.. User defined env available via\nhttps://github.com/mozilla/MozDef/pull/1072. What are the events in this context though? Say for the bruteforce_ssh alert.. So these are the full log events? Ex. say I have a tag on all my logs 'foo', the following would return true?\nbool('foo' in aggreg['events'][0][tags]). I see. _source because all the ground truth log info is contained within _source? . Got it. Many thanks! :). Ah, my bad. Should have searched first. Do I need to rebuild the containers to fix it?. Cool thanks.. In testing I'm getting errors with this option\n[root@ip-172-23-4-100 MozDef]# make run-env-mozdef -e ENV=my.env\n/root/MozDef/my.env\ndocker-compose -f docker/compose/docker-compose.yml -f docker/compose/docker-compose-user-env.yml -p mozdef up -d\nWARNING: The ENV_FILE variable is not set. Defaulting to a blank string.\nERROR: /root/MozDef/docker/compose is not a file.\nmake: *** [run-env-mozdef] Error 1\nIn particular it seems that the env variable isn't being set/set long enough. If I manually set ENV_FILE before running then all is well. Adding an echo shows a blank string.\nENV_FILE=$(abspath $(ENV))\n@echo $(ENV_FILE)\n```\n[root@ip-172-23-4-97 MozDef]# make run-env-mozdef -e ENV=my.env\nENV_FILE=/root/MozDef/my.env\ndocker-compose -f docker/compose/docker-compose.yml -f docker/compose/docker-compose-user-env.yml -p mozdef up -d\nWARNING: The ENV_FILE variable is not set. Defaulting to a blank string.\nERROR: /root/MozDef/docker/compose is not a file.\nmake:  [run-env-mozdef] Error 1\n. I can indeed curl from the mq worker container. Looks like the env isn't being set properly with $(ESURL) being taken literally.\n[mozdef@384ef1bea0e0 mq]$ env\nHOSTNAME=384ef1bea0e0\nTERM=xterm\nES={\"servers\": [${ESURL}]}\nLS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:.tar=01;31:.tgz=01;31:.arc=01;31:.arj=01;31:.taz=01;31:.lha=01;31:.lz4=01;31:.lzh=01;31:.lzma=01;31:.tlz=01;31:.txz=01;31:.tzo=01;31:.t7z=01;31:.zip=01;31:.z=01;31:.Z=01;31:.dz=01;31:.gz=01;31:.lrz=01;31:.lz=01;31:.lzo=01;31:.xz=01;31:.bz2=01;31:.bz=01;31:.tbz=01;31:.tbz2=01;31:.tz=01;31:.deb=01;31:.rpm=01;31:.jar=01;31:.war=01;31:.ear=01;31:.sar=01;31:.rar=01;31:.alz=01;31:.ace=01;31:.zoo=01;31:.cpio=01;31:.7z=01;31:.rz=01;31:.cab=01;31:.jpg=01;35:.jpeg=01;35:.gif=01;35:.bmp=01;35:.pbm=01;35:.pgm=01;35:.ppm=01;35:.tga=01;35:.xbm=01;35:.xpm=01;35:.tif=01;35:.tiff=01;35:.png=01;35:.svg=01;35:.svgz=01;35:.mng=01;35:.pcx=01;35:.mov=01;35:.mpg=01;35:.mpeg=01;35:.m2v=01;35:.mkv=01;35:.webm=01;35:.ogm=01;35:.mp4=01;35:.m4v=01;35:.mp4v=01;35:.vob=01;35:.qt=01;35:.nuv=01;35:.wmv=01;35:.asf=01;35:.rm=01;35:.rmvb=01;35:.flc=01;35:.avi=01;35:.fli=01;35:.flv=01;35:.gl=01;35:.dl=01;35:.xcf=01;35:.xwd=01;35:.yuv=01;35:.cgm=01;35:.emf=01;35:.axv=01;35:.anx=01;35:.ogv=01;35:.ogx=01;35:.aac=01;36:.au=01;36:.flac=01;36:.mid=01;36:.midi=01;36:.mka=01;36:.mp3=01;36:.mpc=01;36:.ogg=01;36:.ra=01;36:.wav=01;36:.axa=01;36:.oga=01;36:.spx=01;36:.xspf=01;36:\nPATH=/opt/mozdef/envs/python/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nPWD=/opt/mozdef/envs/mozdef/mq\nTZ=UTC\nSHLVL=1\nHOME=/opt/mozdef\naffinity:container==3d976843239f7f1a732d89666035ef3ad8aee24d7a2788b342d000ea34c414f2\nOPTIONS_ESSERVERS=${ESURL}\nLESSOPEN=||/usr/bin/lesspipe.sh %s\n_=/usr/bin/env\n```\nEdit:\nLooks like setting OPTIONS_ESSERVERS directly does get things working (I've got mozdef references in ES at least)\nEdit 2:\nAnd logs are forwarding. So, it seems like variable resolution is a problem with an env file?. ~The more I look at this the more I think it might be a better idea to have the docker-compose file be the user supplied information and make available a template with static env file. Everything I'm reading leads me to believe that gnu make cannot set environmental variables for processes outside of itself. Compound this with the lack of input options for docker-compose and the \"single config input from mozdef user\" is difficult to achieve.~\nNever mind. There was a very simple solution to the env file issue.\n@jeffbryner Any thoughts?. @pwnbus Any chance of a review?. @pwnbus Any chance I can get some help on namespacing here?. So in my case I was able to create Negative tests which would happily accept an expected alert. Should this logic instead live within the alert test suite with a if pos/neg then else sort of structure?. @pwnbus  I do not. I thought I had one on a branch but I can't find it and trying to recreate it I also get errors. I must have been working doing something else. Thanks for checking.. For what it's worth all of our logs are currently coming from private IP address ranges. I'm not sure if we can map them to public or not, but it's not a big deal.. I think most of our logs will be routed over private IP space for the foreseeable future. A nice thing to have in the future would be to be able to define which field the IP address is in for a given type of log. ex. we have some application logging which has the same /16 (/24 maybe) subnet used for the log source, but we could embed the IP of given user session in those logs. This would allow for application level visuals which would be nice (for me :)). That's good to know about, but I don't want to alter the logs and change the source of truth (which is what I think that function does). I suppose I could alter the logging logic to send two copies; one with and one without the 'message['details']['cluster_client_ip'] field.. Gotcha, updated.. Ah indeed that is nicer. I wasn't aware that I could inherit with a docker compose file. Give me a bit and I'll update the file.. How does the compose.yml file know about ENV_FILE in this case? Is it just known as a global environmental variable?. Gotcha. My branch has been updated. I'm unsure if the make file is correct. I followed your edits but broke up the variable assignment and the docker call. Also I'm unsure if the docker compose pull is required.. Removed. Sure thing. I've added a crude, but direct message.. Curious. On my system I can't get the docker-compose line to respect the variable ENV_FILE. What did your single line solution look like?. ",
    "hyandell": "Thanks all :). ",
    "kpcyrd": "this disables tls verification. I'd recommend you do /usr/bin/node /opt/mozdef/envs/meteor/mozdef/bundle/main.js instead, you can access the logs with journalctl -u mozdefweb. You might have to remove Type=forking. ",
    "arcrose": "Just to make this easier to grok at a glance, I suggest we replace this \"magic string\" with a useful variable name.\npython\ndetails_delimiter = '.'\nif details_delimiter in key:. Would it be useful for this to log something like \"starting sqs_queue_status getQueueSizes\"?. I don't think we should continue using an arguably suboptimal style  just because it's standardized. Adding more information here will only serve as an improvement and a reminder that we should make similar changes elsewhere.  My experience, and it seems our team's experience, with delaying such changes for a future concerted effort is that this future effort is never made.. Should we not flesh this out some before putting this into master?. Even just a link to the related doc would work IMO.. For consistency, let's rename this to is_location as per PEP 8.. I think we should really avoid falling into the habit of mixing snake_case and camelCase as per PEP 8.. Class names should always be in PascalCase as per PEP 8. Is it not considered bad style to put conditions in parens when they don't have to be?\nThis condition could be shortened from\npy\nif not A and not B and not C\nto\npy\nif not (A or B or C)\nas per DeMorgan's Laws.. We should prefer to move shared code to a module where it can be imported in both cases. Unnecessary duplication will get us back to a point where we have modified duplicates littered everywhere and make it difficult to refactor.. As before, class names should be in PascalCase.. As before, class names should be in PascalCase.. I'm not a fan of the idea of using a decorator here.  This function does not modify its function argument at all, but rather has side-effects, which makes for a leaky and obtuse abstraction.\nWhile requiring more code, we should simply write the three lines of this function's body where calls to an async function occur rather than hiding its behavior this way.. Function names should be in snake_case as per PEP 8.. Rather than looping through plugins to find the one we're looking for, we should store all 'registered' plugins in a dictionary contained by the SlackBot so that lookup can be done in constant time and more explicitly. We're using the Command Pattern anyway, so we might as well do it properly.. I'm not sure that this approach will serve us well in the long term.\nIt wouldn't take much more effort to build parsing/tokenizing logic into each command. This way, the SlackBot only has to be able to identify commands by the first \"word\" in a command string, and can delegate the work of making sense of tokens to the command itself.. Would it make sense to give the channel argument to post_attachment a default value of None so that callers don't have to specify it in what I assume will be the majority of calls?. I'm basically just saying that instead of writing\npy\n@async_run\ndef function(...):\n  # ...\nwe should prefer to write\npy\nhandler = Thread(target=function, ...)\nhandler.start()\n...\ndirectly.. I see what you mean. My suggestion might not be necessary.. "
}
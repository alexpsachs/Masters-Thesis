{
    "ppwwyyxx": "Just an update, there is a draft api doc now at http://tensorpack.readthedocs.io/. The tutorial is still missing, and I know that's most important.. I was thinking something like that, but I don't know if there is a good way to parameterize this as an augmentor.. \"perspective\" may not be very important to use plus I don't have a clear idea what it is or how to parameterize it for people to use. Closing.. Done in 3f743301af46a93e.\n. Sorry I introduced a bug yesterday. I'll work on a fix soon. For the moment you can checkout to fd21c3b1afa4b870bbd5f12c1f68e264aafe4c02 or before.\n. Fixed in 13f76d5c3774d99e62\n. Found that ILSVRC12 data flow only has a speed of 6it/s while the queue is full. Probably due to serializing and message passing among processes.\n. Confirm that pickle is really slow for ILSVRC-size images:\n``` python\nimport numpy as np\nimport cPickle as pickle\nimport pickle\nimport time\nd = np.random.rand(32, 299, 299, 3).astype('float32')\nprint(time.time())\ndump = pickle.dumps(d)\nprint(time.time())\ndd = pickle.loads(dump)\nprint(time.time())\n```\ncPickle doesn't help. But python3 is much better.\nDill helps as well in python2.\n. Confirm the performance issue is due to slow socket communication of multiprocessing.\nWill work on a better version.\n. If data becomes a bottleneck, PrefetchDataZMQ should be used instead of PrefetchData. Usually several times faster.\n. That's probably a problem on the environment. Something you can try:\nUse a different tensorflow binary. Some previous versions are known to have performance problems.\nDon't use cuda 8, there are performance problems reported.\nAlways use tcmalloc.\n. I doubt this alone would make such a huge difference.\nWhat's your speed with one gpu? Ideally it should also be around 2 iter /s.\n. That's very strange.\nIf I ran it with desktop CPU I had 5.54s/it, which is very close to your speed.\nAre you sure GPU is used? Can you see your process in nvidia-smi? If not then some error must have been printed.\n. Yes, so it shows it's using cudnn. Although cudnn v5 might be better, from their documents.\n. For the record, it's confirmed to be cudnn v4 problem after working with Yixuan.\n. Thanks a lot for your interest. Currently the project is usable but still in a very immature stage: there are a lot of design choices to be made, and some performance issues to be solved. APIs might change, and distributed training are not even on the calendar (probably will require more refactoring). These are why it's probably not a good time to write documentations.\nDespite that, the dataflow module and the model module are mostly self-contained and reusable components that probably won't change a lot in the future. I may start working on some documentation on these parts.\n. Thanks a lot for sending the first pull request! \nHowever, examples are for illustration purpose of the usage of this library. Since the cifar100 example is almost identical to cifar10, it's not necessary to add an extra one specifically for cifar100.\nSame for the dataflow code. We would really prefer minimal duplicated code and more modularity, because that's all this project is intended for. \nSomething you may try:\nHave Cifar100 being a subclass of Cifar10, or more generally, use a  CifarBase and have a new Cifar10 and Cifar100 being subclasses of CifarBase. Cifar10 and Cifar100 would only be different in how they load the data, and that could be done by calling the corresponding function by different arguments (different URLs, different keys for the label, etc.) This way most existing code can be reused.\n. Hi,\nI'm not sure what's going wrong. But I tested on 1080 before and did see some weird things on that.\nNow I don't have a 1080 at hand for testing, but maybe you could run on CPU to see if things go well. For mnist it should get 98~99% accuracy after first epoch. \nI'm settings CUDA_VISIBLE_DEVICES in mnist-convnet.py to 0 by default, so maybe you'll need to delete that line to run on CPU.\n. Maybe my code uses some part of tensorflow that's broken on 1080. I may get some access to test on 1080 in a week but not now.\nAlso, I think only cuda 8 supports Pascal Architecture. How can you use 1080 with older cuda versions?\n. Hi,\nWe're not planning to release the bit-op runtime library in the near future. \nThe model isn't going to be released.\n. Yes, you can specify different learning rate for each parameter, by scaling its gradient. You can overwrite the get_gradient_processor() method in your Model with something like:\npython\n    def get_gradient_processor(self):\n        return [ScaleGradient([('conv.*/W', 0.5), ('fc.*', 2)])]\nScaleGradient takes a list of (regex, multiplier) tuple. Parameters that are scaled will get printed out so you could check.\n// UPDATE: this usage was later deprecated. See comment below.. This usage was deprecated one year ago. Now gradient processors can be added onto optimizer with http://tensorpack.readthedocs.io/en/latest/modules/tfutils.html#tensorpack.tfutils.optimizer.apply_grad_processors .\nFor example: https://github.com/ppwwyyxx/tensorpack/blob/9972b15036c851ce82e63721cfe0905e68115475/examples/SpatialTransformer/mnist-addition.py#L90-L96. haven't had a chance to look. is there still an issue?\n. Do you want to do this in inference time? Just add the names of those tensors to output names.\n. When model is written in this syntax-sugar, it'll be harder to find out names of those tensors.\nWhat you can do is to split the expression and print the tensors to find out the names:\npython\n            logits = (LinearWrap(image)\n                .Conv2D('conv0', 96, 12, stride=4, padding='VALID')\n                .apply(activate)())\n            print(logits)\n            logits = (LinearWrap(logits)\n           #.......\nI also just added a convenient print method for the linear model builder in the latest commit. After a pull you can use the following:\npython\n            logits = (LinearWrap(image)\n                .Conv2D('conv0', 96, 12, stride=4, padding='VALID')\n                .print()  # conv0/output\n                .apply(activate)\n                #.......\nto find out the name of the tensors.\n. Sorry but I found that the previous .print() feature only works in Python3 as print is a reserved keyword in Python2. I changed it do .print_tensor() instead.\n. Thanks. https://github.com/ppwwyyxx/tensorpack/blob/master/examples/load-alexnet.py#L60 has a function to load an existing model (through the session_init option) and run the model on new data.\nIn the example it's loading from a dict. You could use session_init=SaverRestore('train_log/xxx/model-xxx\") if you're loading from a TF-format checkpoint.\nDoReFa net examples also contain such functions at here. \nFeel free to ask if it's not clear enough.\n. You can see an example in the docstring in alexnet-dorefa.py. Something like --load path/to/model --run a.jpg b.jpg c.jpg.\nAlso see alexnet-dorefa.py --help.\n. A linear operation (convolution or gemm) on [0,0.33,0.67,1] can be computed, by first applying the operation on 3 * [0,0.33,0.67,1] = [0,1,2,3] and then a linear transform. Since [0,1,2,3] are all fixed-point integers of k bit, the operands in the first part can be efficiently  represented and computed on FPGA.\n. In our current settings, only the expensive operations (convolution & gemm) are performed in low-bitwidth. The elementwise scaling and nonlinearity are still in floating point.\n. I don't have available segmentation code for now. Will have some later.\n. I had an example script to reproduce an edge detection paper. The same architecture (FCN) is supposed to work for image segmentation as well. \nUPDATE: Mask-RCNN was added.\n. Could you try again? I can access the link even in chrome incognito mode. \n. From the log it looks like your repo was not up-to-date. It actually runs well with latest commit.\n. Fixed a missing import line in 2e238998a\nAlso, the error indicates that you cannot download from https://github.com/BVLC/caffe/raw/master/src/caffe/proto/caffe.proto, so you may need to check your network as well.\n. That's a python issue, and I don't think it's related to proxy.\nurllib in python definitely supports http_proxy environment variables. But it looks like there is something wrong with SSL. Googling the error would give you something you can try.\n. ./scripts/dump-model-params.py --meta train_log/alexnet-dorefa/graphxxxx.meta train_log/alexnet-dorefa/model-xxx output.npy\nSee ./scripts/dump-model-params.py -h for more.\n. I said model-xxx, not model.. model-xxx means files whose names start with model-.. The script itself has documented how to use it. It now uses npz.. You should in general read a file by referring to the library that creates the file.\nSpecifically, if you mean \"pb files\" created by tensorflow, then tensorflow will have the tool to read it, not tensorpack.. I've released the code for Atari in Gym with A3C.\nDon't have much document.. but maybe it'll help. \n. Thanks! Fixed in latest commit.\n. Oh! I didn't know you had a PR already. Thanks again!\n. Fixed in fbf93d44c. Thanks!\n. It still works for me. Are you using the latest tensorflow? If yes, could you paste more detailed error?\n. In the latest version of paper released in July: http://arxiv.org/abs/1606.06160, we have corrected the equation to bitcount(and(xi, yi)).\n. Yes it should be xor. Sorry for the confusion!. for any m, c_m(x) is a bit vector, not a bit. similar for c_k(y). The and is element-wise and operation of two bit vectors (of the same length p == q).\nif say M=K=2, p=q=2. x = [3, 1] = [11, 01] in base 2, , y = [2, 0] = [10, 00].\nThen c_0(x) = [1,1], c_1(x) = [1, 0], c_0(y) = [0, 0], c_1(y) = [1,0]\nx \\dot y = 3 * 2 + 1 * 0 = 6 = 1 * 0 + 2 * 1 + 2 * 0 + 4 * 1\n. The formulation doesn't necessarily mean how we implement it.\nF = 2 quantize(stuff) - 1. Therefore for whatever linear computation F is involved, we can use quantize(stuff) instead to compute the result, and then do a scaling/addition later accordingly.\nThis is mentioned in Sec 2.6, \nBy construction, there is always an affine mapping between these low bitwidth\nnumbers and fixed-point integers. As a result, all the expensive operations can be accelerated signif-\nicantly by the fixed-point integer dot product kernel\n. You mean this ?\n. If it is a cost function of several different cost, you just simply add them.\nIf each time the data is different, you can implement such DataFlow that generates different data for different tasks, and generate an indicator as well so that the model knows how to deal with them.\n. You can simply use indexing W[row] or W[:,column] to select the weights, where row or column is an integer scalar tf.Tensor. The gradient would work well. tf.gather would also work. Only the weights for that task would change.\n. Hi,\n1) 3) the maximum is taken over all axis of the gradient tensor dr except for the mini-batch axis. This is  the definition of max_0, and it is the line:\nmaxx = tf.reduce_max(tf.abs(x), list(range(1,rank)), keep_dims=True)\nwhich takes the maximum on all axes except the first (batch) axis.\n2) In the formulation of the paper we need the stochastic noise parameter in the open interval (-0.5/(2^k-1), 0.5/(2^k-1)), but in tensorflow, tf.random_uniform return a random number in the left-close-right-open interval.\nWhen the left endpoint of the interval is reached, we'll need to quantize -0.5/(2^k-1), and this may lead to a negative output, depending on whether round(-0.5) is implemented to be 0 or -1. Although this scenario is unlikely to happen, I clipped the value here only for safety.\n. Don't know which line are you talking about, but in general the code should work for any n-dimensional tensor, where the first dimension is the mini-batch dimension.\n. Usually when a mini-batch of B images of shape [H,W,C] is fed into the network for training, they are stacked to form one big tensor of shape [B,H,W,C] and get processed together. This is the case for most frameworks.\nIt's the same case for all the intermediate layer inputs/outputs. The first dimension is usually the batch dimension.\n. Yes C is for channels.\n. Yes that's right. Because it looks like there is no mini-batch (or in other words, a batch-size of 1).\n. Apart from the quantization it's just standard back-propagation. For example if your cost function is the average cost of all samples then the gradient w.r.t some weight would also be averaged across all samples.\n. Yeah I had that impression as well but I'm not sure it has to be the case.. \nAnyway, if cost function is an average then averaging the gradient is necessary by chain-rule.\n. Bias are not of a specific interest because they don't involve expensive computation, and presumably similar quantization methods would work for them as well. In our work we use batch normalization so the model doesn't contain bias term for those quantized layers.\n. Quantization is done in step 12 already. Not step 13.\nbackward_input doesn't involve anything with the activation function because it is handled in step 11.\nYou can use whatever bitwidth large enough to store these intermediate values because they only get involved in cheap operations.\n. backwad_input computes the gradient of C w.r.t.  a_{L-1} (or L-2, L-3,...), i.e. the output of previous layer. So the next loop iteration can use it.\nbackward_weight computes the gradient w.r.t. W_l (or L-1, L-2,...).\nYes in step 11 we just assume this is a trivial element-wise operation so we didn't introduce new notations to distinguish what's before the activation and what's after. (or \"gradients\" and \"errors\" in your words). So g_{a_k} is a variable that holds the gradient w.r.t. the output of layer k either before or after activation.\n. It means applying the inverse function of the transform.. >  Is there any special reason? \nTo keep things in the same range.\n\nAlso I can't get the intention of multiplying max_0(|dr|) in the front. Can you explain little bit in detail?\n\nThat's part of the inverse function.. 1. It's clipped: \nhttps://github.com/ppwwyyxx/tensorpack/blob/8ca3ce56b1a789a21b838bb8752c2a48d25165d5/examples/DoReFa-Net/dorefa.py#L46-L49\n\nIt's explained in the paper.. They are implemented and not open source.\nbtw recently caffe2 open source some relevant low-precision kernels: https://github.com/caffe2/caffe2/tree/master/caffe2/mobile/contrib/ulp2.. You can only initialize the weights when training a neural network.. Initialized means initializing with weights from pretrained 32bit model. Otherwise they're randomly initialized.. 1. The underlying weights Wk  are always full-precision, including the initialization stage. It is quantized to Wkb to compute Wx+b or conv(W,x)+b.\nIt is full precision. After training the network, we only need to keep the quantized value but a full precision update is needed in training.\nIt is standard chain rule. There is a explicit formula to compute Wkb from Wk, where the derivative of quantize() function is already defined.\n. You cannot find it, because standard chain rule comes with tensorflow when you compute tf.gradient.\n\n(5) and (6) defines the forward and backward (i.e. the derivative) of quantize() function. This is also in the code in the definition of quantize() function.\n. \u2202Wkb/\u2202Wk == 1 is not right, because Wkb != quantize(Wk).\nWkb = f_w(Wk) defined in equation (9) so your second strategy is correct.\n. The derivative of max function is a zero vector but the position of the maximum is 1.\n. d quantizek(ri)/dri = 1. This is correct. That's exactly how we define the function quantize_k in (5) and (6).\nmax() actually does have a gradient. It varies with respect to the maximum value. You might need to take that into account.\npython\na = tf.placeholder(dtype=tf.float32, name='a', shape=[4])\nb = tf.reduce_max(a)\nc = tf.gradients([b], [a])[0]\nwith tf.Session() as sess:\n    v = np.asarray([1, 2, 3, 4], dtype='float32')\n    print sess.run(c, feed_dict={a:v})  # 0, 0, 0, 1\n. I wrote the code to show what the gradient is.\nSTE can be thought of as a function whose derivative is defined differently, so that gradient can propagate better.\nI don't understand your argument on expectation. But the function quantize_k, mathematically, has zero gradient almost everywhere: http://www.wolframalpha.com/input/?i=round(x+*+7)+%2F+7\nWe define the gradient to be 1 so that it can be trained.\n. max(x) is an amount that depends on x, more specifically it depends on the maximum element in x.\nThat's why the gradients would be 0,0,0,1. At least that's what's implemented in tensorflow.\nSince this gradient is just a subgradient, treating max(x) as a constant and use 0,0,0,0 may also work in training. But I cannot guarantee that.\n. Yes. It works for optimization purpose and this is what is implemented in most frameworks. Technically this is not a derivative anymore, but a subgradient.\n. With subgradient, the result can actually be any number in [0,1]. \nIf you modify my code example above to try you'll see if input is [1,2,2,2], output will be [0,0.33,0.33,0.33]. But that's just a choice by tensorflow. Other library may have different choices. It's just people usually don't use zero because it's always better to have some gradient..\n. Yes.\n. 1. the input to the max function is full precision, so yes.\n2. yes.\n. This looks OK to me. Nevertheless you can easily compute some numerical gradients to check if the final formula is correct.\n. Reopen #27 .Closing this.\n. I don't understand what you're asking. What exactly do you want to do?\nIf you just want to see it play Breakout, use run-atari.py as the instructions in the readme and it'll produce a directory with videos.\n./run-atari.py --load Breakout-v0.tfmodel --env Breakout-v0\n. 1. There is a symbolic-link common.py in the OpenAIGym directory, it looks like you lost that file somehow.\n2. the command readlink -f may not work on MacOSX. You just need to use\n   export PYTHONPATH=$PYTHONPATH:/absolute/path/to/PE11/gym/tensorpack\n. If you had the file but it's not pointing correctly, then either your filesystem doesn't support such link, or you have copied the project to other filesystem (which doesn't support link) before.\nIn this case you can just copy the file from Atari2600/ to here.\nThis has nothing to do with python.\n. The error is saying that the output file gym-submit/* already exists. If you delete the directory then it will run.\n. Yes, one major assumption in this framework (right now) is that you have one cost function to optimize. So joint training of multi-task is easy, but alternative training and GAN style training is, although possible, a bit hacky. I'll think about what a proper abstraction is, to allow a model with multiple costs, but here is what can be done now:\nTwo DataFlow is equivalent to one DataFlow with an indicator, e.g., you have a dataflow which produces\n[data1, label1], another produces [data2, label2]. Writing a DataFlow which produces [data1, label1, data2, label2, True/False] is sufficient for the model to know which cost to use and how to compute it. Then you can setup a model with 5 InputVar (the last is a tf.bool) and use cost = tf.cond(indicator, get_loss1, get_loss2). \nThis should work fine in terms of computation cost, because tf.cond has lazy evaluation of its arguments fn1 and fn2, so you won't waste any computation, if you write something like:\n``` python\ndata1, label1, data2, label2, indicator = inputs\nno code should be outside of the loss functions\ndef loss1():\n    return whatever_model_with_cost(data1, label1)\ndef loss1():\n    return whatever_model_with_cost(data2, label2)\nself.cost = tf.cond(indicator, loss1, loss2)\n```\nHowever, the hacky part is that the mixed DataFlow has to produce some garbage data for the unused inputs, and pass this garbage data to the graph. Because with tf.cond, the graph would still assume the cost depends on both loss1 and loss2 and will require the user to feed some inputs for both losses. \nSo the only inefficient part is generating and feeding garbage data for the unused inputs. If the inputs for your different tasks happen to have the same shape (or just same rank) you can use the same InputVar for both tasks, then there will be nothing inefficient. Otherwise you can define your InputVar with partial shape and only feed the minimum possible tensor to reduce the data feeding cost (although the cost won't be significant), but this is just not very elegant.\n. You want different tasks for different samples in a batch, not different tasks for different batches. This is completely different.\nTo do what you want you can use tf.select on the two cost vectors. And it'll always compute both costs, because you cannot only compute part of a tensor, not in any existing libraries. Alternatively you can split your batch to two based on the indicator,  compute two costs and return a sum.\n. If you want different tasks for different samples, then your cost should certainly be a vector of shape (?,) as well. tf.select only works for tensors of the same shape.\n. Now you can do alternated multi-cost optimization by implementing the trainer logic in a separate trainer class. See examples of Generative Adversarial Networks.. Yes I saw that as well. Haven't got time to investigate that but it seems to me this complaint is not harmful. You should still get the output.\n. Code is updated. The code earlier has some bug on performance. The updated code now should produce the numbers I mentioned above (or better).\n. The total batch size is fixed so it shouldn't be very sensitive to the number of GPUs.\nSome of the models are trained with 2 GPUs and some with 4. I couldn't remember which is which.\nWhat's the number you get?. On imagenet larger batch usually help improve the model. If you made it smaller to fit on your GPU it may get worse.\nI'm just using the batch size 256 used by fb.resnet.torch. It probably will train better with a larger batch size.\nThere're some other discussions here about batch size. They tried larger batch size.. Sure it won't be hard to make the changes. But meanwhile I'm also curious about what your use case is and see if there's a better solution for that.\n1. To run things more frequently you can change the epoch size, or use PeriodicCallback to adjust the frequency. Unless you really want something to run very frequently (every several iterations), and I wonder what that could be.\n2. What do you mean by tensor statistics? before the train_op seems no different from after the last train_op, unless you're using the less efficient SimpleTrainer which loads data between two calls to the op.\n3. As said above, you cannot accurately test the time of data loading by this two callbacks, because it is supposed to run in parallel to train_op. \n   To test the speed of your dataflow, you can either write a small function which simply loops over the dataflow, or do TestDataSpeed(dataflow, size=1000).start_test() which does the same thing.\n   The iteration speed given in training should be the speed of train_op, unless the data is slower, in which case you can use FakeData instead to obtain a speed measurement of train_op. And what do you mean by other_run_ops?\n. I. I'm still not sure.. do you mean \"if the loss is small enough, then don't train on this data point\"?\nIf that is the case, you can probably just implement this in the graph, with tf.cond and tf.stop_gradient. But  maybe doing something on dataflow to not feed such datapoint would be nicer.\nII. So you are using SimpleTrainer. And then you can get a reliable benchmark of train_op and data from the loop body. But this kind of benchmark won't work for any other queue-based trainers where data and train_op happen in separate threads.\nAlso, dataflow or tensorflow might need some time to warm up, so I'd just use TestDataSpeed to test the data and FakeData to test the graph, for a couple of hundred iterations.\n. I. It is definitely already computed in that way. But currently the only way to access this information is through the graph (or summary). I was planning to allow train_op to support some output tensors (and accessible through trigger_step()).\nII. For queue-based trainer, the timing is done of the body loop. It's just in a multi-queue pipeline, there is no way to accurately benchmark each component while the whole pipeline is running, because every component waits for others. I can add some statistics of, for example, the occupancy of the queues which reveals which component is the bottleneck. This is also used in some of the tensorflow official examples. \n. I. You can just use self.cost in the graph however you like (print it with tf.Print, skip training conditioned on it (with tf.cond and tf.stop_gradient). It is the cost of the current datapoint before backprop happens.\nMaybe I lost some context here, but you don't need trigger_step to do this.\nIf you want to write some Python logic based on the loss, now you can do it by the new Callback interface with trigger_step:\npython\ndef _extra_fetches(self):\n    return ['name of the cost tensor:0']\ndef _trigger_step(self, cost):\n    print(cost)\nII. (i) The queue occupancy statistics was added for quite a while.\n(ii) Do you mean you want the time of the minimization op only, but don't want others?\nFor efficiency, the other ops that you need in each step (like the callbacks I drafted above) are run together with the minimization op in one sess.run call, so the time alone cannot be measured.\nThe time of the sess.run call and the time you process the data (the trigger_step method) can be measured separately. Is that really necessary? I'm expecting trigger_step should never do anything heavy, and you can tell whether something is heavy or not from the training speed already.\nIII. Now you can use your own timer in trigger_step. I even implemented the progress bar as a callback (which means you can use a different progress bar). But as I said, you'll only be able to measure the total time of running the graph + running all the trigger_step from all callbacks.. A ResNet-18 model with (W,A,G)=(1,4,32) should get 60% accuracy. But the training was done in a private framework and was not converted to tensorflow.\n. We've released the 1,4,32-ResNet18 model on DoReFa-Net page.. The training was done not in tensorflow. There are some differences in the definition of Ops and also in architecture, e.g. we forgot to do the average in global average pooling (divide by 7x7) when we trained the model. \nThe current model file is just an equivalent in tensorflow. We expect that these small differences won't make a big difference to the model, but ideally a clean model should just remove those hacks.. I've tried the original architecture from DeepMind (no pooling), not much difference.\n. 2.6.1 is OK.\nThe function is straightforward:\npython\ndef get_caffe_pb():\n    dir = get_dataset_path('caffe')\n    caffe_pb_file = os.path.join(dir, 'caffe_pb2.py')\n    if not os.path.isfile(caffe_pb_file):\n        proto_path = download(CAFFE_PROTO_URL, dir)\n        ret = os.system('cd {} && protoc caffe.proto --python_out .'.format(dir))\n        assert ret == 0, \\\n                \"caffe proto compilation failed! Did you install protoc?\"\nMaybe it failed to download the file successfully. Or maybe protoc isn't in your path. You can also run protoc caffe.proto --python_out . yourself to fix this.\n. https://github.com/google/protobuf/issues/592 Does this help?\n. It works for others because caffe.proto doesn't have BOM.\nWill you check your caffe.proto is downloaded correctly? Maybe it's not BOM but other unexpected characters.\nI also tried 16.04.\n. Broken proto file. Closed.\n. It should support python 3 already, except for some of the examples maybe.\n Please report if you find it incompatible with python 3.\n. Probably that's top-5 instead of top-1.  I just found a typo in the code which only prints the top-5 error rather than the top-1 error. Will fix soon.\nTop-5 error of 30% at epoch 48 is normal. \nFixed in 95b6437a.\n. the statistics has a different name for this config.\nyou might need to use something like jq '.[] | .train_error_top1,.\"val-top1-error\"' instead\n. The code is here: https://github.com/ppwwyyxx/tensorpack/blob/master/examples/OpenAIGym/train-atari.py\nAnd someone just wrote a paper to explain this design: http://openreview.net/pdf?id=r1VGvBcxl\nAlso see #20.\nFinal performance is roughly the same.\n. I can train resnet101 with total batch size of 256 on 4 TitanX.\nI haven't trained 152 myself yet.\n. Sorry I was doing some significant changes in the design and it looks like some part was broken.\nI'm preparing a fix and meanwhile you can switch to some earlier commit e.g. efbf256e85eb9 to play with it.\n. It looks like the latest 775f5c9ad96846e20 is working. Please let me know if you encounter further error.\n. Possible reasons:\n- Models are not mathematically the same. For example, with certain shape tensorflow may have a different padding mechanism from other frameworks such as caffe.\n- floating point arithmetic. e.g.\n  - (a+b)/c != a/c + b/c\n  - a * b + c with FMA instruction can have different outputs compared to simply doing multiplication and add.\nIf there is no problem with (1), then I'm not surprised to see a absolute difference of  ~1e-7 at first several layers and ~1e-5 at some last layers. If the error is very large you can check it layer by layer to see if some layer is mathematically different.\n. If you are using output_var_names to specify which output you want to use, you can simply print E to find out the name of the tensor E so you can run inference on this tensor.\n. I believe np.float is np.float64. And when you use different floating point precision this behavior is not a big surprise.\n. With floating point a+b+c != a+c+b, especially for large numbers. When there are millions of numbers it might be the case. But I also think the difference here is quite large. \nI'm closing this and you can track the relevant issue in tensorflow: https://github.com/tensorflow/tensorflow/issues/5527\n. tensorpack assumes you're only using flloat32, because higher precision doesn't improve models.\nIf you really want to do so you can monkey-patch tf.get_variable and add dtype=tf.float64 whenever dtype is None.\n. You should see the progress bar moving immediately after seeing this. It's working for me with the current head.\nIt's hard to tell what's the reason. You could look at your output to see if there is any error message.\n. btw, the problem could happen if your PWD is on a non-local filesystem (NFS, glusterFS, etc). That's the one thing I can think of now.\n. This library doesn't require GPUs. \nIf you're talking about any specific examples in the project, usually the --gpu argument is optional.\n. Yes. a3c didn't support running on CPU. \nThe latest commit 6087698d4c8d2e859cbe now added the support . \ni7 skylake is about 50x slower than TitanX.\n. FYI, though it can run on CPU, I found it didn't produce reasonable score.\nI guess it's because of the async update issues -- with everything running at slower speed the data I feed may become more outdated and affect training. If I add some random time.sleep in the code, even with GPU it learns nothing.. What do you mean by not working fine, and why do you choose such quantization equation? The equation looks rather arbitrary and seems like a lot of the entries would end up being zero.\n. The E used here is derived for binary quantization and it seems like there are different equations for ternary cases in the paper.\nYou can write a small snippet to compute the output/gradient and see if it is expected. The gradient looks wrong to me. You probably need to override the gradient of clip_by_value and the division to identity.\n. clip_by_gradient is implemented by a Minimum and a Maximum.\n. Identity takes one input and min/max takes two, so they are not consistent and you cannot use the gradient of Identity directly. You'll need to write another function which implements the gradient of identity and register it.\n. Thanks @Junsong-Wang! Although not the same as the paper, this looks very smart.\nI implemented the quantization following TWN paper today, inspired by @thadpasce16. \n```python\ndelta = 0.7 * tf.reduce_mean(tf.abs(x))\nmask = tf.logical_or(x < -delta, x > delta)\nmaskf = tf.cast(mask, tf.float32)\nx_side = x * maskf\nWl = tf.reduce_sum(tf.abs(x_side)) / tf.cast(tf.count_nonzero(x_side), tf.float32)\nmaskf = tf.stop_gradient(Wl * maskf)\nwith G.gradient_override_map({\"Sign\":\"Identity\", \"Mul\": \"Add\"}):\n    output = tf.sign(x) * maskf\nreturn output\n. The gradient is already cut by the other `stop_gradient` call. Adding `stop_gradient` to the beginning changes nothing.. @thadpasce16 has an implementation in tensorpack. I haven't got time to test it but I was told it had good results.. It's just the configuration we happened to be using. There are no specific considerations.. If your image has 3 channels then the input `state` should have 3x4 channels (4 history).\nBut somehow you're feeding a `state` of 16 channels.. You should add the path to this repo to PYTHONPATH, as said at the end of [README.md](https://github.com/ppwwyyxx/tensorpack/blob/master/README.md).. Roughly yes.... it sounds like you're not very familiar with command line and variables.\nBasically you did `export PYTHONPATH=$PYTHONPATH:/ABC/DEF/G` where \"/ABC/DEF/G\" is the absolute path to the whole directory you cloned. `readlink` just helps you find it and you can ignore that part.. `export PYTHONPATH=$PYTHONPATH:/ABC/DEF/G` where \"/ABC/DEF/G\" is the absolute path to the whole directory you cloned, before running the program.. `echo $PYTHONPATH` should contain `/opt/tensorpack`, not `/opt/tensorpack/tensorpack`.. The project contains symbolic link. It looks like you're using Windows, or using a filesystem which doesn't support symbolic link.\nIn that case you need to copy the file `examples/Atari2600/common.py` to `examples/OpenAIGym`.. It will affect. You need to copy the file like I said.. You don't have permission to write in the `examples` directory.\nThat's likely because you're using virtualbox and put things under `/opt`.\nYou can use a different directory such as the user's home directory.. It shouldn't complete training without running for days.. yes.. There are no changes in this PR. Is that a mistake?. The model is not for `load-resnet.py`. It is the trained model by `imagenet-resnet.py`. They are different variants of ResNet.\n`load-resnet.py` is \"A script to convert and run ImageNet-ResNet{50,101,152} Caffe models released by Kaiming.\" You'll need to download the caffe model and use the converter to build a npy model, as mentioned in the readme.\n. It should be in `train_log/train-atari`. It is saved every 6000 iterations. On a decent machine it should run 6~10 iterations/s.\n(I've removed your original comment under the gist).. It should be something like \"model-xxx\". There won't be `.tfmodel` suffix.\nThe model will be saved after each epoch.. We haven't used it for object detection tasks.\nWe have applied it on [semantic segmentation](https://arxiv.org/abs/1612.00212) and [RNN](https://arxiv.org/abs/1611.10176).. Thanks for the fix!. Thanks. This is a bug of tensorflow: https://github.com/tensorflow/tensorflow/issues/5888\nBefore the upstream fixes it, you can use an earlier version (0.11rc2 seems to work, but I'm not sure).. You can also change `tf.round(xxx)` in `dorefa.py` to `tf.floor(xxx + 0.5)`. As in the readme:\nWe're not planning to release those runtime bit-op libraries for now. In this repo, bit operations are run in float32.\n. Yes. This code was just meant to be a proof of effectiveness of our proposed quantization method.. If you look at the code of `fw`, it uses binary-weight-network which includes a scaling factor on the binary values. Similar technique is also used in XNOR-Net.. quantize() with 2 bit would return values in {0,1/3,2/3,1}.. k bits can represent 2^k different numbers.\n2 bits can represent 4 different numbers.\nAnd we choose those numbers to be {0, 1/3, 2/3, 1}.. `tf.summary.histogram` is added very recently to tensorflow. I think you're actually not using r0.12rc0. Could you double check your version?. I'm using 1080 and it could get 9it/s.\nCould you double-check it is actually using GPU?\nYou can look at the log to see if it really detects the gpu, or run nvidia-smi while running, to see if gpu is being used.. You should look for logs from tensorflow about which device it's actually using, such as the following:\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\nname: GeForce GTX 1080\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7335\npciBusID 0000:02:00.0\nTotal memory: 7.92GiB\nFree memory: 7.81GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y\n```\nAlso, if your cpu is heavily occupied it will also not run as fast.. Another thing that's probably related. nvidia-smi shows that your GPU is in P8 state. Normally (if autoboost properly configured) the GPU would automatically change to high-performance state when some heavy jobs started. P8 is the low-power-low-performance state. You should expect it in P2 at least.. It says 80M steps, and by steps I believe it means 1 action in the game. But here 1 iteration is a batch of 128 steps.\nThere are also differences in the model/input state, etc.. Sorry but we're not going to release the implementation in very detail. But in general you wouldn't see bad performance as long as data is loaded in a cache-friendly way and all computation is performed in SIMD instructions.\nThis issue is not very related to tensorpack so I'm closing it for now. If you have questions about the paper please write to the authors.. I pushed a fix just now which uses an older version of BN when it couldn't find fused_batch_norm.\nBut still I'd suggest updating TensorFlow is possible.. 0.12 rc0/rc1\u662f\u6709\u7684\uff0e\u5e94\u8be5\u662f\u6587\u6863bug.. It could be a GPU problem\nIt could be that you're using distributed file system.\nOr maybe your dataset directory is incorrect, then you'll see some errors during initialization.\nYou can try simpler examples (e.g. mnist, cifar) and see what works and what doesn't. You can monitor your gpu/cpu/memory usage. Otherwise there is no way to tell what's the reason.. It looks like you're using your own modified script but not any of my examples, and it sounds like my examples run correctly.\nSo it's likely that you didn't implement the DataFlow correctly to produce your data, or did some other modifications which break the program.. You should just write the DataFlow d such that:\nfor dp in d.get_data():\n    print len(dp)\nworks at a reasonable speed. You can implement it any way you want.\nI cannot debug your code without seeing any code.\nAlso, you'd better also run at least one a bit complicated examples, such as svhn-digit-dorefa.py , to make sure that it's not a problem on the library side. mnist might be too simple.. Have you confirmed\npython\nds = IDCard12('xxx', 'train', shuffle=True)\nfor dp in d.get_data():\n    print len(dp)\nworked or not?\nAlso since you're using mini batch, the above code should print more lines than the batch size.. sorry. You need to add ds.reset_state() before the loop. Could you try again?. Thanks. The dataset itself looks good. You also need to make sure it produces enough data (>4*batch size at least).\nThere are other problems as well, e.g. you kept the slicing for imagenet mean, which will make your mean empty. You read greyscale images but you didn't change the channel of InputVar.. Oh wait.. do you mean that your data stop printing after your screenshot?... That's something you can debug on.. I see. How did you install your opencv?. It should be relative path if you read your own data code. But abs path could also work.\nThe problem is probably with opencv. It is found before that using a prebuilt opencv would cause tensorflow to get stuck.. You can confirm that first, by first import cv2 and resize an image, and then execute a tf operation.. Sorry. you should use absolute path. I see you have commented out my os.path.join.. anaconda search -t conda opencv show me a list of (non-official) opencv 2.4.12 built. Could you be specific about which one you use?. I see. Also probably you cannot easily reproduce the stuck with several lines of code -- otherwise you wouldn't be able to run svhn examples..... You seem to be still keeping the line pp_mean_224 = pp_mean[16:-16,16:-16] which will slice the tensor to empty in your case. This is for imagenet only.. I found a weird thing that anaconda python doesn't throw exceptions when it should, it just silently failed.\nI can reproduce your problem if I use an empty mean and anaconda python. It did throw exception from the data loader with a normal python environment, but silently failed with anaconda python.\nIt's likely a bug of anaconda python. I'll figure that out later. Meanwhile, you can try the following:\nadd\npython\nds.reset_state()\nfor k in ds.get_data():\n    print len(k)\nbefore ds = PrefetchDataZMQ(xxx\nIn my tests, this would make anaconda throw the exception.. Have you fixed the channel of your inputvar already?\nWith a wrong input shape I could also reproduce your problem, with anaconda python, because it doesn't throw the exception.. Anyway, you could add print len(dp) before op.run at input_data.py, line 81. It you could see it print many 2 then the data part should be fine. . Oh you probably need [None, 32, 32] instead of [None, 32, 32, 1], because imread return an image of shape [h, w].. Normally you are supposed to see exceptions of such errors, but with anaconda we have to guess... But after that change you'll need image = tf.expand_dims(image, 3) in _build_graph to reshape it back to [None, 32, 32,1] . Otherwise the rest of the model code wouldn't work.. This gonna be very annoying.. You don't see many 2s so the feed operation in that loop probably failed , but silently.\nMaybe removing the PrefetchDataZMQ line can make the exception visible.\nOr you can try-except that loop and print the exception.. Are you saying that tf.transpose change the value of elements? You can print them to confirm this, and reprort bugs to tensorflow, with your detailed environment info. You can also try it without opencv to see if it still happens.\nI cannot reproduce any of that. And I'm still skeptical about the conda environment... By the way the exception is saying that you feed a tensor  of shape [128,32,32,32] (32 channels) instead of [?, 32,32].\nThis is because you removed my original code # arr = np.transpose(arr, [1,2,0]) in get_per_pixel_mean.. OpenCV was reading the images successfully. The problem is that you change the shape of images.\nAre you saying that after updating opencv to 3.1 (and keep PrefetchDataZMQ still there), it can throw error now?. Yes. I found that using opencv3.1 in conda can make exception visible again. It's not a problem of conda itself.\n. You must have other kind of exceptions if speed is 0it/s.\nI have tests to confirm that opencv2.4.12 in conda doesn't work in multiple processes (and therefore doesn't throw exceptions in multiple processes), and switching to a different version fixes it. I think that would be the conclusion of this issue.. As a reference, the following code doesn't work in conda opencv2.4.12:\n```python\nimport multiprocessing as mp\nimport numpy as np\nimport cv2\nif name == 'main':\n    def f():\n        v = np.random.rand(299, 299, 3).astype('float32')\n        v = cv2.resize(v, (64,64), interpolation=cv2.INTER_CUBIC)\n    v = np.random.rand(299, 299, 3).astype('float32')\n    v = cv2.resize(v, (64,64), interpolation=cv2.INTER_CUBIC)\np = mp.Process(target=f)\np.start()\n\n```. Hi, thanks for your PR!\n\n\nYour implementation, with BITA=32 and no fg, is not DoReFa-Net. It's just BWN (Binary Weight Network). It's still good if we can have a BWN example, though.\n\n\nWe don't include examples without a meaningful performance number (except for examples which illustrate a different type of problem), otherwise this would be just a combination of code snippets from other examples, which doesn't really teach others anything new.\nBy \"meaningful\", it should either be a state-of-the-art result, or a reproduced result of some paper. If you could reproduce the Cifar performance in BWN (binarize weights only), or BNN (binarize both weights and activations), this would make it a good example.\n\n\nSame for ImageNet. We currently have a ResNet-18 model on ImageNet with 1 bit weights, 4 bit activations and 60% accuracy and it might get released soon. You can contribute if you have a similar performance with ResNet-18.\n\n\nAbout the architecture, we chose the other ResNet variant which uses convolution to increase channels instead of pooling, so we don't have such problems.. The only layers get quantized in DoReFa-Net is Conv and FC.\nIn general fg is added directly after Conv (without any non-linearity) and fa is added directly before Conv. Then both forward and backward get quantized.. It is ILSVRC12. It has train,val and test. The val images from ImageNet website are in one big folder originally. \nOther repo also confirms this, e.g. https://github.com/soumith/imagenet-multiGPU.torch.\n\n\nI didn't use the test set.. 1e-4 or 1e-3 is usually good for Adam optimizer.\nbatchsize/optimizer/learning rate are all in the example. You can just change them.. 1. regularize_cost is designed only for regularizing variables, so it doesn't work for activations.\nTo regularize activations, you can use l1r = 0.001 * tf.nn.l2_loss(l1), and then add l1r to cost.\n\n\nget_variable with regularizer option would add the loss to a collection. Then you would still need to get that collection and add it to the total cost. \nAllowing this option would require most layers in tensorpack to have extra arguments (W_regularize, b_regularize, etc) and this doesn't make things much easier. I think regularize_cost with regular expression is OK.\n\n\nThere are a lot of things that cannot be done with LinearWrap. It's just a syntax sugar to use when your model is very simple. And regularizing activations is not a common things people do, so there is no functionality specifically designed for that.. Issues are for potential problems with this library. Since you're training with your code, there are a lot of things that could go wrong in your code and we don't have time to look through it.\n\n\nIf you have 6k classes why do you think the error rate you got is not good? If you have a better model of some other architectures, I'd suggest making changes starting from that model.\nFrom the information you provided, there are also issues in your architecture that are not consistent with how DoReFa-Net is designed. You quantized the activations before bn0 which doesn't have much run-time benefits but hurt performance. Activations before fct is also quantized but we prefer not to do this as discussed in the paper.. What was the problem?\n. Version is the only change?\nHow did you install opencv?. So it is cv 2.4? Is it 2.4.11 or 12?\nIn the first message you said that your cv is 3.1.0\nAnaconda packages are built by third-party and could have problems. \nThis one is similar to #68. But here I assume you didn't modify any source code, so it's not a problem of exception.. TF0.11 and cv2.4 works fine for me. It's probably just a problem of anaconda packages.. I see.. Now I updated the version. Thanks!. An augmentor takes one item and produces one item only.\nIt can be made a simple utility function, or a dataset adapter. . You're right that it does not fit into the scope of augmentor. That's why it's better to implement it into the data pipeline.\nFor instance you can use MapDataComponent to transform each 3D image to a 4D tensor with crops.. Normalize by a factor doesn't change the sign at all.\nThis normalization is only to make auto-differentiation be consistent with the paper.. It's already quite easy to do transfer learning, compared to other frameworks. It's just a --load.\nThere are automatic ways within the framework, e.g. you can define both models and use an input to choose which model to use. But this is not easier.\nIf you just don't want to stop in the middle, you can write them together, with something like:\npython\nTrainer(config1).train()  # max_epoch=15\nTrainer(config2).train()  # sess_init=....\n. The idea is that config2 is different from config1 (it's a different graph). And config2 contains the loader of the model saved by trainer1.. This was actually intended, but yes it might be a bit misleading. Fixed in eb11e29c09fdec776682da9.. 1. These are tensorflow Saver V2 format. You can load it by --load path/to/model-25000. See https://github.com/tensorflow/tensorflow/issues/6142 for more information (and some caveats).\n\n5000 is just a random number I chose. Regardless of this number, all images will be trained because:\nImageNet data get shuffled every time it is exhausted.\n\nEven if it didn't do the shuffle, the data didn't get reset after every epoch, i.e. in the next epoch, the first batch will start from the 128000th image.. Of course you can. Currently the default directory is set by logger.auto_set_dir. You can delete that line and use logger.set_logger_dir('some/directory').. 1. This script assumes images are all 256x256. In training, all training images are resized to 286 and cropped to 256 (get_data()) following the exact setup in the paper. Therefore to use the trained model, you'll need to resize the images, or retrain with 400x400 resolution (modify the SHAPE variable and the image preprocessing in get_data).\n\n\nFor Tensorflow SaverV2 format, you'll need to pass /path/to/model-last to the saver, instead of the .index or .data file. (See #79). \nI don't like this feature of Tensorflow either, so in one recent commit (f1fdb42e2f3d8), the loader in tensorpack now automatically replace model-last.index with model-last. If you update the code it should work fine.. Of course. In line 187 of Image2Image.py:\npython\n        o = o[0][:,:,:,::-1]\no would be a tensor of shape (6, h, w * 3, 3) containing 6 BGR images. Each image is like the visualization I put in tensorboard: it's (input, ground truth, output).\n\n\nYou can delete the line 188 viz = next(build_patch_list(o, nr_row=3, nr_col=2, viz=True)) which opens a window for visualization, and add extra code to save the 6 images manually to somewhere. . That looks like an RGB-BGR issue.. I think it's important to keep things controllable by the users -- at least the entire forward pass, instead of hiding it in the trainer.\nFor example, BN updates is not necessarily applied before the cost tensor: in my implementation it is applied inside BN layer. If UPDATE_OPS collection is used for some other purposes, differences like this may be important.\nYou can provide very strong utilities function such as apply_tflim_collections, or a subclass such as SlimModelDesc which automatically applies these in get_cost, to simplify what an user has to do. But I'd still like the users to enable them explicitly.\nThis may make it a bit complicated for the users. But as long as there is a flexible enough backend, an easier wrapper can always be built upon it.. I see. Now I'm accepting the idea of applying the two collections by default, because:\n1. Nothing different when the two collections are not used\n2. We can assume these two collections are only used for these two purposes\nBut still, I hope the trainer keeps as simple as possible. An alternative way is to apply these collections in ModelDesc.get_cost() by default. This way it goes into models, and users will also have a way to disable it (by override the function). What do you think of this?\nEDIT: this also gives a nice boundary: all forward pass defined in Model, and trainer does the backward pass and update.. LGTM. I'll merge it soon. Thanks again! \nMy idea is that the existing framework provides Trainer and ModelDesc for the most common use case (single cost optimization). For tasks more complicated than that I'll expect users to write some of their own code -- it'll just be too hard to write anything general.\nFor example, for GAN if you want to help users handle the collections, there is going to be a lot of issues about which cost should the collection be applied on -- ideally those appeared in generator should be on generator cost, and those appeared in discriminator should be on discriminator cost. But for extensions like mode-regularized GAN, there is an extra \"encoder\". For variants like Image2Image, BN statistics doesn't need update at all.  -- These are situations that cannot be simply addressed by an abstraction, so I'd rather just provide utility function and let users call them.\n. I'm still not following why do you need the scope.\n1. In tensorpack, get_cost was never called under a variable scope other than the default scope (with name == ''). In multi-tower training it is called under different name scopes, though.\n2. In multi-tower training, some of my tests show that the regularization losses won't get repeatedly added to the collection, so there is not a need for de-duplication. You already uses ctx.is_main_training_tower, so UPDATE_OPS is well-handled in multi-tower setting.\nBTW, looking at the TF code it looks like tf.get_collection accepts scope of string type.\n. Anyway, it should work already. I guess I can merge it first and look at the issues later.. At some time I stop summarizing \"cost\" by default, and this creates a problem with slim.\nBecause gradients don't actually depend on cost, so the UPDATE_OPS were never called.\nI'll fix it soon.. I still thinks that users should handle the REGULARIZATION_LOSSES collection by themselves. Some reasons:\n1. consistent with where regularization is done in tensorpack. This is causing confusion such as #611\n2. It's not always easy to handle it automatically, especially when now we have many different multiGPU/distributed strategies I found it hard to make every scenario correct. Similar things has happened to UPDATE_OPS as well.\nAnd even if it's correct, it's not the most efficient to use the collection on every GPU, as shown by https://github.com/tensorflow/benchmarks/commit/7fc628d041fd7b7fafceccf60ba1f52448c50330. If the user can do it explicitly, this can be easily done.\n3. Also, imagine using tf.layers inside GAN -- get_cost is never used, so you have to manually process the collection anyway. This looks like a surprise to users so perhaps it's better to be explicit than to do too much under the hood.. I fixed some of the documentation issue, simplify some code, and rearranged the location of some functions. \nAnother question I'm having in mind: could it be confusing to call it saliency ? This word never appears in the paper, and the author described the method as a visualization method to find out what the layers has learned. But \"saliency\" has its own field of research in CV.. I think I'll merge it after clean-up the README a bit. I planned to use fewer images and stack images together to leave space for other type of visualizations. For example I really like Fig.3 in https://arxiv.org/pdf/1412.6806v3.pdf.\nI'm OK with the term saliency map, since the paper is using it. \nI moved the saliency function out because I was confused with the name myself. People could understand the function very easily by looking at the code alone but might get confused by looking at the name and document. Since it is well-defined in the literature we can move it back and explain it better.. Thanks! I'll run autopep8 and see what happens. My understanding is that it doesn't always do the right thing (?) so it might take some time to fix the style. It's modifying code everywhere so probably it's better I work on it.\nI'll leave this open and we can look at the CI tools after existing style issues have been fixed.\n. Thanks! I've fixed pep8 warnings across the project in several recent commits. And also added travis to run flake8: https://travis-ci.org/ppwwyyxx/tensorpack.\nNote that I had a different tox.ini for examples/ which ignored warnings about import *.. I was making changes on import and introduced a bug.. 1. In sample(), just append conv8/output to output_names. Then o[0] would still contain 6 RGB images, and o[1] would be 6 e8 tensors (of shape 6x512x1x1).\n\n\nYou can look at DCGAN-CelebA.py which uses an input z vector.\n. Sorry I misunderstood your question just now. I thought you want to use a random z vector.\nYou can inject your own e8 vector, but the generator still will need e7 ~ e1 so you didn't bypass the conv. I expect changing e8 won't make much difference. \nThe simplest way to try it out is just to replace e8 in the code by something like e8 = tf.constant(xxx).. I've no idea what's causing the difference. Closing this for now.. You can use tf.select(mask, cost, tf.stop_gradient(cost)) to mask out some samples.. 1. Loading a pre-trained model is just simply --load model handled in most examples. --load will try to load all parameters with the same name and print warning about the ummatched weights in the model, so to re-train some layers you should change their names before loading. HED is such an example which loads pre-trained vgg..\n\n\nFAQ about freezing variables.. https://tensorpack.readthedocs.io/tutorial/faq.html. Yes that would be nice! Haven't got time to look into that.. Now at\nhttps://pypi.python.org/pypi/tensorpack. Just the bin file.. It is not guranteed that dataflow produces datapoints of the same shape. In tasks like image segmentation, speech recognition, NLP, data can be any shape. So printing the shape of the first data doesn't make a good sense. . Yes it would be good, but it's not general enough to make it a default inside trainer because there are cases it doesn't make sense.\nYou can implement it by adding a dataflow adapter, which prints the shape of the first datapoint. This way users can choose to use it, by e.g. (ds = PrintShape(ds)). This also avoid modifying code for every InputData type.. Thanks for pointing out. It's working in HEAD now.\n\n\nI still have a bunch of old code under my private examples/, so I haven't setup a pre-commit hook for it. Will make that later.. Thanks a lot and especially for the detailed documentation!. There is a plan to more generally support trigger_step() with user-specified data. Some discussion were in #39. The idea is that a Callback should define what it needs every step and what to do with it, not only to print them. And then the trainer would fetch those tensors every step and pass them to the callback.. It will not need an extra sess.run. The trainer should know what all callbacks need, and fetch those tensors together with train_op in one sess.run.. In fact I'd prefer no logic been implemented inside trainer if they can be done outside. The current epoch-wise logging is also done outside.. I can consider an option to disable tqdm, so that users can have more control over what to display within steps.. Please leave the PR as-is or close it for now. We'll revisit it and can adopt the live data monitor when trigger_step is ready.. Now that tqdm is a callback, this can probably be implemented as a callback as well. And then users get to choose what to use.. Do you think this tqdm feature works for the case here?\nIf so we can make it an extra option in the ProgressBar callback.. What do you mean?. I started to use all for all module to work with sphinx api doc. This was causing some problems with import.. Why does the import error happen?. fixed in HEAD. sorry... __init__ usually should do nothing interesting because it is executed even before you had a TrainConfig.\nTo access the session you can use self.trainer.sess inside Callback. This is available after (included when) setup_graph is called.\nThe session is the default session in before_train, trigger_epoch, after_train, but not in setup_graph (because when graph is not finalized there is not a valid session).\n\nAbout your task, if you want to do prediction, probably it's easier to use trainer.get_predict_func. It will return an OnlinePredictor in a separate tower with is_training=False. But if you don't want that for some reason you can certainly create the OnlinePredictor yourself, in either setup_graph or before_train.. Sorry about that. Fixed in HEAD.. This is normal.. Closing since this is a duplicate of #36.. I didn't know that would happen. I was ignoring this error all the time and things are fine.\nMaybe there are some changes in gym that make this error matter? I'll try it when I have time.. Thanks for the rich explanation! I'm OK with merging these now, but please know that I may rearrange/modify glance.md a lot in the future, probably split some of the content into separate pages.. Windows doesn't support ipc communication: http://stackoverflow.com/questions/15386121/does-zeromq-support-ipc-as-a-transport-channel-on-windows.\nTo make it work you'll have to use tcp. The invalid argument error is probably because you didn't write the address correctly. It should be something like tcp://127.0.0.1:1234.\nUsing tcp, it probably will run as slow as the python multiprocessing module. So maybe you can just use PrefetchData instead of PrefetchDataZMQ.. inproc wouldn't work at all. Regarding the error, are you using any of the examples or have you modified the data? It shouldn't try to pickle the augmentor. Do you have an stack trace of the first error?\nBtw, if you are running Image2Image.py, maybe just use PrefetchData(ds, 100, 1), or just remove prefetch completely. Dataflow is not the bottleneck here. . Yes it's alright. The first one was fixed in 6d67faf93a7 today. The second one is because tensorflow changes their API recently on master: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard5/tf.nn.sigmoid_cross_entropy_with_logits.md. @Neltherion You can use for example PeriodicCallback(ModelSaver(), 10) to let ModelSaver get triggered less frequently, if you feel that models are taking too much space.\nA better solution will be there after step-callbacks are ready.. @Neltherion  Now you can use PeriodicTrigger(ModelSaver(), every_k_steps=100, every_k_epochs=1) to trigger the saver every epoch, and also every 100 steps (the counter was reset every epoch).\nAlso, ModelSaver has some other options to automatically delete old checkpoints to save disk space.. Don't worry that's a bug of mine, should be fixed already.. Thanks! I'll run it some time to figure out what's happening. Some general comments:\n1. I use --gpu everywhere only because it's shorter than CUDA_VISIBLE_DEVICES, but I hope not to provide a set of predefined \"standard\" arguments, at least not now. That reminds me of caffe where I'll need to make some changes in code, some in config file, some in command line.\nIt will look better to me if the interface is different, such as one line of enable_common_flags(), to make it clearer this is optional. But one single line of os.environ['CUDA_VISIBLE_DEVICES'] is probably not worth the complexity to introduce new utilities either.\n2. There is no need to use variable_scope and the reuse option, because there are no variables defined in those functions. (If there are, I would put them in models/ instead of symbolic_functions.) You probably want tf.name_scope.\n3. the argscope from slim would make things complicated because I had an argscope for models/ already. And seem like you're not using them anyway.\nI think argscope only helps when you need to call a function with long argument list for multiple times. This is usually not true for what's in symbolic_functions.\n4. animate.py introduces too many dependencies, but putText and VideoWriter in cv2 should be enough (may not support gif format specifically, though). But if I were to write it, I'll just write one function which produces animation from a iterator of images. I wouldn't assume people want to put text at (0,0).\nAlso, if you just use it to create animation from files, ffmpeg might be simpler.. About the naming in examples, I started to use ucfirst for directories simply because I found a lot of them starts with abbreviations.. like HED, GAN, A3C, DQN. So I used uppercase for all of them.\nOther files are lower-case unless they need to be imported (then it has to be lower_case).. Another question, is there a reason why eps is different? I saw 1e-12, 1e-10 and 1e-6 in the file.\n1e-6 seems much larger and you only use it on sqrt, so I wonder this.. You'll also need to fix tf.nn.rnn, replace it by tf.contrib.rnn.static_rnn.\nMaybe it's better to write several dummy training scripts containing lots of features for testing. This may avoid downloading mnist or prepare dummy data for examples. What do you think? . I'm not sure what's the best way. Dummy data would work for some of the examples, but for others like mnist, it has to download the whole dataset. \nBut maybe we'll only test those that work with dummy data? Sounds OK.. About improved GAN, it interests me as well because it introduces a lot of new ways of training a model, so it would be good to implement them to test whether the design of tensorpack is really flexible.\nAbout InfoGAN, @PatWie could you elaborate what do you mean by \"correct log-likelihood\"? I used OpenAI code as a reference when I wrote the examples but maybe I missed something.\nAlso it would be nice to have distribution classes like the original code.. You can simply change the one line which reads the mnist dataset to two other lines which reads the CelebA dataset like the DCGAN-CelebA.py example.\nBut you'll certainly need to change your network architecture and it's impossible to do it automatically.\nI would refuse to use command line to define complicated things like a network, because it only makes things less flexible. A .py file can expose everything configurable to you, but a short command line string only has a small portion of its ability. You can certainly add the parameters you want to customize to command line options, by just one line of parser.add_argument, but I wouldn't do that in the examples because different people want to customize different parts.\nWhat's in #107 is only about InfoGAN but not improved GAN.. The xxx.meta generated by tensorpack is a MetaGraphDef protobuf.\nThe scripts expects a GraphDef protobuf.\nYou can take GraphDef from a MetaGraphDef like this:\npython\nfrom tensorflow.core.protobuf import meta_graph_pb2\nG = meta_graph_pb2.MetaGraphDef()\ndata = open('input.meta.pb', 'rb').read()\nG.ParseFromString(data)\nG = G.graph_def\nopen('graph.pb', 'wb').write(G.SerializeToString())\nI'm not sure if it will work for the script then. I don't know what that script is doing.. The uniform distribution doesn't look as good as the papers (from figure and curve), is that what you mean?\nThe current code looks good, thanks a lot for that! Could you put the changes related to improved GAN in a separate PR? Then this PR would be only about InfoGAN and easier to manage.\nThe other PR might depend on GANModelDesc so I'll try to merge this soon.. There are some problems I found:\n Naming confusions. Comparing to the original InfoGAN code, your \"sample\" is actually their \"sample_prior\", and they do have a \"sample\" method that does \"sampling from a batch of dist_info\" as one could expect from the name. Your \"entropy\" is actually \"entropy under the prior distribution\" but their \"entropy\" is different (and not used). \"activate_dist\" was OK (it performs an activation) but \"model_param\" is probably more confusing (?).\n You probably can check the static shape of input/output in the base class. For example, prior()  should return some thing with shape \"param_dim\". This would be very helpful especially when now it's confusing what these methods really mean.\n They have \"prior_dist_info\" (like your \"prior\" but returns a batch), and a \"sample\" method which takes a batch of \"dist_info\". Combining the two would give a \"sample_prior\" which is your \"sample\". This naturally solves the 1D/2D shape problem you're having as the TODO, as you can then require loglikelihood to take 2D input.\n From their code the \"prior\" of Uniform should be zero-mean, unit-stddev, and \"sample_prior\" is override to not sample from this prior. The prior in your code is different, and this will affect how \"entropy under prior\" is computed. Perhaps this explains your results.. Yes even the figure in the paper has some 7 vs 9 confusion.. I just noticed this line:\nNote, we swap 0, 1 labels as suggested in \"Improving GANs\".\nWhat does it mean? I only know one trick used here: min log (1-D) -> max(log(D)), and this is suggested by the original GAN paper.. @PatWie  Is there a reference? It doesn't sound like it will work -- it sounds equivalent. You let the discriminator predicting P(sample is fake) instead of P(sample is real) = 1 - P(sample is fake), but sigmoid(x) = 1 - sigmoid(-x) and weights are symmetrically initialized, so predicting either of them is the same.. Thanks for reporting!. They are available in the model zoo.\nBtw, I read that simpler architectures like VGG/resnet is better in terms of transfer learning than inception. . The printing only happens if you use the layers defined in tensorpack.models.\nYou can use slim but slim doesn't have the print feature.. Please see the above answer.  And there is a script \"scripts/dump-model-params.py\" which does what the above script is doing.\nAlso @nickfraser do you know why it crashed? If not could you post some logs so that I can check if it is my problem.. Could you post the error message?. OK. Btw, it's normal if there are some exceptions thrown in the end of training (after the final epoch) about ZMQ or socket.. No activity any more.. closing. Thanks. I don't have plans for that in the near future.. We don't take \"paper implementation request\". Closing now.. Can you post your logs in detail?. The CI can import tensorpack, so it's not likely to be a problem of the code.\nDo you happen to have a file called freeze.pyc somewhere? (e.g. in tensorpack/dataflow/).. I see. I'll have to write it differently to avoid importing some external packages by mistake. Thanks for finding it out.. [0121 00:22:28 @stats.py:101] max_score: 864\n[0121 00:22:28 @stats.py:101] mean_score: 543.19\nSo it already have very good score. It learns very well.\nFrom the log I cannot tell why it trains very slow. Maybe it's because the CPU is not fast enough for the simulation.. This one is only quad-core. Normally a server CPU would have 12~40 cores.. The bottleneck is probably the CPU computation. It needs to simulate dozens of games at the same time.. Something like this:\nGPU: TitanX\nCPU: Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz\nmemory: 128G. 100 is usually enough.. You can stop it by pressing Ctrl-C (maybe several times).. You also need to \"--load\" your own model instead of the model you downloaded. It should be like \"train_log/train-atari/checkpoint\". No limit is set except max_epoch=1000. If you want to stop it when it completes you have to first define what is \"complete\".. It means the model at that time was not good.\nTraining can diverge sometimes, especially on difficult games.. The env is still breakout-v0 but you should only replace the model with train_log/train-atari/checkpoint.. OpenAI changes the gym API in 0.7.0. \nYou can either downgrade gym to an older version, or update tensorpack to the latest commit.. My bad. Could you update tensorpack again?\nTo submit, you'll need to write the submission script yourself.. It seems so. But I haven't touched the code for months (and there are changes in gym as well) so I'm not completely sure.. OK it's still related to the latest change in gym monitor API. It's only submitting the first episode.\nI made another change which should make the submission work.. This won't happen if you do the submission in another script and run the submission after all the runs.. The documents only contain snippets but not a complete script, so there might be confusions.\nWhat trainer are you using? It looks like you're using \"Trainer\" , but \"Trainer\" is just a base class. You should use \"SimpleTrainer\" or \"QueueInputTrainer\".\nSince there isn't a complete script for it, you might meet other problems as well. Currently all the tutorial pages are just drafts, that's why there isn't a link to the tutorial. Before the tutorial is finished you can take a look at examples/ to find some actual scripts.. Yes the GAN trainer doesn't implement the interface so it's using the base class.\nOne simple way to make GANTrainer support this kind of inference, is:\n```\n--- i/examples/GAN/GAN.py\n+++ w/examples/GAN/GAN.py\n@@ -7,7 +7,8 @@ import tensorflow as tf\n import numpy as np\n import time\n from tensorpack import (FeedfreeTrainerBase, TowerContext,\n-                        get_global_step_var, QueueInput, ModelDesc)\n+                        get_global_step_var, QueueInput, ModelDesc,\n+                        MultiPredictorTowerTrainer)\n from tensorpack.tfutils.summary import add_moving_summary\n from tensorpack.tfutils.gradproc import apply_grad_processors, CheckGradient\n from tensorpack.dataflow import DataFlow\n@@ -68,9 +69,10 @@ class GANModelDesc(ModelDesc):\n         return [CheckGradient()]\n-class GANTrainer(FeedfreeTrainerBase):\n+class GANTrainer(FeedfreeTrainerBase, MultiPredictorTowerTrainer):\n     def init(self, config):\n         self._input_method = QueueInput(config.dataflow)\n+        self._setup_predictor_factory()\n         super(GANTrainer, self).init(config)\n def _setup(self):\n\nThen you'll be able to use \"get_predict_func\".\nThis is not like an official solution -- it's quite weird actually. I'm doing some refactoring of the trainers to make things more straightforward.. You can use\n.Conv2D(....)\n.print_tensor()\n.Conv2D(...)\nAlso, for named layers like Conv2D, Deconv2D, the output name is usually just \"prediction/output\", or \"deconv7/output\". The documentation would [mention](http://tensorpack.readthedocs.io/en/latest/modules/tensorpack.models.html#tensorpack.models.Conv2D) the output name.. Currently the script only tries to initialize from a tensorflow checkpoint.\nYou can change it to load from a dictionary.\n-        config.session_init = SaverRestore(args.load)\n+        config.session_init = ParamRestore(np.load(args.load, encoding='latin1').item())\n```. I think they should've produced similar warning messages, could you post what you saw?\nOne thing to note is that, a lot of variables that're in the checkpoint won't be in the npy dict. The npy dict I published was intended for inference, so a lot of summary variables and internal state of optimizer were removed already. For ResNet this could make the npy dict about 1/2 size of the checkpoint.. Are you also expecting \"WR\" in the first log? \nIf not, then these logs look like no problem to me. The variables printed in the second log are removed from npy.. Sorry about this. Should get fixed now.. TF 1.0.0rc0 was released yesterday. It was making quite a lot of incompatible changes, including some involving RNN/LSTM.\nI found one more incompatible change from 12.1 to 1.0, so I updated the code a bit, now the code works with version 1.0.\nYou can find the 1.0 packages here. The PTB example works fine here. \nLooking from the log, you might not have the correct validation data.. I didn't make linearwrap to work with custom layers. Will try to fix that.. Note that by default a layer should be under some variable scope as the first argument, just like Conv2D. So you should call it with: revReLU('relu', x). And in LinearWrap this becomes .revReLU('relu'). With the latest commit this would work.\nYou can register it with @layer_register(use_scope=False) to disable the scope, then your original code would work.\nAlso, in your case you don't have to register it as a layer to use LinearWrap. You can use apply.\n```python\ndef revReLU(x):\n    return -1 * tf.nn.relu(x)\n....\nc = (LinearWrap(a).Conv2D('c', 8, 7, stride=2, nl=tf.identity).apply(revReLU))()\n```. The rename sounds good. It's more straightforward that way.\nThe error is from tf.reshape so it seems to suggest something wrong with the symbolic code. . Renamed in 6e24b953218dc.. why is this closed?. I also think it would be a nice example if the results can be better (now it seems worse than bicubic?) And I like the SuperResolution topic so I may try it when I have more time.. Tried training with one GPU of batch_size 64. Get 95% top1 error after the first epoch, with TF 12.1.\nSo it's probably the upstream dequeue issue which makes multiple GPUs behave the same as one.. It's a TF bug that seems to be fixed now. But I haven't got time to test it.. I've also started a job to train it from scratch. At epoch 60, resnet-18 trained with 4 GPUs is getting 35% top1 val error. This is similar to the number I had before.\nAlthough it haven't finished I guess the issue was resolved.. Also, just curious, @rohitgirdhar From the log you seem to have very good hard disk, to be able to train without data loading overhead. Is it a RAID or SSD or anything? If it is a RAID could you tell me the configuration?  I don't have resources to test which setting is enough for the current implementation. . I was using the original ResNet18 script, the original ILSVRC12 dataset.\nHardware: 4 TitanX, normal hard disk (cannot fill the queue).\nSpeed: 2.3~2.5 it/s.\nIt will be faster if you 1. use a single file such as lmdb and 2. don't globally shuffle all training data, but maintain a pool to locally shuffle the data.\nI think I'll put a page in docs about this when I have time to test the numbers.. I think you should store jpeg string instead of the raw array. The decompression is faster than reading.\nI've converted to lmdb before and the whole db should be less than 150G.. Single lmdb file with jpeg string may still be faster. But since you already have an SSD the benefit could be very small.. The lmdb format caffe used is very inefficient. It stores a serialized raw array without image compression.. rc1 is old. You can use the 1.0 release version.. Sorry, looking at the comment in https://github.com/tensorflow/tensorflow/issues/7038, it looks like 1.0 is still not new enough. Maybe you have to use the nightly version or a custom build.. Use the nightly tensorflow bulid, or build yourself.. There is a lot of \"hooks\" under tf.train. The design of hooks is similar to step callbacks (the before_run method is like the dynamic fetch list) , and it could be helpful to integrate them.\nIn particular, the SummarySaverHook may help with the use case mentioned above.. This issue is very strange because I'm seeing different speed on two similar machines. May not be a TF issue this time.. What kind of metrics do you want?\nAs long as you can compute it in the graph, you can use the InferenceRunner callback with the ScalarStats inferencer.. Almost every example prints something evaluated on the validation set. For example, in mnist-convnet.py, the following line:\npython\n            InferenceRunner(    # run inference(for validation) after every epoch\n                dataset_test,   # the DataFlow instance used for validation\n                # Calculate both the cost and the error for this DataFlow\n                [ScalarStats('cross_entropy_loss'), ClassificationError('incorrect')]),\nNote that \"cross_entropy_loss\" must be an op you defined in the graph already.\nThe validation is run every epoch. You can use PeriodicTrigger(InferenceRunner(.........), every_k_epochs=10) to run it every 10 epoch.. Dropout by default does this automatically. (use is_training=None)\nIf you set is_training=True, then even in inference time it is still True.. Though this is due to opencv, I think it's good to do a reshape inside GaussianBlur to make it less surprising to users (including me).. Yes I also think it's helpful to have a separate branch for developing. Will look at this when I have more time.. After a quick look at the whole complicated process I think I'll first trying to write a good setup.py and setup.cfg (with all the settings configured properly, such as requirements) and then look at how to publish on travis. \n. Thanks a lot and I'll refer to the resources you provided!\nI've added setup.* which seems to work with python setup.py install in a virtualenv. Will test more later.. Maybe you do need a rgb/bgr option?\nOr, since Hue also depends on this, an augmentor which does the RGB/BGR flip would also work.. Thanks! I renamed the class name to CamelCase, and replace keep_dim by keep_dims used by tensorflow (although it's different from keepdims used by numpy)..\n// UPDATE:\na search shows that TF is likely going to move to keepdims as well. https://github.com/tensorflow/tensorflow/issues/6815\n. Thanks for solving this issue! There was an issue about loading keys originally, reset_state() shouldn't need to load all the keys again. This will also slow down the dataflow a lot if you use prefetch.\nThe original idea was that, lmdb is just a k-v database, and the keys can be any string -- doesn't have to contain numbers. That's why it iterates the db to find the keys.\nI like the idea to provide the keys, but the format is assuming the keys are numeric. Maybe you can just add a method in the base lmdb class to set the keys (by giving a list)? Then you can:\nds = CaffeLMDB('/data/imageNet/ILSVRC2012/lmdb/train')\nds.set_keys(np.arange(ds.size())\nThis will also make the API easier to understand, otherwise you'll need to explain that the format will be used against the index.. Oh I didn't notice that. Could you rename the argument to keys and document this as the main feature (and the less-general string formatting as a syntax sugar)?. Thanks, this looks good to me now.\nOne thing I'm thinking about changing (probably later) is: LMDBDataDecoder, CaffeLMDB, etc are  just a mapping on the base LMDBData. If they are implemented with MapData, a change to the base class (like this one) don't have to affect any of them.. I don't know what exactly you want, but we certainly don't have anything like this.\nIf your input->output is one image -> one image, you can write an augmentor, by subclass ImageAugmentor and implement the related methods.\nIf you want one image -> several images in one tensor, or one datapoint -> one datapoint, you can use MapData to specify a mapping function.\nMore generally you can write a DataFlow subclass to implement whatever you want to do with your data, as long as the output can be represented as a tensor in the end.\nWe have a very draft version of tutorials that may help: DataFlow.. @Skylion007  I still don't understand what you mean especially the context. Is this for training as well or only for testing? Doing stuff after getting the output of a network is not a job of DataFlow, but it doesn't sound like anything you would do for training either.\nThis is not a GAN repo, and I'm not even sure what patch-based GAN means to you. If you are talking about the PatchGAN mentioned in the image to image paper, it is fully convolutional and you can run a trained model on super large images already, without having to split things into patches and building a batch.. Thanks for the details and we love to hear different use cases from different people.\nFrom the paper it looks like this is only used for prediction, after a model has been trained? If so it's not about DataFlow or callbacks. \nYou'll notice that in, e.g. the GAN examples the testing part is written completely independent of the training part. It prepares the input tensor, runs an OfflinePredictor, and uses the output tensor. These are just python code you can write freely and they have nothing to do with tensorpack except the call to the predictor.. The code in master now all works for TFv1.0.0rc0 as far as I know. \nWhy do you want to change it to work for v0.12?. I had a wgan implementation locally. Didn't see very surprising results so I didn't intend to push it.\nHave you seen anything interesting from your results?. I don't know you're still working on this! Unless WGAN becomes a very common model in the future, I don't think it's necessary to complicate the existing GAN.py. Maybe just a single file like my gist is fine.\nThere are others factors holding me back otherwise I would have pushed my code earlier already. WGAN reveals some design problems which prevent me from implementing it in easier ways. \nOne is that I cannot use anything similar to the existing VariableClippingOptimizer without writing the _setup method myself. I really wish users don't have to touch the trainer for something as simple as clipping the variables.\nAnother is, when there are >1 ops to run in iterations, running SummaryMovingAverage() together with them may let the graph compute something it shouldn't have (i.e. summary some tensors that were not used by the current op). This doesn't happen in the current implementation, but it could happen with the current design.\nI'm thinking about solving the issues (especially the first one) recently, so I could write WGAN easier. Currently this PR and the gist above can be a reference implementation if people want to see it, but I'd like to push it when appropriate changes are made to simplify it.. Thanks! My mistake.. Do you mean putting the for k in range(5): logic into optimizer?. The clipping is added to the optimizer to avoid having a call like \"optimizer.post_update\". It's more efficient if everything is done in one op. It's also why GAN example only have one train_op.\ntrigger_step() would be enough for clipping if you run another op. (in this case I suppose it'll still be efficient).\nBut there will always be some logic cannot be easily handled by a single op, like scheduling two minimization ops in the WGAN case. \nI don't have a clear boundary between the two. Writing WGAN without the clipping optimizer (just apply the clipping manually) is also fine to me. I added the clipping optimizer because it is common (so there should be a tool for it), not because it shouldn't be in trainer.\nWhat I had in mind is that: optimizers are just simple extensions of tf.train.Optimizer and should be mostly compatible with it (so it should produce an op in the end). \nTrainer does everything necessary to define what run_step is. It can use a slightly better optimizer if that's helpful, but in general all the non-standard logic should appear here. (Or in other words, tensorpack has done enough for you already.. take care of the rest yourself!). It may also be a good idea to move the python-side logic to \"optimizer\" (a new kind of optimizer incompatible with tf.train.Optimizer), so that trainer only handles the graph. I mentioned this because currently in GAN _setup() both build the graph and define the optimization ops.\nBut it's still not clear what is the boundary. Is multi-GPU training a kind of optimizer (because all its logic is to play with gradient stuff and make the update)? \nI still found it very hard to separate the two. So maybe I'll still let trainer do everything.. what do you think?. It would be very ideal to use WGAN under multiGPU automatically, but I don't how I could do that. \nThe boundary is still not clear in what an optimizer should do.\nSyncMultiGPUTrainer produces one train_op in the end, should it use optimizer to implement the logic? AsyncMultiGPUTrainer uses several different train_ops, should it be in trainer?\nThere are ways to put the ratio into the graph with some loop operators, but it may look very complicated (not sure about how simple it can be made). And in general users shouldn't be forbidden writing python loops inside run_step.. Not of any interest for the moment. Closing... But currently it's very hard to use it during training. Now there is unfortunately only little code reuse between \"predict during training\" and \"predict after training\" (offline predict).. None of the prefetching technique was applied, therefore the performance of DataParallelInferenceRunner is very pool. It's becoming a bottleneck for my training. Reopen.. - [x] make a fast dataflow for inference (with epoch-preserving prefetch)\n- [x] let InferenceRunner support QueueInput\n- [x] let DataParallelInferenceRunner support QueueInput. The result looks very promising!\nWhile writing the tutorial I found the dataflow in imagenet-resnet can be written more efficiently -- should use uint8 image instead of float32, I'll benchmark the dataflow more carefully before looking at integrating staging operators.\nStagingArea is on GPU, unlike the queue. So you cannot put too many tensors to it.. To avoid OOM, we have to pre-fill the staging area with some data, and run stage_op together with train_op (instead of in a separate thread). This will avoid staging area growing too large. This is from tensorflow/benchmarks.\nI'm trying this but haven't got any faster yet.. I may have used it in a wrong way.. Able to see some improvements on multigpu training.\nM40, ResNet18, batch per GPU=128. Use float32 image. \nCompare StageInputWrapper(QueueInput) with QueueInput:\nEach epoch is 50steps. Listing per-epoch time (not including warm up epoch):\n1GPU: 17.92s. about the same\n2GPU: 18.5s vs 19.6s. 6% faster\n4GPU: 19.26s vs 21.6s. 12% faster\n6GPU: 20.55s vs 22.9s. 11.5% faster\nUgly code: https://gist.github.com/ppwwyyxx/4e45ab862e0589e2941edc490187ba17\nI'll add it as default for MultiGPUTrainer.\nIt probably will run faster by getting rid of python threads and use TF pipeline.\nNOTE: \ntested on 08/06, 4GPU number can be (roughly) reproduced with TF1.2.0rc0, tensorpack 9d0b28a. (19.9s v 21.6s)\nwith TF1.3.0rc2, tensorpack 1dbf615430703, 19.22s vs 20.3s.\nAlso, it is observed that staging doesn't improve speed of resnet50.. tf.ConditionalAccumulator is related. But it's API seems to be designed for async training, not for increasing batch size.. @rohitgirdhar In case it helps.. Yes I found that as well. In lighting, you'd better clip the return value to [0,255] when old_dtype==np.uint8. Thanks for looking at this!\nFrankly I haven't looked at tensorflow distributed training in depth yet. Because I found now it's still hard to feed 8 GPUs on one machine, let alone multiple machines. That's why I'm working on data recently.\nbtw, TF say they'll publish a \"good\" example code of distributed training very soon, with the use of GPU StageArea. It'll include the best practice so I guess we can refer to that (rather than finding it out by ourselves) when it come out.. Official code and benchmarks were released:\nhttps://www.tensorflow.org/performance/performance_models\nhttps://www.tensorflow.org/performance/benchmarks\nI'll study it when I have time and see how much we can integrate them.. Just feel mnist is too boring... Will add some examples.\nNote that the code actually doesn't scale very well, neither does the official benchmark in my tests. Their benchmark number appears good but that's on slow K80s so communication is less of a problem. I'll bring the issue to them.. I think that's only on a single machine?. Now the following two lines are the same:\npython\ndeprecated(\"step_per_epoch\")(\"Use steps_per_epoch\", \"2017-11-8\")\ndeprecated(\"step_per_epoch\", \"2017-11-8\")(\"Use steps_per_epoch\")\nWhat is fix and why do you need both fix and eos? Seems to me only having eos is enough?. It's very weird how I should use the arguments for logging. I don't know if there is a natural way to use one function to work both as a logging function and as a decorator, so I've split the logging to a separate function. Let me know if you have a better solution.. It seems good to just use SessionRunHook to implement callback. Probably will also rename extra_fetches and trigger_step to before_run and after_run with similar API.\n\nCalling before_run every step doesn't have visible overhead.\nThe way _HookedSession is implemented is better: it injects the run call directly, so the hook is transparent. However with how callback is implemented, you need to use get_extra_fetches() and return the results in every run_step call.\nI'll keep both trainer.sess and trainer.monitored_sess (or add an alias to trainer.hooked_sess), so that a custom trainer can choose whether to trigger the hooks in a specific run, just like how it is done in WGAN now. \n\nA tricky issue:\n4. It's better to allow users to trigger the hooks multiple times in one run_step call. \nThen how should global_step be updated? This is tricky under the assumption that global_step variable should be consistent with trainer.global_step as well as steps_per_epoch.\nWhen you implement a function run_step, you should expect what's inside is really one step, no matter how many hooked_session.run() get called inside. \nBut then the problem becomes: some need to be triggered once for each session.run call, but some (e.g., global step increment, progress bar) need to be triggered once for each run_step call. How to make this happen from design?. I think setup_graph and before_train are better names than begin and after_create_session. Won't do more renaming.. Just realized there is tf.contrib.learn.monitors, also with similar design. . I'm not using monitors from tf.contrib. My \"monitors\" mean something different.. The second problem is because TF 1.0 changes API of tf.concat. You should swap the argument order of tf.concat in inception_resnet_v2.py.\nThe first is because the checkpoint contains an unused variable \"global_step:0\" which happens to conflict with a variable tensorpack defined. In general there is no good way to solve this and it's best to remove unused variables from a checkpoint. But I'll push a change later to make it behave less strict in this case (print a warning instead of crash).. You should be able to load the model with the current HEAD now (don't need to remove the variable).. I took at look the checkpoint. It seems to be the very old checkpoint format (before TF 0.8 maybe?) where the variable names don't contain the :0 in the end. So there is a mismatch.\nI'll fix some code to be compatible with that format.. After 6640f9bbaa71bf81ecd9c7042d4ce, it can start training with multi-GPU. You should only see the following variables not found (just some summaries):\n[0216 14:52:07 @sessinit.py:110] WRN Variable learning_rate in the graph not found in checkpoint!\n[0216 14:52:08 @sessinit.py:110] WRN Variable tower0/train-error-top1/EMA in the graph not found in checkpoint!\n[0216 14:52:08 @sessinit.py:110] WRN Variable tower0/cost/EMA in the graph not found in checkpoint!\n[0216 14:52:08 @sessinit.py:110] WRN Variable tower0/total_cost/EMA in the graph not found in checkpoint!\n[0216 14:52:08 @sessinit.py:110] WRN Variable input_queue_size/EMA in the graph not found in checkpoint!\n[0216 14:52:08 @sessinit.py:110] WRN Variable tower0/train-error-top5/EMA in the graph not found in checkpoint!\n[0216 14:52:08 @sessinit.py:110] WRN Variable tower0/regularize_loss/EMA in the graph not found in checkpoint!\nLoading checkpoints with type checks (and casting) seems to slow it down a lot, because now it loads data from checkpoint to python, then to TF, instead of going to TF directly. I'll do some other tests on the speed.. Inception consumes a lot of memory. I can run it with batch 32 per GPU but not 64, Google also used this number in their recent papers.. https://github.com/tensorflow/models/tree/master/slim#the-resnet-and-vgg-models-have-1000-classes-but-the-imagenet-dataset-has-1001. No. Have you make sured that the network takes the same input format (rgb/bgr, value range, etc) and output class id? It may not be the same as what's used by the code here.\nYou'd better eval the model before training on it.. As I said they may not use the same class id:\nhttps://github.com/tensorflow/models/blob/master/inception/inception/data/build_imagenet_data.py#L119. You can convert the label either on the dataflow (you can use MapDataComponent) or inside the model (you can use tf.gather or tf.gather_nd).\nThe class order I'm using is the same as caffe. You can see it at $TENSORPACK_DATASET/ilsvrc_metadata/synset*.txt. In the paper it was 200M frames if I recalled correctly.\nEach iteration here is a batch of 64 frames. It would take about 2.5\\~3 days to run 200M frames (assume a training speed of 12\\~18 it/s).. But this implementation uses different (larger) network architecture and different batch size. To reproduce the score (not the number of frames) on breakout game you'll only need to run it for about one day. Haven't tested a lot of games.. Here, a minibatch was trained after obtaining 4 new transition tuples. This is the \"update frequency\" parameter in the original paper.\nSo then I guess you should expect 12500000 steps to be equivalent to 200M frames in the paper, assuming all parameters (batch size, etc) are consistent with the paper.. Currently only added for the most common layers.\nSaw 30~40% speed up on cifar10 resnet. Most examples are still left unchanged now.. Seeing 30% improvement on ResNet-ImageNet.\nHowever Image2Image becomes slower if changed to NCHW. (maybe deconv is not fast? haven't tested a lot). TensorFlow doesn't know what is a layer, what is forward/backward. So there is no way doing this.\nIt does have a lower-level profiling which I may integrate.. The most likely reason this happens to you is that you installed different versions of some packages at different places and they got messed up with wrong environment variables or other improper settings. You'll need to google more.\nA suggestion is to never run sudo pip install on anything. \nOne way that might solve your current problem is to sudo pip uninstall everything under /usr/local/lib/python2.7/dist-packages/. Not sure if it will work.. An update after email communication with the user:\non some systems pip install --user -U pip setuptools is needed to update pip&setuptools. Otherwise you may see UNKNOWN as the package name.. Actually \"datapoint\" was the term I planned to use, on anything yielded by a dataflow. It might not be a good name though... Great! Is that all?. It's either a hardware or a cuda/driver problem. That's not something I can solve.\nPossible things you can try:\nupgrade/reinstall cuda or driver\nrun with only 1 of the GPU (with CUDA_VISIBLE_DEVICES=0 or 1)\nuse a newer version of tensorflow (0.12 is not supported by tensorpack) \nreboot. ok. Two possible methods were mentioned in the comment of this issue: https://github.com/NVlabs/GA3C/issues/3. Theoretically you can use any symbolic functions. This includes Keras layers as long as you can call it on a tensorflow tensor,  e.g.:\npython\nfrom keras.layers import Dense\n...\nlogits = Dense(10, activation=None)(logits)    # logits is a tf.Tensor\nBut there are some issues:\n1. If you use any keras layers with variables in tensorpack, none of the inference functionality will work anymore (including validation on test set every epoch) , because keras doesn't respect tf.variable_scope. MultiGPU won't work either because of the same reason.\n2. If you use any keras layers which has different training/testing behavior (e.g. dropout), keras require an extra \"is_training\" tensor to be fed into the graph. You'll need to add a callback:\npython\nclass KerasHook(Callback):\n    def _before_run(self, ctx):\n        return tf.train.SessionRunArgs(fetches=[],feed_dict={'keras_learning_phase:0':1})\nThis will feed an extra tensor for every training iterations.\nUPDATE:\nIt looks like keras does weight sharing by calling the same keras layer multiple times: https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html . Though it's not compatible with tensorflow variable scope, maybe inference & multiGPU training can still work if you save the layer object in the model, and reuse the layer in subsequent calls to _build_graph. I'll test whether this would work when I have time.. That tensorlayer example doesn't use any multi tower training or inference (it uses the same tower for inference).\nIt will also work here if you don't use multi tower training or inference.\nThe problem is to have multi-gpu training work with Keras.. The above commit added a keras example. Tested with latest Keras installed from pypi (since Keras is upgrading recently, versions may matter).\nNow people can define keras model and utilize the more efficient tensorpack trainers, and even multi gpu training!\nSince I'm apparently not a keras user, I only tested a very simple keras model. Not sure how it goes with more complex models. Feel free to report errors you may encounter.. We use standard TF checkpoint, but Keras has its own format if I recall it correctly, so definitely would need extra effort.\nTensorpack doesn't assume you're doing classification, or assume you want to print the error rate. So you need to write them with symbolic functions. Using keras functions would run as well but be less informative.. Closing because the current status (a mnist-keras example) is probably all we can support. The use of tf.Variable everywhere in Keras makes it very hard to work with any other frameworks.. The fundamental reason is that calling tf.Variable will ignore variable scope reuse, which is the fundamental mechanism of multigpu training. I'm not sure is this necessary for Keras or is it possible to change. There is actually a question on Keras about this: https://github.com/fchollet/keras/issues/7992\nThe current way to build keras model inside tensorpack isn't too bad IMHO. Apart from that maybe more things can be done on tensorpack side, e.g. integrate with Keras loss/regularization automatically.. https://github.com/tensorpack/tensorpack/blob/c2c895abcf8e7e13bc8cca9f1cee4a8631b9d341/examples/mnist-keras-v2.py#L40-L71\nA WIP to train keras model easier... https://github.com/tensorpack/tensorpack/blob/130f60ac4491d55a13632eabbad839c2e23138ba/examples/mnist-keras-v2.py#L35-L59\nA more keras-like API which could run faster than keras especially when data is large.\nIt supports multi-gpu, but there are potential issues for now, e.g. weight-decay is probably not performed correctly.. Presumably it should be able to train any Keras model. Although there might be unknown issues because I haven't tried anything other than mnist yet.. Played with this a little bit yesterday. This experimental interface is too Keras-like and therefore cannot be made efficient to the best of my knowledge. Both of the two best-known scaling techniques (replicated and parameter_server mode in https://github.com/tensorflow/benchmarks) on single-machine require the use of variable scope. Therefore I cannot let users create their Keras models directly under a root variable scope -- it can never reach the best performance. Models have to be created by tensorpack like the way the old mnist-keras example is doing. I'll make the change on this but the rest of the interface can probably still be Keras-like.. Now a state-of-the-art ImageNet model can be trained with Keras+Tensorpack. https://github.com/ppwwyyxx/tensorpack/tree/master/examples/keras\nClosing this issue.. OK. I think we can ask for a new name when \"new\" is chosen.. To not producing any train-logs, just remove ModelSaver from callbacks, and do not set logger dir. The rest should run as usual. (effective after 000b2ea328dd35e60c)\nYou can just add your action to the argument parser, and both auto_set_dir and set_logger_dir have the option action.. lol I've been there as well deleting my models....\nBut directory is set at the very beginning before loading the models. I wonder how this can be detected and warned.. Adding an extra name like this doesn't help: dirname + label can still be an existing directory.\nMoreover, since you have a python variable already containing the actual name of the log directory, why still use auto_set_dir instead of calling set_logger_dir directly? auto_set_dir is just to automatically generate a name which \"probably is what you want\", saving just one line of code.. maybe it will cause small performance degradation. there are other differences compared to caffe as well.\nduplicate of #54.. Thanks!. Not currently. But you don't have to make something into tensorpack to use it. \nIt should be very easy to implement (just use sklearn.metric.auc).. Oh I see. Will add it soon.. The shape constant is not very helpful in the script, because the network definitely need some changes if working with a different shape.\nSimple extension is to add one more layer and use 512x512 images.. Relevant discussion about using different shapes: https://github.com/phillipi/pix2pix/issues/56. The second one is just imgaug.MapImage(lambda x: cv2.transpose(x)) ?\nThe first one does look like something others may need, but this probably won't come to top of my todo list very soon. Contributions are welcome.. Most examples have a --load option which can restore from a saver.\nBasically set the session_init option in your training config, to SaverRestore(...)\n. Yes. If you want to start from a different epoch number, you can use the starting_epoch option.. If you use that option, the logs it will print will be changed to N.. And what epoch number it prints usually doesn't have any effect to the training. The only difference it may make is when you use a epoch-number-based learning rate setter.. Yes. An \"epoch\" can have different semantics to different people.\nBut anyway, I think it's good if I could add a feature to (optionally) automatically recover the epoch number from log. I'll see what can be done.. Was it? I don't remember I've saved the epoch number in checkpoint before.. I couldn't reproduce this problem. Note that there are multiple threads running so it's unlikely it is a problem with data reading, but more likely to be in training.\nCould you try if the simple examples (mnist-convnet.py, cifar-convnet.py) can run?\nAnd how do you install opencv? Are you using anaconda?. How do you install opencv?\nAnd could you try running without GPU to see if the error occurs? (use CUDA_VISIBLE_DEVICES=).  Could you post the full log so I can see which stage it was in?\nOnce it starts reading data, it starts using the TF session as well.\nIf you compile opencv yourself, it's likely you've enabled CUDA or OpenCL support in opencv which will cause segfault. Try running without GPU to see it happens.. If this is where it crashes I have no clue now. \npython -c \"from tensorpack import dataset; c = dataset.Cifar10('train')\"\nshould also crash for you if it really breaks there.. Did the crash happen when loading training data or validation/test data?\nIf the latter, could you remove the prefetch (or even the augmentors) in cifar-convnet.py to see if it segfaults?. Then cifar-convnet.py should be equivalent to the following:\npython\nimport tensorflow as tf\nimport argparse\nimport numpy as np\nimport os\nfrom tensorpack import *\nimport tensorpack.tfutils.symbolic_functions as symbf\nfrom tensorpack.tfutils.summary import *\nfrom tensorpack.utils.gpu import get_nr_gpu\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nwith tf.Graph().as_default():\n    logger.auto_set_dir()\n    ds = dataset.Cifar10('train'). If the above code crashes, could you import cv2 at the first line and try again?. OK. I thought this problem was fixed by tensorflow already. Turns out it still happens on some systems.\nI'll update some examples to avoid this.. On a HDD RAID with 1GB/s bandwidth, after clearing cache, running the equivalent of:\npython\nds = LMDBData('/path/to/ILSVRC-train.lmdb', shuffle=False)\nds = BatchData(ds, 256, use_list=True)\nLMDB sequential: 42it/s\nTFRecord sequential 35it/s.. That's what I observed at least. After all LMDB has more low level optimizations.. You probably are bounded by data loading. It's very hard to feed 8 TitanX Pascal, it will be blocked either by hard disk or CPU preprocessing speed.\nYou can either reduce disk pressure or use less preprocessing depending on what's the bottleneck. Or you may use better hardware.\nSome relevant information at http://tensorpack.readthedocs.io/en/latest/tutorial/efficient-dataflow.html.\nYou can also use pure symbolic input as what the official examples are using. They should be more efficient as everything is written in C++.\n #202 is tracking more solutions to this problem.. You're right! Thanks!\nHowever, changing the code may affect the claimed accuracy of my pre-trained models (and I don't want to train again), so I'll run some tests before fixing this.\nUPDATE: code and models were corrected.. OK, I was thinking about how I would use it, but keeping the size seems useful as well. You can keep it this way.. It is supposed to be exactly the continuation of the last saved checkpoint. \nCould you provide more information (command, logs)?. Could you post the log of the pip command?\nAlso, anaconda probably itself does something tricky about packages and import path, though I never used it. I may try it later to see if I can reproduce the problem. \nMeanwhile could you make sure you're using both pip and python from anaconda but not from the system? http://stackoverflow.com/questions/21201003/anaconda-not-finding-my-packages-installed-with-pip. Check the stackoverflow link. Does which pip give you the correct pip location?. The problem seems to be:\n1. don't use --user in anaconda\n2. setuptools in anaconda is too old. I upgrade it with pip install -U setuptools.. Just --load your model-xxx.data-xxx file.\nThey are not the same format, but --load option in the script takes care of both.. If you move it out you have to move the index file as well. That's tensorflow requirement.. The new version does equivalent computations, but faster. You don't need to do anything.. The new one uses transpose and NCHW format. The old one uses NHWC format.. Right. This fix has to be added as long as opencv is involved.. Maybe you don't have enough disk space?\nImage2Image models are quite large.. Now it saves the variables by tf.train.Saver and also save a MetaGraphDef protobuf file. These are exactly what a SavedModel is saving.\nApart from these, a SavedModel also saves assets, tag names for different metagraphs, signatures, etc. These are specific to applications and users, and I don't think I can make decision for users what to save and what not to save. \nIf you need these extra features, you can write a new callback just like ModelSaver and save what you'd like to save.. Closing for now.\nIf you find anything you think is \"very common\" that should be saved with ModelSaver by default you're welcome to reopen.. Added in #290 a while ago.. The parameter is used to set available gpus and one gpu will be used.\nThis example doesn't include multiple gpu support.. I'm not sure what you mean exactly.\nBut if you want to load the latest model in a separate script, \"LOGGER_DIR/checkpoint\" is the last saved checkpoint.. This would depend on how \"sample\" is written.\nIf it uses SaverRestore(model_file) in the end, then model_file can either be dir/checkpoint, or what you have right now will also work.. yes.\nIt will end up here and use the latest checkpoint.. You can. Just use tf.nn.l2_loss or any other tensorflow symbolic functions.. So what is the issue?. Look at the docstring in *-dorefa.py to see detailed usage and performance.. \"\"\"\nThis script loads the pre-trained ResNet-18 model with (W,A,G) = (1,4,32)\nIt has 59.2% top-1 and 81.5% top-5 validation error on ILSVRC12 validation set.\nTo run on images:\n    ./resnet-dorefa.py --load pretrained.npy --run a.jpg b.jpg\nTo eval on ILSVRC validation set:\n    ./resnet-dorefa.py --load pretrained.npy --eval --data /path/to/ILSVRC\n\"\"\"\nThese are the only two usages of this script. It is not a training script.. That's very strange. I will make a change to adopt your fix.. Did you mean you have this error only in inference (not training)?\n. For the moment you can hack https://github.com/ppwwyyxx/tensorpack/blob/master/tensorpack/train/base.py#L133.\nCustomized session creation is available in prediction time, but not in training right now. I'll try to fix it.. Now you can pass an optional session_creator in TrainConfig. In theory you can choose what session to use for the entire training as long as you write a SessionCreator yourself.\nTo create a debug session, SessionCreatorAdapter may help you. You can use:\npython\nSessionCreatorAdapter(NewSessionCreator(), \n    lambda sess: tf_debug.LocalCLIDebugWrapperSession(sess, other_options))\nI also just found there is a debug hook in tensorflow, so an easier way might be just to convert the hook to callback:\npython\n        callbacks=[\n            HookToCallback(tf_debug.LocalCLIDebugHook()), ...\nI never used tf_debug so there might be usage I'm not covering. Please leave a note if you found tensorpack makes it harder to do something you could have done easier with tensorflow.. I'll take a look. The second method should work fine.. The reason is that:\n1. The default session config being used doesn't allow soft placement (fixed above)\n2. When tensor placement failed, TensorFlow doesn't give the correct error message. (https://github.com/tensorflow/tensorflow/issues/8507). 1. If you added the layer to tensorpack/models as the first snippet, it should be accessible, just as any other layers. \nMaybe you've installed tensorpack through pip, so you are not running the actual code you're changing?\nAnd you can also just register the layer in your own code and import it.. 2. This A3C implementation doesn't use T_MAX in a batch as the original paper, but a large batch with randomly shuffled experience, so simply adding a LSTM layer shouldn't work. \nIt may need a continuous sequence in a batch, plus some way to manipulate states of each game simulator. Others have given a good way to do this: https://github.com/NVlabs/GA3C/issues/3#issuecomment-281247793. A simple file like this placed in models is enough for from tensorpack import * to find Lstm:\npython\nfrom .common import layer_register\n__all__ = ['Lstm']\n@layer_register()\ndef Lstm(x):\n    pass\nYou can also manage the states by letting the predictor fetch the hidden state, and feeding the hidden state in training. This is supported under the framework as well. I mentioned this in that thread also, but I like the other solution.. 1. Then it's very likely you pip install it, as I said above. Then any changes locally wouldn't appear unless you're in the top level directory.\n\nWhat's input into the graph is determined here, and then getting batched here\nIf you want extra inputs, just add more InputDesc and change what's put into the queue. . Do you just want multiGPU? Distributed training are not supported at the moment.\n\nIf just multiGPU, is slurm anywhere different from a normal machine with multiple GPUs? I know nothing about slurm so I'm not sure what's the issue.\nThere are quite a number of examples with SyncMultiGPUTrainer. And the usage is just the same with Async version.\nexamples/HED is a segmentation example. Nothing really different from other tasks.. MultiGPU on a single machine should run properly then. Are there unresolved questions?. how much RAM you have ?. Could you try without tcmalloc?\nTensorFlow now builds with jemalloc together, so maybe there is a problem mixing the two.. I was using a private TF API which may be different between TF versions.\nI will push a fix soon.. That private API still exists in latest TF.\nBut the above fix should solve the issue for earlier versions.\nI'll also update the doc to not mention tcmalloc anymore, since jemalloc seems better.. That's the term TF guys use for data-parallel training: https://github.com/tensorflow/models/tree/master/inception#how-to-train-from-scratch\nHere I extend the concept to use in inference as well.. Thanks!. Your tensorflow version seems too old.. You should always call reset_state() before using a dataflow.\nhttp://tensorpack.readthedocs.io/en/latest/tutorial/dataflow.html#reuse-in-other-frameworks. Just added a function utils.fix_rng_seed in 546af8b2b3b0dd82b0.\nYou'll also need to avoid any kind of parallelism:\n1. set 1 inter/intra op parallelism thread in session config\n2. don't use any kind of prefetch\n3. set use_locking=True in the optimizer\nAnd set seed at the beginning of your program by both fix_rng_seed and tf.set_random_seed.\nAnd this still cannot guarantee deterministic results, they just make things more deterministic. After doing the above things I see very tiny differences when training mnist for a couple of epochs.\nBecause a lot of common ops (both on GPU and CPU) are non-deterministic. https://github.com/tensorflow/tensorflow/issues/3103\n. Right. I'll use a better name.. Could you post your command and the full log?. You may need to upgrade your protoc.\n$protoc --version\nlibprotoc 3.2.0. For newer versions you're expected to see something like this: https://github.com/google/protobuf/issues/7#issuecomment-64992592 in the generated caffe_pb2.py file. Otherwise it won't work with python 3.. ZMQ IPC protocol doesn't support windows.\nFor A3C, zmq is necessary.. + matplotlib is in opt-requirements, as it's not necessary for most functionailities\n+ does it? I think it only imports caffe_pb2.py, which doesn't require caffe to be installed.. The default dataset directory for ILSVRCMeta (containing train.txt, etc) is ilsvrc_metadata, not ilsvrc.\nBut you can pass meta_dir argument to ILSVRC() as well. It is the full path to the directory.. Btw, resnet example now uses NCHW, which is not supported by TensorFlow on CPU. You might encounter this problem later... I think it's OK. There is already a line of FakeData because I use it for testing as well.. Removed fake_ilsvrc12, doesn't seem necessary? \nThanks very much!. Are you using tensorpack models without tensorpack trainer?\nWhen used separately, any tensorpack model needs to be used under a TowerContext. If you really want that you can wrap your model with \npython\nwith TowerContext(tower_name='', is_training=True):\nto let the model work properly.\nIt is now added in the tutorial. Brief Documentation: http://tensorpack.readthedocs.io/en/latest/tutorial/efficient-dataflow.html#more-efficient-dataflow. Thanks byron for your work in GDR!\nWe don't use IB in clusters and for data feeding the use of IB could be limited. It reduces the copy latency, but in practice I was never bounded by copy since the latency can usually be hidden.. Yes. We didn't use protobuf either.\nOur zeromq receiver in Python uses msgpack to deserialize, and our zeromq receiver op just uses memcpy to deserialize.\nAnd deserialization is also just a pre-processing stage -- it can always run asynchronously. As long as it's not too slow it doesn't affect training speed from my experience. But I think there will be scenarios (e.g. much faster GPUs as you said) where it matters. . It generates a continuous vector field of NxM so that it gives each pixel a small displacement but continuous in spatial domain. The field is generated by a mixture of gaussian centered at several spatial locations.\nYou can adjust the parameters.\nAugmentors doesn't gurantee they don't modify input image. They can do whatever to the input to speed up their processing. This specific augmentor doesn't modify the input image, though.\nTo avoid the modification goes to your original images (in case you keep them persistent in memory, for example), you can yield a deepcopy in your first dataflow.. Just give it a second thought. Maybe it's not a very good idea to use a dangerous default, allowing the augmentors to modifiy the image. \nMaybe we should use a safe default (make a copy explicity) and allow turning it off when users know it's OK.. FYI, added a \"copy\" option to AugmentImageComponent but still defaults to false now.. ```python\nclass MyAug:\n   def _get_augment_params(self, img):\n        return any params that have to be kept same for both input and output\n        # any random number has to be generated with self.rng\n    def _augment(self, img, params):\n        return new_img using img and params\ndf = MyDataFlow() # produce [input, output]\ndf = AugmentImageComponents(df, [MyAug()], (0,1))\n``. Thanks! The results look interesting. I should try it some day.\nPlease note that, I added models or augmentors only when they are very common (or sometimes for my personal convenient). I found everyone likes to do augmentation very differently, but then it's very hard for me to maintain all the pieces.\nThe nice thing is that you can just import and use your augmentor without adding it into tensorpack. I also had a bunch of private code with different ideas I tried, and there are also several augmentors inexamples/` but not in the library. Because I'd rather keep the main library small and only include the extensions as examples.. LinearWrap is a syntax sugar useful when you don't need intermediate layer output.\nIf you need fc0 layer output, just stop LinearWrap at fc0.. There isn't a very straightforward way because I don't know where it can be added. \nCurrently the docs tell you the name of the variables, which are 'W', 'b': http://tensorpack.readthedocs.io/en/latest/modules/models.html#tensorpack.models.Conv2D\nThen you can access the tensor by, e.g. Graph = tf.get_default_graph(); W = Graph.get_tensor_by_name('conv0/W:0'), assuming the name of the layer is 'conv0'.\nYou can also implement Conv2D yourself if you need more intermediate inputs.. @PatWie Yes that's my first thought as well and the interface would be very simple. But since tf.Tensor doesn't really have such attributes this may not be a safe option. . Closing this and continue as a feature request in #228 .. Thanks!. A bug was introduced lately. It always tries to evaluate the summary tensor in a separate sess.run call, which leads to error because the call may depend on feed_dict.. You're right! My mistake. The copy option was not working.\n@msbauer \nAugmentor may modify your datapoints for efficiency. In your case you yield references to the same datapoints, so the modification to dp will be visible. You can either yield copy.deepcopy(dp), or use the copy=True option in the augmentor.. In #203 I was thinking about making copy=True the default. Maybe I should really do that since users can easily meet this problem... Thanks very much!. It seems to be a limitation of tfdbg that it cannot work very well with queues.\nSwitching to SimpleTrainer might have tfdbg to work, since you won't care the speed anyway.\nI'll do some more investigation later.. Seems that a session.run call of a debug session cannot be run simultaneously in two threads. (which makes sense given what it did).\n But if using TF queues, then thread is necessary. It'll be hard to get around this limitation.\nApart from using SimpleTrainer, you can also just use the debug hook I mentioned last time. It should be less error-prone than injecting a debug session and make every ops under inspection.. This is just a question of how to implement XNOR-Net in tensorflow. You'll have more supports from tensorflow communities.\nBasically just first quantize things to 0 and 1 (with tf.sign), and transform it to -1 and 1 (with x * 2 - 1). And use tf.stop_gradient trick to make the backward correct.. Oops. My mistake. Just realized that tf.sign outputs {-1, 0, 1} already.\nThen applying the binary weight quantization on both weights & activations should make some kind of XNOR-Net. But still some effort are needed to make gradient correct.. I don't understand what exactly you want to do here. But you need to implement \"how to load the data\" yourself anyway, so you can do whatever to your data.. Feel free to reopen if you still have questions.. Thanks!. TF0.8 was like a year ago (around last April). You can checkout to tensorpack a year ago to see how things work. But it's very likely that either tensorflow or tensorpack at that time don't have the utilities to train timit.\nCompared to that it would be much easier to just use 1.0. There's a AugmentImageComponents doing exactly what you said.. Yes. And that's exactly what the HED example did.. Cannot reproduce the problem.\nThis is usually related to environment and setup. You may see more details through some local debugging and figure out which part is going wrong.. I'll fix it soon.. Thanks for finding it out. It looks like tensorflow changed their API at some moment.\nWhat do you mean \"find the edit distance\". It is named \"error\" https://github.com/ppwwyyxx/tensorpack/blob/6ba19a977509e911daaea2367cd0aaa62936b37e/examples/CTC-TIMIT/train-timit.py#L75\nand get printed every epoch, on both training and validation set.. Found 111 entries in train.mdb\nFound 11 entries in test.mdb\nThey shouldn't be only contain 111 and 11 elements, if that's what you're really using.\nThe files are very large I cannot upload them.. Does shutil.move work on windows in this case?. JoinData is already like that.\nAnd this is also how data is read in the DiscoGAN example.. The exception is catched and warned.. You're right. But it's fake anyway.. so it doesn't really matter to me.\nThis option works as a sanity check, but 256 could be just too large to run. You can change it for your need if you want something else for benchmark.. You just need to modify the Model a little bit.. ```\n3c3\n< # File: imagenet-resnet-fcneval.py\n\n\nFile: imagenet-resnet.py\n31c31\n<         return [InputDesc(tf.float32, [None, None, None, 3], 'input'),\n\n\n\n    return [InputDesc(tf.uint8, [None, INPUT_SHAPE, INPUT_SHAPE, 3], 'input'),\n\n101c101\n<                 argscope([Conv2D, MaxPooling, AvgPooling, BatchNorm], data_format=self.data_format):\n\n\n\n            argscope([Conv2D, MaxPooling, GlobalAvgPooling, BatchNorm], data_format=self.data_format):\n\n110,112c110,114\n<                       .AvgPooling('gap', shape=7, stride=1)\n<                       .Conv2D('linear', 1000, 1, nl=tf.identity, use_bias=True)())\n<         logits = tf.reduce_mean(logits, [2, 3], name='logits')\n\n\n\n                  .GlobalAvgPooling('gap')\n                  .FullyConnected('linear', 1000, nl=tf.identity)())\n\n    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n    loss = tf.reduce_mean(loss, name='xentropy-loss')\n\n119a122,125\n        wd_cost = regularize_cost('.*/W', l2_regularizer(1e-4), name='l2_regularize_loss')\n        add_moving_summary(loss, wd_cost)\n        self.cost = tf.add_n([loss, wd_cost], name='cost')\n125c131,133\n< def get_data(train_or_test):\n\n\n\ndef get_data(train_or_test, fake=False):\n    if fake:\n        return FakeData([[64, 224, 224, 3], [64]], 1000, random=False, dtype='uint8')\n127c135\n<     assert not isTrain\n\n\n\n130,143c138,192\n<                           shuffle=True if isTrain else False, dir_structure='train')\n< \n<     old_aug = imgaug.AugmentorList([\n<         imgaug.ResizeShortestEdge(256),\n<         imgaug.CenterCrop((224, 224))])\n< \n<     class ResizeAug(imgaug.ImageAugmentor):\n<         def augment(self, img, ):\n<             if min(img.shape[0], img.shape[1]) >= INPUT_SHAPE:\n<                 return img\n<             else:\n<                 return old_aug.augment(img)\n<     ds = AugmentImageComponent(ds, [ResizeAug()], copy=False)\n<     ds = BatchData(ds, 1, remainder=not isTrain)\n\n\n\n                      shuffle=True if isTrain else False, dir_structure='original')\nif isTrain:\n    class Resize(imgaug.ImageAugmentor):\n        \"\"\"\n        crop 8%~100% of the original image\n        See `Going Deeper with Convolutions` by Google.\n        \"\"\"\n        def _augment(self, img, _):\n            h, w = img.shape[:2]\n            area = h * w\n            for _ in range(10):\n                targetArea = self.rng.uniform(0.08, 1.0) * area\n                aspectR = self.rng.uniform(0.75, 1.333)\n                ww = int(np.sqrt(targetArea * aspectR))\n                hh = int(np.sqrt(targetArea / aspectR))\n                if self.rng.uniform() < 0.5:\n                    ww, hh = hh, ww\n                if hh <= h and ww <= w:\n                    x1 = 0 if w == ww else self.rng.randint(0, w - ww)\n                    y1 = 0 if h == hh else self.rng.randint(0, h - hh)\n                    out = img[y1:y1 + hh, x1:x1 + ww]\n                    out = cv2.resize(out, (224, 224), interpolation=cv2.INTER_CUBIC)\n                    return out\n            out = cv2.resize(img, (224, 224), interpolation=cv2.INTER_CUBIC)\n            return out\n\n    augmentors = [\n        Resize(),\n        imgaug.RandomOrderAug(\n            [imgaug.Brightness(30, clip=False),\n             imgaug.Contrast((0.8, 1.2), clip=False),\n             imgaug.Saturation(0.4),\n             # rgb-bgr conversion\n             imgaug.Lighting(0.1,\n                             eigval=[0.2175, 0.0188, 0.0045][::-1],\n                             eigvec=np.array(\n                                 [[-0.5675, 0.7192, 0.4009],\n                                  [-0.5808, -0.0045, -0.8140],\n                                  [-0.5836, -0.6948, 0.4203]],\n                                 dtype='float32')[::-1, ::-1]\n                             )]),\n        imgaug.Clip(),\n        imgaug.Flip(horiz=True),\n        imgaug.ToUint8()\n    ]\nelse:\n    augmentors = [\n        imgaug.ResizeShortestEdge(256),\n        imgaug.CenterCrop((224, 224)),\n        imgaug.ToUint8()\n    ]\nds = AugmentImageComponent(ds, augmentors, copy=False)\nif isTrain:\n    ds = PrefetchDataZMQ(ds, min(20, multiprocessing.cpu_count()))\nds = BatchData(ds, BATCH_SIZE, remainder=not isTrain)\n\n```\nAnd of course the data needs to preprocessed differently.\nBecause in tensorflow, weights for convolution and fc happen to have the same layout, so the above code can evaluate the pre-trained ResNet model directly (and get a warning about reshape).. Could you post the relevant code?. The config should get passed to tensorflow successfully. This might be a TF issue or the config is not enough to make it control the memory usage.\nThe allow_growth option in the config may help. . It works for me. After applying your patch on a different example I can see the memory change.\n\n\n\n```\ndiff --git i/examples/cifar-convnet.py w/examples/cifar-convnet.py\nindex a8e18dc..8761bb2 100755\n--- i/examples/cifar-convnet.py\n+++ w/examples/cifar-convnet.py\n@@ -10,6 +10,7 @@ import os\n\n import tensorpack.tfutils.symbolic_functions as symbf\n from tensorpack.tfutils.summary import *\n+from tensorpack.tfutils.sesscreate import *\n from tensorpack.utils.gpu import get_nr_gpu\n\n \"\"\"\n@@ -114,6 +115,13 @@ def get_config(cifar_classnum):\n     dataset_train = get_data('train', cifar_classnum)\n     dataset_test = get_data('test', cifar_classnum)\n\n+    tf_conf = tf.ConfigProto()\n+    tf_conf.gpu_options.per_process_gpu_memory_fraction = 0.05\n+    tf_conf.allow_soft_placement = True\n+    session_creator = NewSessionCreator(config=tf_conf)\n+\n+\n     def lr_func(lr):\n         if lr < 3e-5:\n             raise StopTraining()\n@@ -127,6 +135,7 @@ def get_config(cifar_classnum):\n             StatMonitorParamSetter('learning_rate', 'val_error', lr_func,\n                                    threshold=0.001, last_k=10),\n         ],\n+        session_creator=session_creator,\n         max_epoch=150,\n     )\n```\n\n. The backends are the same for all the examples.. That's interesting.. I'll try DQN.. Thanks for the pointer! \nBut I don't know anywhere a Session would be created earlier. From that thread it looks like the config issue is quite complicated.. btw, I cannot reproduce this in DQN either (at least not in the first epoch).\nI'm using some nightly prebuilt version of TF I downloaded last week.. User reported that this is fixed after reinstalling TF. Closing.. I'm using 4.2.1-1.  cppzmq only contains the cpp header but maybe the c header is incompatible.. It looks like my package manager automatically includes zmq.hpp for me. (https://git.archlinux.org/svntogit/community.git/tree/trunk/PKGBUILD?h=packages/zeromq)\nAnd none of the other packaging system seems to do this. Yay archlinux... As long as you did export PKG_CONFIG_PATH=$PKG_CONFIG_PATH:$HOME/zeromq/lib/pkgconfig/, I suppose you don't need CXXFLAGS += -I/home/patwie/zeromq/include/ -L/home/patwie/zeromq/lib/ -lzmq.. Could you confirm pkg-config --cflags --libs libzmq produce your CXXFLAGS after setting PKG_CONFIG_PATH?\nAlso, the Makefile at first didn't include libzmq in LIBS. If that's the case you may need to pull.. https://gitter.im/tensorpack/dev try this. Which document is it? I thought the dynamic-loader design was just aimed to allow people to put ops anywhere.\nThere is still a small protobuf dependency (only used for data type and shape), and I don't like it either.\n I'll see if TF is willing to export the symbols otherwise it'll be very hard to build.\n Only dozens of bytes of protobuf is used, but it consumes about 1/4 of the runtime.\nSince it's only data type and shape, I will definitely try a new format (probably a custom one to avoid more dependencies).. Seems a good reference!\nIt requires known shape of tensor? That's a big limitation. But there is no existing serialization protocol for \"list of tensors\". It looks better from the interface. But it'll introduce two copies. One from zmq to a string tensor, and one from the whole string tensor to decoded list of tensors.\nIf another queue is added after the decode, the latency of copy can be hidden.. Thanks for the note. I've checked the Example proto, it supports variable size, but it doesn't contain the meta data inside (the size and dtype). So it's still not an ideal way to transfer \"list of tensors\".\nAlso we tend to move away from protobuf because they are slow, and causing build issues.. Nice work! Looks like it's using msgpack to replace protobuf. I guess the next question is how to do the equivalent encode method in Python (so that we can send a DataFlow to TF through zmq).. Generate tf.Tensors and with what protocol to send them?. But it doesn't build smoothly..\nI won't have time for this one in the near future.. It would be nice to have this op! But recently I don't have to time to look at your implementation in details. I guess there are at least these points to clarify:\n1. Make it support more tensor types? (including string)\n2. How does Python side and C++ side agree on the serialization format they use? Maybe that's some msgpack internals (packb) but I'm not very familiar.\n3. The performance? If it's still slower than queues, it would be necessary to at least understand what amount of time copy and deserialization contribute.. The only problem with the current zmq op seems to be protobuf, and it's easy to solve: just need to serialize shape + dtype somehow. Maybe it is easier to do this way.. I just removed protobuf dependency. The op should be at least runnable now but there is still work to do.. Thanks very much for trying out! I'll play with your code soon.. https://www.tensorflow.org/versions/master/deploy/distributed \nThe tutorial I'm looking at doesn't use supervisor. Why do you think it's needed?. In sync training, the optimizer needs to maintain the global_step by itself, so I shouldn't use a callback to maintain it. That's a problem.. Cannot reproduce the problem.. The model is unrelated here. I'd guess it's better to try a different version of numpy or tensorflow?\nOthers have seen this as well: http://stackoverflow.com/questions/43071987/tensorflow-typeerror-float-argument-must-be-a-string-or-a-number-not-dict. Or if you're curious you can print the feed_dict at \"/home/agupta/TensorPack/tensorpack/tensorpack/train/input_data.py\",. It should be a dict of numpy array.. Then it's probably msgpack or zmq related issue.\nI'm using numpy 1.12.1, msgpack-python 0.4.8, msgpack-numpy 0.3.9, pyzmq 16.0.2.. The shape and type are\npython\n        return [InputDesc(tf.float32, [None, None, None, 3], 'image'),\n                InputDesc(tf.int32, [None, None, None], 'edgemap')]\nWhere the first dimension (batch size) is set to 8 in the data pre-processing stage.\nIf an upgrade works for you, could you identify which package is causing the problem? so I can update the requirements.. I don't remember what number you should get in the end, but the shape looks good.. The conversion was done in get_data already.\nShould add it in build_graph if train from scratch. Just change the mean/std constant.\nAlso #176 . Yes. Just the --load option. It works for both npy and the checkpoint.. Sorry. For imagenet-resnet.py it works only for evaluation.\nBecause for evaluation it has:\nsession_init=get_model_loader(model_file),, which supports both format.\nHowever for training it is config.session_init = SaverRestore(args.load), which only accepts saver checkpoint.\nYou can change the code for your need. Note that get_model_loader is very simple (stupid):\npython\n    if filename.endswith('.npy'):\n        assert os.path.isfile(filename), filename\n        return ParamRestore(np.load(filename, encoding='latin1').item())\n    else:\n        return SaverRestore(filename). Duplicate of #117 .. The augmentor now would copy by default. So that issue was fixed. If no dataflow makes modifications by default, then no dataflow needs to make an explicit copy for potential modification.\nOtherwise every ProxyDataFlow would have to make an explicit copy.. Looks like I missed it.. just uploaded.. ClassificationError by default looks for this tensor in the graph. Otherwise it doesn't know \"what error\" to compute (e.g. you can have many \"errors\" in your model).\nFor example here it reports two errors, and the two corresponding tensors are defined here.\nSee the documents of ClassificationError. No file format restrictions. But you'll need to write such function in python to yield your images and labels and change the existing get_data function to use your data and (maybe) your own preprocessing.\nSee the documentation. The existing BSDS dataset yields data of specific shape and size and value range (see its documentation). So if your dataflow is different, the preprocessing may have to be different.. Feel free to reopen if you still have questions.. How do you install tensorpack? Please use the latest version of tensorpack. (see README. Usually we don't take \"example request\", but I'm interested in this one as well.. I already have my code ready and producing faces.. Uploaded a BEGAN model and some samples. They do look much nicer.\nhttps://github.com/ppwwyyxx/tensorpack/tree/master/examples/GAN. But we don't have to use the saving/restoring routines of it right?\nI haven't tried running with supervisor so I'm not sure what part of it is necessary.. Closing this and tracking the issue in #291.. Sorry but there's nothing I can do about it given the little information.\nIn most cases, any segfault is a problem of either tensorflow or the environment.\nIt's better if you could give more information, e.g., when does it segfault. Does it always segfault at the same place. At which line does it segfault. Have you tried other TF versions (e.g. TF1.1). Have you tried disable/enable GPU. Have you tried other machines, etc.... Have you tried to disable GPU? How did you install opencv? Opencv can cause segfault if installed from 3rd-party prebuilt or sometimes on GPU.. There is a plan about it but not ongoing.\nBasically the goal is to let the monitor support data types other than scalar.. If it is just a fixed validation set, why do you need to do it during training instead of before training?\nAnd you can do that by writing an Inferencer. No need to hack anything.. You can now use self.trainer.monitors.put_image(name, array) inside a callback, to put a numpy array to tensorboard.. The \"theory\" doesn't necessarily work in practice.. Yes. For classic GAN, the number is unrelated to image quality.. In theory it maximizes the likelyhood not the loss.\n. Another difference is that, with\nsess.run(self.d_min)\nsess.run(self.g_min)\nG and D are trained on different batch of input images. When written together they are trained on same batch of images.. @Skylion007 just made a MultiGPU GAN trainer in 01486c39dfbe141adbff3. \nTested on BEGAN, it scales 1.95x on 2 GPUs and 3.88x on 4 GPUs.\nMay not be the most efficient way but the performance is OK.. You're not using the correct \"alexnet-126.npy\" file.. You have to have protoc to run anything related to imagenet.\nI don't know how to install it on windows.. You need to write a DataFlow to handle your own data format. See documentation. Feel free to reopen if you still have questions.. Do you have a use case for this function?. The function was just a hack to go against TF scope rules. \nI'm not aware any current use of this function. If you don't have a good use of the function I'll just remove it.\nIf you want to open a non-reuse scope inside a reuse scope, it may be better to just open an issue in TF and question their design.. You can just use tf.nn.dropout in your model code with the argument you like.. What do you mean by \"load\"?\nYou can easily use multiple models. Just create multiple PredictConfig and then predictors. Is that what you want?. You can just write \n'with tf.device'\nin your two models.. This may be a silly question but did you modify \"NR_GPU\" in the code?. You need NR_GPU=4 for 4 gpus. Otherwise it'll actually use a total batch of 256.. NR_GPU is hard-coded as 8, so BATCH_SIZE will be calculated wrong.. Yes.\nDo you mean you cannot reproduce with my code or others?\nI'm using double DQN and that's what I'm trying to reproduce. A cmdline flag can change it to DQN and I believe it can be reproduced as well.. Yes. But I tried the original architecture in the paper and it works similarly well.. Yes.\nIn the code there is some lines implementing the original arch. I just uncomment it and run.\n1. I don't remember how long it takes. It iterates 2x faster but may take more iterations to train. The gpu is TitanX. Framerate has different definitions.\n2. About 400 on breakout.. I didn't know about that project.. It doesn't matter. These libraries are only used in python2.. Why?\nPython3 is supported. Those libraries (subprocess32, functools32) doesn't matter.\nThe warning you saw is intended.. hmm I've hard-coded to use the first GPU. I can make some changes.\nBut using a smaller batch size in validation will be faster than CPU anyway.. Now you can use config.predict_tower = [-1] to let it use CPU. Or config.predict_tower = [1] to let it use GPU1 instead of GPU0 (default).\nBut note that TF doesn't support convolution with NCHW on CPU. So it won't work out of the box for convnet anyway (if you use NCHW for speed).. TF just released their \"good\" distributed code yesterday. So it will take some time.. Not supported yet.\nSupported but performance is as slow as tensorflow/benchmarks.\nDuplicated of #144 . Closing.. This has nothing to do with TrainConfig.\nYou can tell whether you're in training phase in the model by get_current_tower_context().is_training. See this line: mnist example, and it is also explained in the docs.. The links are now updated.. I like this interface. Not too hacky and convenient.\n. Turned out the only change needed is to assign variables to different GPUs, and make sure sum of gradients is colocated with the variable.\nAlso, because variables are scattered around, BN update needs to be delayed (otherwise will hurt performance a lot).\n+-------------------------------+----------------------+----------------------+\n|   2  Tesla M40           On   | 0000:09:00.0     Off |                    0 |\n|  0%   60C    P0   170W / 250W |  10998MiB / 11443MiB |     99%      Default |\n+-------------------------------+----------------------+----------------------+\n|   3  Tesla M40           On   | 0000:0A:00.0     Off |                    0 |\n|  0%   62C    P0   173W / 250W |  10996MiB / 11443MiB |    100%      Default |\n+-------------------------------+----------------------+----------------------+\n|   4  Tesla M40           On   | 0000:0B:00.0     Off |                    0 |\n|  0%   52C    P0   176W / 250W |  10996MiB / 11443MiB |     99%      Default |\n+-------------------------------+----------------------+----------------------+\n|   5  Tesla M40           On   | 0000:0C:00.0     Off |                    0 |\n|  0%   60C    P0   180W / 250W |  10996MiB / 11443MiB |    100%      Default |\n+-------------------------------+----------------------+----------------------+\n|   6  Tesla M40           On   | 0000:0D:00.0     Off |                    0 |\n|  0%   56C    P0   159W / 250W |  10994MiB / 11443MiB |    100%      Default |\n+-------------------------------+----------------------+----------------------+\n|   7  Tesla M40           On   | 0000:0E:00.0     Off |                    0 |\n|  0%   58C    P0   164W / 250W |  10994MiB / 11443MiB |     99%      Default |\n+-------------------------------+----------------------+----------------------+. Thanks for reporting. Should be fixed now.. I feel very confused about what it really does when these convolutions are mixed in one function, especially when the concept of \"separable\" and \"depthwise\" already kind of overlap.\nI hope we can just provide individual functions so that they have their corresponding counterparts in tf.nn.* or tf.layers.*, with the same set of arguments. . In the future we can hopefully be free from maintaining all kinds of layers, and just copy or wrap tf.layers or other well-maintained wrappers. tf.layers have the attribute accessor as well (.variables, etc) so this could be possible. The only inconsistency now is the naming (W vs weights, out_channel vs filters, etc), so I hope to keep them in sync at least in terms of functionality.\nIf I restart the project I would just use tf.layers.. but at that time there was no such thing. \ud83d\ude1e . dtype argument is needed, but why will the domain be useful?\nMy thought was, that if you ever need \"RandomUniformData\", then it's better to write a \"RandomUniformData\" rather than using something called \"FakeData\".. Closed by #262 . TensorFlow only supports NHWC on cpu.\nIf you really want to use CPU, you can change data_format to 'NHWC' at this line, and remove the layout transform at this line.. cifar10-resnet.py is written for GPU.\nYou need to change the relevant symbolic code after changing data_format.. If you can understand tensorflow symbolic code, the changes are easy to identify in build_graph. If not I suggest you learn basic tensorflow symbolic programming before using tensorpack.. Shapes should have something to do with data_format, for sure.. ```diff\ndiff --git i/examples/ResNet/cifar10-resnet.py w/examples/ResNet/cifar10-resnet.py\nindex d2cbc6c..764ca75 100755\n--- i/examples/ResNet/cifar10-resnet.py\n+++ w/examples/ResNet/cifar10-resnet.py\n@@ -48,12 +48,10 @@ class Model(ModelDesc):\n     def _build_graph(self, inputs):\n         image, label = inputs\n         image = image / 128.0\n-        assert tf.test.is_gpu_available()\n-        image = tf.transpose(image, [0, 3, 1, 2])\n     def residual(name, l, increase_dim=False, first=False):\n         shape = l.get_shape().as_list()\n\n\nin_channel = shape[1]\n\nin_channel = shape[3]\n     if increase_dim:\n         out_channel = in_channel * 2\n\n@@ -68,12 +66,12 @@ class Model(ModelDesc):\n             c2 = Conv2D('conv2', c1, out_channel)\n             if increase_dim:\n                 l = AvgPooling('pool', l, 2)\n-                    l = tf.pad(l, [[0, 0], [in_channel // 2, in_channel // 2], [0, 0], [0, 0]])\n+                    l = tf.pad(l, [[0, 0], [0, 0], [0, 0], [in_channel // 2, in_channel // 2]])\n         l = c2 + l\n         return l\n\n\n\nwith argscope([Conv2D, AvgPooling, BatchNorm, GlobalAvgPooling], data_format='NCHW'), \\\n\nwith argscope([Conv2D, AvgPooling, BatchNorm, GlobalAvgPooling], data_format='NHWC'), \\\n                 argscope(Conv2D, nl=tf.identity, use_bias=False, kernel_shape=3,\n                          W_init=variance_scaling_initializer(mode='FAN_OUT')):\n             l = Conv2D('conv0', image, 16, nl=BNReLU)\n``. @Superlee506 When reporting problems please following the issue template. Thanks!. Even though you have multiple GPUs you still need to split the task to run on multiple of them..logger.set_logger_dir('/project/logs/' + os.path.basename(file))` would work fine.. Probably RAM on my test machines wasn't very good. I just ran a hdparm benchmark on a new machine and see 11G/s cached read speed. This seems to be close to the speed you get so I think we can update the docs.. No. my number was after batching to 256, the exact code in the tutorial.\nAnd it seems like you've done imdecode so the performance is lower.. I agree they are common and I'm only concerned with the type confusion. \"Encode\" always means \"convert to bytes\" except in opencv. In fact, encoding to nparray is only useful when you decode with opencv. \nWhen we add distributed training, we probably have to use jpeg-encoded bytes in a TFRecord and decode with TF, to reach maximum speed. At least that's what google is doing. In that case we'll need encode/decode to handle bytes.\nI don't mind adding it if it could at least support bytes, but it will just look too complicated (both the docs and the code) given that it could be done in one line. Originally I used bytes in the tutorial and I only added nparray conversion for speed (and not even sure if it really helps).\n\nI agree on the rename. But MsgPackDecoder is not very good because it decodes dp[1], which is an unexpected behavior from the name. Decoding from dp[1] is LMDB-specific behavior (just like decoding nparray is cv-specific behavior) so we better keep LMDB in the name.. I didn't get why this is needed. What's the benefit of using a new API serialize/deserialize compared to the original APIs dump_to_lmdb and LMDBDataPoint? You need to write decode/encode somewhere anyway.. Thanks!. Slice is only useful when the memory buffer is full. link to doc\nLike:\nlearning_rate: 1e-5\nother_param: 1e-4. It will prompt you for action if the logging directory exists.\nYou can set a different directory #266 , or pass an action so it won't ask you for it, e.g. auto_set_dir('d').\nhttp://tensorpack.readthedocs.io/en/latest/modules/utils.html#tensorpack.utils.logger.set_logger_dir\nAll the logs from tensorpack will be saved in logging directorey/log.log by the way.. Looks like this is section4.1 of the paper. \nWere you able to see the network learning expected filters? If it's successful it would be a good example to add.. Since they did open source their code https://github.com/dbbert/dfn/blob/master/experiment_steerableFilter_tensorflow.ipynb, I guess it would be better to follow the network & hyperparam rather than making things up.\nPlus they seem to have a more efficient dynamic conv implementation.. Do you know why they get to a very small loss with only 490 steps (and their loss is summed over the image and averaged over batch). Maybe because their dataset is easier?\nAnyway the current result you showed is pretty good.. What's the issue with logging?\nI do find that you may need to set_logger_dir to a different place otherwise the second training would see the old logs and thinks the training has finished already.. Even when you've set logger_dir to a different place? Do you know which file it is referring to?\nI did find that I should've unload the existing file handler when logger_dir was changed. But this seems unrelated.. Oh I see. It attempts to delete the old log file while it is open. It is allowed in linux.\nI'll see if I can fix it.. Don't have a windows to test, but from strace log it looks like now all files are closed before performing the delete action.\n[pid 14405] open(\"train_log/mnist-convnet/stat.json.tmp\", O_WRONLY|O_CREAT|O_TRUNC|O_CLOEXEC, 0666) = 11</home/wyx/Work/projects/tensorpack/examples/train_log/mnist-convnet/stat.json.tmp>                                             \n[pid 14405] close(11</home/wyx/Work/projects/tensorpack/examples/train_log/mnist-convnet/stat.json.tmp>) = 0                                                                                                                            \n[0518 16:02:39 @monitor.py:294] accuracy: 0.29745                                                                                                                                                                                       \n[0518 16:02:39 @monitor.py:294] cross_entropy_loss: 1.9657                                                                                                                                                                              \n[0518 16:02:39 @monitor.py:294] lr: 0.001                                                                                                                                                                                               \n[0518 16:02:39 @monitor.py:294] param-summary/conv0/W-rms: 0.46391                                                                                                                                                                      \n[0518 16:02:39 @monitor.py:294] param-summary/conv1/W-rms: 0.083028                                                                                                                                                                     \n[0518 16:02:39 @monitor.py:294] param-summary/conv2/W-rms: 0.084169                                                                                                                                                                     \n[0518 16:02:39 @monitor.py:294] param-summary/conv3/W-rms: 0.083865                                                                                                                                                                     \n[0518 16:02:39 @monitor.py:294] param-summary/fc0/W-rms: 0.034486                                                                                                                                                                       \n[0518 16:02:39 @monitor.py:294] param-summary/fc1/W-rms: 0.062386                                                                                                                                                                       \n[0518 16:02:39 @monitor.py:294] regularize_loss: 0.0049347                                                                                                                                                                              \n[0518 16:02:39 @monitor.py:294] total_cost: 1.9706                                                                                                                                                                                      \n[0518 16:02:39 @monitor.py:294] train_error: 0.70253                                                                                                                                                                                    \n[pid 14405] close(10</home/wyx/Work/projects/tensorpack/examples/train_log/mnist-convnet/events.out.tfevents.1495148557.KeepLearning>) = 0                                                                                              \n[pid 14478] +++ exited with 0 +++                                                                                                                                                                                                       \n[pid 14405] close(9</home/wyx/Work/projects/tensorpack/examples/train_log/mnist-convnet/log.log>) = 0                                                                                                                                   \n[0518 16:02:39 @logger.py:92] WRN Directory train_log/mnist-convnet exists! Please either backup/delete it, or use a new directory.                                                                                                     \n[0518 16:02:39 @logger.py:94] WRN If you're resuming from a previous run you can choose to keep it.                                                                                                                                     \n[0518 16:02:39 @logger.py:95] Select Action: k (keep) / b (backup) / d (delete) / n (new):. why not?. StagingArea will be added to contrib, if I'm not mistaken. It's now in contrib in nightly.\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/staging/init.py. You can use the identity workaround. I'll also prepare a fix.. Cannot reproduced the problem. Could you tell how you installed tensorpack and tensorflow? Maybe one of them is outdated.. I can reproduce the bug with 1.0.1. So probably a bug in TF.\nI could fix it in tensorpack as well.. I don't fully understand your question. From what I can tell the two implementation handle the ratio in the same way.\nMaybe you didn't see this line of code? cost = tf.reduce_mean(cost * (1 - beta)). Because that would be wrong I think.\nThe test of whether label is true or false is already done in weighted_cross_entropy. Pos/Neg cost are already weighted.\nIf you think the current code is not equivalent to the original formula please show why.. If that's what's confusing you: pos_weight in my code is not the positive ratio, 1-beta is.\nClosing now. Please reopen if you still have questions.. Oh I probably renamed the collection some time ago.\nThis shouldn't affect dump-model-params. I'll fix it.. In the code.\nOn all variables named \"W\" (no bias, no bn scaling factors) of all layers.. It will depend on what you mean by \"weights\".\nAs I said, no bias, no bn scaling factors.. msgpack is in the requirement, but no version is specified now.\nWhat's the behavior when a wrong version is installed?. Are there more details? \nOtherwise I can only do some guess on what version is needed, based on msgpack changelog.. Thanks for trying to figure out the problem! For your information I've never installed msgpack anywhere. I always only install msgpack-python/msgpack-numpy with pip (and I always use latest version).. Looks like this is mentioned in their docs, that data serialized with an older version (<0.3.9) cannot be deserialized with a newer version.. Just updated the dependency to require msgpack-numpy>=0.3.9 1175aade9a482. Thanks for your help!. Could you provide the command you're using, and the full log of the second run?\nIt looks like you're not running the examples. Could you explain/post what changes you've made to the code?. Have you used --load at all?. If you have such code already, you can call your code or tflearn wrapper directly within a tensorpack model.\nWe might add some simple wrappers around tf.layers in the future (#236), but directly calling tf.layers works the same.. Closing this and tracking the issue in #291.. Oh it's actually still working. I thought I broke it.. It should require a feed if your \"lastLayerRepresentation\" depends on a feed.. What do you mean by real data?\nDo you want to run the network in training mode or test mode?\nDo you want to run it on the data you're currently trained on or some other data?\nThe easiest way if you just want to run the training network on the data you're currently trained, is to use before_run and after_run, which have identical API with tf.train.SessionRunHook.\nAnd another problem is that why would your \"lastLayerRepresentation\" requires a feed. It shouldn't happen if inputs are properly defined by tensorpack. Could you post some logs on that?. I see what's going on. You're not supposed to use model.lastLayerRepresentation directly -- there are many of them. When you use multiGPU there is multiple of this tensors on each GPU. When you enable inference there is one extra of this tensor in the inference graph. Using model.xxx cannot give you the tensor you need.\nFor before_run/after_run:\n```python\ndef before_run(self, ):\n    return ['fc/output']  # the name of the tensor(s) to eval. (you can also return a SessionRunArgs to be compatible with SessionRunHook). \nYou can print the tensor in _build_graph to figure out the name of your tensor (remember there could be multiple of them)\ndef after_run(self, , values):\n    # values.results contain the value of the tensors\n``\nSee https://www.tensorflow.org/api_docs/python/tf/train/SessionRunHook.\n. You can also checkout an existing summary callback usingbefore_run/after_run: https://github.com/ppwwyyxx/tensorpack/blob/7775587529118f2c3dcb28fd9bc30a2b259e57c9/tensorpack/callbacks/summary.py#L54. No because different GPUs train on different data.\nIf you need them all just add them in before_run.. before_run/after_run is called together with everysess.run. That is every step in an epoch.\nHaven't got time to write any tutorials about writing callbacks.. I should definitely do that soon.. It would be nice of course.\nBut you can just calltf.layers.conv2din your model, with thedilation_rateoption.. Nowbuild(checkpoint)` can only be called once per instance. So this class is essentially one-time-use only.\nI don't know if this use case really exists, but if you move the checkpoint argument into export, you can then do the following:\n```python\ne = Export(model, inputs, outputs)\ne.build() # or do it in init\ne.export(checkpoint1, export1)\n...\ne.export(checkpoint2, export2)\n``. One last thing, could you put the file intfutils/` instead?. What is \"version\"? Not used anywhere.. Yes I hope to keep the shape output. Just want to use tf.layers for the actual implementation.\nI guess first step we can switch the implementation but keep all the APIs/names unchanged (by custom_getter). \nOne bottleneck now is that nested custom_getter is unsupported for the moment (tensorflow#6634). The change is supposed to be transparent to users. There won't be any trouble as said in the issue.. You use whichever you like, and implement new layers in whichever ways you like. They are just doing the same thing with different interface (different function name, argument name, etc) and you can always use one in place of anther. Why would there be confusion?. Closing. Only two layers were not switched to tf.layers implementation:\n1. Conv2D. tf.layers doesn't support group conv.\n2. BatchNorm. tf.layers doesn't support use_local_stat != is_training.. op.run().\nhttp://tensorpack.readthedocs.io/_modules/tensorpack/callbacks/graph.html#RunOp. What is your use case? I think you can always just write tf.device around your predictor.. You can try again now.. batch norm uses moving averages which get changed whenever training data pass through.. You can disable moving average update by overwrite the default callback list (remove RunUpdateOps from extra_callbacks). http://tensorpack.readthedocs.io/en/latest/tutorial/callback.html. https://github.com/ppwwyyxx/tensorpack/issues/87#issuecomment-270545291. I think bn averages should be updated when you fine tune.\nIf you'd like to only update some of your bn update ops, you can write a new callback similar to RunUpdateOps, and filter the ops by your self. The code is very simple:\n```python\nclass RunUpdateOps(RunOp):\n    def init(self, collection=tf.GraphKeys.UPDATE_OPS):\n        def f():\n            ops = tf.get_collection(collection)\ndo your filtering here\n        if ops:\n            logger.info(\"Applying UPDATE_OPS collection of {} ops.\".format(len(ops)))\n            return tf.group(*ops, name='update_ops')\n        else:\n            return tf.no_op(name='empty_update_ops')\n\n    super(RunUpdateOps, self).__init__(\n        f, run_before=False, run_as_trigger=False, run_step=True)\n\n```. Could you loop over the dataset and do that yourself?. Why would mean/std of your dataset change?. >  However, if you wanted to experiment about say dividing by the difference between your predictions and the actual dataset, that would be interesting.\nIt's easy if this is what you want to do. Though I don't see it has anything to do with your original question.\nYou can just write a callback which runs some forward predictions and do whatever with its output. CycleGAN example has such a callback.. #58 . I don't have any. I do know that VGGs are very hard to reproduce..  #214. 1. The \"high-water-mark\" is what people called buffer size. http://tensorpack.readthedocs.io/en/latest/modules/dataflow.html#tensorpack.dataflow.PrefetchDataZMQ\nIf both cpu and gpu are starved you might want to reduce the communication by e.g. compression.\n\nA lot of examples run testing after training, for instance this one.. If logger.set_logger_dir sees an existing directory, it will still give an interactive menu. Of course it's harder to write with the dumb set_logger_dir interface. But \"auto\" is not so smart either. \nI can totally see that adding a \"name\" option would make things easier, but what if I have to store to a different directory (this is happening to me now because of disk space)? Another \"dir\" option? What if someone doesn't like \"train_log\" (#266 seems to suggest so)? Another option? Maybe the only thing people wouldn't hate is the basename part... That said, I'm OK with making a function for \"getting the basename for the entry point\" into utils. It's just 3 lines but not trivial lines..\n\nTo try new ideas I just rename the main script. I used to modify the scripts in-place until I realize it's hard to stay on track. I ran idea1-run1, idea2-run2... in parallel, and the next morning I forgot what exactly is idea1 and run1, because the file is now on idea2.\nI've seen some scripts from openai, where every time something is run, a git diff is created to the log directory. That could be also a solution to my problem... An environment variable for log directory may not be a good idea, because I don't always want logs in the same place -- for small experiments I prefer them in the current directory. Env vars should be used for something that's always the same.\nImaging I don't have this function at all and someone sends a PR to add the same auto_set_dir, I certainly wouldn't like it because it's personal preference: current dir + \"train_log\" + basename. I think it's already a mistake to bake in my own preference -- clearly now it doesn't work for everyone (including you, #266, and me as well).\nI don't know now what's the best thing to do about it. I guess one thing is to reduce the use of it in examples so others know the existence of set_logger_dir. Adding an option to partially fix the awkwardness might be OK, but the fix is very partial -- I still have the directory problem.. More options can be added of course.. but the point is that essentially people want to customize every part in the path. Using the more general interface is the clean solution for users to understand what to do, rather than adding more and more options and explaining in the doc how the actual path is made from those options.\nAnyway I think the name options is OK. . It is data parallel but variables are spread among GPUs.\nhttps://www.tensorflow.org/performance/performance_models#parameter_server_variables. In the other mode https://www.tensorflow.org/performance/performance_models#replicated_variables each tower has its own var scope.\nYes.. There are too many things you can do in opencv and I don't expect to cover all of them. Users who need this can write their own augs, and I think people usually don't care much about stuff like border value.\nYou're welcome to send PRs. But this feature is probably not something I'm going to work on.. Most likely your data is too slow. You can probably see that input queue size is nearly zero in the log.\nhwm in PrefetchDataZMQ rarely affects speed and it's better to keep it as default. You can use TestDataSpeed to benchmark your dataflow pipeline and figure out which part is slow, or in particular which part may become slower overtime. See some info in tutorial as well.. If your data is 90it/s and training is only 1.8it/s, bottleneck is not on the data but in the graph.\nIf GPU utilization is low, it's the CPU operations in the graph that's slow, or too many CPU/GPU transfer, or (unlikely) tensorflow doesn't implement some GPU kernels very well.\nIf the above statement is correct you may want to check the operations involved in your graph. For example if you used preprocessing ops you might want to explicitly put them on CPUs as mentioned in tensorflow performance guide. Usually a bisection can quickly point me to the slow area. You can try profiling as well.. Oh I don't know such machine exists. I'll try to fix.. Cannot reproduce your problem. Have you seen logs like \"Model saved to train_log/cifar10-resnet/model-390\"? It'll appear after the first epoch.. Maybe.\ntensorpack did nothing special about saving the events. I think it's not a bug in this project. Closing for now.. ClassificationError is only (slightly) not equivalent to ScalarStats('some_error_tensor_in_the_graph'), when the size of validation dataset is not a multiple of validation batch size. Most of the time, as long as you can compute the per-batch error in the graph, ScalarStats('tensor') (which gives you mean error for all batches) should be enough and you don't need to add new metrics to callbacks.\nIn terms of the interface, I think it's a TensorFlow design flaw that they provide only streaming interface without a non-streaming interface. And do not have reset is an even bigger issue..\n. But as mentioned https://github.com/tensorflow/tensorflow/issues/9498#issuecomment-307884950 a non-streaming version is usually very simple to implemented from scratch. So in tensorpack using a non-streaming version + ScalarStats should be enough for most situations.. #334 . Btw I have a faster-rcnn code in tensorpack which gets reasonable performance on coco. But I'm busy with other stuffs and don't have time to clean it up and open source it recently.. https://github.com/ppwwyyxx/tensorpack/tree/master/examples/FasterRCNN\nStill WIP but should work now.. Providing more and more predefined layers is not a focus of this project. You can use any symbolic function available in tensorflow and it's easier if you implement this by yourself in your training code.. @kdplus Please try the above suggestion.\n@PatWie This is a big problem with the current interface, also brought out by some others earlier. I would want to improve clarity of ModelDesc in different aspects, but haven't got a concrete plan yet.\nFixing it in the callback can only solve it momentarily. Similar problems happen in a lot of other places. In general, there should be an API to use the name 'cost' to get all costs from each tower. And users should be able to define more tensors like this to use in their callbacks.. About the original question, after latest changes 897d29e to the trainer, using \"cost\" would be sufficient. \"tower0/cost\" also works.. When training with queues, the input tensor is called input_dequeue:0, input_dequeued:1, the output tensor is loss:0\nIt also builds the graph for validation, where input tensor is named e.g. input:0, image:0, and output tensor is towerp0/loss:0. You can use these names as well.\nYou can print(tensor.name) to see the name. This is currently very unintuitive, I'll fix it in the future.. Could you be more specific what's wrong with the current graph in tensorboard?\nMy understanding is that it includes something you don't want to see. But a graph is complicated because neural net training is just so complicated, no way I can automatically guess what you want to see in the graph and what you don't. If you're OK with what your code sample produces, then you can just use it.\nWhat's wrong with batch norm? I think their ops would be grouped under the variable scope so it should look fine.. I still could not see what's the problem with the current graph in tensorboard. In your code you still dumped everything in _build_graph to tensorboard. I can see the only difference to what is now done in tensorpack is that 1. you won't dump the backward part 2. you won't dump the inference tower. I would expect these two parts won't make too much mess in tensorboard.\nSee docs for your problem about the context.\nCould you maybe open a new issue? I think this question is unrelated to this thread.. @PatWie I updated the issue with some more concrete use cases.. In addition to the above comments to @Skylion007 \nPartial inference: yes the graph might vary a lot in prediction and that's one of the use case I put in this issue.\nLoading a network: I don't see any problems with that.\nInputDesc: it exists mainly for working with InputSource, which is an abstraction of all possible input source, not only including placeholders, but could switch among tensors, queues, etc.\nIn tensorflow you cannot pump data into any part of the subgraph when data comes from an op.\nParameterize ModelDesc: as @PatWie said you can do it.\nMore complicated models: as mentioned there is no plan to add more models.\ncollect_variables is not a function of tensorpack. \nIt's an example -- meaning that's something the user (in this case myself), should write.\nThe user might not write it clear enough but that's unrelated to the design of tensorpack.\n. Possible solutions to the problems left:\n\n(DONE) Allow users to build the whole graph by themselves. (i.e. no ModelDesc at all)\nThis seems to solve all the problems mentioned, and provide best flexibility for complex graphs. \n\nIt's easy to do: currently the trainer does two jobs: setting up the whole graph from ModelDesc, and run the loop. We just need a trainer which does not set up the graph.\n\n\nModelDesc will be only for simple graphs, then. But it's still awkward to access tensors. The ideal way is to simply do model.towers[0].cost, or model.towers['tower0'].cost for a tensor set by self.cost = .... This is possible but I still have to see how nicely it can be done.\n\n\nMake it easier to build graph before initialize trainer. (by SomeTrainer.setup_graph()?). This would avoid the \"guess the name of that tensor\" game.. One general philosophy about design is that: when you pass something to someone that, in certain cases, logically doesn't need it, it's a design problem. Packing a bunch of (possibly None) objects together is a way to hide this design problem (because then you only pass one giant object), but the problem still exists.\n\n\nModelDesc packs three things: InputDesc, build_graph, get_optimizer.\nInputDesc is only for working with different types of InputSource. Different InputSource creates very different tensors but they share this common metadata. InputDesc is not used for anything else in the whole project.\nget_optimizer is only used for training, so ideally ModelDesc shouldn't be used to inference.\nTrainConfig packs a number of things. A lot of them are not useful in certain cases, e.g. only multiGPU trainers need towers. A custom trainer may not need model or dataflow.\nPacking all stuff together introduces another problem in TrainConfig: those stuff cannot depend on each other, although they may logically do. For example, a callback may depend on the outcome of building the model (need to use tensor names).\nOne story is that at the beginning of tensorpack, optimizer is actually part of TrainConfig. This again is showing these options are packed together a bit arbitrarily.  \nThe new trainer interface in development doesn't depend on either ModelDesc or TrainConfig. Instead of taking those packaged objects, it takes individual options. ModelDesc and TrainConfig can then be used through high-level wrappers around the trainers. Then users will get more control over the details when the high-level interface isn't enough, or when a different high-level interface is needed. For example, it will be much easier to train a Keras model with the new trainers.\n(nothing is broken for the moment. just sharing some progress). With the new trainer API, we can answer the problems at the beginning:\n\nIs it OK to create placeholders inside build_graph?\nWhat symbolic functions are allowed to use and what not? (e.g. tf.layers.batch_norm? tf.train.input_producer?)..\n\nYou can use anything as long as your training iteration can still run. e.g., if you use a placeholder, you need a callback to feed it when the placeholder is a dependency. If you use tensorflow input ops, you may need a callback to start the queues.\n\nWhat to put in get_inputs and what not? Is this interface even necessary?\n\nThey are used only to setup InputSource in SingleCostTrainer as well as InferenceRunner. You don't have to use InputDesc if not working with InputSource. But you'd better use InputSource for performance reasons.\n\nHow to access a tensor a bit later? Because setting self.xxx sadly doesn't work (#287), and using the tensor names is not easy.\n\nA new abstraction called TowerTrainer will cover models whose graph can be built by calling a function (tower function) with input tensors one or more times. SingleCostTrainer belongs to TowerTrainer. \nThe tower function will keep track of the scopes every time it get called, and then the tensors can be accessed. The API is still not ready yet, but should be something like trainer.tower_func.towers[0]['cost'], or trainer.tower_func.towers['tower0']['cost'].\nOf course, you can only access the tenors after the graph has been built. So if using high-level one-shot training wrappers, you will only have the chance to use this interface inside callback.setup_graph.\n\nOn the contrary, self.cost needs to be set. This seems very hard-coded, and the reason behind it is that self.cost is only set because some (but not all) trainers need it. This contract between Model and Trainer needs to be addressed in a clearer way.\n\nIt is hard-coded. We'll keep it this way. But now SingleCostTrainer takes a function which explicitly returns one cost, i.e., the hidden contract become an API requirement. Other trainer doesn't have any notion about \"cost\".. Btw. build_graph(tensorA, tensorB) looks more natural than _build_graph(inputs): tensorA, tensorB = inputs. The latter exists for historical reasons. But maybe it's better to use the first one?. I think current calling convention for predictors are positional-args, which means you don't need to use list. You should be able to call with pred(tensorA) or pred(tensorA, tensorB). Please tell me if this fails. The idea for build_graph is similar because I feel this is more intuitive, then pred([tensorA]) and pred([tensorA, tensorB]). But maybe I'm wrong.. Sure you can always convert between the two.\nJust feel writing tensorA, tensorB, tensorC = inputs all the time is redundant.. Will happen in the future. Along with that I'm thinking whether it's better to do return cost instead of self.cost = cost.. I think the future (and final) changes to ModelDesc would be:\n1. Prefer build_graph(self, a, b) instead of _build_graph(self, inputs)\n2. Prefer return cost instead of self.cost = cost\n3. Won't automatically apply REGULARIZATION_LOSSES collection if using the new build_graph+return cost interface. Keep (?) the old behavior if using the current interface. Reasons explained in the old thread.\nThese can probably be smoothly executed without breaking anything.\nI found a lot of changes I made recently were to remove some \"automatic\" parts from the framework (or at least make them more explicit): setting logger directory, automatically load epoch number, clear the inference queue before inference, etc. A lot of these \"automatic\" behaviors look good at the beginning but end up causing trouble in unexpected scenarios. . You can use *args, but that's a good point. I'm not confident which one is better.\nreturn cost will be used by ModelDesc which is intended for single-cost single-optimizer training only. Other models should use ModelDescBase or (probably better) build the graph by their own.. Looking further, however, I think letting users to \"write a class\" is the one thing that makes tensorpack very hard to use at the first sight. Usually you don't use a library by inheritance..\nFor example, if we could transit to something like:\npython\nTrainConfig(\nmodel=ModelFactory(\n  inputs_desc=[],\n  tower_func=lambda image, label: return cost,\n  optimizer=tf.train.GradientDescentOptimizer(0.1)\n)\n...\n)\nThis would probably make everything looks much simpler. The Tensorpack+Keras example is written somewhat like this.. Yes I agree using a class is more powerful, e.g. able to change number of inputs, able to share attributes between these three functions, etc. These concepts are strongly connected and having them in a class makes sense. I thought about how I would write some of the complex models without the class and that doesn't look very good.\nIn the meantime, having a class makes it hard to write things. Somehow I recently felt tensorpack was too complicated for the easiest tasks. I'll need a \"boilerplate.py\" to start working -- that's a bit sad. I don't know whether ModelFactory can be a solution, but I think both renaming and tf.placeholder sound good!. How about 39fa465655a2767b ? Both methods can be supported together, and I think inputs() + tf.placeholder look great.. I'm happy with the current ModelDesc interface:\ninputs() with placeholders\nbuild_graph(self, a, b) which returns a cost (or nothing for generic ModelDescBase)\noptimizer()\nclosing this finally.. This is because gym change their environment in recent versions.\nOn Jun 28, 2017 8:21 PM, \"Qi Wu\" notifications@github.com wrote:\n\nUPDATE: It works on other model and env, such as WizardOfWor-v0\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/ppwwyyxx/tensorpack/issues/319#issuecomment-311851772,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABUTteDHs5t8qTacE4Biz7LMvUaIUJ3zks5sIxg8gaJpZM4OIxRo\n.\n. This is https://github.com/openai/atari-py/issues/16. Closing now. I might upload a new model if I have time.. The model was now updated for latest atari-py.. You've given an implementation already, therefore no plan.\n\nThe implementation claims to be a \"drop-in\" replacement of other tf optimizers. If that's true you can just use it.. Number of GPUs shouldn't affect the speed of your ds, because ds doesn't use GPU at all. You might have done something wrong somewhere, or test the wrong number.. You need to check your dataflow again then. Make sure it could run that many of datapoints without slowing down or throwing exception. I don't think anything in tensorpack will slow it down over time.\nSome users have reported that their dataflow code doesn't throw exception (but silently died) due to bad opencv installed (#68). This may also lead to your situation.. Thanks!. Thanks for reporting!. It's too bad if it takes >10s for a datapoint. Anyway I'll increase it and provide an option.. Things would work just fine if you don't use Keras model. I believe that's because Keras use tf.Variable to create variables instead of tf.get_variable.\nIf you really want to use Keras model, either add a tf.name_scope('tower0') at predict time, or modify your checkpoint to remove the tower0 prefix.\n. You can add keras.backend.learning_phase().name to input_names in PredictConfig, and call it with an extra zero: result = pred([batch, 0])[0]. You might be using old version of Keras.\n$ keras.backend.learning_phase()\nOut[4]: <tf.Tensor 'keras_learning_phase:0' shape=<unknown> dtype=bool>. Just looked at keras code a bit, looks like there is no way to know what the actual name of learning_phase will be before the graph is created.. I don't know what happened in your model.\nAnd I don't know what name scope problems you saw. Keras models were only tested with the most basic functionalities in tensorpack.. If you still have any specific questions feel free to reopen the issue.. These parameters should only affect speed. I don't know why would you fine-tune to a worse score, but in my experience vanilla a3c models are not very stable anyway.. Some parameters (learning rate, entropy factor) may need some tuning for difficult games. These models were trained a year ago and I don't remember much details. 2 days might be somewhere around 2million steps?. https://arxiv.org/pdf/1502.03167.pdf Section 3.2.\n. https://github.com/ppwwyyxx/tensorpack/issues/87#issuecomment-270545291\nJust added it to FAQ. https://github.com/ppwwyyxx/tensorpack/issues/296. Method 2 and 3 works for regularization, because regularization is a loss.\nAlso you can choose what variables to regularize.\n. Opened an issue in tensorflow/tensorflow#11484.. A (better) work around is easy so I also pushed one.. I think you can use whatever names as long as the variables created inside the layer don't conflict with others.. 1. https://github.com/junyanz/CycleGAN/blob/master/models/cycle_gan_model.lua#L88\n2. I'm lazy. It doesn't matter I think. https://gist.github.com/ppwwyyxx/e1900b0e49bfb6771af22cc882b57bb5\nDoesn't make much difference in my experience.. Sure. done. Maybe it's OK to move to RGB default to be consistent within dataflow/.. 1. There is a more general DataFromGenerator\n2. Why do you want this? I don't think this'll be useful for training. For testing you can call whatever webcam libraries anyway.\nIn general just write whatever code in get_data().http://tensorpack.readthedocs.io/en/latest/tutorial/extend/dataflow.html. Feel free to reopen if you have more to discuss.. The _fprop_coord method was originally designed for this. But I never found time to implement it.. Just did a rename because the original name (which I probably made a year ago) wasn't very good.\nThanks a lot for helping! The design is like the following (and you can certainly make suggestions):\n1. If some augmentor changes geometry of an image (more precisely, change the coordinates of pixels), it should implement _augment_coords(self, coords, param), where:\ncoords is an nx2 (or 2xn, whatever) array of (x, y) floating point coordinates.\nparam is the augmentation parameters returned by _get_augment_params.\nAnd returns the new coords.\nDon't have to do this to every augmentor. There might be too many. You can leave others as raise NotImplementedError().\n\n\nImplement this method for AugmentorList as well, which basically calls _augment_coords on all the augmentors one by one.\n\n\nA DataFlow AugmentImageCoordinates, which maps every datapoint of an existing dataflow by the following:\npython\ndef func(dp):\n    img, coords = dp[image_index], dp[coords_index]\n    im, prms = self.augs._augment_return_params(im)\n    dp[image_index] = im\n    coords = self.augs._augment_coords(coords, prms)\n    dp[coords_index] = coords\n    return dp\nThe existing AugmentImageComponents should be a good reference.\n\n\nSome notes:\n1. bounding box is a collection of coordinates, and there are other coordinate-based tasks that don't use bounding boxes (e.g. keypoint localization, non-rectangular bounding box, etc). So it's better to use coordinates.\n2. Coordinates rounding error and misalignment can lead to significant degradation in performance, so it's important to use floating point coordinates, and always consider a pixel as a square rather than a point. It's a bug that some open-source implementations of RCNN doesn't take this into account. For example, for a 4x4 image, the top-left corner point is (0,0), bottom-right corner point is (4,4). The center point of the first pixel is (0.5, 0.5), etc. \nActually this probably makes implementation easier than integer coordinates. For example a resize will be just a simple scaling of coordinates.. Yes. I think it's OK to change it in-place which is currently the way how images are augmented.\nI think assuming numpy arrays is fine, similar to what have been done for images. If we ever need a type check it can be added in the dataflow.. To simplify things, I think for now you can leave it the way it is. And in the future we can either add support of coordinate mapping to it, or improve documents saying that MapImage doesn't support coordinate mapping.\nTo add support to it, I think it's OK to pass an optional function which by default throws exception.. Closed by #335. Thanks again for the PR!. One caveat of the current design: \nAssuming you have box parameterized as (x1, y1, x2, y2), after a horizontal flip, coordinates are mapped to (x1', y1', x2', y2'), but this is not a valid bounding box anymore. The actual bounding box should become (x2', y1, x1', y2).\nI don't know if there is any better way to handle this other than letting the user do it. In general the problem is that the different coordinates in the Nx2 array might have some structure that can be broken by the augmentor -- so probably the user do have to take care of it manually. . Maybe one way is to add the _augment_boxes method, but let it call _augment_coords by default. Then only the rare cases such as flip & transpose needs to be dealt with.\nWhat do you think @haamoon ? since you seem to have worked on r-cnn.. Are you saying that you parameterize a bounding box as 8 floats (instead of 4)? i.e. (x1, y1, ...., x4, y4) . That's the only way transformed points can be converted to a box.. Thanks! I like this solution.. Thank you very much!. This is what \"split\" means. Each output only connects to half channels in the input.\nhttps://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf. TF checkpoint only stores variables. \nWhat do you mean by \"redirect the input placeholders to the loaded pretrained model\"?. In a serialized graph, there is nothing called \"inputs\", \"layers\". These are just human-made abstractions that will get lost once you dump the graph.\nTo do fine-tuning, the best way would be to copy the model code from the old script and modify upon it.. The graph is just so complex, and I don't know how tensorboard organizes a graph and how to make it looks better. Maybe I'll try adding some name scope here and there but I'm not sure.. Until now tensorflow still doesn't have a nice way to reuse a name_scope (https://github.com/tensorflow/tensorflow/issues/6007). Without this feature it's hard to group relevant ops together in tensorboard. . This exists in #221 that hasn't been fixed yet. The op isn't ready for use, now you'll have to build it against the exact same version of protobuf your tensorflow is using, which is hard to tell.. Closing as a duplicate of #221. If we are going to fix it in the future probably we'll not use protobuf at all.. You can do it through a callback. You have to create the ops in _setup_graph, and run them in _trigger_step. This design is exactly to prevent you from doing self.sess.run(tf.trainable_variables()[counter].assign(new_weights) which will add thousands of ops to the graph and blow everything.\n . You may wish to make your assignment an explicit dependency of the training iteration, to save the extra \"sess.run\" overhead. To do this, PostProcessOptimizer, VariableAssignmentOptimizer may help do some easy per-variable assignment, for example, the Wasserstein GAN example clips the weights every iteration. If you need something more complicated, please give some more details -- there is always a way to do it.. It'll be slow if you do the operations in numpy. But there are ways to do it.\nYou should be able to just call var.eval() and var.load(value) to read and write to a variable in trigger_step, in this case you don't need to worry about setup_graph because you created no extra ops in the graph. If you meet errors please post them.. If you have complicated control logic it's easier to do it in callbacks. As long as you don't call var.eval() and var.load() it's not going to hurt much in performance.\nJust create the assignment ops you need to run in _setup_graph() and run them in _trigger_step(). You can access the epoch number in a callback by self.epoch_num.\nYou can do it in the graph as well: In the graph there is no notion of \"epoch\". You can use global_step (with tf.train.get_or_create_global_step()) and tf.cond for the condition.. the WGAN example was just updated to include two ways to clip the weights.. Feel free to reopen if you have more questions.. The two arguments are scaling ratio (e.g. 0.8-1.2), not pixels.. Sounds like a reasonable feature to add.. We do not focus on models/symbolic functions and don't intend to add any more layers. We will shift the existing ones to tf.layers instead. You should be able to use any symbolic functions in tensorflow for your model.\nIn particular, varreplace.remap_variables will be a helpful function to write weight normalization.. You can use either regularizers of tf.layers, or the existing regularize_cost() function -- it's just a symbolic function and you can call it any time anywhere when it suits.. Switched to tf.gfile.IsDirectory. Please reopen if the problem still exists.. With keras you simply cannot do replicated training. That's by design of keras.\nApart from that, you should be able to replace SyncMultiGPUTrainerParameterServer with SyncMultiGPUTrainerReplicated anywhere.. I tested with mnist-convnet.py. Just replace the trainer and it works.. There are examples using other layers. You need a scope name as the first argument. \nX1 = BatchRenorm('bn', X1, rmax, dmax, data_format='NHWC')\n. After dc448bdab391a6eac87de47efd5f8a64caebaf99, API documentation was now updated to include the name argument.. Your method summarizes gradients w.r.t layer outputs, not variables.. It's your decision what to summarize.. Then just summarize both...what's the question?. No. Everything happens in one graph, and it's not trivial to share variables between two graphs.\nThe existing tools are for the most common cases only, for your situation you can setup the validation ops in _setup_graph and write the inference yourself.\nbtw, training and validation uses different model doesn't mean they have to be in different graphs.. Also if the difference is small you can just check if ctx.is_training in _build_graph.. InferenceRunner will call _build_graph with ctx.is_training == False.\n_build_graph was also called multiple times if you use multi-gpu training.. What do you mean \" the performance is the same as 1 GPU\"?\nPlease describe what you saw, not only the interpretation of what you saw.\nTraining logs will be helpful, in particular.. This would depend on how you write get_data().\nIf you use the same batch size in get_data(), the total batch size would be doubled when you use a multigpu trainer.. Then you should set batch size to be batch_size / num_gpu, as is done in resnet example. In each iteration each tower takes one datapoint from your dataflow and train on it.\nTherefore if your total batch size changed, you might want to change your epoch size.. what's the definition of total throughput around all Gpu cards?. If you're talking about images per second, that's just iteration/s * total_batch_size. I don't think there will be confusion if you know what happen in an iteration. Progress bar has nothing to do with this -- it doesn't know what is a batch anyway.. Thanks and I'll find a place in tutorial to mention what is an iteration.. tensorpack requires tensorflow>=1.0.. before_run only adds \npython\nopt = tf.RunOptions()\nopt.trace_level = tf.RunOptions.FULL_TRACE\nto sess.run. I have no clue why it crashes, but it's probably not a tensorpack issue.. It's weird, but likely a TF or cuda issue. And since it works on newer version probably no one in TF team would look at it.. this follows the original paper.. I think what you meant to say is value_loss = tf.reduce_sum(advantage * advantage, name='value_loss'.\nAnd this is wrong, because advantage has stop_gradient.. Ah I see what you are talking about. The two equation actually have different value, but you're right they have the same gradient (actually differed by a factor of 2 I think). The value_loss I mentioned above has the same value, but the wrong gradient.\nI think it's fine to keep it in the current equation to be consistent with the formulation used in the paper.. #218 . @maciejjaskowski also see #353 . You're right.. Btw I'm going to fix it by just letting it call tf.layers.batch_normalization with renorm option, since wrapping symbolic functions to layers is not a focus of this project. You can also just call tf.layers instead of tensorpack layers.\nThe two implementation are checkpoint-incompatible because they have different number of variables. I would just trust the TF version since they wrote the paper.. Thanks a lot!. It is a bug.. Thanks for reporting !. No. It's expected to be used to send data for training.. Moved to a separate project at https://github.com/tensorpack/zmq_ops.. Btw, after the recent updates, I found that zmq reader is slightly faster than using queues or dataset.from_generator.. The above commits should improve it a little bit, but the majority of time is spent in tensorflow.\n```python\na.py:\nimport tensorflow.contrib.framework           \nimport cv2            \nimport time           \ns = time.time()       \nimport tensorpack     \nprint(time.time() - s) \n``time python a.pytakes 8s to finish, but prints 0.25s. Better to ask whytensorflow.contribimport all modules in [init.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/init.py).\nTF is improving it recently. See\nhttps://github.com/tensorflow/tensorflow/pull/11919 https://github.com/tensorflow/tensorflow/pull/11830. Probably a bug I introduced while working on #139. I will take a look.\nbtw are you able to see any speed improvements over the non-parallel InferenceRunner? It's quite slow in my cases so I reopen #139.. You shouldn't enable multi-process prefetch for inference (and that's one of the reasons why inference is slow and that's what I'm fixing).\nYou can use at most one process for prefetching in inference, otherwise you have many processes reading validation data at the same time -- processes don't talk to each other, you may end up getting one data point multiple times. The second process might produce image 1,2,3 which the first process already read and produced.\nAs a result you get an estimate of the validation error, but not the true validation error. When you use shuffle=True for validation you get an unbiased estimate of the validation error.. Yes. For stochastic training it doesn't matter.. I guess there are two types of solutions: either fork ImageNet reader or not.\nMy current plan was to use a single reader (to just produce file names, for example) but multiple workers to process it (ThreadedMapData is already doing something similar). This way the imagenet reader doesn't need to be aware of potential parallelism. \nIt might be also a solution to fork but let each reader have its own range.. ```python\n    ds = ILSVRCNames()\naug = imgaug.AugmentorList(augs)\ndef mapf(dp):\n    fname, cls = dp\n    im = cv2.imread(fname, cv2.IMREAD_COLOR)\n    if im.ndim == 2:\n        im = np.expand_dims(im, 2).repeat(3, 2)\n    im = aug.augment(im)\n    return im, cls\nds = ThreadedMapData(ds, 30, mapf, buffer_size=1000, strict=True)\nds = BatchData(ds, batch)\nds = PrefetchDataZMQ(ds, 1)\nTestDataSpeed(ds).start_test()\n\n``\nWith this [code](https://bitbucket.org/ppwwyyxx/tensorpack-benchmark/src/66999b67b20712575e2ed87f0f2a32e817897813/ImageNet/benchmark-ImageNet.py?at=master&fileviewer=file-view-default), andILSVRCNames` producing simply (filename, label) pairs, I am able to get about 20x speed up (about 1.8k images/s) over the non-prefetched dataflow, while still keeping it equal to the whole validation set. This speed is often enough to keep at least a couple of GPUs busy. \nGPUs are becoming so fast that it's already pretty hard to keep them busy at training -- for inference it's harder, especially with Python. I'll go with this solution for now and update examples/docs these days unless some day I found it's becoming a bottleneck again.\nClosing this now and track #139 on inference performance. . Yes. #139 has a todo list about improving the performance.. Thanks for your help as well! \nI think tflearn and keras are easier to learn and therefore welcomed by a larger community, although they maybe less powerful.\nAnyway I'll continue improving it. If it could ever reach a reasonable size of community I will also move it to the organization account from you.. We had sparse inputdesc support for a while, but later it introduces some issues mainly because not all tensorflow features work for sparse placeholders. We might revisit this feature at some time. \nIf my understanding is correct, in tensorflow a sparse tensor is just 3 dense tensors? You can check out CTC-TIMIT example which uses 3 dense InputDesc to get a sparse tensor.. My bad.. opencv compiled with cuda may slow down tensorflow initialization. It seems perfectly fine to me.\nAlso opencv is an optional dependency of tensorpack. . Your previous dataflow produces a list of list, which is not supported by BatchData. BatchData only works for a list of [ndarray or scalar].\nI don't know any good way to support it, because a list can contain anything.... In any case you can use BatchData(use_list=True) which does no type check. Otherwise it only makes sense to batch ndarray from ndarray.. Prefetch has different mechanism, please see the tutorial for more details. And in practice it depend on what interface you have to your data file, and what kind of dataflow you need, e.g. can you afford random access in the file? Do you want dataflow to be shuffled? If each datapoint is a random sample, do you want each pass of the dataflow to be exactly the whole data file? All these questions affect how you write your dataflow.\nIf you have any specific question after reading the tutorial, feel free to ask with your specific scenario.\nIf you just don't want parallel workers producing the same data, ThreadedMapData may help.. That's how PrefetchDataZMQ is implemented.\nSome possible combinations of dataflow are shown in the tutorial. As mentioned above, ThreadedMapData may help if fork is undesired, but it all depends on the actual problem.. The trainer is saying it finds no variables to optimize. This agrees with the warning messages you saw ( No Gradient w.r.t tower0/emb_1/emb1) You might need to check your models. . QueueInput doesn't check this (#285). If you want data-parallel you need tf.get_variable instead of tf.Variable , this way you can reuse the variables.\nFor the rest I don't have time for now to look at it. Maybe it's just you use some ops that doesn't have gradients?. Oh it's probably just the get_vatiable issue. It creates duplicate variables in each tower. So there is no gradient for cost and variables that are on different tower.. You can import your file directly... not import from tensorpack.\ne.g. this line: https://github.com/ppwwyyxx/tensorpack/blob/5c241e091f0bfa3723bb031482e727e946ad0304/examples/CTC-TIMIT/train-timit.py#L21. Feel free to reopen if you still have questions.. Last time I tried tensorflow still didn't have good distributed performance. So I never wrote an example about it... dorefa.net says:\nWe're not planning to release our C++ runtime for bit-operations. \nIn this repo, bit operations are performed through tf.float32.. What inferencer do you use?. Thanks. So you wrote a inferencer, and the methods were recently renamed, so it prints a warning.. The readme already made it clear:\nTraining code of ResNet on ImageNet, with pre-activation and squeeze-and-excitation. The pre-act ResNet follows the setup in fb.resnet.torch (except for the weight decay) and gets similar performance (with much fewer lines of code).\nload-resnet.py has its own usage (and its only usage) and it's also mentioned in the readme.. As said in the readme, this implementation follows the architecture in fb.resnet.torch.\nAnd this is the file I referred to: https://github.com/facebook/fb.resnet.torch/blob/master/models/preresnet.lua\nThe arguments of BNReLU is BNReLU(layer_name, input).\nUPDATE: you can use BNReLU in two ways:\nBNReLU(layer_name, input, name=output_tensor_name)\nor \nBNReLU(input, name=output_tensor_name).\nThe first way is undocumented.. You also need tf.reset_default_graph before using ModelExport.. The reason is simply because that's what most people use. The same reason why other wrappers including tf.layers.conv2d assumes float32.\nFor special needs, you don't have to use tensorpack layers. You can write symbolic function yourself, or some other wrappers, e.g.:\npython\nfrom tensorflow.python.layers.convolutional import Conv2D\nConv2D(..., dtype=tf.float16)\nLayers are not being deprecated. #291 is only talking about using tf.layers for the implementation of layers.\nUPDATE: float16 is fully supported in layers for a long time now.. Because the input image is originally NHWC.. Those examples only support one data format.. Variables in this scope don't need to be trained. So they shouldn't be added to the TRAINABLE_VARIABLES collection.. No. These variables will not be trained. In tensorflow, only variables added to this collection tf.GraphKeys.TRAINABLE_VARIABLES will be trained by default. These variables are added and them removed from the collection.\nYour observation is correct. You can remove freeze_collection and it will work the same. But tensorpack will print warnings when some \"TRAINABLE_VARIABLES\" have no gradient, which is usually a bug. freeze_collection avoid this warning message. . To simplify the code I removed the freeze_collection part in 9850edf5342a27f. It also fixed an import bug which caused crashes in evaluation.. The protobuf saved by tf.saved_model.builder.SavedModelBuilder is not the format expected by freeze_graph.\nThis issue is not related to tensorpack, closing.. That part of code in tensorflow is not documented well and is not even a public API. After a glance I think it's probably easier to just call tf.graph_util.convert_variables_to_constants directly. You can get the graphdef from the metagraphdef file saved by tensorpack.. #106 had something similar. \nYou can probably also just use tf.get_default_graph().as_graph_def().\nAccording to tensorflow documents you seem to need those variables restored in the session. If you are starting from scratch you might need to load the metagraph first, than use SaverRestore(checkpoint).init(sess) to initialize the session (http://tensorpack.readthedocs.io/en/latest/modules/tfutils.html#tensorpack.tfutils.SessionInit).\nOf course you can just use tf.train.Saver().restore(sess, ...) as well. Things are saved in standard TF format already so you don't have to worry anything about tensorpack.\nBtw I never use these TF features so I'm not sure what is correct. . Thanks! The code looks reasonable. However it's not relying on anything in tensorpack and not used by anything in tensorpack -- it's only demonstrating a usage of tensorflow. So it may not be a good idea to include them here.. Oh putting it in scripts sounds OK. . But I guess that's what the freeze_graph scripts in tensorflow is for? (although it's unclear how to use it... Does pickle work? Or do you really want human-readable serialization?\nFor human-readability you can always just implement __str__ for the augmentors and use pickle + print. You can let __str__ print those parameters.. Having a human-readable serialization in general is not always a possible thing, because conceptually an augmentor is allowed to contain any object. To repeat an experiment it's better to still use binary format.\nHaving a serialization which can be transformed into human-readable format is easier, and that's what __str__ is for. \nAlso, for __repr__, this is from Python official document:\nobject.__repr__(self)\nCalled by the repr() built-in function and by string conversions (reverse quotes) to compute the \u201cofficial\u201d string representation of an object. If at all possible, this should look like a valid Python expression that could be used to recreate an object with the same value (given an appropriate environment).\nSo if __repr__ is written well, you can get human-readable serialization for most augmentors (actually all existing augmentors) already. And in this case you don't need pickle anymore.\nFor RandomCrop, this would be:\n```python\ndef repr(self):\n    return \"imgaug.RandomCrop({})\".format(self.crop_shape)\nstr = repr\n```\nTests would be nice, of course.. But I myself don't have plans for it now.. ```python\nclass MyCallback(Callback):\n    def _setup_graph(self):\n    t = self.graph.get_tensor_by_name('conv0/output:0')\n        self._fetches = tf.train.SessionRunArgs(fetches=[t])\ndef _before_run(self, _):\n    return self._fetches\n\ndef _after_run(self, _, rv):\n    t = rv.results\n    np.save('output-{}.txt'.format(self.global_step), t)\n\n. This is probably a common need (I'm using it recently as well), so I just added a callback `DumpTensors` that basically implements the above snippet: http://tensorpack.readthedocs.io/en/latest/modules/callbacks.html#tensorpack.callbacks.DumpTensors. Create that tensor in the graph with `tf.gradients` and use the above callback.. DumpTensor expects a list of names:\nParameters: names (list[str]) \u2013 names of tensors\n. This diff can make `mnist-visualizations.py` dump the gradients:diff\ndiff --git i/examples/mnist-visualizations.py w/examples/mnist-visualizations.py               \nindex b9fc60c..1ff6803 100755                 \n--- i/examples/mnist-visualizations.py        \n+++ w/examples/mnist-visualizations.py        \n@@ -112,6 +112,12 @@ class Model(ModelDesc):  \n                               regularize_cost('fc.*/W', tf.nn.l2_loss),                       \n                               name='regularize_loss')                                         \n         self.cost = tf.add_n([wd_cost, cost], name='total_cost')                              \n+                     \n+        c1_input = p0\n+        c1_W = c1.variables.W                \n+        grads = tf.gradients(self.cost, [c1_input, c1_W])                                     \n+        print(grads) \n+                     \n         summary.add_moving_summary(cost, wd_cost, self.cost, accuracy)                          \n     summary.add_param_summary(('.*/W', ['histogram', 'rms']))\n\n@@ -144,6 +150,8 @@ def get_config():         \n             ModelSaver(),                    \n             InferenceRunner(                 \n                 dataset_test, [ScalarStats('cross_entropy_loss'), ClassificationError('incorrect')]),\n+            DumpTensor(['gradients/conv1/Conv2D_grad/Conv2DBackpropInput:0',                  \n+                'gradients/conv1/Conv2D_grad/Conv2DBackpropFilter:0'])                        \n         ],           \n         steps_per_epoch=dataset_train.size(),\n         max_epoch=100, \n``. I usedprint(grads)in the above snippet to see the name. I agree this is annoying. \nFor multigpu training you can still use this to access per-tower gradients, but it'll be hard to access the aggregate gradients, because gradients are internal to the trainers. One feasible way is to write aGradientProcessorthat does nothing but print the tensor.. I don't understand your question. Could you explain \"According to the formula, r0's derivative on r1 is 1-tanh2(r0)\". It can be computed by standard chain rule.. It should work. But I don't use it for my work so I can't be sure.\nGradient update happens every k steps.. I don't. You can train it.. now there is a resnet152 pretrained model.. Yes. Windows has some different fork behavior so PrefetchData may not be available either.. You can try to make the functionfinMapDataComponent` become its member function instead of a closure. Maybe that'll make it pickleable.. Did you find a solution?. The solution is to write your function in a way that's pickleable (no closures, etc).\nIf the unpickleable function is from tensorpack, please report it as an issue.\nThe original issue was fixed already because now the function MapDataComponent.__init__.<locals>.f was turned into a member function rather than a closure.. When something inherits threading.Thread, it's run() method will be called under a different thread.\nCallback methods are always called by the trainer, so they are called under main thread.. Validation statistics do go to tensorboard.. Feel free to reopen if you have more questions.. The simplest way to try it now is to just replace dataflow=mydataflow in TrainConfig by:\npython\nds = TFDatasetInput.dataflow_to_dataset(dataflow, [tf.float32, tf.int64]).prefetch(50)\nTrainConfig(\ndata=TFDatasetInput(ds)\n). get_data will only be called when Dataset.Iterator.initializer is called, which is when InputSource.reset_state() is called, which happens once before training.\nAbout speed, when python-side iterator is fast enough, dataset is as fast as queues.. Now InferenceRunner only takes something that has a size, so it can know when to stop inference.\nThe whole predictors are designed to use Python data for quick demo.\nTensorpack doesn't support predictors with tf dataset. You'll need to do them by yourself, i.e. call build_graph on tf.dataset tensors and evaluate the output in your own loop.. The latest commit may support InferenceRunner without size (but DataParallelInferenceRunner still requires size), as long as the tf dataset will stop (e.g. no repeat()). I don't have a use case to test whether it works.. I think you need to clear the graph with tf.reset_default_graph() before starting a new training.. Should be fixed now. BTW, because you created a new graph each time, you probably don't need a reset.. Using reset_default_graph under tf.Graph() is already forbidden in latest TensorFlow (I'm using nightly).\nUsing this and it will be fine:\npython\nfor val_fold_index in range(n_folds):\n    with tf.Graph().as_default():\n      # no reset\nOr this:\npython\nfor val_fold_index in range(n_folds):\n    # no new graph\n    tf.reset_default_graph(). What I had in mind is a MapData as well. Or maybe a subclass of AugmentorBase which calls iaa.\nJust found that the keypoints transformations in iaa seems to assume integer coordinates -- this is going to hurt training for coordinate sensitive tasks.. Don't know exactly why, but running gm convert results.jpg new.jpg makes the image half the size without visible quality change \n(See \"swipe\" \n in https://github.com/ppwwyyxx/tensorpack/pull/400/commits/12648f22c69393447cb2f1aab4746ce8a76b65fe). Apologies for the break. I should've marked them as deprecated first.\nThey're removed for the same reasons why GAN is not included or freeze graph is not included. Being useful is never the whole reason to add something. . TF has a few in tf.losses.xxx and a few in tf.nn.xxx_loss. But yes they are very limited.\nHowever I do need to remove huber_loss and use tf.losses.huber_loss.\nHaving some symbolic function collections is never a bad idea but tensorpack isn't the right place for it. I'd like to see a project called \"a bunch of useful TF symbolic functions\" and then not only tensorpack but also anyone who uses tensorflow could benefit from it.. The document says, \"The iterator state of the underlying DataFlow will be kept if not exhausted\". So this is actually an intended behavior.. Maybe an option can be added to get a different behavior.. That's OK. But note that although you can get a subset of the dataset, you still lose the opportunity to fully shuffle the data. So maybe this is not that useful.\nAs it is a streaming interface, a full shuffle can only happen at the very beginning of the dataflow.. This is the current implementation of PrefetchData -- it forks the dataflow. There is no way to automatically make a single iterator run in parallel, unless you fork it to become many iterators.\nThreadedMapData also runs in parallel but doesn't do the fork. (Similar things can be done for multiprocess as well but that hasn't been implemented). \nBut they essentially have different semantics. By forking a dataflow it accelerates that dataflow. If only running the mapping function in parallel, the underlying dataflow doesn't get accelerated. Then you can let the underlying dataflow produces simple things such as indices or filenames, and let the mapping function do all the heavy work.. The interface is different in that we don't have __getitem__. DataFlow is an iterator interface, while pytorch dataset is an array interface.\nAn array interface is less performant because it needs to keep track of the indices of every data point. The difference is probably tiny, but keeping track of indices is also harder to write.\nDataFlow can emulate the array interface, by having a dataflow producing the indices, and a MapData to gather the actual data from the indices. This way, parallel dataflow can avoid producing duplicated data as long as they take indices from the same queue, and this is exactly how pytorch multiprocess prefetch is implemented.\nTo do this, you can:\npython\ndf = DataFlowProducingIndices()\ndf = PrefetchData(df, 100, 1)  # use 1 multiprocessing.Queue for indices\ndf = MapData(df, from_indice_to_data)\ndf = PrefetchDataZMQ(df, 10)   # fork the mapping part\nThere are plans to make it more intuitive (something like MultiProcessMapData, similar to MultiThreadMapData).. It's up to you to batch before or after prefetch.. There are currently two multigpu methods in tensorpack (parameter server and replicated), which are both data-parallel but different in the communication pattern. They may have different performance on your task.\nTo get better performance for scenarios like this, you might need to design specific multi-gpu strategy for your task (e.g. sharding of variables, mix of data parallel and model parallel, etc). Tensorpack doesn't implement these cases.. Different GPUs always communicate directly when they can.\nThe strategy would depend on the actual model and will need a number of trials, provided that you understand what and when communication is happening. TF documentation has something about sharding and it may help.. I cannot reproduce your problem.\nIf you're using cudnn 5.1.5 you should upgrade it. I cannot think of any other reasons.. cudnn5.1.10 should work. not really... The document actually said you need to use tuple. \nAnyway, now list is allowed.. What's the queue size shown in the training log? If the queue is near empty you're blocked by data. It's easier to understand why data is slow than to understand why tensorflow is slow.. You can check out the tutorial about efficient data loading: http://tensorpack.readthedocs.io/en/latest/tutorial/efficient-dataflow.html. Added a page about performance tuning: http://tensorpack.readthedocs.io/en/latest/tutorial/performance-tuning.html. Feel free to reopen if you still have questions.. That's the code for resnet-v1 now. v2 is named preresnet_group.. It should be fixed 14 days ago.. You're running on afs. See note 4 in PrefetchDataZMQ.. It can run on afs. Could you paste the command you run, the directory you set, and the logs.\nAlso, you don't have to use PrefetchDataZMQ. Use PrefetchData which is slower but doesn't have filesystem requirement.. TENSORPACK_PIPEDIR should not point to anywhere on afs.\nYou can use, for instance, '/tmp', '/dev/shm', which are usually not on afs.\nThe command to set it is export TENSORPACK_PIPEDIR=/tmp.\nThere are no suggestions for the parameters, usually this is something for users to tune. Your parameters seems OK.. k bits can represent 0...2^k - 1, not 0...2^k. Closing because keeping the order of data often isn't useful.. It is not a tensor. Otherwise you wouldn't have all the features of it.\nThere is a pair of braces in the end, to make it a tensor: https://github.com/ppwwyyxx/tensorpack/blob/master/examples/mnist-convnet.py#L58. I'm not familiar with gcs, but from the code, it looks like IsDirectory() can return true for gcs path.\nDo you have a better idea why it returns false for your path?. Thanks for the explanations. After reading more on the code, I think it'll work to call CreateDir when IsDirectory returns false -- it will create a so-called \"directory marker\" in gcs. I can make this the default behavior in ModelSaver (since this is how tensorflow decides to make gcs compatible with traditional fs).. Ah you're right. A lot of code need to be moved to gfile.. Github fails to track references.. the commit is 1a19939ea3ea02fb977f08975c2b344ed0c333c1. pip install -U git+https://github.com/ppwwyyxx/tensorpack.git will install from github. The code looks fine and I don't know why this happened. Random initialization should not make such a big difference.. http://tensorpack.readthedocs.io/en/latest/modules/train.html#tensorpack.train.TrainConfig\nUse the session_config option,. From the logs it looks like it will fail if you import tensorflow.contrib (from tensorflow.contrib.framework import add_model_variable\n). I don't think it's related to tensorpack.. They will be trained. It's a common technique to improve resnet.. https://arxiv.org/pdf/1706.02677.pdf mentioned this.. This is not something I can help with because any tiny difference between your code and the original ones can cause this and I couldn't know what that difference is.\nYou can actually just call slim.inception_v3 inside tensorpack, and this might help reduce the differences.. btw, our examples contain an inceptionv3 code which reaches 74% accuracy. This is what you should expect to get if you're training from scratch. 78% will need super large batch size (e.g. 1600). See Google's comments (https://github.com/tensorflow/models/blob/master/research/inception/inception/inception_train.py#L66). https://eng.uber.com/horovod/. 95%~97% speed of existing trainers on single machine.\nDon't have the resource to try distributed now. Not sure it works... Distributed training was supported. Didn't test speed but according to horovod benchmarks I assume it will be fast... For 8 bit there are better options than DoReFa-Net. Even quantize a float32 network after training might be better. DoReFa-Net is designed for <=4 bits.\nYou can call those quantization ops in tensorflow directly. Although I doubt you'll see any performance improvement.. This feature you need already break the abstraction of a \"layer\". It's best to then write the layer yourself.. GradientProcessor is designed for manipulating the gradient. You can subclass GradientProcessor and implement something similar to ScaleGradient like you've already done.. If you need to change it frequently then it makes sense to make that tensor one of your inputs.\nIf not, you can also create an op which modifies that variable, and run that op once a while, maybe through the RunOp callback.. Feel free to reopen if you still have questions.. It's not saving the quantized version.\nYou can call tf.summary.histogram on the quantized tensor.\n```diff\n--- i/examples/DoReFa-Net/svhn-digit-dorefa.py\n+++ w/examples/DoReFa-Net/svhn-digit-dorefa.py\n@@ -65,7 +65,9 @@ class Model(ModelDesc):     \n                 return v                     \n             else:    \n                 logger.info(\"Binarizing weight {}\".format(v.op.name))                         \n-                return fw(v)                 \n+                ret = fw(v)                  \n+                tf.summary.histogram(ret.name, ret)                                           \n+                return ret                     \n     def cabs(x):   \n         return tf.minimum(1.0, tf.abs(x), name='cabs')\n\n``. You are right. You can pass them through quantization function __before__ inference as well.. yes.. @erdollar It's already answered above.. You usetf.summary.histogram(ret.name, ret) `. So you should look for \"ret.name\" in tensorboard, whatever it is. The name is definitely not \"param-summary/conv2/W-histogram\".. As I said in #421 already there are many places things can be different. Calling a model from slim doesn't make them equivalent.\nMost likely your issue is because of this: https://github.com/ppwwyyxx/tensorpack/issues/148#issuecomment-285965752. Crop, resize, mean subtraction, or any other possible preprocessing. Basically you need to go through a working evaluation code (assuming they have one) and figure out everything that's happening. . You can override _get_cost_and_grad method of ModelDesc.. What's the reason you want to change the aggregation method? I don't expect this to be changed as the default should be good.\nAfter some recent refactoring there is no _get_cost_and_grad any more (it wasn't a public method so I didn't take too much care on it) and the relevant code to compute gradients is buried deeper.. I'll try the aggregation methods. \nWriting a GAN trainer should have nothing to do with this method because you're supposed to build the graph by yourself anyway. I can make some utilities (e.g. average the gradients) public, though.. You can access model.g_loss, model.g_vars. What is hard to access?. What do you mean by new interface? GAN.py hasn't been changed much since MultiGPUGANTrainer was added.. ModelDesc is for single-cost models only, so whatever changes should not affect GANs.\na GAN model should build the forward/backward graph by itself, not using anything in ModelDesc other than build_graph, get_inputs.\nGANModelDesc mistakenly subclasses ModelDesc at the beginning but not anymore. So they are totally unrelated now.. Not a tensorpack question. Closing. You can ask this in some more general machine learning communities.. After the changes you can load your trained model.. Could you try again?. Yeah. TF by default allocate all GPU memory although a PS may not need GPU. You can start the worker first as a workaround.. I remember TF will print some duplicated logs when starting up distributed training. Can't recall if this is the one.. I can use single machine to run 2ps 2 workers using your cluster_spec. Not sure what's going on at your side. I did notice that I cannot run distributed training inside a chroot-based container but not sure that's related.. Seems like I can reproduce the problem. I'll look into it.. Should've been fixed now. Sorry for these bugs -- I never really used distributed trainer for anything serious so a lot of functionalities are not well tested.. The use of all_variables was removed today. . Added an option which can control the queue size of zmq on the receiver side.. Assuming you are using the right model with the right command, you may need a much smaller learning rate to start with.. After training you can use OfflinePredictor with PredictConfig to get a callable. It is quite easy -- you usually only need to provide 4 arguments (model, where to load, input, output). Docs is here. Most examples contain the use of OfflinePredictor if you could grep.\nWe won't provide more sophisticated use -- because it's just a normal tensorflow model and you can use whatever features of tensorflow for inference.. 1. Just print the matrix in your code and don't use monitors.put. Monitors don't support \"matrix\" anyway.\n2. Your inference dataflow might not produce the same set of data every time, e.g. due to batching, random shuffling. You may need to check the dataflow. . prefetch with nr_proc>1 will not give you the same data. See http://tensorpack.readthedocs.io/en/latest/modules/dataflow.html#tensorpack.dataflow.PrefetchDataZMQ\nWhen 73768 % BATCH_SIZE != 0, use of batch can also be a problem (missing a few datapoints in the end). You can use remainder=True (http://tensorpack.readthedocs.io/en/latest/modules/dataflow.html#tensorpack.dataflow.BatchData). You might be using some operations in the model e.g. slicing of variables, that this implementation of accumgrad doesn't support yet.. tensorpack is already using tensorflow queue. What features have you done/do you want in addition to that?. ```python\nfrom tensorpack.graph_builder.input_source import EnqueueThread\nplaceholder = [tf.placeholder(...), tf.placeholder(...)]\nqueue = tf.FIFOQueue(50, [x.dtype for x in placeholder])\nthread = EnqueueThread(queue, ds, placeholder)\ntensors = queue.dequeue()\nwith sess.as_default():\n    thread.start()\nsomething like this may work.. It seems OK, but I'm not familiar with that interface.. You can write the dataflow yourself and add this option..\n\u2570\u2500$python3 -c 'from tensorpack import *; x = TrainConfig(dataflow=FakeData([1]))'\nFailed to load OpenCL runtime (expected version 1.1+)\n[1010 09:18:31 @monitor.py:198] WRN logger directory was not set. Ignore TFEventWriter.\n[1010 09:18:31 @monitor.py:238] WRN logger directory was not set. Ignore JSONWriter.\n``\nCould you double check?. It sayslogger directory was not set. So there is **no log file** at that time. You can't expect it to store things in a log file that is created in the future.. Technically, the log file stores any logs generated after you've set the log directory. If you set the directory earlier you can see logs about TrainConfig.. I don't know what's the reason. It's best if there is a minimum failure example.\nAre you using anything likeself.xxx = xxxinsidebuild_graph(except for the cost)? This might cause problems like this.. Or could you provide what exactly did you add to make it fail?\nPlease note thatbuild_graphwill be called multiple times (#gpu times for training and 1 time for inference). Adding things that have side effect may cause problems like this.. You should only callupdate_emawhenctx.is_main_training_toweris True. Or at least whenctx.is_trainingis True.\nOtherwise you're adding inference tensors toUPDATE_OPSas well. Evaluating inference tensors during training will give you the error..tf.Print` may work for you. https://github.com/ppwwyyxx/tensorpack/issues/389 is also related. But I'm not sure are those exactly what you need.\nTo access the weight tensor of a layer,also see https://github.com/ppwwyyxx/tensorpack/issues/389#issuecomment-327971447 .. It won't necessarily run in serial. It's allowed to run in parallel and tensorflow runtime engine will decide what to do.\nYou can write whatever model implementation with tensorflow. Conv2D is just a simple wrapper over a common layer, you can and should write your own layer if you need more than what's commonly used.. With replicated training, the tensor name becomes tower0/cost. Could you try with that?. Could you explain what is \"Between-graph mode\". Are you still talking about multi-gpu training or distributed training?. This issue isn't about tensorpack. You should report to tensorflow/benchmarks.. About the original question, after latest changes 897d29e3f1dbc5 to the trainer, using \"cost\" would be sufficient. \"tower0/cost\" also works.. During training there is no need for quantization because batchnorm is fast.\nDuring inference, quantized batch norm becomes quantize([quantized number] x [constant] + [constant]), which can be implemented by a lookup table on FPGA.\nThis is not a tensorpack question. I suggest you email the authors about the paper details.. I have no interest in it .. \nand implementing papers for others is far beyond what a maintainer of a deep learning library can do.. It uses similar time but does twice more work.\nThis means it has over 90%  efficiency. See #353, #359 and the documentation.\n. tensorpack doesn't care about inference or deployment so we won't add this feature. It gives you a tensorflow checkpoint and you can use whatever inference features tensorflow supports. The simplest thing is to just open many Python threads to run sess.run, but I don't know does tensorflow support anything more than that. You'd better ask this question in the tensorflow community.. OK. I thought you want to run multiple predictors.\nSetting the thread to 0 make single predictor multi-threaded. This setting is usually not good for training.\nYou don't need to modify any code. You can pass this to PredictConfig:\n```python\nconfig = tf.ConfigProto()\nchange config\nPredictConfig(session_creator=NewSessionCreator(config=config)) \n``. Make sure that_build_graph` build the part of graph from input to cost.\nFor your case, it could be your model has very different communication-computation ratio so it spends more time in communication. Or you might mistakenly introduce some dependencies among the two GPUs so they don't run together. It's best if you could show some code or at least describe what you've done.. The two queues are for training and validation respectively. Queues being empty basically says your data is slow.\nWhen data is the bottleneck you should've seen the performance goes roughly from 3.6 -> 1.8 when going from 1 GPU to 2.\nI don't understand what have you done. Which \"two versions\" have you used in the end exactly? How could you possibly get 3.6it/s when data is only 0.4it/s. . Become faster from 0.4it/s to 3.6it/s, with the same batch size 256? This is impossible and you probably read the number too early when it's unstable. . You mean drop from 4.3 to 3.6? I would expect the speed number for GTX 1080 is between 3~5 but I don't remember. \nYou can check your queue size and GPU utilization. If you see above 95% utilization then that's the best you can do.. It drops from 28.3 to 3.9 because that's about best your GPU can do. Yes it drops a lot but what else could you expect?\nWith a 1080Ti I can only get 2.7it/s for resnet18 batch=256. Don't even know how you could get 3.x.\nIf data is fast enough and you didn't modify the code about batch size, you should see about linear speed up. In no way it should drop and I still doubt whether you have measured things correctly.\nIf you still can't figure out, please attach the modification you've done and training log for at least two epochs with a small epoch size (like steps_per_epoch=50).. As I said when you have about 95% utilization, that's the best you can get.\nTwo GPU performance depend on a lot more other things (e.g. PCIe speed), so it may not be able to scale linearly on \"any\" two GPUs.\nI don't know about why your 1080Ti seems slower. I would doubt it's a version issue. I'm using the latest of TF/cuda/cudnn/nvidida driver, if that matters. Now there seems to be no problems with the use of tensorpack.. You can. That's what the resnet example is doing.. Actually nvidia 387. But I think this doesn't matter too much.\nTF1.4 has to be compiled from source to work with cuda9/cudnn7.. It's not \"broken\" because it never worked. VGG-16.npy uses tensorpack naming, not tf.layers naming.. It calls when it can and all the names never changed.. Override the \"extra_callbacks\" list and replace the callback with PeriodicRunHooks. No. You can refer to other implementations such as https://github.com/dgriff777/rl_a3c_pytorch. You should read the tutorial on performance tuning first.\nhttp://tensorpack.readthedocs.io/en/latest/tutorial/performance-tuning.html. As in the code.. We don't discuss general (non-tensorpack) machine learning questions like this in the issue. . See this http://tensorpack.readthedocs.io/en/latest/modules/callbacks.html#tensorpack.callbacks.ProcessTensors\nor \nhttp://tensorpack.readthedocs.io/en/latest/modules/callbacks.html#tensorpack.callbacks.DumpTensors. ValueError: Cannot feed value of shape (224, 224, 3) for Tensor 'input:0', which has shape '(?, 224, 224, 3)'\nThis is saying that you didn't batch your data.. Probably related to note 4 in PrefetchDataZMQ.. Could you post the log again?. Are you using sqlite reader of TensorFlow?. Your log does not contain the error you mentioned.\nFrom your log it looks like the same issue about zmq pipe dir.. Could you post what speed have you seen on what GPUs training what models?\nAlso see http://tensorpack.readthedocs.io/en/latest/tutorial/performance-tuning.html. > Could you post what speed have you seen on what GPUs training what models?\nCould you post for 1, 2 and more GPUs to show the problems you've met?. A section about debugging scalability was added to the documentation recently. Without more details no more suggestions can be given. Closing this thread now, but if you have more questions feel free to reopen.. @PeisenZhao Please open a new issue following the issue template or add those information here. I don't know if it's related to this issue or not without you providing those information.. This is probably not the same as the original issue.\nYour dataset is not producing data with homogeneous shape. I would recommend you to check how VideoDataset is implemented and what it is producing. Take some data out of the dataflow and check their shapes. See http://tensorpack.readthedocs.io/en/latest/tutorial/dataflow.html#use-dataflow-outside-tensorpack.. Also, could you specify what do you mean by \"change the data list\"? What happen if you change it back?. This is very strange. Could you try run it without PrefetchDataZMQ to see if the bug still exists?. Then TENSORPACK_PIPEDIR may still be the reason behind it. /tmp may not always be a local directory. Could you post your output of df command?. Not sure what's going on with your /tmp. You can definitely try /run or /dev/shm, or some of your own directories under /home or /home1. Make sure to check the environment variables before running.\nAnd sometimes running inside a container can also make unix pipes behave weird. \nI won't be able to help more if you cannot figure it out what's special about your system. You can just use PrefetchData. It's not very efficient for large data but can still bring you somewhere.. I found that ZMQ supports abstract socket which is filesystem-independent on linux: http://api.zeromq.org/4-1:zmq-ipc \nMaybe you can try the above commit, though I'm not sure now if filesystem is the root cause for the issue.. --load a model file, not the directory.. fused_batch_norm is faster than batch_normalization.\nThis is not a tensorpack question. closing.. No, they are slow. https://www.tensorflow.org/performance/performance_models#use_fused_batch-normalization\nhttps://github.com/tensorflow/tensorflow/issues/12419. What's the \"other resnet implementation\" you're talking about? I would love to know.\nBtw, tensorpack reaches about the best possible resnet performance with tensorflow (with fused batch norm, or course). Comparing the wrong setting (non-fused batch norm) isn't of much interest to me. \nBut I still hope to take a look in your case to see if there's an simple answer. My doubt would be there are other factors that affect your conclusion, e.g. not comparing the same model with the same setting, bottlenecked by data, etc.. \"tensorpack slower than others\" is a potential performance issue then. To get more serious you'd better also include what you did (command and settings), what you observed (logs and speed numbers), if possible.. One possible explanation, for example, is that non-fused batch norm is faster with NHWC (according to https://github.com/tensorflow/tensorflow/issues/12419) than NCHW. Our resnet uses fused batch norm and therefore NCHW. If the other code uses NHWC, then you're not comparing apple-to-apple.. > The only difference is they use moving_mean and moving_var in training for which they need to use moment&batch_normalization\nI don't understand from this sentence what the difference is. AFAIK nn.moments doesn't give you moving_mean and moving_var.\nAs I said, if you want this to be an issue about \"tensorpack slower than others\", I would need much more details.. > With nn.moments they calculate bm and bv, and update bm and bv with moving_mean and moving_var, after which they are fed to non-fused batch norm to output xn in training.\nThanks. This looks like standard (if not the only) way to use non-fused BN. How do you do it differently in tensorpack?. You said that the \"other implementation\" use the same data layout, right?\nThe issue is \"tensorpack vs the other implementation\", not \"fused vs non-fused\", because non-fused is slow as expected.\nThe resnet example in tensorpack support both data layout.. Fused batch norm do have 2nd order gradient IIRC. If not you should report this to tensorflow.. Thanks a lot!. It's a long-standing issue when data depends on the model and may get dead-locked during initialization. Now you can run it with (<=2) GPU (--gpu 0,1) but not more. I'll try to fix it.. Btw, as mentioned in the README, running with many GPUs doesn't give you much benefits.. Thanks a lot!. One big distinction between eager mode and graph mode: \nIn eager mode, variable creation and the model math must be separate. Variables are created once, and the model is then executed many times. However, in graph mode they are both run only once and therefore they are usually together. A tf.layers.dense(x) call will create both the variables and the math operations.\nAs a result, in eager mode you'll almost always have to rewrite your model code anyway, into pytorch-style or Keras-style. That's not a tensorpack limitation. Eager documentation also mentions:\n\nSome API calls (such as the functional-style tf.layers.dense, tf.layers.conv2d) are not compatible with eager execution. Use of such methods should raise an error indicating the alternative (e.g., the tf.layers.Dense and tf.layers.Conv2D classes).\n\nSo probably this feature won't happen soon.. Hi Bichen,\nSeems like I can't reproduce this problem. Could you update tensorpack to latest  master and try again? (with pip install -U git+https://github.com/ppwwyyxx/tensorpack.git). tensorpack is going through some major changes recently, so there are some unstable versions.. Tensorflow saver by default will delete some old models: https://www.tensorflow.org/api_docs/python/tf/train/Saver#init\n\nmax_to_keep: Maximum number of recent checkpoints to keep. Defaults to 5.\nkeep_checkpoint_every_n_hours: How often to keep checkpoints. Defaults to 10,000 hours\n\nYou can change the behavior by the same options in ModelSaver: http://tensorpack.readthedocs.io/en/latest/modules/callbacks.html#tensorpack.callbacks.ModelSaver. I can't reproduce this problem either.  \ud83d\ude22 \nEvaluating the model shouldn't use much memory, and since you can eval it during training I have no idea why this happens. \nIf you're sure no one else is using the GPU at the same time, maybe you can try a different version of tensorflow/cudnn.. It's probably an issue in lower level. Maybe specific to driver or GPU and their interaction with softwares like cudnn or tensorflow. I know some version of driver can cause CPU memory leak with cudnn.\nAnother related environment variable is TF_CUDNN_WORKSPACE_LIMIT_IN_MB which is default to 4G.  Although ideally it shouldn't be a problem.\nAlso the related code for \"no-autotune\" (i.e. default algorithm) is https://github.com/tensorflow/tensorflow/blob/6a8322f6dc007573e97a452e056b76f2be4794a7/tensorflow/stream_executor/cuda/cuda_dnn.cc#L2200-L2262 (assuming you don't have XLA or JIT enabled, otherwise the code path might be different), which indeed has something to do with memory, and you might find some clues from the error log if you wish. But unfortunately I cannot reproduce it, so I can't help much.. @lbin You mean in training?\nAnd 127 epochs not steps? This sounds like a different problem.. It's been a while now and I'm not sure if the problem still exists. I've never seen OOM but I don't have TitanX or TitanXp or K80 either. The training runs fine on M40 (also 12G memory) and P100. Others have run it successfully on V100 as well. \nWith more results been published recently the code is more stable now, although I still have no clues why you guys saw OOM. I did find that using PrefetchDataZMQ here increase about 100MB GPU memory in training but this is probably irrelevant. \nAfter all even you two saw different types of OOM (one in training but one in evaluation). So I still believe this is an environment-specific problem.. @tranorrepository I don't understand what your issue is. Could you open an issue following the issue template.. As a closing remark, even if this issue still exists, it's probably a very platform-specific problem because many people have already successfully used this piece of code. My last suggestions to anyone who may encounter OOM in the future: \nMake sure no other processes (e.g. Xorg) are running on GPUs.\nFor training, try smaller batch sizes (FASTRCNN_BATCH_PER_IM).\nFor evaluation, try smaller TEST_POST_NMS_TOPK.. I'm not sure I understand what do you mean by \"testing\".\nI assume you want to use the model in testing mode during training, with the get_predictor or InferenceRunner API. If that is the case you can just write the model differently for testing, like this:\n https://github.com/ppwwyyxx/tensorpack/blob/9f6b58d547c74e61422bf0a3f99565009b2dc500/examples/FasterRCNN/train.py#L93-L94. You can write them as part of your model.\nhttps://github.com/ppwwyyxx/tensorpack/blob/9f6b58d547c74e61422bf0a3f99565009b2dc500/examples/ResNet/imagenet_utils.py#L181-L195. It's in the readme.. No we don't have it.. Looks like a python2 only issue. I'll fix it soon. Next time please include relevant version information for such bugs.. My best guess is that you use some inappropriate data preprocessing in training. But I can't help more since there is no code I can run and it's unlikely a tensorpack issue.. It will call dataflow.get_data() again, and it depends on the get_data implementation whether it will start over. For most dataflow it will start over.\nYou can use FixedSizeData to keep the iteration state.. I guess this is a bit counter-intuitive and may cause subtle error. I'll see if I can change this somehow.. Ah my bad. dataflow.get_data() actually won't be called at epoch beginning -- input source already takes care of the iterator state when sending them to the graph. So the iterator state will be preserved and dataflow won't start over at each training epoch.. Isn't this #454?. DumpTensors was added Oct 15. If your tensorpack is newer than that you should be able to import.. Thanks very much for finding this subtle bug. I think your solution makes sense.. Use MultiGPUGANTrainer with CUDA_VISIBLE_DEVICES should work.. raw_devices = ['/gpu:{}'.format(k) for k in range(nr_gpu)] This is using the first x GPUs in CUDA_VISIBLE_DEVICES.. In tensorfllow, '/gpu:0' and '/gpu:1' means the first and second GPU in CUDA_VISIBLE_DEVICES, as long as you set the environment variable.. Write your own dataflow for it. In that dataflow you can call _get_augment_params(image) and then _augment(image, params). \nOr you don't have to write your augmentation under the imgaug API, it can be written more casually since you need your own dataflow anyway.. Done. See updated readme. https://github.com/ppwwyyxx/tensorpack/tree/master/examples/ShuffleNet\nOnly after uploading the model I've realized how small it is.... The tutorial has more information about how to find the bottleneck.\nPrefetchData is not efficient especially for large data and you'd better use PrefetchDataZMQ whenever possible. docs\nI don't know why it uses large VM but in my experience tensorflow always uses large VM. I don't think that's a big problem. Just using 4 TitanX alone would've accounted for 48G VM space already.. Feel free to reopen if you still have questions.. It's best to first figure out what makes your script crash. Virtual memory, as the name suggests, is virtual. There is no reason for slurm to limit your virtual memory usage. And using TB of virtual memory is exactly how lmdb works.\nSlurm probably limits your resident memory use and it may overestimate it by adding resident memory of each process although they are actually shared. Using a smaller number of processes can probably make it happy. . In the paper, \"CIFAR-10 PreAct ResNet-18\" error goes from 5.6 to 3.9. Sorry but we hope to only include examples with reproducible performance. It's hard and that's why it's important. Otherwise it becomes a big collection of code that appears useful but frustrates the users, like most of the deep learning code I've seen on github. Pretty much everyone knows how to write a new model, but the hard part is in achieving the performance.\nIn anycase a README is a must. Without a result that people can reproduce, open source deep learning code is of no meaning. . Briefly talked to the author. Their \"CIFAR-10 PreAct ResNet-18\" is the architecture here, not exactly what we have in the examples.. Cool. What's the number without mixup?. Good result! Is this still following the kuangliu/pytorch-cifar setting?. Sure. Please also change the docstrings at top of the file, to include notes about the settings and the performance.. Could you merge two scripts into one with a flag? For example purposes it doesn't make sense to have many files with almost identical contents.. Each resnet block in the reference code has conv{1,2,3} and another conv in shortcut. https://github.com/kuangliu/pytorch-cifar/blob/886af4caa4ee224c3fd4b42e997e43b1b4f234b9/models/preact_resnet.py#L57-L64 \nOne seems to be missing here?. Thanks! The code is OK. If you don't have time I can train it as well.. I guess the real question is how do you clip the gradients.\nIf you could just remove the None values before clipping you won't have the problem.. btw, MultiProcessDatasetPredictor doesn't scale very well. Doing inference is not a focus of the library and everything that predicts offline in tensorpack is feed_dict based and not well optimized.. Any valid tensorflow symbolic code is valid in tensorpack. You can just use it the same way. Just remember that your model code might get called multiple times if trained with multiple GPUs.\ntensorpack trainers don't use the loss collection for training. You can use it for your own purpose, of course.. Feel free to reopen if you have more questions.. This is not a tensorpack-related issue.\nShuffleNet is designed to have low-flops with good accuracy. Low-flops doesn't mean faster. To be fast you'll need good kernels (mainly for group conv) which tensorflow doesn't have.. The way you're using should work. The error is saying the model path is wrong. Any chance you may have removed the checkpoint by mistake?\nEverything other than the training can be done in callbacks. You can write a callback which, when triggered, run the evaluation (or get previous evaluation results) and change train_dataflow.dist. You can have the DataFlow be a subclass of Callback as well. In reinforcement learning the data always depend on the training, so this has been used in both RL examples, e.g. in DQN dataflow is a callback.. Ah that's a problem. If you use queues, there will definitely be several datapoints from the old distribution after you made any changes to the dataflow. \nTo really avoid this you will need to 1. pause your dataflow (make it block) and 2. Eval the inputs with sess.run + timeout option, until it timeout so the queue is empty. Perhaps this can be made a builtin method of QueueInput.. The dataflow will be stopped. The queue will be closed but not removed until you reset the graph.. btw, by default training data won't be reset between epochs.. data=QueueInput(dataflow)\ncallbacks=[..., CallbackFactory(trigger=lambda _: data.reset_state())]\nto reset it every epoch.. This function creates more problems than it solves, mainly because dataflow doesn't guarantee anything about being reset-able. Calling reset_state or get_data for more than one times is not guaranteed to do useful things, and for a lot of parallel dataflows it's usually hard to do anything. As a result this function actually will lose some data.\nBefore there's a clearer semantics on what it means to reset the state, this function was renamed to refill_queue to avoid being called by inferencerunnner.. ```diff\ndiff --git i/examples/ShuffleNet/shufflenet.py w/examples/ShuffleNet/shufflenet.py\nindex fa27ad1..42a890b 100755\n--- i/examples/ShuffleNet/shufflenet.py\n+++ w/examples/ShuffleNet/shufflenet.py\n@@ -88,9 +88,9 @@ class Model(ImageNetModel):\n         with argscope([Conv2D, MaxPooling, AvgPooling, GlobalAvgPooling, BatchNorm], data_format=self.data_format), \\\n                 argscope(Conv2D, use_bias=False):\n             group = 8\n-            channels = [224, 416, 832]\n+            channels = [384, 768, 1536]\n\nl = Conv2D('conv1', image, 16, 3, stride=2, nl=BNReLU)\nl = Conv2D('conv1', image, 24, 3, stride=2, nl=BNReLU)\n             l = MaxPooling('pool1', l, 3, 2, padding='SAME')     with tf.variable_scope('group1'):\n\n```\nThe above code runs without problems.\n\n\n(384-24) / 4 = 90 % 8 != 0. this formula comes from nowhere. \n384 % 16 == 0 is enough. -24 only happens in the last conv of each unit.. I don't understand what you're saying but since the code can run and you can print the channels of every tensor I assume you can figure these out by your own.. You can print the static shape of tensors by something like print(l).\n Inside a sess.run you can still print by tf.Print.. The original idea was to allow some errors to happen occasionally as long as the training is still going, because failing the whole training due to some dirty data can be frustrating.\nBut I think this is not a good default, due to reasons you mentioned. You can add an option and change the default behavior. Do you want to work on it?. The current exception handling code piece is repeated for 3 times -- which means it needs to be extracted out to a common function, with an option that disables exception handling.. All models are hosted on google drive with limited space. VGG is much larger than any other models and I'm afraid I can't share that before we use a better place to host models. . https://help.github.com/articles/distributing-large-binaries/\nMaybe we can try this, probably on a separate repo, (e.g. tensorpack/models).\nNot sure if github is OK with us putting GBs of models in an empty project.. But now the model converted from caffe can be downloaded at http://models.tensorpack.com/caffe/.\nThanks @PatWie for hosting the models! When I have good network I'll move more models there.. You change your layers to NCHW. But your input is not in NCHW.\nAlso you may need to modify the quantization function since it may assume NHWC.. The resnet example supports both data format. But you do need to learn some tensorflow before you can change the quantization function.. We have horovod trainer already. Implementing a slower strategy does not make a lot of sense. \nTensorflow will probably improve its native distributed all-reduce performance in the future and before that horovod is the best solution for distributed training.. Yes. I've seen the same error on one machine (but not the others) and use the same solution. I think that's because the GPU on that machine is in exclusive mode -- so using multiprocess may cause problems like this.. Can you check your nvidia-smi -q | grep 'Compute Mode' ?. Interesting. Mine is P100. It might have something to do on how the new GPUs handle the fork. It works on old GPUs, though.\nI'll take a deeper look when I got time. Meanwhile you can just disable the prefetch because data is not a bottleneck for detection.. The performance will get stable only after about 3k steps. The default settings will take 70~80 seconds per epoch. GPU utilization is 70%~80% with the current default setting.. The dataflow problem should be fixed now.. Here the speed roughly decreased from 70 sec / epoch @epoch10 to 120 sec / epoch  @epoch700. It decreases because of more and more positive predictions. I haven't seen 200.\n1. Those images are filtered out.\n2. Precision was only added yesterday so I don't know. It's probably OK for training -- if rpn predicts everything as negative (even just for once) it will be nan forever. But I'll change it because this metric becomes useless.. Yes.. This looks more like a problem of your environment.. I can't understand exactly what is \"dynamically change the image resolution\". But from all I can see tf.image.resize support target size as a \"tensor\" so it's dynamic.. FYI I have a bug introduced in Nov 13 and fixed just now. It will affect the precision.\nLots of changes are being pushed recently. I tested the model periodically which means bugs will be found with a delay.. Tensorflow convolution needs warm up. For variable-size inputs it needs more.\nThe overall speed will always first increase (until about 10 epochs) and then decrease. A subtle bug that makes the result 2 points worse: https://github.com/ppwwyyxx/tensorpack/commit/6fc4378cc65c3896e802f037bd8d289412d6b2c1  .\nNow the training curve looks the same as what I had before -- it hasn't finished, but probably is correct now.\nThis again shows how important it is to match the paper's performance -- if I didn't try to compare with some reference number, I'll never find hidden mistakes like this.. Thanks! I know the model probably gets slightly better than before, but haven't got time to train it.. There are many factors here:\n1. 3k steps may be not enough warmup. I checked my recent logs and saw about 20% better speed at around 10k steps. I'll update the notes later.\n2. I don't have numbers but I won't be surprised if K80 is 4~5 times slower than V100.\n3. https://github.com/ppwwyyxx/tensorpack/blob/8b4d4f779dc48eea299a0b15fe7a0f714e5b8113/examples/FasterRCNN/model.py#L315-L319\nThese two lines are recently added. They may impact speed (probably not much, if any) but I haven't run a benchmark yet.. 4. @chunfuchen was not training a mask-rcnn, but a faster-rcnn. (because mask-rcnn implementation was added later). The document says: \"To reduce the effect of GIL to your main training thread, you want to uncomment the line so that everything above it (including all the threads) happen in an independent process.\"\nMaybe you can try this. The gap indeed looks too large.. There is no recent activity so I'm closing it. In general for any performance problems please do some investigation as in http://tensorpack.readthedocs.io/en/latest/tutorial/performance-tuning.html because I'm unable to run your code.\nFeel free to reopen if you still have questions. . You can easily add images to tensorboard in a callback, with self.trainer.monitors.put_image:\nhttps://github.com/ppwwyyxx/tensorpack/blob/7f55d502aa181073f6ca0b9f2d4131c2c87369c7/examples/GAN/CycleGAN.py#L196-L202\nSee what you can do in the callback. This is unrelated to tensorpack and is a tensorflow bug. See https://github.com/tensorflow/tensorflow/issues/9374  https://github.com/tensorflow/tensorflow/issues/9831. Possible workaround:\n1. Remove all use of get_nr_gpu in imagenet, so that cuda context is not initialized before session creation. This may let TensorFlow repsect your session config.\n2. Setting per_process_memory_fraction in session config will probably limit the maximum memory to use (at least on my laptop). allow_growth seems broken and I don't know the workaround.\n. Thanks for the workarounds! Closing this now.. Those variables will be saved.\nBy \"when testing\", did you mean \"when you define the graph for testing\"?. In any case, for others to understand what the problem is, please post \"what you've observed\", as suggested in the issue template.. You start with the epoch number but didn't load the previous model. The example has a --load option.. @erdollar \nAre you aware that \"/train_log/alexnet-dorefa/model-660000.index\" and \"./train_log/alexnet-dorefa/model-660000.index\" are two different paths?. PReLU takes two arguments. See http://tensorpack.readthedocs.io/en/latest/modules/models.html#tensorpack.models.PReLU.. the line PReLU('prelu', x) got executed multiple times. You have to use different names each time.. You're the one who wrote things with PReLU and you'll make this decision by yourself.. It's expected and not a problem.. 3 days ago, opencv supports disable this warning. https://github.com/opencv/opencv/pull/10155. The documentation should have enough information.\n. Having both load-vgg16 and load-vgg19 still feels weird. Maybe later will move things (with load-alexnet, or maybe load-cpm as well) to a separate directory.. Ideally similar code should not co-exist, like I'm saying in another PR: https://github.com/ppwwyyxx/tensorpack/pull/481#issuecomment-345672569\nAnd having two models (or just two tower_func) sounds good.\nBut you're right, for something like VGG, people usually just want something simple to copy and paste.. . 1. The first argument (gpus) needs to be a int or a list of int. Somehow it's missing from the documentation.\n2. Please post full error when you report bugs.. Closing due to lack of activity. Feel free to reopen and include details if you still encounter error.. There are some bugs in distributed trainer. Should be fixed now.. You'll create a predict function inside the callback, and call it however you want when the callback is triggered. You'll need to compute those statistics on your own.\nSee http://tensorpack.readthedocs.io/en/latest/tutorial/extend/callback.html#explain-the-callback-methods\n\nself.trainer.get_predictor() is a helper function to create a callable under inference mode.\n\nExamples:\nhttps://github.com/ppwwyyxx/tensorpack/blob/aaf62f2db9ae4fcf40772a35df8b41d4f5c5ddf1/examples/GAN/CycleGAN.py#L188-L199. You bring two issues if I understand correctly:\n1. Reading tfrecord is slower than reading images directly, both with tf.data APIs.\nThis is only about \"how to write more efficient tf.data pipeline\" and is nothing about tensorpack. We won't  discuss it here.\n\nYou used a snippet to test raw speed of tfrecord and found it faster than reading images.\n\n-- How do you know the speed of image_dataset? Have you written a similar script to test it? If not, you're probably not comparing things apple-to-apple. . It's just a simple iterator.get_next() as you can see. No magic is happening: \nhttps://github.com/ppwwyyxx/tensorpack/blob/ef9fb4b8b7b1c2c4b1b8b19bd92650665a48f822/tensorpack/input_source/input_source.py#L433-L445\nI don't have a good explanation for that tfrecord become slower than image_dataset during training. Some points you might want to investigate:\n1. How do you know IO is the bottleneck? Is that a correct conclusion?\n2. Use only one GPU to make the problem simple\n3. Have you tested more than one times to make sure the observation is stable?\n4. During training a different session config is used (tensorpack.tfutils.get_default_sess_config). Maybe it affects the speed and you can try to benchmark with this config instead.. There isn't a good setting that works for everyone's machine and you'll need to figure it out by testing.  Things can be very different especially with slow hardware. For example, you use 30 threads but I don't even know whether your machine has 30 cores or not, how can I comment anything on it? \nYou can run some benchmarks easily to figure out what is causing the slow down.\nSee tutorial on performance tuning.\n. The purpose of benchmark is to figure out what is the bottleneck, by changing the pipeline. Only knowing how slow the whole pipeline is doesn't help you improve it.\nThe documentation of PrefetchDataZMQ is clear that it requires the underlying dataflow to be fully shuffled.\nThere are other multi-process dataflow such as MultiProcessMapData.. multiprocessing is just a way a program runs but how it can be used is up to the user. What you described sounds like MultiProcessMapData but that's very different from PrefetchData as you can see from their APIs.\n\nAssume that I have A, B, C, D functions to process the data.\n\nI assume you mean \"map\" the data, i.e. you have x and you need D(C(B(A(x)))). Then yes, you can use multiple processes to run the mapping on a lot of xs simultaneously, to perhaps improve speed, if you can beat the overhead of processes. The overhead is mainly on copying the data to processes and copying the result back.\n\nThat means A doesn't have to wait for B,C,D to finish before starting a new round of work. \n\nPerhaps you mean BCD don't have to wait for A? Not true because logically B always depends on the output of A. But A may finish sooner because you run it multiple processes.\n\nthe best way to use multi-process should be adding a multi-process function to any computationally intensive step.\n\nNot true. Using processes has significant communication overhead so you can't use it everywhere. If you have four mapper ABCD to apply, it's better to merge into one function f(x) = D(C(B(A(x)))) and run f(x) in multiple processes so you don't end up doing too much communication.\nPrefetchDataZMQ is a completely different mechanism. You can think that it makes several copies of a dataflow and run them in multiple processes. When they are running the results are copied back to main process for use.. The tutorial http://tensorpack.readthedocs.io/en/latest/tutorial/efficient-dataflow.html contains some explanation of \"what's going on\" that can help you understand. For the code you posted, there are a total of 3 processes because you use two prefetch with nr_proc=1.\nProcess A reads LMDB and put data in a queue.\nProcess B decodes data from the queue, run mapping with 30 threads, put results to ZMQ.\nMain process takes data from zmq and batch it. . There is no recent activity so I'm closing it. In general for any performance problems please do some investigation as in http://tensorpack.readthedocs.io/en/latest/tutorial/performance-tuning.html because I'm unable to run your code with your hardware.\nFeel free to reopen if you still have questions.. There are other ways to print tensors other than summaries: https://tensorpack.readthedocs.io/tutorial/faq.html#how-to-print-dump-intermediate-results-in-training\nSummaries are added to tensorboard in the callbacks:\nhttps://tensorpack.readthedocs.io/tutorial/summary.html#tensorflow-summariesAnd you can use a different period option: https://tensorpack.readthedocs.io/modules/callbacks.html#tensorpack.callbacks.MergeAllSummaries. For DictRestore this is not needed. Users can just use\ndict = {prefix + name: value for name, value in six.iteritems(dict)} to do the same thing.. Users should care about the format if they use DictRestore. That's why it takes a dict but not a file name.. In fact using the dict and npy format is specifically for users to load and modify the model manually. Being easy to load and modify is the only benefit it has compared to tf format.. About forks, you can also just write a collection of functions for your convenience, e.g. a wrapper around DictRestore or logger.set_logger.dir. This might be easier than maintaining a personal fork.\nAbout light-weight. Yes I hope it's lightweight -- though there is a lot of complexity already. So in general I don't like one-line features. \nAbout consistency: if there are 5 Restore and one is different from others, I'll call it a consistency issue. These two in my mind were just two things with different functionalities and therefore different APIs. I don't even know if there will be a third Restore, because I can't imagine what else one can need.. You can always write print in your code if you like. I don't see why this would become a problem. A3C-Gym/common.py is a symbolic link. You did something during clone that broke symbolic links.. tensorpack saves models in standard format so theoretically there is nothing you cannot do as long as tensorflow supports it, regardless of what symbolic libraries you use.\nI don't know what you mean by \"doesn't work\". Segfault always sounds like a tensorflow bug. Without you providing information about what you did I cannot help more.. What result do you expect?. For summarize_graph: by \"inputs\" it means \"placeholders\". But indeed there are no placeholders in the graph, because placeholders are slow. So it works as expected.\nFor toco: tensorflow seems to only support NHWC for tflite, but the graph you used is for GPU and therefore is NCHW. So the graph cannot be converted to tflite. \nTo work around both of the above issues you'll need to build the graph again, with placeholders & NHWC, rather than import the metagraph.. To build the graph for inference you do not need to use tensorpack. Just build like normal tensorflow. All you need to do is create placeholders and call your symbolic functions on them.\nTo avoid writing all the symbolic functions again you can use model.build_graph(placeholder1, placeholder2) but honestly it's equivalent to writing them with pure tensorflow.\nYou can leave _get_inputs the way it was. It has nothing to do with whether the graph uses placeholders or not.. Still, it has nothing to do with tensorpack.\nFrom tensorflow source code contrib/lite/toco/import_tensorflow.cc it seems like it has something to with convolution data_format.. It depends on your model and I cannot answer that.. And FYI for most tensorflow models I've seen, the result can be the same if you correctly rewrite the model with a different layout.. This seems to be a side effect of the GPU utilization tracker callback. Should be fixed in the latest commit.. http://tensorpack.readthedocs.io/en/latest/tutorial/save-load.html#work-with-tf-checkpoint\nIf you could print the names it should be easy to know which name you need.. You can also use NewCheckpointReader.get_tensor(name).. The code looks reasonable. If I'm to debug this problem I'll need something I can run. If you're sure dataflow is the cause, post a snippet that only runs TestDataSpeed (i.e. no model and no training), and don't include file dependencies (e.g. use a constant numpy array) so I can run it.\nSee more at http://tensorpack.readthedocs.io/en/latest/tutorial/performance-tuning.html. There is no recent activity so I'm closing it. In general for any performance problems please do some investigation as in http://tensorpack.readthedocs.io/en/latest/tutorial/performance-tuning.html because I'm unable to run your code.\nFeel free to reopen if you still have questions.. When I played with SR before I found opencv bicubic baseline has very bad PSNR compared to matlab bicubic, which makes some results very hard to reproduce. Maybe you want to check that.. ```\nopencv:\nimport numpy as np\nimport cv2\nimport sys\nim = cv2.imread(sys.argv[1])\nim = im.astype('float32')\nimy = cv2.cvtColor(im, cv2.COLOR_BGR2YCR_CB)\nprint imy.shape\nimy = imy[:,:,0]\ndown = cv2.resize(imy, (0,0), 0, 0.5, 0.5, cv2.INTER_CUBIC)\nprint down.shape\nup = cv2.resize(down, (0,0), 0, 2, 2, cv2.INTER_CUBIC)\nv = np.square(up - imy).mean()\nprint v\nprint 20 * np.log10(255 / np.sqrt(v))\nmatlab:\nfunction bicubic\nim = imread('barbara.bmp');\nim = rgb2ycbcr(im);\nimy = im(:,:,1);\nup_scale=2;\ndown = imresize(imy, 0.5, 'bicubic');\nup = imresize(down, 2, 'bicubic');\ndiff = up - imy;\nl2 = mean(mean(diff.*diff))\n20 * log10(255/sqrt(l2))\nend\n```\nThis was the test code I was using (two years ago when I tried to reproduce SR-CNN). It only compares the y-channel. I remember on Set14 opencv is much worse than matlab.\nMaybe on 3-channel images it's different.\nAfter fixing the bicubic issue we were able to reproduce SRCNN, and that's also when I stop working on it cause I felt the dark side of SR.. Where do the roms in \"site-packages/atari_py/atari_roms\" come from? I think they came from exactly the same place (i.e. atari_py).. The error is telling you already. Your label shape is (128, 1), but (128,) is expected.\nBecause before batching, your label is a vector of length 1, not a scalar.\nnp.array([int(temp_str[1])]) apparently this line of code creates a vector of length 1.. self.file_list.append((os.path.join(dir_path, temp_str[0]), np.array(int(temp_str[1])))). Sure you can. As long as you can build the graph and define what to do in an iteration, you can train it with tensorpack.\nThere are no documents about GAN training. GAN is just an example of how to define a trainer for non-standard tasks. GAN is not part of the library. The documents about defining a trainer are http://tensorpack.readthedocs.io/en/latest/tutorial/trainer.html and http://tensorpack.readthedocs.io/en/latest/tutorial/extend/trainer.html.. There is no way I would know why your model does not work for your data. There is no indication that this is a tensorpack-related problem. Closing now.. Do you think you need align_corners=True for tf.image.resize_*? \nTo be honest I don't like tf.image.resize_ because they never align pixel to pixel regardless of align_corners. But maybe True makes more sense here?\nYou can check this out:\n```python\nimport numpy as np\nfrom skimage.transform import rescale\nnp.set_printoptions(linewidth=10000)\narr = np.array(\n    [[1,2,3,4],\n    [5,6,7,8],\n    [9,10,11,12]], dtype='float32')\ninput = tf.constant(arr)\ninput4D = tf.reshape(input, [1, 3, 4, 1])\nresize = tf.image.resize_bilinear(input4D, [6, 8], align_corners=True)[0,:,:,0]\nsess = tf.Session()\nr1 = sess.run(resize)\nr2 = rescale(arr/100.0, 2, mode='edge') * 100\nprint(r1)\nprint(r2)\n. Is the visualization produced by Saccade?. At last, rename to `SuperResolution`. What is \"enet-pat\"....? will you rename it to \"enhancenet\"?. And how long did you train these models?. Why would you have `checkpoint model-806817`? Are we using the same dataset?:\n100%|#######################|19714/19714[59:02<00:00, 5.56it/s]        \n[1206 04:49:54 @base.py:255] Epoch 40 (global_step 788560) finished, time:3542.78 sec.                                                           \n[1206 04:49:55 @saver.py:82] Model saved to train_log/enet-pat/model-788560.                                                                     \n[1206 04:49:55 @monitor.py:363] GAN_loss/discrim/accuracy: 0.99414     \n[1206 04:49:55 @monitor.py:363] GAN_loss/discrim/loss: 0.019299        \n[1206 04:49:55 @monitor.py:363] GAN_loss/gen/accuracy: 0.0026996       \n[1206 04:49:55 @monitor.py:363] GAN_loss/gen/loss: 8.9464\n[1206 04:49:55 @monitor.py:363] QueueInput/queue_size: 49.585\n[1206 04:49:55 @monitor.py:363] add: 19.204\n[1206 04:49:55 @monitor.py:363] additional_losses/loss_LA: 8.9464\n[1206 04:49:55 @monitor.py:363] additional_losses/loss_LP1: 1.3098\n[1206 04:49:55 @monitor.py:363] additional_losses/loss_LP2: 0.0015531\n[1206 04:49:55 @monitor.py:363] additional_losses/loss_LT1: 2.3112e-37\n[1206 04:49:55 @monitor.py:363] additional_losses/loss_LT2: 2.2718e-37\n[1206 04:49:55 @monitor.py:363] additional_losses/loss_LT3: 2.3154e-37\n[1206 04:49:55 @base.py:245] Start Epoch 41 ...\n100%|#######################|19714/19714[59:03<00:00, 5.56it/s]\n[1206 05:48:58 @base.py:255] Epoch 41 (global_step 808274) finished, time:3543.57 sec.\n``. vgg19 should be applied to images of range [0,255]-mean. But just found that here it is on images of [0,1]-mean/255. (check the [initial commit](https://github.com/ppwwyyxx/tensorpack/pull/527/commits/b10effc0bff7882d8ec18af231f7974247e6d70b) )So the entire additional_losses would probably do nothing good.. Ah. I'll correct the name. These visualization code wasn't maintained for a while.. OK.. I'll run it when I get a time. Now--visualize` should be runnable.. Yes.\nThis is not a tensorpack question. I'll close it if there is no more questions.. Look at your log directory. That's where your models are. Breakout-v0.npy is my released pretrained model.. Please tell me what you did as mentioned in the issue template.\nAlso for such problems your environment info is relevant.. Closing because necessary information about the problem wasn't provided. \nIt's very likely that upgrading TF will just solve this issue.. Not a tensorpack issue. As the model is just in standard checkpoint format and you can definitely load it.\nThere could be millions of reasons why the \"result is not right\". You have to confirm it's because of model restoring. I would sess.run() a weight tensor after restoring, and see if it equals the one saved in the checkpoint.. Is there any tensorpack code involved in your code?..\nI don't know what you mean by \"result not right\". I don't even know what your code is trying to do. From the information you provided it seems like model restoring itself is working fine.. No indication that this issue is a tensorpack problem. Closing now. Reopen if you have more information to provide.. Things work fine on my side. By \"pulled latest code\" do you mean you update both tensorpack and fasterrcnn? From the log they are located in different directories.\nOlder examples will run with new tensorpack - that's back-compatibility. But newer examples don't necessarily run with old tensorpack.. Oh.. if you made changes you should tell.\nThe code is written for NCHW. To work for NHWC you need to do more than global string replace.\nI'm sure the change is the cause of your issue. Please debug that.. > Any plan to add NHWC support in tensorpack? Most of frameworks support test/inference on CPU.\ntensorpack supports NHWC because it doesn't care what your model is at all. But you'll always need to implement NHWC version of fasterrcnn which I don't plan to do.\nYou can just remove the line imgaug.Flip(horiz=True). It's impossible.\nI said remove imgaug.Flip(horiz=True), not horiz=True.. works for me.. it's enabled by default in tensorpack. You do not need to do it.. Did the following in https://gist.github.com/ppwwyyxx/6da15f2d9087373635254d4e6fbe4891:\nUsed the paper's normalization\nFixed VGG scale\nUsed same learning rate for G and D\nUsed SeparateGANTrainer\nAnd now I can always obtain a model better than baseline (in all the 3 times I've tried).. If you could just make a mathematical (rather than conceptual) definition of ROIAlign, it'll be easy to do it correctly then. And it'll be easy to see why some coordinate transform is necessary. It's hard to explain it by words. The simplest answer is, their formula is different, and you can only see that if you derive the formula.\nThe definition would be something like: given an NxN matrix and a bbox (x0,y0,x1,y1) following my definition of bounding box in the notes, get an roi of size kxk, write down the formula for pixel (i, j) in the roi. . To see the problem with crop_and_resize, try cropping the top-left 4x4 square from a 5x5 square. You may have to try a couple of time to succeed, because it just doesn't behave as what you may expect.\nClosing as this is a tensorflow issue.. TOTAL_BATCH_SIZE is always 256 no matter how many GPUs you use. So one epoch is always one epoch. The number of GPUs only affects the number of samples per GPU.\nThe original resnet paper are the same in terms of the definition of iterations and epochs. It may differ a little bit (i.e. 5004 vs 5000) but that's negligible.. Yes.. Please always paste your observation instead of describing it. I do not understand what you observed.\nAs far as I know you will not see anything like 4908/26690 if changing TOTAL_BATCH_SIZE is the only thing you did.. You will NOT see anything like this if you're using the ResNet example in this repo and only changed the batch size. Please confirm what you did, including the changes you made and command you run.. Closing because information about the problem was not provided. Reopen if you still have questions. . If this is two independent runs, you cannot expect them to have the same results in any case.\nAnother thing to note is, if you didn't half the batch size correctly you may have trained the dataset twice so you got better results with two gpus.\nBut there isn't enough information given. You're not using any examples and there is not enough code to tell what happened to you.. I prefer to believe you did something wrong but there is not enough code to tell. And to be honest, there could be many other reasons specific to your task that can cause performance differences.\nIf you think there is something wrong with DataParallelInferenceRunner, the way to test it is to enable both InferenceRunner together and see if they gives consistent results. That's how I tested its correctness.. If your dataset_test is a DataFlow, it's unrelated to this comment.. Because by default they use the same name for logging.\nBoth ScalarStats and ClassificationError has options to use a different name. See docs: http://tensorpack.readthedocs.io/en/latest/modules/callbacks.html#tensorpack.callbacks.ScalarStats. My best guess is you did something wrong with batch size. But again, there is not enough code to tell. You set batch_size  /= len(nr_tower) but I don't see where it is used.. Since batch size is halfed and #GPU is doubled already, steps_per_epoch should be fixed, rather than doubled. Maybe you've missed the doc here.\nWhen you don't have enough devices it will still run. TensorFlow will try to put the graph on existing devices.. Keep batch_size (i.e. batch_size_per_gpu) * #GPU * steps_per_epoch unchanged.\nSimplest way is just to keep steps_per_epoch a constant.. Again, if these are two independent runs, you cannot expect them to have the same results anyway. There could be many other reasons in your code that cause differences. \nAt least I wanted to make sure you're using batch size & steps correctly. On existing examples such as cifar10, when batch size & steps are changed accordingly with different #GPUs the performance will be roughly the same.\nIf you want this to be a bug report you'd better provide more information such as code and instructions and logs. \"the problem is still there\" isn't helpful at all in solving any problems.\n. ppwwyyxxc@gmail.com\ncode & logs.. How did you install tensorpack? There was an old version which may have this problem.\nIf it's installed with pip, try: pip install -U git+https://github.com/ppwwyyxx/tensorpack.git.. Oh looks like latest master is actually broken. It's probably related to a recent commit a couple days ago.. I'll check that.. Should be fixed now. It's always very hard to run tensorflow on multiple processes. The code worked before but is broken now probably because I'm using a newer tensorflow. \nI'll just disable multiprocessing for now, since it shouldn't affect speed for fasterrcnn.. It tries to clear the queue. It does not hurt but I'll see if I can get rid of this annoying message.. Have you modified the config to use resnet101 at all?. https://github.com/ppwwyyxx/tensorpack/blob/3b994877d4502539d83f101597de2277a1b00919/examples/FasterRCNN/config.py#L18-L19\nAlso, to report a bug, please follow the issue template.. I don't see much value in bringing in TFGAN. From what I can see the training utilities will only make the current code more complicated. The symbolic functions aren't very interesting either, and if you need them for a new type of GAN you can just import and call them.. There is no way to automatically \"guess\" which part of graph is useful for inference.\nYou can just write (copy) pure symbolic code to construct the graph again.\nAlso see https://github.com/ppwwyyxx/tensorpack/issues/513#issuecomment-348093710\nand tutorial http://tensorpack.readthedocs.io/en/latest/tutorial/inference.html#inference-after-training. That script is perhaps not very reliable. IIRC it assumes input tensors for inference are placeholders, but that's not always true in a graph saved by tensorpack because placeholders are slow. It works for InferenceRunner(some_dataflow), though.. Unrelated to tf.summary.image. But you do need to read the doc before calling an API.. You were not writing a dataflow. You need to yield your datapoints one by one.\nSee docs: http://tensorpack.readthedocs.io/en/latest/tutorial/extend/dataflow.html. You were not writing a correct dataflow. A datapoint has to be a list of python objects. It can be a list of 1 element.\nSee http://tensorpack.readthedocs.io/en/latest/tutorial/dataflow.html#what-is-dataflow\nand http://tensorpack.readthedocs.io/en/latest/tutorial/extend/dataflow.html. they are False. The graph has ops that are unsupported on GPUs. In training allow_soft_placement=True was enabled already so nothing happened.. http://tensorpack.readthedocs.io/en/latest/tutorial/save-load.html\nhttp://tensorpack.readthedocs.io/en/latest/tutorial/faq.html#how-to-freeze-some-variables-in-training. If you keep TOTAL_BATCH_SIZE *steps_per_epoch a constant, this means one epoch go through the same amount of data. . That's not how I define an epoch but different people have different definitions. Please note that 1281167 is a prime. . I don't think pyglet display works on ubuntu windows.. > I am using the Ubuntu Windows store app and have installed opencv and pyglet, yet I get the error message.\nI assume it means this? https://www.microsoft.com/en-us/store/p/ubuntu/9nblggh4msv6. It doesn't support this feature. The code is only 5 lines so you can easily extend it or write a new one: http://tensorpack.readthedocs.io/en/latest/_modules/tensorpack/tfutils/gradproc.html#GlobalNormClip. You probably need to upgrade setuptools and pip.. Switched to another syntax following https://github.com/pypa/setuptools/issues/1087, so it works with older version of setuptools.. Is there a reproducible code? Could you provide your environment info?. Interesting. The code can exit correctly on my machine.. To manually close processes you can call ds.__del__() when ds is PrefetchData or PrefetchDataZMQ.\nName conflicting is a general Python issue and you should just avoid putting your own code into PYTHONPATH.. They were terminated but not joined, so they became zombie process. The above commit should fix it.. Closing since there is no response from @tonyw on details of the original issue.. When you \"specify output node name\" that is not for only one node. You model will be used more than one time and multiple nodes will be created. One may be named \"tower-pred-0/output\", one may be named \"tower1/output\", etc, depending on the name scope under which the tensor is created. This is how tensorflow works.. Yes. Why do you need to control it?. All built-in trainers are for single-cost optimization task. You need to write your own trainer if you need to alternate between different costs. The tutorial has more details: http://tensorpack.readthedocs.io/en/latest/tutorial/extend/trainer.html ; http://tensorpack.readthedocs.io/en/latest/tutorial/trainer.html\nYou can also re-formulate your problem to a single-cost optimization one, e.g. by using tf.cond. But this may create some overhead.. You can have InputSource i1 and i2. Call  tensors1 = i1.get_input_tensors() and tensors2 = i2.get_input_tensors() respectively to get tensors from the two input source. Then build your graph out of those tensors. . Also be careful with certain callbacks which evaluate tensors every step. \nFor instance, all moving averages summary ops are by default in the same collection (added by the add_moving_summary function) and will be run every step (by the MovingAverageSummary callback), regardless of which cost you're optimizing. These ops probably depend on your input (e.g., when you're summarizing the output of some layer),  so you may want to disable this callback so that you won't evaluate inputs2 when optimizing cost1.. FullyConnected is merely a symbolic function with get_variable inside. To share variables, call FullyConnected the second time in a previously-used variable scope with reuse=True.. Thanks for the code. It's very helpful to have runnable code for debugging.\nqueue.put will block when a queue reaches its max size. When you are unlucky: one queue reaches max size and the other queue is empty, the whole thread blocks. \nYou can proof that the expected time it happens is (maxsize)^2. \n  . Some other notes:\nfor dataflow, desc in zip(inputs, inputs_desc_list):\n            cbs = dataflow.setup(desc)\n            self.register_callback(cbs)\nBecause of this, every hooked_sess.run will feed both data to the graph, though only part of them will not be used for training. This is because data feeding is performed through callbacks. You can use QueueInput instead which doesn't feed anything.\nsummary.add_moving_summary(self.y1_loss, self.y2_loss)\nI mentioned earlier about summaries. This will let hooked_sess.run to evaluated both loss every time, though only one of them will be used for training. This function allows you to use a different collection name which can avoid this issue.\nThese are only performance-related, though. Shouldn't affect the correctness of your code.. 1. you can have two threads for both data so they will not be blocked\n2. correct. QueueInput will avoid this.\n3. yes it will use a different collection name. then you'll need to get the ops from the collection and eval them together with the corresponding hooked_sess.run.\nSimplePredictBuilder simply calls your build_graph. So it should be fine. In fact the existing trainer.get_predictor should work for your case.. net1/fc1_out is the name scope of the fc layer. The tensor name is actually net1/fc_out/output if you print it. The documentation only vaguely mentions this. I'll add some more.\nAlso you have conflicting names in inputsdesc, so it's ambiguous when you do get_predictor. I should make it raise an error.. You can either try to formulate the problem to a single-cost optimization problem with tf.cond or tf.control_dependencies. If this is not possible, you can define the training step yourself, like the GAN trainer examples or the code above.. Thanks for pointing out! Could you try again?. Make sure no other processes (e.g. Xorg) are running on GPUs.\nTry different configuration, e.g. one mentioned in another issue: https://github.com/ppwwyyxx/tensorpack/issues/467#issuecomment-341592143\nAs a last resort, change config.py to produce fewer predictions. There is a comment about it. . The code was only written for two layers. Now it should work for one layer as well.. callbacks=[\n            HookToCallback(tfdbg.LocalCLIDebugHook()), .... Maybe colocate_gradients_with_ops is not a good option. I saw that tensorflow/benchmarks didn't set this option either, so I was considering removing it. . The option was kept unchanged because I found it still helps my vision models. But now you should be able to set it by trainer.COLOCATE_GRADIENTS_WITH_OPS=False, before building the graph.. It's supposed to be called by you. The first tutorial has more details about it.. It said cannot find git. @dongzhuoyao . If I'm not mistaken, the two block functions were equivalent to the ones in resnet_model.py  (preresnet_basicblock, preresnet_bottleneck). Better to import them than rewrite them if that is the case.. https://github.com/hongyi-zhang/mixup/ FYI I found the authors have released code here.. Differences include:\n1. how mixup is implemented (mix within 1 batch or between 2 batches)\n2. initialization\n3. input normalization. Tried the above code but found it cannot reach the paper's number either.. So maybe we can live with a slightly lower performance.. I can get same performance as the reference code, by using a different initializer for FC layer. Using the pytorch initializer somehow gave me worse performance starting from the first epoch and sometimes even NaN. I don't have time for now to investigate what's going on -- probably a very subtle issue. So I'll merge this one now. Thanks to @yselivonchyk and @dongzhuoyao for pointing out the bug!\nAlso I've removed the model code for other depth for simplicity. The same logic already exists in the resnet example so in this example it's better to just focus on mixup itself.. http://tensorpack.readthedocs.io/en/latest/tutorial/save-load.html\nand http://tensorpack.readthedocs.io/en/latest/tutorial/faq.html#how-to-freeze-some-variables-in-training\nhave everything you need to know to do this.. Run fw manually on the saved actual weights.. Tensorflow can only save variables. It cannot save a tensor which is computed from another variable. \nTo save binary weights you'll need to create a variable to each of the weights, and assign them the value of fw(actual weights) every epoch before saving the actual weights. Since it is not useful at all to save binary weights during training, I recommend you just do the post-processing after training.. ```python\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.eager as eager\neager.enable_eager_execution()\nbitW = 1\ndef fw(x):\n    if bitW == 32:\n        return x\n    if bitW == 1:   # BWN\n        E = tf.stop_gradient(tf.reduce_mean(tf.abs(x)))\n        return tf.sign(x / E) * E\n    x = tf.tanh(x)\n    x = x / tf.reduce_max(tf.abs(x)) * 0.5 + 0.5\n    return 2 * quantize(x, bitW) - 1\narr = np.random.rand(10, 10) - 0.5\nprint(arr, fw(arr))\n``.a.item()` is a dict.. > the decoded_boxes is defined on the feature_map which is 1/16.0 scale smaller than original image, \nNo. It's defined on the original image.. 1. fm_anchors is defined on original image\n2. The comment is talking about the shape of box_logits, and has nothing to do with its scale.. You can print the return value of get_all_anchors to confirm what it is.. And https://github.com/ppwwyyxx/tensorpack/blob/master/examples/FasterRCNN/train.py#L86 is still only about shape, not the scale.. The input image can be small and may not use all the anchors. So I slice the anchors.\nShape is the number of elements in a tensor. Scale is about the value of elements in a tensor. Changing the shape doesn't change the values. Slicing changes the shape. boxes = boxes / 16.0 changes the value.. > Slicing changes the shape. boxes = boxes / 16.0 changes the value.\nThe number of boxes on the image is (height/16 * width/16 * 15). This is SHAPE.\nIt has nothing to do with whether the coordinates (i.e. VALUES) are divided by 16 or not.. Yes. The anchor code is copied elsewhere which used a different definition of box.. tensorpack is written in python and you debug in the same way you debug python. I personally use import IPython as IP; IP.embed().\nNot a tensorpack issue. Closing.. yes. you can only break in python.. I use IPython as IP; IP.embed() to set break point.\nSometimes I use tf.Print to log.. Please follow the issue template to report problems.\nI think you didn't use a valid COCO annotation file.. There is no b if I print it. It may be due to some difference in python version or its json implementation.. After a thought I think it's due to how msgpack serializes/deserializes the data. What's you msgpack version and could you perhaps upgrade it?\n  . Turned out that msgpack always transforms strings to bytes. Should be fixed now.. You cannot. That's why it requires TF>=1.4. > Any chance to update the readme?\nThe readme is correct. Tensorpack depends on TF>=1.2; FasterRCNN depends on TF>=1.4. Both are mentioned in the corresponding readme.\n@sharpstill I'll look at that in the future at #463.. You can call slim or tf.layers in tensorpack.\nAs mentioned in the README, tensorpack is NOT a model wrapper.\nDuplicate of #236. Closing.. Also easy to implement by yourself (about 10 lines of code), e.g. in shufflenet:\nhttps://github.com/ppwwyyxx/tensorpack/blob/c266152797221365d5f9b7377c99423b8c1a1aca/examples/ShuffleNet/shufflenet.py#L29-L44. I don't know what you mean by \"the example tutorial\". All examples are in https://github.com/ppwwyyxx/tensorpack/tree/master/examples/ as you can see.. You can set channel_multiplier to 1.. >  the doment says output has in_channels * channel_multiplier channels\nSo if you set channel_multiplier to 1, output channel == input channel.. AFAIK tensorflow still does not use cudnn7 group conv.. It's your choice to make your models and I do not give guidance on that.. The document says \" a tower function is a callable that takes input tensors and adds one replicate of the model to the graph\". This is the definition.. It's the expected behavior that all process will give you randomly-sampled files from your list. Therefore they could be the same.. The document says: \n\nWhen nr_proc=1, the dataflow produces the same data as ds in the same order.\nWhen nr_proc>1, the dataflow produces the same distribution of data as ds if each sample from ds is i.i.d. (e.g. fully shuffled). You probably only want to use it for training.\n\n\"Having the same distribution\" implies that there may be duplications.\nI don't know what behavior you want, but you may be interested in MultiProcessMapData or MultiThreadMapData. \n  . https://github.com/rbgirshick/py-faster-rcnn/blob/96dc9f1dea3087474d6da5a98879072901ee9bf9/lib/fast_rcnn/config.py#L89\nNormalize the boxes.\nGiven the questions that're asked recently you don't seem to have enough knowledge on either tensorflow or faster rcnn to understand many of the code. But please I do not teach tensorflow or deep learning for unknown guys on github for fun. One or two questions are fine but there have been too many of these. I develop and maintain this library and hope github will be used for issues related to this library only.. Tensorpack is a pure-python library, so any segfault is unlikely to be a tensorpack problem.\nIt's probably an environment problem. If you could follow the issue template, and figure out which line in python does it segfault, maybe there are more clues I can tell. A stack trace doesn't provide much information.. Closing as there is no details about the issue.. This is ShuffleNet 0.5x (arch2) g=8 in the paper. Not table 1.. imagenet-resnet.py is on travis and it works.. The comment is unrelated. Your changes probably cause the ereor.. Oh this may be a bug (which rarely happens). Let me think about it.. Could you post your changes? I never saw this though I've trained it dozens of times.. ```diff\n--- i/examples/FasterRCNN/eval.py\n+++ w/examples/FasterRCNN/eval.py\n@@ -15,7 +15,7 @@ from pycocotools.cocoeval import COCOeval\n import pycocotools.mask as cocomask\nfrom coco import COCOMeta\n-from common import CustomResize\n+from common import CustomResize, clip_boxes\n import config\nDetectionResult = namedtuple(\n@@ -75,6 +75,7 @@ def detect_one_image(img, model_func):\n     scale = (resized_img.shape[0] * 1.0 / img.shape[0] + resized_img.shape[1] * 1.0 / img.shape[1]) / 2\n     boxes, probs, labels, *masks = model_func(resized_img)\n     boxes = boxes / scale\n+    boxes = clip_boxes(boxes, orig_shape)\n if masks:\n     # has mask\n\n``\nMaybe this will fix it.. OK. Maybe it's just the dataset difference that makes it happen.. You may have usedBatchData(remainder=False)` in evaluation?. I don't know what is the \"eval\" method and what's in there.\nAnd what do you mean by \"new batch of data\"? What data would be an old batch of data?. I see. It uses a new batch of data because it's a new sess.run call. \nWhat you want is to evaluate the tensor together with the training and you'll need to use the before/after_run method in callbacks.. before/after_run are just tf.train.SessionRunHooks. Refer to TF documentation: https://www.tensorflow.org/api_docs/python/tf/train/SessionRunHook. Are you sure were you setting per-gpu batch size or total batch size?. That's just my choice in the model and you can certainly use different strategies in your code.\n\nIf I want to train it on 4 GPUs, the max Batch size theoretically should be 4(1 per GPU), but the fact is even batch_size=4 or 3 or 2 is not ok\n\nIf what you're talking about is total batch size, how can you possibly train a total batch of 3 on 4 GPUs? I can't imagine how this can be implemented in tensorpack.. If you haven't read: http://tensorpack.readthedocs.io/en/latest/tutorial/trainer.html#multigpu-trainers. Essentially this is to transform two components with some parameters shared (shape) and some parameters not shared (interpolation). This is way beyond what's included and I recommend you process it by yourself.. Maybe \n``\ncallbacks=[\nRunOp(lambda: tf.add_check_numerics_ops(), run_before=False, run_as_trigger=False, run_step=True),\n ...].. Feel free to reopen if you still have questions.. Several combinations of opencv builds and tensorflow builds can often cause segfault and there has been several issues in TF about this.\nIn general I do not recommend using any of the pre-built opencv unless those that come with the OS distribution.\nNot a tensorpack issue. closing.. 1. It'll be in 1.5, or maybe 1.6, I'm not sure. https://github.com/tensorflow/tensorflow/commit/3a3b7530ebc9a6bfbfff8ed50bc7622a78b5b36b\n2. I don't understand what is the \"shape difference\" you're talking about. I don't have the context about what's going on in your code. But this sounds like a general tensorflow question to me. Are you talking about a potential bug in your code that you couldn't find or what?. FYI, FPN added in examples: https://github.com/ppwwyyxx/tensorpack/tree/master/examples/FasterRCNN. > why rpn loss is computed level-wise\nI'm not sure I understand what you're asking. What is the alternative if not level-wise? Each level has different image sizes, so I compute loss on each level separately.\nWhen the loss does not exist (no boxes at all) I don't want it to be shown as NaN. A placeholder can be set to any number and it will not affect training. It just makes the logs and tensorboard look nicer.. If the loss is accumulated over all the valid anchors, that sounds equivalent to what is in my code.. I expect people to use from tensorpack.dataflow import PrefetchData. The name prefetch is actually quite arbitrary, and recently was changed.. It was changed to parallel but I don't think you should use it either. You can from tensorpack.dataflow import PrefetchData. I don't know what do you mean by \"ignore\". Write your math down and convert it to tensorflow.\nYou might be interested in tf.gather,  tf.boolean_mask.. Again, look at tf.gather, tf.boolean_mask.\nThe documentation of tensorflow is very clear that tf.nn.sparse_softmax_cross_entropy_with_logits does not support -1.\nNot a tensorpack question. Closing.. http://tensorpack.readthedocs.io/en/latest/tutorial/save-load.html the tutorial has everything needed to do this. DictRestore allows you to load an arbitrary dict of values .. The default value for steps_per_epoch is dataset_train.size(). It does not know how you train the model (data parallel on multi GPU or not).\nThat's why you should not remove the line.\nAlso, http://tensorpack.readthedocs.io/en/latest/tutorial/trainer.html#multigpu-trainers. It depends on what do you mean by an \"epoch\", a \"step\", and how the input data is defined, etc, so there is not an answer to that.\nBut usually the default value will make one epoch larger than what most people would expect.. It's chain rule. Because r_o is a function of r_i.\n. https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/OgGd7sfu1bE has more discussion about this topic. Github issue of a deep learning library is not a proper place for discussing a paper, even though I happen to be one of the authors.. It's there.. I don't have such model.. This seems to be a opencv-tensorflow incompatibility. Certain prebuilt opencv & tensorflow don't work well together.. Try using import tensorflow as the first line in your program. Most of the times it solves the problem.. http://tensorpack.readthedocs.io/en/latest/tutorial/symbolic.html#use-models-outside-tensorpack. Looks like the shape should at least have known dimension. Could you check again with the above commit? I don't have TIMIT data with me now.. That looks like a data-processing error (Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]). I'll find the dataset and check it.. Training works fine for me. Looks like there is something wrong with your data processing: you got a extremely small feature and the feature has the same length for all audios. However audios are of different length so you should get different length of features.\nI got something like this:\npython\n(798, 39) (59,)\n(680, 39) (35,)\n(720, 39) (35,)\n(901, 39) (59,)\n(494, 39) (35,)\n(401, 39) (33,)\n(443, 39) (36,)\n(425, 39) (28,)\n(484, 39) (28,)\n(647, 39) (33,)\n(750, 39) (46,)\n(666, 39) (47,)\n(529, 39) (31,)\n(780, 39) (44,)\n(468, 39) (32,)\n(644, 39) (44,)\n(502, 39) (34,)\n(693, 39) (32,)\n(557, 39) (35,)\n(332, 39) (29,)\n(628, 39) (42,)\n(811, 39) (50,)\n(667, 39) (41,)\n(871, 39) (70,)\n(552, 39) (38,)\n(509, 39) (32,)\n(553, 39) (28,)\nPlease first check your *.wav files and make sure they are valid.. You used this dataflow instance: train_ds = BatchData(train_ds, batch, use_list=True) twice and that's not allowed for PrefetchDataZMQ.\nYou should either create two dataflow instances or not use PrefetchDataZMQ.. Most parallel dataflows do not support this, and even if some do, it may not work the same way you may expect. For example, MultiThreadPrefetchData can be used twice, but each data point from it will only go to one of the two but not both of them. In general it's best to not use it twice to avoid confusion.\n. Does SyncMultiGPUTrainerParameterServer work this way?\nI think this is a NCCL or tensorflow issue.. Looks like a NCCL bug: https://discuss.pytorch.org/t/nn-dataparallel-model-cuda-hangs/1204/9. The readme now looks better! I'll add that somewhere in the documentation as well. . There should not be \"ale\" inside env.\nA3C is based on gym and to get lives in gym you need to print(info).. Override the default argument of extra_callbacks.\nhttp://tensorpack.readthedocs.io/en/latest/modules/train.html#tensorpack.train.TrainConfig. It's taken care by this line: https://github.com/ppwwyyxx/tensorpack/blob/b2a396e5ddb8591c8bd7ff813a73d38ed271bd7e/tensorpack/graph_builder/model_desc.py#L142. Yes this looks like an IDE usage question. I have no knowledge of pycharm so I'm unable to help.\nI guess there is some place in pycharm you can set paths to some external libraries.\nAs a hack, you can always do sys.path.insert(0, '/path/to/cloned/tensorpack') to make import succeed.. This was probably fixed in #656, btw.. Indeed! I think it's a uint8 overflow. After the saturation changes, values could go out of 255.. Pushed a fix. Clipping was used in most other augmentors but somehow saturation was missing.. It is the reason. And it has already told you why it failed.\nYou can print the size of your dataflow by print(dataflow.size()).. http://tensorpack.readthedocs.io/en/latest/tutorial/performance-tuning.html. 5000 is the buffer size of the queue used by PrefetchData.. See tensorpack.readthedocs.io/en/latest/tutorial/performance-tuning.html.\n\nWe do not know why your training is slow\nIf you're going to open an issue about slow training, PLEASE do them and include your findings.. It's a typo. Should be ILSVRC12Files.\n\nTo know why your data is slow, do the benchmarks:\nhttp://tensorpack.readthedocs.io/en/latest/tutorial/performance-tuning.html#investigate-dataflow. > which is strange that, at first the speed is around100 it/s, then to 20% of reading, it quickly drop down to 3 it/s, and maintain 3-8 it/s to the end.\nThis suggests that beginning of the file is cached in memory. But memory may not be big enough to cache the whole file.\n3 it/s is still very slow unless that's the disk speed you've got.  8 it/s is more reasonable on an HDD. I recommend you do more benchmark here. See this paragraph in efficient dataflow tutorial: \n\nDepending on whether the OS has cached the file for you (and how large the RAM is), the above script can run at a speed of 10\\~130 it/s, roughly corresponding to 250MB\\~3.5GB/s bandwidth. You can test your cached and uncached disk read bandwidth with sudo hdparm -Tt /dev/sdX. As a reference, on Samsung SSD 850, the uncached speed is about 16it/s.\n\nAlso your CPU is probably too weak, you need some Xeon E5 series CPUs for complicated preprocessing pipeline. But you can always remove some augmentors to speed it up.. 150M/s upper bound your overall speed to 4-6it/s. And you seem to be also limited by CPU speed.\nIt does not assure all images are visited. The chunk is updated like a queue.. Closing due to inactivity. The conclusion seems to be slow hardware.. Install tensorpack by pip install -U git+https://github.com/ppwwyyxx/tensorpack.git.. Since it uses OpenCL I don't think it will work with tensorflow.. This is a Python issue. Python multiprocessing depends on pickle, and pickle has limitations on what type of objects it supports. You need to design your GameDecoder so that it doesn't include such objects.. Tensorpack is compatible with tf1.5. Upgrading tensorpack may help.\nTo report an unexpected problem, please follow the issue template.. It does not split a batch. Nor does training. Validation feeds 1 data points to each GPU, and update the progress bar by #GPUS. Training feeds 1 data point to each GPU and update the progress bar by 1.. Depend on how you write the code.\nIf you're talking about shufflenet example, it's in the code: \nhttps://github.com/ppwwyyxx/tensorpack/blob/c14d14961fee17116f062680624a04946fca7890/examples/ShuffleNet/shufflenet.py#L146-L147. For validation, 64 * 782 ~= 50000. Because no one can guarantee that 782 % #GPU == 0. \nFor training this is not a problem. For validation you want to evaluate on precisely the whole dataset.. For any unexpected problems, please follow the issue template.\nTENSORPACK_DATASET is the default path to store downloaded data.\nshuffle=False is correct to generate LMDB.. What's the problem just using RandomResize + CenterPaste + Crop ?\nAdding giant features is not a good idea because it's less likely to be useful to others.  . I still don't understand what exactly is the feature you're requesting.\nIt seems to be something like \"Pad to a certain shape only if the image is smaller, do something else when image is larger\". I don't think this is a general enough operation to be added to a lilbrary, because \"something else\" can have many different choices, unless you believe one of them is more reasonable than the rest.\nNevertheless if you need any extra augmentations you can implement it by subclassing ImageAugmentor.. Closing due to inactivity. Feel free to reopen if you want to discuss more.. Any of those values < 0 seem to be caused by floating point rounding.\nAnd since boxes are used for regression, I don't think it will be an issue. . Tensorflow requires you to run tfdebug inside a valid normal terminal. Pycharm is not.. From what I can see this is just how tfdbg works.. Mentioned in https://github.com/ppwwyyxx/tensorpack/blob/master/examples/FasterRCNN/NOTES.md.. You can compute a bounding box from segmentation mask.. > replace COCODetection.load_many in data.py by your own loader.\nIt can be any python code that loads data from any format. As long as in the end it returns the same data structure as documented:\nhttps://github.com/tensorpack/tensorpack/blob/d2c5cc162aaf15b4f9077fb8370ed4b98ee8b848/examples/FasterRCNN/data.py#L261-L279. My implementation follows the paper. I think you must have misread it somehow.\nThere are at least three different batch size in a fasterrcnn.. Yes.. Of course. In addition to training on your own dataset, If you also want to evaluate on your dataset, you need to have data loader for your evaluation set and implement the evaluation metrics.. I'm not familiar with other implementations except for Detectron so I don't know what to answer.\nIf you're talking about matterport implementation, it has a lot of differences from the original paper, some of which are mentioned in their readme. Given all those differences it's hard to say what contribute to the suboptimal performance.. Would be nice to train on other datasets. But I don't have time to do this in the near future.. It's indeed much faster than matterport's one for standard models, and slightly faster than the one in this repo as well.. I plan to add such things to callbacks, but right now you can just write the above expression in the graph. You can get the global step with tf.train.get_or_create_global_step().. As I said, the easiest way to do it now is to do it in the graph, with tf.train.get_or_create_global_step(). You can also write a new callback to do this. Hacking with the existing one will not work.\n\nthe summary is also updated epoch based, can we modify it to global_step based?\n\nIt can and is mentioned in docs. http://tensorpack.readthedocs.io/en/latest/tutorial/summary.html. What you asked for in the original issue is just one line of code in the graph.\nhttps://github.com/ppwwyyxx/tensorpack/blob/9bbdf94da8be9976725fb131bfb5c9f5523b25ac/examples/mnist-convnet.py#L86-L90\nget_global_step_var is just a wrapped call to tf.train.get_or_create_global_step.. ```diff\ndiff --git i/examples/mnist-convnet.py w/examples/mnist-convnet.py\nindex 9459533..c94ed57 100755\n--- i/examples/mnist-convnet.py\n+++ w/examples/mnist-convnet.py\n@@ -90,7 +90,7 @@ class Model(ModelDesc):\n             decay_rate=0.3, staircase=True, name='learning_rate')\n         # This will also put the summary in tensorboard, stat.json and print in terminal\n         # but this time without moving average\n-        tf.summary.scalar('lr', lr)\n+        tf.summary.scalar('lr', lr, collections=['NEW_COLL'])\n         return tf.train.AdamOptimizer(lr)\n@@ -116,6 +116,10 @@ def get_config():\n             InferenceRunner(    # run inference(for validation) after every epoch\n                 dataset_test,   # the DataFlow instance used for validation\n                 ScalarStats(['cross_entropy_loss', 'accuracy'])),\n+            MergeAllSummaries(period=1, key='NEW_COLL')\n+        ],\n+        monitors=DEFAULT_MONITORS() + [\n+            ScalarPrinter(enable_step=True, enable_epoch=False)\n         ],\n         steps_per_epoch=steps_per_epoch,\n         max_epoch=100,\n``\nThis makes mnist example print learning rate every step.\nAnd you can also just simply usetf.Print` in the graph.. Yes. \ntf.train.piecewise_constant, tf.train.exponential_decay can be useful but you can just write any symbolic function to compute your lr.. Total loss didn't show up because I forgot to add its summary when no regularizer is given. Will add it.\nLoss not decreasing is not a tensorpack issue unless you believe there is a tensorpack bug causing this.. > readability and documentation.\nI expect people to read tf.layers documentation instead of mine. I could copy docs from there. \nI could also add a new argument list plus **kwargs, i.e.:\nConv2D(inputs, filters, kernel_size, strides=None, ........., bias_constraint=None, **kwargs).\n\nthe principle of least astonishment.\n\nHow does this astonish a user who doesn't go and read source code?\nThe click library seems to only work for command line args if I'm not mistaken. Also what I need to do is not only rename.. Decorator looks good. I'll give it a try.\nTensorflow promises to not do breaking change so using it is fine. It's their job to not break things and I'd rather not care about it.. You are responsible to call it, as mentioned in the dataflow tutorial.. It depends on how you want to \"restart\".\nIf you want to load an old model you need to pass it to session_init. Otherwise the model will be initialized from scratch.. Please first consider how this can be done in pure tensorflow -- if it's possible then it's usually easy to make it work in tensorpack.\nSlots depends on minimize call, minimize depends on knowing all the variables in the models, which requires that the model was built already. Therefore you cannot use slots when you're building the model. They way you're approaching seems impossible in tensorflow, so maybe you need to come up with something else first.. Everything is based on tensorflow so you'd better forget tensorpack and come up with a tensorflow-way of doing it first. \nI don't know the paper and not sure what exactly you want to do, so I don't know whether tfutils.optimizer.PostProcessOptimizer will help or not.. In inference it calls build_graph with is_training=False. So you can use it to build a different model.\nbuild_graph is defined in imagenet_utils.py which calls get_logits.. Depend on what trainer and training interface you use there are many different places.\nStarting point: https://github.com/ppwwyyxx/tensorpack/blob/54c5a42db3c33df7310dc80106dd123e4e04d1b4/tensorpack/train/interface.py#L92. https://github.com/ppwwyyxx/tensorpack/blob/54c5a42db3c33df7310dc80106dd123e4e04d1b4/tensorpack/callbacks/inference_runner.py#L135-L140\ntrainer.tower_func is what you set in trainer.setup_graph(). I don't know what you're talking about.\ntrainer.tower_func is what you set in trainer.setup_graph(), which you've called in interface.py.. Should it be reported to tensorflow instead?. A separate and simpler file would be much better. . I would use:\nEnableCallbackIf(\nGraphProfiler(dump_tracing=True, dump_event=True), \nlambda self: self.trainer.global_step > 10 and self.trainer.global_step < 20)\nhttp://tensorpack.readthedocs.io/en/latest/modules/callbacks.html#tensorpack.callbacks.GraphProfiler. ```python\nclass A(Callback):\n    def before_run(self, ):\n        self._start = time.time()\ndef _after_run(self, _, _):\n    self.trainer.monitors.put_scalar('step_time', time.time() - self._start)\n\nMaybe this?. It's using shutil.rmtree already.\nIf files are being opened (e.g. by tensorboard) you'll also see this error. I saw this error quite often actually.. I'm not quite following. Why do you umount it? Do you need to mount it back so you can use it?. I would like fractional values as well, but queue does not have an API to get its capacity.. Yes they've been smoothed first inside the graph.\nThey idea was that summarizing all scalars in every iteration may have overhead (or maybe not), meanwhile summarizing once a while has variance. So they were smoothed before getting into tensorboard. . It is.\nJust write a different dataflow to lmdb.. No plan.. Added in examples: https://github.com/ppwwyyxx/tensorpack/tree/master/examples/FasterRCNN. I don't understand why you see 6. How did you log total_cost? If you did nothing special about it the default interval is every epoch.. If I ran mnist-convnet.py in examples, all step number are multiples of 468.. To adjust flush frequency:\nmonitors=[TFEventWriter(flush_secs=1), JSONWriter(), ScalarPrinter()]\n. http://tensorpack.readthedocs.io/en/latest/tutorial/save-load.html has everything about this topic.\nThree ways you can do this:\n1. rename variables in your graph\n2. SaverRestore supports the argument to ignore variables\n3. convert the model to a dict and remove some variables from it.. Do you understand the fact that:\nwith some augmentation, a dataflow could generate infinite number of different images. I have..\n imgaug.RandomOrderAug(\n                [imgaug.BrightnessScale((0.6, 1.4), clip=False),\n                 imgaug.Contrast((0.6, 1.4), clip=False),\n                 imgaug.Saturation(0.4, rgb=False),\n                 # rgb-bgr conversion for the constants copied from fb.resnet.torch\n                 imgaug.Lighting(0.1,\n                                 eigval=np.asarray(\n                                     [0.2175, 0.0188, 0.0045][::-1]) * 255.0,\n                                 eigvec=np.array(\n                                     [[-0.5675, 0.7192, 0.4009],\n                                      [-0.5808, -0.0045, -0.8140],\n                                      [-0.5836, -0.6948, 0.4203]],\n                                     dtype='float32')[::-1, ::-1]\n                                 )]),\nremoving some of them doesn't hurt much.. In the graph. Yes it is on GPU.. You can try.. `ProcessTensors` uses `_before/after_run` rather than `_trigger`. You need to use `PeriodicRunHooks` to schedule it.. That said, it seems possible to integrate the two and have a single `PeriodicCallback` that works for every method. Marking this as a potential feature.. You can use RunOp as well. There is a difference though -- RunOp is an extra `session.run` call , that's why it uses `_trigger` instead of `_before_run`. If your op only operates on weights, an extra call doesn't hurt anyway.. Tensorpack cannot be imported after pytorch, for now. It will be fixed after I upgrade pyarrow.\nRelevant commit in pyarrow is: https://github.com/apache/arrow/commit/ff28c7647c1910f1a0d1d97b8ba68b2b554e5ce1. I mean, you should be able to import it before pytorch.. It was correct. The goal is to hide torch so that pyarrow cannot see it.. I see. I should deal with torch outside of the try block.. @yaroslavvb do you know what https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/memory_optimizer.cc is doing? Is it also doing recomputation in backprop?. Before figuring out how to make IDE happy, please note that `from tensorpack.graph_builder.model_desc import ModelDesc` is not guaranteed to work in the future. The official name for this class, as written in the documentation, is [graph_builder.ModelDesc](http://tensorpack.readthedocs.io/en/latest/modules/graph_builder.html#tensorpack.graph_builder.ModelDesc), and it's better to import it with `from tensorpack.graph_builder import ModelDesc`.. So I tried [jedi](https://github.com/davidhalter/jedi). In my tests, putting names of objects into `__all__` explicitly doesn't help it to find out where the objects are defined, and as a result it cannot autocomplete the docs & methods of the objects. Seems like it has to import everything line by line in all `__init__` file to make autocompleter happy.. These are two separate issues here:\n1. The dynamic import in `__init__.py`  under each subfolder. Now it simply does `import *` of each file under the folder. I think it's OK to replace it with a bunch of explicit imports:python\nfrom .model_desc import \nfrom .training import \nfrom .distributed import \nfrom .predict import \nfrom .utils import *\n```\n\nThe top-level __init__.py conditions on HAS_TF. The static hack seems fine. Does this hack work with pycharm & sublime?. For 1, I found that __all__ is necessary for sphinx to work. As a result, the current code (dynamically populate __all__) has to be kept. \nHowever, the static hack can probably tell parser how to find each name. \nI found that adding\npython\nif False:\n    from .model_desc import *\n    from .training import *\n    from .distributed import *\n    from .predict import *\n    from .utils import *\nis enough for jedi to autocomplete everything. So I'll push this first.. Pushed the static hacks.\nBasically all methods are hacks because no IDEs can be smart enough to understand python. I can push some of these hacks to perhaps make things better. But I don't know whether it would actually work for people with different IDEs. \nNow I assume a static analyzer can at least figure out all the dynamic imports. If people are interested, please try them out and make suggestions on improving it (because I don't really know how to hack these IDEs).. Can this be done by just using standard python library rather than a 3rd-party one?. The paper has many settings. This is just one of them which I got from the authors. You can set it to whatever you want.. I don't have any other results.. That's impossible from a design perspective. All the modules are independent of each other. \n\nJson loader continues the old epoch number so that new data can be appended to the json file, and this has nothing to do with the rest of the world. Maybe the message can be changed a bit to avoid confusion. And frankly I'm not a big fan of this epoch-loading feature since it has caused other troubles in the past.. What do you mean by \"checkpoint names\"? Do you mean the logging directory name?\nYou can use logger.set_logger_dir to set directory name.. You can use logger.set_logger_dir, perhaps with a command line argument to do this.\nInteracting with users is usually a bad idea. Despite all the other trouble it could cause, it assumes the existence of a user in front of the terminal, which isn't necessarily true at the first place.. Yes in this case it's already interacting with users, which I don't like. I actually have never used \"backup\" or \"new\" myself. They turn out to be bad ideas. I usually used set_logger_dir and mentally made sure the name doesn't conflict. If I were to rewrite it I would just throw an error when names conflict.\nThe message was changed to \"Rename the first epoch of this training to epoch #{}.\".format(epoch)\". Hope this won't give users an impression that the checkpoint will be loaded.\nThe epoch number is by design not supposed to be changed dynamically. The current way of changing it is a bit hacky. And because of this change, callbacks that run before and after the JSONWriter will see different epoch numbers which is troublesome. Also this overwrites the starting_epoch even if users have manually set it, which is also undesirable.. To resume training, if you load the last checkpoint and select \"keep\", everything will appear as if the training was never interrupted.. > everything will appear as if the training was never interrupted.\nNot exactly.. because how hacky the epoch number is restored, the new epoch number is not visible to hyperparametersetter. Therefore if the learning rate is scheduled by epoch, it will be wrong after restore.\nJust one more reason for me to remove the feature... or at least do it differently. Please point people first to docs, not code.\nThere is a name argument in docs: http://tensorpack.readthedocs.io/en/latest/modules/models.html?highlight=conv2d#tensorpack.models.Conv2D\nSo there should be no confusion.. PeriodicTrigger(ModelSaver(...), every_k_epochs=10)\nThe meaning of keep_checkpoint_every_n_hours is in tensorflow document.\n. You can use every_k_steps as well.\nhttp://tensorpack.readthedocs.io/en/latest/modules/callbacks.html?highlight=periodictrigger#tensorpack.callbacks.PeriodicTrigger. Think of the 4 tensors as 4 independent tensors that happen to have similar names. \nThey are not necessarily concat-able at all. \nYou can implement this feature by writing a callback, either fetching all tensors to numpy and concat them there or create a concatenated tensor in the graph and fetch it.. DumpTensors callback runs in every training step so I don't see why it makes sense to evaluate inference tensors in it. That's up to you, though.\nThe tensor name is supposed to end with \"output:0\". Getting \"FusedBatchnorm:0\" is a bug. I'll fix it soon.\n\ndo you store the average of all the model tensors across the towers?\n\nNo. And as I said tensors on different towers, though have similar names, are not necessarily of the same shape at all.. Using self.trainer.get_predictor() in a callback is the easiest way to do it. See http://tensorpack.readthedocs.io/en/latest/tutorial/inference.html.\nYou can also write:\n```python\nclass A(Inferencer):\n    def _get_fetches(self):\n        return ['conv0/output:0']\ndef _on_fetches(self, outputs):\n    print(outputs)\n\n``. Read the tutorial here: http://tensorpack.readthedocs.io/en/latest/tutorial/faq.html#how-to-freeze-some-variables-in-training. What do you mean by \"very beginning\"?. It depends on what do you mean by \"set variables to be non-trainable\".tf.get_variable(trainable=True)` does nothing but adds the variable to a collection.  If what you want is to get the variable out of the collection, either modify the collection manually, or surround any layers with a variable scope with a custom getter. These have nothing to do with tensorpack.. > Tensorpack's builtin trainer minimizes variables in the collection \"TRAINABLE_VARIABLES\", doesn't it? \nIt does.\n\nCan I modify the variable collection to minimize without re-writing the trainer?\n\nYou can. As I said you can edit the collection or use a custom getter for the variable scope so that the variables never gets into the collection.\nCheck tensorflow docs on tf.get_collection_ref, tf.variable_scope for the usage.\n. Above commit adds \"skip_collection\" support in freeze_variables. \nThough we provide such functions, in general how to build the layers is not the job of tensorpack. If you need more control over what and how variables are freezed, it's better to learn the usage of tensorflow custom_getter and implement something similar to freeze_variables by yourself.. For unexpected problems, please post issues following the issue template. In particular, include what you did (changes you made, command you run) and what you observed (logs), and how you installed tensorpack.. atexit only get executed when the process exits in a clean way.. How?. Cannot catch sigkill.. If prctl solves the problem for Linux I'll use it. You're welcomed to add better support for windows if you like.\n\nAnother really clever method is the following:\n\nThis assumes the child is still able to run code when parent dies, which is not necessarily true. The child can be stuck in IO.. For example when parent dies and a child is waiting data from the parent.. It could be waiting data from a queue, from a named pipe, from shared memory, etc.. Oh there was a similar issue before. This doesn't affect training. What happens is at some iteration there could be no valid boxes so box loss became NaN. Then the moving average stays NaN after that.. A trivial fix would be to check for empty boxes, e.g. https://github.com/ppwwyyxx/tensorpack/commit/7f55d502aa181073f6ca0b9f2d4131c2c87369c7#diff-56a46708ffaa79088f015119c942d470 . I didn't think about it thoroughly, but very likely you provide some training data without valid foreground boxes, causing NaN. However this was assured by the current data processing code. I think I'll just add an assertion somewhere.. I thought huber loss returns NaN on emtpy tensors. Turned out that it seems to return 0.\nAnother reason could be that you provide some boxes that have zero area. As a result encode_bbox_target will return a tensor with NaN values.. Closing due to lack of activity. Feel free to reopen if you have follow-ups.. There was an issue about it #542. The short answer is \"this is how tensorflow works\".\nI don't want to explain it more because it's complicated and you need to read tensorflow source code to figure out everything.. Oh but one thing you're correct: tensorflow pads zeros on borders, which makes my implementation of roi_align also wrong. But it's only wrong on the borders, so it's not a big deal. Usually boxes are not on the borders.\nI put some proof-of-concept snippet in the code for people to verify the problem of tf.image.crop_and_resize.\n```python\n    import numpy as np\n    import tensorflow.contrib.eager as tfe\n    tfe.enable_eager_execution()\n# want to crop 2x2 out of a 5x5 image, and resize to 4x4\nimage = np.arange(25).astype('float32').reshape(5, 5)\nboxes = np.asarray([[1, 1, 3, 3]], dtype='float32')\ntarget = 4\n\nprint(crop_and_resize(\n    image[None, None, :, :], boxes, [0], target)[0][0])\n\"\"\"\nExpected values:\n4.5 5 5.5 6\n7 7.5 8 8.5\n9.5 10 10.5 11\n12 12.5 13 13.5\nOur implementation is not perfect either. When boxes are on the border of\nimages, TF pads zeros instead of border values. But this rarely happens so it's fine.\n\nYou cannot easily get the above results with tf.image.crop_and_resize.\nTry out yourself here:\n\"\"\"\nprint(tf.image.crop_and_resize(\n    image[None, :, :, None],\n    np.asarray([[1, 1, 2, 2]]) / 4.0, [0], [target, target])[0][:, :, 0])\n\n```. UPDATE: now our implementation of roialign is correct on borders as well.. As mentioned above I do not want to comment more. I cannot understand your question because terms like \"an origin wrt the current cell\", \"relative cell from the original position\" are not mathematically defined and I do not know what they mean. This is why the issue is complicated: I do not believe it can be explained well with plain words.\nThe source code is CropAndResizePerBox.. Opened an issue in tensorflow (https://github.com/tensorflow/tensorflow/issues/26278) since there is a chance it gets fixed in TF 2.. 1. You're using a very old tensorpack. Please upgrade and try again.\n2. Tensorflow does not have a native fast distributed trainer. Only horovod trainer can get a reasonable speedup.. The above commit should fix the problem. Another way to fix the problem is to upgrade your tensorflow to 1.5. For speed, read tutorial: http://tensorpack.readthedocs.io/en/latest/tutorial/performance-tuning.html\nFor accuracy: I don't know what happened -- any line of code that's written wrong could cause it. If you can post more information following the issue template maybe I'll be able to tell more. . > The codes are copyed from your tutorials and shown as above.\nThen you're using a different batch size compared to the original code.. It's better to paste the changes (in the form of diff, for example) you made instead of describing them. As you can see from the comment above it's not easy to describe them accurately.\nIn addition to that there are a lot of other issues that may matter. What's the version of tensorflow? How do you install tensorpack? For the two runs you mentioned originally, are they on the same system with consistent software stack?. I don't know what happened. This is not an issue I can reproduce, but the code looks fine. Maybe check the lmdb file, something may went wrong when you generate it. For example if it's smaller than 140G then it's incomplete.. I read raw jpegs. But reading from LMDB will have similar performance.\nThe numbers in the first table does not use preact.. Cannot reproduce. Please update. I can not. Maybe upgrade/reinstall msgpack?. The code successfully exits. I use the latest version of msgpack & msgpack-numpy. Maybe also try upgrading pyzmq.. https://github.com/msgpack/msgpack-python#pypi-package-name\nMaybe related.. I can reproduce the issue in docker. Looks like I'm using zmq socket in a wrong way where messages can get lost if the sockets aren't created with a desired order. . are you using CPUs?. Oh you're using improved-wgan, which requires 2nd order gradient. \n2nd order gradient for fused batch norm is only supported since TF 1.4: https://github.com/tensorflow/tensorflow/commit/4f3b13cc650ec08a246c16d31f9a10cf5bb96d65#diff-09d0717441656db7d7338b7febc2251a\nI'll add a note in the code.\nClosing as this is not a tensorpack issue.. Because you set the output node name to the variable, only the variable will be left.. The name would depend how keras implement the layer so I don't know -- and it may change across versions. Look at the graph or print the nodes to find it out.. That's why I would recommend you to build the inference graph by yourself. See http://tensorpack.readthedocs.io/en/latest/tutorial/inference.html#inference-after-training . If you want to do something during training, see above comments.\nIf you want to do something after training, see http://tensorpack.readthedocs.io/en/latest/tutorial/inference.html.. To do inference during training, https://github.com/ppwwyyxx/tensorpack/blob/26d792d490cea55855f10e9f02da738726eb664b/examples/GAN/CycleGAN.py#L185-L201.\nTo do inference after training, https://github.com/ppwwyyxx/tensorpack/blob/26d792d490cea55855f10e9f02da738726eb664b/examples/HED/hed.py#L217-L228. ls-checkpoint list the variables in the checkpoint. The output of the layer is a tensor, not a variable.\n. Thanks a lot!. Upgrade msgpack. msgpack unpackb has the argument 'raw': https://github.com/msgpack/msgpack-python/commit/5569a4efcdc913d343eaff4e55c9b19fafde4268#diff-82bf3c7d2bf3fc852729c754ffa93870 \nMaybe you have multiple msgpack installed in different places.. Maybe you also need to uninstall msgpack-python first. See https://github.com/msgpack/msgpack-python#pypi-package-name. For speed, read http://tensorpack.readthedocs.io/en/latest/tutorial/performance-tuning.html.\nAlso note that the speed of first 50 iterations are not reliable.. Please be more clear about what you've done. What is 5.99 and 0.53it/s? Are they the speed of data or the speed of training? By saying you \"set\" a line of code, what exactly is happening to your code?. Anyway, since fake data is faster than true data, your hardware is probably not fast enough to run the given dataflow and the tutorials have the information on how to benchmark and improve dataflow.. Please report issues following the template in your issue.. \nIf this is the number you're referring to, it's a completely unrelated thing.. Those are two different network architectures.. BatchData batch 256 data points together so each iteration takes 256x more time.\nWhat you need to worry about is which step before it takes the most time. Follow the steps in the instructions: http://tensorpack.readthedocs.io/en/latest/tutorial/performance-tuning.html#investigate-dataflow. Some examples to benchmark:\npython\nds = LMDBData('/path/to/ILSVRC-train.lmdb', shuffle=False)\nds = BatchData(ds, 256, use_list=True)\npython\nds = LMDBData(db, shuffle=False)\nds = LocallyShuffleData(ds, 50000)\nds = PrefetchData(ds, 5000, 1)\nds = LMDBDataPoint(ds)\nds = PrefetchDataZMQ(ds, 25) # set to your cpu count\nds = BatchData(ds, 256, use_list=True)\npython\nds = LMDBData(db, shuffle=False)\nds = LocallyShuffleData(ds, 50000)\nds = PrefetchData(ds, 5000, 1)\nds = LMDBDataPoint(ds)\nds = MapDataComponent(ds, lambda x: cv2.imdecode(x, cv2.IMREAD_COLOR), 0)\nds = AugmentImageComponent(ds, [imgaug.Resize(224)])\nds = PrefetchDataZMQ(ds, 25)\nds = BatchData(ds, 256). This is measuring disk speed and it means you can read 2.97*256 images per second from disk into memory. It's up to you whether you'll call it a bottleneck.\nIt's significantly faster than the whole pipeline so at least it's not \"blocking\" the rest.. Oh I found E5-1620 v4 only has 4 cores? This CPU definitely cannot do all the preprocessing faster than 2.97*256 im/s.. Then do fewer preprocessing steps.\nhttps://github.com/ppwwyyxx/tensorpack/blob/9972b15036c851ce82e63721cfe0905e68115475/examples/ResNet/imagenet_utils.py#L61-L74\nMay slightly affect accuracy.. A typical server machine in my experience has 2 CPUs each with 10 cores or more. . ChainInit([SaverRestore(path1), SaverRestore(path2)]). This seems to be used by the authors: https://github.com/hongyi-zhang/mixup/blob/master/cifar/easy_mixup.py#L45-L50\nShouldn't be a big deal anyway, I assume. An easy change is to use CenterPaste(background_filler=ConstantBackgroundFiller(128)).. The script was not expected to work for other checkpoints, but you're right this is the solution.. https://tensorpack.readthedocs.io/modules/tfutils.html#tensorpack.tfutils.varmanip.save_chkpt_vars\npython\nsave_chkpt_vars(np.load(\"a.npz\"), \"a.ckpt\"). Unfortunately 925MB is just not enough.\nFrom the log, setting the environment variable TF_CUDNN_USE_AUTOTUNE=0 may help.. Feel free to reopen if you still have questions.. Protobuf compiler is required to use imagenet data.. After the above commit, protobuf will be required only if you call http://tensorpack.readthedocs.io/en/latest/modules/dataflow.dataset.html#tensorpack.dataflow.dataset.ILSVRCMeta.get_per_pixel_mean .\nAs a result running shufflenet will not require protobuf.. Please paste what you did and more logs following the issue template.. To load data in custom format, you need to write a dataflow for that format:\nhttp://tensorpack.readthedocs.io/tutorial/extend/dataflow.html. Similar to #681 you may have more than one versions installed in different places (by conda or pip or system).\nmsgpack master still has the raw option: https://github.com/msgpack/msgpack-python/blob/master/msgpack/_unpacker.pyx#L161-L169. You can load a npz  by np.load. \nAfter that, how to create a tf checkpoint is not a tensorpack question, and I don't know the answer either.\nAt least you can manually create variables in the graph, load the values and use tf saver to save the checkpoint.. It uses tf.nn.in_top_k which handles ties as positive: https://github.com/tensorflow/tensorflow/issues/10767. You can set it to whatever you want.. Then you'll also need to change the learning rate schedule. Alternatively you can use the internal_update option.. BatchNorm has the training option: http://tensorpack.readthedocs.io/en/latest/modules/models.html#tensorpack.models.BatchNorm\nIt is default to ctx.is_training but you can change it.. It would not work and you're supposed to see an error.\nYou can use a different argscope around each Conv2D. But it's better to not use BNReLU or reimplement BNReLU if you have complicated use cases.. > with argscope([Conv2D, BatchNorm], is_training=False)\nThis is not how argscope works. Conv2D and BatchNorm do not accept the is_training argument.. argscope(BatchNorm, training=False). It will not. Conv2D does not accept the training argument.\nwith argscope(BatchNorm, training=False):\n      Conv2D(...,nl=BNRelu)\nwith argscope(BatchNorm, training=True):\n      Conv2D(...,nl=BNRelu). Then you're using an old version of tensorpack.. To be honest, there is no benefit in using tensorpack layers if you don't use tensorpack trainers. It only causes more trouble with a set of strange interface.\nThe training option used to be named use_local_stat, but recently it was renamed to be consistent with official tf.layers.. It will work only if you did not use tensorpack trainers to train the model.. We don't know. DoReFa-Net was designed mainly for ultra-low bit (<8). We haven't done many experiments on 8 bits or more.. > But I believe it also works for 8-8-32 right? \nYes\n\nBy the way, dorefa will generate some checkpoints, however, how can I resume training from the checkpoints? Thanks,\n\nhttps://github.com/ppwwyyxx/tensorpack/blob/550988138574d903798ab47ee82202fff8ac3c03/examples/DoReFa-Net/alexnet-dorefa.py#L265-L266. All sources were checked by flake8. I think that's enough.. 1. It does not.\n2. It is in the repo:\n./tox.ini\n./examples/tox.ini. 1. import_meta_graph and saver.restore is not the best tool for reading a TF checkpoint. You can use tf.train.NewCheckpointReader instead.\n2. To use import_meta_graph you'll need the clear_devices option.\nI'm closing it since this is a question unrelated to tensorpack. You can follow up if you still have questions.. What's saved in the checkpoint is W, not W_b. Duplicate of #573.\nrms is root-mean-square. (a * x + b) \\dot (c * y + d) = ac (x \\dot y) + a * d * sum(x) + b * c * sum(y) + bd. Also see https://github.com/ppwwyyxx/tensorpack/issues/27#issuecomment-255015193\n. No.. You're using a (old) version of tensorpack that doesn't match the version of examples.. 1. I don't know. Maybe I'll be able to tell more if you give more information, such as those in the issue template.\n2. Save and Load models. @PatWie . For 4x: I think that's intended. 128->512 is superresolution. 512->2048 is also superresolution. The latter one is used.\nThe original code is here (but it cannot run anymore because it should have copied GAN.py). The model there is still downloadable. But it's not compatible with code in tensorpack examples.\nI changed the code a bit to make the results slightly better in my experiments, but I never succeeded in getting something as good as the demo image. So bad results you're getting is not a surprise to me.. I think a non-appendable filesystem would be a potential problem for a lot of other things anyway. Could you just use logdir option so that event file won't be on GCS?\nSplitting event files looks very hacky. I don't think Supervisor is doing anything like that. Could you point where?. OK. Just to clarify what you're trying to achieve: how are people expected to use this feature? Setting logdir to somewhere on gcs?\nIf yes, what happened when you set logdir to gcs without this feature?. If I understand it correctly, the issue is that each flush creates a new copy of the entire file due to versioning, right?\nIt looks like it's just because versioning doesn't make sense if you're appending to files. What if you turn versioning off?. OK. When versioning is turned off, each flush recreates the entire file (i.e., takes unnecessary extra time but no extra space), is this correct?\nAlso, tf.summary.FileWriter itself will internally flush the file once a while. Will that also be a problem for you?. I see. Make sense! \nI think \"rotate\" means \"deleting old ones while adding new ones\" which is not really what's going on here. Maybe rename to \"split_files\" or \"split_events\" ?. The error message is a bit confusing, but it's saying you didn't install lmdb.. Advice would be to install lmdb.\npip install lmdb.. The ternarynet project used very old tensorpack and tensorflow that are not compatible with what they are now. It has actually included a version of tensorpack in its repo. It's not expected to work with latest tensorpack.. FYI I've included the implementation of Trained Ternary Quantization paper inside tensorpack examples now: https://github.com/ppwwyyxx/tensorpack/tree/master/examples/DoReFa-Net. It's mentioned in the readme that: \n\nWe're not planning to release our C++ runtime for bit-operations. In this repo, bit operations are performed through tf.float32.\n. http://tensorpack.readthedocs.io/en/latest/modules/callbacks.html#tensorpack.callbacks.Callback.chief_only. Adding a new argument which most people won't care for ALL callbacks doesn't sound like a good idea to me -- it requires extra coding effort for every new callback to be written.. http://tensorpack.readthedocs.io/tutorial/trainer.html. You inputs have zero channel. How are you supposed to do batch norm.. Please report issues following the issue template.. Closing because no issue is reported.\nMost of the features in this library support windows.. The training wasn't meant to run on osx at the beginning.\nAn easy fix would be to use different types of sockets conditioned on the OS.. If your dataflow is not randomized (either explicitly or implicitly by paralleism), and has the correct size, then both methods should give you the same error rate.. It means after dataflow.size() datapoints, the dataflow.get_data() method reaches the end and stops producing more datapoints.. What does your validation dataflow look like?. Not sure what is going on in your case. \nAfter training the imagenet-resnet examples I can get exactly the same validation number using offline evaluation, if evaluated in the same environment (same software & hardware). The example is using image files instead of LMDB, but I don't think this will lead to issues.\n\nYou can also check whether there is any environment difference or preprocessing difference.. Another method I used to test similar issues before was to use two DataParallelInferenceRunner callbacks -- they should not use the same dataflow, but use two dataflows created in the same way. And will produce the same error rate. This can rule out certain types of problems with data or the model.. It's an implementation detail that is not visible to users at all. An user can use either NCHW or channels_first.. Adding the iteration number sounds OK. But this makes it harder to track (and overwrite) the last minsaver checkpoint, if you resume a training.. Looks like MinSaver simply can not work with resume now.\nThe fundamental problem is that callbacks like MinSaver, JSONWriter are stateful. Therefore once the training was interrupted the state is lost. The way JSONWriter tries to recover its state when resuming is already kind of hacky. I will think if there is a better way to support stateful callbacks.. PrefetchData probably does not support windows either. You can just remove it and expect a minor slow down in speed.. You need to remove GPUUtilizationTracker from the callbacks. Looks like it does not support windows either.. Regarding the original issue, turned out that PrefetchData does support windows, but windows has a more strict picklability requirement for processes, i.e. it requires to pickle get_train_dataflow.preprocess. Ref: https://docs.python.org/3/library/multiprocessing.html#the-spawn-and-forkserver-start-methods. What you want to do is up to your own choice. Tensorpack allows you to do either one.\nNot a tensorpack question - closing -- unless you already know what you want to do but don't know how to do it in tensorpack.. 1. Tensorpack is a training interface agnostic of symbolic models. You can use whatever models - so different data types are already supported by design.\n\n\ntensorpack.models is a small set of model implementation that exists mainly for historical reasons -- so don't expect any feature update there. You can use whatever other model implementation, such as tf.layers.\n\n\ntensorpack.models uses tf.layers for many implementations already -- as a result it supports some other data types. I don't know what types tf.layers supports, but it supports fp16 at least. To use tf.layers with fp16 weights, just give it fp16 inputs. I've trained a lot of fp16 models with tensorpack.\n\n\nSome utilities in tensorpack may have some false assumption on data types. I've fixed it for regularize_cost function last week. If you need to use it you might need to update.. @Skylion007 I think that's a reasonable thing to do. I just hope to review those args and understand what they do before supporting them. I added this as a TODO in #627 . Please use latest tensorpack.. You give:\nFile \"tensorpack/examples/FasterRCNN/train.py\", line 354, in \noutput_names=get_model_output_names()))\nBut in either master or 0.8.5, line 354 in train.py is not this:\nhttps://github.com/ppwwyyxx/tensorpack/blob/master/examples/FasterRCNN/train.py#L354\nhttps://github.com/ppwwyyxx/tensorpack/blob/0.8.5/examples/FasterRCNN/train.py#L354\n\n\nCould you make sure you're using either master or 0.8.5 and without code changes, and post your error again if any?. There were no recent changes and all recent commits are supposed to work.\nCan you post the entire log?. Is there still an error?. Please follow the issue template and provide relevant information for any progress to be made.. You should use val_fun/pred_value because that's probably the name tensorflow decides to give to your tensor. You can print(pred_val) if you're unsure about what the name is.. Not a tensorpack question, so closing.\nStereo-Pose-Machines is not maintained any more as mentioned in its readme. The code is there for a reference if anyone is interested in studying such projects but this means people will need to figure out the code by themselves. It requires specific hardwares so it's not meant for anyone to actually run. \nIt is developed around Dec 2016 with tensorpack version at that time. The error you saw is due to an API change half a year ago, which can probably be fixed in https://github.com/ppwwyyxx/Stereo-Pose-Machines/commit/cadb5d057f224e5d53d17c33e776b5296ca7eccf,  following tensorpack changelog.\ncpm.npy was the model we trained for that project that is not available now.. Interesting, that's indeed a problem. It's just DataParallelOfflinePredictor was never useful (it's not a good option for deployment) so I wasn't thinking about it.. Gpu is required. Either you don't have a nvidia GPU or your tensorflow is not built with GPU support.\nPlease run\npython\nfrom tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())\nAnd make sure you can see a GPU device.. Dataflow is just a python generator - it should have no built-in concept about \"training\" or \"machine learning\".\nYou can write a callback which, after every epoch, change some attributes of your dataflow.\nBe careful with some of the parallelism, though. Some parallel dataflow (e.g. PrefetchDataZMQ) create forks of a dataflow, so setting attributes on the original dataflow will not be effective.. reset_state is called only once for each dataflow.\nget_data is called after the iterator from the previous call reaches the end.. get_data will be called in each process independently. In each process it is called the same way (called again after the iterator from the previous call reaches the end).. You can use tf.get_default_session().. Yes. The model is pure tensorflow code and you change it in the same way you change a tensorflow model. For example you may want to change MODULE_SIZES in the for loop.\nThe two scripts are not comparable at all even with the same number of layers because the architecture have many other differences.. No. I believe it's trivial to do if one has already learned tensorflow.. No. I've never trained a WRN.. Thanks for reporting. Looks like I was using a feature unavailable in TF 1.4. I'll update the code soon.. virtual_batch_size is only available after TF 1.5. Now the code won't touch this option if running under TF1.4.. The \"long\" issue is a python2 specific issue that should be fixed in the last commit.\nCould you post your error log?. The long type problem should be fixed by the early commit. I was asking about other issues (if any).. Could you please post full error?. Looks like there are a couple of problems with this kmeans.\nFirst of all it creates trainable variables of unknown shape. Some of the utilities in tensorpack assume variables have fully defined shape. The above commit should help with that.\nSecondly it does not respect variable reuse. It creates variables even under a \"reuse=True\" variable scope, which means you cannot use it with tensorpack TowerTrainer and all its subclasses.\nThirdly it is not trained by gradient descent, although it creates trainable variables. This means you cannot train it with tensorpack ModelDesc which assumes single-cost gradient-based optimization.\nTo use kmeans you'll probably need to build your graph manually and write your own trainer. I don't know if there are other ways around because I'm not familiar with the kmeans module.. PredictConfig can take a tower function and InputDesc. Both are already arguments of KerasModel in the example.. Names are defined by Keras so I have no control. You can use print to see the name of a tensor, or use tf.identity to give a tensor an alias name.. What Keras prints does not necessarily correspond to tensor names. It's up to Keras what it prints and I have no control over that.\nYou can use print (the python builtin function) to see the name of a tensor.. Oh if you're using the mnist-keras-v2.py example -- it's not written with Keras's functional API (unlike imagenet-resnet-keras.py example). It is not very obvious to me how to get tensor's name with Keras's container API.. By \"tensor\" I mean tf.Tensor. A layer is not a tensor. Therefore a layer's name is not a tensor's name.\nI just went through Keras's container API, and it looks like you can access a tensor with something like M.layers[10].output. There you can see the name of the tensor.. I found some other issues using offlinepredictor for Keras models. I'll try to do a quick fix.. There were some changes in the the arguments of \"tower function\" that was not updated in the Keras example.\nNow with the latest code, you should be able to build a predictor in mnist-keras-v2.py like this:\npython\n    pred = PredictConfig(\n        tower_func=model_func, \n        inputs_desc=[\n            InputDesc(tf.float32, [None, IMAGE_SIZE, IMAGE_SIZE, 1], 'images')\n        ],\n        input_names=['images'],\n        output_names=['activation/Softmax']\n    )\n    pred = OfflinePredictor(pred)\nTo get tensor names from Keras, use something like print(M.layers[10].output.. The ops definitely will run when you call sess.run\nSo the only explanation I see is that the op does not do what you expected it to do.. Interesting. Thanks for the report!. KerasModel.fit in tensorpack also takes the session_init option.. > The error occured when re-train the model or using ctrl-c to interupt the training precedure\nI don't understand what do you mean by the \"or\". What exactly did you do to see this log? . Please clarify:\nWhat will happen if you don't press ctrl-c ?\nWhat will happen if you press ctrl-c a lot of times?. What will happen if you press ctrl-c a lot of times? --- Does this give you back to the command line or not?\nAnd by saying \"Can not start the training precedure again\" what exactly did you observe? Does this give the error you've posted above or something else? Could you post the error?. OK now I understand. This is related to issue #668 and can probably be solved in the master branch if you've installed the \"python-prctl\" package.. It should be available in the latest version.. Ah yes. Before switching to pyarrow this was needed.. Do you have a failure case? I don't remember what the issue was.. Seems to work fine. Chances are it was not an issue any more for msgpack.. btw, TENSORPACK_SERIALIZE was added.. Please paste your logs instead of describing them.. Are you using some functions from tensorflow to create these bidirectional rnn variables or are you creating them with your own symbolic code? If it's from tensorflow, what is the function?. It's best if you could paste some relevant code. They're always more helpful than words. Now it looks like some Keras layers create variables in a way incompatible with tensorpack's multigpu trainers. There may be solutions but I don't know how to reproduce such error now.\nAlso, full logs are more helpful than xxxxxx. If you prefer you can use \n<details>\n<summary>Click to expand</summary>\nlogs\n</details>\nso that they don't take too much space.. The problem comes from the following code in ConvRNN2D:\n```\n  def get_initial_state(self, inputs):\n    # (samples, timesteps, rows, cols, filters)\n    initial_state = K.zeros_like(inputs)\n    # (samples, rows, cols, filters)\n    initial_state = K.sum(initial_state, axis=1)\n    shape = list(self.cell.kernel_shape)\n    shape[-1] = self.cell.filters\n    initial_state = self.cell.input_conv(initial_state,\n                                         K.zeros(tuple(shape)),\n                                         padding=self.cell.padding)\nif hasattr(self.cell.state_size, '__len__'):\n  return [initial_state for _ in self.cell.state_size]\nelse:\n  return [initial_state]\n\n``\nIt callsK.zeros`, which in my test case creates a all-zero VARIABLE\ninstead of a all-zero constant TENSOR.\nThis abuse of variables by itself looks like a bad choice because it hurt\nperformance and memory.\nIt does not create this variable with the variable scope name, but still\nput the the variable into TRAINABLE_VARIABLES collection, which breaks a\nlot of contract of a \"tower function\". Therefore such symbolic functions\ncannot be used with tensorpack.\n. In the Conv2DLSTM case I think it happens to do the correct thing. Because tensorpack trainer just ignores this variable. But I'm not totally sure.. I'll take a look later. Maybe there are ways to handle this better.. Some checks were added to fix the collection after calling Keras models. Variables are now marked trainable iff it was marked trainable by Keras AND it appears in Keras's model.weights. This seems to make ConvLSTM work and produce a more reasonable warning.. Which links are you talking about?\nhttp://tensorpack.readthedocs.io/ works fine.. https://github.com/ppwwyyxx/tensorpack/blob/712fd299805f04c4095ad80509f94ff7db245ff6/examples/GAN/DCGAN.py#L122-L134\nhttps://github.com/ppwwyyxx/tensorpack/blob/712fd299805f04c4095ad80509f94ff7db245ff6/examples/CaffeModels/load-vgg16.py#L55-L72\nhttp://tensorpack.readthedocs.io/modules/predict.html#tensorpack.predict.PredictorBase.call. You can use small batch size per GPU.. Similar. See http://tensorpack.readthedocs.io/tutorial/trainer.html#multigpu-trainers. Thanks! OMG I didn't know I had made so many typos.. You didn't paste the full log, I think.\nThe sess.run actually succeeded, otherwise it's a tensorflow bug. You can ignore the error and still get the output. And in an actual training you'll not see such an error.\nThe error comes because you use a constant as input so the graph optimizer tries to first run everything on CPU as a constant folding step.. DataFlow is meant to be stateless when they could. i.e., calling get_data twice is allowed and is designed to return two iterators that have no direct relationship with each other. Therefore the behavior you're seeing is expected. Certain DataFlow cannot be made stateless and therefore they are marked not reentrant.\nSince DataFlow is stateless, then the one and the only consumer of a DataFlow can keep ownership and reuse a data point. Therefore to do this you need to use something like MapData(indices, lambda x: make_features(x) + make_targets(x)).\nYou can also write a custom version of something like DataFlowFromFixedList that does not shuffle inside get_data at the beginning, but in a shuffle() method which you can call when you need. \nI think considering it as a graph doesn't help and its behavior is indeed different from tensorflow iterators. They are just iterators.. Closing since things are working as expected.. Makes sense!. Some possibilities: your TF does not support GPU. Your GPU was occupied or has other errors.\nPosting full logs would be more helpful.. Did you install tensorflow or tensorflow-gpu from pip?. That's the problem. ...uninstall it and install tensorflow-gpu. ...uninstall it and install tensorflow-gpu. The error message doesn't tell a lot. And since the original example is working (I assume) and I have no idea what you wrote in the code, I can't tell what went wrong from the error message. It feels like some error with the device placement.\nPerhaps try removing AccumGradOptimizer and try again. From the error log it seems related. Or you can try a newer version of tensorflow.\nAnyway if you have the unmodified version working, you should be able to find out a minimal set of changes needed to produce this error so people will be able to help.. FPN added in examples: https://github.com/ppwwyyxx/tensorpack/tree/master/examples/FasterRCNN. It's a tensorflow issue that tf.layers.batch_normalization cannot be used inside conditionals. See https://github.com/tensorflow/tensorflow/issues/14809 and https://github.com/tensorflow/tensorflow/issues/14699.\nUsing tensorpack BatchNorm layer's internal_update option can probably solve this.. It's already printed out:\n[0514 20:52:53 @parallel.py:175] WRN MultiProcessPrefetchData does support windows. However, windows requires more strict picklability on processes, which may lead of failure on some of the code.. Or you can remove PrefetchData from your get_data function.\n. I found the reason is that on windows, expressions like:\npython\n            imgaug.MapImage(lambda x: x - pp_mean),\ncannot be used with multiprocessing (the lambda is not pickleable).\nAn alternative fix is to define a function at global scope that does the subtraction. Then use \npython\nimgaug.MapImage(func)\ninstead.. I don't think I'll be able to spend any more time on supporting Keras.\nPlus I think having sample_weight is  a very ugly design... Yes. Keras support is just a proof-of-concept. And obviously it is unlikely to be a focus of this project.. If you can do something once, then you can do something multiple times. What is the issue?. No there isn't.. Paste full logs please. print_tensor() is not for printing the value of the tensor. According to #12 it is for printing the name of the tensor. To print values during training, see FAQ.\nIt's impossible that adding sys.exit() has no effect -- I think you're not running the code you're modifying.. Now most DataFlow assumes list. I feel some inconvenience with it as well. Allowing dict will be great!\nProbably not nested list now -- it's harder to implement and unclear how to map it to inputs in the graph.. It is.. Then you didn't installed tensorflow-gpu correctly.. Your log said assert tf.test.is_gpu_available() failed.. Just need to replace some layers - so it should be easy.\nClosing since this is not a bug report, feature request or usage question.. Because tensorpack is a training interface and does not care about inference. Tensorflow has many inference techniques (serving, quantization, const folding, tensorrt, tflite,...) for different scenarios. These are probably more important than queues.. That said it would surely be nice to have these features. Mark it as a possible future feature but it won't be my priority.. After image_dtype set to float32 and using ResNet50 I observed about 15% improvement for imagenet evaluation.. Ah good catch!. As mentioned in the issue template:\nAn issue has to be one of the following:\n1. Unexpected Problems / Potential Bugs\n2. Feature Requests\n3. Usage Questions\nClosing for the same reason as #771.. Change SimpleTrainer to something else.\nhttp://tensorpack.readthedocs.io/modules/train.html. The permutation will be per-datapoint, i.e. a datapoint is a unit that's sent to one GPU.\nIf you want to permute over larger batches you can write a dataflow that mixes more images than it yields every time.. I don't know what do you mean by \"batch size\" because the term is ambiguous under the context of multi gpu training. But the tutorial should have an answer to your question: http://tensorpack.readthedocs.io/tutorial/trainer.html#multigpu-trainers \nTo write a dataflow, see http://tensorpack.readthedocs.io/tutorial/extend/dataflow.html . You can first use BatchData with a larger batch and then a custom dataflow to split them and yield.. Exactly.. > Any build-in function for the split? \nNo. It should be like 10 lines of code.\n\nAlso, is it possible to have two batches during test phase\n\nI don't understand what this means. Could you describe in details what you want to do?. You can write a dataflow that does this, like ds = mix(test_ds, train_ds). Sure! This looks right. You can write a dataflow that takes datapoints from ds = BatchData(ds, batch), then produce two datapoints together.\nJoinData([ds, ds])  does something similar to this.. python\nitr = ds.get_data()\ndata1 = next(itr)\ndata2 = next(itr)\nwill retrieve two datapoints from ds.. The top level get_data() function in the script defines and returns a dataflow. It does not take data from a dataflow -- that will happen when training starts.\nThe tutorial has a section about writing such dataflow: http://tensorpack.readthedocs.io/tutorial/extend/dataflow.html#more-data-processing .\nPlease refer to JoinData or ConcatData if it's not clear enough, e.g. http://tensorpack.readthedocs.io/_modules/tensorpack/dataflow/common.html#ConcatData. Two best C4 models are uploaded already. \nI may upload a FPN one later, but I won't have time to manage every one of them... R50FPN model was uploaded.  I don't plan to upload those non-standard models. Closing this now.. All the math involving the weights are in the model file https://github.com/ppwwyyxx/tensorpack/blob/master/examples/DoReFa-Net/alexnet-dorefa.py so I'm not sure what you are asking.. If you're looking for kernel implementation, https://github.com/caffe2/caffe2/tree/master/caffe2/mobile/contrib/ulp2 contains some of the best open source ones I know.\ngemmlowp only works with 8 bit and is quite slow. I don't know about BMXNet.. Q1,2: I guess it's better to let users do this explicitly since this is a monkey-patch hack. And if this is the case, hard-coding a list may not be necessary.\nQ3: I don't know what the future of these libraries would be. I think for now supporting tf.layers alone is good enough. \nBtw I've heard from a couple of TF people that they are encouraging people to use tf.keras.layers instead of tf.layers... You set nr_tower to 0. You should load a ResNet50 (ImageNet) model to train, not a MaskRCNN model. The MaskRCNN model is fully trained already and evaluating that model without training should get you the right score. See the README.. I don't know how to reproduce this error and this is some error I've never seen before. Could be a TensorFlow bug as well. Having a piece of runable code that can reproduce the error would be awesome. Apart from that, some information that may make it easier to debug:\n1. Have you seen the log \"creating the session....\" ? Usually this line of log comes after \"Applying collection UPDATE_OPS of 8 ops.\". Knowing where it crashes can help a lot.\n2. Does your code consistently crash or crash by chance? What about a different version of TensorFlow?\n3. What does \"config\" look like?\n4. If you add some weird stuff to TF collections (i.e., things that are not ops or tensors), try removing them. OK. Looks like that's a tensorflow bug then. Seems to happen when you have a very large constant in the graph.. No I think it's caused by large constant tensor in the graph -- not necessarily in the data part, could be anywhere in the graph.\nBut that's just my guess and you'd better wait for responses from the TF team.. See http://tensorpack.readthedocs.io/tutorial/trainer.html#tower-trainer \nYour model function will be called multiple times, and as a result will change your collection.. I'll close this since this appears to be a tensorflow bug https://github.com/tensorflow/tensorflow/issues/19657 that a graph cannot be too large.. This is a TensorFlow error that ops cannot be placed on GPU.\nYou need to initialize the server with a tensorflow sessionconfig, which enables soft placement.\nIn tensorpack, you can just pass config=get_default_sess_config() to tf.Server.. Btw, as mentioned in the docs, you're not recommended to use TensorFlow's native distributed trainer because they are not as efficient as HorovodTrainer.. Looks like a tensorflow bug: https://github.com/tensorflow/benchmarks/issues/165.\nAnd Google's team also suggests you to use horovod for the moment.\nHorovod trainer also has better support in tensorpack than the native trainers, e.g.: https://github.com/tensorpack/benchmarks/tree/master/ResNet-Horovod. InputSource cannot be used alone.\nYou're recommended to implement a custom trainer: http://tensorpack.readthedocs.io/tutorial/extend/trainer.html if you want to do anything substantially different from single-cost optimization.. From the docs https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/model_pruning it looks like you want to run extra ops after each iteration. \nSessionRunArgs is only for before_run and after_run, to provided an interface to tensorflow's SessionRunHooks. It is for running extra ops along with each iteration.\ntrigger_step is for running anything after each iteration. It is just a plain python function so you need self.trainer.sess.run(mask_update_op), or simply mask_update_op.run().. Ah, if mask_update_op a tensor, then it should be .eval() instead of .run().. I would love to but I don't have access to a TPU to debug.. 1. \n\nwhen I pass bind=True to send_dataflow_zmq ,I got the error:\n\nThen you need bind=False for the receiver side.\n2.\n\nI find the error because class RemoteDataZMQ don't init cnt1 in init function.You may init cnt1 in init or tell the user must reset_state before get_data clearly\uff01\n\nIt's clearly told in http://tensorpack.readthedocs.io/tutorial/dataflow.html\n\n\nCould you describe what you did to cause such error? I assume you pass something other than format=None on the sender side. As mentioned in docs http://tensorpack.readthedocs.io/modules/dataflow.html#tensorpack.dataflow.send_dataflow_zmq, the other format is for use with https://github.com/tensorpack/zmq_ops. To be able to use it, you need to install zmq_ops and use those ops on the receiver side.\n\n\nUse the sender with bind=True, use the receiver with a remote ip address and bind=False. When there are multiple receivers, zmq will send 1 message to only 1 receiver.\n. 1. If the error is zmq.error.ZMQError: Cannot assign requested address, then it's your network issue. Check your ip address is correct. Check your port is not occupied, etc.\n\n\nThe zmq_ops repo has an example to use it as a separate package. https://github.com/tensorpack/zmq_ops/blob/master/benchmark.py\nTo use it inside tensorpack, you can use ZMQInput as the input source for the trainer.. 1. And what is the address you use? I hope it's not tcp://xxx:2222.\n\n\nThis has nothing to do with pyarrow. As said you need ZMQInput as the receiver.. Same as #721. Please use latest tensorpack with \npip install -U git+https://github.com/tensorpack/tensorpack.git. The training script for that model is https://github.com/tensorpack/tensorpack/tree/master/examples/ResNet, which imports the preprocessing at https://github.com/tensorpack/tensorpack/blob/3e65266830e2eadb924998be8e0c0cc65a7c4f8d/examples/ImageNetModels/imagenet_utils.py#L215-L230. This is how the model is trained on ImageNet.\nThis does not imply what you're supposed to do on a different dataset. That's your decision to make. \n\n\nI would guess using the same constants I pasted above to preprocess the image is more reasonable. But such discussion is out of the scope of the project.. In tensorflow, loading a metagraph and modifying it for training is NEVER a good idea. It almost always causes undesired trouble. In general you should write python code to recreate your model, and only load variables from a checkpoint.. To load a model for training, conceptually what's needed to be loaded is some mathematical model plus the variables. MetaGraph (or graph) is not the right abstraction for it.\nA metagraph contains the entire graph. For the one saved by tensorpack, it includes not only the mathematical model, but also the training settings: queues, data iterators, placeholders, evaluations, summaries, collections. Loading all of these settings from the last training means it will be very hard (if possible at all) to change them - essentially making the entire tensorpack not functional. In general as long as you want to make changes to the last training settings, it's going to be problematic to load a metagraph.\nThe tensorpack way to work around this problem is to encourage writing the mathematical model by python and load variables only.\nAnother way is the tensorflow-hub way: provide a \"cleaner\" version of metagraph (but not the actual graph used in training) that only contains the math, and use many hacks during import_meta_graph to avoid bad interaction between the imported graph and the existing graph. . What do you expect?. > I need the graph to ensure horovod only broadcast the variable at the begin of train from root_rank.\nOther ranks get the grad after ring allreduce and apply it on its own and begin next step\nThis is exactly what happened.\ndump_event dumps stuff to tensorboard event files. tensorboard event files will only be activated in the processes where logger.set_logger_dir was set. Maybe that's the reason? My example only set the directory for the master process, but you can set a different logger_dir for each process.\nIf this does not solve the problem, could you paste the log of a non-master process?. You can look for \"WRN logger directory was not set. Ignore TFEventWriter.\" in logs to see if the above is the reason.. could you paste the log of a non-master process?. Oh right. I forgot that monitors are by default chief-only.\nYou can change that by provide monitors=[TFEventWriter().set_chief_only(False), JSONWriter(), ScalarPrinter()] in TrainConfig. Let me know if this is not the case.\nI'll think about whether it's a good default or not.. Why?. That means some images cannot be read. You may have a corrupted dataset. Glad to hear!. These are too many changes and it effectively adds a new interface of DataFlow, and you are on the road towards reimplementing this new interface for every existing DataFlow..\nThere are simpler solutions for your use case. See https://github.com/tensorpack/tensorpack/issues/401#issuecomment-328963606.\nYou just need to write one \"base\" dataflow that produces different set of indices in each process.. I was a bit misleading. The solution in https://github.com/tensorpack/tensorpack/issues/401#issuecomment-328963606 does not guarantee strictly non-duplication. It could have a small amount of duplication at the end of the epoch, because those data could possibly mix with the next epoch.\nAt that time there was no MultiProcessMapData. What I had in mind is to have a single dataflow that produces all indices (so it's fast enough and don't need parallelization), and then run the function that maps indices to data in parallel, such as:\n```python\nds = DataFlowProducesIndices()\ndef mapper(i):\n    return cv2.imread(filenames[i])\nds = MultiProcessMapData(ds, 10, mapper, strict=True)\n```\nI think this can achieve what you expect.. ds = DataFlowProducesIndices() will only exist in one process. It will not be forked.\nMultiProcessMapData creates processes to only run the mapper function.. Closing as it looks like the problem was solved.. Why not a function that dumps a dataflow to npz, and a dataflow that reads from npz? Like DataFlowFromNpz(filename), DataFlowFromNpz.dump(ds, filename). This way we add new stuff but it's easier to understand and composable. Also allows other formats this way. \nAnd you can then move the logic of if file exists:   ... else: ... to your code. Having to explain an if-else logic in docs may suggest that the function may be doing too many things.. > the validation data which can be generated on-the-fly is stored in some persistent way for a fair comparison of different models.\nSince this is the main goal, I think it naturally expects a logic to tell \"do I need to generate data\" or \"do I need to load data\". Check the existence of file is one way to do that and I'd prefer the user to manage such high-level logic on \"what to do\".\nTo reduce code, what about the following?:\npython\ndef get_val_data():\n    if not exists(file):\n        ds = ....\n        DataFlowFromNpz.dump(ds, file)\n    return DataFlowFromNpz(file). Thanks for reporting! Forgot to update the initial learning rate when I refactor the code. Should be fixed now.. For pyarrow dumps() does not return bytes. It returns a buffer that works like bytes most of the time.\nThere are many changes that I'll take time to digest but I like the direction to organize readers and writers. For now I don't think it's worth increasing the complexity of interfaces just to share a progress bar among writers. This for example makes the write_frequency option in lmdb become a bit unnatural to implement.\nI still prefer using functions for writers (i.e. dump_lmdb(ds, filename) instead of LMDBWriter(ds, filename).write()) because I think you'll never need to use an instance of writer twice. So you'll always call it with one line of LMDBWriter(ds, filename).write() which can be achieved with a function call.. About interface:\nThe concept of writer exists in many libraries because there you can create a writer and call write(bytes) multiple times with different data -- that actually corresponds to \"puts\" here. That's why I was uncomfortable with the interface.\nTo match common terminology of writer, the shared public interface would be .write(datapoint) and .close() and possibly also a .dump(dataflow) which closes itself in the end. \nAbout function vs class:\nIt looks nice when they are sharing interfaces but I think LMDBWriter(filename).dump(dataflow) may not look as simple as a single function call like LMDBWriter.dump(filename, dataflow) or even LMDBReader.dump(filename, dataflow).\n. About names:\nLMDBDataReader is not a better name than LMDBDataPoint because this simpler name sounds too general and may become misleading given that the class does not read any LMDB. LMDBData, LMDBDataDecoder and LMDBDataPoint are already a bit confusing but I'm afraid that LMDBDataReader makes it worse. Same for NumpyDataReader, and I should add a decoder= option in TFRecordData. \nI think the names should at least convey the message that it reads some tensorpack-specific thing. But I also like the correspondence between XXReader and XXWriter because the reader can ONLY reads something dumped by the writer.\nTaking a step back and look at the whole problem, these functions are just doing serialization-deserialization of dataflow. The traditional way of doing serialization/deserialization in python is to have .load and .dump (or  getstate/setstate, tostring/fromstring) defined in the class and users are never expected to understand the format at all. The only difference here is that DataFlow needs to be serialized to multiple formats.\nGoing from the traditional way to creating one class for each serializer/deserializer is too far a step. I've been talking without giving any actual suggestions. But now I would propose the following interface:\npython\nDataFlowSerializer.to_numpy(ds, filename)\nDataFlowSerializer.from_numpy(filename)\nDataFlowSerializer.to_lmdb(ds, filename, options)\n....\nor \npython\nNumpySerializer.dump(ds, filename)\nNumpySerializer.load(filename)\nLMDBSerializer.dump(ds, filename, options)\n...\nThe benefits are:\n1. They are more tightly paired (inside one namespace now, just like the traditional dump/load).\n2. It's clear that they are serializers, i.e. you cannot use a LMDB deserializer to load an arbitrary lmdb file.\nThis is only about designing the interface and names. Implementation-wise the progress bar may still be shared.. Different formats may support different options (like write_frequency in lmdb and the data path for hdf5) so it's probably impossible to share the same function and dispatch based on the filename.. There is not much to be shared among them -- and that's what I expected. There is not much duplication among them. \nInterface-wise they are different: for save, they have different options. Even \"filename\" is not a universal argument because LMDB can take a directory. And overwrite=True should be the user's responsibility. Would you call it a good design for np.save(filename, overwrite=True) or pickle.dump(obj, file, overwrite=True)? For load, they have different options (e.g. shuffle).\nImplementation-wise the only code duplication is setting up the dataflow and progress bar, which is about 6 lines.\nSee #802. It's WIP but I think this design is much simpler.. I don't mind moving the implementation of TFRecordData into load().\nThe only benefit (compared to implmenting TFRecordData in load()) is that TFRecordData is for reading data with a user-defined decoder. i.e., we have one API that reads an arbitrary tfrecord, and one API that's a dataflow deserializer, and they share code. Here they are named TFRecordData and TFRecordSerializer.load respectively.. @PatWie do you have other suggestions on #802?. Tensorpack simply reads them, and does nothing about caching. It's up to your operating system.. After some investigation and communication on gitter, it turns out that it is an expected behavior that not all files are cached.\nSince each worker created by PrefetchDataZMQ randomly produces images independently, when the number of workers is larger than one, there will be duplication and therefore not all files are read.\nTo ensure a whole dataset is used, use indices + MultiProcessMapData (https://github.com/tensorpack/tensorpack/pull/794#issuecomment-398429188)\nSome more details:\nAssuming there are K workers, each worker will create a local shuffle and produce roughly the first N/K images in the first epoch. The probability for a particular worker to produce a particular image is 1/K.\nThe probability an image will not appear in the first epoch will be [(K-1)/K]^K.\nWhen K=40, this number is 0.36 so the cache will be 64% full. Considering the prefetching the cache should be a little bit more than 64% full.. 1. Fixed now.\n\nIf you want to do this during training, http://tensorpack.readthedocs.io/tutorial/faq.html#how-to-print-dump-intermediate-results-in-training\nIf you want to do this after training, add the names of tensors you want to dump to OfflinePredictor. http://tensorpack.readthedocs.io/tutorial/faq.html#how-to-print-dump-intermediate-results-in-training. There are input_names and output_names given to the PredictConfig, where you can define what do you want to feed to and fetch from the graph.\nSee http://tensorpack.readthedocs.io/modules/predict.html#tensorpack.predict.PredictConfig. > ProcessTensors() doesn't instance list names of tensor is exist or not?\n\nI don't understand what you mean. You can find its documentation at http://tensorpack.readthedocs.io/modules/callbacks.html#tensorpack.callbacks.ProcessTensors\n\nDo I need use callbacks() in OfflinePredictor?\n\nAs you can see from the documentation http://tensorpack.readthedocs.io/modules/predict.html, there is no such argument. Callbacks are only for training, and it is supposed to be set in TrainConfig.. The accuracy should be similar to those in the readme. The code you added did nothing.. Closing now as this is unrelated to the original issue. If you got accuracy that's very different from the readme, you can open an issue following the issue template so we can investigate.. If the example runs fine it's probably unrelated to tensorpack.\nPlease check that cfg.crop_size is an integer.. ```\n\u2570\u2500$python3 -c 'import tensorflow as tf; tf.placeholder(tf.float32, (None, [32], 32, 1), \"abc\")'\nTraceback (most recent call last):\n  File \"/home/XXX/.local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\", line 141, in make_shape                                                                                \n    shape = tensor_shape.as_shape(v)\n  File \"/home/XXX/.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\", line 946, in as_shape                                                                         \n    return TensorShape(shape)\n  File \"/home/XXX/.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\", line 541, in init \n    self._dims = [as_dimension(d) for d in dims_iter]\n  File \"/home/XXX/.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\", line 541, in  \n    self._dims = [as_dimension(d) for d in dims_iter]\n  File \"/home/XXX/.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\", line 482, in as_dimension                                                                     \n    return Dimension(value)\n  File \"/home/XXX/.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\", line 37, in init \n    self._value = int(value)\nTypeError: int() argument must be a string, a bytes-like object or a number, not 'list'\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/home/XXX/.local/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1734, in placeholder                                                                              \n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/home/XXX/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 4922, in placeholder                                                                          \n    shape = _execute.make_shape(shape, \"shape\")\n  File \"/home/XXX/.local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\", line 143, in make_shape                                                                                \n    raise TypeError(\"Error converting %s to a TensorShape: %s.\" % (arg_name, e))\nTypeError: Error converting shape to a TensorShape: int() argument must be a string, a bytes-like object or a number, not 'list'. \n```\nI seriously doubt that... I did not see any tensorpack issue in this thread.\nIf you meet unexpected problems, please post details according to the issue template.. btw, I can understand that numpy is good for its simplicity, but is there any advantage of HDF5, compared to lmdb?. I can't understand what is your question. Could you simplify and clarify?\nIt seems that the number \"410624\" in the log is not what you expected, but I have no idea what code prints that number or how is that related to tensorpack or dataflow.\nIt seems that the part of logs that contain \">>>>>>>>>>>>>>>>>>>>>>>>><><><><>< -------------------- .\" is also different from what you expected but I have no idea what you're expecting.. I still don't fully understand what is the issue. Now my understanding is:\n1. Sounds like it has nothing to do with model restore, is that correct? \n2. You're saying that BatchQueueInput should give you 1024 instances per step and 10 steps per epoch -- this I agree. But how do you know it did not do so? What code prints the number 410624 and how does this number relate to what BatchQueueInput has done?. 1. What callbacks do you use for training? \n2. Did you write any callbacks that contains things like sess.run or tensor.eval()? If yes, what do they look like?. The callbacks look fine.\nYou can try this code below, modified from the mnist example. It is based on my understanding of what you're doing, and this code works as I expected:\n```python\n!/usr/bin/env python\n-- coding: utf-8 --\nimport os\nimport argparse\nimport numpy as np\nJust import everything into current namespace\nfrom tensorpack import *\nfrom tensorpack.tfutils import summary\nfrom tensorpack.dataflow import dataset\nIMAGE_SIZE = 28\nclass Model(ModelDesc):\n    def inputs(self):\n        \"\"\"\n        Define all the inputs (with type, shape, name) that the graph will need.\n        \"\"\"\n        return [tf.placeholder(tf.float32, (None, IMAGE_SIZE, IMAGE_SIZE), 'input'),\n                tf.placeholder(tf.int32, (None,), 'label')]\ndef build_graph(self, image, label):\n    \"\"\"This function should build the model which takes the input variables\n    and return cost at the end\"\"\"\n\n    # In tensorflow, inputs to convolution function are assumed to be\n    # NHWC. Add a single channel here.\n    image = tf.expand_dims(image, 3)\n\n    image = image * 2 - 1   # center the pixels values at zero\n    # The context manager `argscope` sets the default option for all the layers under\n    # this context. Here we use 32 channel convolution with shape 3x3\n    with argscope(Conv2D, kernel_size=3, activation=tf.nn.relu, filters=32):\n        logits = (LinearWrap(image)\n                  .Conv2D('conv0')\n                  .MaxPooling('pool0', 2)\n                  .Conv2D('conv1')\n                  .Conv2D('conv2')\n                  .MaxPooling('pool1', 2)\n                  .Conv2D('conv3')\n                  .FullyConnected('fc0', 512, activation=tf.nn.relu)\n                  .Dropout('dropout', rate=0.5)\n                  .FullyConnected('fc1', 10, activation=tf.identity)())\n\n    tf.nn.softmax(logits, name='prob')   # a Bx10 with probabilities\n\n    # a vector of length B with loss of each sample\n    cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n    cost = tf.reduce_mean(cost, name='cross_entropy_loss')  # the average cross-entropy loss\n\n    return cost\n\ndef optimizer(self):\n    return tf.train.AdamOptimizer(0.01)\n\ndef get_config():\n    def gen():\n        cnt = 0\n        while True:\n            cnt += 1\n            print('------------', cnt)\n            for k in range(1024):\n                yield [np.random.rand(28, 28), 0]\n    dataset_train = DataFromGenerator(gen)\n    steps_per_epoch = 10\n    return TrainConfig(\n        model=Model(),\n        data=BatchQueueInput(dataset_train, 1024),  # the DataFlow instance for training\n        callbacks=[ ],\n        steps_per_epoch=steps_per_epoch,\n        max_epoch=100,\n    )\nif name == 'main':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--gpu', help='comma separated list of GPU(s) to use.')\n    parser.add_argument('--load', help='load model')\n    args = parser.parse_args()\n    if args.gpu:\n        os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n# automatically setup the directory train_log/mnist-convnet for logging\nlogger.auto_set_dir()\n\nconfig = get_config()\nif args.load:\n    config.session_init = SaverRestore(args.load)\n# SimpleTrainer is slow, this is just a demo.\n# You can use QueueInputTrainer instead\nlaunch_train_with_config(config, SimpleTrainer())\n\n. This is by design of tensorflow. `sess.run([a, b])` is not equivalent to `sess.run(a); sess.run(b)`. You can run the entire list instead of running them separately.. Closing as things are working as expected and it's a tensorflow design.. I changed the preprocessing of the model in 18b19d6dec901415ab8877a1405145cb5dda919b, but forgot to update the `run_image()` function accordingly.\nI'll push a fix. You can also evaluate the model with this diff:diff\ndiff --git i/examples/DoReFa-Net/alexnet-dorefa.py w/examples/DoReFa-Net/alexnet-dorefa.py      \nindex 95a1e84..1b30f1b 100755                  \n--- i/examples/DoReFa-Net/alexnet-dorefa.py    \n+++ w/examples/DoReFa-Net/alexnet-dorefa.py    \n@@ -220,6 +220,13 @@ if name == 'main':\n     else:                                     \n         BITW, BITA, BITG = map(int, dorefa)     \n\nfrom imagenet_utils import eval_on_ILSVRC12 \nfrom tensorpack.tfutils.sessinit import get_model_loader                                     \nBATCH_SIZE = 128                            \neval_on_ILSVRC12(Model(), get_model_loader(args.load),                                       \nget_data('val'))                    \nimport sys; sys.exit()                      \n\nif args.gpu:                              \n     os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu                                            \n\n\n```. The goal of the paper is to make the input of every conv layer quantized.. > this can be the input of the first conv layer of the next resblock, right?\nIf so, could you point out in code which conv layer?. You're not answering my question.. \nThe output of each block is not quantized. The first layer in each block is batchnorm followed by quantization, not a conv layer.. > is that all?\nThat is all.\nIf you're wondering what you did wrong, I can not tell much because I can only guess what you did, based on the limited description you've given. From the code snippet it looks like you're subtracting the mean of the image somehow, but in the code you can see that the mean/std are constants.. img in the graph has type uint8. Oops. That's a deprecated usage of add_moving_summary.. It's not expected to be run like that. And there is a small error in the __main__ part of the file  (http://tensorpack.readthedocs.io/tutorial/dataflow.html#use-dataflow-outside-tensorpack).. import_meta_graph should not be used to run a model. That's a very common tensorflow mistake: https://github.com/tensorpack/tensorpack/issues/790#issuecomment-396726606\nhttp://tensorpack.readthedocs.io/tutorial/inference.html#inference-after-training. Please upgrade tensorpack to github master. The change to use tb.summary was reverted last week. Or use latest version of tensorflow + tensorboard. Then you'll have tb.summary. https://github.com/tensorpack/tensorpack/commit/2a712afea3e4eaefbaa1b85be59e9c8eebf53455 should fix the issue.\nThe reason MultiProcessMapData is not cleaned when you call del is because there is a line of useless code holding reference to it. . You have a network failure or something that made the download fail. Please download it by yourself at http://dl.caffe.berkeleyvision.org/caffe_ilsvrc12.tar.gz. Ah right. Please download and decompress it.. You should tar xzf caffe_ilsvrc12.tar.gz. There should be files such as ilsvrc_metadata/synsets.txt. It works fine under TF 1.8. Maybe an early version of TF cause this failure. Which version are you using? I can try to add some code to work around early versions.. diff\n--- i/tensorpack/models/batch_norm.py\n+++ w/tensorpack/models/batch_norm.py\n@@ -166,7 +166,7 @@ def BatchNorm(inputs, axis=None, training=None, momentum=0.9, epsilon=1e-5,\n                 center=center, scale=scale,\n                 beta_initializer=beta_initializer,\n                 gamma_initializer=gamma_initializer,\n-                fused=True,\n+                fused=(ndims == 4 and axis in [1, 3]),\n                 _reuse=tf.get_variable_scope().reuse)\n             if TF_version >= 1.5:\n                 tf_args['virtual_batch_size'] = virtual_batch_size\nCould you check whether this works for your version?. The above commit should fix this one (and the other ones in this issue).\nThe dorefa-net example actually explicitly said in the read me that it requires TF>=1.7: https://github.com/tensorpack/tensorpack/tree/master/examples/DoReFa-Net#use.\nTo really use TF1.3 you'll need to copy-paste an older version of dorefa.py as well (I assume you've done that already?). You can print(tensorpack.__file__) to see where to modify. You can also run pip install -U git+https://github.com/tensorpack/tensorpack.git again to upgrade.. 1. http://tensorpack.readthedocs.io/tutorial/performance-tuning.html\n2. Don't know where the npz is from. But you'll need to modify the code to load from npz instead of checkpoint. http://tensorpack.readthedocs.io/tutorial/save-load.html#load-a-model\n3. The same as above. Extract with tar xzf xx.tar.gz. You'll get a directory called cifar-100-python.\n4. http://tensorpack.readthedocs.io/modules/callbacks.html#tensorpack.callbacks.PeriodicTrigger. btw, even with 4 TitanX and TF 1.3, and standard alexnet (in https://github.com/tensorpack/tensorpack/tree/master/examples/ImageNetModels), I can see 4.8 it/s if each iteration is a total batch of 256.\nDoReFa-Net is not a standard alexnet. On 4 TitanX and TF 1.3 I saw around 1.5it/s -- just a reference. You still need to figure out why your environment is slow. It's probably caused by slow data loader (i.e. slow cpu or disk). Since you're using anaconda, it's worth noticing that opencv in anaconda is usually very very slow: https://github.com/tensorpack/benchmarks/blob/master/ImageNet/benchmark-opencv-resize.py .\nYou might want to check that and use a different opencv (e.g. pip install opencv-python). As the website says it is a caffe version of AlexNet, which has many differences from the alexnet-dorefa code.. Extract it where it is supposed to be downloaded, i.e. tensorpack_data/cifar100_data. Not tensorpack_data.. See the class ImageNetModel. 1. https://twitter.com/karpathy/status/1013244313327681536?s=19\n2. I have no comments.. 1. This open source code does not contain the experiment you mentioned. We will not provide the full precision model.\n2. PeriodicTrigger(ModelSaver(), every_k_epochs=5).. 1. I don't know what you mean by \"zero the gradients\". TensorFlow has correct gradients for the clip operation.\n2. It's correct. But there is also a BatchNorm layer that will bring the distribution back.. You're correct. For weights it's not related to batchnorm.. This is the official implementation of the paper. It never has the so-called de-normalization process.. http://tensorpack.readthedocs.io/tutorial/save-load.html\n--load model-xxxxxxx. It restores anything stored in the checkpoint.. http://tensorpack.readthedocs.io/tutorial/save-load.html#work-with-tf-checkpoint\nThe page has explained it.. The docs also has it: http://tensorpack.readthedocs.io/modules/tfutils.html#tensorpack.tfutils.sessinit.SaverRestore.init . Conceptually yes. It loads whatever in weights.npz.\nBut your command line arguments are incorrect. Please read the docs dump-model-params.py -h. this looks correct. http://tensorpack.readthedocs.io/tutorial/save-load.html\nThe page has explained it.. http://tensorpack.readthedocs.io/modules/tfutils.html#tensorpack.tfutils.sessinit.DictRestore\nThe docs said it needs a dict.. You wrote DictRestore(args.load).\nargs.load is a filename of type str, as printed in the error message.. I think so. The paper was >2 years old and I did not perform this experiment at that time so I do not remember what was the setting.. For unexpected problems, please post issues following the issue template. I cannot understand what your issue is. Could you post with the issue template? In particular please include all the logs.. Of course when get_data returns, the generator ends.\nYou can use RepeatedData(dataflow, -1) which will call get_data again after it returns.. This example is not written for multi gpu training.. http://tensorpack.readthedocs.io/tutorial/trainer.html#multigpu-trainers. The code now produces the data with a fixed order. You probably want each GPU to take different data so you'll also need to modify the data loading code.. The logs you posted does not contain any error. So the question is what makes you think it crashed?\nThe code runs well on linux. I recommend you to run some basic mnist examples  (in examples/basics) to rule out problems in your environment first.\nAlso, as mentioned in the issue template:\n\n\nBetter to paste what you did instead of describing them.\n. Your modification still runs well on Linux. I believe this is an environment problem (software incompatibilities among each other, etc). Sorry but I cannot help.. Maybe also try some larger official code (e.g. ResNet/cifar10-resnet.py). Perhaps you have something like an out-of-memory error but windows does not tell you.. Probably an issue with opencv. When tensorflow first released it has some bad interaction with certain build of opencv, on Linux. Maybe the problem still exists on windows.\nFor example, if your opencv is built with cuda support or opencl support, it's very likely to cause crash when used together with tensorflow. There are other possible causes as well.. I'm closing this now since it's not a tensorpack problem. Maybe cv2.getBuildInformation() can tell something, e.g., you can check whether CUDA or opencl is enabled in cv2.getBuildInformation().. Check your logs carefully. There is probably a warning.\n\n\nNext time please post issues following the issue template. In particular, include the entire logs.. Adding this to the mnist example works fine. So please provide more details on what you did and what you observed.. Some debugging shows that ProfileContext is not thread safe.\nTherefore it cannot be used with QueueInput. QueueInput will start a new thread to feed data into queues.\nThis is a tensorflow issue so there isn't much I can do.. You can use alternative to QueueInput such as data=TFDatasetInput(TFDatasetInput.dataflow_to_dataset(dataset_train, [tf.float32, tf.int32])) which does not use threads.\nIn general I don't think ProfileContext is a good thing to use. It's implemented in a very very hacky way and that's doomed to cause trouble.. You can use StagingInput.\nI think originally they have not seriously tested such features, just like many other \"fancy\" features in tensorflow that does not work.\nYou can use GraphProfiler to profile. It works for all types of input.\nYou can use ProfileContext in your own code. You do not need to touch tensorpack source code.. I don't know what is the \"original tensorflow way\".\nThis does not give you any prefetch. You can use tf.data API to add prefetch yourself, e.g.: TFDatasetInput.dataflow_to_dataset(dataset_train, [tf.float32, tf.int32]).prefetch(100).. Thanks, we are aware of this. #802 will fix this.. The latest commit has also fixed it, since #802 will need some improvement before it can be merged.. It's common to see this since you've removed GoogleNetResize().. We do not help people tune the models. It's out of the scope of tensorpack.. It's answered in the code already.. How is your question related to tensorpack?. How is \"keras tensorflow backend\" related to tensorpack?. \"Using tensorflow\" does not imply you're using tensorpack. Tensorflow is a dependency of tensorpack.. What functions or features in tensorpack are you having questions about?. There is no such function in tensorpack.. Not a tensorpack issue..\nIs this reproducible?. Could you post more information following the issue template? If I could reproduce your problem I can file an issue to tensorflow. Otherwise there is nothing I can do.. Interesting.. I was able to reproduce it. Will investigate it a bit.\nTypically I only run GraphProfiler for about 10 steps (as mentioned in is docs).. Closing. Please track the corresponding issue in tensorflow.. There shouldn't be much to modify. You can just run it and fix whatever exceptions you meet.. It's not right. Please understand what the code does before changing it. You can find the syntax here: https://www.python.org/dev/peps/pep-3132/. For me it only takes 1 min, so it's certainly not something I would want to spend time improving. Not sure if the use of python2 contributes to the low efficiency.. 16G is too small. You can modify data.py to use smaller nr_proc or buffer_size to reduce memory use. http://tensorpack.readthedocs.io/modules/dataflow.html#tensorpack.dataflow.MultiProcessMapDataZMQ\nAlso, I don't know what does \"on init\" means exactly. Please post unexpected issues following the issue template.. config.py has the options about GroupNorm. Refer to those docs for details.. data.py. All the configurations are written in the last column. I don't quite understand what you're asking about.. Inputs are padded (in resnet_fpn_backbone()). This is the same as detectron.. Use http://tensorpack.readthedocs.io/modules/callbacks.html#tensorpack.callbacks.PeriodicTrigger .\nOr just set the epoch size to a smaller number... You're looking at an old version of conv2d.. I don't understand your question completely but it seems like you want to know why \"my model\" has different shape for some weights from your models.\nHowever I don't know which of the \"my model\" are you talking about, and how did you obtain \"my model\". Could you answer these questions with more details?. > 'your model' is vgg16.ckpt from google drive\nCould you give more details? I have not put any vgg16.ckpt on google drive IIRC.. You need to do similar things for mask_rcnn_head_func as well. I'm not sure if there are other places you need to also hack around.\nAlso, the ff_false part should return [0, ncls, 4] as the second element, although it was ncls-1 in an earlier version.\nI'm closing since the reason I drop TF 1.4 support is exactly because it's annoying and I don't want to deal with it more, especially for FPN. Hope you can solve it and get it running with my above comments. You can install a new version of TF with old cuda by compiling from source.. It's up to you.. If you're asking about whether it affects accuracy, I don't know but I think not.\nTo be consistent with the paper.. There is no.. You can modify the resnet training code following what's in the fasterrcnn basemodel.. It corresponds to the TF_PAD_MODE option. What it does can be found in the fastrcnn basemodel.\nIt does not matter much.. Everything I have are in the table.. Training code for the GN model is now available at https://github.com/ppwwyyxx/GroupNorm-reproduce/. You can use .ckpt as BACKBONE.WEIGHTS.\nYou'll need to build a graph that can correctly use the weights you load.\n. C for Conv. I'd recommend you to read the papers listed in the README first. They have all the answers.. It looks like what's needed from a user is just to:\n1. create a layout summary\n2. call trainer.monitors.put_summary() somewhere (e.g. in before_train, or manually outside the training loop.\nThere seems to be many many options in creating a layout:  https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/custom_scalar/custom_scalar_demo.py#L60-L97. So I'm not sure if tensorpack should do it and make those decisions in step 1. But I'm not familiar with this feature, maybe those options are not important at all.. ScalarStats uses prefix, not inference runner. You can probably do this for ScalarStats automatically. But it will be hard to do this automatically for other Inferencer.. As said in the issue template, we don't take feature requests on examples. . The log is quite clear that you have environment problems with your cuda/cudnn.. If the original examples themselves are working, it's unlikely to be a tensorpack issue.\nWe don't help people debug their code and model. If you think you found a tensorpack bug, please post more details following the issue template.\nMy suggestion is to add some tf.check_numerics ops around some suspicious ops. Use smaller reward if they are dense. And use smaller learning rate.. No I have never trained them and have no plans to do so. . The provided models are trained on COCO dataset and are just about as good as the official detectron models on this dataset. . Have no plans to do so. Duplicate of #828. Use http://tensorpack.readthedocs.io/modules/tfutils.html#tensorpack.tfutils.optimizer.apply_grad_processors with http://tensorpack.readthedocs.io/modules/tfutils.html#tensorpack.tfutils.gradproc.ScaleGradient.\nMore generally, you can implement a tf.train.Optimizer yourself by inheriting an existing optimizer such as Adam and making some modifications.. That when you need to write an optimizer yourself. Just call opt1.apply_gradients or opt2.apply_gradients in your own apply_gradients.. Sounds like you want to do something on numpy arrays. If that's the case this question is unrelated to tensorpack. \nYou can implement the function with whatever tools you like, with or without tensorflow. You can use the corresponding function in tensorflow by creating a graph, a session, and run the op.. Sounds like it was solved.\nIn general, modifying the config file is more robust than setting everything from the command line. You can also add your own command line options for the config you want to change. For example --load can be used for BACKBONE.WEIGHTS.. Please post issues following the issue template.\n\n\nDepend on what trainer you're using. Some trainer make variables local by design. Is there any reason you want them global?\n\n\nYou're recommended to use HorovodTrainer for distributed training. TensorFlow is not actively supporting its native distributed training features.. They are similar. How is this issue related to tensorpack? It sounds like you just don't have enough memory.. The input occupy the same amount of memory, but the computation is different and therefore will need different amount of memory.\n\n\nBecause it's usually faster this way.. It's answered in the issue template:\n\n\"Could you improve/implement an example/paper ?\" \n  -- the answer is: we have no plans to do so and we don't take feature requests for\n  examples. If you don't know how to do it, you may ask a usage question.. 1. Duplicate of #573 \n2. Answered in the readme already: https://github.com/tensorpack/tensorpack/tree/master/examples/DoReFa-Net#speed. 1. Yes\n2. \nAlthough the quantized activate function are applied, the conv1 still perform float32-based convolution calculations in your open implementation.\n\nYes\n\nAnd the npz model is also trained under this environment which explains the float32 weights it contains.\n\nUsing float32 weights for training is by design of the listed quantization methods. It is not about this specific implementation.. Each step is a sgd step on a batch of training data.\nThe relationship between this and frames is answered in the readme already.. I'm missing some contexts on what you're doing. I downloaded flownet2-models.tar.gz at https://github.com/lmb-freiburg/flownet2, and saw the following:\nFlowNet2/    FlowNet2-C/   FlowNet2-CS/   FlowNet2-CSS/        FlowNet2-CSS-ft-sd/  FlowNet2-S/   FlowNet2-ss/  FlowNet2-sss/\nFlowNet2-c/  FlowNet2-cs/  FlowNet2-css/  FlowNet2-css-ft-sd/  FlowNet2-s/          FlowNet2-SD/  FlowNet2-SS/\nDid you port the weights of FlowNet2-s or FlowNet2-S? What's the difference?. I hope we don't include any ops. I agree with https://github.com/tensorflow/tensorflow/pull/21392#issuecomment-412354516 that it's not easy to maintain external ops and let it build automatically. After copying the missing CMakeLists.txt from your repo it still takes me about 10 minutes to get things running. I have to deal with:\n1. Set the compiler that nvcc uses to gcc <= 5\n2. TF version must match exactly. Did some hacks but seems unable to make it work with my TF 1.9.0rc1. Reinstall TF 1.10\n3. download cub and set path\n4. change --gpu-architecture.\nThere shouldn't be so many manual steps to get an example running.. So they call it a \"patch\" but only use kernel_size=1?\nIn this case it's not too bad to write it in python with a for loop over D^2 slices, compute each slice and concat them in the end. The op is only used once so the speed may not be a huge concern.. But a CPU version would be slower than a python version running on a GPU. I made an implementation although I'm not very sure about its correctness. It's about 3~5x slower on GPU\n```python\nfrom init import correlation_cost\nh = 64\nw = 64\nCHAN = 256\nBATCH = 4\nshape = [BATCH, CHAN, h, w]\nina = tf.random_normal(shape, dtype=tf.float32)\ninb = tf.random_normal(shape, dtype=tf.float32)\nd = 20\nD = 21\nassert d % 2 == 0\nout = correlation_cost(\n        ina, inb, 1, d, stride_1=1, stride_2=2, pad=d,\n        data_format='NCHW')\ninb = tf.pad(inb, [[0, 0], [0, 0], [d, d], [d, d]])\nres = []\nfor k1 in range(0, D):\n    start_h = k1 * 2\n    for k2 in range(0, D):\n        start_w = k2 * 2\n        s = tf.slice(inb, [0, 0, start_h, start_w], [-1, -1, h, w])\n        ans = tf.reduce_mean(ina * s, axis=1, keepdims=True)\n        res.append(ans)\nres = tf.concat(res, axis=1)   # ND^2HW\ndef bench(op, warmup, iter):\n    for k in range(warmup):\n        op.run()\n    start = time.perf_counter()\n    for k in range(iter):\n        op.run()\n    duration = time.perf_counter() - start\n    return duration\nsess = tf.Session()\nwith sess.as_default():\n    o1, o2 = sess.run([out, res])\n    print(np.abs(o1 - o2).max())\nprint(bench(out.op, 10, 20))\nprint(bench(res.op, 10, 20))\n\n. What you did look similar to my code. Mine was specialized for stride_1=1 and stride_2=2.. Thanks! I can reproduce the 2.1 results. But with flownet2-s:\npython3 flownet2.py --load flownet2-s.npz  --model flownet2-s --sintel_path ~/data/Sintel/training/\n```\nI got an error of 13.365433.\nAnd with flownet2-c, I saw:\nWRN The following variables are in the graph, but not found in the dict: upsampled_flow3_to_2/bias:0, upsampled_flow4_to_3/bias:0, upsampled_flow5_to_4/bias:0, upsampled_flow6_to_5/bias:0\nwhich also gives me an error 13.36 in the end. It seems that these two models are not doing anything useful.. What's inside build_graph is your own TensorFlow code and has nothing to do with tensorpack. As a result we don't provide support on how to implement a TensorFlow model.\nDespite of that, you can refer to tensorpack examples at http://dorefa.net for a correct implementation of TTQ.. Regarding Callbacks vs SessionRunHooks:\n\nWhy aren't Callback and tf.train.SessionRunHook compatible?\n\nThey are compatible: you can convert between them with http://tensorpack.readthedocs.io/modules/callbacks.html#tensorpack.callbacks.HookToCallback .\nTensorpack callbacks exist mainly because it is written long before SessionRunHook ever exists.\nIt has one small extra feature about scheduling -- it has the concept of epoch which allows you to do something once a while. But you can easily implement this with SessionRunHook as well. \nRegarding trainer vs tf.estimator\nI personally think tf.Estimator is not well designed and therefore not flexible enough. But I also think it does have something that tensorpack trainers can learn from.\nAlso, tf.Estimator does not support efficient data-parallel training, until very recently (still in contrib). However tensorpack trainer supports it from the very beginning.. Keras is much more popular than tensorpack, so what they did is quite reasonable.. I'm apparently not an estimator user so I may be wrong. By just looking at the documentation I can identify many flexibility issues in its design.\nFrom https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator\n\nmodel_fn: Model function. Follows the signature:\nArgs:\nfeatures: This is the first item returned from the input_fn passed to train, evaluate, and predict. This should be a single tf.Tensor or dict of same.\nlabels: This is the second item returned from the input_fn passed to train, evaluate, and predict. This should be a single tf.Tensor or dict of same (for multi-head models). If mode is tf.estimator.ModeKeys.PREDICT, labels=None will be passed. If the model_fn's signature does not accept mode, the model_fn must still be able to handle labels=None.\nmode: Optional. Specifies if this training, evaluation or prediction. See tf.estimator.ModeKeys.\nparams: Optional dict of hyperparameters. Will receive what is passed to Estimator in params parameter. This allows to configure Estimators from hyper parameter tuning.\nconfig: Optional estimator.RunConfig object. Will receive what is passed to Estimator as its config parameter, or a default value. Allows setting up things in your model_fn based on configuration such as num_ps_replicas, or model_dir.\nReturns: tf.estimator.EstimatorSpec\n\nBad abstraction: it assumes all your inputs are either \"features\" or \"labels\". I often have inputs that do something else and I would feel uncomfortable calling them either \"features\" or \"labels\". Of course you can still put them into one of the two buckets but it's just ugly.\nTensorpack: everything is just \"inputs\"\n\nevaluate(\n    input_fn,\n    steps=None,\n    hooks=None,\n    checkpoint_path=None,\n    name=None\n)\n\nBad assumption: It takes an input_fn and will just use the existing model_fn with mode=EVAL.\nSo it assumes you'll want to evaluate the same model_fn with different input_fn. What if I want to evaluate also with different model_fn (which may still share all the trained parameters) ? Looks impossible. \nTensorpack: support evaluation with different input + different tower function. The code release of our recent paper (https://github.com/facebookresearch/ImageNet-Adversarial-Training/) does exactly this type of evaluation.\nLet's look at EstimatorSpec (https://www.tensorflow.org/api_docs/python/tf/estimator/EstimatorSpec) which is what model_fn is supposed to return:\n\nDepending on the value of mode, different arguments are required. Namely\nFor mode == ModeKeys.TRAIN: required fields are loss and train_op.\nFor mode == ModeKeys.EVAL: required field is loss.\nFor mode == ModeKeys.PREDICT: required fields are predictions.\n\nStrong assumption: assumes that the training \"runs one train_op in a loop\".\nTensorpack: assumes that the training is \"runs in a loop\".\nBad assumption: assumes you can compute a loss in evaluation. There are counter examples in tensorpack.\n\neval_metric_ops: Dict of metric results keyed by name. The values of the dict can be one of the following: (1) instance of Metric class. (2) Results of calling a metric function, namely a (metric_tensor, update_op) tuple. metric_tensor should be evaluated without any impact on state (typically is a pure computation results based on variables.). For example, it should not trigger the update_op or requires any input fetching.\n\nBad assumption: It assumes that metrics can be computed by ops. What if I have to compute them in Python?\nMaybe there are still ways to fetch tensors in the network and compute the metrics in Python. But then it gets unclear how to get those metrics to work with the rest of estimator (e.g., send them into tensorboard).\nIn general TF seems to assume you want to do all the work in TF (e.g., the entire dataset module), which is often just impractical.\nThese are just from a brief scan of the docs. I'm not an estimator user and these opinions may be subjective or wrong. \nAnd I would definitely agree if you say that estimator is good enough for 95% of use cases, or 100% good enough for your use cases. I think Keras is also 100% good enough for perhaps 90% of TF users.. It was handled because get_input_tensors will be called on each device, 8 times in total.. data=PlaceholderInput(). > if this means the most time cost are in backbone which has many Conv2D\nI don't know if this is true. You'll need more detailed benchmarks.\n\nHow would I improve the train speed\n\nYou can let the dataloader produce images of more similar sizes together.\n\nWhen I export TF_CUDNN_USE_AUTOTUNE=0 as you said in the link ,the step cost is still very different and no improve of performence\n\nThe image sizes are still different so this option does nothing useful for this issue.. monitors only take care of where to put logging data. It does not manage what to log and when they are produced. See http://tensorpack.readthedocs.io/tutorial/summary.html#tensorflow-summaries\nFor your use case it's better to read http://tensorpack.readthedocs.io/tutorial/faq.html#how-to-print-dump-intermediate-results-in-training. What is your question?. It will improve performance at the beginning as mentioned in the fastrcnn notes.\nI don't know whether it improves overall performance or not.. 1. warmup mainly means autotune\n2. the number is a rough estimate and has no mathematical meaning\n3. (1) you can write a callback and put a timer in trigger_step\n    (2) I don't know how to get time spent of a specific op in tensorflow except to enable tracing and parse the result. What is your question?\nYour log shows it's not 0.5it/s but >1.3it/s.. 2 GPUs also make tensorflow warmup faster because it sees more shapes.. The export will not work because tensorpack enable autotune when import.\nI'm sure multi-gpu is not faster than 1 gpu if you wait for like 50 epochs.. If you still see multi-gpu running faster than 1 gpu, please run it with no modification to the code and post your command and full logs like what's in the issue template.. I run some tests with today's master code and --config MODE_MASK=True MODE_FPN=True on 1 & 8 V100s, with export TF_CUDNN_USE_AUTOTUNE=0 so the speed is faster and more stable at the beginning.\nStarting from the 2nd epoch, 1 V100 takes 2min29s per epoch and 8 V100s take 2min51s per epoch.. http://tensorpack.readthedocs.io/tutorial/trainer.html#tower-trainer\nIf you can't figure out what's wrong after reading the above tutorial, post your layer implementation.. The link above explicitly said do not use tf.Variable. It is a bug introduced two days ago and it's now fixed. Thanks!. You should reinstall tensorpack using the command in the README. I've run into the same issue (not NaN, but segfault) with horovod before, not sure which library is causing it.\nThe replicated trainer should be more stable to use.. It may also help to set NCCL_DEBUG=INFO to see if there is any issue in communication. Also make sure horovod was rebuilt and reinstalled after each time you install tensorflow.. The NaN issue turns out to because the learning rate wasn't correctly scaled for <8 GPUs. The code was only tested with 8+ GPUs. I'll push a fix for that one later.. Closing as the crash does not seem to be a tensorpack issue. Uninstall and reinstall horovod is worth trying.. Thanks. The code works for newer version of tensorflow. But I'll adopt your fix.. There is an inference function in the TTQ example code already:\nhttps://github.com/tensorpack/tensorpack/blob/17955e8955284d9f91f079dd515ebf1b1a29ec9b/examples/DoReFa-Net/alexnet-dorefa.py#L169-L193. 1. quantize the saved weights by running fw() on them, as mentioned in https://github.com/tensorpack/tensorpack/issues/573\n2. change the model so it does not run fw() on the weights.\nThe 1. and 2. cancel each other so there is actually no point doing so.. If you're asking about what each class means in cifar10, that's not a feature tensorpack provided, although it will make sense to add it. Now please figure that out yourself.. You can. And you probably don't want to nest them.. Use https://www.tensorflow.org/api_docs/python/tf/train/NewCheckpointReader to read your checkpoint.\nOr with tensorpack predictor, add the name of variables to output_names.\nOr use tf.Print.. https://tensorpack.readthedocs.io/modules/tfutils.html#tensorpack.tfutils.varmanip.load_chkpt_vars\nhttps://tensorpack.readthedocs.io/modules/tfutils.html#tensorpack.tfutils.varmanip.save_chkpt_vars. I have no context of what you're doing or how you met the error so I can't help. The error is unrelated to tensorpack so you'd better solve it by learning tensorflow.. The model is a standard TF checkpoint so you can read it with tf.train.NewCheckpointReader, or use https://tensorpack.readthedocs.io/modules/tfutils.html#tensorpack.tfutils.varmanip.load_chkpt_vars.\nAfter loading a model you can refer to numpy documentation on how to save it into numpy format.. This is a tensorflow question and not related to tensorpack.. Build the graph yourself (by calling build_graph, for example) in inference mode only.\nhttps://tensorpack.readthedocs.io/tutorial/inference.html\nThen follow tensorflow documentation on how to use tensorboard.. @brisker  if you mean during training: https://tensorpack.readthedocs.io/tutorial/faq.html#how-to-print-dump-intermediate-results-in-training\n@minhson https://tensorpack.readthedocs.io/tutorial/save-load.html#load-a-model. It's a callback and you can read the relevant tutorials on how to use it: https://tensorpack.readthedocs.io/tutorial/callback.html. You can use inputs with known batch size. The batch size of the tensor is the batch size of your dataflow.. The answer will be in your code which I don't know. If it's the examples then it should be 32.. Please post any problems following the issue template.. You can let weight decay be a tensor that's computed from global step (tf.train.get_or_create_global_step IIRC). This is unrelated to tensorpack.. Every tensor has a name.\nYou can see the name of a tensor by print(a). Or you can give it a new name by a = tf.identity(a, name='name').. Yes.. PeriodicCallback. https://tensorpack.readthedocs.io/modules/callbacks.html#tensorpack.callbacks.EnableCallbackIf. https://tensorpack.readthedocs.io/modules/tfutils.html#tensorpack.tfutils.get_global_step_var if you want to use it in the graph and https://tensorpack.readthedocs.io/modules/tfutils.html#tensorpack.tfutils.get_global_step_value if you want to use it in python.. You cannot use \"value\" of tensor in a layer. Inside a layer you're defining the graph therefore you have to use symbolic tensor. When you define the graph there is no actual value of global step at all.. You cannot use get_global_step_value when you define a graph because the training has not started and there is no value. You can only use get_global_step_var. Yes.\nAlso you cannot use if step == 100 in graph definition because when you define the graph, there is no value and the comparison is impossible. You need to use tf.cond and other related functions.\nI recommend you to read some tutorials about the concept of tensorflow graph. . You can use tf.cond.. These questions you've been asking are unrelated to @minhson 's issues (in fact this one is unrelated to tensorpack either). Please use a new issue thread for different questions.. There is no difference. They will give you the same result under the same inputs provided if your model is deterministic.. https://tensorpack.readthedocs.io/_modules/tensorpack/predict/dataset.html#SimpleDatasetPredictor\nIf you can't figure out, please post an issue following the issue template with a reproducible example.. Your third column is way too fast. I believe you've made mistakes in your code or setup or command and it's actually doing something other than what you think.. As a reference, when trained from scratch with standard settings, the time for the first several epochs on 8 V100s should be: 17min, 9min, 7min, 6min, 5min, for the same R50C4 model.. I run some tests with today's master code and --config MODE_MASK=True MODE_FPN=True on 1 & 8 V100s, with export TF_CUDNN_USE_AUTOTUNE=0 so the speed is faster and more stable at the beginning.\nStarting from the 2nd epoch, 1 V100 takes 2min29s per epoch and 8 V100s take 2min51s per epoch.. Upgrade tensorpack.. Did the training stop or not?\nIf yes, either upgrade your hardware or use a less memory-consuming model (e.g. FPN).. Then it's normal.\nTF sometimes prints unharmful error.. Please post issues following the issue template. Evaluating (instead of finetuning) the model will give you the correct performance. (If not, upgrade fasterrcnn code as well)\nIf you are asking why finetuning performance is not what you expected, that's a machine learning question so we don't answer it. A smaller learning rate is definitely needed. . No I don't have the old models.. Actually with TF 1.4 the code should be able to run most of the times. It may crash for some inputs due to TF bugs.. __iter__ and __len__ sounds good!\nFor compatibility, we need to consider \n\ncalling old code with new method\ncalling new code with old method.\n\nHaving get_data wrap __iter__ would only solve 2. Having them wrap each other sound weird.. > I guess overwrite get_data in the base class is sufficient:\nThis does not solve the case to call old code (user-writter dataflow) with new method (iter).. I do hope nothing breaks.. I want to learn if there are any magic alternatives, before adopting a breaking change and worrying about version numbers... Something like this would be cleaner:\n```python\nimport six\nclass MyMeta(type):\n    def new(meta, name, bases, dct):\n        print(\"New class\", name)\n        if 'iter' not in dct:\n            print(\"Rename!\")\n            dct['iter'] = dct.pop('get_data')\n        return super(MyMeta, meta).new(meta, name, bases, dct)\n    def init(cls, name, bases, dct):\n        print(\"Initializing class\", name)\n        super(MyMeta, cls).init(name, bases, dct)\n@six.add_metaclass(MyMeta)\nclass MyBase(object):\n    def iter(self):\n        raise NotImplementedError\ndef get_data(self):\n    return self.__iter__()\n\nclass MyDerived(MyBase):\n    def get_data(self):\n        yield \"Derive\"\nx = MyDerived()\nprint(list(x.get_data()))\nprint(list(iter(x)))\n```. It was added. But in general most dataflows still assume list instead of dict. #768 aims to make dataflow work for dicts.. Yes. Setting the capacity to 1 is enough to achieve what you said because the staging op will block if capacity is reached.\nThat sounds like a reasonable thing to do. However some benchmarks will be needed to see how it affects speed or memory.. Capacity is now set to 1.. I revert the change of interface on InputSource. I think len doesn't apply very well since InputSource is not really iterable. The size is more like an attribute of it.\nAlso, from https://docs.python.org/3/library/functions.html#len:\n\nlen(s)\nReturn the length (the number of items) of an object. The argument may be a sequence (such as a string, bytes, tuple, list, or range) or a collection (such as a dictionary, set, or frozen set).\n. StagingInput(QueueInput(...)).\nBy calling launch_train_with_config, most examples already do this automatically.. Yes. Please post issues following the issue template (new issues -> unexpected problems).. upgrade tensorpack from github.. I assume the problem was easily solved.. https://tensorpack.readthedocs.io/modules/callbacks.html#tensorpack.callbacks.PeriodicTrigger\n. Can you make sure your dataflow can actually run? You can check it by TestDataSpeed(ds).start().\nIf it does, what does the elements look like? Can you print the first element?\nYou can print it by:\npython\nds.reset_state()\nprint(next(ds.get_data()))\nIf you don't know how to write a dataflow, read https://tensorpack.readthedocs.io/tutorial/extend/dataflow.html .\nData does not have to come from files. It can be arbitrary python code.. Dataflow is unrelated to tensorflow, it should produce numpy arrays.\n\nTo use tensorflow tensors as inputs, you can check the \"input pipeline\"\ntutorial for options other than dataflow.\nOn Wed, Aug 29, 2018, 6:30 PM Qi Fan notifications@github.com wrote:\n\nThank you for your kind reply!\nWhen run\nds.reset_state()\nprint(next(ds.get_data()))\nTestDataSpeed(ds).start()\n``\nThe print is :\n=================\n[, ]\n0%| |0/5000[00:00<?,?it/s]\n[0830 01:19:38 @param.py:194] Use train_log/hed/hyper.txt to set\nhyperparam: 'learning_rate'.\n[0830 01:19:42 @training.py:51] [DataParallel] Training a model of 4\ntowers.\nMy generator is:\nslim = tf.contrib.slim\ndataset_data_provider = slim.dataset_data_provider\ndef _get_data(data_provider, dataset_split):\nif common.LABELS_CLASS not in data_provider.list_items():\nraise ValueError('Failed to find labels.')\nimage, height, width = data_provider.get(\n[common.IMAGE, common.HEIGHT, common.WIDTH])\nSome datasets do not contain image_name.\nif common.IMAGE_NAME in data_provider.list_items():\nimage_name, = data_provider.get([common.IMAGE_NAME])\nelse:\nimage_name = tf.constant('')\nlabel = None\nif dataset_split != common.TEST_SET:\nlabel, = data_provider.get([common.LABELS_CLASS])\nreturn image, label, image_name, height, width\ndef get(dataset,\ncrop_size,\nbatch_size,\nmin_resize_value=None,\nmax_resize_value=None,\nresize_factor=None,\nmin_scale_factor=1.,\nmax_scale_factor=1.,\nscale_factor_step_size=0,\nnum_readers=1,\nnum_threads=1,\ndataset_split=None,\nis_training=True,\nmodel_variant=None):\nif dataset_split is None:\nraise ValueError('Unknown dataset split.')\nif model_variant is None:\ntf.logging.warning('Please specify a model_variant. See '\n'feature_extractor.network_map for supported model '\n'variants.')\ndata_provider = dataset_data_provider.DatasetDataProvider(\ndataset,\nnum_readers=num_readers,\nnum_epochs=None if is_training else 1,\nshuffle=is_training)\nimage, label, image_name, height, width = _get_data(data_provider,\ndataset_split)\nif label is not None:\nif label.shape.ndims == 2:\nlabel = tf.expand_dims(label, 2)\nelif label.shape.ndims == 3 and label.shape.dims[2] == 1:\npass\nelse:\nraise ValueError('Input label shape must be [height, width], or '\n'[height, width, 1].')\nlabel.set_shape([None, None, 1])\noriginal_image, image, label = input_preprocess.preprocess_image_and_label(\nimage,\nlabel,\ncrop_height=crop_size[0],\ncrop_width=crop_size[1],\nmin_resize_value=min_resize_value,\nmax_resize_value=max_resize_value,\nresize_factor=resize_factor,\nmin_scale_factor=min_scale_factor,\nmax_scale_factor=max_scale_factor,\nscale_factor_step_size=scale_factor_step_size,\nignore_label=dataset.ignore_label,\nis_training=is_training,\nmodel_variant=model_variant)\nsample = {\ncommon.IMAGE: image,\ncommon.IMAGE_NAME: image_name,\ncommon.HEIGHT: height,\ncommon.WIDTH: width\n}\nif label is not None:\nsample[common.LABEL] = tf.squeeze(label, -1)\nif not is_training:\nOriginal image is only used during visualization.\nsample[common.ORIGINAL_IMAGE] = original_image,\nnum_threads = 1\nyield [sample[common.IMAGE], sample[common.LABEL]]\nAnd I generate my dataflow through:\ndef get_data():\ndataset = segmentation_dataset.get_dataset(\nFLAGS.dataset, FLAGS.train_split, dataset_dir=FLAGS.dataset_dir)\ntf.gfile.MakeDirs(FLAGS.train_logdir)\ntf.logging.info('Training on %s set', FLAGS.train_split)\nsamples = MyDataFlow(\nsamples = input_generator_sync.get(\n      dataset,\n      FLAGS.train_crop_size,\n      FLAGS.train_batch_size,\n      min_resize_value=FLAGS.min_resize_value,\n      max_resize_value=FLAGS.max_resize_value,\n      resize_factor=FLAGS.resize_factor,\n      min_scale_factor=FLAGS.min_scale_factor,\n      max_scale_factor=FLAGS.max_scale_factor,\n      scale_factor_step_size=FLAGS.scale_factor_step_size,\n      dataset_split=FLAGS.train_split,\n      is_training=True,\n      model_variant=FLAGS.model_variant)\nds = DataFromGenerator(samples)\nds = BatchDataByShape(ds, 8, idx=0) # when remove it and the PrefecthDataZMQ, the test works.\nds = PrefetchDataZMQ(ds, 1)\nreturn ds\nand\ndataset_train = get_data() #get_data('train')\ndataset_train = input_generator_sync()\nsteps_per_epoch = 100 # dataset_train.size() * 40\ndataset_val = get_data('val')\nprint '================='\ndataset_train.reset_state()\nprint(next(dataset_train.get_data()))\nTestDataSpeed(dataset_train).start()\nAnd the dataset is generated by:\ndef get_dataset(dataset_name, split_name, dataset_dir):\n\"\"\"Gets an instance of slim Dataset.\nArgs:\ndataset_name: Dataset name.\nsplit_name: A train/val Split name.\ndataset_dir: The directory of the dataset sources.\nReturns:\nAn instance of slim Dataset.\nRaises:\nValueError: if the dataset_name or split_name is not recognized.\n\"\"\"\nif dataset_name not in _DATASETS_INFORMATION:\nraise ValueError('The specified dataset is not supported yet.')\nsplits_to_sizes = _DATASETS_INFORMATION[dataset_name].splits_to_sizes\nif split_name not in splits_to_sizes:\nraise ValueError('data split name %s not recognized' % split_name)\nPrepare the variables for different datasets.\nnum_classes = _DATASETS_INFORMATION[dataset_name].num_classes\nignore_label = _DATASETS_INFORMATION[dataset_name].ignore_label\nfile_pattern = _FILE_PATTERN\nfile_pattern = os.path.join(dataset_dir, file_pattern % split_name)\nSpecify how the TF-Examples are decoded.\nkeys_to_features = {\n'image/encoded': tf.FixedLenFeature(\n(), tf.string, default_value=''),\n'image/filename': tf.FixedLenFeature(\n(), tf.string, default_value=''),\n'image/format': tf.FixedLenFeature(\n(), tf.string, default_value='jpeg'),\n'image/height': tf.FixedLenFeature(\n(), tf.int64, default_value=0),\n'image/width': tf.FixedLenFeature(\n(), tf.int64, default_value=0),\n'image/segmentation/class/encoded': tf.FixedLenFeature(\n(), tf.string, default_value=''),\n'image/segmentation/class/format': tf.FixedLenFeature(\n(), tf.string, default_value='png'),\n}\nitems_to_handlers = {\n'image': tfexample_decoder.Image(\nimage_key='image/encoded',\nformat_key='image/format',\nchannels=3),\n'image_name': tfexample_decoder.Tensor('image/filename'),\n'height': tfexample_decoder.Tensor('image/height'),\n'width': tfexample_decoder.Tensor('image/width'),\n'labels_class': tfexample_decoder.Image(\nimage_key='image/segmentation/class/encoded',\nformat_key='image/segmentation/class/format',\nchannels=1),\n}\ndecoder = tfexample_decoder.TFExampleDecoder(\nkeys_to_features, items_to_handlers)\nreturn dataset.Dataset(\ndata_sources=file_pattern,\nreader=tf.TFRecordReader,\ndecoder=decoder,\nnum_samples=splits_to_sizes[split_name],\nitems_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\nignore_label=ignore_label,\nnum_classes=num_classes,\nname=dataset_name,\nmulti_label=True)\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/tensorpack/tensorpack/issues/877#issuecomment-417160022,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABUTtTue9JzC-0Jzj1nK68tged5D6Irwks5uV0A9gaJpZM4WReTN\n.\n. Next time please post full error. I do not know how you get those errors but it feels like you did not use the correct API: https://tensorpack.readthedocs.io/modules/train.html#tensorpack.train.TrainConfig\n\nYou should use dataflow= if using a dataflow, or data= if using an InputSource.. No. As the document said, data= takes a InputSource. FIFOQueue is not an InputSource. Please use the functions according to the docs.\nIf you have created a tf.data.Dataset, you can use TFDatasetInput. If you have a function that creates tf.Tensors directly, use TensorInput.. InputSource has to produce list as inputs that match the order you declared in your model.\nYou can make a list from a dict by [dic['image'], dic['label']], assuming that's the order you declared them.. As the document of TensorInput said, it takes a function which returns a list, not a list directly.. Thanks! I'll refine the docs. 1. contributions are welcome\n2. No. all trainers in tensorpack are single-cost trainers, as you can see here: https://tensorpack.readthedocs.io/modules/train.html\n3. both are ok\n4. Would you elaborate?. Where do you think it is not right?\nIt's faster to reduce the gradients.. Because it's faster.. If you reduce the loss and compute gradients on the reduced loss, tensorflow will copy the gradients among GPUs.. https://github.com/tensorpack/tensorpack/issues/833#issuecomment-406142988. Please post issues following the issue template (New Issue -> Unexpected Problems).\nIt looks like you're loading a model that does not match your graph definition, and from the logs it looks like the graph definition is not using any tensorpack layers. So the issue does not seem to be related to tensorpack.. Since your model is not originally defined by tensorpack's layer, you should not use tensorpack's layer to define it when you load it. You should refer to whoever provides the model for the code that defines the model.. I think most code in tensorflow/models/research are too specific (therefore only useful to a small group of people) or poorly maintained.\nThe imagenet models there https://github.com/tensorflow/models/tree/master/research/slim are worth porting. Some of the translation models also look good but I'm not very familiar with them.. I was thinking only one or two files, that construct the network by importing their code and do the inference. That provides an easy access to those pretrained models. \nI would not want to train them again or copy-paste their code.. The npz format contains key-value pairs that stores the trained parameters. With the example code that builds the graph, it is sufficient to restore and use the model, because now you have both the graph and its parameters.\nHowever, if you want to use the model with a particular tool, you'll need to figure out what format it needs and how to construct a model which the tool can use. Maybe the export_compact function in examples/basics/export-model.py can help, but it all depends on what your tool needs. I'm not familiar with TOCO and inference tools are out of the scope of tensorpack.. > Which export.py it refers to (maybe export-model.py)?\nYes. export-model.py.\nYou can copy the export_compact function into your shufflenet code:\npython\n    pred_config = PredictConfig(\n        session_init=get_model_loader(model_path),\n        model=MyShuffleNetModel(),\n        input_names=['my input names'],\n        output_names=['output names I need'])\n    ModelExporter(pred_config).export_compact('/tmp/compact_graph.pb')\nThis gives you a serialized tf.Graph as a pb file but you need to figure out whether this pb file is in the same format that your tool needs. \"pb file\" is not a format.. Your question is unrelated to this issue. Please open a new one and follow the issue template (click \"New Issue\" -> \"Unexpected Problems / Bugs\", or visit this link to post issues about unexpected problems).\ninput_names and output_names can be any tensor defined in the graph. If you want to input an image, the name of that tensor is \"input\".. It seems that while TF is been moved to TF2, the official model repo will be unmaintained and rely on deprecated slim. Therefore closing this issue.\nIt's likely that TF will create a new set of pre-trained models/code for TF 2 and we can asses it then.. Please provide more details following the issue template.\nIn particular, please provide the command you use to:\n1. get the source code to run\n2. run. The issue template has something more. This is still missing:\n\n\nWhat you observed, including but not limited to the entire logs.\n  Better to paste what you observed instead of describing them.. The code works well for its original purpose (training on COCO dataset).\nSince you're using it for a different task you'll be responsible for making any necessary changes, including changing hyperparameters or data processing, to make it work. \nFor example, if many of your image does not have a ground truth box, it could easily make training unstable. And you'll need to figure out the best setting to train it (e.g. change the fg/bg sampe strategy, etc).\n\n\nSuch problem is not considered a tensorpack issue, unless you have reasons to believe it's actually caused by a bug in the code rather than the change of dataset.\n. Honestly I don't like the tf-lite and mobile thing in general because my impression is that they are premature and won't work well for any actual model I care about... Last time I tried those graph transform, it does not do what I want (fuse BN into conv layers) and I end up rewriting the graph and transformed the weights manually.. The features to be added to the library by this PR seems to be just: exporting the model to saved_model format (for serving) or a pruned graph format.\nI think the features make sense as part of a training framework because the output can be seen as a result of the training. After exporting to standard format a user can then do other fancy stuff such as toco or tensorrt conversion which I don't want to include in tensorpack.\nFor implementation I think it's simpler to just have the old name (ModelExporter) and two methods for two different formats. Also the pruned graph does not seem to be very related to \"mobile\" so maybe a better name is needed?. https://tensorpack.readthedocs.io/tutorial/save-load.html. See the tutorial on writing a dataflow.. You're right. To be more precise it should handle the case of zero. But the probability the weight become exactly zero is very small, so in practice it does not make a big difference.. Considering that 150/512=0.3, 460/512=0.9, your speed numbers look just reasonable. What do you expect?. There is no concept of image or batch in dataflow. It just gives you \"datapoints\", whatever it is.. Please post issues following the issue template.. Closing because there is no response.. I think you're right and __class__ should not be set.. learning rate is a variable defined in optimizer(). You can define it in build_graph() instead and access it in optimizer().\nThe current exploration prob is not a tensorflow variable so there is no way to access it in the graph. You need to modify many code to do that.. If weights in your model are fp16 then you certainly need to load fp16 weights.\n\nDo I have to load the numpy dict in python and then cast to numpy float 16?\n\nyes.. The last one is correct. If something fails, please post the issue following the issue template.. Closing since there is no response.. Thanks \u653e\u795e\uff01. 1. It is from personal communication with the author. It helps a bit.\n2. https://github.com/tensorpack/tensorpack/blob/0dbcbac7ae5dc62f89ecdbe0961731513c11f2ec/examples/FasterRCNN/config.py#L181. MultiGPUGANTrainer automatically adds StagingInput. You have to remove StagingInput in your data because StagingInput cannot be nested together.\nAfter the above commit it should now throw an error about this situation.. The ':0' is probably not an issue (although the warnings are indeed confusing, and should be improved). The issue is you have \"-\" in the graph but \"_\" in the checkpoint as you can see from the warnings.\nAs for the shapes, I recommend you to check more carefully, and copy your old model code as much as possible.. I ran some tests with the cifar-convnet.py example. After those changes I'm not able to get deterministic results either, and observed similar gap in validation error.\nHowever running the same code on CPU gave me deterministic results (up to the very last digit).\nSo I guess it's just how the ops work on GPU and there isn't much we can do in tensorpack.. There are many related issues in tensorflow, e.g. https://github.com/tensorflow/tensorflow/issues/18096 . Closing here.. The readme has a mistake.\nI cannot find the number 18000 in DDQN paper, but let's take the code as ground truth.. This is an issue I fixed before but forgot to take a note about it. Unfortunately it was introduced again lately..\nI will think about how to properly address this. . Like it said you cannot do multiple processing within a MPI process.\nYou can start separate processes that run data processing and send the data to the training process. This is done in the example here https://github.com/tensorpack/benchmarks/tree/master/ResNet-Horovod.. Any subset of pixels in an image can be represented by a list of polygons (in the extreme case you can represent each pixel by one polygon), so there should be no issues with the use of polygon. I do not see how the line of code you refer to is related to the discussion.\nIf you want to use masks for augmentation you can still augment each pixel in the mask.. If you use only one polygon for the object with holes then that's a problem of the dataset.\nLike I said any subset of pixels in an image can be represented by a list of polygons . That again is just a problem with the dataset. You can apparently represent a donut by two polygons: the left part and the right part or the donut. If your data is not annotated like that there is nothing I can do.. And again, if your data is originally in the form of masks, you can just augment all the pixels directly without converting to polygons.. augment_coords is the function to use.. Or you can just use augment if your label is image... Examples that use augment to augment image and segmentation: #214 \nExamples that use augment_coords to augment image and segmentation: https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN. Fixed in master already.. What parameter to use is up to you. You're welcome to share new results with different parameters.. See the tutorial on summary and logging.. According to the tutorial, the quickest way is to use this callback.\nNote that by default this callback controls all the summaries. If you only want to customize certain summaries, you'll need to add those summaries into a different collection: https://www.tensorflow.org/api_docs/python/tf/summary/scalar. Also #923, #509.. Without code I can't tell much about the issue. A stuck can be caused by issues with your data, your model, or some unexpected use of the library (e.g. wrong use of StagingInput). I would recommend to replace your dataflow by FakeData to see if your issues still exist or start from a working example to see where things went wrong.\nFor your dataflow testing: it should run until the end of your dataflow (which is the length of your list divide by the batch size). You can make a dataflow infinite by RepeatedData(ds, -1), which is automatically done if you use a dataflow for training.. If FakeData has the same issue, next step is to replace your model with a trivial model (e.g. return tf.reduce_sum(input + tf.get_variable('var', shape=[], dtype=tf.float32)).\nIf the problem still exists, please post significant parts in the rest of code because I assume there isn't much code left after removing data and model.\nAlso, please make sure to try with the latest tensorpack.. Could you try SyncMultiGPUTrainerReplicated? . It doesn't matter.\nWith ParameterServer I'm still able to use sync_statistics.\nCould you try a naive model with only one batchnorm layer and please make sure you're using a constant layer name (BatchNorm('layername', inputs)) ?. It would still be interesting to know why it does not work for you when using ParameterServer.\nCan you try ParameterServer with a naive model?\nCan you make sure that the layername in your model does not depend on other factors such as variable scope? Different GPUs have to have those BatchNorm layers under the same name to be able to sync statistics.. ```diff\ndiff --git i/examples/basics/cifar-convnet.py w/examples/basics/cifar-convnet.py\nindex 81e59e2c..2339c098 100755\n--- i/examples/basics/cifar-convnet.py\n+++ w/examples/basics/cifar-convnet.py\n@@ -45,7 +45,8 @@ class Model(ModelDesc):\n     image = image / 4.0     # just to make range smaller\n     with argscope(Conv2D, activation=BNReLU, use_bias=False, kernel_size=3), \\\n\n\nargscope([Conv2D, MaxPooling, BatchNorm], data_format=data_format):\nargscope([Conv2D, MaxPooling, BatchNorm], data_format=data_format), \\\nargscope([BatchNorm], sync_statistics='nccl'):\n             logits = LinearWrap(image) \\\n                 .Conv2D('conv1.1', filters=64) \\\n                 .Conv2D('conv1.2', filters=64) \\\n``\nThe above change on the cifar example (which usesSyncMultiGPUTrainerParameterServer`) can run well. Does it run on your machine?. Since the cifar example is naive enough and it works on your machine as well, your code probably is not naive and may have some interesting parts that make it hang. I think that's where  you want to look at.. Adding things to such collections is unsafe, as mentioned in the tutorial: https://tensorpack.readthedocs.io/tutorial/trainer.html\n\n\nIt may get called multiple times for data-parallel training or inference. As a result:\nYou\u2019ll need to be careful when modifying global states, e.g. adding ops to collections, setting attributes of a model instance.\n\nSo your loss in the last GPU will contain the loss tensor or previous GPUs, that may be the issue.\nYour questions:\n1. Per-step. They are mathematically equivalent.\n2. The docs of these two class has explained it.. Nothing special happens to losses (if you don't use the collections). It is the gradients that are averaged, not the loss, as said in the above docs you quote.\nWhen you use the collections, the losses in one GPU may contain losses from other GPUs, depend on what you've put in the collection.. The method is correct. As for why the value is periodic, maybe your validation data is not written to produce the same data each time. Or it may be just how your training works: such thing does happen in certain types of training (e.g. when you don't shuffle the training data).. >  I tried to build multiple models using model.get_logits(image) under TowerContext, and feed the scaled input image which is a tf queue tensor, then restore the model variables using SaverRestore(), and use sess.run(final_pred) to get the final prediction. \nI'm not completely sure what you did and not even sure if any steps above (except for building the model) involves tensorpack. You need to set variable scope to reuse for multiple calls to get_logits, and if possible use placeholders to find bugs easier. Print tf.global_variables to see what variables you've created, etc.\nTo report any unexpected issues please use the issue template.. This issue seems unrelated to tensorpack because it sounds like an issue running your own code.. Confirmed that this is not a tensorpack issue. Closing now.\nWe are sorry for your problems but this is not the place for tensorflow questions.. I started to see the same issue after an upgrade of tensorflow. The fix is :\npython\nfrom tensorflow.contrib.nccl.python.ops.nccl_ops import _validate_and_load_nccl_so\n_validate_and_load_nccl_so()\n. I'm not sure what you're asking. If you're asking about why the model does not perform well on your data, we do not answer such questions because it's a general machine learning / computer vision question and not necessarily related to tensorpack.. The warning said  \"The following variables are in the checkpoint, but not found in the graph\". It means some variables are used in training but not found (i.e. not used) in inference, which is OK.\nIt would not be OK if it's the other way around.. 1. \"some variables are used in training but not found (i.e. not used) in inference\", and in the models I provided, I have manually removed such variables, so you won't see any warnings.\nAs for why your model does not perform well, it is a machine learning/computer vision question that we'll not be able to answer.\n\nYou do not need to use that format because tensorpack recognize both. To make the conversion, load and save the variables with https://tensorpack.readthedocs.io/modules/tfutils.html#tensorpack.tfutils.varmanip.save_chkpt_vars. I believe this is a tensorflow question (i.e. how to build such a graph). You'll need to customize the gradients of quantized convolution, just like how the gradient of quantization is customized now in dorefa.py. You can refer to tf.custom_gradient to learn more.. Tensorpack does not and will not provide any non-standard layers like this. Whatever models it supports will be implemented in tensorflow anyway so there is nothing you need from tensorpack.\n\nThe thing you described can be implemented with custom_gradient. It allows you to do any computation with any tensors in the backward. And in fact it's best to not involve tensorpack layers and implement your customized version of layers in this case, because you'll need to access the full-precision tensors in the gradient function. . Yes, something like that should work. These kind of things are usually not very easy to implement correctly so be careful.. In terms of the backward, you can either  (1) implement it yourself by calling conv2d_transpose, or (2) implement a new forward pass and call tf.gradients.\nOn this point I think TF is more flexible than any other frameworks. Others only allow you to do (1).. And it does not require a separate instantiation of variables. Variables can be created in side the decorated function and the documentation has mentioned how to do this.. They can both do (1). But in tensorflow you can do:\n@tf.custom_gradient\ndef func(x, W, QW):\n  o1 = f1(x, QW)\n  o2 = f2(x, W)\n  def grad(dy):\n      return tf.gradients(o2, [x, W, QW], grad_ys=[dy])\n  return o1, grad\nI don't know if this is easy to do in pytorch or not.. I would recommend you to copy them because those code can change in any incompatible ways. This is how I work now. I would not want to see a new user starting to import these.\nWhat about a huge warning at import saying these code can break?\nOr an alternative: allow the import, but don't include any example files in pip install. In this way a normal user will have no chance relying on these code with no compatibility. And only developers (which use tensorpack locally and hopefully know the risks) are able to import it. . Locally I have a lot of copies of \"imagenet_utils.py\" around as well. Isn't there a time you want to modify GAN.py very quickly but don't want to affect the other code? That's why I prefer copying myself.. I do not know how to achieve this without any super ugly path hacking, because examples is not under tensorpack.. I can reproduce the issue, however the same thing works for TF 1.10.\nThis could be a bug in tensorflow and I'll investigate. For the moment please use 1.10 instead.. Closing since it's a TF bug. Tracking https://github.com/tensorflow/tensorflow/issues/22750 instead.. I don't remember any specific reasons we use abs() for SVHN. I think clip(abs(x), 0, 1) and clip(x, 0, 1) should both work well as an activation function.\nIIRC it's just one of the things we tried and happened to use on SVHN but not AlexNet, for no particular reasons. . The code was updated to use the same activation as alexnet to avoid confusion. It has similar performance.. Anything other than the training is done in the callback. Looking at https://tensorpack.readthedocs.io/tutorial/extend/callback.html, there are two ways you can add this logic:\n```python\nclass A(Callback):\n    def before_run(self, ):\n        return tf.train.SessionRunArgs(fetches=[], feed_dict={'learning_rate:0': np.random.rand()})\nclass B(Callback):\n    def _setup_graph(self):\n        self._lr = [k for k in tf.global_variables() if k.name == 'learning_rate:0'][0]\n    def _trigger_step(self):\n        self._lr.load(np.random.rand())\n```. It will not be an input. It can be any tensor that you've previously defined.. The code you posted is correct. I believe you've deleted the buggy part when you simplify your code for posting.. In case you want to do training twice in one script, you need to create a new graph for each training. That's one possibility of your bug.. get_current_tower_context can only be called in the tower function (i.e. the forward function you wrote.)\ntrigger_step is only called for each training step. (See the callbacks tutorial). So what you need is already done.. It's answered in the readme. The models were trained on trainval35k which overlaps with val2014.. As a tutorial said: https://tensorpack.readthedocs.io/tutorial/summary.html, the logging frequency is controlled by the MergeAllSummaries callback.. Also a duplicate of #509 and #911.. Then you need to also mark \"what\" to summarize following the tutorial.\nWhen you create the summaries with tf.summary.xxx, tag it with a different collection. See https://www.tensorflow.org/api_docs/python/tf/summary/scalar.\nThen call MergeAllSummaries with the non-default collection name.\nThe tutorial reads:\n\nMergeAllSummaries callback is in the default callbacks. It runs ops in the SUMMARIES collection (by default) every epoch (by default), and writes results to the monitors.\n\n\"by default\" implies that you can change it.. Please remove BNLeakyReLU. Tensorpack will not add any symbolic layers. The existing layers exists only because there were no alternatives at the beginning. See more at the tutorial: https://tensorpack.readthedocs.io/tutorial/symbolic.html. A \"tower\" is a tensorflow terminology which roughly means the model's forward function. More in the tutorial: https://tensorpack.readthedocs.io/tutorial/trainer.html#what-is-tower-function\nThe tower function can be called many times for different purposes (e.g. multi-gpu training, inference). Tensorflow collection is a per-graph global data storage, and modifying global state (especially for multiple times) can lead to bugs that are hard to find. Therefore tensorpack prints such messages as info.\nIn your case it can be safely ignored.. It seems it's supported since TF1.8: https://github.com/tensorflow/tensorflow/commit/5c469e6bafb479ef110b2f02f070507a3711664d#diff-280d77a571d7e6e2e5c11563762014b2\nPlease from tensorpack.tfutils.common import get_tf_version_tuple and only enable this feature when the version matches.. Thanks!. That's not how the code is supposed to be used. You only need to call\npython\n    predictor = tp.OfflinePredictor(tp.PredictConfig(\n        model=Model(),\n        session_init=tp.get_model_loader(model_path),\n        input_names=['image'],\n        output_names=['saliency']))\nonce because it builds the graph and load the models. Calling it many times under the same graph will make tensorflow throw the above error.\nThe predictor, after being built, can be used many times. . Many functionality in tensorpack requires some extra ops to be run together with each step. For example, in your case the use of StagingArea requires the stage&unstage ops to be run to fill the data buffer. Otherwise the training will hang due to empty data buffer.\nAs a result there is a trainer.hooked_sess that will automatically do the above for you and that's what you should use.\nNote that it will also do the following: (1) waste a data point for this sess.run instead of for training. (2) increase global_step. This may not be what you want. But this is also inevitable if you want to make a new sess.run call. That's why it's usually better to use before_run/after_run callback to run things along with the training iteration instead of after the iteration. (https://tensorpack.readthedocs.io/tutorial/extend/callback.html).. Could you please post relevant details following the issue template so that I understand what you did and what you observed. (click \"New Issue\" -> \"Unexpected Problems / Bugs\"). . Closing as there is no response.. No such conversion exists. In general you can read TFRecord from pure python, with tf.python_io.tf_record_iterator. This will give you serialized bytes from the TFRecord and you'll need to decode the bytes into data points.. tf.gradients(cost, some_tensor). You can also wrap a tf.identity function with tf.custom_gradient. This will let you access to the gradients in the backward pass and is more efficient in runtime. You can refer to tensorflow documentation to use it or tf.gradients.. Closing as there is no response, and how to get gradients w.r.t. each layer's output is a tensorflow's question unrelated to tensorpack.. Could you try export TENSORPACK_SERIALIZE=msgpack?\nThere is a bug in pyarrow that conflicts with horovod.. Well I'm not sure if \"your own version\" will avoid import pyarrow after setting that environment variable.\nMaybe just pip uninstall pyarrow and see what happened afterwards.. I would assume you've made some mistakes in the model code so it actually is something else. But without enough details I cannot help. You can easily verify that the official examples can all run with a large batch size (e.g. ResNet50 can run with a batch size of 128 on 16GB GPU).\nAlso try removing stagingarea to see if it changes anything. . Also, to report any unexpected issues please always post full log rather than describing what you saw. Even you don't want to post code, logs can still be helpful. Another hypothesis is that you've manually created a session that takes all memory before starting tensorpack trainer. With logs this can be easily seen.\nSee https://github.com/tensorpack/tensorpack/issues/new?template=unexpected-problems---bugs.md on how to write an issue.. Is there any update?\nStagingArea should not be an issue because the input is not very large. I would assume something (either other processes or a initialized session in the same process) took the memory before tensorpack starts training.. There is no such functionality. Please consider implementing it yourself.. Closing as a duplicate of #942. Reopen because #942 is empty. Closing #942 instead.\nFor anyone to better diagnose your issue, please post relevant details following the issue template (click \"New Issue\" -> \"Unexpected Problems / Bugs\", or visit this link to post issues about unexpected problems). Closing as there is no response. Closing as a duplicate of #945. Closing as a duplicate of #945. It is not available in tensorpack 0.8.9.. It needs to be the latest version (github master branch). You've posted the installation instruction above.. Please post relevant details following the issue template. \n\nWhat you observed, including but not limited to the entire logs.\nImportError: cannot import name 'FeedfreePredict'\n\nThis is not the entire logs.\nWith a correct installation, this command:\nbash\npython -c 'from tensorpack.predict import FeedfreePredictor'\nwill exit without errors.. It's in the examples.. We have not trained such models.. Duplicate of #470 .. Please check whether dataset_test.__len__() is correct and see if for cnt, data in enumerate(dataset_test): print(cnt) can produce this many of data.. The issue is that LocallyShuffleData for now does not handle the case when the buffer size (50000) is even larger than the whole dataflow -- this case does not make sense at all. I'll add a warning for this.\nAlso, using LocallyShuffleData for inference is wrong at the first place because it randomly mixes datapoints.. https://tensorpack.readthedocs.io/tutorial/performance-tuning.html. Yes the bottleneck is the data but https://tensorpack.readthedocs.io/tutorial/performance-tuning.html#investigate-dataflow has more information on figuring out which part of data is the bottleneck. Before doing that it does not make sense to do any optimization.. https://github.com/tensorpack/tensorpack/blob/eb408ed0e45204059a938b6fae46f8db7205fd15/examples/FasterRCNN/train.py#L588-L589. If you want to use session.run, don't use tensorpack trainer.\nYou can copy the graph building code to somewhere else and use sessions yourself.. See FAQ: https://tensorpack.readthedocs.io/tutorial/faq.html#how-to-print-dump-intermediate-results-in-training\nYou can get the name of the gradient by tf.gradient(cost, some_tensor).. https://github.com/tensorpack/tensorpack/blob/eb408ed0e45204059a938b6fae46f8db7205fd15/examples/DoReFa-Net/dorefa.py#L61-L62\nAdd x = tf.Print(x, [x]).\nAlternatively, print(x) to see the name of the gradient tensor, and use other methods in https://tensorpack.readthedocs.io/tutorial/faq.html#how-to-print-dump-intermediate-results-in-training to print the tensor.. This sounds like a machine learning question rather than a tensorpack usage question. We do not answer general machine learning question in issues.. Closing as there is no response. Feel free to reopen if you have more specific questions about what you attempt to do.. For anyone to better diagnose your issue, please post relevant details following the issue template (click \"New Issue\" -> \"Unexpected Problems / Bugs\", or visit this link to post issues about unexpected problems). Please post relevant details following the issue template in the link above. In particular, I do not understand what you did, what you observed and what you expected.. Please post the details following the issue template in the link above, i.e.:\n\nWhat you did:\nIf you're using examples:\nWhat's the command you run:\nHave you made any changes to the examples? Paste them if any:\n\n\nIf not, tell us what you did that may be relevant.\n    But we may not investigate it if there is no reproducible code.\nBetter to paste what you did instead of describing them.\nWhat you observed, including but not limited to the entire logs.\nBetter to paste what you observed instead of describing them.\nWhat you expected, if not obvious.\n\nSince you mentioned both training and prediction, I assume that's two separate commands. Therefore please post two group of relevant information (what you did and what you observed) for the two commands, respectively.. Closing this due to #955.. python train.py is not the correct command to train. You need to use a pre-trained weights. Read the README for details.. The pre-trained weights to load for coco training is trained on imagenet and has nothing to do with the 80 categories.\nI did not say that there must be a pre-trained backbone. There are ways to train without a pre-trained backbone, but python train.py is just the wrong command.\n. You need to set BACKBONE.WEIGHTS in either config.py or the command line.. And if you change your config.py to use ResNet101, you need to load a ResNet101 pre-trained model.. Feel free to reopen if you have other information to share.. For anyone to better diagnose your issue, please post relevant details following the issue template (click \"New Issue\" -> \"Unexpected Problems / Bugs\", or visit this link to post issues about unexpected problems)\nI assume you're running one of the standard settings in the README. When evaluated with only 1 GPU, the evaluation time you saw is normal.. The partial logs suggest that you're using one GPU.\nIf not, please post issues following the issue template: including the command you run and the entire logs.. Closing as there is no update. Feel free to reopen if you have more information to share.. super quick means quick training, not quick inference.. > What you did:\nI would like to export my model to .pb file, following this link: https://tensorpack.readthedocs.io/tutorial/inference.html#exporter\nThis is what you want to do, not what you did. I still don't know what you did.\n\nWhat you observed, including but not limited to the entire logs.\nI can not import ModelExporter, ImportError: cannot import name ModelExporter, also examples/basic/export-model.py in the tutorial is missing, maybe should update the docs.\n\nThis is not the entire logs. Please post full error message.\nThe example export-model.py can run with latest tensorpack (github master).. Then you probably need to upgrade tensorpack. The upgrade method is in the README and also in the issue template.. For anyone to better diagnose your issue, please always post relevant details following the issue template (click \"New Issue\" -> \"Unexpected Problems / Bugs\", or visit this link to post issues about unexpected problems).\nYour issue is a duplicate of #919.. A duplicate of #875 . tensorpack 0.9 was just pushed to pypi: https://pypi.org/project/tensorpack/\nReinstall from pypi or github master should solve the issue.. That means you're still using the old tensorpack.\nThe command to install latest tensorpack is in the README as well as the issue template:\npip install -U git+https://github.com/ppwwyyxx/tensorpack.git\nThe command to see current tensorpack version is also in the issue template:\npython -c 'import tensorpack; print(tensorpack.__version__)'.\n. Your GPU does not have enough memory to run it. You'll need to change your models.. The FasterRCNN example does not include small models. For large models you can also try: 1. use FPN and 2. use a smaller FRCNN.BATCH_PER_IM.. I think 8G is probably enough if you use FPN and a small BATCH_PER_IM. MODE_FPN=True. The original models were trained without modifications to the code and on 8 GPUs. Since you've made changes to the code, getting different results is not surprising.. Here is the log 4 days ago that reproduced the experiments, with no modification to the code.\nlog.log\nI'm using TF 1.11, cuda 9.0.176, cudnn 7.2.1\nAre you using newer version of TF?. Could you provide your environment info (especially TF version and cudnn version)?. > cudnn version is 9.0\n9.0 is cuda version.\nMade another run with TF 1.10, cuda 9.0, cudnn 7.1.1 and still able to reproduce results.\nI recommend you to take a look at https://docs.nvidia.com/deeplearning/sdk/cudnn-release-notes/index.html which does mention some bugs in certain cudnn versions.. Made another run with TF1.12, cuda9.0, cudnn7.4.1, latest tensorpack mater on Jan 14, unmodified example code. Observed the same results. Closing as the issue cannot be reproduced and is likely an environment issue.. > Would you say it is typical behavio\nIt's typical that GPU utilization becomes lower because no systems can scale linearly.\n\n(maybe due to Python's multi-threading/processing inefficiency at this moment?\n\nTo know what is the reason you'll need to benchmark the training and data separately to see whether the data is fast enough.. For anyone to better diagnose your issue, please post relevant details following the issue template (click \"New Issue\" -> \"Unexpected Problems / Bugs\", or visit this link to post issues about unexpected problems)\nIn particular, I do not know what you did and what you've observed. Please do not describe what you did and what you saw, paste them.. If I understand your questions correctly,\nround((2^k-1) r_i) is a k-bit fixed-point integer.\nr_o is a k-bit fixed point number, but not an integer. \nBut because r_o * (2^k-1) is fixed-point integer, the dot product can still be done with integer convolution kernel because you can ignore a coefficient and apply it back later.. For anyone to better diagnose your issue, please post relevant details following the issue template (click \"New Issue\" -> \"Unexpected Problems / Bugs\", or visit this link to post issues about unexpected problems). Closing as a duplicate of #977 \nA complete bug report can often easily lead to the solution.. The global_step is saved in the checkpoint. You can use the ignore option when you load a model: https://tensorpack.readthedocs.io/modules/tfutils.html#tensorpack.tfutils.sessinit.SaverRestore. Nice catch. This should be conveyed more clearly in the docs, and detected automatically.. (1), (2), (3):\nhttps://tensorpack.readthedocs.io/tutorial/save-load.html#transfer-learning\nAs mentioned in the docs, you only need to change the name in your code and don't need to do anything to the checkpoint, (or you can change the checkpoint as well). Missing tensor will be printed as warning and will not be loaded.\nOn the other hand, same-name-but-different-shape tensors will become an error.. > This parts are for loading, so dump_chkpt_vars should be load_chkpt_vars, maybe?\nYes. Hi, thanks for your thoughts! In general I doubt google will seriously consider significant TF2 RFCs from outside google, partially because someone is going to be responsible for implementing and maintaining it. \nAbout the features you talk about:\n1. DataFlow is pure-python, framework-agnostic. I even tried to split it to a separate project (but was too lazy to do so). It is not very reasonable to integrate it into TF since it's not strongly tied to any parts of TF. Just think of DataFlow as a third-party library, what you need is just a good way to connect it to TF, (which may be tf.data.Dataset.from_generator?).\n\n\nTF2 plans to have dist-strategy work naturally with Keras. If things goes well, this will do an equivalent work of tensorpack trainers, and make Keras models train faster.\n\n\nI actually don't know. I'm not very familiar with Keras callback API. I think the main thing to care is \"what you can do\" inside a callback. For tensorpack this was roughly documented here. If all these can be done in Keras callbacks, then there should be no issues converting among them . del does free the dataflow. And you can see \"successfully cleaned-up\" if you run the following code:\n```python\nfrom tensorpack import *\n\n\ndef create():\n    x = FakeData([[3,3,3,3], [2]], 1000)\n    x = BatchData(x, 3)\n    x = PrefetchDataZMQ(x, 3)\n    return x\nx = create()\nx.reset_state()\nfor idx, dp in enumerate(x):\n    print(idx)\ndel x\nimport time; time.sleep(10)\n```\nSo I assume there are other objects in your code that holds a reference to the dataflow so it never gets cleaned.. Thanks for the short repro! There might be a bug somewhere that mistakenly keeps reference to the dataflow. I'll take a look at that!. Do you see the message before sleep or after sleep? It should appear before the sleep, otherwise it's still a bug.. After the above fix, your code can now print the \"successfully cleaned-up\" message before the sleep finished.. There is a bug in the first commit that was fixed by a later commit.. Just add the name of the cost tensor to output_names.. Yes. You can use overwrite the train_op, as mentioned in https://tensorpack.readthedocs.io/tutorial/extend/trainer.html#write-a-trainer. You can also take a look at how the GANTrainer is implemented at https://github.com/tensorpack/tensorpack/blob/master/examples/GAN/GAN.py.\nHowever, writing a custom trainer means you'll have to reimplement multi-gpu data-parallel logic.\nInstead, if you only want to make modifications to the gradient computation process or the computed gradients, it's better to use tf.custom_gradient in your graph, or use a different (possibly a custom) optimizer (e.g., https://tensorpack.readthedocs.io/modules/tfutils.html#tensorpack.tfutils.optimizer.apply_grad_processors) so that the existing code \ngrads = self._make_get_grad_fn(input, get_cost_fn, get_opt_fn)()\ncontinues to work.. You're using tensorpack correctly. Maybe check whether you're using the same preprocessing or postprocessing between the callback and the offline evaluation?. Since you know the is_training parameter is the difference, why would you assume that the is_training parameter does not matter?\nIn tensorpack's tower function, use get_current_tower_context().is_training to get the correct mode: https://tensorpack.readthedocs.io/tutorial/trainer.html#what-you-can-do-inside-tower-function\n. This is likely a tensorflow bug. If you cannot provide a minimal reproducible code, there isn't much I can do. I cannot reproduce the same issue by making some simple changes to an existing example. For example, this works fine:\n```diff\ndiff --git i/examples/basics/cifar-convnet.py w/examples/basics/cifar-convnet.py\nindex 81e59e2c..cb482fc7 100755\n--- i/examples/basics/cifar-convnet.py\n+++ w/examples/basics/cifar-convnet.py\n@@ -43,12 +43,18 @@ class Model(ModelDesc):\n         else:\n             data_format = 'channels_last'\n\ndef f(x):\nreturn tf.cond(get_global_step_var() < 100,\nlambda: Conv2D('convtrue', x, filters=64),\nlambda: Conv2D('convfalse', x, filters=64))\n+\n         image = image / 4.0     # just to make range smaller\n         with argscope(Conv2D, activation=BNReLU, use_bias=False, kernel_size=3), \\\n                 argscope([Conv2D, MaxPooling, BatchNorm], data_format=data_format):\n             logits = LinearWrap(image) \\\n                 .Conv2D('conv1.1', filters=64) \\\n                 .Conv2D('conv1.2', filters=64) \\\n.apply(f) \\\n                 .MaxPooling('pool1', 3, stride=2, padding='SAME') \\\n                 .Conv2D('conv2.1', filters=128) \\\n                 .Conv2D('conv2.2', filters=128) \\\n```\n\nFrom the log, the error happened at the last iteration of the epoch (not at inference). Maybe you have some callbacks that are enabled at the last iteration of the epoch. \nBy default the summary callback is enabled at the last iteration of the epoch. And in tensorflow, summary ops cannot be used inside tf.cond. This may also cause your issues.. Tensorpack will not summarize anything in the graph if you did not add things by tf.summary or other tensorpack summary functions in the graph.\nTo avoid the summary callbacks, see documentation about extra_callbacks: https://tensorpack.readthedocs.io/modules/train.html#tensorpack.train.TrainConfig. Is summary the issue?. Yes. Both weights will be saved.\nCould you also try cond_v2 following the suggestions in https://github.com/tensorflow/tensorflow/issues/15874#issuecomment-436833266. I don't know if your issues is similar to that one but it's worth a try. Cond_v2 is available only in very recent tensorflow versions. . As the docs said: https://www.tensorflow.org/api_docs/python/tf/contrib/summary, the mechanism of writing summary is different between tf.summary and tf.contrib.summary. \nThe tf.contrib.summary mechanism is not included in tensorpack and will need a new callback to do so.. I do not know what you did. I suppose you'll need to learn more about variable scopes and tf.AUTO_REUSE can solve issues like this.. Again I don't know what you did so I don't know how I can help.\nSince you made modifications on top of working code, it's easier for you to figure out what is causing training to get stuck.. saver is not a tensorflow tf.Operation therefore it's not possible to use it inside build_graph. You need to use a callback.. The path was wrong. It should be http://models.tensorpack.com/Caffe-Converted/vgg19.npz. It was added yesterday. Please upgrade to github's master branch. Alternatively, you can use the examples from an older commit.. You may be using the examples from the master branch but the tensorpack library from an older commit (which you may have previously installed).. Looks like an issue with msgpack-numpy. Could you downgrade msgpack-numpy from 0.4.4.1 to 0.4.3.1 and retry?. https://github.com/lebedov/msgpack-numpy/issues/34. It is mentioned in the docs that nesting two PrefetchDataZMQ is not allowed. I actually don't know what will happen if you do so.\nSometimes people install multiple versions of the same library by mistake and pip only shows one version. This might explain. But anyway, to downgrade, the safest option is to first uninstall it twice (in case there are more than one version of it installed).\nNo need to downgrade msgpack.. The usage is in the top of the code.. Your code has a large amount of duplication with the existing code, and is very inefficient because it builds a predictor for each image. Therefore it cannot be accepted.. Do you still wish to add this functionality?\nIt's easier to just modify on top of the existing apply function. Make it take a list of lowres instead of a single one, and evaluate them one by one. Python's argparse can take a list of arguments. This way the usage of the script for single / multiple images will also be compatible.. You only have to initialize it once by:\npython\npredict_func = OfflinePredictor(PredictConfig(\n         model=Model(LR_SIZE_H, LR_SIZE_W),\n         session_init=get_model_loader(model_path),\n         input_names=['Ilr'],\n         output_names=['prediction']))\nin the code. Then just use predict_func as many times as you want. That's what I meant by writing it inside the exiting apply function.. Then you'll need to modify the model to support arbitrary size (i.e. None size). It's mentioned in the readme that there is no such kernels.. > In this implementation, quantized operations are all performed through tf.float32. They don't make your network faster.\nFrom the readme. Please format the post so that it's easier to read.\nThe error is \nValueError: Input 0 of layer conv2d_1 is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: [None, 40, 40]\nwhich says the convolution layer needs 4D inputs but you're giving a 3D inputs. This is a bug in your graph code which you need to fix.. https://tensorpack.readthedocs.io/tutorial/save-load.html. I'm not familiar with any similar work that does SSD with our quantization. We've done semantic segmentation which seems to work fine.\nClosing as this is not a tensorpack issue. I would in general recommend you to not quantize too many layers (some layers should not be quantized), and use pre-training on imagenet.. Do not use reuse_variables() in the root scope. It affects all other code. This sounds like the issue you're having.. Added it as another rule at https://tensorpack.readthedocs.io/tutorial/trainer.html#rules-of-tower-function\n\nDo not modify the reuse option (e.g., by scope.reuse_variables()) of a variable\n     scope that is not created by you. This affects other's code.. As said in the code, we did not do this filtering because the official implementation (Detectron) did not do this.\nAnd in practice I've found no differences.\nDid you find any accuracy difference by using your filtering?\nFor example, small anchors inside large crowd box would have small iou so not being ignored.\n\nIt's arguable whether small anchors inside large crowd box should be ignored.. >  Isn't true positives inside crowd box are usually small? Like a person in a crowd of people?\nCrowd is labeled as boxes. In coco the labeled boxes are sometimes unnecessarily big due to lazy labelers. The crowd box may as as result overlap meaningful objects, especially objects of a different category (e.g. a car in a crowd of people).\nWhat you said about crowd boxes all make sense -- that's why there is crowd box handling code, though disabled. And they are disabled because there is importance in maintaining consistency with official code.\nTherefore how about let's add your improvement but at the same time still keep things consistent by default.\nCould you change the default CROWD_OVERLAP_THRESH to something like 1.0 or larger (since it's not used at all for now) and add a comment that this is by default disabled? . Just made the above changes to this PR. Let me know if you found anything is incorrect. For anyone to be able to understand your issue, please post relevant details following the issue template (click \"New Issue\" -> \"Unexpected Problems / Bugs\").. Running the FasterRCNN example with the configuration you provided does not produce any errors in my test.\nSince you're making nontrivial modifications to the code. If your modifications to the code causes errors, it's likely not a tensorpack issue and not what we'll help with.. > but the only part of the network that uses GroupNorm is the code for faster r-cnn, but I have not made any changes to it\nThe error does come from faster r-cnn. However this does not imply that it is due to a problem of faster r-cnn.\nAs you can probably verify yourself (and you'd better do so in case this is a bug of tensorflow), that things will work if you train faster r-cnn with the above configurations.\nAnother option that may help is that perhaps you can distill a MCVE that anyone can easily run and see the errors. It's almost impossible to debug something that may not be a tensorpack issue at all, just from the logs. . Your code here https://github.com/stoensin/IC/blob/755fb3e8b2bb8698ad76e93c17bd5c6a6be20559/dense2p/dense2p.py#L421-L427 looks very wrong and please double check their shapes and dimensions. . Please always include entire logs.\nIt looks like it's an environment issue of your environment and your tensorflow cannot use your GPUs. You can debug by, e.g. runing the mnist examples and see if the GPU is used.. Tensorflow cannot access your GPU. That is your own environment problem and others' solution is unlikely to be helpful to you.. The error is unrelated to tensorpack because os.path.isdir is a python function.. > [MultiProcessMapDataZMQ] buffer_size cannot be larger than the size of the DataFlow!\nThen just make it smaller.. Shufflenet is highly bound by the speed of data (because the compute cost is very small), therefore the speed is mainly determined by your CPU and disk, not GPUs. On my machine it's about 10~15 minutes every epoch.. For anyone to better understand and diagnose your issue, please post relevant details following the issue template. See the first line of your issue.\nTensorpack requires:\nhttps://github.com/tensorpack/tensorpack/blob/c4a68c6c5b4e6c7dc7b73014870edeee6e1cfe76/setup.py#L31\nbut you do not have it.\n. msgpack 0.5.6 has the strict_types option: https://github.com/msgpack/msgpack-python/blob/0.5.6/msgpack/_packer.pyx#L117-L119\nTherefore you're not using msgpack 0.5.6. You may have different versions installed at different places.. I would recommend you to uninstall all msgpack related packages in your system and reinstall it.\nHow to install msgpack is unrelated to tensorpack so I'm closing this issue.. No.\nIt's answered in https://github.com/tensorpack/tensorpack/issues/new?template=feature-requests.md. 1. \nSee \"When to log\" in https://tensorpack.readthedocs.io/tutorial/summary.html\n\nhttps://tensorpack.readthedocs.io/modules/callbacks.html#tensorpack.callbacks.ProgressBar has such an option. You can also use https://tensorpack.readthedocs.io/modules/callbacks.html#tensorpack.callbacks.TensorPrinter.\n\nNote that both 1 and 2 involves modifying the default callback list: therefore you'll need to overwrite the extra_callbacks option in https://tensorpack.readthedocs.io/modules/train.html#tensorpack.train.TrainConfig.. It's already in the document of TrainConfig.. Please read the tutorials about data.. By memory do you mean CPU memory?\nYou can use fewer prefetching processes in that case.. DataFlow has nothing to do with GPU memory.. You can use small images, small networks, small batch size, etc.... I do not.. As said in the issue template, we do not answer machine learning questions.. > So, if I have to use AugmentImageComponents (because it has index=(0, 1) argument so I can apply same augmentations to images and labels equally), then is there no way to use MultiThreadMapData?\nCorrect. There are also MultiThreadPrefetchData which works similarly to PrefetchDataZMQ. And MultiProcessMapData which works similarly to MultiThreadMapData.\n\nAn example I can think of is something like map_func=[augmentor.augment(dp[0]), augmentor.augment(dp[1])], but then, I imagine two different augment will apply to dp[0] and dp[1] respectively (due to random factors).\n\nYes. The map function needs to written with augmentor.augment_return_params. In fact the implementation of AugmentImageComponents uses map function as well: https://tensorpack.readthedocs.io/_modules/tensorpack/dataflow/image.html#AugmentImageComponents\nI'll add more documentation about these methods of augmentors.\n\nI'm curious if one can use a multithread way when using AugmentImageComponent(ds, augmentors, (0, 1)).\n\nYou can use a correct map function (as mentioned above), or MultiThreadPrefetchData.. Your tensorflow is unable to access GPU.. Closing due to no response.. from tensorpack.utils import logger\nthen use logger.info, etc.\nhttps://docs.python.org/3/library/logging.html#logging.Logger.info. It would be better if you could post more logs according to the issue template.\nIn particular, I'm wondering whether your epoch is too short (<3 seconds) and the GPUUtilizationTracker hasn't collected enough samples. \n. Thanks. Then commit 7158eed should resolve this issue.. Your code never changes the learning rate. self.lr = xxx does not modify the value of learning rate variable. It only modifies the attribute \"lr\" of the ModelDesc. tf.assign and similar functions need to be used to modify a tensorflow variable.\nAlso, this code is not safe for multi-gpu training because the tower function will be called multiple times: https://tensorpack.readthedocs.io/tutorial/trainer.html\nClosing since this is a tensorflow usage problem and therefore unrelated to tensorpack. . Return None in the function.\nThis is mentioned in https://tensorpack.readthedocs.io/modules/dataflow.html#tensorpack.dataflow.MapData. The mode was now changed to \"nccl\".\nActually I don't have such machine to test if \"nccl\" works for 10 GPUs. Let me know if anything goes wrong.. You can downgrade msgpack to 0.5.6. In validation, the graph does not compute the cost, as can be seen in build_graph (if is_training: ...).. There is no existing definition of \"validation loss\" for FasterRCNN anywhere as far as I know. The definition of such a concept is a machine learning question, so I'll not discuss it here.\nIf you have a definition of such a loss, you can implement it yourself in the build_graph function when is_training=False. Implementing a graph will only rely on tensorflow knowledge and is unrelated to tensorpack.\nPlease also note that the current evaluation only depends on the input image, therefore the validation dataflow only produces the input image. If your definition of such a loss depends on other input data (such as labels), you'll need to also define a dataflow to produce them.. Yes.. You can use either style and it will not cause error.. The code will behave normally. The data format will be translated automatically to a uniform style.. The translation happens at this line:\nhttps://github.com/tensorpack/tensorpack/blob/82aa4b2bed83f52c5886e7f0e19378f907657f7f/tensorpack/models/conv2d.py#L77\n. @junsukchoe 'shuffle' will do random access on the file and will therefore put more pressure on the hard disk.. Please see updated documentation on what this dataflow does: https://tensorpack.readthedocs.io/modules/dataflow.html#tensorpack.dataflow.LocallyShuffleData. \"thread\" does not \"fork\". The data integrity of MultiThreadMapData was explained in: https://tensorpack.readthedocs.io/modules/dataflow.html#tensorpack.dataflow.MultiThreadMapData. You'll not see duplicates\nAlso, threads in python are unlikely to improve the performance substantially. As the error log suggests, this is a CUDA installation issue.\nSee also https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html. In general you can use the RunOp callback to run any ops.\nHowever the table initializers should be run by default by tensorpack. The above commit 168f729 should resolve this.. No that's not how things works. You'll need to modify the code to use a different base model.. The npz file contains no information about network architecture.. Tensorpack issues are for questions/bug reports about the library. We do not answer general machine learning questions such as \"why my model doesn't work\".\nFrom https://github.com/tensorpack/tensorpack/blob/master/.github/ISSUE_TEMPLATE.md:\n\nSome typical questions that we DO NOT answer:\n\"Could you improve/implement an example/paper ?\" -- We have no plans to do so. We don't consider feature requests for examples or implement a paper for you. If you don't know how to do something yourself, you may ask a usage question.\n\"The examples do not perform well after I change the models/dataset/parameters/etc.\" Tensorpack maintainers make sure the examples perform well without modification. But it's your job to pick the model and parameters that are suitable for your own situation. We cannot help with such questions unless they appear to be a bug in tensorpack.\n\"Why my model doesn't work?\", \"I don't understand this paper you implement.\", \"How should I change the examples for my own dataset?\" We do not answer machine learning questions.\n. As quoted above:\n\"The examples do not perform well after I change the models/dataset/parameters/etc.\" Tensorpack maintainers make sure the examples perform well without modification. But it's your job to pick the model and parameters that are suitable for your own situation. We cannot help with such questions unless they appear to be a bug in tensorpack.\n\nThe implemented SE is for training resnet on ImageNet and it is effective there.. Thanks for spotting this! This line has no effect and has been removed now.. It is the correct way. Tensorpack does not create new tf.Graph so I don't know why they appear to be in two different graphs. It's better if you could come up with a minimal reproducible example to demonstrate the error.\nAlso please also make sure you did not create any graph in your code.. Maybe that's because it's actually creating the default one (if no one has been created yet) . Then this appears to be a tensorflow bug. But still it's best if you could create a minimal reproducible example that demonstrates the bug (i.e. the crash), not to demonstrate that it creates two graphs (perhaps there is a reason for it to do so).. Do you have a minimal reproducible example that demonstrates the bug? Otherwise it's hard to tell why it happens or whether it is related to tensorpack or not.. We have no plans to do so, especially at this early stage when \"Mesh TF\" is still premature. I have found no evidence that this library works well for real-life problems on GPUs.. It looks just like a normal OOM to me. I cannot see whether \"the gpu allocator allocates all operations to one gpu instead of all gpus\" or not from your observations, and cannot see whether your issue is related to tensorpack or not.\nNote that even if the same code works on a different machine before, there could be many factors that affect memory allocation, such as TF version,  gpu numbers and model, cuda version, etc.. I don't know how much this picture actually tells. Perhaps it shows this only because the program encounters an OOM even before starting to work on other tensors on the other 3 GPUs. I wouldn't assume there are bugs just because of seeing this picture.\nFrom public data it seems that Titan XP does have 1GB more memory than 1080Ti.. > In that case, gpu 6 was allocated with all tensors while the other three remained 215MiB. It seems the tensor allocation only allocate tensors on the first gpu.\nAnd I assume this behavior should happen on a different machine as well.\nIt's hard to comment anything without a reproducible code. Perhaps you override the device setting of tensorpack in your model in some way.. Have you solved this issue?. The easiest way to debug is to remove your model and replace it with a simple model of several FullyConnected layers and see if the wrong behavior still exists. \nIf the error is gone, that means your model may have override the device setting somewhere and you can debug from there by adding parts of your model back.\nIf the wrong behavior is still there, remove the data and replace it with FakeData. Remove all the custom callbacks. \nIf the wrong behavior is still there then, you should have already created a small enough example that you can share.. The config file currently has NUM_CATEGORY=80. That's the only place you need to modify.. Oh I misread your issue. You're also loading a pre-trained model.\nYou can remove fastrcnn/outputs/*, maskrcnn/conv/* from the pre-trained model because it's impossible to load them when your dataset has different number of outputs.\nAlternatively, you can rename them in the graph. If the names are different, these values won't be loaded.. To modify the pre-trained model you can load it with np.load and save it with np.savez. Refer to numpy documentation for details.. Closing due to lack of activity. Feel free to reopen if the problem was not solved.. Thanks for finding this! The links were fixed now and should be effective in several minutes. Please see the following from https://tensorpack.readthedocs.io/tutorial/symbolic.html:\n\nThese layers were written only because there were no alternatives when tensorpack was first developed. Nowadays, these implementation actually call tf.layers directly. Tensorpack will not add any more layers into its core library because this is not the focus of tensorpack, and there are many other alternative symbolic libraries today.\n\nYou can use tf.layers.Conv3D directly in tensorpack. You do not need to add a tensorpack wrapper for them.. For logging and argscope, see #778 by @PatWie \nYou can use:\npython\nenable_argscope_for_module(tf.layers)\nto get the \"tensorpack-style\" logging and argscope for tf.layers.\nhttps://github.com/tensorpack/tensorpack/blob/master/examples/basics/mnist-tflayers.py. https://www.wikiwand.com/en/Moving_average#/Exponential_moving_average. See https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN#inference\nThe --load has to be a model you downloaded in the \"results\" table.. > The --load has to be a model you downloaded in the \"results\" table.\nNot \"model_cascade.py\".. Yes.. > Can I run the code\nWhat code?. You can.\nYou do not need to modify any parameters. However you might get very different results in the end, even if you're willing to wait for that long.. Your batch size will be 1.. See https://github.com/tensorpack/tensorpack/blob/master/.github/ISSUE_TEMPLATE.md:\n\n\"The examples do not perform as expected after I change the models/dataset/parameters/etc.\" Tensorpack maintainers make sure the examples perform well without modifications. But it's your job to pick the model and parameters that are suitable for your own situation. We cannot help with such questions unless they appear to be a bug in tensorpack.\n\n. CPU is not supported and a small change is not enough.. \"A small change is not enough\" implies that with a large change this is possible.\nIt can be deployed if you write the model in a way that tensorflow supports on CPU.. Easiest way to make it run on CPU is to replace all the NCHW layers (conv, pooling, maybe batchnorm) by  transpose + NHWC layers + transpose back.\nBut this will apparently going to be very slow and you'll want to rewrite the graph in a way that's more efficient on CPU.. >  it seems the tf.nn.max_pool support the NCHW format\nNo. Not on CPUs.. The code does not work for me or @susueah on CPU. \nMaybe it works in some version.\nIf NCHW works for you already, sounds good and you may have less work to do then.. It's more likely that your tensorflow is built with MKL which includes support for NCHW layout, however ours are the default build without MKL.\nIn any case such issues do not seem to be related to tensorpack and I don't have any other comments on your questions.. The lines are documentation. Not code.. Your error log is about convolution, not max pooling. Only replacing maxpooling is certainly not enough.\nYou never need to retrain the model because both data_format uses the weights in the same format.. Without a GPU, inference can also run in NCHW format if tensorflow is built with MKL support.. Please see https://tensorpack.readthedocs.io/tutorial/training-interface.html, https://tensorpack.readthedocs.io/tutorial/extend/trainer.html.\nTo answer your question: ModelDesc and the existing trainers are all for single-cost optimization. So you can either (1): formulate your task as a single-cost optimization or (2) write your own trainer (see tutorial above).\nIt appears to me that your problem can be formulated as a single-cost optimization problem. You just need to implement the function f(inputs, weights) -> cost in your equation, and your final cost is just f(inputs, \u03b8 - \u03b1\u2207\u03b8f\u03b8), as your equation suggests. . Tensorpack is unrelated to how you implement your model. You can use any tensorflow symbolic functions that computes the output from the input.\nYou'll not be able to use \"layer\"-like abstractions because they do not provide you an interface to use different weights. You'll need to write the model on your own.. Yes. The interface for \"layer\" in any libraries is f(input) -> output, not f(input, weights) -> output.. >  Do not know how to sync variable 'tower0/Meta_FasterRCNN/conv0/beta:0' across GPUs. Reason: Name should not have prefix 'tower0' in this trainer! This variable is trainable, so this is probably a fatal error.\nAs the error log says, you're not allowed to create a variable whose name starts with \"tower0\". This name is reserved for tensorpack to use. https://tensorpack.readthedocs.io/tutorial/trainer.html#rules-of-tower-function\n\nFile \"train.py\", line 422, in _setup_graph\nself.predictors = [self._build_coco_predictor(k % num_gpu) for k in range(self.num_predictor)]\nFile \"train.py\", line 422, in \nself.predictors = [self._build_coco_predictor(k % num_gpu) for k in range(self.num_predictor)]\nFile \"train.py\", line 436, in _build_coco_predictor\n...\nKeyError: \"The name 'tower-pred-0/output/boxes:0' refers to a Tensor which does not exist. The operation, 'tower-pred-0/output/boxes', does not exist in the graph.\n\nAs the error log says, your callbacks needs the tensor \"output/boxes:0\" to do inference but this tensor does not exist in your graph.. https://tensorpack.readthedocs.io/tutorial/save-load.html#load-a-model-to-a-session. The above was the standard way to load checkpoints, which is used by all the examples. If you like you can also just do get_variable(initializer=tf.constant(...)).. You can modify the dataflow so that it produces two datapoints at a time.. https://tensorpack.readthedocs.io/tutorial/extend/dataflow.html#more-data-processing. ```python\nclass ProcessingDataFlow(DataFlow):\n  def init(self, ds):\n    self.ds = ds\ndef reset_state(self):\n    self.ds.reset_state()\ndef iter(self):\n    a = {}\n    for datapoint in self.ds:\n      if a:\n          a.update({k + '-2': v for k, v in datapoint.items()})\n          yield a\n          a = {}\n      else:\n          a.update(datapoint)\n.python\n        idxs = np.arange(len(self.lst))\n        self.rng.shuffle(idxs)\n        for k in idxs:\n            yield self.lst[k]`\n```\nYou can see from the code above that it will produce everything in the list after one shuffle.\nThis has nothing to do with training iteration because dataflow is unrelated to training.. > So, will it shuffle again after it produce everything in the list?\nYes. That's literally what the code says. You can find tutorial online if you don't understand what \"yield\" means.\n\nsuch as, after training an epoch, will it shuffle the list again\n\nThat depends on your definition of \"epoch\". Dataflow knows nothing about training and nothing about \"epoch\".. > Hi, I'd like to know how to implement an Update_Weights function :\nweights = Update_Weights( loss, weights)\nWhat does this function do and how is it related to tensorpack?. If you want to obtain the gradients you can just call tf.gradients. The optimizer interface seems not useful to you in this case.\nThe name is generated automatically by tensorflow and is you don't have to care about that.. CropAndResizeGradImage is the gradient of CropAndResize and the error is saying tensorflow does not know how to compute the gradient of CropAndResizeGradImage which is required by your method. If you didn't use tf.gradients then your forward pass won't contain the CropAndResizeGradImage operation.. The formula you wrote in your original question requires second order gradient of f.\nf contains CropAndResize because f is a faster r-cnn.\nThis is a tensorflow question and unrelated to tensorpack. You can solve it by implementing the tensorflow operation that computes the second order gradient.. https://tensorpack.readthedocs.io/modules/dataflow.imgaug.html\nhttps://tensorpack.readthedocs.io/tutorial/extend/augmentor.html. Look at the output images and see what makes most sense to you.. 1. For the crash, I recommend you to try..except the relevant lines and save the data that causes cocomask.frPyObjects to fail. It's likely that certain data in your dataset can make the function crash, or maybe your data is in an unexpected format. This function is from pycocotools so I have no idea what types of inputs can make it crash. But it seems to be quite robust from what I tested just now:\nIn [22]: cocomask.frPyObjects([[0, 0, 32.2, 100, 100, 100, 100, 34.5], [3.2, -3.3, -2.2, 0, 10, 10]], 50, 300)\nOut[22]: \n[{'counts': b'01a14M2M3N2M3M3N2M3M3N3L3M3N2M3M3N1N0001O00001O00001O00001O001O00001O00001O00001O00001O00001O00001O00001O00001O001O00001O00001O00001O00001O00001O00001O00001O00001O001O00001O00001O00001O00001O00001O0000^g9',\n  'size': [50, 300]},\n {'counts': b'02`11O1O1O0101O1O1O1OjT>', 'size': [50, 300]}]\nThe cocoapi does work with Python 3.\n\n\nFor opencv: I use pip install opencv-python and it seems to work well for me. But I don't know what happens to you and this is a separate issue (and probably irrelevant to tensorpack anyway).\n\n\nThis is a TODO item in NOTES.md but it's unlikely it will be implemented any time soon since this is not super interesting to me. . > I can confirm getting this error message as well.\n\n\nYour error message is different from the original one in this issue and I don't understand how they are related. Your issue appears to come from a bad installation of horovod and can probably be solved by uninstall and reinstall horovod with the --no-cache-dir option. Please open a new issue following the issue template if it still exists.. > It may not be interesting, but it's clearly a bug and therefore should be addressed\nBy \"not interesting\", I'm not referring to the crash. I'm referring to (3) in the original post which is about custom dataset.\n\nAlso the original poster made a great request for\n\nAs noted in the issue template, generally we only take feature request on the tensorpack library, not on the examples. \"Examples\" mean that it's something that a user should read, understand and be able to implement. I can understand how demanding such feature can be but it's not on my list for the near future.\n. Cool. I'll add a check for that.. > (1) Include the ENTIRE logs here:\nPlease include the entire logs which will help with the investigation. Otherwise I'll have to modify the code and run it myself.  The logs is like this:\nTraceback (most recent call last):\n  File \"./shufflenet.py\", line 252, in <module>\n    model = model,\n  File \"/home//tensorpack/tensorpack/predict/config.py\", line 85, in __init__\n    assert_type(self.output_names, list)\n  File \"/home//tensorpack/tensorpack/predict/config.py\", line 57, in assert_type\n    assert isinstance(v, tp), v.__class__\nAssertionError: <class 'NoneType'>\nwhich means you have to provide output_names.. See the tutorial on https://tensorpack.readthedocs.io/tutorial/symbolic.html#access-relevant-tensors\nThe name of the output tensor of the 'linear' layer will be 'linear/output'.\nIn general you can print a tensor to see its name by print(x).. The code runs well on my machine, which suggests that it may require certain version of tensorflow to work. I'm using TensorFlow 1.12. cc @PatWie who wrote the export function. TensorFlow's inference optimization tool only works for toco after TF 1.8: https://github.com/tensorflow/tensorflow/commit/c8e3d2b43e4cbf9a9e32567a2e59597916f5b0b9\nThe above commit adds support for exporting a TOCO-compatible graph if using TF>=1.8.. Maybe you did not provide the input_names. By default the inputs are the one used by your model, which is both \"input\" and \"label\", as declared here: https://github.com/tensorpack/tensorpack/blob/860f7a382f8dc245e46c5f637866ef6384db1733/examples/ImageNetModels/imagenet_utils.py#L323-L325\nHowever, when converting to toco, it appears that the conversion tool does not like it when an input is not used. In this case since you're asking for linear/output, this output does not depend on 'label' so you should not add 'label' to the input_names.. This is a tflite question that's unrelated to tensorpack.\nIt asks you to provide input shape so you should do it. Google tells me that you can do it with command line: https://github.com/tensorflow/tensorflow/blob/6932e795621a76a7e520bee529d915265b40e9bd/tensorflow/lite/python/tflite_convert.py#L310-L313. As mentioned in the tutorial: https://tensorpack.readthedocs.io/tutorial/inference.html#exporter , it is frozen.. weights do not have NHWC or NCHW format.\nThe ops are in NCHW format as written in the code.. The modification you need to do is completely in the tensorflow domain and not related to tensorpack much. So that's not something I can offer much help. You can just print the shapes of tensors to see where it goes wrong. From your log it appears this layer is already wrong:\n```\n[0104 13:06:37 @registry.py:121] stage2/block0/dconv input: [None, 56, 56, 58]\n\n[0104 13:06:37 @registry.py:129] stage2/block0/dconv output: [None, 56, 28, 58]\n. It's not a bug. It should be changed according to the data format..python\n@under_name_scope()\ndef channel_shuffle(l, group):\n    in_shape = l.get_shape().as_list()\n    H, W, in_channel = in_shape[1:]\n    assert in_channel % group == 0, in_channel\n    l = tf.reshape(l, [-1, H, W, in_channel // group, group]) //guess the reshape is critical\n    l = tf.transpose(l, [0, 1, 2, 4, 3]) //guess, this tranpose need to change \n    l = tf.reshape(l, [-1, H, W, in_channel])\n    return l\n```. Duplicate of #783 . No, it's not possible.. I very occasionally saw strange crashes when running with horovod, but never when using the \"replicated\" trainer. My guess is this is horovod-specific, but I cannot tell because I can never reproduce such errors. https://github.com/uber/horovod/issues/404 is one such example and I've seen other crashes with different behaviors. Unless you want to do distributed training, using the \"replicated\" trainer is the same as horovod.\n\nYou can also check more logs: in addition to the logs saved by tensorpack, mpirun also saves logs for you (if you use the -output-filename option) and may contain more information.\nAlso, your job can crash if CPU memory is not enough.. https://tensorpack.readthedocs.io/tutorial/extend/model.html. examples/FasterRCNN is not a library. examples/FasterRCNN is an example, with multiple files. \nYou cannot expect things to work if you combine an old version of one file with a newer version of another file.. > A previously working training (on own dataset) \nI assume this means your own old code (and own config)?\nDoes the code in tensorpack by itself (without modifications, trained on COCO) work? I cannot be responsible for the modifications you made.\nFrom the error log it seems that your config file does not have the key RESNET_NUM_BLOCKS, however the latest config file does have it.. Which versions did you upgrade it from and to? There is an assertion on msgpack version at \nhttps://github.com/tensorpack/tensorpack/blob/4eaeed3f6f4651aece29181266194beb4960774f/tensorpack/utils/serialize.py#L77\nIt should assert the version during import and I wonder why it wasn't functional. I cannot see evidence that this is a tensorpack issue. From the error log:\nTypeError: 'float' object cannot be interpreted as an integer\nIt appears that your dataflow returns a float as its length, not an integer. You may want to check that.. KerasModel takes a function which returns a keras model:\nhttps://github.com/tensorpack/tensorpack/blob/f5d1714a13002a7f04755a1c1afe7d70c2b71ef1/tensorpack/contrib/keras.py#L223-L224\nSee examples at https://github.com/tensorpack/tensorpack/blob/master/examples/keras/mnist-keras-v2.py\nIt appears you're not calling KerasModel with correct argument types.. See examples at https://github.com/tensorpack/tensorpack/blob/master/examples/keras/mnist-keras-v2.py\nYou're not calling KerasModel with correct argument types.. Your issue can probably fixed by upgrading tensorpack to github's master.\nIf not, please also verify whether the example runs on multiple gpus, and provide details of failure following the issue template.\nPlease also note what's written in the example: https://github.com/tensorpack/tensorpack/tree/master/examples/keras#note. > Tensorpack version: python -c 'import tensorpack; print(tensorpack.version);'.\nYou can install Tensorpack master by pip install -U git+https://github.com/ppwwyyxx/tensorpack.git\nand see if your issue is already solved. 0.9\n0.9 is not latest. The command to upgrade is above.\nCould you verify whether the example without modification works on multiple gpu first?. If the example works, then your model may have some behavior that's not allowed in tensorpack. I cannot tell without a runnable code to reproduce the issue. Again, note in https://github.com/tensorpack/tensorpack/tree/master/examples/keras#note that keras support in tensorpack is experimental and may not work for all types of models.. fit takes the same keyword arguments as Trainer.train_with_defaults, which includes the number of steps in training. It's by default dataflow.size().\nThe number of steps in inference is determined by your inference dataflow which you can obtain by dataflow.size(). . No. You should make sure the size is reasonable and make sure your dataflow does go back to the beginning and start producing the beginning of your dataset after size() number of datapoints have been produced.\nOtherwise the inference results will be meaningless.. Closing due to lack of activity. If you have more questions you can open a new issue.. https://tensorpack.readthedocs.io/tutorial/performance-tuning.html. > It's will load whole datasets just one time  when  launch_train_with_config() ?\nThat depends on how your dataflow is implemented.. If you're using a custom dataset in COCO format, modify the class COCODetection to make it work. You may need to change the dict COCO_id_to_category_id to an identity mapping.. Thanks for reporting! The code uses the thread_name_prefix argument which is available only after Python 3.6. Removing that argument will work. ( btw, this change is not needed:\n20 _INSTANCE_TO_BASEDIR = {\n21 'train2017': 'train2017',\n22 'val2017': 'val2017',\n23 }\nbecause by default it will use the same name. Thanks a lot for finding this!\nA great example of what I wrote here: https://tensorpack.readthedocs.io/tutorial/symbolic.html#use-other-symbolic-libraries\n\nIt\u2019s best to not trust others\u2019 layers!\nFor non-standard layers that\u2019s not included in TensorFlow or Tensorpack, it\u2019s best to implement them yourself. Non-standard layers often do not have a mathematical definition that people all agree on, and different people can implement it differently. Also, deep learning models on github often have bugs, especially when there is no reproduced experiments with the code.\nFor your own good, it\u2019s best to implement the layers yourself. This is also why Tensorpack does not contain non-standard layers.\n\nBilinearUpSample was made un-public and removed from documentation (https://tensorpack.readthedocs.io/modules/models.html) a while ago because it's a non-standard layer and we should not provide an implementation for it. But it's good to know the bug!. New implementation:\n```python\ndef BilinearUpSample(x, shape):\n    \"\"\"\n    Deterministic bilinearly-upsample the input images.\n    It is implemented by deconvolution with \"BilinearFiller\" in Caffe.\n    It is aimed to mimic caffe behavior.\n    Args:\n        x (tf.Tensor): a NHWC tensor\n        shape (int): the upsample factor\n    Returns:\n        tf.Tensor: a NHWC tensor.\n    \"\"\"\n    #log_deprecated(\"BilinearUpsample\", \"Please implement it in your own code instead!\", \"2019-03-01\")\n    inp_shape = x.shape.as_list()\n    ch = inp_shape[3]\n    assert ch is not None\nshape = int(shape)\nfilter_shape = 2 * shape\n\ndef bilinear_conv_filler(s):\n    \"\"\"\n    s: width, height of the conv filter\n    https://github.com/BVLC/caffe/blob/99bd99795dcdf0b1d3086a8d67ab1782a8a08383/include/caffe/filler.hpp#L219-L268\n    \"\"\"\n    f = np.ceil(float(s) / 2)\n    c = float(2 * f - 1 - f % 2) / (2 * f)\n    ret = np.zeros((s, s), dtype='float32')\n    for x in range(s):\n        for y in range(s):\n            ret[x, y] = (1 - abs(x / f - c)) * (1 - abs(y / f - c))\n    return ret\nw = bilinear_conv_filler(filter_shape)\nw = np.repeat(w, ch * 1).reshape((filter_shape, filter_shape, ch, 1))\n\nweight_var = tf.constant(w, tf.float32,\n                         shape=(filter_shape, filter_shape, ch, 1),\n                         name='bilinear_upsample_filter')\nx = tf.pad(x, [[0, 0], [shape - 1, shape - 1], [shape - 1, shape - 1], [0, 0]], mode='SYMMETRIC')\nout_shape = tf.shape(x) * tf.constant([1, shape, shape, 1], tf.int32)\n\n@tf.custom_gradient\ndef depthwise_deconv(x):\n    ret = tf.nn.depthwise_conv2d_native_backprop_input(\n        out_shape, weight_var, x, [1, shape, shape, 1], padding='SAME')\n    def grad(dy):\n        return tf.nn.depthwise_conv2d(dy, weight_var, [1, shape, shape, 1], padding='SAME')\n    return ret, grad\n\ndeconv = depthwise_deconv(x)\n\nedge = shape * (shape - 1)\ndeconv = deconv[:, edge:-edge, edge:-edge, :]\n\nif inp_shape[1]:\n    inp_shape[1] *= shape\nif inp_shape[2]:\n    inp_shape[2] *= shape\ndeconv.set_shape(inp_shape)\nreturn deconv\n\n```\nI haven't verified the backward is correct, but forward seems right now.. > A separate non-standard layer codebase might be helpful though.\nMight be helpful as a reference. But it will have the same issue: personally I would not trust any non-standard layers written by others.\ntf.image.resize_bilinear has its own issues as well and it certainly is not equivalent to this layer in tensorpack during upsampling. That's why they're called non-standard.. Also, just found that the HED example uses the layer with channel=1 only, so there is nothing to fix. But I'll add some notes to the code.\nClosing as resolved. Thanks again!. Segmentation expects a different format (x1, y1, x2, y2 ,x3, y3 ...). The documentation should have the details. Closing as this issue is about COCO format and not very related to tensorpack.. 1. apply them after batching by calling the augmentor's methods yourself\n\n\"inverse\" is not well-defined and impossible for many augmentors. So you'll need to do that yourself. The parameters an augmentor used is available when you call its methods yourself.\n\nSee docs at https://tensorpack.readthedocs.io/modules/dataflow.imgaug.html#tensorpack.dataflow.imgaug.Augmentor. You don't need to call warpaffine yourself. You'll need to call augment_return_params and augment_with_params yourself.. Closing due to lack of activity. Feel free to reopen if the problem is not solved.. https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/NOTES.md. For anyone to better understand and diagnose your issue, please post relevant details following the issue template (click \"New Issue\" -> \"Unexpected Problems / Bugs\", or visit this link to post issues about unexpected problems)\nPreliminary suggestions:\nCheck your checkpoint by tf.train.NewCheckpointReader\nCheck what your code has loaded by printing. Closing due to no response.\nFeel free to reopen if you have any updates to share. It is mentioned in the pull request template https://github.com/tensorpack/tensorpack/blob/master/.github/pull_request_template.md, which you can see when opening a pull request, that we do not add layers into tensorpack.. I cannot reproduce your issue when running with the same config on 2 GPUs. It's best to include your cuda version, cudnn version as well and try upgrading cudnn to a newer version. Also it may help to check that your dataset looks reasonable, your opencv is able correctly read images.\nAlso note that running with 2 GPUs is not how the models in the model zoo or in the paper are trained. It may require some parameter tuning and we cannot help with tuning parameters. You may try to use a smaller initial learning rate (WARMUP_INIT_LR), for example.. It does look like you may have some issues with your dataset, or the data loading code. My run prints the following in the log:\n| class          |   #box |                                                                                                                                                                         |:---------------|-------:|                                                                       \n| BG             |      0 |                                                                                                                                                                         | person         | 257221 |                                                                       \n| bicycle        |   7056 |                                                                                                                                                                         | car            |  43532 |                                                                       \n| motorcycle     |   8654 |                                                                       \n| airplane       |   5129 |                                                                                                                                                                         | bus            |   6061 |                                                                                                                                                                         | train          |   4570 |                                                                       \n| truck          |   9970 |                                                                       \n| boat           |  10575 |                                                                       \n| traffic light  |  12834 |                      \n| fire hydrant   |   1865 |                      \n| stop sign      |   1983 |                      \n| parking meter  |   1283 |                      \n| bench          |   9820 |                      \n| bird           |  10512 |                      \n| cat            |   4766 |                      \n| dog            |   5500 |                      \n| horse          |   6567 |                      \n| sheep          |   9223 |                      \n| cow            |   8014 |                      \n| elephant       |   5484 |                      \n| bear           |   1294 |                      \n| zebra          |   5269 |                      \n| giraffe        |   5128 |                      \n| backpack       |   8713 |                      \n| umbrella       |  11265 |                      \n| handbag        |  12340 |                      \n| tie            |   6445 |\n| suitcase       |   6112 |\n| frisbee        |   2681 |\n| skis           |   6620 |\n| snowboard      |   2679 |\n| sports ball    |   6291 |\n| kite           |   8792 |\n| baseball bat   |   3273 |\n| baseball glove |   3742 |\n| skateboard     |   5536 |\n| surfboard      |   6093 |\n| tennis racket  |   4803 |\n| bottle         |  24070 |\n| wine glass     |   7839 |\n| cup            |  20574 |\n| fork           |   5474 |\n| knife          |   7759 |\n| spoon          |   6159 |\n| bowl           |  14323 |\nwhich is quite different from what's printed in your logs. Yours have many classes with no training examples.. After looking closer it appears that you probably have made changes to the code and the COCO id mapping was messed up.. duplicate of #1004.. Will push a fix soon. Note that Deconv2D is not a public interface and you should use Conv2DTranspose instead: https://tensorpack.readthedocs.io/modules/models.html, which will not have such issues.. Being able to automatically resume requires logging directory. Using horovod requires different logging directory for each process. So it appears there is no simple solution to this.\nWhat I did is to manually set the epoch and load from a given checkpoint. In all processes:\nstarting_epoch=[the next epoch]\nIn main processes or all processes (doesn't matter):\nsessinit=[load your model]\nThese will do what AutoResumeTrainConfig is supposed to.. (The document of AutoResumeTrainConfig was now updated to reflect this. For anyone to better understand and diagnose your issue, please post relevant details following the issue template (click \"New Issue\" -> \"Unexpected Problems / Bugs\", or visit this link to post issues about unexpected problems). You're not importing a correct (latest) version of tensorpack. And it's likely you have multiple versions of tensorpack installed in your system. Please remove tensorpack by running pip2 uninstall tensorpack, pip uninstall tensorpack each for several times and clean your PYTHONPATH environment variables. Then install tensorpack following the issue template you posted above.\n. You did not follow the issue template to post the output of python -c 'import tensorpack; print(tensorpack.__version__);'. \nPlease do so and also print tensorpack.__file__ which will help you find your installation problems.. Since the issue comes from import tensorflow.contrib, this is not a tensorpack issue.\nSince the stack trace eventually come from dask, you should try either upgrading dask or uninstalling dask. Good to know. \nProbably dask is too old and require an old version of numpy.\nClosing.. dict is still not supported in many dataflows so I haven't touched these codepath yet. Thanks for finding the bug!. I apologize that I did not find enough time to learn the paper and this code.\nIf you're still interested in adding this example, here are some high-level comments:\n1. There is a significant amount of code duplication with CycleGAN, and ideally this should be avoided. I don't know yet what's the most beautiful way to do it though -- probably putting the shared part into a separate file and import it.\n2. I need the instructions to run it (including getting the datasets). And some nice-looking sample outputs at certain steps (or when it finishes), so I know what to expect when I test the code.. import horovod.tensorflow first. It would be great to have. But I'm not sure how elegant it will be given that we still support Python 2. Contributions/suggestions are very welcome. . You're using new version of the command line arguments with old version of code.\nThe new version of code uses RESNET_NUM_BLOCKS only and has no issues.. https://tensorpack.readthedocs.io/tutorial/inference.html. Closing due to inactivity.. The easiest way is to print(tensor). It will tell you the name of the tensor.. 1. Find the input tensor in the code. In your case it's probably image: \nhttps://github.com/tensorpack/tensorpack/blob/274c754444ac35492e32b67764e2466b46f8a179/examples/ImageNetModels/shufflenet.py#L114\n2. Add print(image).\n3. Find the output tensor in the code. In your case it may be logits:\nhttps://github.com/tensorpack/tensorpack/blob/274c754444ac35492e32b67764e2466b46f8a179/examples/ImageNetModels/shufflenet.py#L153\n4. Add print(logits).\n5. Use random strings as input names and output names and run the export code. It will print the names of image and logits.. RPN does use anchors and you can find the word \"anchors\" in the code.. > I didn't find any anchors in this line\nBecause anchors are not needed in this particular line and is needed later.\nI recommend you to read the series of object detection papers instead. In general we do not answer machine learning questions in tensorpack issues. This is a place to learn to use the tensorpack library, not a place to learn machine learning models.. Thanks for the details!\nI can reproduce your issue, and removing the optimize_for_inference function call at  https://github.com/tensorpack/tensorpack/blob/21c494697faa40db0e280a3c53abad2521b2e1f6/tensorpack/tfutils/export.py#L74-L79 makes it work, which agrees with your findings.\nSo it appears to be a bug in tensorflow. It optimizes a good graph to something wrong.. I do not have other recommendations except to manually write a faster graph (which I sometimes do). Inference is not the focus of this project, so we only do what tensorflow can do in this project.. For the record, this was the code I used in train.py to reproduce the issue:\n```python\n    from tensorpack.tfutils.export import ModelExporter\n    finalize_configs(is_training=False)\n    pred_config = PredictConfig(\n                model=MODEL,\n                session_init=get_model_loader(\"/PATH/models/FasterRCNN/COCO-R50FPN-MaskRCNN-Standard.npz\"),\n                input_names=MODEL.get_inference_tensor_names()[0],\n                output_names=['output/boxes', 'output/scores', 'output/labels'])\npbfile = \"./compact_graph.pb\"\nModelExporter(pred_config).export_compact(pbfile, optimize=False)\n\nwith tf.gfile.FastGFile(pbfile, 'rb') as f:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(f.read())\n\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsess = tf.Session(config=config)\nsess.graph.as_default()\nwith tf.Graph().as_default():\n    tf.import_graph_def(graph_def, name='')\nimport sys; sys.exit()\n\n```\nClosing since it's a tensorflow issue. Will add an option in exporter to skip optimize_for_inference to avoid this.. You're not pointing at the current implementation. > Won't the proposals be using the coordinate system of the feature map, \nNo. The proposals are generated from the feature map but their values are in the coordinate system of the input image.. 1. No. Box logits are not defined on any particular coordinates. It only encodes the ratio w.r.t anchors and does not have its own absolute scale.\nRefer to the fast rcnn paper for details.\n\n\nYes. Anchors are defined on coordidates of the input image.\n\n\n\nSee 1. . As the documentation said:\n``\n    \"\"\" Use :func:xla.compile` to compile the tower function.\n    Note that XLA has very strong requirements on the tower function, e.g.:\n\nlimited op support\ninferrable shape\nno summary support\n\nand many tower functions cannot be compiled by XLA.\nDon't use it if you don't understand it.\n\"\"\"\n```\n\n\nTo optimize mask rcnn you may need to manually pick a subgraph and use xla.compile yourself on it.\nThis question is unrelated to tensorpack and just due to tensorflow's limitation.. (I think there was some bug in github and I couldn't see this issue a couple of days ago, thus the late reply)\nI do not see why this issue is related to tensorpack. The log says that tf.gather on that device does not support indexing with -1, and this is a tensorflow issue.\nFeel free to reopen if you have any evidence to believe otherwise.. https://tensorpack.readthedocs.io/tutorial/save-load.html#load-a-model-to-a-session. To load a model to do fine tune: https://tensorpack.readthedocs.io/tutorial/save-load.html#load-a-model-to-a-session\nTo train on a new dataset: https://tensorpack.readthedocs.io/tutorial/dataflow.html\nhttps://tensorpack.readthedocs.io/tutorial/extend/dataflow.html. Closing due to lack of inactivity.\nFeel free to reopen or add new issues if you have more questions.. Should be fixed now. In general I recommend logger.set_logger_dir(path) to have a full control over the file name. . 1 is a python issue\n2 NHWC is a tensorflow issue\n2 remove GPU tracking is the natural thing to do if you don't use GPU\nSo I did not see anything that tensorpack should do regarding your issues. Do you have any proposals?. Closing due to no activity and it's unclear whether there is anything tensorpack should do about the issues being discussed.. 1. That makes sense. I don't believe it will have visible effect to accuracy since the difference is < 0.5 out of image range of 255. But using the training mean is the legitimate thing to do.\n\nTensorpack saves with tensorflow's saver, which saves variables in the graph only. If you want to save anything else to the checkpoint, you can create a variable in the graph with this value. This is a tensorflow issue and unrelated to tensorpack.. 1 was fixed now.\n\nFor 2, an alternative way to perhaps simplify prediction is to do the mean subtraction in the graph, and just call the same build_graph function in inference.. Tensorpack is a training interface and we do not implement any particular deep learning algorithm as a feature in tensorpack. For now at least I did not see anything that stops you from optimizing model/parameters, but this certainly depends on what exactly you want to do.. Thanks! Now it's disabled.\nIt was enabled it at first because it solves some bug in COND_V1.\nBut later I realized that V2 has some of its own bugs as well (e.g. https://github.com/tensorflow/tensorflow/issues/24517). So let's leave the decision to users.. You can create arbitrary operations in the graph in your callback's setup_graph method. For example:\nwith tf.variable_scope(tf.get_variable_scope(), reuse=True), TowerContext('mypredict', is_training=False):\n    placeholders = [ my new placeholders ]\n    build_graph(placeholders)\nAnd you can just use tf.get_tensor_by_name to get your input/output tensors and use session.run to evaluate them in your callback.. 1. Yes\n2. My bad. get_tensor_by_name is a method of a graph (self.graph.get_tensor_by_name). \n3. sess, not hooked_sess. See https://tensorpack.readthedocs.io/modules/train.html#tensorpack.train.Trainer.hooked_sess . self.trainer.sess or tf.get_default_session.. The argument --load should take a path to model, not graph.\nIf your directory does not contain a model, it may be just the training job haven't finished one epoch and saved one.. On a GPU it should take <20min per epoch.\nAs the README says, training without GPU may not result in a good model.\nChanging the step number only has an effect on the parameter schedulers defined in the callbacks.. Closing as it's a TF installation issue as discussed in #1077.\nIt would be easier to tell if full logs were provided, as suggested in the issue template.. config.nr_tower should be num_gpu instead. Thanks for the report!. If it/s is really 10, and STEPS_PER_EPOCH is defined 6000 (https://github.com/tensorpack/tensorpack/blob/49675590da8a39c649b5f0f5ba522a22b90e2d69/examples/A3C-Gym/train-atari.py#L40 )\nThen you should finish 1 epoch in 10 minutes, right?. You have 10s/it, not 10it/s.\nFrom your log it does not appear that you're using GPU. Normally tensorflow should print something GPU-related when creating the session.. Apparently it's telling you that your tensorflow does not support CUDA. You need a different version of tensorflow and I'm not familiar with how it work in colab. You need either tensorflow-gpu or tf-nightly-gpu (and only one of them but not two).. I don't know whether the other packages affect this but you can remove them all, and they will be automatically installed if tensorflow actually depends on them. . 3it/s is actually better than 3s/it. https://tensorpack.readthedocs.io/modules/models.html#tensorpack.models.BatchNorm has mentioned the common reasons why it may hang.\nIf you cannot figure out the reason, please try running the built-in example with syncbn first -- if that fails, please fill the issue template.. Another possibility:\nIn every iteration, either all GPUs execute the BatchNorm once, or no GPUs execute the BatchNorm. If only a subset of GPUs execute the BatchNorm, or if the BatchNorm is executed more than once, this layer may hang because there is nothing to sync.\n. > When would the BatchNorm be executed more than once?\nFor example when it is executed in a while loop. You may also want to remove some custom callbacks to see if they affect this. Callbacks in general should not use hooked_sess and it may cause issues like this if you use hooked_sess.. > I have put the batch_norm in a for loop ( slim mobilenet v2 )\nThis is unrelated. What I meant is a symbolic loop (like tf.while_loop) which will execute the same layer in each iteration.\nA loop in python side like this will create a new layer in each iteration.. > If a put batch_norm outside the for loop, then it works well.\nThat sounds like you may have used the same name for different batchnorm layers.. You can print(shared_name) at https://github.com/tensorpack/tensorpack/blob/49675590da8a39c649b5f0f5ba522a22b90e2d69/tensorpack/models/batch_norm.py#L248\nand see if there are any duplications within one GPU, and if different GPUs produce the same name.. Thanks a lot for finding this out! \nIndeed, unused batchnorm will also cause this. I'll fix this or add this note to documentation.. Btw, it will not cause this issue if you use the internal_update=True option in BatchNorm.. Should've been fixed after https://github.com/tensorpack/tensorpack/commit/c366eebf04f5e53022b1b266cc0f940bf6789f61\nDocs (https://tensorpack.readthedocs.io/modules/models.html#tensorpack.models.BatchNorm) was also updated.\nIn general, the model will run faster with internal_update=True when some batchnorm are unused. This was also now mentioned in the docs.\nThe default was kept as internal_update=False to be consistent with tf.layers.. This question does not appear to be related to tensorpack.\nPlease read the issue template.\n. https://tensorpack.readthedocs.io/tutorial/inference.html. > The warm-up schedule is (steps, value): [(0, 0.0033000000000000004), (1000, 0.01)] and the self.global_step is 11000. So, the learning_rate will stuck in 0.01.\nThis is not true. As you can see in the code, when the current step is after the end of all scheduled point (1000), it will return None, and the parameter will not be changed.\nThe second parameter scheduler (the epoch-based one) is epoch base and without interpolation. As you can see in the code above, it is designed to work only at the exact schedule point. At other points, the parameter will not be changed.\nThe learning rate is 0.01 probably because that's the learning rate in your checkpoint. . Related to that, the design choice that's made here can be reconsidered. \nPerhaps it's OK to let the scheduler set the parameter at the beginning of training as well, in addition to at the exact scheduled points.. To avoid future confusion, after e79d74f85, loading a checkpoint with a learning_rate that's inconsistent to the schedule will lead to a warning message like this:\n```python\nlogger.warn(\"According to scheduler {}, parameter '{}' should become {} at the current point. \"\n            \"However its current value is {}. \"\n            \"If this is the only scheduler being used, you may want to check whether your \"\n            \"initialization of the parameter is as expected\".format(\n                self, self.param.readable_name, v, actual_value))\n```. For anyone to better understand and diagnose your issue, please post relevant details following the issue template (click \"New Issue\" -> \"Unexpected Problems / Bugs\", or visit this link to post issues about unexpected problems). Closing in favor of #1083 . Dataflow always runs purely on CPUs.. > In the newest version, it actually asks for GPU resources\nIt must be something else that's asking for GPU resources.\n\nCould you please double check about this or provide a link of the package that only uses CPU for dataflow?\n\nThe entire dataflow package uses CPU and has not one line of code about GPU.. No it does not. The only thing PrefetchOnGPUs does is to change an environment variable.\nAnd PrefetchOnGPUs has also been marked deprecated for a long time: it does not exist in documentation, and a user should not use it.. For anyone to better understand and diagnose your issue, please post relevant details following the issue template. Posting \"what you observed\" will let others have a better idea about the reason that causes your issue.. Closing as there is no response. Feel free to reopen and add the details about the issue.. You're responsible for setting a correct steps_per_epoch that works best for you.\nIn general, many trainers have no concept of \"number of GPUs\".. Thanks! The preprocessing should follow fb.resnet.torch (which is also the preprocessing used by many papers). The correct version is to use random_normal. Now it's fixed.\nThis does not affect any models because I only used dataflow-based preprocessing and these TF-based pre-processing are only there for benchmark.. Activation function on fixed point values (i.e., quantize->float + activataion + float->quantize) can be implemented as a lookup table.\nTherefore in actual use, no floating point is involved and no extra quantization is needed.\n. Now I'm confused about what you're asking. \nI've probably misunderstood your  questions. There is no floating point operations involved in an actual deployment. And there won't be any extra quantization error. Why do you think the opposite?. > 1. Is there any way to hide the floating point bn calculation? LUT maybe, but the size of the table could be very large if the bitwidth is long (>6)?\nLUT. The table size is 2^activation bitwidth; And in practice we never use any bitwidth >4.\n\n\nEven for the fixed point operation, how to deal with the potential accumulation overflow? It seems impossible to make the intermediate additon result in int16 if there are more channels with large kernel size.\n\n\nYou can save the addition result (the popcnt) into int16 or larger ints. They don't have to be stored, because the LUT is monotonic. Once you obtain the value, you immediately know which item in the LUT you need.. Fa(a1) only has 2^activation bitwidth possible output values. So these are the number of elements needed in the lookup table. You don't need to cover every possible inputs in the range, because Fa(clip(gamma / K * a0 + beta, 0, 1)) is weakly-monotonic w.r.t a0.. Maybe it will be less confusing if we just call it \"table\", not \"look up table\".\nYes multiple inputs map to the same output. And in terms of the actual implementation, I'm not sure what we used in actual deployment but at least you can use integer comparisons to achieve it.. Clip need to come after BN so that the inputs to Fa can be within [0, 1].\nIn general, it's best to not trust third-party layer code, as mentioned in https://tensorpack.readthedocs.io/tutorial/symbolic.html#use-other-symbolic-libraries.. Closing and feel free to reopen or just ask if you have more questions.. > In general, what's your recommendation on implementing hooks that are run after trainer.train_op in tensorpack, since the trainer.train_op is constructed after the callbacks are specified in the training config. (The example in the end also shows how to do this in general TF).\nlaunch_train_with_config is almost equivalent to the following two lines of code:\ntrainer.setup_graph(\n        model.get_inputs_desc(), input,\n        model.build_graph, model.get_optimizer)\n    trainer.train_with_defaults(\n        callbacks=config.callbacks,\n        monitors=config.monitors,\n        session_creator=config.session_creator,\n        session_init=config.session_init,\n        steps_per_epoch=config.steps_per_epoch,\n        starting_epoch=config.starting_epoch,\n        max_epoch=config.max_epoch,\n        extra_callbacks=config.extra_callbacks)\nYou can run the first line first, which will create the train_op in trainer.\nYou can then construct your callbacks, and call the second line to start training.. Feel free to reopen if the above does not answer your questions.. Thanks! It's a typo in docs.. > If convenient, could you please give some advice on how to support training/testing in FasterRCNN in Tensorpack with batch>1 per GPU?\nIt's much more complicated to implement than single-batch in tensorflow and that's why it is not supported. And I have no recent plans adding it, for reasons below.\n\nI don't know how the engineers in big company achieve the large batch training in FasterRCNN.\n\nYou're correct that it takes a lot of memory. Even with batch>1 support, standard models typically can run at most 3 images per batch, which is not very interesting or super useful.\nSure you can also run smaller models / smaller images, but those are not very interesting to me, either.\nAnd of course, if you count the batch size over all GPUs you can get much larger batch size.. https://github.com/tensorpack/tensorpack/tree/master/examples/ResNet. The one in FasterRCNN also uses activation:\nhttps://github.com/tensorpack/tensorpack/blob/d8d35fb57fd8524ee53e030c062455c37b89474f/examples/FasterRCNN/basemodel.py#L76\nAs to why it goes to NaN, it is a machine learning question and do not appear to be related to tensorpack.. Thanks for the suggestions. Now ale_python_interface will not be required if you use gym environment.. It would be incorrect because input data has different shapes and cannot be naively batched, and the model implementation does not handle >1 images.. You will need to modify the inputs method of the model so that it matches whatever produced by the dataflow. And also write the model implementation which takes a list of inputs.\n. Tensorflow takes numpy arrays. So you cannot use use_list=True.. More correctly, I think tensorflow may be able to accept lists, if the lists can be converted to a numpy array. But anyway it's still better for you to convert it (by use_list=False) so it fails early. Perhaps there are still certain field in the data that has different shapes and cannot be batched naively.. Like I said you need to use numpy arrays, not Tensor. It sounds like this goes out of the abstraction of augmentors because you need to make changes on the datapoint-level, not on the image-level. You need to use MapData or similar methods to modify the data points directly.. Since the existing code already uses MapData, you can add your changes in preprocess.. exploration and epoch are reset because they are not tensorflow variables, and therefore cannot be saved in a tensorflow checkpoint.\nSame thing in A3C: epoch will be reset.\nYou can start the training by providing a starting_epoch, as mentioned in the above logs. (See https://tensorpack.readthedocs.io/modules/train.html#tensorpack.train.TrainConfig).\nYou'll also need to modify init_exploration=1.0 to a different initial exploration you need.. Closing and feel free to reopen if the problem is not solved.. https://stackoverflow.com/questions/1535596/what-is-the-reason-for-having-in-python\nThe model requires input shapes to be a multiple of 16.. I cannot understand what exactly you did and what you saw. If you're looking to solve an unexpected problem you met, please post relevant details following the issue template (click \"New Issue\" -> \"Unexpected Problems / Bugs\", or visit this link to post issues about unexpected problems). > In the original code, you've divided height and width with a value of 16 x 16 (on line 282).\nThis is not true. a // 16 * 16 will divide a by 16 and multiply it by 16. See https://docs.python.org/3/reference/expressions.html#operator-precedence. My view of this is that you actually want to use two different transformations on two different data (although they share some parameters), so it gets out of the abstraction of our \"imgaug\" module, so you need to do it manually in your dataflow.\nMaking the Rotation augmentor return less information in get_augment_params does not solve the problem of the imgaug abstraction. In your solution above you still need to construct two different transformations separately. The only difference after the change you proposed, is that you can then achieve what you want by writing less code, because the function would behave in a way that happen to be reusable by you. \nBut I don't think this is a generic solution. It only makes the function more reusable if used it in a non-standard way. Calling augment_with_params with params produced by a different augmentor instance is not the \"official\" usage and is also hacky: if you use different angles in the two Rotation instances, the same issue still arises. Because what you need is just fundamentally not supported by Augmentor.. > If I limit the STEPS_PER_EPOCH and EVAL_PERIOD to trivial values,\nwhat do you mean by trivial values?\nPresumable changing these two values should not have an effect on failures.\nFor debugging I would start with (1) trying to run the dataflow in a loop to see if it fails after a while (2) train without custom callbacks. MultiProcessMapDataZMQ has a buffer_size option which can control the memory consumption.. The memory usage should not rise during training and you can probably see that it does not rise (and in my experiment, stays <20GB with multiprocessing) with the examples unmodified.\nIf running the dataflow alone does not increase memory usage, dataflow is probably unrelated to the growth in memory you observed.. You are responsible for calling it. https://tensorpack.readthedocs.io/tutorial/dataflow.html#use-dataflow-outside-tensorpack. What you described is mentioned as Point(4) here: https://tensorpack.readthedocs.io/tutorial/extend/augmentor.html#design-of-tensorpack-s-imgaug-module. If you use the builtins like AugmentImageComponent, reset is done automatically for you. Otherwise you'll have to reset it yourself now.\nI wanted to find a better place to mention this to users (or warn users), but haven't yet done it.\nOr there might be a way to automatically do this (by some fork handler, for example).\nMarking this issue as an enhancement.. Now the augmentors automatically reset their RNG after fork, in Python>=3.7.\nPython's random module does the same (reseed after fork) in Python>=3.7, and does inefficient syncing in Python <3.7 (ref.: https://github.com/numpy/numpy/issues/9650#issuecomment-327144993).\nAdded some more docs to clarify this behavior.. It means the storage space the parameters will take.. The logging was now updated to include the explanations.. These are machine learning questions and are not related to tensorpack. As mentioned in the issue template we do not answer such questions.. It does not use loss control. Right. It does not matter.. It can be used and there are examples that use it.\nFor anyone to better understand and diagnose your issue, please post relevant details following the issue template (click \"New Issue\" -> \"Unexpected Problems / Bugs\", or visit this link to post issues about unexpected problems). If it is really stuck at this line:\nhttps://github.com/tensorpack/tensorpack/blob/f42036acc675dbbff86e25f25506ccf4bff30f67/tensorpack/train/tower.py#L261\nthen it is a tensorflow issue.\nAs you can probably verify yourself, that using tf.gradients is OK in most normal cases (e.g., if you take a simple example and add gradient regularizer). As a result I cannot reproduce your issue.\nCould you provide something that others can run and reproduce the same issue? This is needed no matter where (tensorflow or tensorpack) you report this bug to.. Closing as no instructions to reproduce the issue were provided.. As the issue template said:\n\nIf you expect higher speed, please first read https://tensorpack.readthedocs.io/tutorial/performance-tuning.html\n\nIt can be seen from the log that your data gets slower in the 2nd epoch and you may want to investigate why.. It should be available now.. There seems to be some setting issues with the website. please use https://tensorpack.readthedocs.io/tutorial/performance-tuning.html or https://tensorpack.readthedocs.io/en/latest/tutorial/performance-tuning.html: at least one of the two should work.. What happened in your log is that the data becomes slower in the 2nd epoch.\nIt's unlikely that batch size is the issue unless your batch size is different in each epoch. I would recommend you dig into it further.. https://github.com/tensorpack/tensorpack/blob/master/examples/ImageNetModels/shufflenet.py. Closing since the question seems to be resolved. Feel free to reopen if otherwise.. For anyone to better understand and diagnose your issue, please post relevant details following the issue template (click \"New Issue\" -> \"Unexpected Problems / Bugs\", or visit this link to post issues about unexpected problems). Also, if you're using your own models or datasets, then how to make your training converge is not our responsibility, unless there is a reason to think it is a tensorpack bug.\nYou can see these in the issue template when you file an issue:\nFrom https://github.com/tensorpack/tensorpack/blob/master/.github/ISSUE_TEMPLATE/unexpected-problems---bugs.md:\n\nIf you expect higher accuracy, only in one of the two conditions can we help with it: (1) You're unable to match the accuracy documented in tensorpack examples. (2) It appears to be a tensorpack bug.\nOtherwise, how to get high accuracy is a machine learning question and is not our responsibility to figure out.\n\nFrom https://github.com/tensorpack/tensorpack/blob/master/.github/ISSUE_TEMPLATE.md\n\n\"The examples do not perform as expected after I change the models/dataset/parameters/etc.\"\nTensorpack maintainers make sure the examples perform well without modifications. But it's your job to pick the model and parameters that are suitable for your own situation. We cannot help with such questions unless they appear to be a bug in tensorpack. . Closing as the requested details in the issue template were not provided.. > I expect everything goes right as before (when set FPN=True).\nIt's cost not nan with right evaluation results.\n\nThat was not a valid expectation. You've changed the hyper-parameters to bad ones and that may result in models that cannot train. \nTensorpack examples are (obviously) not responsible for the accuracy of examples after arbitrary hyper-parameter changes by the user. It is your responsibility to figure out the right hyper parameter for your goal.\nTo perform \"quick\" training, refer to the \"quick\" config in the model zoo for a working set of hyper parameters that are cheaper to train than standard models.. The function does not contain mask visualization.\n--predict contains it.. >  I am left with the impression that the weights of the standard FPN model should still work when restored from a pre-trained checkpoint resulting from a GN config.... where the inputs to the GN layers would be redirected to the outputs of the GN layers, but maintain the same tensor names... effectively bypassing the GN layers, and ignoring the associated weights. Temporarily ignoring the discussion of how to properly load the weights, is this assumption correct? Or am I misunderstanding the implementation of the model architecture?\nWhat you said about the architecture is correct. This means the model will load properly. This does not mean the model will predict properly.\n\nMoving on with the assumption that a standard FPN model will support weights trained with a GN config\n\nSame as above. The weights will load like you did. The weights will not predict properly, and there is no way to make it so.\n\nDoes tensorpack provides a means of restoring a model given the architecture defined by the model provided in the Config\n\nIn tensorpack, restoring a checkpoint means loading the variables in the checkpoint to the graph, and that's what it is.\nI do not understand what \"restoring a model given the architecture defined by the model provided in the Config\" means exactly. It's better if you would clarify what is the desired behavior.\n\nhoping that the internal SessionUpdate is effectively restoring intersecting tensors by name. \n\nIt is already \"restoring intersecting variables by name\", no matter whether you use DictRestore or SaverRestore.. What we do is the same as what the paper and the official implementation do. As a result it certainly can work.\nI have no knowledge of google's object detection API and why it cannot work is not something for us to answer. . 1. Without runnable code there is nothing I can tell from this error message. It's likely that your modified code produces incompatible shape with other tensors.\nFor anyone to better understand and diagnose your issue, please post relevant details following the issue template (click \"New Issue\" -> \"Unexpected Problems / Bugs\", or visit this link to post issues about unexpected problems)\n\n\nI'm not sure I understand what that means. If you just want to print tensor values during training, it is in the FAQ. 1. As I said: \nFor anyone to better understand and diagnose your issue, please post relevant details following the issue template (click \"New Issue\" -> \"Unexpected Problems / Bugs\", or visit this link to post issues about unexpected problems)\n\n\nNo that's not how to print things in tensorflow. As I said, to print tensor values during training, the methods are in the FAQ. You cannot print tensor values before training because they have no values at that time.. This is a tensorflow or cudnn bug and is now reported at https://github.com/tensorflow/tensorflow/issues/26797 .. Closing as this is not a tensorpack issue. Feel free to reopen if you have more questions.. Latest tensorpack (353cd04fe81ea8920bc67f702bf492174b000768) now avoids the bug in tf.layers. If you use latest tensorpack, dilated conv should work now.. That is weird. Did you make other changes (about trainer or data)?\nI have not seen this with the default trainer and don't have an idea why this may happen.. It is worth trying to upgrade tensorflow. I've seen some weird errors (unlike yours) in the past but haven't seen any recently.. Then your issue sounds similar to #1097 : memory gradually increases, and lead to a DeadlineExceedErrror (which is somewhat similar to getting stuck). However there, the issue does not appear for COCO dataset.\n\n\nOne possibility is to replace MultiProcessMapData in data.py by MultiThreadMapData or MapData (probably will be slow) -- this may make a difference, though I'm still not sure what the issue is at the moment.. Another thing to try is to run python data.py. It will just iterate over the the entire dataflow for 50000 iterations (which you can change to more iterations). You can use this to test whether the dataflow will grow the memory usage indefinitely. (in my test it does not grow, but perhaps this is different across systems).. I do not understand this question. If you're asking about an unexpected problem which you do not know the root cause, use the template you posted.. \nAs said in the image, the data loader will fill out these two config items. This is implemented in DetectionDataset.__init__ in dataset.py.\nThe correct way to train on your data is written in NOTES.md.. Since that is a different issue, please post details following the issue template.\nPlease include full logs whenever possible.. Please include details following the issue template above:\n\n\nWhat you did:\n(1) If you're using examples, what's the command you run:\n(2) If you're using examples, have you made any changes to the examples? Paste git diff here:\n(3) If not using examples, tell us what you did:\nIt's always better to copy-paste what you did than to describe them.\nPlease try to provide enough information to let other reproduce your issues.\nWithout reproducing the issue, we may not be able to investigate it.. > _C.TRAIN.EVAL_PERIOD = 0 # period (epochs) to run evaluation\n\n\nTo disable evaluation, set it to a large number, e.g.: 99999999. As said in the issue template:\nIf you expect higher speed, please read\nhttp://tensorpack.readthedocs.io/tutorial/performance-tuning.html \nbefore posting.\nIf you expect certain accuracy, only in one of the two conditions can we help with it:\n(1) You're unable to match the accuracy documented in tensorpack examples.\n(2) It appears to be a tensorpack bug.\nOtherwise, how to train a model to certain accuracy is a machine learning question and is\nnot our responsibility to figure out.. The current contract is that augmentors are allowed to modify the image in-place to be efficient. In that case the lowest-level dataflow should avoid its data being damaged by augmentors (by yielding a copy, see dataset/cifar.py).\n. [deleted]\n. Maybe it's easier to understand to use two argument black_prob and white_prob instead of amount and s_vs_p.  It confused me at the beginning.\n. ?\n. not used, same for sio and another csv below.\n. load-resnet.py\nalso, do you need --depth 101?\n. seems that this is not used (my bad)\n. please clean the unused debug code\n. We can remove this comment here because it's not import *.. Is this equivalent to just loss_op = control_flow_ops.with_dependencies(update_ops, loss_op) ?. the parenthesis look wrong. I could not access this file. If it's just a mean file, could you just use ILSVRCMeta().get_per_pixel_mean() ? Different mean files shouldn't make much difference.. These docstring are too verbose and probably it's better to remove them in this example.. Same for \"# input_vars contains a list of input variables defined above\". I think putting the detailed instructions in one place (either README or the script) is enough.. Could you allow a command line argument to specify the input image?. Could you use google-style docstring to be consistent with the rest of the library? See here: http://www.sphinx-doc.org/en/1.5.1/ext/napoleon.html#google-vs-numpy\nAlso, TYPE doesn't seem to work well with sphinx. You can just leave it blank if the type is unspecified or too obvious. . Could you explain that \"saliency\" is actually the gradient w.r.t to the maximum? Also, it looks like this function works for any input type other than image.\nOr, maybe just move this to examples, because it seems to me that this is neither a well-known function nor a common one people will use for other tasks.. These two functions are not used in this commit. I can take them, though.\nFor symbolic functions, I think it's important to document the dimension/shapes of input/output. . Could you briefly document the behavior and add reference to the paper? This name is not commonly known.\nAlso, please use \"ReLU\" to be consistent with the naming of all the existing ReLUs in \"nonlin.py\".. There isn't a background argument.. Could you document the expected format of input/output image? Usually I will be especially confused with the range (0-1 or 0-255?). RGB/BGR is not a big deal in this case, though.. Could you move this function out to the example? I think different people always have different ideas about how to best blend two images for visualization, for their own tasks. For instance. they may want those regions with 0 intensity also partially visible. In one of my recent code here I used a different way of blending.. Could you move the implementation to reset_state()? reset_state of an underlying DataFlow only get called when you call reset_state from a high level DataFlow, this is currently true for all DataFlow except Prefetch.\nI'm not sure will this cause any issues, though. I'm trying to think if I'm using this assumption somewhere.. Could you use logger.info for the logging?. This is already implemented the same way in ProxyDataFlow.. Could you name it something more informative about what is happening? Such as PrintData/LogData/PrintDataShape/PrintDataInfo, similar to TestDataSpeed I'm having.\nPrintData might not be very bad because in the future, options can be added to print k datapoints instead of just 1.. The protocol is that reset_state should always get called before a DataFlow start producing. But of course putting it in __init__ can make it print the log earlier and probably that's more preferable for the debugging purpose.. Why did you remove the linting for examples?. Yes but I ignored examples in /tox.ini, and examples/tox.ini is different from tox.ini (ignore certain type of errors).. Wow thanks for pointing it out! I'm now using cubehelix as default.\nBlues might be a boring default \ud83d\ude04 . I don't think reducing on axis=1 (not axis=None, not axis=[1,2,...end], but axis=1) is what people would expect on a function named l2_norm. And it would also take more documentation to explain that it reduces on axis=1, than writing the code itself.\nCould you just put this function where it is used so users won't see it?. It's not consistent when some functions in this file accept eps as an option but some just use a constant.\nAlso, could you use a more descriptive name rather than a,p,n?. In general, summary ops only gets run once after each epoch (and on the last data point only) so the results can be quite noisy. For scalar summaries you probably want to use add_moving_summary, which computes moving average over every iteration.. Maybe you'd like self.checkpoint_dir better?\nself.path would look like \"train_log/dirname/model\". It's just a prefix of where the actual model is saved.\nOr you can use tf.train.get_checkpoint_state(self.checkpoint_dir).model_checkpoint_path to get the path to the actual model.. reuse is not used?. Should \"code\" be renamed to \"sample\"? That's essentially what it is doing.\n\"code\" is a name used particularly in InfoGAN.\nAlso, the placeholder_with_default hack is used in InfoGAN only to make inference work. It should stay in InfoGAN but not as a default behavior in distributions.. Regardless of what is actually used for training, this is a \"distribution\" class so it should follow strictly the mathematical definition. Entropy should have a negative sign.. This as far as I know is an approximation of the cross entropy (that is used in InfoGAN specifically but not in general(?)). Is it really a good idea to put it here?. Is it a good idea to use reduce_mean(y * dist)? Wouldn't it be better to use something like reduce_sum(y * dist) / tf.count_nonzero(y)?\nMight not make a big difference when 0/1 labels are perfectly balanced, though.. So cosine_loss is summed over the batch, but others are averaged over the batch? This is inconsistent and will confuse people when they want to interpret the magnitude of loss.. Could there be either a formula or a reference to a paper explaining what it actually computes?. Yeah I noticed that I had a problem like this in another function and just fixed it in eb1fa920e81e32d8da5a45ede9b.\nFor this case, maybe return 0? Or maybe assertion?. I'm talking about pos_dist and neg_dist (only used for summary), not the loss.\nBut anyway it's OK if you want to leave it like this.. I didn't replace it with max.\nThe function should only documents what it computes -- max(0, m+...)\n\\min is how it should be used (minimized), and this is literally what \"loss\" means.. You can. They look the same to me. maybe will have some very slightly performance difference?. The protocol is that a user always have to call reset_state() before get_data is first called.. I wasn't sure if it is safe and efficient to share a lmdb handle among processes so I reopen it again in reset_state(). But the keys don't need to be loaded again.\nRemoving this line may need some investigation into lmdb.. The trainer and TestDataSpeed do that for you. But often you are supposed to call that yourself, although it doesn't always give you exception.. Shoud it be (\"\") instead of () ?. OK!. It's not random. No need to add this feature again. It will be equivalent to Rotation (because there is no need to crop when rotating 90 degrees).. OK.. but I can't imagine how that's a useful feature... If Rotation keeps the size when rotating 90 degree then this augmentor will be different (it doesn't keep the size).. why not import stuff from tensorpack? (as another form of testing). Is this still needed now that you have --fake?. Domain means (min, range) ? Would it be simpler to use (min, max)?. Can we just keep the Map? It's a one-liner anyway, and user can see and choose what decoder they want (I know faster decoder than opencv). The wrappers also lose the options available in imdecode/imencode (IMREAD mode, image quality, etc).\n. Yeah I think FixedCrop is especially unnecessary. When I see the function I'll be wondering whether it crops to max+1 or max.. Would bytearray introduce a copy when input is already a uint8 nparray?... Also, if it is provided as a dataflow called ImageDecode, people would want to use it with one datatype or another (at least bytes and nparray are common). Similarly for encode, we cannot assume users want an nparray in the end, bytes can be useful as well.\n Although they seem so, these two functions are not so well-defined like Clip,ToUint8,Resize which you don't need to read docs to know what they do.\nI don't mind removing SelectComponent, actually I would hope to let users get familiar with MapData/MapImage so they won't expect all simple functionalities built into tensorpack.. Right.. Probably 'conv3' isn't the desired name?. tf.summary.image will do roughly the same kind of normalization when input is float. Maybe this is not necessary?. range(8)*4 is not python3 compatible.. You added Grayscale augmentor but is not using it ... Since there are public APIs tf.saved_model.builder etc https://www.tensorflow.org/api_docs/python/tf/saved_model/builder/SavedModelBuilder, we can avoid these internal imports.. Why pass in a class ? would it be more natural to just pass a model instance?. what is \"version\"?. Don't have to worry too much about these weird augmentors.. They are not quite well defined and not used by anyone, I think.. I think interpolation doesn't affect the alignment in opencv.. Some sort of random bounding box perturbation is definitely helpful, but the problem is there isn't a standard way of doing this. If something isn't a standard, I think it's best to let users write their own version of it than to provide some arbitrary implementation. I might just mark this augmentor as deprecated. . What is this for? Keep the center unchanged?. Could you get the scripts back?. Could you put this in the example script? And it would seem easier to write it as a MapData on top of a batch.. What you've written is literally MapData(BatchData(ds, 2 * batch_size), mixup).. mixup data should be inside \"if isTrain\".. The name allow_exceptions confused me at the first look. The current code is saying exception will be raised when exception is \"not allowed\", which is weird.\nMaybe use \"hide_exceptions\" or \"catch_exceptions\"?. You can make it a contextmanager, so the try ... catch logic can be shared as well. \nAn example: https://github.com/ppwwyyxx/tensorpack/blob/2fa49895399ec33298f2b774929aada5fbed3e89/tensorpack/dataflow/prefetch.py#L51-L65.. remove the original line. use the constants. tf.losses.mean_squared_error(reduction=Reduction.MEAN). tf.space_to_batch_nd + reshape ?. use tf.stop_gradient for VGG?\nEven if lr=0 gradients are still computed and wasted. And why p-1 instead of p?. Perhaps. Just feel looping over pixels isn't very tensorflowish.\npython\nx = tf.space_to_batch_nd(x, [p, p], [[0,0],[0,0]])\nx = tf.reshape(x, [p, p, -1, h, w, c])\npatches_a, patches_b = tf.split(x, 2, axis=2)   # each is b,p,p,h/p,w/p,c. with varreplace.freeze_variables():. Shouldn't it be range(0, h-p+1, p)?. Or equivalently range(0, h, p), when h%p==0. But using space_to_batch_nd fixes this anyway, so... No it'll only stop the gradients on variables (inside the with block), not on any layer output.. fix:\npython\nx = tf.space_to_batch_nd(x, [p, p], [[0,0],[0,0]])\nx = tf.reshape(x, [p, p, -1, h/p, w/p, c])\nx = tf.transpose(x, [2, 3, 4, 0, 1, 5])\npatches_a, patches_b = tf.split(x, 2)   # each is b,h/p,w/p,p,p,c. why this? seems like inference=not ctx.is_training is always true. Is this faster than putting ImageDataFromZIPFile + preprocess here?\nI guess maybe just the zipfile module is really slow. You are doing random read with LMDB anyway.\nNot an issue, just asking. Wish there is a simpler (without lmdb) efficient pipeline . OfflinePredictor always creates model with ctx.is_training=False. I understand the need, but you're not using inferencerunner. Instructions are in readme already. better to not duplicate them. unused?\nsame for size. Well yeah, I also felt having these two phases isn't perfect, though users can always do their own stuff like you did.. The texture loss seems too small to be useful (there is already a very small loss weight for them). Does it really average over these dimensions here? The paper only mentions L2 norm.. \"For the perceptual loss LP and the texture loss LT , we\nnormalized feature activations to have a mean of one [4].\". why \"area\"?. @PatWie . You can put the message here on the assertion.. different from the original repo:\npython\nout = F.relu(self.bn1(x))\nshortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\nSame for the other block function.. What files/directory structure does it require? What data (the meaning, shape, scale) does it produce? Could you add these documentation and maybe send a separate PR for it?\nIt's hard to work with such a large PR. No matter whether the example works or not, a popular dataset loader alone can be a great feature to add.\n  . This seems to be equivalent to CenterPaste(background_filler=constant) + RandomCrop(). But I may be mistaken.. We've decided to not add any more layers to tensorpack. Because tensorflow has done the same thing already (tf.layers) and will probably maintain it better. See https://github.com/ppwwyyxx/tensorpack/issues/291.. Same for symbolic functions. We'll not add more symbolic functions and most of the existing ones are deprecated with a \"Please implement it by yourself\" notice. These functions are specific to applications and should come with the applications.\nIf a symbolic function is not common and general enough to be included by tensorflow itself, there is no reason why tensorpack should include it.. It's error-prone to put any user code under tf.AUTO_REUSE. Because there you don't know whether your scope is reuse or not -- it's both. So a user can end up creating unexpected variables. So I prefer to keep it as it is.\nAlso, AUTO_REUSE is a tf1.4 thing.. By design it was meant to affect the entire scope. There is a name confusion with tf.AUTO_REUSE because I write this function before AUTO_REUSE appears.. Creating unexpected (new) variables under reuse=True leads to an error. But it will silently pass under tf.AUTO_REUSE. That's the kind of error I'm talking about. That's not the syntax of a decorator. I know tf.AUTO_REUSE is useful in many situations. But because it's also error-prone at the same time, I would let users to decide to use it, and implement the same functionality with slightly more lines of code.. Many tf stuff (EMA, tf.layers) create variables secretly that I wouldn't have known them until I saw the error under a reuse scope.. This import line doesn't run.. Not necessary to make it global, right?. I don't think UUID and name of card will ever be useful. They can be removed together with call() and BUFFER constants.. Apart from +1 and -1 I don't see this variable used anywhere. Pair create & destroy using a with statement?\nwith NVMLContext() as ctx: .... But everytime load() is called the so file gets loaded again. \nWhat's singleton is _NVML = NVML(). This so can just be an attribute.. And even if it's made an attribute, is it correct to load the so everytime load() is called? Should it be cached?. Not used anywhere. And docs should be with __init__. Yes that's a bit inconsistent.. but at least arguments should be with the function.. \"dp 0:\" should be removed here?. Now are documented. I would remove the duplicated docs from here, or mark it \"the same as init\".\nDoes the readme in https://github.com/ppwwyyxx/tensorpack/tree/master/docs work?. name was missing. I was thinking https://stackoverflow.com/questions/2119472/convert-a-timedelta-to-days-hours-and-minutes. An easier way would be like: datetime.datetime(2000,1,1) + datetime.timedelta(seconds=9999999). cv2 is an optional dependency. Don't import it at the beginning of file but put it here.. Seems like these two lines can be removed.. Is there an alternative way? For example check if the object is in tf.layers module?. Maybe don't need to pass in the module, since this function is written for tf.layers only. Why a decorator argument?\nIf the purpose of this function is \"enable_argscope\" then why would a user want to use a different decorator?. \"module\" may be a better name than \"lib\". What about adding an option like log_shape=True here? Easier to use than a new API.. Let's make the format the same as tensorpack.models. Agree. In tensorpack.models, layers can be nested, layers can have >1 inputs or outputs. That's why the logs are like that.\nI think this format is fine.. Yeah it's best to make multigpu training only print shapes in the first tower, but I'm OK without this feature. It's fine for files in tfutils to depend on tf.\nIn fact I'm not sure how variable/name_scopes in tf.layers work.. I remember it's quite complicated... Because the only valid use case for LMDBDataPoint is to use with LMDBSerializer.save (or previously dump_dataflow_to_lmdb), and LMDBDataPoint has a worse name than LMDBSerializer.load so should be deprecated.\nTFRecordData is a bit different because it has other valid use cases. It can be used to load with custom decoder, at least in this PR for now -- I think that's a very very rare use case anyway, so we maybe just don't support it.. eventually no. But I should keep msgpack working.. why?. This can be fixed by:\nnp.savez_compressed(path, buffer=np.asarray(buffer, dtype=np.object)). should better be 'pool2'. This will actually do the dropout? Tensorpack's dropout layer knows the context is inference, and will not perform the dropout. . why is it optional and has a default value? from the code it looks like it is necessary and has to have the same length as datapoints.. from collections import defaultdict ?. The verification is not meaningful to anyone but you because you provide the inputs and outputs. It does not tell anyone else how well the code is doing. It's better to remove it (or keep it personal for debugging).\nFor verification will you (not necessarily in this PR) implement an evaluation on certain dataset? I guess if you want to reproduce a paper later you'll need to do that anyway. . What is this /20 ???. This part does not make sense for now. Can be removed.. Perhaps it does not matter much, but is this actually what they're doing? It seems to be some custom caffe code so I didn't follow much into it.. Could you make \"20\" a constant variable, and add some words and references about it? That would make me more comfortable seeing it around.\nAlso, is it redundant here to multiple it by 20 and then divide by 20 later?. It seems to me that all the other parts of the model uses the [2, C, H, W] layout. Why using a different layout here?\nYou can use [2, C, H, W] here to compute rgb_mean, and let graph_structure take [2, C, H, W]. I think this will remove many of the concat+stack and preprocess=False. When the metaclass is not ABCMeta I guess abstractmethod will have no effect.\nInstead we should check in our own meta, that one of the two methods get_data and __iter__ was implemented.. Let's deprecate them after a while. For big changes like this let's first apply it and see what happened.. This is probably not necessary though I haven't tested. Why not len(x) but x.__len__() ?. This is not needed. get_tqdm(iterable=df) is the same.. Or maybe add *args, **kwargs.. This will fail if df has no size. _reset_df_and_get_size(df) was meant to provide a default size of 0.. Is there a reasonable use case for this dtypes option? (i.e. to not use the default?). please fix this comment. Do you mean \"converts graph and checkpoint\"?. What is the issue? If the dtype parameter for optimize_for_inference_lib.optimize_for_inference cannot match tensor.dtype, that sounds like a tensorflow bug to me, is it?. I would like to remove this function. Npz is used for the model zoo because it's easier to load and only one file. But I don't mean to make it feel like \"standard for tensorpack\" and users may use any format they like given the standard TF checkpoint support.\nAnd this function isn't doing anything useful, after all.. Passing self around is an unusual design. Here we can just write them as methods of GANTrainer.. Is it better if we just clear the history?. I tried to avoid printing this information too often because of how noisy it can be. But I understand there is a confusion when it's only printed once per epoch. Maybe the message or the docs can be improved.. Only keeping this one line is enough. The other branch can be removed since gt_masks is not used later.. This behaves differently for Python 2 and 3.\nAre you using Python 2 or 3?. suggestion\n                elif isinstance(dt, (six.binary_type, six.text_type)):. If it's really going to handle arbitrary user function, it's best to (1) document what types of functions is supported in the documentation and (2) add a check for it (either skip the logging or raise an exception, both seems OK). Cannot import here because horovod is not a required dependency of tensorpack.. The default value should be None because Compression may not be imported when horovod is not installed.. fix typo. should add a check because this feature is only supported after horovod 0.15. Please put the import where horovod is currently imported. ",
    "PatWie": "What do you mean by perspective? Something like:\n````python\nimport cv2\nimport numpy as np\nimg = cv2.imread(\"original.jpg\")\nimageplane_coords = np.float32([[-30, -60],[368,52],[28,387],[389,390]])\nworldplane_coords = np.float32([[0,0],[300,0],[0,300],[300,300]])\nM = cv2.getPerspectiveTransform(imageplane_coords,worldplane_coords)\ndst = cv2.warpPerspective(img,np.random.rand(3,3),(300,300))\ncv2.imwrite(\"output.jpg\", dst)\n````\n?\nShould this use a random transformation matrix?\nSaturation is already in the code.. see issue #84. Rebased on current HEAD after fixing #84 . About your points:\n\n\nI put the batch_norm collection into the trainer because it has to be connected to the computation graph. I changed the place slightly in the last version of this pull-request.\n\n\nYou are right. This belongs to the model --> fixed. For the simple MNIST example, I think it is sufficient to put it there. However, in the long run, if you want to support tfSlim it might be good, to handle the regularization in the background. I mean, if set(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)) is empty, then there is nothing added to the graph. So this can be transparently done in the background.\n\n\nNow, BN-updates are only considered from a single tower which is sufficient for updates. \n\n\nI am not sure if the updated version - a mixture of your layers and tfSlim - is easier to understand :wink:. I understand your point. But in tfSlim the only purpose of the UPDATE_OPS collection is to manage batchnorm updates. In this sense the implementation from the pull request is totally equivalent to your BatchnormV2.\nAs there is no way of overriding the slim layers without patching the tf-repo, I have put it into the trainer.\nCurrently there is no overhead since it only updates the batch statistics in the main tower if the user asked for batchnorm.\n\n\nSame for regularization: As long as there is no explizit regularization within in the ModelDesc, there are no aditional operations added to the graph.\nIn these two points: I would argue that since it is well documented behavior of tfSlim one can add these in the backend. I would further say it is the only way to add these as the user expect these additional operations to happen when using tfSlim.\nIf you still want, I can rebase again on your current HEAD of the repo.. I put that new stuff into ModelDescr. However, this currently does not affect possible GAN-models. For the examples, I will probably create another repo not to mess up your code. You can then decide whether you want them or not.\nI think ModelDesc should have something like GanModelDesc such that the cost-function can be overridden.. Please wait before merging, there are still some issues with the scope:\n````\n  File \"run.py\", line 96, in main\n    SimpleTrainer(config).train()\n  File \"/home/patwie/git/tensorpack/tensorpack/train/base.py\", line 60, in train\n    self.setup()\n  File \"/home/patwie/git/tensorpack/tensorpack/train/base.py\", line 108, in setup\n    self._setup()\n  File \"/home/patwie/git/tensorpack/tensorpack/train/trainer.py\", line 78, in _setup\n    cost_var = model.get_cost()\n  File \"/home/patwie/git/tensorpack/tensorpack/models/model_desc.py\", line 129, in get_cost\n    regulization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES, scope=scope)\n  File \"/home/patwie/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 4163, in get_collection\n    return get_default_graph().get_collection(key, scope)\n  File \"/home/patwie/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2761, in get_collection\n    regex = re.compile(scope)\n  File \"/usr/lib/python2.7/re.py\", line 194, in compile\n    return _compile(pattern, flags)\n  File \"/usr/lib/python2.7/re.py\", line 247, in _compile\n    raise TypeError, \"first argument must be string or compiled pattern\"\nTypeError: first argument must be string or compiled pattern\n````. The last commit is tested with batch-norm and regularization:\nhttps://github.com/PatWie/tensorpack-recipes/blob/master/mnist/run.py\nand works.. Yeah. Fixed. But the exception still happens iff the scope has no name.. This requires #81 to be merge beforehand. I will rebase this pull-request on the master branch when #81 was merged.. Rebased on HEAD.. Should I further make some edits? How? Amend previous commits, make new commits? I originally planned to do this tomorrow.\nThe correct term is probably: saliency maps. The best references are:\nVisualizing and Understanding Convolutional Networks\nDeep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps\n\nThe guided backprob is based on these. Just being curious: Is there a reason to move the saliency_op from the symb-functions to this example.\nI did a quick search on google and found a related Theano version.\nBtw: related work is https://arxiv.org/pdf/1604.00825v1.pdf. If you put these files\ntox.ini\n````ini\n[tox]\nenvlist = py26,py27\n[flake8]\nmax-line-length = 120\nexclude = .git,init.py\n```\ninto the root-dir, thenflake8 .should give the same output as intravis.yml`\n````YAML\nlanguage: \"python\"\npython:\n   - \"2.7\"\nsudo: false\ncache: pip\nbefore_script:\n  - pip install flake8\n  - flake8 --version\nscript:\n  - flake8 .\nnotifications:\n  email: false\n````. Looks good! I will try to put my code in ModelDescr instead of trainer.. You should also not forget to add this to the settings of this github project such that each pull-request is tested.. That's why there is a try-catch-block. For most image-processing models it is good to double-check the dataflow-output on at least one sample. . That's a nice idea. I will update the code.. The linting failed because the current master has one issue:\n\n./ConvolutionalPoseMachines/load-cpm.py:27:1: E302 expected 2 blank lines, found 1\n\nThis was introduced in 13dc646b408e88532744c626691727a68e025bd0, see\nhttps://github.com/ppwwyyxx/tensorpack/commits/master. But the problem with any before/after step design is that this needs explicit addition sess.run. Here in this implementation it is simply extracted from the graph, when it is already computed.. So you mean, for some basic statistics of scalars you prefer callbacks instead of simple tf.collections? That's the purpose of those: collecting tensors.. I understand the current design. But within epochs you need to pass the data to tqdm or modifications of it to print it to the console. Another idea would be a TrainingStepCallback which contains the code of the most-inner loop from the training-main-loop. This would probably also help to change training-logic to GAN training. What do you think?. Any clever idea to adapt this feature as a step-trigger? Overriding just the default? Or replacing the tqdm callback?. I am not sure if the progress bar really has to be a progress.. @ppwwyyxx very nice refactoring of the step_trigger! \nI added these extra_fetches to Progressbar directly using postfix. Example is in SimilarityLearning.\nIt would be really cool, if tqdm could display both: examples/sec and sec/example.\nhttps://github.com/tqdm/tqdm/issues/341. It seems to be nearly impossible to run TensorFlow(CPU) and OpenCV with Travis-CI. I will reopen this pull-request, when I found a solution. . That's the point. There are several issues with Python under Travis:\n- NumPy uses ucs2 and TensorFlow ucs4\n- there is no OpenCV package for Python3\n- there seem to be inconsistent Python versions of 3.4 within a single Travis-CI container\n- compiling OpenCV from source takes quite some time (6min)\nI will further try to debug the process here:\nhttps://github.com/PatWie/travis_python_cv2_tf\nwhich is a cleaned version of my 75 commits from yesterday:\nhttps://github.com/PatWie/python_cv2_test\nUnfortunately, running the Travis-containers locally gives not the same environment and same paths. Even worse, the \"Trusty\" containers are in beta. So if they change their stuff as frequently as TF does, it will be tedious to adapt these changes.\nMaybe some of these might be worth to look at:\n- https://circleci.com/features/\n- http://codeship.com/\nI would be happy if even a combination of Python2.7+TensorFlow0.12+OpenCV3 would work. I did not try to compile TensorFlow from source. But I think this is not desirable.. I switched to CircleCI which saved a lot of time. \nOutputs:\nwith issue #92:\nhttps://circleci.com/gh/PatWie/tensorpack-fork/5#config/containers/0\nwith current HEAD+this pull-request:\nhttps://circleci.com/gh/PatWie/tensorpack-fork/7\nThis nice thing is that the entire run only takes bout 2minutes.\nThis also reveals that there might be another problem, although it is running fine here. This might be fixed when packing tensorpack as a pip package.. Alright\npython\ndef _setup_graph(self):\n        self.pred = self.trainer.get_predict_func(['noise', 'code'], ['generator/fake_samples'])\nis perfect!. I change those thing and added\nbash\npython -c \"import cv2; print('OpenCV '+ cv2.__version__)\"\npython -c \"import tensorflow as tf; print('TensorFlow '+ tf.__version__)\". You can use\npython\nstep_per_epoch=500\nin your train-config.\nThen the callbacks will be triggered after 500 updates. This is also related to #39 and step-callbacks.. How many images do you want to dump after each epoch?\nI use something related in my private examples to generate an animation from image files. \nThere is a small writeup how I export Images directly to JPEG without tf.summary in  OnlineExporter. There are two ways:\n- preload some images\n- use another dataflow instance. @ppwwyyxx \nDo you know what is going on with the python3 version here. I only use python2.. I will apply these changes 1-3.\nUnfortunately, there is no way using OpenCV for animated gifs. But I can remove the animated-gif script. I though the scripts folder is more or less a collection of scripts.. 1. change_gpu is really nice!\n2. fixed. I added variables during playing around with these loss-functions\n3. it's a bad habit of mine\n4. I removed it. But it would be a nice and helpful script. Avconv should be the replacement for ffmpeg on Ubuntu 16.\nAny naming-conventions? Upper-case seems strange. \n- directories: ucfirst\n- scripts: lower-case\n- images lower-case (jpgs)?. Did you turned Travis-CI off? (rebased on master). > Maybe it's better to write several dummy training scripts containing lots of features for testing. This may avoid downloading mnist or prepare dummy data for examples. What do you think?\nDo you really want to maintain additional examples without any function? Maybe just putting some images (some of image2image produced to not violate copyrights) and a small textfile into a repo  for wget then within travis. The release to run the real examples on very small datasets.\nThis will require a more flexible testing with teardown to clean up dirs such as train_logs. I have implemented these changes in my version:\n- mini-batch discrimination\n- switched the labels 0 <-> 1\n- label-smoothing\nHowever, I did not observe any (dramatic) changes in the examples. The results always look different in seperated runs. Did you run some tests with these changes? I can made another pull-request. But I do not want to exhaust @ppwwyyxx time with multiple pull-requests if they have no big improvement.\nBut the OpenAI guys have the correct log-likehood term in InfoGAN :wink:  I don't know what @ppwwyyxx's plans are. But the next example I planned to move to tensorpack is a clean-up of InfoGAN (GANTrainer should be a derived class of SimpleTrainer btw) with a distribution-class for different codes.. > correct log-likelihood\nI didn't saw the sign.. So they claim, they solved all issues with GAN :wink: ?\nAs far as I can tell the differences to implement are:\n- good weight initialization  (trivial to implement)\n- get all weights from discriminator (alias critic). I only know a hacky solution. Maybe @ppwwyyxx has an elegant way to implement this\n- add clipping (trivial to implement)\n- run several optimization steps of the discriminator (currently ugly hack when implementing)\nI just came up with a few lines, but I have to read the paper in detail.\n. It is not my primary focus but I really like to see a working wGAN as well. So far the progress is described here #135 hopefully without ugly hacks. \nThis thread should be only for \"Improving GAN\".. I think this is out-dated.. @ppwwyyxx Can you have a look before at the InfoGAN before I adjust the other examples.\n\nI would say it definitely learns some reasonable mappings from a latent factor ~ uni(-1, 1)\n\n\nBut it is not what I expected.. > The uniform distribution doesn't look as good as the papers (from figure and curve), is that what you mean?\nYeah, I do not get why my results are not correct.\n\nThe current code looks good, thanks a lot for that! Could you put the changes related to improved GAN in a separate PR?\n\nThe only change mentioned in #105 is currently \"swap labels\". But anyway when merging this one needs to update image2image, ... a well. If you say these changes are ok, I would adjust the other examples to make them work with these changes this evening.. Well, I haven't looked at their code in detail and started from your code. I just wrote a LaTeX file with all ugly calculations. In the end the entire InfoGan is simply a weighted sum of mutual information (including the GAN objective). Maybe I missed something. It is a good idea to get a 1:1 mapping to their code as a sanity check. \nI did not get the last point and have to look it up. \nEdit:\n\nthere is only one entropy term in the objective\nmaybe i should improve the documentation here\nthe sample also produces a batch\nconsider the uniform-distribution class: it samples also have mean 0\n\nCurrently, I am surprised why I expected these nice results because it is really a product of independent distributions. So it has to look the way from the post above. . This gives\n\nThe issue with the \"7\" and \"9\" columns can be probably solved by other hyper-parameters or even another run. Even the previous version had some trouble about \"1\" and \"7\". The paper says:\n\nUnless noted otherwise, learning rate is 2e-4 for D and 1e-3 for G; \u03bb is set to 1.. You accidentally used an old Readme during  merging ;-) But really nice edits!. I somewhere read it. But I never notice any differences. It is probably better to flip these labels real(1) and fake(0).. They talk about label-smoothing, not flipping.. Yeah, there is no difference. I think I misinterpreted\nreplaces the 0 and 1 targets for a classifier with smoothed values, like .9 or .1\n\nI would go with the default formulation. This affects almost everything ones_like, zeros_like, <, > in\nhttps://github.com/ppwwyyxx/tensorpack/blob/master/examples/GAN/GAN.py#L50-L64\nSorry. It should be\n````python\nwith tf.name_scope(\"GAN_loss\"):\n    score_real = tf.sigmoid(logits_real)\n    score_fake = tf.sigmoid(logits_fake)\n    tf.summary.histogram('score-real', score_real)\n    tf.summary.histogram('score-fake', score_fake)\nwith tf.name_scope(\"discrim\"):\n    d_loss_pos = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n        logits=logits_real, labels=tf.ones_like(logits_real)), name='loss_real')\n    d_loss_neg = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n        logits=logits_fake, labels=tf.zeros_like(logits_fake)), name='loss_fake')\n\n    d_pos_acc = tf.reduce_mean(tf.cast(score_real > 0.5, tf.float32), name='accuracy_real')\n    d_neg_acc = tf.reduce_mean(tf.cast(score_fake < 0.5, tf.float32), name='accuracy_fake')\n\n    self.d_accuracy = tf.add(.5 * d_pos_acc, .5 * d_neg_acc, name='accuracy')\n    self.d_loss = tf.add(.5 * d_loss_pos, .5 * d_loss_neg, name='loss')\n\nwith tf.name_scope(\"gen\"):\n    self.g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n        logits=logits_fake, labels=tf.ones_like(logits_fake)), name='loss')\n    self.g_accuracy = tf.reduce_mean(tf.cast(score_fake > 0.5, tf.float32), name='accuracy')\n\nadd_moving_summary(self.g_loss, self.d_loss, self.d_accuracy, self.g_accuracy)\n\n````. Reading values into numpy arrays is pretty straight forward:\n````python\n-- coding: UTF-8 --\nimport tensorflow as tf\nimport os\nread Graph + Checkpoint\nwith tf.Session() as sess:\n    graph_path = \"path/to/graph/foo.meta\"\n    model_path = os.path.dirname(graph_path)\nloader = tf.train.import_meta_graph(graph_path)\nloader.restore(sess, tf.train.latest_checkpoint(model_path))\ntrain_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\nfor v in train_vars:\n    name = v.name\n    tshape = v.get_shape()\n    # numpy from here\n    value = v.eval()\n    shape = value.shape\n    print name, tshape, shape\n\n. I use\nself._predictor_factory = PredictorFactory(self.sess, self.model, [0])\n````\nafter the super.init. You just need to switch the lines in the constructor.. Ok it works. I will push again, when the model is trained and I have some results. Can you please change the psnr with your next commit?. The network model and parameters are the same as in the paper. However, there are some difference in the pre-processing:\n- in their work they use YCbCr colorspace (should not make differences when using large batchsize, so I use RGB and observed no saturated pixels)\n- they first blur and then sub-sample for low-res, here I simply use bilinear downsampling\n- for their final results they trained in ImageNet, this implementation just uses BSDS500 (I should download ImageNet at some point)\nWarning: I cannot replicate the quality which is mentioned in the paper. This probably requires longer training and a larger dataset.\nedit:\n- it further seems to be an idea to standardize the visualization part from all examples across all example by create a new function in tfutils. Ah, I was cleaning unused branches and accidentally deleted this branch. Sorry for that.. I trained this network on ILSVRC12 for several days with 1.) Adam as in the paper and 2.) a scheduled LR (which gave better results). However, these are still far away from the results in the paper. I think this should be just a demo for training on small images and deploying to arbitrary images. \nI wonder, why they do not use this technique in their CVPR submission?!\nThe PixShift layer has been checked manually. \nedit: The result and question is an observation, not an appraisal.. Cannot reproduce results even when playing with hyperparameters. Still it is nice example to show how to use fully-convolutional networks during inference.. Superresolution is more or less a hobby for me. So I do not really rely on this. And the result is really worse than bicubic (I tried to optimize MSE or PSNR directly).\nI just though, this would be the most easy example for fully-convolutional.. Sorry for polluting this thread. But I am curios: What's the speed (it/sec or sec/iter) when training ResNet on your machine on imagenet (and what's the batch size, num gpus)? I get a very slow pre-fetching on my hardware here.\n@rohitgirdhar @ppwwyyxx \nAnd you are really reading from all these cluttered single imagefiles with their original size from the tar files? I just currently try to figure out the most efficient way.\n. I converted the entire tar file into lmdb. This gives 788GB for the training (without cropping) and is slow as hell when reading. Even sequential reading from tar is very slow (reads 17 images per second from ssd). That's why I was asking.\nThank you both for providing some concrete numbers.. I unpacked all JPEGS to SSD without cropping and now I can read 422 images per second. \nHow wrong I was thinking a single file would be faster. Is this roughly the same you get?\n````python\nfrom tensorpack import *\nds = dataset.ILSVRC12('/scratch/imagenet', 'train')\nds = MapData(ds, lambda dp: [dp[0][:, :, ::-1]])\nds = TestDataSpeed(ds)\nds.start_test()\n````. I used:\nhttps://gist.github.com/PatWie/9537acc0e1f24687613856b60dba7eab\nThe important part is Line 140.. I hope the TF-team get things fixed. I am a little bit afraid of doing git pull origin master inside the TensorFlow-repo to build an updated version for now.. merged into #128. This is not an issue of tensorpack or imaug.GaussianBlur. OpenCV already removes the last dimension\npython\nimport cv2\nimg = cv2.imread(\"original.jpg\")\ngrey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\nprint grey.shape # is (H, W) instead of (H,W, 1)\nYou should use something like\npython\nds = AugmentImageComponent(ds, [imgaug.MapImage(lambda x: x[..., None])])\nTo get the last axis back.. While I also use grayscale mappings often, I am not sure if it is really a good idea to include this into tensorpack because it depends on the image format: rgb or bgr.\nYou can copy\n````python\nclass Grayscale(ImageAugmentor):\n    \"\"\"\n    Convert image to grayscale.\n    \"\"\"\ndef __init__(self, keep_dim=False):\n    \"\"\"\n    Args:\n        keep_dim: return image of shape [H, W, 1] or [H, W]\n    \"\"\"\n    self._init(locals())\n\ndef _augment(self, img, _):\n    grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    if self.keep_dim:\n        grey = grey[..., None]\n    return grey\n\n````\ninto your script. And then\n````python\nfrom tensorpack import *\nds = ImageFromFile([\"original.jpg\"], channel=3)\nds = AugmentImageComponent(ds, [Grayscale(keep_dim=True)])\nprint next(ds.get_data())[0].shape # gives (H, W, 1)\n````\n. It is not inside GaussianBlur. It is the rgb2gray. At least with opencv 3.1.0.. Then feel free to close this PR when starting from scratch. I agree, creating a pip package is not the most easiest thing.. The current master branch has setup.*, so this can be closed.. The current implementation can do this, too.\nI like the API-design of having set_keys. But the function open_lmdb is already called in the constructor. So this means one has to write\npython\nds = CaffeLMDB('/data/imageNet/ILSVRC2012/lmdb/train')\nds.set_keys(np.arange(ds.size())\nds.open_lmdb()\nThe current version supports\npython\nds = CaffeLMDB('/data/imageNet/ILSVRC2012/lmdb/train', key_format=a_list_of_keys)\n. This is what I understand in proper documentation, but probably nobody else.. Ok,\nI tested the latest version with:\n````python\nfrom tensorpack import *\nds = CaffeLMDB('/data/imageNet/ILSVRC2012/lmdb/train', keys='{:0>8d}')\nds = PrefetchDataZMQ(ds, 4)\nds = PrefetchData(ds, 4)  # or this one, or none of the prefetching\nds.reset_state()\nfor i in ds.get_data():\n    img, label = i\n    print(img.shape, label)\n````\nI also prefer to let each process open a filehandle during reset_state as I observed some errors when directly dealing with tar files (in another dataflow reader).\nSo it works as expected. I will add the converter script (in another pull-request) as soon as I have figured out to write the compressed JPEG data and how to totally omit Caffe dependencies (optional). I saw, that already provide dump_data_to_lmdb.. Sounds good. One should not forget to add CaffeLMDB as an alias/wrapper, because it is very nice to load Caffe data within one line.\nI can do another pull-request with this refactoring, after I spend some time on the testing script to add more test cases and handle/caching the TENSORPACK_DATASET content.. You can encode this as a part of the graph. (I did it here https://arxiv.org/abs/1607.04433)\nThe dataflow provided the patches as a stack (tensor of 5 dimensions). It is just transposing and reshaping the data to handle a virtual batchsize of batchsize*patches. However, I am not aware of an easy solution to actually use the combined image (all patches stiched) again within the loss function. Usually this is trained on patches individually or one uses fully convolutional networks.\nBut if your 10000x10000px does not fit to the gpu, it won't fit as patches on the gpu.\nEdit: If I understand you correctly, you should google about Hanning-Window. Maybe there is already public code for that. But again this is usually done only during inference.. Then the definitiv answer is that the data provider needs to generate patches. Do this as a python generator to just feed a certain amount of patches into the net.\nFetch the output and combine everything  by Hanning-Window averaging method or similar.. Ok, MNIST is at least working and gives reasonable results when running all iterations. \nThis commit is rebase on last HEAD and contains some refactoring of GAN as well.\nI think its a good idea to put it into examples as it demonstrates how to change the trainer class when necessary.  How to proceed?. Aggree on #137 and will reopen when a more elegant way is available.. My understanding is, that the optimizer contains all code for an update-step (probably consisting of multiple ops). And the trainer feeds data into the model, calls the optimizer and manage gpus. Then the trainer should call optimizer.post_update or sth like this?\nSo, isn't this:\nhttps://github.com/ppwwyyxx/tensorpack/blob/master/examples/GAN/WGAN-CelebA.py#L70-L87\npart of the optimizer itself?. That would be the way if I would have tried to implement the changes. So the trainer just handles sync-multi-gpu, or so and each of these trainers can be applied to GANOptimizer, WGANOptimizer. Otherwise, whta is the advantage of putting the optimizer into the model, when it is just RMSprop, Adam and co?\nWould be nice to hear your definition of an \"optimizer\" vs \"trainer\".. I was in the middle of nowhere for some days with bad internet connection and missed several commits of tensorpack.\nI think the trainer is related to the infra-structure: multi-gpu, multi-device, sync, asyn, QueueInput, TensorInput and a good optimizer should not know which hardware is used or where the data is coming from. An optimizer should provide a compute_gradients or train_op.\nIn the ideal case any GANoptimizer, WGANoptimizer should work under all Trainers, which is currently not the case.\nSo clipping and run multiple steps can be part of the optimizer itself, like you already did.\nI would argue that the ratio to two optimization ops in the GAN should be part of the model. I will let you know if the is a discussable alternative, as the current implementation is understandable.. Isn't this already implemented in class DataParallelOfflinePredictor(OnlinePredictor)?. A quick&dirty test reveals a small speedup on imagenet-resnet.py for small batchsize. But for a proper benchmark the output of tqdm is to unstable.\nstandalone-test: https://gist.github.com/PatWie/7bd193c07ff72110997649cc83057699\ngives\n[staging] finished in 222 ms\n[FIFO] finished in 848 ms\n. \nWhen using ResNet-training it sometimes gives CUDA_OUT_OF_MEMORY:\nhttps://github.com/PatWie/tensorpack-fork/tree/staging\n. I also notice that the QueueInput is coupled to all trainers directly and StageInput as simple drop-in replacement (argument) is not possible without using hasattr since it lacks many features. \nSo this might be only interesting for inference or when using small datasets (MNIST).. This issue is about re-implementing the mentioned distributed version within TensorPack and not adding more dependencies. Probably the most straight-forward way is something like a multiple-devices-sync-trainer.. It seems that this requires more changes than I expected. Currently, I am not sure how the incoming data can be handle in the \"tensorpack\" way.\nBut I managed to produce a minimal working example\nhttps://gist.github.com/PatWie/89950d3f8491a9f0d84125dca0945afa\nwhich I actually tested on the following settings:\n- machineA: ps + worker, machineB: worker\n- machineA: ps + worker + worker\nI will try to add a \"distributed.py\" to the trainer section.. I looks like there was an annoucement at the devSummit. I rarely see machines that handle 8GPUs and each machine could locally read from the SSD. So the bottleneck would be at some other point.. Why did you remove the examples/distributed.py?. But an imagenet Resnet example to just get the usage would be nice.. Well, they do have benchmarks on the NVIDIA\u00ae Tesla\u00ae P100!. Ah, I see. The NVIDIA\u00ae DGX-1 is the $129,000 US single machine monster :scream::laughing:. Example:\n```python\nfrom tensorpack import *\n@deprecated(\"This is renamed to _get_inputs()\", \"2017-12-24\")\ndef _get_input_vars(a, b):\n    print((a, b))\n@deprecated(\"Use get_reused_placehdrs()\")\ndef get_input_vars(a, b, c):\n    print((a, b, c))\n_get_input_vars(1, 2)\nget_input_vars(3, 4, 5)\ndeprecated(\"step_per_epoch\")(\"Use steps_per_epoch\", \"2017-11-8\")\ndeprecated(\"config.set_tower\")(\"Set config.tower or config.nr_tower directly.\")\nclass Dummy(object):\n    @deprecated(\"remove this call\")\n    def test(self, a, b=7):\n        pass\ndef nested():\n    d = Dummy()\n    d.test(6) \nnested()\ndeprecated(\"_get_input_vars() is renamed to _get_inputs().\", \"2017-04-11\")(\"\")\n```. Standalone python3 version seems to work:\nhttps://repl.it/FgKc/0. Hi, \nI was also thinking about splitting the deprecation stuff into a function and decorator. The only prerequisite I would say is to allow something like \nbash\ngrep -rn . -e \"deprecated\" --include=*.py\nThere is probably no elegant way to use a decorator as a function itself without messing up with duck typing in python.. So you are using monitors? They will be deprecated:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/contrib.learn.monitors.md\n\nDEPRECATED FUNCTION\nTHIS FUNCTION IS DEPRECATED. It will be removed after 2016-12-05. Instructions for updating: > Monitors are deprecated. Please use tf.train.SessionRunHook.\n\nhttps://www.tensorflow.org/versions/master/api_docs/python/contrib.learn.monitors/other_functions_and_classes\n\nMonitor is deprecated in favor of SessionRunHook. If you're using a monitor, you can wrap it with a hook using function. It is recommended to implement hook version of your monitor.. See #200 . Ok I will restore changes belonging to datapoint.. I will do another pass tomorrow or so.. Probably it is better to merge this now to prevent manually resolving merge conflicts.. I do not really like the interactive prompt. I would do the following way #164. There should also be a way to not produce any train-logs (something like a debug-mode). Is there any elegant way to handle the entire thing \"keep/new/backup/delete\" via command line arguments?. I think these are two different information:\n- user-defined label\n- timestamp from current run\n\nMaybe something like [scriptname]-[userlabel | default:\"default\"]-[timestamp] as the subdir in train_log?. There is a small bug, when loading a check point from the current file and choosing \"delete\" the loaded checkpoint is delete. Good that I always set my best models to read-only ;-)\nBut you are right, setting the label should be done manually.. Wasn't there already an older version of tensorpack that saved the epoch number an global step into the checkpoint file?. Please note, that TF-record files do not support random-access as they contain only compressed information. Even for the number of entries, one need to iterate over the entire DB (similar to the LMDB without having a keys entry).\n. There are interesting findings:\n- matplotlib is not in requirements.txt. I am not sure if you want to add this.\n- the ResNet script relies on \"import caffe\". This is only for reading the mean-file, right? Do you mind to use a pickle or numpy msgpack file instead? Another solution is to create a repo: github.com/tensorpack/data and place the synsets.txt, synset_words.txt, ... imagenet_mean.bin there. I can also add a test for this mean-file, later.\nsee log:\nhttps://travis-ci.org/ppwwyyxx/tensorpack/jobs/214659882. The ResNet problem was the missing protobuf-apt package.\n@ppwwyyxx Can you have a look at some point. It seems that the ilsvrc-class reads from a different file the paths of the imagenet data. I am trying to fake a very small amount of image files on the fly and update the \"train.txt\" and \"val.txt\" files to read only these 4 images.\nHowever, I get\nhttps://travis-ci.org/ppwwyyxx/tensorpack/jobs/215508412#L356\nFile \"/home/travis/virtualenv/python2.7.12/lib/python2.7/site-packages/tensorpack/dataflow/dataset/ilsvrc.py\", line 184, in get_data\n              assert im is not None, fname\n          AssertionError: ilsvrc/train/n02481823/n02481823_2939.JPEG\nThis is surprising, as there is not such a line 'ilsvrc/train/n02481823/n02481823_2939.JPEG' in train.txt. I currently thinking about using FakeData by a flag in the ResNet example, which would simplify this testing.. After several edits, I was able to convince travis-ci to let this pull-request pass the tests. I changed some lines in the resnet example:\n- add argument for data_format (for training)\n- add argument for faking data\nYou probably want to squash and merge these changes.. I vote for adding this to the library. \n@tmquan \nMaybe the best way is to add an example and define it directly there. . What about patching the layers:\n```python\n@layer_register()\ndef Conv2D(x, out_channel, kernel_shape,\n           padding='SAME', stride=1,\n           W_init=None, b_init=None,\n           nl=tf.identity, split=1, use_bias=True,\n           data_format='NHWC'):\n...\nreturn_value = nl(tf.nn.bias_add(conv, b, data_format=data_format) if use_bias else conv, name='output')\nreturn_value.W = W\nreturn_value.b = b\nreturn return_value\n```\nThen one can write\npython\nl = Conv2D('conv1', image, out_channel=96, kernel_shape=11, stride=4, padding='VALID')\nprint sess.run(l.W)\n. One solution is patching the AugmentImageComponents class or using\n```python\nfrom tensorpack import *\nds = FakeData([(10, 10, 3)], random=False)\nds = MapData(ds, lambda dp: [dp[0], dp[0], dp[0]])\nds = AugmentImageComponent(ds, [imgaug.RandomCrop((4, 4))], index=0)\nds = AugmentImageComponent(ds, [imgaug.RandomCrop((4, 4))], index=1)\nds = AugmentImageComponent(ds, [imgaug.RandomCrop((4, 4))], index=2)\nds = PrintData(ds, num=5)\n```\ngives\ndatapoint 0<5 with 3 components consists of\n   dp 0: is ndarray of shape (4, 4, 3) with range [0.0204, 0.9997]\n   dp 1: is ndarray of shape (4, 4, 3) with range [0.0042, 0.9929]\n   dp 2: is ndarray of shape (4, 4, 3) with range [0.0204, 0.9997]\ndatapoint 1<5 with 3 components consists of\n   dp 0: is ndarray of shape (4, 4, 3) with range [0.0204, 0.9997]\n   dp 1: is ndarray of shape (4, 4, 3) with range [0.0042, 0.9929]\n   dp 2: is ndarray of shape (4, 4, 3) with range [0.0042, 0.9929]\ndatapoint 2<5 with 3 components consists of\n   dp 0: is ndarray of shape (4, 4, 3) with range [0.0286, 0.9997]\n   dp 1: is ndarray of shape (4, 4, 3) with range [0.0340, 0.9862]\n   dp 2: is ndarray of shape (4, 4, 3) with range [0.0286, 0.9408]\ndatapoint 3<5 with 3 components consists of\n   dp 0: is ndarray of shape (4, 4, 3) with range [0.0003, 0.9862]\n   dp 1: is ndarray of shape (4, 4, 3) with range [0.0286, 0.9300]\n   dp 2: is ndarray of shape (4, 4, 3) with range [0.0286, 0.9794]\ndatapoint 4<5 with 3 components consists of\n   dp 0: is ndarray of shape (4, 4, 3) with range [0.0207, 0.9929]\n   dp 1: is ndarray of shape (4, 4, 3) with range [0.0003, 0.9929]\n   dp 2: is ndarray of shape (4, 4, 3) with range [0.0286, 0.9300]. The problem is that AugmentImageComponents is implemented to\n\nApply image augmentors on several components, with shared augmentation parameters.. What about:\n````python\nfrom tensorpack import *\nimport copy\n\nclass RepeatedDataPoint(ProxyDataFlow):\n\"\"\" Take data points from another DataFlow and produce them a certain number of times\ndp1, ..., dp1, dp2, ..., dp2, ...\n\"\"\"\n\ndef __init__(self, ds, nr):\n    \"\"\"\n    Args:\n        ds (DataFlow): input DataFlow\n        nr (int): number of times to repeat each datapoint.\n    \"\"\"\n    self.nr = nr\n    super(RepeatedDataPoint, self).__init__(ds)\n\ndef size(self):\n    \"\"\"\n    Raises:\n        :class:`ValueError` when nr == -1.\n    \"\"\"\n    return self.ds.size() * self.nr\n\ndef get_data(self):\n    for dp in self.ds.get_data():\n        for _ in range(self.nr):\n            yield copy.deepcopy(dp)\n\nds = FakeData([(10, 10, 3)], random=False)\nds = RepeatedDataPoint(ds, nr=5)\nds = AugmentImageComponents(ds, [imgaug.RandomCrop((4, 4))], index=[0])\nds = PrintData(ds, num=5)\n````\nThe issue is that tensorpack overrides the input in AugmentImageComponent[s]. Therefore, after the first crop the next cropping is applied to an input which is already cropped. You can check this\nby\npython\ndef get_data(self):\n    for dp in self.ds.get_data():\n        for _ in range(self.nr):\n            print\"input is\", dp[0].shape\n            yield dp\nwhich gives\n```\ninput is (10, 10, 3)\ninput is (4, 4, 3)\ninput is (4, 4, 3)\ninput is (4, 4, 3)\ninput is (4, 4, 3)\n[0327 12:12:31 @common.py:655] DataFlow Info:\ndatapoint 0<5 with 1 components consists of\n   dp 0: is ndarray of shape (4, 4, 3) with range [0.0190, 0.9341]\ndatapoint 1<5 with 1 components consists of\n   dp 0: is ndarray of shape (4, 4, 3) with range [0.0190, 0.9341]\ndatapoint 2<5 with 1 components consists of\n   dp 0: is ndarray of shape (4, 4, 3) with range [0.0190, 0.9341]\ndatapoint 3<5 with 1 components consists of\n   dp 0: is ndarray of shape (4, 4, 3) with range [0.0190, 0.9341]\ndatapoint 4<5 with 1 components consists of\n   dp 0: is ndarray of shape (4, 4, 3) with range [0.0190, 0.9341]\nvspython\ndef get_data(self):\n    for dp in self.ds.get_data():\n        for _ in range(self.nr):\n            print\"input is\", dp[0].shape\n            yield copy.deepcopy(dp)\nwhich gives\ninput is (10, 10, 3)\ninput is (10, 10, 3)\ninput is (10, 10, 3)\ninput is (10, 10, 3)\ninput is (10, 10, 3)\n[0327 12:14:04 @common.py:655] DataFlow Info:\ndatapoint 0<5 with 1 components consists of\n   dp 0: is ndarray of shape (4, 4, 3) with range [0.0074, 0.9956]\ndatapoint 1<5 with 1 components consists of\n   dp 0: is ndarray of shape (4, 4, 3) with range [0.0010, 0.9808]\ndatapoint 2<5 with 1 components consists of\n   dp 0: is ndarray of shape (4, 4, 3) with range [0.0074, 0.9956]\ndatapoint 3<5 with 1 components consists of\n   dp 0: is ndarray of shape (4, 4, 3) with range [0.0094, 0.9222]\ndatapoint 4<5 with 1 components consists of\n   dp 0: is ndarray of shape (4, 4, 3) with range [0.0276, 0.9933]\n``. @ppwwyyxx I wonder whyds = AugmentImageComponents(ds, [imgaug.RandomCrop((4, 4))], index=[0], copy=True)`\nfrom the latest version does not work here.\nI think it should be:\ncopy_func = copy_mod.deepcopy if copy else lambda x: x\nedit: You also want to copy the entire dp instead of the components:\nsee #208. Nope,\nhttps://github.com/ppwwyyxx/tensorpack/blob/master/tensorpack/dataflow/common.py#L417. Ah, I should update my local copy. Sorry.. After the following commands\n```\ngit clone https://github.com/zeromq/libzmq\ncd libzmq\n./configure --prefix=$HOME/zeromq\nmake\nmake install\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/zeromq/include/\nexport PKG_CONFIG_PATH=$PKG_CONFIG_PATH:$HOME/zeromq/lib/pkgconfig/\nwget -O $HOME/zeromq/include/zmq.hpp https://raw.githubusercontent.com/zeromq/cppzmq/master/zmq.hpp\n```\nand explicitly adding\nCXXFLAGS += -I/home/patwie/zeromq/include/ -L/home/patwie/zeromq/lib/ -lzmq\nI get at least linking errors from tensorflow.. I have tested it and I have to add it. I currently try to create a bazel build file. Here I managed to let bazel download all dependencies automatically.\nCreating an userop at a different place then inside the TF repo is probably not the common use-case. I tried this some time ago without success and today also add a question to stackoverflow\n. You definitely need Gitter or Slack. I will try that.. Just a reminder:\nAs already discussed this only works with the correct protobuf version. When installing TF from source it is easy to find the correct protobuf version in\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl#L309\nThen\n```bash\nwget https://github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz\ncd protobuf-2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a\nreplace_by_sed() {   local regex=\"${1}\";   shift;   if echo \"${OSTYPE}\" | grep -q darwin; then     sed -i '' -e \"${regex}\" \"$@\";   else     sed -i -e \"${regex}\" \"$@\";   fi; }\nreplace_by_sed 's#https://googlemock.googlecode.com/files/gmock-1.7.0.zip#http://download.tensorflow.org/deps/gmock-1.7.0.zip#'   \"autogen.sh\"\n./autogen.sh\n./configure --prefix=$HOME/protobuf\nmake -j 4\nmake install\nexport LD_LIBRARY_PATH=$HOME/protobuf/lib:$LD_LIBRARY_PATH\nexport PKG_CONFIG_PATH=$PKG_CONFIG_PATH:$HOME/protobuf/lib/pkgconfig\n```. I strongly argue against sending Tensor-Protos over ZeroMQ. I tried to create a C++ example, but the documentation of TensorFlow clearly says, that they want you to put custom C++ code in the TF directory. I vote for using msgpack directly with a custom dataformat to further reduce the overall requirements when sending from C++. This would remove the TensorFlow-dependency.\nedit:\nhttps://github.com/cjweeks/tensorflow-cmake seems to be an alternative but need a compilation from source.. https://github.com/tensorflow/tensorflow/pull/8728\nmight be related. But I like the tf.decode part, such that you just need to send raw data. So it can directly send msg packed data.. Maybe a better design would be to implement two ops:\n- zmq_receive\n- msg_decode. As TF does not export these symbols, I wrote a small tensor_msg class which can be serialized by msgpack: \nhttps://github.com/PatWie/tf_zmq. I have thought about that. But if you are working in python, then it is easier to directly generate tf.Tensors and just send them. Anyway, my latest attempt to create a write.py is here:\nhttps://github.com/PatWie/tf_zmq/blob/bin/python/example.py#L9. Ah, I see your point.\nI was experimenting with pybind (seems easier than swig) and msgpack. \nThen,one just needs the sbuffer from msgpack.. Alright, it seems to be quite easy to send and receive the tensor_msg format with native python.\nSo all combinations are now working:\nsend (py, c++) and receive (py, c++, tf).\nhttps://github.com/PatWie/tf_zmq. As you already implemented the ZMQ operator --> close.. Should I create an example with that zmq and polish the implementation to add it to TensorPack?. So the usual business: make it compile, make it link, make it work, make it correct, make it fast, make it flexible. \nI will ping this thread, when I prepared something. But it will take sometime as it has low priority.. As far as I know, it is currently not possible to specify the GPUs for each worker. That's why I used a dict instead.\nMy current version (I haven't pushed yet) just creates a ClusterTrainer relying on clusterSpec and uses all GPU from CUDA_VISIBLE_DEVICES like the default behavior of TF. But the code is currently not complete. I will post any updates this week if I have time to code.. Now, \na few updates and findings on this:\n\nthe global_step tensor has to be placed on a specific device\nsometimes TF needs the global_step op in the optimizer call. It is required to prevent a stale optimizer.\nthere is an complex interplay between the MonitoredSession and the tf.train.Supervisor and I am not willing to re-implement most parts of the Supervisor class.\nit requires a lot of additional changes in terms of where to log and when to log or when to write a summary. It seems that the tf.train.Supervisor handles this by knowing whether the worker is a chief-worker or not.\n\nFor the current version I set the correct ip addresses in:\npython\nMACHINE_Q = 'xxx.x.xx.xxx'  # noqa\nMACHINE_G = 'xxx.x.xx.xxx'  # noqa\nand then fire up the example by\npython mnist-distributed.py --task_id 0 --job ps --gpu 0 # on MACHINE_Q\npython mnist-distributed.py --task_id 0 --job worker --gpu 0 # on MACHINE_Q\npython mnist-distributed.py --task_id 1 --job worker --gpu 0 # on MACHINE_G\nThe graph is building fine on all machines.  Although, I currently get an issue with the monitored session.\nAttributeError: 'DistributedTrainer' object has no attribute '_monitored_sess'\n2017-04-20 11:04:53.496895: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:647] failed to record completion event; therefore, failed to create inter-stream dependency\n2017-04-20 11:04:53.496958: I tensorflow/stream_executor/stream.cc:4125] stream 0x5c544a0 did not memcpy device-to-host; source: 0x330a3c0800\n2017-04-20 11:04:53.496972: E tensorflow/stream_executor/stream.cc:289] Error recording event in stream: error recording CUDA event on stream 0x5c54570: CUDA_ERROR_DEINITIALIZED; not marking stream as bad, as the Event object may be at fault. Monitor for further errors.\n2017-04-20 11:04:53.496989: E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_DEINITIALIZED\n2017-04-20 11:04:53.497000: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:203] Unexpected Event status: 1\nAborted\nIt seems that the sessioncreate.py in this repo already should distinguish between chief-workers and non-chief-workers to make a distributed version to work. The relevant reference is in https://github.com/tensorflow/tensorflow/blob/52dcb2590bb9274262656c958c105cb5e5cc1300/tensorflow/python/training/monitored_session.py#L343\nI . I messed up the last commit and will rebase again.\nedit: commit is rebase on the current HEAD. Their tutorial describes a distributed async training. But this pull request deals with a sync training procedure, which I found more intuitively and less hacky. I have no experience which version is faster (async/sync) or better. But I guess this depends on the actual problem. . closed in favor of the distributed branch. Nice work @ppwwyyxx !!. This is not the copy parts. These changes are about the RepeatedDataPoint.. Removed the copy-function. But this data-mapping is quite common.. Isn't the novelty of their work just adding a new update-op\nhttps://gist.github.com/PatWie/47ca4a676698952536d4e9bc07a4f49d#file-began-py-L43\n(this is not working, I have not the CelebA dataset here and should download it at some point.)\n. I have updated the gist and downloaded the celebA dataset. There is still a bug somewhere and I will have a look at it tomorrow. So it is actually \"running\" but producing nonsense.. Nice! Currently, I cannot see where I wired my layers in the wrong way. Would be cool to check it against your implementation.. So much luck dring your training! I also ran the models, but they also generated creepy faces. So BEGAN basically shows that that others just generate much more creepy faces. Their paper also states that they used their own dataset.. In the distributed settings, the init_op is passed to the Supervisor-class. I did not find a way to separate the creation of the Session and the call of the init_op (for global variables). I updated the pull-request and close this issue to have only one place to discuss.. The new tensorboard has a slider below each image-summary. If tf.summary.image is called on the same input, one could inspect the result directly in the browser from different epochs. This is currently not possible in tensorpack (at least this seems to be requested).. You can also write\nsess.run(self.d_min)\nsess.run(self.g_min)\nlater in the GANTrainer. But the \nhttp://devdocs.io/tensorflow~python/tf/control_dependencies\nconnect both operations within the graph, which reduces the sess.run overhead.. But this only works because of BatchNorm and leakyReLU. . Silly me. \nedit: Sorry: It is always the case after opening an issue, the mistake becomes totally visible.. A comment:\nI think it is hard to parse the current mnist-convnet example when starting working with tensorpack. Further, I always start by trimming the MNIST example, when I start a new project. I think a well-maintained boilerplate template without deprecation warnings is a nice start point.\nI think I started to mess up the code in mnist-convnet.py by hacking in the tfSlim library (which I never used again from that day).. It might be worth to refactor this into a class with __call__ to reuse code.. Just wrapping tf.layers will be a breaking change as you will never be able to load old models again without a lot of code in the ModelLoader. Further, some layers like BatchNorm in your lib have some extra versions (local stats) which are helpful.\nSo, we need \n- mnist-prettytensor.py\n- mnist-sonnet.py\n- mnist-tf.layers.py\n- mnist-sugartensor.py\n? :-)\nMaybe, providing the common layers is ok. They won't be changed again (maybe just the fused conv2d+bias). So I closed this pull-request.. You need the domain for fake-labels of type uint8.. You are right! I changed the behavior of the domain arg. Maybe range is more consistent (not correct, but consistent). There is a collision with the six.range.. You need to extract patches from the image and process each individually if the memory is not large enough. I barely see deep-learning algorithm which works on fullHD. Just change the line in:\nhttps://github.com/ppwwyyxx/tensorpack/blob/master/examples/HED/hed.py#L198. The line\noutputs = predictor([[im.astype('float32')]])\nruns the network on im and returns the result. So you have to do something like (pseudo-code)\nim = some_image\nfor patch in im:\n  rs = predictor([[patch.astype('float32')]])[5][0]\n  put rs at correct position in im\n. This is like a kind of MultiGpuPredictor with data-parallel in-graph replication.. You can use logger.set_logger_dir(/mounts/trainlogs/myproject') instead of logger.auto_set_dir(). @ppwwyyxx Are you really sure, that the reading speed is just 130 it/s (cached)? I get 400 it/s on local SSD with reasonable RAM.\n[0511 19:25:24 @format.py:84] Found 1281167 entries in /ssd/patwie/imagenet.lmdb\n[0511 19:25:24 @common.py:702] DataFlow Info:\ndatapoint 0<1 with 2 components consists of\n   dp 0: is ndarray of shape (375, 500, 3) with range [0.0000, 255.0000]\n   dp 1: is int of shape () with range [49]\n  3%|#3                                                |34741/1281167[01:27<49:52,416.50it/s]. I would still vote for these atomic on-liners ImageEncode, ImageDecode  as they are pretty common in any good LMDB based training procedure. And they do not force the user to remember the specific cv2-calls. In general, I agree on removing one-liner and promote to use MapData instead. However, the decoding and encoding are pretty much standard.\nPlease note, that the code from the current documentation does not work:\npython\nds = LMDBData(db, shuffle=False)\nds = LocallyShuffleData(ds, 50000)\nds = LMDBDataPoint(ds)\nIn LMDBDataPoint there is a isinstance(args[0], LMDBData) which is obviously False if there is a LocallyShuffleData between.\nMaybe a renaming from\nLMDBDataPoint to MsgPackDecoder or LMDBdumpDeoder is also more intuitive. . At least there should be a convenient usage:\n- get data\n- encode\n- dump\nand then\n- load\n- decode\nWhat about a combined Serializer class which can return properties like a data stream for reading and writing, like\nds = ...\nwrapper = MyFileTypeWrapper(file='/tmp/test.lmdb', format='lmdb')\nwrapper.serialize(ds)\nand then\nwrapper = MyFileTypeWrapper(file='/tmp/test.lmdb', format='lmdb')\nwrapper.deserialize(ds) # a combined version of image-encode, decode and loads and dump\n. Closing for now. But you should remove the lines:\nhttps://github.com/ppwwyyxx/tensorpack/blob/master/tensorpack/dataflow/format.py#L168-L171\nas we discussed here:\nhttps://github.com/ppwwyyxx/tensorpack/pull/267/files#diff-4a08f729b0314e28cdde668081b6ce22. if you run this network, you can use tensorboard and will see these filters after a while. Tensorboard will show (OnlineTensorboardCallback) both the expected and learned filters.. I changed the conv2d to fullyconnected layers and get a faster convergence:\n\n. I don't think, that their \"dynamic conv implementation\" is more efficient (in terms of speed and memory). Rather than  using their explicit im2col, this implementation uses a highly optimized call of TF. \nFurther, TensorFlow uses the cuda streams, so this would also hide the latency from data transfers between kernel calls from the [... zip(...)].\n. I have collected a large image data set, where I simply rejected images without any prominent edges. Using this custom dataset helps a lot to converge faster.\nSo, I would guess it depends heavily on the training data, e.g. try MNIST.. I changed these lines and also partly tested python3 compatibility.. see:\nhttps://github.com/ppwwyyxx/tensorpack/blob/master/.travis.yml#L39-L40\nI would suggest to build OpenCV from source and not let this library dictate how to build OpenCV.. You are wrong. The correct slogan is \"One more thing (tm)\".. The old API had a versioning. I am pretty confident, they will add this also to the new one.. The link is offline for a good reason: There are many changes in Tensor flow and I do not want to contribute another deprecated guide.\nThe file\nhttps://github.com/tensorpack/tensorpack/blob/35beb43c4b30f703f885358fd9eb2776b47378ee/examples/basics/export-model.py\nShould be pretty self-explanatory. If something changes, I will provide another PR to update this file whenever necessary. The serving part (Serverside) is up-to-date and covered as well under\nhttps://github.com/PatWie/tensorflow-cmake/tree/master/serving\n. Does tf.layers follows a common parameter pattern (name, input, param1, param2)? Does their BatchNorm provides \"use_local_stats\"?\nI also thought about a \"@tflayer_register()\", which is probably the smoothest transition towards tf.layers. Hopefully, the output of \"incoming-shape, outgoing shape, the number of params, size\" will still exist in the future tensorpack versions.\n. related log is: https://travis-ci.org/ppwwyyxx/tensorpack/jobs/239682860. The graph is finalized after the _setup_graph(). Why not writing a Callback for this task?. You can provide different shapes for different phases, if the Model-InputDesc have a shape-format like [None, None, None, 3] for rgb images. This is more about how to use a TensorFlow graph and not TensorPack.\nedit:\nBut using the test-split like you described is abusing it as a validation set. The common way (like @ppwwyyxx mentioned it) is to run the testing after the training+validation.\n. Do you really want us to write\npython\nlogger.set_logger_dir(os.path.join('train_log', basename[:basename.rfind('.')]) + \"idea\")\nall the time instead of \npython\nlogger.auto_set_dir(name='idea')\nedit\nHow do you solve this issue? The ResNet example only has logger.auto_set_dir(). What about TENSORPACK_LOGDIR. You already have TENSORPACK_DATASET and TENSORPACK_PIPES or so.. That git diff is only working, when one is regularly doing commits and there is a git-repo :-) But I like it.\nOk, maybe the word idea is misleading.\nCurrently, I am directly facing a problem, where I can easily adjust a hyper-parameter about e.g. the depth of the network and I just find it convenient to have a function which adds a suffix to the path. Still, my current solution with pen and paper is not optimal.. About the path issue: What about adding yet another argument \"root\" with Default \"./train_log/\"?. thanks. I though each Tower had its own varscope. Is this the new LeastLoadedDeviceSetter ? . Do not forget that TensorPack is still TensorFlow. Just do profiling!\n```python\nm = Model()\nhdrs = m.get_reused_placehdrs()\nwith TowerContext('', is_training=False):\n    m._build_graph(hdrs)\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n        options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n        run_metadata = tf.RunMetadata()\n        sess.run(tf.global_variables_initializer())\n    prediction = tf.get_default_graph().get_tensor_by_name('........')\n    sess.run(prediction, {hdrs[0]: .....}, options=options, run_metadata=run_metadata)\n\n    fetched_timeline = timeline.Timeline(run_metadata.step_stats)\n    chrome_trace = fetched_timeline.generate_chrome_trace_format()\n    with open('profile.json', 'w') as f:\n        f.write(chrome_trace)\n\n```\nThis will tell you which ops are executed on CPU and which are on GPU (even on which cuda stream). In addition, you get all timings.\nedit: It is an interplay with staring at nvidia-smi -l and top to see the performance.. In the multi-Tower setting you need to use \npython\nProgressBar(['tower0/cost:0']),\nEach tower has its own scope. To get the name of the Tensor, I do something like\npython\nself.cost = bla bla bla\nprint(self.cost.name)\nThis should give the name for the progressbar.\n@ppwwyyxx Should I add some logic to look into Tower 0, if no tensor was found?\nedit I think it is bad idea to edit the ProgressBar.. @ppwwyyxx I still do not get it, why it should be a problem. After all it is an entire computation graph. And the issues comes with Tf itself.\nTensorPack can provide a function get_tensor which tries to be smart enough to test whether there is a training tower or not. . I really like the current design. Let me just make an objection:\nIt would be frustrating to refactor all previously written models. \n\nYou want to have placeholders in the graph for the OfflinePredictor and exporting models.\nall stateless TF functions?\nIf you do not put it into the ModelDescr, do you want use to put it multiple times in the TrainConfig, PredictConfig, ... ?\nThe Trainer.tower[0].model.tensor_obj would be helpful in some cases. When previously defining self.tensor_obj = ... in the ModelDescr. Or just make tower0 the default.\nThere always will be a cost tensor in a model and it will be always a scalar. So it needs to really be  hard-coded there.\nAs far as I know you can always nest the tf.device context and I do not see any use cases. Most approaches are simply data parallel.\n\nIt is not really hard to get a tensor. You can always use the get_tensor_by_name method of TF. This is what makes TensorPack quite unique: it allows to use plain TensorFlow methods almost everywhere. This follows the \"Principle of Least Astonishment\", which is good.\nMaybe it is just a documentation issue.\n. @vqdang Are you looking for (?):\n```python\nm = Model()\nwith TowerContext('', is_training=False):\n    m._build_graph(m.get_reused_placehdrs())\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    # do you stuff here\n\n```\nThen the ctx is available.. The InputDescr is needed, otherwise the data flow does not know how the data goes into the graph. \nIf it wouldn't be there models hard to read as you have to backtrack the cost tensor to the input which sometimes require a large mental stack.\nAnd you can use Models in Models as the constructor is empty. I did this multiple times. Models parameterization is also possible, just add a constructor with the parameter. I used it to build entire recurrent networks and specify the time steps which can varying during interference. \nSo it is not as bad as it looks like \ud83d\ude04 \nAlso the partial inference: you just specify the output tensor and your done?! \nAnd in TensorFlow you can send data to placeholders only. This is not a limit of tensorpack which even provide a workaround.\nI just see the issue with the namespace, which depends on which by trainer is used. Maybe a default 'tower0' fixes this.\nIt might be that I inhaled already too much tensorpack code. Please do not try to make a second ideas or of it and oversimplify things.\nEdit: one more point about 'self.cost': why not changing this into a class-method 'self.cost()'? This might be helpful also in GanModelsDescr to hide collect_variables.\n. Be careful for these changes, e.g. the Offline-predictor now requires a single tensor as an input or a list. I found this a little bit unintuitive as I now have some cases, where I want to evaluate the perceptual loss only sometimes. So sometimes I feed in two images (and need a list) and sometimes only one image (without a list). This makes writing the code a little bit cumbersome. \nAre there any similar side effects here.. Given a List, we can anyway use func(*my_list). So there is no huge difference.. Update examples to use build_graph instead of _build_graph?\npython\ndef build_graph(self, image, label):\nand \npython\ndef _build_graph(self, inputs):\n   image, label = inputs\nare identical.\nCurrently, the internal API seems to use the nice version of build_graph(self, image, label) but the intended way (see examples) still seems to be build_graph(self, inputs). Shouldn't it be the other way around? The log_deprecated is never called in build_graph in the examples.\n  . Not sure, if build_graph(self, a, b) is better than _build_graph(self, inputs). In a recent project, we had something like\npython\ndef _get_inputs(self):\n    InputDescs = []\n    for i in range(ONLY_KNOWN_AT_RUNTIME):\n        InputDescs.append(InputDesc(tf.float32, (B, H[i], W[i], 3), 'input_%i' % i))\n    return InputDescs\nThis works quite nice in the current version, but might not work in the proposed changes. How does _build_graph looks like? How do you handle this case:\n```python\nclass Parent(object):\n    def init(self):\n        super(Parent, self).init()\ndef _get_inputs(self):\n    pass\n\ndef _build_graph(self, inputs):\n    pass\n\nclass Child(Parent):\n    def init(self, num):\n        super(Child, self).init()\n        self.num = num\ndef _get_inputs(self):\n    self.inputs = []\n    for i in range(self.num):\n        self.inputs.append(object)\n\ndef _build_graph(self, a, b):\n    print a, b\n\nc = Child(3)\nc._get_inputs()\nc._build_graph([1, 5, 6])\n```\nHow does return cost works with the SeparateGANTrainer?. For a second, it sounds like a nice idea, which might ease the way of using tensorpack for new users.\nBut again input-desc should be a function. Maybe, this would even allow to directly use tf.placeholders. I think for small examples (Mnist) the function style is much easier to understand. \nBut thinking about a bit longer, I feel VERY uncomfortable.\nPersonally, I like the way of using a class rather than writing multiple lines of a config. Passing tons of lines as a config instead of objects usually smells like a bad design.  In addition, inputs, graph and optimizer are not a loose collection they are strongly connected. In C++ you usually want to pass struct  objects instead of its members to functions, right? Your proposal sounds like dropping C++ and going back to C.\nWhen touching ModelDescr, maybe using the design of\npython\nclass Model\n  def inputs\n    return tf.placeholders\n  def graph\n    return cost\n  def optimizer\n    return Adam\nmight be a proposal from my side: just rename these methods. Directly using tf.placeholder might be possible as well. You can always place them first in a second helper-graph to get their shapes and just delete this helper graph.\nHopefully, when changing anything you will still support the class-based way. Tensorpack is now in a state, where people like myself do not want to maintain all old examples -- besides maybe renaming ModelDescr to ModelDescrOld.\nBut when ModelFactory can be replaced by ModelDescr without further steps, thumbs up for this nice addition! I am just attached to the current design :-)\n . This is awesome and exactly what I had in mind :+1: \nLet me now nag about _build_graph vs graph and _get_optimizer vs optimizer :-). Considering *args as a default in\n```python\nclass Parent(object):\n    def init(self):\n        super(Parent, self).init()\ndef inputs(self):\n    pass\n\ndef graph(self, *inputs):\n    pass\n\nclass Child(Parent):\n    def init(self, num):\n        super(Child, self).init()\n        self.num = num\ndef inputs(self):\n    self.inputs = []\n    for i in range(self.num):\n        self.inputs.append(object)\n\ndef graph(self, *inputs):\n    image, label = inputs\n    print image, label\n\nclass Child2(Parent):\n    def init(self, num):\n        super(Child2, self).init()\n        self.num = num\ndef inputs(self):\n    self.inputs = []\n    for i in range(self.num):\n        self.inputs.append(object)\n\ndef graph(self, image, label):\n    print image, label\n\nc = Child(3)\nc.inputs()\nc.graph(1, 5)\nc = Child2(3)\nc.inputs()\nc.graph(1, 5)\n```\nwould make the examples look much more intuitive. Looking forward to something like \n```python\n!/usr/bin/env python\n-- coding: utf-8 --\nFile: mnist-tflayers.py\nimport os\nimport argparse\nimport tensorflow as tf\nJust import everything into current namespace\nfrom tensorpack import *\nfrom tensorpack.tfutils import summary, get_current_tower_context\nfrom tensorpack.dataflow import dataset\nIMAGE_SIZE = 28\nclass Model(ModelDesc):\n    def inputs(self):\n        \"\"\"\n        Define all the inputs (with type, shape, name) that\n        the graph will need.\n        \"\"\"\n        return [tf.placeholder(tf.float32, (None, IMAGE_SIZE, IMAGE_SIZE), 'input'),\n                tf.placeholder(tf.int32, (None,), 'label')]\ndef graph(self, image, label):\n\n    image = tf.expand_dims(image, 3) * 2 - 1\n\n    l = tf.layers.conv2d(image, 32, 3, padding='same', activation=tf.nn.relu, name='conv0')\n    l = tf.layers.max_pooling2d(l, 2, 2, padding='valid')\n    l = tf.layers.conv2d(l, 32, 3, padding='same', activation=tf.nn.relu, name='conv1')\n    l = tf.layers.conv2d(l, 32, 3, padding='same', activation=tf.nn.relu, name='conv2')\n    l = tf.layers.max_pooling2d(l, 2, 2, padding='valid')\n    l = tf.layers.conv2d(l, 32, 3, padding='same', activation=tf.nn.relu, name='conv3')\n    l = tf.layers.flatten(l)\n    l = tf.layers.dense(l, 512, activation=tf.nn.relu, name='fc0')\n    l = tf.layers.dropout(l, rate=0.5,\n                          training=get_current_tower_context().is_training)\n    logits = tf.layers.dense(l, 10, activation=tf.identity, name='fc1')\n\n    tf.nn.softmax(logits, name='prob')   # a Bx10 with probabilities\n\n    # a vector of length B with loss of each sample\n    cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n    return tf.reduce_mean(cost, name='cross_entropy_loss')  # the average cross-entropy loss\n\ndef optimizer(self):\n    return tf.train.AdamOptimizer(0.001)\n\ndef get_data():\n    train = BatchData(dataset.Mnist('train'), 128)\n    test = BatchData(dataset.Mnist('test'), 256, remainder=True)\n    return train, test\ndef get_config():\n    dataset_train, dataset_test = get_data()\n    # How many iterations you want in each epoch.\n    # This is the default value, don't actually need to set it in the config\n    steps_per_epoch = dataset_train.size()\n# get the config which contains everything necessary in a training\nreturn TrainConfig(\n    model=Model(),\n    dataflow=dataset_train,  # the DataFlow instance for training\n    callbacks=[\n        ModelSaver(),   # save the model after every epoch\n        MaxSaver('validation_accuracy'),  # save the model with highest accuracy (prefix 'validation_')\n        InferenceRunner(    # run inference(for validation) after every epoch\n            dataset_test,   # the DataFlow instance used for validation\n            ScalarStats(['cross_entropy_loss', 'accuracy'])),\n    ],\n    steps_per_epoch=steps_per_epoch,\n    max_epoch=100,\n)\n\nif name == 'main':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--gpu', help='comma separated list of GPU(s) to use.')\n    parser.add_argument('--load', help='load model')\n    args = parser.parse_args()\n    if args.gpu:\n        os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n# automatically setup the directory train_log/mnist-convnet for logging\nlogger.auto_set_dir()\n\nconfig = get_config()\nif args.load:\n    config.session_init = SaverRestore(args.load)\n# SimpleTrainer is slow, this is just a demo.\n# You can use QueueInputTrainer instead\nlaunch_train_with_config(config, SimpleTrainer())\n\n``. Sometimes the pre-processing data can benefit from running on a GPU, e.g. you can use GPU 1&2 for training and 3&4 for pre-processing data on-the-fly (e.g. OpenCV supports GPUs or custom code)\nsee: https://github.com/ppwwyyxx/tensorpack/blob/master/tensorpack/dataflow/prefetch.py#L207. One iteration means one [update-step](https://github.com/ppwwyyxx/tensorpack/blob/master/tensorpack/train/base.py#L102). It is thenbatchsize*towers` unless you change it explictly.\nThe shuffle should be the same as in single-gpu.. What about introducing ranges for RngDataFlow? So each process handles a disjoin subset.. They use the \"slow\" K80 for benchmarking. I can confirm that this is an unrealistic setting (try that with the new TitanX). So far I can only use 2 GPU for real-world task and 4 GPUs for image classification on a single machine.. Why not using examples/scripts to put an example (maybe for the mnist-classification) for freezing a graph.. I used opencv to combine the old and new one. Too lazy to run all algorithms again.. I started writing a loss.py file. But I do not want to maintain it and it seems to be not in line with the decision to use tf.layers. But it might be worth at thinking about such a loss.py file as TensorFlow only supports streaming-metrics (not loss functions).\nThis can also incorporate all millions of proposed GAN-losses :wink: \n. 74ca05d, 1a262e8 Awesome! Any benchmarks :wink: ?. Working again. Update because of Spectre-NG-Foreshadow.. Did you try Ctrl+F5. I double checked it. I can see the page.. comparison\n\n\nI think, that this model is equally good at the Place Dataset, but does not generalize as good as the authors model. Seems to be reasonable, it is their teaser image which usually shows the best results. Further, there is some resizing going on in their inference code and they filtered out some images from the Place-Dataset, without describing details. And of course, I did not train the model for 3 weeks.\nAny suggestions?\n. I cleaned the README and the pre-trained model is here.. As this is commonly used, I am not sure if we should merge them into a single file hide the model behind flags and if-branches. But a good alternative would be having VGG16Model and VGG19Model.. Ok too late :grin: (already merged). Pretrained model is available as well.. Users dont care about the internal format of npy files. They want to restore weights from npy files like restoring from checkpoint. It is a matter of consistency.. I see your point. Just wondering, if class *Restore shouldn't share the same interface. It is not convenient if they differ.\n. I just code them, when I need them without asking anybody.\nHonestly, I don't care if @ppwwyyxx wants them to be included a public version of tensorpack or not. If not, I close the pull-request, like I did in the past. That's the nice thing about GIT and forks or monkey-patches.\nI understand the point, that this library wants to be light-weight. But I this case I am still convinced, it might be good to keep the interfaces at least consistent. So I use it in my local version. \nI respect @ppwwyyxx decision. So there should be no discussion.. Most of these papers kind of cheat by blurring the highres images before bi-cubic downscaling the image for a low-res input. If so, the gap between bi-cubic and network visually seems to be larger than here. This Gaussian blur usually helps the network as the information from the highres is partly available in the lowres.\n Further, the public benchmarks I found, use gray-scale images. \nI observed the opposite (matlab vs. opencv2 which agrees to https://stackoverflow.com/a/22093004). But I guess, this again depends on the specific input :-) I was a little disappointed seeing these public benchmarks only contain very few images....\nI don\u2019t have Matlab here anymore.. I do not have the time to test the entire dataset (it took enough time to install Matlab R2016a), but for the first example I took:\ncomparison.tar.gz\npython is slightly better:\npython 25.3957339631 db\nmatlab 25.0888434195 db\nI would guess there is no clear winner.\nBut I agree\n\nthe Matlab results (left) looks much better (although it has a lower PSNR) than the Python+OpenCV2 result (right).\nmay the PSNR be with you ... :-). Interestingly enough from Image import PIL gives significantly better performance\npython 25.3957339631 db\nmatlab 25.0888434195 db\npil    26.8381044874 db\nArrrgh, starting training from scratch again both pull-requests. But I still just hope, this makes no difference .... I might be easy to further convert the released model by the authors into a python dictionary, which can be restored by DictRestore. I haven't tried this yet and just used the author's code to compare to this implementation.. Results are available and comparable with the authors implementations (there is no clear winner)\nresults.tar.gz\n\n*.pngprediction.jpg: this implementation\n*-EnhanceNet.jpg: official weights\n\nI compressed them to JPGs for GitHub. I think this pull request is ready to be reviewed. My trained model is here:\nhttp://files.patwie.com/models/enet-pat.npy\nFeel free to add suggestions or edit when required.\nPlease consider, I am not that patient to train it more epochs and cherry pick the best model.\nComparison can be done by\nbash\nCHPT=/external/patwie/CHECKPOINTS/enet-pat.npy\nfor i in $( ls enhancenet_pretrained/input); do\n    python enet-pat.py --apply --load ${CHPT} --lowres \"enhancenet_pretrained/input/$i\" --output \"enhancenet_pretrained/output/$i\" --gpu 1\ndone. TensorFlow does very strange things when resizing images  with/without align_corners. For the sake of consistency I would like to keep the current version. \nFeel free to retrain the model :wink:. The visualization is pure LaTeX + Tikz. Saccade is only an image viewer to flip between images easily or the other stuff from the readme.. \"enet-pat\" (patch-, adversarial, & texture-loss) is from the paper, see Figure 3 in the paper. I trained them with two GPUs (hence batch size 12) until global step 806817. I use the checkpoint model-806817, because I felt the results were really good at this point already. Discriminator had accuracy around 95%, though.. This is a an issue from a cub primitive usually used in CUDA kernels and it is very likely to be caused by a TF issue itself.. Pull the latest version of TensorPack and the error seems to be gone.. The results are clearly better, though the training is not that stable.. closed in favor of 81d5fbd84da905e1a3aa0d168465079da35980dd. That's better. You are right. I just was looking for red warnings.. really strange, I cannot even reproduce it today on my machine.. What about adding a link to the model-zoo?\nhttps://img.shields.io/badge/pre--trained-models-brightgreen.svg\n. Keras has btw a very nice start-page:\nhttps://keras.io/\ncompared to\nhttp://tensorpack.com\nIs it possible to just use the readme.md as the landing page in the docs?. Not sure, if you want to use a separate file nvml.py. Here is a stand-alone version:\nhttps://gist.github.com/PatWie/68abc24de854ba87ff07c68dea61e778. I think\n```python\n    nvidia = NvidiaContext()\n    nvidia.create_context()\nnum_gpus = nvidia.NumCudaDevices()\n\nfor device in nvidia.Devices():\n    print(device.Name())\n    print(device.UUID())\n\n    print(device.Memory())\n    print(device.Utilization())\n\nnvidia.destroy_context()\n\n```\nis pretty simple. For proper usage of ctypes you need all these exception handling to not left the user with unkown errors.. In particular these files just do\npython\n__all__ = ['BatchNorm', 'BatchRenorm', 'layer_register', 'VariableHolder', 'Conv2D', \n'Deconv2D', 'FullyConnected', 'ImageSample', 'LayerNorm', 'InstanceNorm', 'LinearWrap', \n'Maxout', 'PReLU', 'LeakyReLU', 'BNReLU', 'MaxPooling', 'FixedUnPooling', 'AvgPooling', \n'GlobalAvgPooling', 'BilinearUpSample', 'regularize_cost', \n'l2_regularizer', 'l1_regularizer',\n 'Dropout', 'ConcatWith', 'SoftMax']\nA git-pre-commit-hook could produce these files directly. As far as the other pull-requests are discussed I could start working on this issue. I always need to look up the correct name for \"FixedSizeData, FixedDataSize, ...\". This would help!. I have the same issues in sublime text. Further, it is a waste of computational resources to compute the imports over and over again for autocompletion (even when there is a cache). (I see it as a waste of used brain cells to remember the concrete function names.\nI even saw the mess of writing all imports to make autocompletion work in the atom editor by a colleague. \nThese unsupported wrong imports would not happen if the init files would be human-readable and \"IDE readable\". Overall the framework has a very clean design. However, this is a recurrent issue in the entire framework.. sure see edits. See https://github.com/ppwwyyxx/tensorpack/blob/0ffdcc44a9288057be69ef9328a16dcc9526fa69/tensorpack/models/registry.py#L77-L101\nThe function wrapper adds this argument. This has some other subtle side effects like error messages like:\n\nfunction expects 5 arguments, but 5 are given\n\nwhich happens if you forget the name. This basically means:\n\nfunction expects 6 arguments, but 5 are given\n\nHope this changes, when transitioning to tf.layers. Honestly, it never came to my mind to artificially downscale the input image before running the inference in practice. Still, it makes no sense for me in this particular work with GANs (things are different for ENet-E and PSNR).\nBut if this is an issue for you @KindleHe, feel free to process the images with this implementation without down-scaling. And add a down-scaling afterward. Although it does solve the same task, it might be not entirely fair.\nTo cite Colorful Image Colorization\n\nWelcome! Computer vision algorithms often work well on some images, but fail on others. Ours is like this too.\n\nThis is ~~probably~~ definitely the case here as well. You will ~~probably~~ find images, where the author's implementation beats this implementation. \nOne last thought: I am frequently wondering how people publish methods: Do we benchmark these algorithms against specific down-scaling implementations and on synthetic cases? This adds another complexity by introducing aliasing artifacts besides other subtleties.\nOr do we want to solve real-world problems?\nWith/without anti-aliasing? Bicubic, bi-linear, Lanczos? OpenCV, PIL, TensorFlow implementation?\nIt is really ludicrous when all this literature is about inverting some specific down-scaling operation implementation.\nFeel free to disagree.\nAnd yes you get the same behavior with the other linked implementation from the recipes repo.. How does \"augmentors\" looks like? If you introduce randomness there --like random crop etc-- the evaluation error will naturally varies.. I do not see the issue here. Are you referring to the Conv2D layer in tensorpack.model? It was communicated that the preferred way of writing models is using the official TF layer implementations like tf.layers.Conv2D. Conv2D is more or less a wrapper for TF.layers.\nSo this seems to be an unsupported feature there.\nAnd you can always remap the get-variable calls with the var-utils. Cross-reference #382 . :scream: : Could this please handled by the specific load function implementation itself rather than directly removing bytes, iff bytes was necessary for msgpack. Currently, new tensorpack versions would break all existing code relying on msgpack.\n. > iff bytes was necessary for msgpack\nIt is really about \"if and only if\".\nI am not using the git-HEAD of tensorpack anymore since pyarrow was used as the default and installed by the requirements. So I did not test it and just saw this change.\nBasically all LMDBs which took quite some time to build here are serialized using msgpack. I went through some trouble when pyarrow was set to the default.\nI highly vote along these changes for introducing an ENV-var -- call it TENSORPACK_SERIALIZE=pyarrow/msgpack  or so. De-installing the pyarrow package is not an acceptable solution here when relying on loads with msgpack.\nI reverted back to an old version which did not break data loading.\nedit: I now fear when seeing some changes applied to the returned value from loads.. The current state allows to write something along the lines\n```python\n!/usr/bin/env python\n-- coding: utf-8 --\nimport tensorflow as tf\nfrom tensorpack import *\npatch_tflayers(tf.layers)\nnet = tf.placeholder(tf.float32, (None, 28, 28, 1), 'image')\nwith argscope([tf.layers.conv2d], padding='same', activation=tf.identity):\n    net = tf.layers.conv2d(net, 32, 3, name='conv0')\n```\nwhich will produce the output\n('passed kwargs', {'padding': 'same', 'activation': <function identity at 0x7f6092031ed8>, 'name': 'conv0'})\nas expected. This is just intended to discuss possible side-effects I am not aware of. Ideally, this tf.layers patch would further support the overview table of all used layers.\nPossible points to discuss:\n- Q1: should we hard-code the list of supported tf.layers.methods?\n- Q2: any better way than using patch_tflayers (should this run automatically)?\n- Q3: if decided to support tf.layers in argscope any other libraries which should be considered in the same edits\nQ1: I am afraid, that no hard-coding these methods would make this hack unmaintainable\nQ2: I feel there are low costs when asking the user to ad this line explicitly which further avoids unexpected behaviour.\nQ3: As tf.layers is officially supported in TensorFlow I would say, it is reasonable to stick with tf.layers only. I changed the mnist example with tf.layers. The function enable_argscope_for_lib is be a little bit more generic as now enable_argscope_for_lib(tf.layers) is sufficient. I do not recommend to hard-coded tf.layers in argscope.py to not introduce another dependency.\nWell, tf.keras.layers :unamused: is a different beast. I have an aversion to a library trying to wrap plain tensors (same reason I dislike the upper-case tf.layers methods). Is there any advantage besides direct access to parameters?. I know the npz can be quite large. A solution is to use two different InferenceRunner (small with CacheData and large with generated on-the-fly data).\nI currently need to write an extra ds.dump_to_lmdb function to make fair comparisons between different architectures. The changes would make it much easier. I am open to change npz to lmdb. But lmdb is not a required dependency and lmdb is without compression (by design).. I was hoping for such a discussion. It not clear to me, what the ideal solution would be. In fact, I don't even think npz is the best way?\nI was thinking about something along the lines of:\nds = CacheData(ds, serialize_obj=NpzSerializer('cache.npz'))\nI just put it in the current draft into \"CacheData\", since otherwise, I don't need CacheData at all. But using DataFlowFromNpz  would put the if-else logic into the user-code and needs to be repeated all the time. That's what I am currently doing all the time.\nI just want to prevent me from doing the same stuff again and again.\nThe problem or discussion should be: How can we ensure, that the validation data which can be generated on-the-fly is stored in some persistent way for a fair comparison of different models.\nMy current way is:\n- writing a data_sampler.py storing stuff into train.lmdb and val.lmdb.\n- writing a data_provider.py doing stuff on-the-fly\n- running data_provider.py for just the validation data and dump ot again into val-augmentation-baked-in.lmdb\n- train network using  'train.lmdbandval-augmentation-baked-in.lmdb`. Any suggestions how to reduce the code and not to just rewrite it? I am looking for a way to make my code easier to understand and I just want to hide the if-else logic somewhere deep in tensorpack.\nI was just hoping, to reduce the constantly recurring python code. I cannot see it anymore.. closed in favor of #797, which allows\npython\ndef get_val_data():\n    if not exists(file):\n        ds = ....\n        NumpyDataWriter(ds, file).serialize()\n    return NumpyDataReader(file)\nor\npython\ndef get_val_data():\n    if not exists(file):\n        ds = ....\n        HDF5DataWriter(ds, file).serialize()\n    return HDF5DataReader(file)\nor\npython\ndef get_val_data():\n    if not exists(file):\n        ds = ....\n        LMDBDataWriter(ds, file).serialize()\n    return LMDBDataReader(file). The issue in the old dump_dataflow_to_lmdb_old has been\npython\ntxn.put(u'{}'.format(idx).encode('ascii'), dumps(dp))\ninstead of \npython\ntxn.put(u'{:08}'.format(idx).encode('ascii'), dumps(dp))\nsee docs:\n\nLMDB is a tiny database with some excellent properties:\n- Ordered map interface (keys are always lexicographically sorted).. Travis test fail due to some pyarrow issues. I do not use pyarrow. A workaround is checking \npython\nif os.environ.get('CONTINUOUS_INTEGRATION', 'false') == 'false':\n   do_test() # we are not in travis and can do the test \n(but this smells very bad).\n\nHere CONTINUOUS_INTEGRATION is a default ENV-var.\nAny idea, why pyarrow creates such issues. MsgPack works fine here on my local machine and passes the unit test.. The fix is working, but benchmarking shows some bad results:\n```python\nimport msgpack\nimport pyarrow as pa\nimport time\nimport numpy as np\nbuf = []\nfor _ in range(1000):\n    buf.append(np.random.randn(256, 256, 3))\nserialized = []\nfor dp in buf:\n    serialized.append(pa.serialize(dp).to_buffer().to_pybytes())\ns1 = 0\nelapsed = time.time()\nfor enc in serialized:\n    el = pa.deserialize(enc)\n    s1 += el.sum()\nelapsed = time.time() - elapsed\nprint('pyarrow.to_buffer().to_pybytes()', elapsed)\nserialized = []\nfor dp in buf:\n    serialized.append(pa.serialize(dp).to_buffer())\ns2 = 0\nelapsed = time.time()\nfor enc in serialized:\n    el = pa.deserialize(enc)\n    s2 += el.sum()\nelapsed = time.time() - elapsed\nprint('pyarrow.to_buffer()', elapsed)\nelapsed = time.time()\ns3 = 0\nfor dp in buf:\nenc = msgpack.dumps(dp, use_bin_type=True)\nel = msgpack.loads(enc, raw=False)\ns3 += el.sum()\nelapsed = time.time() - elapsed\nprint('msgpack.dumps()', elapsed)\n```\ngives\n('pyarrow.to_buffer().to_pybytes()', 0.20410513877868652)\n('pyarrow.to_buffer()', 0.12622380256652832)\nAnd msgpack is for some reasons not working in this benchmark\nBut this slow-down might be ok, if it was just when writing. But the benchmark measures reading/decoding. I guess this can be hided as it depends on i/o speed.\nReading from cached file\n```python\nfrom tensorpack.dataflow.base import DataFlow\nfrom tensorpack.dataflow.dftools import LMDBDataWriter\nfrom tensorpack.dataflow.format import LMDBDataReader\nimport os\nimport numpy as np\nimport time\ndef delete_file_if_exists(fn):\n    try:\n        os.remove(fn)\n    except OSError:\n        pass\nclass SeededFakeDataFlow(DataFlow):\n    \"\"\"docstring for SeededFakeDataFlow\"\"\"\n    def init(self, seed=42, size=32):\n        super(SeededFakeDataFlow, self).init()\n        self.seed = seed\n        self._size = size\n        self.cache = []\ndef reset_state(self):\n    np.random.seed(self.seed)\n    for _ in range(self._size):\n        label = np.random.randint(low=0, high=10)\n        img = np.random.randn(256, 256, 3)\n        self.cache.append([label, img])\n\ndef size(self):\n    return self._size\n\ndef get_data(self):\n    for dp in self.cache:\n        yield dp\n\nds = SeededFakeDataFlow(size=1000)\nLMDBDataWriter(ds, 'tmp.lmdb').serialize()\ns = 0\nds = LMDBDataReader('tmp.lmdb', shuffle=False)\nds.reset_state()\nelapsed = time.time()\nfor dp in ds.get_data():\n    s += dp[1].sum()\nelapsed = time.time() - elapsed\nprint('pyarrow.to_buffer().to_pybytes()', elapsed)\n```\ngives no significant difference when averaging multiple runs\n```\n('pyarrow.to_buffer().to_pybytes()', 0.3695650100708008)\n('pyarrow.to_buffer()', 0.3536560535430908)\n```. Just in case you expected a response from my side:\n\nIt returns a buffer that works like bytes most of the time.\n\nThen I found exactly the situation where this does not hold.\nI do not think this increases the complexity. All these writers are following the same interface. It is a matter of consistency: Adding dump_npy and dump_hdf5 just feels wrong.\nRegarding the naming conventions: I am not strong about the exact naming, I just followed TestDataSpeed, which could be test_data_speed(ds) as well for the same argument.\nFor sure, dump_lmdb and LMDBDataPoint is unintuitive when compared to LMDBDataWriter and LMDBDataReader. And I don't think read_lmdb(path) like dump_lmdb is a good choice either.. Then\n\u02cb\u02cb\u02cbpython\nDataflowSerializer.save(ds, \"db.lmdb\") # same as np.save\nDataflowSerializer.load(\"db.lmdb\")\n\u02cb\u02cb\u02cb\nwould be most straightforward depending on the actual file extension (\"lmdb,npz,tfrecord,h5\") or eventually use .save(ds, filename, format=\"lmdb\", allow_overwrite=False).\nI do not want to rewrite it again and again.\nLet me know this sounds ok for you. \nThere is one issue though: hdf5 requires a datapath argument. Tightly couple all functionalities into a single class/function look a bit weird. But I guess for the first version it would be ok.\n. As I am absolutly convinced that a function-based interface for serialization is not an improvement, I implemented your second version:\npython\nds = SomeData()\nNumpyDataSerializer('mydataset.npz').serialize(ds)\nds2 = NumpyDataSerializer('mydataset.npz').load()\nBut the implementation is really awkward. There is not so much which can be shared between <Some>DataSerializer.serialize() and <Some>DataSerializer.load(). The only thing is the filename.\nCurrently, the load method simply returns the <Some>DataReader() object. This really hurts: just returning another object.\nWriting a dataflow to disk and reading a dataflow from disk are two very different things and require different objects. In Python preferring classes over function keeps the implementation easy and avoids code duplication. \nThe more time I spend thinking about a good design, the more I prefer and really start to like my previous version from a1892b5fe0606437bd0a5f9d83d7c7f2f6239628. I suggest doing a rollback to this commit.\nIn this commit, there was no code duplication and no nesting of different objects, just\n```python\nds = SomeData()\nNumpyDataWriter(ds, 'mydataset.npz').serialize()\nI can live with ...\nNumpyDataWriter('mydataset.npz').serialize(ds)\n... as well, which might support multiple incoming ds in the future\nloading some data\nds2 = NumpyDataReader('mydataset.npz')\n```\nThis looks pretty straightforward and it is something I can easily remember without having a look at the tensorpack docs (a very rare thing compare to TestDataSpeed, dump_to_something).\n\nGoing from the traditional way to creating one class for each serializer/deserializer is too far a step\n\nI think it is exactly the opposite. Think of a base-class which provides logic to prevent overriding files, verifying dataflows.\nRollback to a1892b5fe0606437bd0a5f9d83d7c7f2f6239628?\n. I do not see any gain in having a load function which simply wraps/nests\nhttps://github.com/tensorpack/tensorpack/blob/d5297c04578133fdaf14fe51b36919fd7da1ea76/tensorpack/dataflow/serialize.py#L111-L112\nover a separate TFRecordWriter and TFRecordData object.. I only had a comment about the keys for LMDB.. If you merge this, I would use it as a starting point for the HDF5 saver/loader and adapt/add the unit tests later.. Not if you ask me. But it is overly commonly used.. This is a cuDNN issue and not related to tensorpack at all. \nYou are using a cuDNN version which is not compatible to your TensorFlow version.\nBtw: I am convinced the Issue Template is not intended to be a joke and was meant to be filled out, especially the TensorFlow version is important.. rebased to current HEAD and remove local documentation (mistake). The inference runner uses the prefix validation, no? Hence, the namings are clear by ScalarStats\nhttps://github.com/tensorpack/tensorpack/blob/1554550de5322362af52b198587ee832b17f223b/examples/basics/mnist-tflayers.py#L125\nThese would be good candidates for the custom scalars. This would be a neat way to combine validation and training metrics in one chart without any ugly hacks.. We do not need all options, just these from the PR over there \ud83d\ude09 \nhttps://github.com/tensorflow/tensorflow/pull/20902/files. I know, this is only the inference part (like the VGG, AlexNet examples). The links to the ported Caffe-Model probably need to change towards the model zoo.\nI consider to re-implement the FlowNet2 or PWC. This is a good start to do a verifiable implementation for TensorFlow (part of FlowNet2 is FlowNet2-S).. The weights of FlowNet-S are used. FlowNet-s is a thin version.\n\nFor the stack configuration we append upper- or lower-case letters to indicate the original\nFlowNet or the thin version with 3/8 of the channels.. Added FlowNet2-C. ~~Until they merged this PR somehow, tensorpack needs to ship the custom operation. To keep this C++ stuff reasonable small, I just included a 1:1 refactoring of the pytorch version with giving credits (which is basically a refactored version of the original version) instead of a complete re-implementation GPU, CPU and some modifications (see PR).~~\n\nThe latest commit adds the full implementation.. I think everything is complete for now. The training protocol seems to be just a tedious task when re-implementing. But this would be another PR. If this PR ok, we should copy the weights to models.tensorpack.com as well. I can do this on the server-side (no need to re-upload them). Wow, what an incredible smart move on my side. I didn't saw the CMakeLists.txt being in the .gitignore file :joy:\nI started a rudimentary python implementation but gave it up in favor of the CPU version. \n\nThat's only required when using CUDA >=9.1 (in an incompatible combination with Ubuntu18.04)\nTF 1.9, 1.10 stable versions work (was a TensorFlow issue before)\nis now written in the Readme.md\nI don't see this point. It works fine even on a GTX 960.\n\nAnother workaround would be just the CPU version, which reduces all your points to \"2\". If I find some free time, I could hack a fallback option as pure TensorFlow.. Not sure, what you are doing, but the CPU version has 8 nested for-loops. You might be able to get away with the loop over B. \nAnyway, a working and correct pure TensorFlow implemenation is:\n```python\ndef correlation(A, B, kernel_size, max_displacement, stride_1, stride_2, pad, data_format):\n    import numpy as np\n    \"\"\" This is a fallback option for the correlation cost layer\n    \"\"\"\n    assert kernel_size == 1\n    assert data_format == 'NCHW'\nb, c, h, w = A.shape.as_list()\n\nr = max_displacement / stride_2\nd = 2 * r + 1\nborder = max_displacement\ndr = int(max_displacement / stride_2)\n\nCout = int(d * d)\nHout = int(np.ceil((h + 2 * (pad - border)) / stride_1))\nWout = int(np.ceil((w + 2 * (pad - border)) / stride_1))\n\nApad = tf.pad(A, [[0, 0], [0, 0], [pad, pad], [pad, pad]])\nBpad = tf.pad(B, [[0, 0], [0, 0], [pad, pad], [pad, pad]])\n\nres = []\n\nfor tj in range(-dr, dr + 1):\n    for ti in range(-dr, dr + 1):\n        res_h = []\n        for h in range(0, Hout):\n            h1 = int(h * stride_1 + max_displacement)\n            res_w = []\n            for w in range(0, Wout):\n                w1 = int(w * stride_1 + max_displacement)\n\n                patchA = Apad[:, :, h1:h1+1, w1:w1+1]\n\n                w2 = w1 + ti * stride_2\n                h2 = h1 + tj * stride_2\n\n                patchB = Bpad[:, :, h2:h2+1, w2:w2+1]\n\n                ans = tf.reduce_mean(patchA * patchB, axis=[1], keepdims=True)\n                res_w.append(ans)\n\n            res_h.append(tf.concat(res_w, axis=3))\n        res.append(tf.concat(res_h, axis=2))\nres = tf.concat(res, axis=1)\nreturn res\n\n```\nBut it takes years until the graph is built (input shape [1, 256, 48, 64]). I cancelled the process after a few minutes.\nAnd this requires H and W dimension to be known beforehand (not None). Did you pull some magic tricks here? Your code is fine! :+1: \nStill the network is not fully convolutional anymore.. I add the note about the mysterious 20 (which is essentially the max_displacement the network can handle). I not gonna touch any operation or change them. The version works fine. Feel free to change anything.\nAEE(train) on Sintel-Clean of 2.104513 (authors report 2.03). \nDespite all effort, there must be a different between Caffe and TensorFlow. But I guess this difference is ok. For the evaluation, I followed this implementation and simply crop the input images.. I hope I cherry picked all commits during rebasing on master.. Yes, for FlowNet-S there are some waaaaaaay out of range. However, on a few examples against the reference implementation I get:\nactual (1, 2, 96, 128)  expected  (1, 2, 96, 128)\nsum-error  0.0009913198\nmean-error  3.323065428514557e-14\nI will test those\n\n\n\n\n\nThis is a little bit intriguing as all these modules are used in FlowNet2 itself. But you are right according to the paper the numbers should be somewhere around ~4.50. \n\nIt seems that these two models are not doing anything useful.\n\nFor some inputs, it is the case.. Weights are now fixed (the missing weights for flownet-c) and the model is fixed\nSometimes a single character makes a difference.  :unamused:\nI uploaded a new version of (flownet-c.npz). I tested (downloaded the weights  and started the run) FlowNet-S and FlowNet-C (results are below):\n| Model|  AEE(sintel clean) |\n| ------ | ------ |\n| FlowNet-S | 3.82|\n| FlowNet-C | 3.08|\n| FlowNet2 | 2.10 (authors report 2.03) |. I guess overwrite get_data in the base class is sufficient:\n\nbase-class will take care of get_data and correctly forward to __iter__\n\n__iter__ will be called and everything is ok.. If refactoring the dataflow is ok, there are some other aspects I would like to discuss or even rebel against:\n\n\nreset_state seems to be unnecessary long. Why not simply reset? There is no ambiguity but a high chance for typos when your mind is already pre-paring the code line.\n\nThere should be a consistent naming: Currently, we have: dp for data point (which is ok) and we have df and ds for data-flow (I guess the latter is data-stream?). If would do another pass and just rename ds to df.\n\nAnd the last point (not this PR) would be a guarantee to sample each data-point per epoch. As far as I understand, Prefetch<someSuffix> allows me to hog the entire CPU power and pre-process data. But having 12 worker processes gives me a really high chance to miss some examples in one epoch when each worker shuffles data.. Then the only way is to use get_data() and just add __iter__(). Certainly, we do not want to call reflection magic during pre-fetching. What about introducing a \"(RNG)Dataflow_v2\" class (similar to what TensorFlow is doing as a workaround to not break semver promises eg. concat_v2 vs concat back then).\nFor each technical problem there is a legal solution: just bump the major version ;-)\nEdit Do you really care about this compatibility? Do you care about SemVer? I mean TP is v0.8.8.. With the latest commit the following works:\n```python\nfrom tensorpack import *\nclass SomeOldDf(DataFlow):\n    def size(self):\n        return 2\ndef get_data(self):\n    for dp in [1, 2]:\n        yield [dp]\n\nclass SomeNewDf(DataFlow):\n    def len(self):\n        return 2\ndef __iter__(self):\n    for dp in [1, 2]:\n        yield [dp]\n\nds = SomeOldDf()\nds = BatchData(ds, 2)\nprint(next(ds.iter()))\nprint(len(ds))\nds = SomeOldDf()\nds = BatchData(ds, 2)\nprint(next(ds.get_data()))\nprint(ds.size())\nds = SomeNewDf()\nds = BatchData(ds, 2)\nprint(next(ds.iter()))\nprint(len(ds))\nds = SomeNewDf()\nds = BatchData(ds, 2)\nprint(next(ds.get_data()))\nprint(ds.size())\nds = SomeOldDf()\nfor dp in ds:\n  print(dp)\nds = SomeNewDf()\nfor dp in ds:\n  print(dp)\nds = SomeOldDf()\nfor dp in ds.get_data():\n  print(dp)\nds = SomeNewDf()\nfor dp in ds.get_data():\n  print(dp)\n```\nThis gracefully handles old and new code without any changes, but takes the liberty to throw a deprecation warning.. Agree. Done.. Porting these models is merely a copy paste / import action (tensorpack already supports TF.slim). I wonder if it is worth spending time in porting these imagenet-based models over.\nDo you really want to compete with a repo which is apparently the first choice of people -- independent of the code-readability? You cannot convince people to make a wise choice (have a look at the keras fanbase).\nThe work would be redundant and is first and foremost time consuming work (when we talk about training them).\nAnd I am not sure if one needs to retrain them (ilsvr classification) anyway again and again. Loading the graph from there though might be interesting when fine-tuning in tensorpack.\nThe CaffeModel Zoo is IMHO much more interesting and diverse: DeepHand, VQA, some face-recognition stuff, semantic segmentation. If someone has too much spare time, I guess this would be more helpful for people (they can copy the suggested TF.models and fine-tune from there). This would touch a much wider audience.\nThere are bigger opportunities:\nMy concern is that there is currently a huge gap in the tensorpack documentation of how to bring a trained model in tensorpack to production (tensorflow-serving, tf-mobile, freeze graph stuff). If I remember correctly there is not even the undocumented npz export mentioned.\nSince tensorpack already uses ModelDescr and has a PredictConfig being able to build the inference-only graph, it would be much more straightforward to export models from tensorpack than the way described in the official TF docs (including pruning the graph). The \"apply\" function in the examples is almost doing it.\nI can check if the tfutils/export.py still works and phrase a first documentation draft. It would be huge plus if the entire pipeline is described and supported rather than adding more image classification models.\nBut in the end it is: Training takes time and a proper implementation as well. I do have a mobilenet implementation and I can contribute my cityscape data reader. However, we agree that it is most of the time tedious work to just get the model correct.\nThis is not a wishlist of new examples (time is limited). It is a slightly different (and more broad) view on the topic.. I do not really refer to tf-lite or tf-mobile. I refer to a frozen graph (everything is constant) and pruned graph (only nodes necessary for inference). They do not need to be deployed on mobile devices.\nIn all my cases this worked perfectly and I can restore even graphs which I trained several months ago without any issues.\nSo far, the documentation is missing but the minimal working example probably shows the most important things.. To repeat: This does not apply the TOCO (which indeed does not work). It mainly focuses on how the TensorFlow-Serving export does work. The tf-mobile is just there because it was only a matter of a few minutes.. Merge both classes into ModelExporter and renamed mobile to compact to represent a compressed and self-contained graph. The example is updated as well to reflect those changes.. find . -name \"GAN.py\" | wc -l is 23. And that's definitely not the best solution yet. \nI could life with a big read blinking warning my the terminal saying that \" import tensorpack.examples is highly unstable\" and using it \"will initiate the end of the world.\"\n. That's why I am asking. Having something like tensorpack.hub would be indeed really helpful. . @ppwwyyxx Are you worried about the scope? There is a scope-arg in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard7/tf.get_collection.md. This is from\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/learning.py#L428-L430\nI think it is just a nice way to name this barrier for TensorBoard.. I think it would be better to add this to any trainer,\nIn the model it can be turned off and on by\npython\nwith slim.arg_scope([slim.layers.conv2d, ...], normalizer_fn=slim.layers.batch_norm, weights_regularizer=slim.l2_regularizer(0.001),\n@ppwwyyxx Do you still think it is good here?. I think the best way would to use with_depedencies here too? Currently, there is no guarantee, that this normalization happens before the update.. done like in ModelDump.. If I would put this into reset_state() then there would be no output in\npython\ndef get_data():\n    ds = CaffeLMDB('path/to/lmdb')\n    ds = SomeInscrutableMappings(ds)\n    ds = DebugData(ds, num=2)\n    return ds\nds = get_data(). It should also be linted maybe you should fix that afterwards. But flake8 does its job recursively.. I see. I will fix it and also output the version of tensorflow and opencv.. There was a reason for not using JET in my code.. I do not really get why [0][0] works instead of [0]. Isn't it [output-node][batch-entry] or did I mess up the dataflow?. Yes, the mutual information can only be approximated by a lower bound on the cross entropy.. Yeah, the placeholder should get sample as the default only. I will adjust that after the product-distribution class is finished.. What about y_i = 0 in one batch?. I would stick with the mean as it works even on large scale problems and this is how it is implemented in Caffe.. You mean\npython\npos_dist = tf.where(tf.equal(tf.reduce_sum(y), 0.0), 0, tf.reduce_sum(y * delta_sqrt) / tf.count_nonzero(y)). it is \"min\". Note \"\\Vert a-p\\Vert^2\" should be small and  \" - \\Vert a-n\\Vert^2 \" should be small. Ahh. Maybe we should write \"tf.nn.relu(.....)\".. I think the current version is ok. Really? I never did that before. I will look into that again. But this would be Monday. . I tried that. Maybe I should also add the converter script such that everybody can test it. . Ok. Is that documented? I also got it on MNIST during the GAN refactoring.\nI want to test it again including the Prefetching(ZMQ) before pushing new code. So it would be Monday at the earliest or Sunday evening.. Yep. But this does not cause the issue. My best guess is that @functools.wraps(func) is responsible for that. But I cannot test the python3 version on my system.. Renaming max_pertub_pixel, too?. Sure. I was not sure, if I can import these functions directly.. the latest version uses the import. SelectComponent, ColorSpace, Grayscale, ToUint8 , ToFloat32, FixedCrop, Clip, Identity, JpegNoise, Resize are also one-liner.. It does. This is almost never True.. I really enjoy this empty catch block. I know somebody who had some issues with this line.. Why do you not just give my creativity some freedom? ;-). This should be tf.matmul(v, v, transpose_A=True). should by multiplied by 255.. works without this multiplication. fixed. I feel more comfortable having the loop, which is IMHO much easier to parse than hiding this behind some functions. But I am fine with tf.space_to_batch_nd if this is equivalent. . any other solution?. To get all patches? When using p it skips the last patch in each row/column.. Does this affects the gradients wrt. to fake_hr?. Spiced with a pinch of pytorch style. Honestly, I was not aware of tf.space_to_batch_nd.. Well spotted! I don't think this has a huge impact. \nEven when 128/16=8, I would prefer space_to_batch_nd, if you are sure it is equivalent.. That's awesome!. not during \"apply\". There is a difference subtle difference between a callback inference runner and the apply function. The inference runner might need the loss statistics, the appply function not.. At some point I did benchmarks. And LMDB beats almost everything I tried here so far. This of course depends on the filesystem, hardware, CPU, bla bla bla.. InferenceRunnerCallback?. Oops forgot to remove them. fixed. fixed. fixed (as I do not use the InferenceRunner here). Maybe use ctx.phase = ['train', 'val', 'inference'] at some point. Haven't changed it, but then\n```python\nx = tf.space_to_batch_nd(x, [p, p], [[0,0],[0,0]])\nx = tf.reshape(x, [p, p, -1, h/p, w/p, c])\nx = tf.transpose(x, [2, 3, 4, 0, 1, 5])\npatches_a, patches_b = tf.split(x, 2)   # each is b,h/p,w/p,p,p,c\npatches_a = tf.reshape(patches_a, [-1, p, p, c])\npatches_b = tf.reshape(patches_b, [-1, p, p, c])\n``\nshould work. fixed. fixed byrange(0, h, p),range(0, w, p). I had this issue not before. As long as GANs are not stable (requiring the InferenceRunner), we do not need these different phases :-). But the current auto_reuse_version has some issues as well. It affects the entire var_scope, whiletf.AUTO_REUSE` seems to be per-variable.\nThis sometimes really requires ugly workarounds. The tf.AUTO_REUSE removes workarounds like:\nhttps://github.com/cgtuebingen/learning-blind-motion-deblurring/blob/master/learning_blind_motion_deblurring.py#L41-L47\nhttps://github.com/cgtuebingen/learning-blind-motion-deblurring/blob/master/learning_blind_motion_deblurring.py#L58-L65\nAdding more details:\nWhen unrolling some recurrent structure (reusing code) there are cases you just want to include a slight change to the first time-step (it has no previous timestep). But omitting a tensor in a first timestep makes TF complaining in t=2 that there is no tensor to reuse.\nI have exatcly the same issue current for my progressive-GANS implementation.. Sorry. Then it just a one-liner pull-request. This seems to fix the travis-ci issue.. I thought about this as well. But for the callback version of your GPUUtilization call, you cannot use your way, as it closes and open the context for each epoch.\nBut I changed it. It is probably better to directly close it is instead of leave when Ctrl+C happend.. It think it is correct, I want a singleton-pattern here. FixedSizeData has docs in init, JoinData has docs in class. This is not consistent?!. Just to not being confused in 61e02bc the arguments are documented? Could you somehow document how to locally build the documentation such that it can be fixed & tested without pull-requests?. Sorry, I missed that one during rebasing . Oh silly me. I didn't saw the docs dir. I tested it the documentation looks ok. I would prefer keeping the example in the class scope like PrintData currently does.. Well, you blame the author of this line. Adding symbolic_function here, keeps the changes minimal.. I renamed this function as it is quite general.. I guess\nconv0:    [None, 28, 28, 1] ->   [None, 28, 28, 32]\nis easier to parse than\nconv0: input [None, 28, 28, 1]\nconv0: output [None, 28, 28, 32]. I can add here something similar to https://github.com/tensorpack/tensorpack/blob/fa34d239ce0a6d5cc12bcb12ad690f07df5f1f0e/tensorpack/models/registry.py#L118-L119\nBut this would introduce a tensorflow as a dependency to this file.. Indeed, this is not trivial as I cannot access the scope inside tf.layers.lowercase_name_function.\nThe lastest commit in this PR at least can distinguish between the towers\n```python\nfrom tensorpack import *\nimport tensorflow as tf\nenable_argscope_for_module(tf.layers)\ndef build(x):\n    return tf.layers.conv2d(x, 32, [3, 3], name='conv0')\nans = []\nprint \"----------------------------------\"\nplhdr = tf.placeholder(tf.float32, [1, 28, 28, 3])\nans.append(build(plhdr))\nprint \"----------------------------------\"\nwith TowerContext('tower0', True, vs_name='tower0') as ctx:\n    ans.append(build(plhdr))\nprint \"----------------------------------\"\nwith TowerContext('tower1', True, vs_name='tower1') as ctx:\n    ans.append(build(plhdr))\nprint \"----------------------------------\"\nwith TowerContext('tower2', True, vs_name='tower2') as ctx:\n    ans.append(build(plhdr))\n``. TheInferenceToweris still dumped to the console. Which is ok, as it might differ from the training tower.. Should bekeys = [u'{:08}'.format(k).encode('ascii') for k in range(self.latest_idx + 1)]. Aren't these two lines exactly theLMDBDataPointimplementation? Why not usingLMDBDataPointdirectly here?. That sounds ok, I would really prefer thenLMDBSerializer.load.\n. This breaks legacy implementations based onmsgpack. Ismsgpackstill supported?.ValueError: setting an array element with a sequence.otherwise. This is exactly my reason for writing these tests.. I always test the implementation before doing a PR. The default is inference-mode fortf.layers.dropout` (training=False).. You are right! Done in the next commit.. Good catch!. This is an implementation detail from the original network. The network is trained on scaled flow-field (by factor 20.). Channel-wise rgb-mean subtraction for each input (note NCHW --> NTCHW reduce over NCHW). It is strange indeed. It would be reasonable to remove 0 from axis to get per-channel mean.. Yep, not for now.. The reason is that you could use the models separately. I would change it and place it only once. But I need to stick to the Caffe-Weights. And my numerically debugging (comparison against Cafe) depends on this structure. But there is now a constant and a comment ;-). Same response as above. I did a step-by-step debugging/comparison against Caffe. Changing these lines would destroy this validation script. But feel free to change it. \nMy delivered version yields comparable AEE (with the tolerance one can expect between frameworks). If the AEE stays the same: I am ok. But this is not my current main research focus, so I would not put that much time into improving the code. . My terminal is flashing anyway because of python-prctl. A few more warning would be ok. But I removed them.. Ich changed it now to just use ABCMeta as the parent class and @abstractmethod is working again. It would be weird if we do not support the default ABCMeta.. confirm and removed. fixed.. *args, **kwargs looks good!. For images InferenceModel the dtype here needs to be tf.uint8 although the placeholder has type tf.string. There are some strange corner cases, which I had.. I cannot reproduce it as:\n```python\nimport tensorflow as tf\nfrom tensorflow.python.tools import optimize_for_inference_lib\nfrom tensorflow.python.framework import graph_util\nfrom tensorflow.python.platform import gfile\np = tf.placeholder(tf.string, (None), name='input_img_bytes')\nx = tf.image.decode_png(p, channels=3)\nx = tf.expand_dims(x, axis=0)\nunused = tf.cast(x, tf.float32)\nunused = tf.layers.conv2d(unused, 3, name='conv', kernel_size=[1, 1])\nx = tf.squeeze(x, axis=0)\nx = tf.image.encode_png(x)\no = tf.identity(x, name='final_bytes')\nwith tf.Session() as sess:\n  sess.run(tf.global_variables_initializer())\n  graph_def = sess.graph.as_graph_def()\ngraph_def = graph_util.convert_variables_to_constants(\n      sess, graph_def, ['final_bytes'])\nthe_dtype = p.dtype.as_datatype_enum\n  graph_def = optimize_for_inference_lib.optimize_for_inference(graph_def,\n                                                                ['input_img_bytes'], ['final_bytes'], [the_dtype], False)\nwith gfile.FastGFile('/tmp/tmp_graph.pb', \"wb\") as f:\n    f.write(graph_def.SerializeToString())\ntf.reset_default_graph()\n---------inference ------------------------\nwith tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n  with tf.gfile.GFile('/tmp/tmp_graph.pb', \"rb\") as f:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(f.read())\n    tf.import_graph_def(graph_def)\nprint([n.name for n in sess.graph.as_graph_def().node])\ninput_img = sess.graph.get_tensor_by_name('import/input_img_bytes:0')\n  prediction_img = sess.graph.get_tensor_by_name('import/final_bytes:0')\nwith open('lena.png', 'rb') as f:\n    buf = bytes(f.read())\nprediction = sess.run(prediction_img, {input_img: buf})\nwith open('lena2.png', 'wb') as f:\n    f.write(prediction)\n```\nworks fairly well. However, I had some troubles. I am fine with removing the argument. The issue is probably related to tf.map_fn.. This was just a mental note for myself.. You are correct. I also made a mistake with the StagingInput, which is fixed now.. ",
    "yihui-he": "Many thanks.\n. ",
    "YixuanLi": "Hi Yuxin,\nI am trying to train a ResNet with two towers, and found the training is very slow (0.18iters/s) on 2 TITAN X. It's taking days to train a CIFAR-10 model. The inference is very fast though (8iters/s). I have cudnn installed and setup appropriately. Do you have any suggestion on how to fix that? \nThanks!\n. Thanks for the quick response! That's super helpful. I am using the latest version of tensorflow and cuda7.5. I will look into tcmalloc. Would that make a huge difference (0.18iters/s -> 2iters/s) in terms of performance, or there could be something else going on I should also look into?\n. Yeah I wondered that too. Actually even with a single gpu it still remains 0.18iters/s. Any thing else could go wrong? I am following your original setup without customizing anything.\nif args.gpu:\n        os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\nconfig = get_config()\nif args.load:\n    config.session_init = SaverRestore(args.load)\nif args.gpu:\n    config.nr_tower = len(args.gpu.split(','))\nSyncMultiGPUTrainer(config).train()\nOn Sep 17, 2016, at 8:32 PM, Yuxin Wu notifications@github.com<mailto:notifications@github.com> wrote:\nI doubt this alone would make such a huge difference.\nWhat's your speed with one gpu? Ideally it should also be around 2 iter /s.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHubhttps://github.com/ppwwyyxx/tensorpack/issues/5#issuecomment-247816373, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AGhqh0VaSIMmHgncZxwRV0EE73akPTURks5qrIargaJpZM4IPktX.\n. Yep the speed has been frustrating me - taking almost 2 days to train cifar-10. I can see my processes on two gpus, each of which uses 4GB memory. I am 97% sure it it running in GPU mode\u2026Does the cuda version matter? I am using cuda4.0 as recommended by tensorflow site. Does the code detect error automatically if cudnn is not being used?\n. I do see these at the beginning:\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n. Thanks Yuxin. I\u2019ve configured my environment and made sure tensorflow is all set up appropriately and running in gpu mode. What puzzled me most is the inference still runs very fast whereas the training is drastically slower. Could there be any caveats during training that is causing this? Such as not enough memory allocated, or multi-threading being slow?\n100%|#############################################################################################################################################|390/390[35:36<00:00, 0.18it/s]\n100%|###############################################################################################################################################|79/79[00:09<00:00, 8.13it/s]\n. Test this for svhn classification: https://github.com/ppwwyyxx/tensorpack/blob/master/examples/svhn-digit-convnet.py, and it yields pretty similar performance as yours (44iters/s).\n. Confirm. The issue was resolved by upgrading CUDA to 7.5 and cudnn to v5.\nOn Sep 19, 2016, at 12:07 PM, Yuxin Wu notifications@github.com<mailto:notifications@github.com> wrote:\nFor the record, it's confirmed to be cudnn v4 problem after working with Yixuan.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHubhttps://github.com/ppwwyyxx/tensorpack/issues/5#issuecomment-248037511, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AGhqh9uxUX8Kr95W9Y0JFmpohxor6g52ks5qrrMngaJpZM4IPktX.\n. ",
    "pangolulu": "Thanks for replying me. What you said is true and reasonable. I will concentrate on the self-contained parts now and get my hands dirty. \n. ",
    "cykustcc": "Thanks! Now  CifarBase is the baseclass of Cifar10 and Cifar100. And the duplicate code in dataflow is also removed.\n. Have fixed this as suggested.. ",
    "Kalkasas": "I can reproduce your results on the CPU of the tensorpack example. I can also reproduce the official mnist tensorflow tutorial results with GPU.\nI am using the older cuda and cudnn versions which are compatible with the published tensorflow binaries. Maybe compiling from source with the latest version can help?\n. The issue seems to be resolved by updating drivers and cuda versions. Thank you for your help!\n. Exactly what I needed. Thank you!\n. Thank you for your fast reply.\nLets assume I have n tasks (n linear classifiers at the end of the network, the labels of all tasks have the same \"type/dimension\" but different meaning). At the moment I treat a task ID simply as an input value which flows through the net. I stored my n linear classifier weights in a 2-D Tensor (first dimension is task, second dimension is weights of the tasks).\nThe task is now simply used in tf.gather to pick the corresponding weights for the task. As I am using a momentum optimizer I do not know if some weights of an \"idle\" task could change as well. Mulitple sources I found on the web suggest that multiple optimizers should be used.\nCould I run into issues with my setup?\n. ",
    "coolbay": "I couldn't find any function as get_gradient_processor() in the base class ModelDesc. I tried adding this function into my Model, but it seemed that this function was never called.. \n@ppwwyyxx  Thank you very much! I change the learning rate to a very small one, and now everything becomes normal.. @ppwwyyxx \nI independently tested the codes you provide in 'efficient DataFlow' as following\npython\nds = dataset.ILSVRC12(datadir, 'train', meta_dir=metadir, shuffle=True)\nds = AugmentImageComponent(ds, augmentors)\nds = PrefetchDataZMQ(ds, nr_proc=25)\nds = BatchData(ds, 256)\nand \npython\nds = LMDBData(datadir, shuffle=False)\nds = LocallyShuffleData(ds, 50000)\nds = PrefetchData(ds, 5000, 1)\nds = LMDBDataPoint(ds)\nds = MapDataComponent(ds, lambda x: cv2.imdecode(x, cv2.IMREAD_COLOR), 0)\nds = AugmentImageComponent(ds, augmentors)\nds = PrefetchDataZMQ(ds, 25)\nds = BatchData(ds, 256, use_list=True)\nThe former one gives the performance of 4.35 it/s and the latter one 28.31 it/s. Then I used these two versions of reading data in the 'ResNet example'. I got the performances of  3.62it/s and 3.97it/s in 1-gpu case, and 1.43it/s and 1.42it/s in 2-gpu case. \nI don't understand why this happens. It seems that the use of GPU is influencing  data reading, and that for random read and sequential read, GPU is influencing in different ways.. One more question: are InferenceRunner/QueueInput/queue_size and QueueInput/queue_size the same thing?  InferenceRunner/QueueInput/queue_size seems to be always 50, but QueueInput/queue_size sometimes is very low.. Information of my two GPUs:\n2017-10-22 17:07:07.234903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: \nname: GeForce GTX 1080 Ti\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.582\npciBusID 0000:03:00.0\nTotal memory: 10.91GiB\nFree memory: 10.13GiB\n2017-10-22 17:07:07.765882: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x57d1530 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.\n2017-10-22 17:07:07.767095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 1 with properties: \nname: Quadro K6000\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.9015\npciBusID 0000:04:00.0\nTotal memory: 11.17GiB\nFree memory: 11.10GiB. By two versions, I mean 'random read' and 'sequential read' as shown in the two code blocks. I change nothing except for the get_imagenet_dataflow function in imagenet_resnet_utils.py. I try using these two versions to read the training data, and get the above-mentioned results. 'Random read' becomes faster and 'Sequential read' becomes slower.. I am sorry. I just found that I was using different batch sizes. After correcting the batch size, I have the performances of 4.35 it/s and 28.31 it/s for 'random read' and 'sequential read', respectively. Now it seems that when training using GPUs, the performance is dropping dramatically. I think my GPUs are fine to run fast enough. They shouldn't become such a big bottleneck.. Dropping from 4.3 to 3.6 sounds reasonable. But for the 'sequential read' case, it drops from 28.31 to 3.97, which is too big.  The data should be enough in this case, but when I use 2 GPUs, the performance drop from 3.97 (1 gpu) to 1.42. I don't expect it to be slower with 2 GPUs.\n. For 'sequential read':\nQueue size is 50 and GPU utilization is around 96% in 1-GPU case. For the 2-GPU case, queue size is around 49 and GPU Utility is 35% and 92% with respect to each GPU. It seems that one GPU is always around 90%, and the other is intermittently high and low.. Sorry I forgot to mention that the results above are for resnet 50 with batch size 32. \nWhat I Modify is to replace \npython\n        ds = dataset.ILSVRC12(datadir, name, shuffle=True)\n        ds = AugmentImageComponent(ds, augmentors, copy=False)\n        ds = PrefetchDataZMQ(ds, cpu)\n        ds = BatchData(ds, batch_size, remainder=False)\nin the function get_imagenet_dataflow with \npython \n        ds = LMDBData(datadir, shuffle=False)\n        ds = LocallyShuffleData(ds, 50000)\n        ds = PrefetchData(ds, 5000, 1)\n        ds = LMDBDataPoint(ds)\n        ds = MapDataComponent(ds, lambda x: cv2.imdecode(x, cv2.IMREAD_COLOR), 0)\n        ds = AugmentImageComponent(ds, augmentors)\n        ds = PrefetchDataZMQ(ds, 25)\n        ds = BatchData(ds, batch_size)\\\nAnd switch off validation in training process.\nThe following are for resnet 18 with batchsize 256. This time I use two same 1080 ti GPUs instead of two different ones, and they seem to be collaborating well as you said. \n\n\nUse 1 1080 ti GPU;\npython\n[1023 10:11:52 @base.py:232] Start Epoch 1 ...\n100%|##########|50/50[00:37<00:00, 1.32it/s]\n[1023 10:12:30 @base.py:242] Epoch 1 (global_step 50) finished, time:38.01 sec.\n[1023 10:12:30 @saver.py:90] Model saved to train_log/imagenet-resnet-d18/model-50.\n[1023 10:12:30 @monitor.py:359] GPUUtil/0: 83.667\n[1023 10:12:30 @monitor.py:359] QueueInput/queue_size: 47.456\n[1023 10:12:30 @monitor.py:359] l2_regularize_loss: 0.48821\n[1023 10:12:30 @monitor.py:359] learning_rate: 0.1\n[1023 10:12:30 @monitor.py:359] train-error-top1: 0.89975\n[1023 10:12:30 @monitor.py:359] train-error-top5: 0.68685\n[1023 10:12:30 @monitor.py:359] xentropy-loss: 3.554\n[1023 10:12:30 @base.py:232] Start Epoch 2 ...\n100%|##########|50/50[00:31<00:00, 1.58it/s]\n[1023 10:13:02 @base.py:242] Epoch 2 (global_step 100) finished, time:31.59 sec.\n[1023 10:13:02 @saver.py:90] Model saved to train_log/imagenet-resnet-d18/model-100.\n[1023 10:13:02 @monitor.py:359] GPUUtil/0: 95.267\n[1023 10:13:02 @monitor.py:359] QueueInput/queue_size: 49.818\n[1023 10:13:02 @monitor.py:359] l2_regularize_loss: 0.48518\n[1023 10:13:02 @monitor.py:359] learning_rate: 0.1\n[1023 10:13:02 @monitor.py:359] train-error-top1: 0.89762\n[1023 10:13:02 @monitor.py:359] train-error-top5: 0.67561\n[1023 10:13:02 @monitor.py:359] xentropy-loss: 3.7917\n[1023 10:13:02 @base.py:232] Start Epoch 3 ...\n100%|##########|50/50[00:31<00:00, 1.58it/s]\n[1023 10:13:34 @base.py:242] Epoch 3 (global_step 150) finished, time:31.68 sec.\n[1023 10:13:34 @saver.py:90] Model saved to train_log/imagenet-resnet-d18/model-150.\n[1023 10:13:34 @monitor.py:359] GPUUtil/0: 94.733\n[1023 10:13:34 @monitor.py:359] QueueInput/queue_size: 49.986\n[1023 10:13:34 @monitor.py:359] l2_regularize_loss: 0.48217\n[1023 10:13:34 @monitor.py:359] learning_rate: 0.1\n[1023 10:13:34 @monitor.py:359] train-error-top1: 0.89215\n[1023 10:13:34 @monitor.py:359] train-error-top5: 0.68439\n[1023 10:13:34 @monitor.py:359] xentropy-loss: 4.0553\n[1023 10:13:34 @base.py:232] Start Epoch 4 ...\n100%|##########|50/50[00:30<00:00, 1.63it/s]\n[1023 10:14:05 @base.py:242] Epoch 4 (global_step 200) finished, time:30.64 sec.\n  0%|          |0/50[00:00<?,?it/s][1023 10:14:05 @saver.py:90] Model saved to train_log/imagenet-resnet-d18/model-200.\n[1023 10:14:05 @monitor.py:359] GPUUtil/0: 95.069\n[1023 10:14:05 @monitor.py:359] QueueInput/queue_size: 49.999\n[1023 10:14:05 @monitor.py:359] l2_regularize_loss: 0.47961\n[1023 10:14:05 @monitor.py:359] learning_rate: 0.1\n[1023 10:14:05 @monitor.py:359] train-error-top1: 0.8763\n[1023 10:14:05 @monitor.py:359] train-error-top5: 0.64162\n[1023 10:14:05 @monitor.py:359] xentropy-loss: 4.0434\n[1023 10:14:05 @base.py:232] Start Epoch 5 ...\n[1023 10:14:34 @base.py:242] Epoch 5 (global_step 250) finished, time:29.51 sec.\n100%|##########|50/50[00:29<00:00, 1.69it/s]\n[1023 10:14:35 @saver.py:90] Model saved to train_log/imagenet-resnet-d18/model-250.\n[1023 10:14:35 @monitor.py:359] GPUUtil/0: 94.5\n[1023 10:14:35 @monitor.py:359] QueueInput/queue_size: 50\n[1023 10:14:35 @monitor.py:359] l2_regularize_loss: 0.47672\n[1023 10:14:35 @monitor.py:359] learning_rate: 0.1\n[1023 10:14:35 @monitor.py:359] train-error-top1: 0.86738\n[1023 10:14:35 @monitor.py:359] train-error-top5: 0.64623\n[1023 10:14:35 @monitor.py:359] xentropy-loss: 3.7505\n\n\nUse two 1080ti GPUs:\npython\n100%|##########|50/50[00:26<00:00, 1.86it/s]\n[1023 10:07:56 @base.py:242] Epoch 1 (global_step 50) finished, time:26.88 sec.\n[1023 10:07:56 @saver.py:90] Model saved to train_log/imagenet-resnet-d18/model-50.\n[1023 10:07:57 @monitor.py:359] GPUUtil/0: 62.16\n[1023 10:07:57 @monitor.py:359] GPUUtil/1: 54.36\n[1023 10:07:57 @monitor.py:359] QueueInput/queue_size: 49.353\n[1023 10:07:57 @monitor.py:359] l2_regularize_loss: 0.48833\n[1023 10:07:57 @monitor.py:359] learning_rate: 0.1\n[1023 10:07:57 @monitor.py:359] train-error-top1: 0.91258\n[1023 10:07:57 @monitor.py:359] train-error-top5: 0.70073\n[1023 10:07:57 @monitor.py:359] xentropy-loss: 3.5975\n[1023 10:07:57 @base.py:232] Start Epoch 2 ...\n100%|##########|50/50[00:16<00:00, 3.09it/s]\n[1023 10:08:13 @base.py:242] Epoch 2 (global_step 100) finished, time:16.19 sec.\n[1023 10:08:13 @saver.py:90] Model saved to train_log/imagenet-resnet-d18/model-100.\n[1023 10:08:13 @monitor.py:359] GPUUtil/0: 88.867\n[1023 10:08:13 @monitor.py:359] GPUUtil/1: 79.067\n[1023 10:08:13 @monitor.py:359] QueueInput/queue_size: 49.5\n[1023 10:08:13 @monitor.py:359] l2_regularize_loss: 0.48509\n[1023 10:08:13 @monitor.py:359] learning_rate: 0.1\n[1023 10:08:13 @monitor.py:359] train-error-top1: 0.9074\n[1023 10:08:13 @monitor.py:359] train-error-top5: 0.70409\n[1023 10:08:13 @monitor.py:359] xentropy-loss: 3.8236\n[1023 10:08:13 @base.py:232] Start Epoch 3 ...\n  0%|          |0/50[00:00<?,?it/s][1023 10:08:28 @base.py:242] Epoch 3 (global_step 150) finished, time:15.10 sec.\n100%|##########|50/50[00:15<00:00, 3.31it/s]\n[1023 10:08:28 @saver.py:90] Model saved to train_log/imagenet-resnet-d18/model-150.\n[1023 10:08:28 @monitor.py:359] GPUUtil/0: 93.214\n[1023 10:08:28 @monitor.py:359] GPUUtil/1: 89\n[1023 10:08:28 @monitor.py:359] QueueInput/queue_size: 49.648\n[1023 10:08:28 @monitor.py:359] l2_regularize_loss: 0.4818\n[1023 10:08:28 @monitor.py:359] learning_rate: 0.1\n[1023 10:08:28 @monitor.py:359] train-error-top1: 0.89979\n[1023 10:08:28 @monitor.py:359] train-error-top5: 0.70344\n[1023 10:08:28 @monitor.py:359] xentropy-loss: 4.0616\n[1023 10:08:28 @base.py:232] Start Epoch 4 ...\n100%|##########|50/50[00:14<00:00, 3.34it/s]\n[1023 10:08:43 @base.py:242] Epoch 4 (global_step 200) finished, time:14.98 sec.\n[1023 10:08:43 @saver.py:90] Model saved to train_log/imagenet-resnet-d18/model-200.\n[1023 10:08:43 @monitor.py:359] GPUUtil/0: 93.071\n[1023 10:08:43 @monitor.py:359] GPUUtil/1: 94.214\n[1023 10:08:43 @monitor.py:359] QueueInput/queue_size: 49.676\n[1023 10:08:43 @monitor.py:359] l2_regularize_loss: 0.47896\n[1023 10:08:43 @monitor.py:359] learning_rate: 0.1\n[1023 10:08:43 @monitor.py:359] train-error-top1: 0.89005\n[1023 10:08:43 @monitor.py:359] train-error-top5: 0.67852\n[1023 10:08:43 @monitor.py:359] xentropy-loss: 4.1054\n[1023 10:08:43 @base.py:232] Start Epoch 5 ...\n  0%|          |0/50[00:00<?,?it/s][1023 10:09:01 @base.py:242] Epoch 5 (global_step 250) finished, time:17.26 sec.\n100%|##########|50/50[00:17<00:00, 2.90it/s]\n[1023 10:09:01 @saver.py:90] Model saved to train_log/imagenet-resnet-d18/model-250.\n[1023 10:09:01 @monitor.py:359] GPUUtil/0: 96.25\n[1023 10:09:01 @monitor.py:359] GPUUtil/1: 76.938\n[1023 10:09:01 @monitor.py:359] QueueInput/queue_size: 49.638\n[1023 10:09:01 @monitor.py:359] l2_regularize_loss: 0.47608\n[1023 10:09:01 @monitor.py:359] learning_rate: 0.1\n[1023 10:09:01 @monitor.py:359] train-error-top1: 0.87799\n[1023 10:09:01 @monitor.py:359] train-error-top5: 0.66664\n[1023 10:09:01 @monitor.py:359] xentropy-loss: 3.796\n\n\nThe data reading and processing speed  is 4.27it/s. This time, the speed performances all seem reasonable, aren't they?\nBut the GPU utility is still not full. Is there a good way to increase GPU utility? And I cannot make it run as fast as 2.7it/s (your mentioned speed) with only one 1080 ti.\n. I'll figure out whether I can work the GPU up somehow. Your suggestion really helps!\nI have one more question regarding building my own network. Can I put the functions for network layers outside class Model(ModelDesc) and use a function in _build_graph to call them? . Thank you very much for your kind help!. @ppwwyyxx  By \"latest of TF/cuda/cudnn/nvidida driver\", do you mean TF14/cuda9/cudnn7/nvidia384?. I am using the specified parameters in the code except for the batchsize ( batchsize for training set is 48 and for validation set is1000 ) to train resnet50 on imagenet. What is confusing me is that up to the 59-th epoch, the train-error-top1 and train-error-top5 are 0.3 and 0.1, but the val-error-top1 and val-error-top5 are still 0.8 and 0.7. Does batchsize influence generalization? What can I do to make validation work better?. Thanks for your suggestion. I have resolved the problem. I didn't shuffle the training data when generating the LMDB file and only used the local shuffling strategy when reading the LDMB file. Consequently, only a couple of categories are trained in each epoch, which results in bad generalization results. . One more question is if I set 'steps_per_epoch' smaller than the actual epoch size of the training dataset (e.g. actual epoch size for imagenet with batch size 100 is about 13000), then at the second epoch, will 'Dataflow' continue fetching the data from where it stops at the first epoch or start all over again? I'm afraid that if it is the latter case, some data will never be trained if I use a pre-shuffling strategy.. Good! How do I check input images in each iteration while training so that I can make sure it is going correctly? I know that in evaluation I can do so by passing the variable name to \"PredictConfig\", but I couldn't find such a interface in training.. 'DumpTensors' and 'ProcessTensors' cannot be recognized by Tensorpack somewhat. I have the following error when trying using them.\nImportError: cannot import name DumpTensors. I am sorry. It turned out to be my own carelessness. These parameters are saved fine.  I should remove this issue.\nThanks!. Thanks very much for your suggestion. It's very helpful! I compared the performance of the two cases. The multi-process one has a speed of 87.37it/s and the multi-thread one has only 23it/s.\nNow the question is if I can use multi-process to inference on the validation set. In your tutorial, you mention that the multi-process data reading is not suitable for the validation set. Is my multi-process code reading a unique and complete copy of the validation set? If not, is there any way to modify the code to make it usable for the validation set?\n. I'd like to make sure if I correctly understand how multi-process functions like PrefetchDataZMQ or MultiProcessMapData work. \nAssume that I have A, B, C, D functions to process the data. If I don't use these multi-process functions, then all  A, B, C, D would run sequentially in one process, which is obviously slow. If I add a multi-process function to one of the four, let's say A, then A would run in one or more separate process in parallel with the other 3. That means A doesn't have to wait for B,C,D to finish before starting a new round of work. If I add  a multi-process function to each of them, then all the four can run in parallel. If this is the case, the best way to use multi-process should be adding a multi-process function to any computationally intensive step.\nI would be sorry that if this question sounds too basic, but I would like to hear about your explanation before I can step forward with my project on tensorpack. Appreciate your help a lot! \n. Every time PrefetchDataZMQ is used, it creates one or more extra processes for the function before it. And the function following PrefetchDataZMQ is the main process. Is it correct?\nIn the following example, there is one process reading data from the disk and a second one doing data mapping. If I don't use PrefetchDataZMQ, there would be only one process sequentially handling all the work. Is it what's happening?\npython\n        ds = LMDBData(datadir, shuffle=False)\n        ds = PrefetchData(ds, 5000, 1)\n        ds = LMDBDataPoint(ds, aug)\n        ds = ThreadedMapData(ds, 30, mapf, buffer_size=2000, strict=True)\n        ds = PrefetchDataZMQ(ds, 1)\n        ds = BatchData(ds, batch_size, remainder=True). I know the names of the variables I need, but my difficulty is how to retrieve the variable values using these names. tf.train.NewCheckpointReader(path).get_variable_to_shape_map() seems to only provide a dictionary of the variable name and shape.  Is there a dictionary that can tell me the value corresponding to each name?. I've found it. I am using the functiondump_chkpt_vars() to retrieve the values .\nThanks a lot!. Is there a way to set some variables to be non-trainable from the very beginning?. I mean how I can set some variables to be non-trainable when I build the graph with the functions like \".Conv2D\".. Tensorpack's builtin trainer minimizes variables in the collection \"TRAINABLE_VARIABLES\", doesn't it? Can I modify the variable collection to minimize without re-writing the trainer?. @A-Specker  That solves my problem. Thanks a lot!. I am using a fixed lmdb file. What do you mean by \"has the correct size\"?. I am using the following procedures to retrieve the validation data.\n```\n        ds = LMDBData(data_dir, shuffle=False)\n        ds = PrefetchData(ds, 5000, 1)\n        aug = imgaug.AugmentorList(augmentors)\n        def mapf(dp):\n            fname, cls = dp\n            im = cv2.imdecode(fname, cv2.IMREAD_COLOR)\n            im = aug.augment(im)\n            return im, cls\n    ds = LMDBDataPoint(ds)\n    ds = ThreadedMapData(ds, cpu, mapf, buffer_size=2000, strict=True)\n    ds = PrefetchDataZMQ(ds, 1)\n    ds = BatchData(ds, batch_size, remainder=True)\n\n```. @PatWie  My \"augmentors\" for the validation data is just the following\naugmentors = [\n            imgaug.ResizeShortestEdge(size1, cv2.INTER_CUBIC),\n            imgaug.CenterCrop((size2, size2)),\n        ]\nI don't think this causes any randomness.. ",
    "denru01": "Thanks for your reply.\nI would like to check the output of each layer in the DoReFa-Net example.\nIt uses this format to define the graph:\n\"logits = (LinearWrap(image)\n                .Conv2D('conv0', 96, 12, stride=4, padding='VALID')\n                .apply(activate)...\"\nHow can I get the variable name of the output of each layer?\nI have tried to use tf.all_variables(), but it returned an empty list.\n. ",
    "hamuchu": "Thank you for your replying.\nYour explanation is very clear so that I got confidence to implement it.\nI have one more question.\nI want to use the function of run_image() in DoReFa  net.\nBut, I couldn't understand how to specify the argument to run it.\nI put the  the command like this.\npython alexnet-dorefa.py --run\nBut it did't work well because the training was started.\nThanks a lot!\n. Finally I understand the method to use it.\nThank you for your kindness.\n. Finally,the issue is resolved.\nI will paste the final source code later.\nThanks.\n. Thanks.The source code is this.\nThis source code works well.\n``` python\n!/usr/bin/env python\n-- coding: UTF-8 --\nFile: cifar-convnet.py\nAuthor: Yuxin Wu ppwwyyxx@gmail.com\nimport tensorflow as tf\nimport argparse\nimport numpy as np\nimport os\nfrom tensorpack import \nimport tensorpack.tfutils.symbolic_functions as symbf\nfrom tensorpack.tfutils.summary import \nfrom dorefa import get_dorefa\nfrom tensorpack.tfutils.symbolic_functions import *\n\"\"\"\nA small convnet model for Cifar10 or Cifar100 dataset.\nCifar10:\n    90% validation accuracy after 40k step.\n    91% accuracy after 80k step.\n    19.3 step/s on Tesla M40\nNot a good model for Cifar100, just for demonstration.\n\"\"\"\nBITW = 1\nBITA = 2\nBITG = 6\nBATCH_SIZE = 32\nclass Model(ModelDesc):\n    def init(self, cifar_classnum):\n        super(Model, self).init()\n        self.cifar_classnum = cifar_classnum\ndef _get_input_vars(self):\n    return [InputVar(tf.float32, [None, 30, 30, 3], 'input'),\n            InputVar(tf.int32, [None], 'label')]\n\ndef _build_graph(self, input_vars, is_training):\n\n    image, label = input_vars\n    image = image / 4.0     # just to make range smaller\n\n    fw, fa, fg = get_dorefa(BITW, BITA, BITG)\n    # monkey-patch tf.get_variable to apply fw\n    old_get_variable = tf.get_variable\n    def new_get_variable(name, shape=None, **kwargs):\n        v = old_get_variable(name, shape, **kwargs)\n        # don't binarize first and last layer\n        if name != 'W' or 'conv0' in v.op.name or 'fct' in v.op.name:\n            return v\n        else:\n            logger.info(\"Binarizing weight {}\".format(v.op.name))\n            return fw(v)\n    tf.get_variable = new_get_variable\n\n    def nonlin(x):\n        if BITA == 32:\n            return tf.nn.relu(x)    # still use relu for 32bit cases\n        return tf.clip_by_value(x, 0.0, 1.0)\n\n    def activate(x):\n        return fa(nonlin(x))\n    def cabs(x):\n        return tf.minimum(1.0, tf.abs(x), name='cabs')\n\n    keep_prob = tf.constant(0.5 if is_training else 1.0)\n\n    if is_training:\n        tf.image_summary(\"train_image\", image, 10)\n\n    print \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\"\n    print is_training\n    with  argscope(FullyConnected, use_bias=False, nl=tf.identity), \\\n          argscope(Conv2D, nl=BNReLU(is_training), use_bias=False, kernel_shape=3):\n        logits = LinearWrap(image) \\\n                .Conv2D('conv1.1', out_channel=64)\\\n                .apply(activate)\\\n                .Conv2D('conv1.2', out_channel=64) \\\n                .apply(fg)\\\n                .BatchNorm('bn0',use_local_stat=is_training)\\\n                .MaxPooling('pool1', 3, stride=2, padding='SAME') \\\n                .apply(activate)\\\n                .Conv2D('conv2.1', out_channel=128)\\\n                .apply(fg)\\\n                .BatchNorm('bn2',use_local_stat=is_training)\\\n                .apply(activate)\\\n                .Conv2D('conv2.2', out_channel=128)\\\n                .apply(fg)\\\n                .BatchNorm('bn3',use_local_stat=is_training)\\\n                .MaxPooling('pool2', 3, stride=2, padding='SAME') \\\n                .apply(activate)\\\n                .Conv2D('conv3.1', out_channel=128, padding='VALID') \\\n                .apply(fg)\\\n                .BatchNorm('bn4',use_local_stat=is_training)\\\n                .apply(activate)\\\n                .Conv2D('conv3.2', out_channel=128, padding='VALID') \\\n                .apply(fg)\\\n                .BatchNorm('bn5',use_local_stat=is_training)\\\n                .apply(activate)\\\n                .FullyConnected('fc0', 1024 + 512,\n                       b_init=tf.constant_initializer(0.1)) \\\n                .tf.nn.dropout(keep_prob) \\\n                .apply(fg)\\\n                .BatchNorm('bn6',use_local_stat=is_training)\\\n                .FullyConnected('fc1', 512,\n                       b_init=tf.constant_initializer(0.1)) \\\n                .apply(fg)\\\n                .BatchNorm('bn7',use_local_stat=is_training)\\\n                .apply(nonlin)\\\n                .FullyConnected('linear', out_dim=self.cifar_classnum, nl=tf.identity)()\n    tf.get_variable = old_get_variable\n\n    cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, label)\n    cost = tf.reduce_mean(cost, name='cross_entropy_loss')\n\n    prob = tf.nn.softmax(logits, name='output')\n\n    # compute the number of failed samples, for ClassificationError to use at test time\n    wrong = symbf.prediction_incorrect(logits, label)\n    nr_wrong = tf.reduce_sum(wrong, name='wrong')\n    # monitor training error\n    add_moving_summary(tf.reduce_mean(wrong, name='train_error'))\n\n    # weight decay on all W of fc layers\n    wd_cost = tf.mul(0.004,\n                     regularize_cost('fc.*/W', tf.nn.l2_loss),\n                     name='regularize_loss')\n    add_moving_summary(cost, wd_cost)\n\n    add_param_summary([('.*/W', ['histogram'])])   # monitor W\n    self.cost = tf.add_n([cost, wd_cost], name='cost')\n\ndef get_data(train_or_test, cifar_classnum):\n    isTrain = train_or_test == 'train'\n    if cifar_classnum == 10:\n        ds = dataset.Cifar10(train_or_test)\n    else:\n        ds = dataset.Cifar100(train_or_test)\n    if isTrain:\n        augmentors = [\n            imgaug.RandomCrop((30, 30)),\n            imgaug.Flip(horiz=True),\n            imgaug.Brightness(63),\n            imgaug.Contrast((0.2,1.8)),\n            imgaug.GaussianDeform(\n                [(0.2, 0.2), (0.2, 0.8), (0.8,0.8), (0.8,0.2)],\n                (30,30), 0.2, 3),\n            imgaug.MeanVarianceNormalize(all_channel=True)\n        ]\n    else:\n        augmentors = [\n            imgaug.CenterCrop((30, 30)),\n            imgaug.MeanVarianceNormalize(all_channel=True)\n        ]\n    ds = AugmentImageComponent(ds, augmentors)\n    ds = BatchData(ds, 128, remainder=not isTrain)\n    if isTrain:\n        ds = PrefetchData(ds, 3, 2)\n    return ds\ndef get_config(cifar_classnum):\n    logger.auto_set_dir()\n# prepare dataset\ndataset_train = get_data('train', cifar_classnum)\nstep_per_epoch = dataset_train.size()\ndataset_test = get_data('test', cifar_classnum)\n\nsess_config = get_default_sess_config(0.5)\n\nnr_gpu = get_nr_gpu()#1e-5\u3067e\u306e\u2212\uff15\u4e570.00001\nlr = tf.train.exponential_decay(\n    learning_rate=1e-4,\n    global_step=get_global_step_var(),\n    decay_steps=step_per_epoch * (30 if nr_gpu == 1 else 20),\n    decay_rate=0.5, staircase=True, name='learning_rate')\ntf.scalar_summary('learning_rate', lr)\n\nreturn TrainConfig(\n    dataset=dataset_train,\n    optimizer=tf.train.AdamOptimizer(lr, epsilon=1e-4),\n    callbacks=Callbacks([\n        StatPrinter(),\n        ModelSaver(),\n        InferenceRunner(dataset_train, ClassificationError())#dataset_test\u306b\u66f8\u304d\u63db\u3048\u308b\n    ]),\n    session_config=sess_config,\n    model=Model(cifar_classnum),\n    step_per_epoch=step_per_epoch,\n    max_epoch=250,\n)\n\nif name == 'main':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--gpu', help='comma separated list of GPU(s) to use.') # nargs='*' in multi mode\n    parser.add_argument('--load', help='load model')\n    parser.add_argument('--classnum', help='10 for cifar10 or 100 for cifar100',\n                        type=int, default=10)\n    args = parser.parse_args()\nif args.gpu:\n    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\nelse:\n    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\nwith tf.Graph().as_default():\n    config = get_config(args.classnum)\n    if args.load:\n        config.session_init = SaverRestore(args.load)\n    if args.gpu:\n        config.nr_tower = len(args.gpu.split(','))\n    #QueueInputTrainer(config).train()\n    SimpleTrainer(config).train()\n\n```\n. Thank you for point out the bug of code.\nBut I have no idea to fix my bugs.\nSorry. . ",
    "Junsong-Wang": "Thanks for your reply,  since we implement it in FPGA with pipeline feature. The output of the current feature will directly feed to the next layer.  For example, we use 2 bits to represent the weights, according what you mentioned, the weights will be scaled by 3 in FPGA,. After the operation, we usually need to do the bit trunc operation (divided by 3), however 3 is not easy to trunc.\n. I see, do you think the following scheme is also okay to quantize x\n1. clip it to range [-1, (2^(k-1) - 1)/2^(k-1)], since k bits can only represent -2^(k-1) to 2^(k-1) -1 \n2. quantize by round(x*2^(k-1)), and this is the integer could be used in FPGA.\none point is that the positive part has less resolution.\n. Thanks for your comments....current I only check the FC layer, so there is no issue of padding.... I only dump the last pooling layer in the tensorflow and feed it to first FC layer in my numpy implementation. The most interesting thing is all the output keep a constant ratio.... So I doubt if it is induced by the factor E (tf.reduce_mean(tf.abs(x))), do you know how can I get this E value, so that I can double check, thanks. \n. Thanks for your reply, I have confirmed that this tiny different was induced by the tf.reduce_mean, I write a simple to verify this...\n```\nimport numpy as np\nimport tensorflow as tf\nweight_file = 'alexnet-126.npy'\nparam_dict = np.load(weight_file).item()\ndata = param_dict['fc0/W']\nprint 'Numpy results:{}'.format(np.mean(np.abs(data)))\nvec = tf.placeholder(tf.float32, data.shape)\navg = tf.reduce_mean(tf.abs(vec))\navgs = []\nwith tf.Session() as sess:\n    tf_mean = sess.run(avg, feed_dict={vec: data})\n    print 'Tensorflow results:{}'.format(tf_mean)\n```\nIf run this scripts. the output is\nNumpy results:0.0377448014915\nTensorflow results:0.0374231785536\nIt seems that the difference is much larger than I expected...such as 1e-7, what is your option?\n. I changed the tf.float32 to tf.float64, the output seems as close as I expected....However, if I change the numpy from np.float to np.float64, it seems no change... So it seems tf.float32 has much lower precision\n. It seems the difference has relation to the size of the matrix, the difference increases with the matrix size becomes large. Since the fc0 has tens millions of  weights, the difference becomes a little large... It may relate to the detail of the implementation. Do you know some detail between the tf.reduce_mean and np.mean, thanks.\n. It seems not converged...I tested this equation dependently, the output is what I expected...But I don't know if the gradient is also correct for training. I can't find a good tensorflow fucntion to implement this. The only thing I want to do is if x > E, return 1, elif x < -E return -1, else, return 0, do you have any ideal, thanks.\n. The reference paper is https://arxiv.org/abs/1605.04711\n. The paper said that the 0.7E is the approximate optimal threshold. yes, I also think the clip_by_value has no correct gradient. By the way, how I can get the gradient name of the tf function, so I can overwrite, such as tf.round is gradient name is Floor.\n. If I try to override the clip_by _value to identity, \nwith G.gradient_override_map({\"Maximum\":\"Identity\", \"Minimum\":\"Identity\"}):\n            clip_x = tf.clip_by_value(x/E, -2.0, 2.0) / 2.0\nit occurs...\nValueError: Num gradients 1 generated for op name: \"tower0/conv4/clip_by_value\"\nop: \"Maximum\"\ninput: \"tower0/conv4/clip_by_value/Minimum\"\ninput: \"tower0/conv4/clip_by_value/y\"\ndevice: \"/device:GPU:0\"\nattr {\n  key: \"T\"\n  value {\n    type: DT_FLOAT\n  }\n}\nattr {\n  key: \"_gradient_op_type\"\n  value {\n    s: \"Identity\"\n  }\n}\nwhat is the problem...\n. I found if I didn't override the clip_by_value, the covergence speed is very slow, I think the cause is clip_by_value make most place zero gradient, only has gradient 1 in [-E, E]. So I think override the clip_by_value with identity could make the covergence speed much faster... so could you help to see the above problem.\n. I didn't exactly follow the paper in the scale factor.\nE = tf.stop_gradient(tf.reduce_mean(tf.abs(x))) \nwith G.gradient_override_map({\"Sign\": \"Identity\"}): \n    return 0.5 * E * (tf.sign(x / E - 0.7) + tf.sign(x / E + 0.7)). In ternary weights, using different scale weight for positive and negative could improve the accuracy, the paper is here https://arxiv.org/abs/1612.01064, does anybody have ideals of how to implement it in this tensorpack framework? \n. ",
    "zsc": "In our FPGA implementation, we have avoided all floating point computations.\nConceptually, the output of conv is \"k X + b\", where X contains only low bitwidth integer numbers and \"k\" and \"b\" are floating point numbers,  and then a quantizing NL is applied. W.l.o.g., we next assume activations are 2-bit, then there will be only 4 possible values. We can write a program to find a few integer thresholds so that we can use integer comparisons X > th1, th1 > X > th2, th2 > X > th3, th3 > X to construct the 2-bit activations. More details of the above method will appear in an upcoming arXiv paper.\n. Yes, it matches our method. Thanks for the visualization.\n. Just noticed these.\nWhen M != K, it's also possible to not pad the shorter-bit-width numbers, if we implement specialized multiplications like 3bit-by-5bit in FPGA.\nThe sign-vs-unsign problem is more relevant in FPGA. But as we are only doing summation, unsign numbers should be fine.\n. 1) For accumulating intermediate values before the activation, one may optimize the hardware implementation by using lower bit-width addition at the start and use higher bit-width addition towards the higher stages of addition tree.\n2) We only do multi-bit multiplication bit-by-bit. There will be no overflow in computing \"and(c_m(x), c_k(y))\". Now as explained by 1), bitcount[and(c_m(x), c_k(y)] will be kept as high bit-width number. Then we can compute the scaled sum of these high bit-width numbers (there will be only M K such numbers) by sufficient bit-width to ensure no overflow at a trivially small cost.\nWe already have a DoReFa-net running on FPGA that produces results agreeing with the outputs of GPU, hence we are quite sure there will be no overflows.\n. The output of multiplication will be fed into nonlinear activation functions, which will quantize these values to low bit-width numbers, before these values are passed to the next convolution.\n. For that specific instance, the name seems to be input_deque, and a working command line is as follows:\ncheckpoint-prof.py --model train_log/main/checkpoint --meta train_log/main/graph-0625-115015.meta --input input_deque:0=1,28,28 input_deque:1=1 --output cross_entropy_loss:0 --print-flops. ",
    "themissingpieces": "\nCould you try again? I can access the link even in chrome incognito mode.\n\nTried that again. Looks like it's on my side. Sorry for this. Thanks for your respond.\n. ",
    "RyuLee": "Hi, when I run your code I got an error like this:\n(py35) D:\\workplace\\comp 535\\src>Traceback (most recent call last):\n  File \"\", line 1, in \n  File \"D:\\Software\\Anaconda\\envs\\py35\\lib\\multiprocessing\\spawn.py\", line 106, in spawn_main\n    exitcode = _main(fd)\n  File \"D:\\Software\\Anaconda\\envs\\py35\\lib\\multiprocessing\\spawn.py\", line 116, in _main\n    self = pickle.load(from_parent)\nEOFError: Ran out of input\nany suggestions?. You got the point, I mistook the resnet file as alexnet file... but it came another bug below, it happend as well when I tried to train the model:\n[0426 23:18:57 @sessinit.py:190] Restoring from dict ...\n[0426 23:18:58 @fs.py:99] WRN $TENSORPACK_DATASET not set, using C:\\Users\\Pengyu Li/tensorpack_data for dataset.\ncaffe.proto: 65.5KB [00:01, 43.4KB/s]\nSuccesfully downloaded caffe.proto 58196 bytes.\ncaffe.proto: No such file or directory\nTraceback (most recent call last):\n  File \"alexnet-dorefa.py\", line 311, in \n    run_image(Model(), DictRestore(np.load(args.load, encoding='latin1').item()), args.run)\n  File \"alexnet-dorefa.py\", line 262, in run_image\n    meta = dataset.ILSVRCMeta()\n  File \"D:\\Software\\Anaconda\\envs\\py35\\lib\\site-packages\\tensorpack\\dataflow\\dataset\\ilsvrc.py\", line 33, in init\n    self.caffepb = get_caffe_pb()\n  File \"D:\\Software\\Anaconda\\envs\\py35\\lib\\site-packages\\tensorpack\\utils\\loadcaffe.py\", line 128, in get_caffe_pb\n    \"Command protoc caffe.proto --python_out . failed!\"\nAssertionError: Command protoc caffe.proto --python_out . failed!\n. Cheers! I figured it out, after manually moving some files from one to another. Appreciation. \ud83d\udc4d \nBTW, if I want to use my own dataset, the path should look like:\n    PATH/\n      train/\n        n02134418/\n          n02134418_198.JPEG\n          ...\n        ...\n      val/\n        ILSVRC2012_val_00000001.JPEG\n        ...\nwhere is the label input to your model? If I have all pictures information contained in a file with format like:\n    pic1.path label1\n    pic2.path label2\n    ....\nany suggestions for adapting my dataset to your model?. ",
    "sounansu": "Thank you.\nLine 305 trace back are not occurred.\nYES,YES.\nI have another problem not tensorpack problem.\nMy network environment are under proxy network.\nSo, I set environment variables for proxy ,\nhttp_proxy=http://pxoxyurl:8080/\nhttps_proxy=https://pxoxyurl:8080/\nHTTPS_PROXY=https://pxoxyurl:8080/\nHTTP_PROXY=http://pxoxyurl:8080/\nBut, python or tensorflow can not understand these variables.....\n\n$ ./alexnet-dorefa.py --load alexnet-126.npy --run a.jpg --dorefa 1,2,6\n.\n.\n.\n[0913 10:27:26 @fs.py:40] ERR Failed to download https://github.com/BVLC/caffe/raw/master/src/caffe/proto/caffe.proto\nTraceback (most recent call last):\n  File \"./alexnet-dorefa.py\", line 303, in \n    run_image(Model(), ParamRestore(np.load(args.load, encoding='latin1').item()), args.run)\n  File \"./alexnet-dorefa.py\", line 255, in run_image\n    meta = dataset.ILSVRCMeta()\n  File \"/home/sounansu/tensorpack/tensorpack/dataflow/dataset/ilsvrc.py\", line 34, in init\n    self.caffepb = get_caffe_pb()\n  File \"/home/sounansu/tensorpack/tensorpack/utils/loadcaffe.py\", line 83, in get_caffe_pb\n    proto_path = download(CAFFE_PROTO_URL, dir)\n  File \"/home/sounansu/tensorpack/tensorpack/utils/fs.py\", line 36, in download\n    fpath, _ = urllib.request.urlretrieve(url, fpath, reporthook=_progress)\n  File \"/usr/lib/python2.7/urllib.py\", line 94, in urlretrieve\n    return _urlopener.retrieve(url, filename, reporthook, data)\n  File \"/usr/lib/python2.7/urllib.py\", line 240, in retrieve\n    fp = self.open(url, data)\n  File \"/usr/lib/python2.7/urllib.py\", line 208, in open\n    return getattr(self, name)(url)\n  File \"/usr/lib/python2.7/urllib.py\", line 437, in open_https\n    h.endheaders(data)\n  File \"/usr/lib/python2.7/httplib.py\", line 975, in endheaders\n    self._send_output(message_body)\n  File \"/usr/lib/python2.7/httplib.py\", line 835, in _send_output\n    self.send(msg)\n  File \"/usr/lib/python2.7/httplib.py\", line 797, in send\n    self.connect()\n  File \"/usr/lib/python2.7/httplib.py\", line 1182, in connect\n    self.sock = ssl.wrap_socket(sock, self.key_file, self.cert_file)\n  File \"/usr/lib/python2.7/ssl.py\", line 487, in wrap_socket\n    ciphers=ciphers)\n  File \"/usr/lib/python2.7/ssl.py\", line 243, in init\n    self.do_handshake()\n  File \"/usr/lib/python2.7/ssl.py\", line 405, in do_handshake\n    self._sslobj.do_handshake()\nIOError: [Errno socket error] [Errno 8] _ssl.c:510: EOF occurred in violation of protocol\n. I try to search urllib issue.\nBut current state are not good.....\n(I understand, i can access not secure url, but can not access secure url,,,)\nThank you for your advises.\nI continue to try.....\n. ",
    "erdollar": "Hi,can you help me with my problem during using dump-model-params?\n(tensorflow) xxxxx-Precision-Tower-7910:~/tensorflow/DoReFa-Net$ ./dump-model-params.py --meta train_log/alexnet-dorefa1118-000755/graph-1118-000802.meta train_log/alexnet-dorefa1118-000755/model out.npy\nFailed to load OpenCL runtime (expected version 1.1+)\nTraceback (most recent call last):\n  File \"./dump-model-params.py\", line 35, in \n    init = get_model_loader(args.model)\n  File \"/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/tfutils/sessinit.py\", line 261, in get_model_loader\n    return SaverRestore(filename)\n  File \"/home/xxxx/.local/lib/python2.7/site-packages/tensorpack/tfutils/sessinit.py\", line 106, in init\n    model_path = get_checkpoint_path(model_path)\n  File \"/home/xxxx/.local/lib/python2.7/site-packages/tensorpack/tfutils/varmanip.py\", line 167, in get_checkpoint_path\n    assert tf.gfile.Exists(model_path) or tf.gfile.Exists(model_path + '.index'), model_path\nAssertionError: train_log/alexnet-dorefa1118-000755/model\n. What xxx stands for?\n. Thank you for your patience. \ud83d\udc4d . Hi\uff0cI meet the same problem in runing alxnet-dorefa.py ,and weights are 1 bit. In tensorboard I see a normal distribution weights too. When I follow your advice above, I also get  a normal distribution weights. \n\nShould not the weights be just -1 or 1? Thanks a lot.\n. sir,i do this many times, but i got the weights histogram like normal distribution. Which steps did I make wrong?\n\n. thanks a lot.  I am a newer ,so I may be not change it by myself. Are you have changed any example such as restnet ,VGG ,and I will follow example to change alexnet? . ok ,thanks very much. . I try load model-xxx.data-xx   or model-xx.index but failed. Should i tranform them into .npy and load it? . Thanks . It's does't work .\ninformation is as following  @dongzhuoyao\nTraceback (most recent call last):\n  File \"alexnet-dorefa.py\", line 306, in \n    config.session_init = SaverRestore(args.load)\n  File \"/home/wa/.local/lib/python2.7/site-packages/tensorpack/tfutils/sessinit.py\", line 106, in init\n    model_path = get_checkpoint_path(model_path)\n  File \"/home/wa/.local/lib/python2.7/site-packages/tensorpack/tfutils/varmanip.py\", line 167, in get_checkpoint_path\n    assert tf.gfile.Exists(model_path) or tf.gfile.Exists(model_path + '.index'), model_path\nAssertionError: /train_log/alexnet-dorefa/model-660000\n(tensorflow) w@hu-Precision-Tower-7910:~/tensorflow/BNN/DoReFa-Net$ python alexnet-dorefa.py --dorefa 1,32,32 --load /train_log/alexnet-dorefa/model-660000 --data /resources/data/ILSVRC2012/images/ --gpu 1\n. yes .  File \"/train_log/alexnet-dorefa/model-660000.index\" exists.. wuuwu,I realized where I was wrong. I realize that is the absolute path and the relative path difference.Thanks a lot.. Thanks .. Sir, I use return PReLU('prelu',x)\nand it also doesnt run.\n(tensorflow) Precision-Tower-7910:~/tensorflow/BNN/DoReFa-Net$ python alexnet-dorefa.py --dorefa 1,32,32 --data /resources/data/ILSVRC2012/images/ --gpu 0\nFailed to load OpenCL runtime (expected version 1.1+)\n[1121 14:09:59 @alexnet-dorefa.py:305] Batch per tower: 128\n[1121 14:09:59 @logger.py:94] WRN Log directory train_log/alexnet-dorefa exists! Please either backup/delete it, or use a new directory.\n[1121 14:09:59 @logger.py:96] WRN If you're resuming from a previous run you can choose to keep it.\n[1121 14:09:59 @logger.py:97] Select Action: k (keep) / b (backup) / d (delete) / n (new) / q (quit):\nd\n[1121 14:10:00 @logger.py:74] Argv: alexnet-dorefa.py --dorefa 1,32,32 --data /resources/data/ILSVRC2012/images/ --gpu 0\n[1121 14:10:00 @fs.py:89] WRN Env var $TENSORPACK_DATASET not set, using /home/xxxxx/tensorpack_data for datasets.\n[1121 14:10:02 @prefetch.py:263] [PrefetchDataZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.\n[1121 14:10:02 @ilsvrc.py:118] Assuming directory /resources/data/ILSVRC2012/images/val has original structure.\n[1121 14:10:02 @inference_runner.py:82] InferenceRunner will eval on an InputSource of size 391\n[1121 14:10:02 @input_source.py:180] Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...\n[1121 14:10:02 @training.py:90] Building graph for training tower 0 on device LeastLoadedDeviceSetter-/gpu:0...\n[1121 14:10:02 @registry.py:121] conv0 input: [None, 224, 224, 3]\n[1121 14:10:02 @registry.py:129] conv0 output: [None, 54, 54, 96]\n[1121 14:10:02 @registry.py:121] conv1 input: [None, 54, 54, 96]\n[1121 14:10:02 @alexnet-dorefa.py:94] Binarizing weight conv1/W\nmul:0\n[1121 14:10:02 @registry.py:129] conv1 output: [None, 54, 54, 256]\n[1121 14:10:02 @registry.py:121] pool1 input: [None, 54, 54, 256]\n[1121 14:10:02 @registry.py:129] pool1 output: [None, 27, 27, 256]\nTraceback (most recent call last):\n  File \"alexnet-dorefa.py\", line 310, in \n    launch_train_with_config(config, SyncMultiGPUTrainer(nr_tower))\n  File \"/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/train/interface.py\", line 88, in launch_train_with_config\n    model._build_graph_get_cost, model.get_optimizer)\n  File \"/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/utils/argtools.py\", line 165, in wrapper\n    return func(args, kwargs)\n  File \"/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/train/tower.py\", line 137, in setup_graph\n    train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)\n  File \"/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/train/trainers.py\", line 79, in _setup_graph\n    self._make_get_grad_fn(input, get_cost_fn, get_opt_fn), get_opt_fn)\n  File \"/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/graph_builder/training.py\", line 137, in build\n    grad_list = DataParallelBuilder.build_on_towers(self.towers, get_grad_fn, devices)\n  File \"/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/graph_builder/training.py\", line 95, in build_on_towers\n    ret.append(func())\n  File \"/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/train/tower.py\", line 166, in get_grad_fn\n    cost = get_cost_fn(input.get_input_tensors())\n  File \"/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/tfutils/tower.py\", line 198, in call\n    output = self._tower_fn(args)\n  File \"/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/graph_builder/model_desc.py\", line 169, in _build_graph_get_cost\n    self.build_graph(inputs)\n  File \"/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/graph_builder/model_desc.py\", line 119, in build_graph\n    self._build_graph(inputs)\n  File \"alexnet-dorefa.py\", line 128, in _build_graph\n    .apply(activate) \n  File \"/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/models/linearwrap.py\", line 75, in apply\n    ret = func(self._t, args, kwargs)\n  File \"alexnet-dorefa.py\", line 113, in activate\n    return fa(nonlin(x))\n  File \"alexnet-dorefa.py\", line 105, in nonlin\n    return PReLU('prelu',x)\n  File \"/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/models/registry.py\", line 124, in wrapped_func\n    outputs = func(args, actual_args)\n  File \"/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/models/nonlin.py\", line 55, in PReLU\n    alpha = tf.get_variable('alpha', [], initializer=init)\n  File \"/home/xxxxx/.local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1065, in get_variable\n    use_resource=use_resource, custom_getter=custom_getter)\n  File \"/home/xxxxx/.local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 962, in get_variable\n    use_resource=use_resource, custom_getter=custom_getter)\n  File \"/home/xxxxx/.local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 360, in get_variable\n    validate_shape=validate_shape, use_resource=use_resource)\n  File \"/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/tfutils/varreplace.py\", line 53, in custom_getter\n    v = getter(*args, kwargs)\n  File \"/home/xxxxx/.local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 352, in _true_getter\n    use_resource=use_resource)\n  File \"/home/xxxxx/.local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 664, in _get_single_variable\n    name, \"\".join(traceback.format_list(tb))))\nValueError: Variable prelu/alpha already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\nFile \"/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/tfutils/varreplace.py\", line 53, in custom_getter\n    v = getter(*args, kwargs)\n  File \"/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/models/nonlin.py\", line 55, in PReLU\n    alpha = tf.get_variable('alpha', [], initializer=init)\n  File \"/home/xxxxx/.local/lib/python2.7/site-packages/tensorpack/models/registry.py\", line 124, in wrapped_func\n    outputs = func(*args, actual_args)\n. I just change  return tf.nn.relu(x) to   return PReLU('prelu',x)  in alexnet-dorefa.py .. Thanks.I can implement it when BITA ==32 .  And PReLU in Activaton which is not 32 bit will make no difference. Am I right\uff1f. OK ,Thanks.. Resnet needs to run 1281167 picture ( picture numbers of imagenet train set ) in one epoch ,right? If  TOTAL_BATCH_SIZE *steps_per_epoch != 1281167 , this will be contradiction with definition of epoch? \n\nepoch: one forward pass and one backward pass of all the training examples. Emm. I got it. Thanks .:smile:. Sorry I know it. haven't any ways or idea to directly save binary weights? Thanks. Thanks a lot. I will have a try.. Sir, I failed to do the post-processing after training. I can't make weights in .npy going through Binary Function. Could you help me?. Thanks. I know how to use fw(x). What confuse me is how to get the weights from the npy format ? (Because the weights are saved in a .npy file) I try to use np.load('xx.npy'), and just get :\n\n\n\n. Thank you for your patience. I get it. \nI use this code to get the weights:\na = np.load('1.npy')\nprint a.item()['which weights you want']. ",
    "pribadihcr": "look like .npy not supported now?. thanks it works. ",
    "thlinh": "@ppwwyyxx : Does tensorpack support reading the .pb files? If not, could you recommend other tools that can also extract the nets' weights to numpy arrays? Many thanks in advance!. @ppwwyyxx : Thank you for the answer. Yes, by .pb I mean the pn files created by tensorflow. But the API of the tools is really bad :( especially in tripping out all the unused nodes in inference modes. For example, I don't know how to remove (not turn off) the drop outs to make the graph size smaller.. ",
    "revilokeb": "@ppwwyyxx many thanks your code is really great: out-of-the box learning Pong-v0 on moderately recent hardware (6cores + GPU) in ~11h to a mean score of ~20. Most efficient publicly available A3C (variant) implementation I have come across so far...\n. ",
    "sujaynarumanchi": "My pleasure !\n. Hi,\nI faced the same issue with Tensorflow 0.9, but it worked when I upgraded to 0.10\n. Agreed, have made the change in the latest commit.\n. Sure, have made the changes.\n. ",
    "laceyg": "Using tensorflow 0.9 with CUDA 7.5, Python 2.7, cuDNN 4.0.  Here is a more detailed error when I run on the CPU:\n```\n$ CUDA_VISIBLE_DEVICES=. ./alexnet-dorefa.py --load npy/alexnet-126.npy --run /tmp/ilsvrc12/test/ILSVRC2012_test_00100000.JPEG --dorefa 1,2,6\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.4.0.7 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally\n[0922 13:16:05 @format.py:25] WRN Error in 'import lmdb'. LMDBData won't be available.\n[0922 13:16:06 @concurrency.py:27] WRN Cannot import Future in tornado.concurrent. MultiThreadAsyncPredictor won't be available.\n[0922 13:16:06 @_common.py:61] conv0 input: [None, 224, 224, 3]\n[0922 13:16:06 @_common.py:69] conv0 output: [None, 54, 54, 96]\n[0922 13:16:06 @_common.py:61] conv1 input: [None, 54, 54, 96]\n[0922 13:16:06 @alexnet-dorefa.py:88] Binarizing weight conv1/W\n[0922 13:16:06 @_common.py:69] conv1 output: [None, 54, 54, 256]\n[0922 13:16:06 @_common.py:61] pool1 input: [None, 54, 54, 256]\n[0922 13:16:06 @_common.py:69] pool1 output: [None, 27, 27, 256]\n[0922 13:16:06 @_common.py:61] conv2 input: [None, 27, 27, 256]\n[0922 13:16:06 @alexnet-dorefa.py:88] Binarizing weight conv2/W\n[0922 13:16:06 @_common.py:69] conv2 output: [None, 27, 27, 384]\n[0922 13:16:06 @_common.py:61] pool2 input: [None, 27, 27, 384]\n[0922 13:16:06 @_common.py:69] pool2 output: [None, 14, 14, 384]\n[0922 13:16:06 @_common.py:61] conv3 input: [None, 14, 14, 384]\n[0922 13:16:06 @alexnet-dorefa.py:88] Binarizing weight conv3/W\n[0922 13:16:07 @_common.py:69] conv3 output: [None, 14, 14, 384]\n[0922 13:16:07 @_common.py:61] conv4 input: [None, 14, 14, 384]\n[0922 13:16:07 @alexnet-dorefa.py:88] Binarizing weight conv4/W\n[0922 13:16:07 @_common.py:69] conv4 output: [None, 14, 14, 256]\n[0922 13:16:07 @_common.py:61] pool4 input: [None, 14, 14, 256]\n[0922 13:16:07 @_common.py:69] pool4 output: [None, 6, 6, 256]\n[0922 13:16:07 @_common.py:61] fc0 input: [None, 6, 6, 256]\n[0922 13:16:07 @alexnet-dorefa.py:88] Binarizing weight fc0/W\n[0922 13:16:07 @_common.py:69] fc0 output: [None, 4096]\n[0922 13:16:07 @_common.py:61] fc1 input: [None, 4096]\n[0922 13:16:07 @alexnet-dorefa.py:88] Binarizing weight fc1/W\n[0922 13:16:07 @_common.py:69] fc1 output: [None, 4096]\n[0922 13:16:07 @_common.py:61] fct input: [None, 4096]\n[0922 13:16:07 @_common.py:69] fct output: [None, 1000]\n[0922 13:16:07 @regularize.py:17] Apply regularizer for fc0/W:0\n[0922 13:16:07 @regularize.py:17] Apply regularizer for fc1/W:0\n[0922 13:16:07 @regularize.py:17] Apply regularizer for fct/W:0\nE tensorflow/stream_executor/cuda/cuda_driver.cc:491] failed call to cuInit: CUDA_ERROR_NO_DEVICE\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:153] retrieving CUDA diagnostic information for host: cop3\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:160] hostname: cop3\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:185] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:347] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  352.93  Tue Apr  5 18:18:24 PDT 2016\nGCC version:  gcc version 4.4.7 20120313 (Red Hat 4.4.7-17) (GCC) \n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] kernel reported version is: 352.93.0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:81] No GPU devices available on machine.\n[0922 13:16:07 @sessinit.py:154] Params to restore: bn3/gamma:0, bnfc0/variance/EMA:0, bn1/beta:0, fc0/W:0, conv2/W:0, bnfc1/gamma:0, conv4/W:0, bnfc0/gamma:0, bn3/beta:0, bn1/gamma:0, bn4/gamma:0, bn4/beta:0, bnfc0/mean/EMA:0, bn4/mean/EMA:0, bnfc0/beta:0, bn1/variance/EMA:0, fct/b:0, conv3/W:0, bn3/variance/EMA:0, bnfc1/variance/EMA:0, bnfc1/mean/EMA:0, bn2/gamma:0, bn1/mean/EMA:0, fct/W:0, conv0/W:0, bn3/mean/EMA:0, fc1/W:0, conv1/W:0, bn2/beta:0, bn4/variance/EMA:0, bn2/variance/EMA:0, bn2/mean/EMA:0, bnfc1/beta:0\n[0922 13:16:07 @sessinit.py:164] Restoring from dict ...\nTraceback (most recent call last):\n  File \"./alexnet-dorefa.py\", line 304, in \n    run_image(Model(), ParamRestore(np.load(args.load).item()), args.run)\n  File \"./alexnet-dorefa.py\", line 254, in run_image\n    predict_func = get_predict_func(pred_config)\n  File \"/home/laceyg/tensorpack/tensorpack/predict/common.py\", line 74, in get_predict_func\n    return OfflinePredictor(config)\n  File \"/home/laceyg/tensorpack/tensorpack/predict/base.py\", line 99, in init\n    config.session_init.init(sess)\n  File \"/home/laceyg/tensorpack/tensorpack/tfutils/sessinit.py\", line 31, in init\n    self._init(sess)\n  File \"/home/laceyg/tensorpack/tensorpack/tfutils/sessinit.py\", line 165, in _init\n    upd.update({name: value for name, value in six.iteritems(self.prms) if name in intersect})\n  File \"/home/laceyg/tensorpack/tensorpack/tfutils/varmanip.py\", line 71, in update\n    self.sess.run(op, feed_dict={p: value})\n  File \"/home/laceyg/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 372, in run\n    run_metadata_ptr)\n  File \"/home/laceyg/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 640, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/home/laceyg/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 708, in _do_run\n    target_list, options, run_metadata)\n  File \"/home/laceyg/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 728, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: AttrValue must not have reference type value of float_ref\n     for attr 'dtype'\n    ; NodeDef: Placeholder = Placeholderdtype=DT_FLOAT_REF, shape=[], _device=\"/device:CPU:0\"; Op output:dtype; attr=dtype:type; attr=shape:shape,default=[]>\n```\nI may try to upgrade tensorflow to 0.10 to see if it resolves the issue.\n. Upgrading to Tensforflow 0.10 fixed the problem for me as well - thanks for the advice!\n. Thanks for the helpful advice.  This method seems to work, but I am having one issue with regards to tf.cond.  In my model (similar to DoReFa), I have the indicator read in as an input for each data point:\ndef _get_input_vars(self):\n    return [InputVar(tf.float32, [None, 224, 224, 3], 'input'),\n            InputVar(tf.int32, [None], 'label'),\n            InputVar(tf.bool, [None], 'indicator') ]\nThis indicator is used, as discussed, to condition the loss function:\ndef _build_graph(self, input_vars):\n        image, label, indicator = input_vars\n        ...\n        self.cost = tf.cond(indicator, cost_1, cost_2)\nMeaning that the predicate for tf.cond is a tensor of shape (?,) whereas it expects a boolean:\nValueError: Shapes (?,) and () are not compatible\nIs there a proper way to format the predicate to get around this error?  Also, is there a nice way to confirm that tf.cond is using the correct loss function for each iteration?\n. Yes I see what you are saying, tf.select is more appropriate for an application using different samples in a batch (though one could restructure the problem using both methods).  What I was wondering was more about the correct syntax involved in using indicator of shape (?,) with an unknown dimension in tf.select or tf.cond, when the cost vectors are of shape ().  For example, the following would get around this error, but I believe is only using the indicator associated with the 0th data for each batch:\nself.cost = tf.select(indicator[0], cost_1, cost_2)\n. ",
    "yimintsai": "Hello, in Eq (2), it says that when xi, yi \u2208 {-1, 1}, we can use xnor. \n\n\u03a3ixiyi  = N - 2*bitcount(xnor(xi, yi)), xi, yi \u2208 {-1, 1} \u2200 i\n\nShould it be xor instead of xnor?\nFor instance, x := -1 -1 -1 and y := 1 1 1, the dot product is -3 and N - 2*bitcount(xnor(x, y)) is 3 - 2*0 = 3.\nThus, the equivalence is not hold. Is there any misunderstanding?\nThanks!. ",
    "the-bobo": "It took a while for me to understand, but I think I have it. Can you tell me if my understanding matches yours? I found the need to substantially rewrite section 2.1 to make explicit what the sequences are, how they are defined, and how they relate to leveraging reduced bitwidth computation in the context of neural nets. Please let me know if the following is correct.\n\n\n\n\n\n. Thanks, are my statements regarding \n\n(1) what to do when M != K \n\nand  \n\n(2) the status of all xi and yi as unsigned fixed point integers due to the later use of affine transformations\n\nin agreement with your method?\n. Two follow up questions:\n1) How do you determine the bit-width for intermediate values? For example, when feeding forward we need to calculate weights * previous activation + biases to get the input to the current layer's activation function. Let's call this value z (the \"weighted input\"). What bit-width should z take?\n2) How do you handle overflow with these fixed-bit-width integers? For example, assume you have weights restricted to 3-bits and activations restricted to 2-bits. Let's say you have a weight of 100 (4) and an activation of 10 (2) Their dot product is 1000 (8), and requires 4 bits to be represented. Do we just run the activation function on this 4-bit number, and then quantize down to the 2-bit activation value? Are there any other cases when overflow might happen?\nThank you very much\n. Thanks, i'm struggling to understand your point (2). Let's say we have the numbers:\nA: 1 0 0 (4)\nand\nB: 1 0 (2)\nLet's write out the bitcount and and operations:\n1. We and the 0 from B's least significant digit with each of the digits in A. This will always produce 0.\n2. We and the 1 from B's most significant digit with each of the digits in A. This will produce 0 except for the case where we and with A's most significant digit, which is also 1. In this case, bitcount(and(1, 1)) gives 1, and this value must be bitshifted by 2**(m+k), where m=2 and k=1. \n3. Thus, we bitshift by 2**3 to get our final product of A * B. This value is 1000, which overflows the bitwidth of the two multiplicands.\nAs you point out, the and(c_m(x), c_k(y)) will never overflow, and the bitcount total may be stored in a high-bitwidth number (say, uint_32) so that we can use the multiply routine over very long sequences of numbers.\nMy question is what to do about the output of that multiply routine - the returned product can be greater than the bitwidth of the specified parameters (Weights, Activations, Gradients), and it is unclear what bit-width to use for these intermediate values.\nEDIT: Okay, I see that using a bitwidth of (M+K) for these intermediate values guarantees we have enough bits to represent them. I'm not sure if this is a lower bound on the necessary bitwidth, though I suspect it is. Unsure of how to prove the lower bound beyond empirically.\nProof:\nLet M be the bit-width of multiplicand A, let K be the bit-width of multiplicand B. The maximum value A can take is 2M-1. The maximum value B can take is 2K-1. To calculate the maximum bit-width necessary to represent the product of A and B we can just multiply these maximum values together.\n(2M-1) * (2K-1) = 2(M+K)-2M-2K+1.\nThe maximum number representable with a bitwidth of (M+K) is, by definition, 2(M+K)-1. \nGiven that M, K > 0, (-2M-2K) <= -4, and thus 2(M+K) - 1 > 2(M+K)-2M-2K+1.\nThus, we know a bitwidth of (M+K) will be sufficient.\n. > The sign-vs-unsign problem is more relevant in FPGA. But as we are only doing summation, unsign numbers should be fine.\nHow do you implement this bitwise dot product kernel (equation 3 in section 2.1, DoReFa v2 paper) for negative weights? \nThe quantizek function defined in Section 2.2 as Equation 5 outputs a number ro \u2208 [0, 1]. The affine transform on Fwk(ri) in Equation 9 takes the output of a quantizek function and multiplies by 2 and subtracts 1: Fwk(ri) = 2 * quantizek(stuff) - 1.\nThus Fwk(ri) \u2208 [-1, 1].\nHowever, the procedure you define in Equation 3 only works for unsigned values. If some values xi in the sequence x or yi in the sequence y are negative, then their contribution to the dot product is a subtraction, not an addition, so the simple bitcount(and()) operation no longer suffices.\nHow did you change the bitwise dot product procedure to account for negative weights?\nOne possibility:\n1. Add an additional sign bit to all M-bit fixed point integers xi \u2208 x and all K-bit fixed point integers yi \u2208 y.\n2. This bit is 1 if the number xi is negative, and 0 if xi is positive (likewise for the yi), but does not count as a place-value bit for multiplication.\n3. Let bitwise_and(m, k) = and(cm(x), ck(y)), \u2200(m, k), ignoring the sign bits.\n4. Let bitwise_sign = xor(xisigned bit, yisigned bit). This gives us the sign of the product of xi and yi\n5. \u2200 bitwise_and(m, k) \u2200(m, k), note that bitwise_sign(xi, yi) is a vector giving the sign for each element in bitwise_and(m, k).\n6. For each pair of vectors ( bitwise_and(m, k), bitwise_sign(xi, yi) ) \u2200(m,k), drop all members of bitwise_and(m, k) and their corresponding signs in bitwise_sign(xi, yi) where bitwise_and(m, k) == 0. This leaves us with the cases where the bitwise multiplication produced a 1, along with their signs.\n7. For each pair of vectors ( bitwise_and(m, k), bitwise_sign(xi, yi) ) \u2200(m,k), compute bitcount[ bitwise_sign(xi, yi) ] to get the total number of negatives for the (m*k) place-value. The total number of positives is given by len(bitwise_sign(xi, yi)) - bitcount[ bitwise_sign(xi, yi) ].\n8. Use the negative and positive accumulations in 7 to get the signed contribution to the dot product.\n. Thanks - I've been experimenting with maxx = tf.reduce_max(tf.abs(x), list(range(1,rank)), keep_dims=True) to understand what it does. What do you mean by the \"batch\" axis?\nI understand why you clipped for safety - that makes sense, thank you.\nEDIT: In this code (https://github.com/ppwwyyxx/tensorpack/blob/master/examples/DoReFa-Net/dorefa.py) is x a vector or a matrix?\n. Thanks - I do not know what you mean when you say \"the first dimension is the mini-batch dimension.\" I'm familiar with mini-batch but not with TensorFlow. \n. Oh I see, if I understand correctly B is just the size of the mini-batch, so \"the mini-batch dimension\" just means the number of training samples in the mini-batch. Thank you.\nI'm not very familiar with convolutional nets. I'm currently trying to reproduce your team's results on a simple feedforward network. Is the C in [B,H,W,C] a vector with values for the channels red, green, blue?  (With H and W being height and width?)\n. Thanks for your help. I'd like to confirm that I understand your gradient quantization method.\nLet's say I was implementing DoReFa algorithm on a simple feedforward network. The output layer just outputs a single vector: a list of scalar values: O = [o1, o2, o3, o4, ..., on].\nI compute the gradients at the output layer: G = [g1, g2, g3, g4, ..., gn]\nTo produce the quantized gradients gbj I then do the following (as in Equation (12)):\n\u2200gj \u2208 G, gbj = 2*max(|G|) * ( quantizek [gj / (2 * max(|G|)) + \u00bd + N(k)] \u2013 \u00bd ).\nWhere max(|G|) is the element in G with the maximum absolute value.  So if G := [0, 4, -2, 1, -5], max(|G|) returns 5.\n. Thanks, if I did use a mini-batch with size > 1, say m = 4, then it seems like all that is necessary is to sum and average the quantized gradients. So in other words, if m=4 then we have 4 training inputs, x1, x2, x3, x4 per mini-batch, and the gradient is: 1/4 * \u03a3t=1 to m [ Gtb ], where Gt is the output gradient vector from training input xt, and Gtb is the quantized gradient vector from training input xt. \nIs that correct?\n. Sorry - I should have specified, where mini-batch size m > 1 I was implying that stochastic gradient descent would be used, where an estimate of the true gradient \u2207C = 1/n * \u03a3x (Cx) \u2200x \u2208 X is given instead by \u2207C \u2248 1/m * \u03a3t=1 to m( \u2207Cxt )\nwhere X is the set of all training samples and C is the cost function, and m is the size of a single mini-batch.\n\nFor example if your cost function is the average cost of all samples then the gradient w.r.t some weight would also be averaged across all samples.\n\nI was under the impression that all cost functions must be able to be written as an average over cost functions Cx for individual training samples x \u2208 X. Is this incorrect?\nEDIT: Regardless, I understand that the quantization method as described in Equation (12) of DoReFa-net paper v2 permits the implementer to apply a mini-batch specific scaling factor to weight the contribution of each mini-batch to the gradient. I believe this is what you mean by \"the maximum is taken over all axis of the gradient tensor dr except for the mini-batch axis (therefore each instance in a mini-batch will have its own scaling factor)\". Please let me know if I am mistaken, thank you.\n. I don't remember you mentioning biases in the paper - do you have any method for quantizing biases? Or do you simply imagine a single bias unit connected to all neurons in all layers, and train the weight for those connections?\n. Confirming one more detail. On step 13 of Algorithm 1 in DoReFa v2 paper, you say:\n\n13: gak-1\u2190backward_input(gbak, Wbk)\n\nDoes this mean that we have to re-quantize the gradients gak-1 using fG\u03b3() before backpropagating back another layer?\n. Another question - when we implement backward_input(), we end up doing the following:\ndC/daL \u2299 \u03c3\u2019(z\u20d7L) = \u03b4\u20d7L. \n(where dC/daL is a vector = gaL from step 9 of DoReFa v2 Algorithm 1, and \u03c3\u2019(z\u20d7L) is the derivative of the activation function \u03c3() evaluated over the weighted inputs z\u20d7L)\nThis gives us the error vector \u03b4\u20d7L from which we can use backward_weight() to get the gradients with respect to the weight matrix.\nThe questions: \n1) What bit-width do you use for the components of the vector \u03c3\u2019(z\u20d7L)?\n2) What bit-width do you use for the vector z\u20d7L (this may already be answered here as a bitwidth of (M+K) where M and K are the bit-widths of the multiplicands used to generate the components of z\u20d7L)\nThank you\n. Thanks my mistake - steps 12 and 13 are part of a loop...\n. I think I misunderstood backward_input and backward_weight. Can you please explain what steps 13 and 14 do? It is unclear how they operate within the loop started in step 10 (Algorithm 1, DoReFa v2 paper).\nWhen backpropagating the following things need to happen:\n1) Compute the output layer's gradient: this is dC/daL, the derivative of the cost function with respect to the final layer (layer L)'s output activations. \nThis looks like step 9 in Algorithm 1: \"Compute gaL = \u2202C/\u2202aL knowing aL and a*.\"\n2) Feed this gradient through the derivative of the activation function, to get the error \u03b4L of the output layer: \u03b4L = gaL\u2299\u03c3\u2019(zL), where \u03c3\u2019() is the derivative of the activation function, zL are the weighted inputs to the last layer L, and \u2299 is the Hadamard product (elementwise product).\nThis looks like step 11 in Algorithm 1: \"Back-propagate gak through activation function h.\"\n3) Backpropagate the error \u03b4L \u2200 \u2113 < L. \u03b4\u2113 = ((w\u2113+1)T \u00b7 \u03b4\u2113+1) \u2299\u03c3\u2019(z\u2113).\nThis might be step 13? Except the output of step 13 is non-quantized (note there is no b, the output is simply gak-1).\n4) Apply the update to the weights: w\u2113 \u2192 w\u2113 - (\u03b7/m) * (a\u2113-1 \u00b7 \u03b4\u2113), where a\u2113-1 is the activation vector from layer \u2113-1,  w\u2113 is the weight matrix at layer \u2113, and m is the mini-batch size.\nThis looks like steps 18 and 19 in Algorithm 1: \ngWk = gWkb * (\u2202Wkb / \u2202Wk). \nWkt+1 \u2190 Update(Wk, gWk, \u03b7).\nQuestions:\nQ1) In the four steps outlined above for backprop, when do we apply the quantization routine in Step 12 from Algorithm 1?\nQ2) What is the meaning of backward_input() - is it one of the four steps above or something else?\nQ3) What is the meaning of backward_weight() - is it one of the four steps above or something else?\nQ4) Why is the output of step 13's backward_input() non-quantized?\nEDIT: Upon re-reading, it looks like you're actually quantizing errors not gradients? And step 13 uses the quantized error gakb and the quantized weight matrix Wkb to get a non-quantized quantity for the previous layer, gak-1, which you must then use in Step 11 of the next iteration of the for loop to get the true gak-1, at which point you quantize it to gak-1b with Step 12.\nIs this correct?\nIn this case the mistake would come from overloading the notation gak-1 to refer to the error \u03b4\u2113 at layer \u2113 as well as the quantity A in the following equation: \u03b4\u2113 = A \u2299\u03c3\u2019(z\u2113), where A == ((w\u2113+1)T \u00b7 \u03b4\u2113+1).\n. Thanks, 1 and 2 make sense to me.\nFor 3 - where can I find this calculation being done in your DoReFa code?\nEDIT: Also, when you say \"where the derivative of quantize() function is already defined\" - are you referring to something specific in your paper?\n. (6) says \u2202c/\u2202ri = \u2202c/\u2202ro, where ri = a single real-valued weight, and ro = a single k-bit quantized, real-valued weight.\nWe want to find a component expression for \u2202Wkb/\u2202Wk, where Wkb is a matrix containing quantized weights, and Wk is the full-precision version of that matrix.\nIn component terms, we are looking for \u2202ro/\u2202ri, as ro are the components of Wkb, and ri are the components of Wk.\nGiven (6), that \u2202c/\u2202ri = \u2202c/\u2202ro, are you saying that \u2202ri = \u2202ro, which means \u2202ro/\u2202ri = 1? \nIn that case, line 18 of the algorithm on page six reduces from:\ngWk = gWkb(\u2202Wkb/\u2202Wk)\ngWk = gWkb(1)\ngWk = gWkb\nIs this what you intended? I think I am mistaken somehow but am not sure\u2026\n\nMy other strategy is to attempt computing the derivative with respect to ri for the following equation:\n2 * quantizek[ tanh(ri) / (2*max(|tanh(ri)|)) + 1/2] -1.\nPlease advise, thank you!\n. I'm working on this but am getting stuck - how do you derivate the max() function in the divisor?\n. > Wkb = f_w(Wk) defined in equation (9) so your second strategy is correct.\nOkay, let's do this. Does the following result look correct to you? I am particularly anxious about my understanding of the Straight-Through-Estimator, as I could not find the Hinton video lecture you cite (which the Bengio paper you cite also references).\ndFwk/dri = 2 * \u27e6 (1-tanh2(ri)) * [2 * max(|tanh(ri)|)]-1 \u27e7\n\nLet Fwk := 2 * quantizek[ tanh(ri) / ( 2*max(|tanh(ri)|) ) + 1/2 ] - 1\nGiven quantizek(ri) := (1 / (2k-1)) * round((2k - 1)ri).\nFind dFwk/dri.\nFirst, express Fwk as the composition of multiple functions, so that chain rule may be used.\n(1) Fwk = 2 * quantizek[ tanh(ri) * ( 2*max(|tanh(ri)|) )-1 + 1/2 ] - 1\n(2) Define component functions as follows:\n- a(ri) = 2 * [1/(2k-1) * round((2k-1) * ri)] -1\n- b(ri) = tanh(ri) * [ 2*max(|tanh(ri)|) ]-1 + 1/2\n\u2234  Fwk = a(b(ri))\n(3) (via chain rule) dFwk/dri = a'(b(ri)) * b'(ri)\nStart by expanding b'(ri)\n(4) The max() function in the square brackets is a constant when derivating with respect to ri, as the max value is taken over all the weights ri in a given layer, and thus does not vary with respect to the particular ri with which we are derivating.\n(5) \u2234 (via Identity One) b'(ri) = (1-tanh2(ri)) * [2 * max(|tanh(ri)|)]-1\nNow expand a'(b(ri))\nFirst consider a'(ri)\n(6) (under condition of Expectation of a Straight-Through-Estimator) a'(ri) = 2.\nConsider that the quantizek function decomposes into two scaling factors and a round operator. The two scaling factors are the multiplicands 1/(2k-1) and (2k-1), on the outside and inside of the round operator respectively. \nIf we evaluated this function in the absence of a round operator, the two scalars would cancel, and the function would change in a manner directly proportional to the input ri, leaving a derivative of 1. Since a(ri) is 2 * quantizek(ri) - 1, this derivative would be 2 * 1 = 2.\nThe round operator is simply an additive shift of [-0.5, 0.5], as it either rounds a number down or up to the nearest integer value. Thus our outer scale factor of 1/(2k-1) is technically multiplying the result of (2k-1)*(ri)+ \u03b4, where \u03b4 \u2208 [-0.5, 0.5].\nIn any given evaluation of a(ri) it is unlikely that the two scale factors are exact inverses, due to the low probability of \u03b4 = 0. However, under assumption that \u03b4 is drawn uniformly and at random from [-0.5, 0.5], the expectation is \ud835\udc04(\u03b4) = 0. \n\u2234 under expectation \ud835\udc04(\u03b4) = 0, dquantizek(ri)/dri = 1\n(7) Given (6), a'(b(ri)) = 2, as the derivative a' is a constant, and thus does not vary with respect to the function's input.\nNow compute dFwk/dri = a'(b(ri)) * b'(ri)\n(8) dFwk/dri = 2 * \u27e6 (1-tanh2(ri)) * [2 * max(|tanh(ri)|)]-1 \u27e7 \u220e\nIdentities\n1. Identity One: d/dx tanh(x) = 1-tanh2(x)\nSource: https://en.wikipedia.org/wiki/Hyperbolic_function#Derivatives\n. I'll take a look at that, thank you for the code. Where does that code come\nfrom?\nAlso, does my interpretation of the STE and its value under expectation\nmatch yours?\nOn Oct 11, 2016 9:15 PM, \"Yuxin Wu\" notifications@github.com wrote:\n\nd quantizek(ri)/dri = 1. This is correct. That's exactly how we define\nthe function quantize_k in (5) and (6).\nmax() actually does have a gradient. It varies with respect to the maximum\nvalue. You might need to take that into account.\na = tf.placeholder(dtype=tf.float32, name='a', shape=[4])\nb = tf.reduce_max(a)\nc = tf.gradients([b], [a])[0]with tf.Session() as sess:\n    v = np.asarray([1, 2, 3, 4], dtype='float32')\n    print sess.run(c, feed_dict={a:v})  # 0, 0, 0, 1\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/ppwwyyxx/tensorpack/issues/31#issuecomment-253114839,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AEIejzEcxXrZN6ZOj3ZttIT3x2KoQtT4ks5qzF7HgaJpZM4KRgfU\n.\n. Regarding STE - okay, I think we are doing the same thing, just different justifications. My argument provides a reason for defining the gradient of quantizek as 1. (It is a way to convert the step-wise shape of the round() graph into a linear approximation, whose slope is 1.)\n\nRegarding your tf.gradients() code - I don't understand. If we are differentiating 2 * quantizek[ tanh(ri) / (2*max(|tanh(ri)|)) + 1/2] -1 with respect to ri, then the max(ri) for a given layer is just a per-layer constant. I think I am misunderstanding you. \n. Okay that's interesting. Let me confirm that I understand what you are saying.\nIs this what you mean?\n1. Let r\u20d7\u2113 := a vector with weights from layer \u2113\n2. Let r\u2113i := the ith component of the weight vector r\u20d7\u2113\n3. Let r\u2113max := the maximum component of the weight vector r\u20d7\u2113\n4. \u2200 r\u2113i where r\u2113i \u2260 r\u2113max, dr\u2113max/dr\u2113i = 0.\n5. When r\u2113i = r\u2113max, dr\u2113max/dr\u2113i = 1.\n. Yes, this is definitely a subgradient. However, it seems unstable. What do you recommend in the case where r\u2113i = r\u2113j =  r\u2113max? In other words, what if there are two components of the r\u20d7\u2113 weight vector that are both the same maximum?\nI am not sure I understand fully, thank you.\n. Ohhhhhh I get it, so if there are multiple r\u2113i that satisfy r\u2113max, you just split the unit gradient of 1 evenly among them. \nSo if r\u2113i = r\u2113j = r\u2113k = r\u2113max, then dr\u2113max/dr\u2113i = dr\u2113max/dr\u2113j = dr\u2113max/dr\u2113k = (1/3).\nOkay wow, that's subtle. Please confirm that this is what you mean! \nSecond question - if I can no longer treat d[2max(|tanh(ri)|)]-1/dri as a constant, then I must use chain rule with it and expand my differentiation of 2 * quantizek[ tanh(ri) / (2*max(|tanh(ri)|)) + 1/2] -1 further. Is this correct?\n. Thanks ppwwyyxx, two more questions:\n1) When I'm differentiating 2 * quantizek[ tanh(r\u2113i ) / (2*max(|tanh(r\u2113i )|)) + 1/2] -1 with respect to dr\u2113i , for that max() function in the denominator, you write \"where the maximum is taken over all weights in that layer\" (page 4, DoReFa v2 paper).\nAre you talking about finding the maxima of the full-precision, non-quantized weights?\n2) Does \"full-precision\" for DoReFa v2 paper mean \"float32\" ?\n. Okay, here is an attempt at an explicit proof for the derivative of the weight quantization function used in DoReFa v2 paper. It is remarkable that we trust TensorFlow to auto-differentiate this correctly. Please let me know if this proof looks correct, thank you!\nWhen implementing DoReFa outside of TensorFlow, it is necessary to have an explicit derivative for computing Step 18 of Algorithm 1 from DoReFa v2 paper: gWk = gWkb * \u2202Wkb / \u2202Wk.\n- Note that dri\u2113 refers to the derivative of some function with respect to ri\u2113, while d(ri\u2113) is itself a function.\n- ri\u2113 refers to some real-valued weight i in layer \u2113. \nLet Fwk := 2 * quantizek[ tanh(ri\u2113) / ( 2*max(|tanh(ri)|) ) + 1/2 ] - 1\nGiven quantizek(ri\u2113) := (1 / (2k-1)) * round((2k - 1)ri\u2113).\nFind dFwk/dri\u2113.\nFirst, express Fwk as the composition of multiple functions, so that chain rule may be used.\n(1) Fwk = 2 * quantizek[ tanh(ri\u2113) * ( 2*argmax(|tanh(ri\u2113)|) )-1 + 1/2 ] - 1\n(2) Define component functions as follows:\n- h(ri\u2113) = 2 * argmax(|tanh(ri\u2113)|)\n- e(ri\u2113) = (ri\u2113)-1\n- d(ri\u2113) = tanh(ri\u2113) * e(h(ri\u2113)) + 1/2\n- c(ri\u2113) = quantizek(ri\u2113) = (1 / (2k-1)) * round((2k-1)ri\u2113)\n- b(ri\u2113) = 2 * ri\u2113 - 1\nNote \u2013 to avoid confusion with Fwk(ri\u2113), there is no f(ri\u2113). There is also no g(ri\u2113).\n(3) \u2234 Fwk = b(c(d(ri\u2113)))\n(4) (via chain rule) F\u2019wk = b\u2019(c(d(ri\u2113))) * c\u2019(d(ri\u2113)) * d\u2019(ri\u2113)\nSecond, begin finding the three derivatives in (4), so that F' may be calculated.\nStart with d\u2019(ri\u2113)\n(5) d(ri\u2113) = tanh(ri\u2113) * e(h(ri\u2113)) + 1/2\n(6) Let a(ri\u2113) = e(h(ri\u2113))\n(7) (via product rule) d\u2019(ri\u2113) = dtanh(ri\u2113)/dri\u2113 * a(ri\u2113) + tanh(ri\u2113) * a\u2019(ri\u2113)\n(8) (via Identity One) d\u2019(ri\u2113) = (1-tanh2(ri\u2113)) * a(ri\u2113) + tanh(ri\u2113) * a\u2019(ri\u2113)\nNow find a\u2019(ri\u2113)\n(9) (via chain rule) a\u2019(ri\u2113) = e\u2019(h(ri\u2113)) * h\u2019(ri\u2113)\nBegin with differentiating h(ri\u2113) = 2 * argmax(|tanh(ri\u2113)|)\n(10) (via Lemma One) h\u2019(ri\u2113) = 0 if ri\u2113 \u2209 \u2113max, and 2 * (1/|\u2113max|) if if ri\u2113 \u2208 \u2113max, where |\u2113max| denotes the cardinality of the set \u2113max, where \u2113max is the set of all ri\u2113 that satisfy argmax(|tanh(ri\u2113)|).\n(11) (via (9)) \u2234 a\u2019(ri\u2113) = 0 if ri\u2113 \u2209 \u2113max\nNow find e\u2019(h(ri\u2113))\n(12) (via power rule) e\u2019(h(ri\u2113)) = -1 * [2 * argmax(|tanh(ri\u2113)|)]-2\n(13) \u2234 a\u2019(ri\u2113) = [ -1 * (2 * argmax(|tanh(ri\u2113)|))-2 ] * h\u2019(ri\u2113) = \nCase 1: 0 if ri\u2113 \u2209 \u2113max\nCase 2: [ -1 * (2 * argmax(|tanh(ri\u2113)|))-2 ] * [ 2 * (1/|\u2113max|) ] if ri\u2113 \u2208 \u2113max.\n(14) \u2234 d\u2019(ri\u2113) = (1-tanh2(ri\u2113)) * a(ri\u2113) + tanh(ri\u2113) * a\u2019(ri\u2113) = \nCase 1: (1-tanh2(ri\u2113)) * a(ri\u2113) if ri\u2113 \u2209 \u2113max\nCase 2: (1-tanh2(ri\u2113)) * a(ri\u2113) + tanh(ri\u2113) * a\u2019(ri\u2113) if ri\u2113 \u2208 \u2113max.\nNow find c\u2019(d(ri\u2113))\n(15) (via Lemma Two) c\u2019(d(ri\u2113)) = 1\nNow find b\u2019(c(d(ri\u2113))\n(16) b\u2019(c(d(ri\u2113)) = 2 \nNow find F\u2019wk\n(17) F\u2019wk = b\u2019(c(d(ri\u2113))) * c\u2019(d(ri\u2113)) * d\u2019(ri\u2113) \n= 2 * 1 * d\u2019(ri\u2113)\nCase 1: 2 * [ (1-tanh2(ri\u2113)) * a(ri\u2113) ] if ri\u2113 \u2209 \u2113max\nExpanding Case 1:\n= 2 * [ 1-tanh2(ri\u2113)) * (2 * argmax(|tanh(ri\u2113)|))-1 ]\nCase 2: 2 * [ (1-tanh2(ri\u2113)) * a(ri\u2113) + tanh(ri\u2113) * a\u2019(ri\u2113) ] if ri\u2113 \u2208 \u2113max.\nExpanding Case 2: \n= 2 * [ 1-tanh2(ri\u2113)) * (2 * argmax(|tanh(ri\u2113)|))-1 + tanh(ri\u2113) * (-1 * (2 * argmax(|tanh(ri\u2113)|))-2 ] * [ 2 * (1/|\u2113max|) ) ] if ri\u2113 \u2208 \u2113max.\nWe now have an explicit formula for differentiating Fwk with respect to ri\u2113 over two cases, Case 1 in which ri\u2113 \u2209 \u2113max and Case 2 in which ri\u2113 \u2208 \u2113max.\nIdentities\n1. Identity One: d/dx tanh(x) = 1-tanh2(x)\nLemmas\n[1] Lemma One: subderivative of the argmax function.\nQuoting Wikipedia, \u201cthe subderivative, subgradient, and subdifferential generalize the derivative to functions which are not differentiable. The subdifferential of a function is set-valued.\u201d (https://en.wikipedia.org/wiki/Subderivative).\nIntuitively, the subderivative is a way to differentiate continuous functions at non-differentiable points. \nWithout proof we hold that the subderivative of a function f(x) with domain \u2208 \u211d and range \u2208 \u211d at a non-differentiable point x0 is the closed set [a, b] where a = limx\u2192x0-[(f(x)-f(x0)) / (x-x0)] and b = limx\u2192x0+[(f(x)-f(x0)) / (x-x0)].\nThat is, the subderivative at point x0 is a set whose elements are the closed interval [a, b] where a and b are the left-hand and right-hand derivatives of f(x) as x approaches x0, respectively.\n(N.B.: You can think of the traditional derivative as a special case where the set comprising the solutions to the subderivative has only one element.)\nAccordingly, we define the subderivative of the argmax function by cases.\n(1) Let y(ri\u2113) = argmax(|tanh(ri\u2113)|), where y(ri\u2113) evaluates to true when ri\u2113 produces the maximum value of |tanh(ri\u2113)| \u2200 ri\u2113 \u2208 \u2113, and false otherwise.\nThat is, y(ri\u2113) is evaluated \u2200 ri\u2113 \u2208 \u2113, and is true when its input is the weight in layer \u2113 that produces the maximum value for the |tanh(ri\u2113)| function.\n(2) Let \u2113max := { ri\u2113 | y(ri\u2113) } denote the set of all weights in layer \u2113 that produces a maximum value as defined in (1).\n(3) Case one: argmax\u2019(|tanh(ri\u2113)|) = 0 if ri\u2113 \u2209 \u2113max.\n(4) Case two: argmax\u2019(|tanh(ri\u2113)|) = 1 / |\u2113max| if ri\u2113 \u2208 \u2113max where |\u2113max| denotes the cardinality (number of elements) of the set \u2113max. \nNote that if ri\u2113 is the only element in \u2113max then its subderivative is 1.\nNote that if ri\u2113 is not the only element in \u2113max then it splits the subderivative of 1 equally with all other ri\u2113 \u2208 \u2113max.    \n[2] Lemma Two: differentiating quantizek(ri\u2113) yields 1 under condition of Expectation of a Straight-Through Estimator.\n(1) Given quantizek(ri\u2113) = (1 / (2k-1)) * round((2k - 1)ri\u2113).\n(2) Consider ~quantizek(ri\u2113) \u2254 (1 / (2k-1)) * ((2k - 1)ri\u2113) = ri\u2113.\n(3) ~quantize\u2019k(ri\u2113) = 1.\n(4) Observation: in the absence of a round() function the two scale factors cancel and the derivative is 1. Can we make the round() function \u201cdisappear\u201d in some justified fashion?\n(5) Note that the round(ri\u2113) function adds some number n \u2208 [-0.5, 0.5] to ri\u2113 to round ri\u2113 to the nearest integer. \n(6) If n = 0, then quantize\u2019k(ri\u2113) = ~quantize\u2019k(ri\u2113) = 1.\n(7) if n \u2260 0, then our outer scale factor of 1/(2k-1) is technically multiplying the result of (2k-1)(ri\u2113 + _n), and thus the scale factors do not cancel.\n(8) Note that the probability of n == 0 is low, and so any individual call to quantizek() is unlikely to result in a case where we can ignore the round() function.\n(8) However, under assumption that n is drawn uniformly and at random from [-0.5, 0.5], the expectation \ud835\udc04(n) = 0.\n(9) \u2234 under expectation \ud835\udc04(_n) = 0, quantize\u2019k(ri\u2113) = ~quantize\u2019k(ri\u2113) = 1.*\nConsider that the quantizek function decomposes into two scaling factors and a round operator. The two scaling factors are the multiplicands 1/(2k-1) and (2k-1), on the outside and inside of the round operator respectively. \nIf we evaluated this function in the absence of a round operator, the two scalars would cancel, and the function would change in a manner directly proportional to the input ri, leaving a derivative of 1. Since a(ri) is 2 * quantizek(ri) - 1, this derivative would be 2 * 1 = 2.\nThe round operator is simply an additive shift of [-0.5, 0.5], as it either rounds a number down or up to the nearest integer value. Thus our outer scale factor of 1/(2k-1) is technically multiplying the result of (2k-1)*(ri)+ \u03b4, where \u03b4 \u2208 [-0.5, 0.5].\nIn any given evaluation of a(ri) it is unlikely that the two scale factors are exact inverses, due to the low probability of \u03b4 = 0. However, under assumption that \u03b4 is drawn uniformly and at random from [-0.5, 0.5], the expectation is \ud835\udc04(\u03b4) = 0. \n\u2234 under expectation \ud835\udc04(\u03b4) = 0, dquantizek(ri)/dri = 1\n. ",
    "emailhy": "yes\uff0cthank you\n. ",
    "JINwonLEE": "I have some questions in getting gradient quantization. In the paper, it said that \n\"The above function first applies an affine transform on the gradient, to map it into [0, 1], and then inverts the transform after quantization.\"\nIn this sentence, what is \"inverts\" mean?\nI can't get the points.. Can I get some advice?. Oh, I get it.. However why do we have to invert the transform? Is there any special reason? Also I can't get the intention of multiplying max_0(|dr|) in the front. Can you explain little bit in detail?. Thanks a lot...!!! I got the all equation.\nSorry but I had little questions about N(k), \nNoise function N(k).. In the paper, \n 1. Is there any guarantee that dr/(2 * max_0(|dr|)) + 1/2 + N(k) < 1? (Becuase quantization should be in [0,1])\n\nCan you give me some explanation about specific affect of noise function(N(k)) in gradient quantization? (For example, the performance becomes better because of some aspects... of noise function..). Oh I didn't see the code yet. Thanks a lot. \n\nFor 2., I should see the paper more in detail.. While reading the paper, I have something curious. \nIn the contribution, paper said \n\"DoReFa-Net can use bit convolution kernels to accelerate both the forward pass and the backward pass of the training process.\"\nIn this sentence, what is bit convolution kernel? Is it implemented by authors or just using exist thing?\nI thought that it is implemented by authors, but I want to make sure and if it was implemented then is it open source? . Oh Thanks for reference. \nI had some minor questions in the experiment.\nIn the ImageNet experiment, there are case which is initialized. In the paper, it said that \"training has been initiailized with a 32-bit model.\"\nAs I understand, the case \"initialized\" means that even though they are using 1-bit weight, activation and 32-bit gradients, they initialized all the 3 values(weight, activation, gradients) in 32-bit, different from other case which is initialized in lower than 32 bit. Am I in right track? \nIf not, can I get some advice?. Ah.. you mean that the case \"initialized\" is that they initialized weights with a 32bit and the other case is just randomly chosen by W bits. Is it right? . Ah.. now I understand what the table means, The cases \"initialized\" are start from initialized weight with pretrained 32bit model and the others are initialized with randomized values!!! . ",
    "MotorCityCobra": "That is all I want to do. I'm just beyond my depth. When I type...\n$ python run-atari.py --load Breakout-v0.tfmodel --env Breakout-v0\nTraceback (most recent call last):\n  File \"run-atari.py\", line 24, in \n    from common import play_one_episode\nImportError: cannot import name play_one_episode\nI think the...\n Enable import tensorpack:\nexport PYTHONPATH=$PYTHONPATH:readlink -f path/to/tensorpack\n... part from your instructions is my problem. I don't know.\nIt's to make Python able to find the tensorpack file?\nI enter this and get an error...\n$ export PYTHONPATH=$PYTHONPATH: readlink -f /Users/me/PE11/gym/tensorpack\n-bash: export: -f': not a valid identifier\n-bash: export:/Users/me/PE11/gym/tensorpack': not a valid identifier\n. Thank for all the responses\nThe export PYTHONPATH=$PYTHONPATH thing seems to have worked\nI think I'm down to one last thing...\nThat common.py file is pointing to another common.py file in the /Atari2600 folder if I'm not mistaken.\nI know I should know how to do this before bothering a software wizard but I don't know how to get it to point to the Atari2600 common file\nThe original way it's written I get\n    ../Atari2600/common.py\n    ^\nSyntaxError: invalid syntax\nI dug online and couldn't figure out how to do it for python (OSX)\nI tried saving the path as this instead (in the common.py file)...\n~/Users/AnthonyCelio/PE11/gym/tensorpack/examples/Atari2600/common.py\n     ^\nSyntaxError: invalid syntax\nand tried\n~/Atari2600/common.py\n ^\nSyntaxError: invalid syntax\n. I got past the parts discussed earlier, and thank you again.\nI though I was set up to go but I don't know what I'm doing with this. I'm taking a break from this for a while\n$ python run-atari.py --load Breakout-v0.tfmodel --env Breakout-v0\n[1020 12:37:33 @run-atari.py:87] Environment Name: Breakout-v0\n[2016-10-20 12:37:33,622] Making new env: Breakout-v0\n[2016-10-20 12:37:33,672] Making new env: Breakout-v0\nTraceback (most recent call last):\n  File \"run-atari.py\", line 98, in \n    run_submission(cfg)\n  File \"run-atari.py\", line 66, in run_submission\n    player = get_player(dumpdir=dirname)\n  File \"run-atari.py\", line 27, in get_player\n    pl = GymEnv(ENV_NAME, dumpdir=dumpdir, auto_restart=False)\n  File \"/Users/me/PE11/gym/tensorpack/RL/gymenv.py\", line 33, in init\n    self.gymenv.monitor.start(dumpdir)\n  File \"/Users/me/PE11/gym/gym/monitoring/monitor.py\", line 138, in start\n    You should use a unique directory for each training run, or use 'force=True' to automatically clear previous monitor files.'''.format(directory, ', '.join(training_manifests[:5])))\ngym.error.Error: Trying to write to monitor directory gym-submit with existing monitor files: gym-submit/openaigym.manifest.0.67134.manifest.json.\nYou should use a unique directory for each training run, or use 'force=True' to automatically clear previous monitor files.\n$\n$\n. It's running. Thank you so much for your help.\nDo need to add\n'--display true'\nto the end of... \n$'python run-atari.py --load Breakout-v0.tfmodel --env Breakout-v0'\nto see it play?\nI just have the following in Terminal...\n......[1020 19:43:34 @_common.py:69] conv3 output: [None, 10, 10, 64]\n[1020 19:43:34 @_common.py:61] fc0 input: [None, 10, 10, 64]\n[1020 19:43:34 @_common.py:69] fc0 output: [None, 512]\n[1020 19:43:34 @_common.py:61] fc-pi input: [None, 512]\n[1020 19:43:34 @_common.py:69] fc-pi output: [None, 6]\n[1020 19:43:34 @sessinit.py:70] Restoring checkpoint from Breakout-v0.tfmodel.\n[1020 19:43:34 @sessinit.py:132] WRN Variable fc-v/W in checkpoint not found in the graph!\n[1020 19:43:34 @sessinit.py:132] WRN Variable fc-v/b in checkpoint not found in the graph!\n('Total:', 450.0)\n[2016-10-20 19:44:49,820] Starting new video recorder writing to /Users/me/PE11/gym/gym-submit/openaigym.video.0.69274.video000001.mp4\n('Total:', 850.0)\n('Total:', 473.0)\n('Total:', 860.0)\n('Total:', 436.0)\n('Total:', 461.0)\n. Nevermind. I see know. It saves as an mp4.\nThank you again!!!!!!!!\n. That's Ok. It did compute for a while and create 5 mp4s of it learning the game, but it did stop the function after this error and go back to the $ prompt in Terminal.\nThe error seems to get cropped in places when I copy and paste it so I'm attaching\n\n a screenshot.\nI'll put the entire log below and close the issue cause I'm not too invested in what it's about either...\n$\n$ python run-atari.py --load MsPacman-v0.tfmodel --env MsPacman-v0\n[1021 20:56:46 @run-atari.py:87] Environment Name: MsPacman-v0\n[2016-10-21 20:56:46,646] Making new env: MsPacman-v0\n[2016-10-21 20:56:46,761] Making new env: MsPacman-v0\n[2016-10-21 20:56:46,842] Starting new video recorder writing to /Users/me/PE11/gym/gym-submit/openaigym.video.0.2209.video000000.mp4\n[1021 20:56:46 @_common.py:61] conv0 input: [None, 84, 84, 12]\n[1021 20:56:46 @_common.py:69] conv0 output: [None, 84, 84, 32]\n[1021 20:56:46 @_common.py:61] pool0 input: [None, 84, 84, 32]\n[1021 20:56:46 @_common.py:69] pool0 output: [None, 42, 42, 32]\n[1021 20:56:46 @_common.py:61] conv1 input: [None, 42, 42, 32]\n[1021 20:56:46 @_common.py:69] conv1 output: [None, 42, 42, 32]\n[1021 20:56:46 @_common.py:61] pool1 input: [None, 42, 42, 32]\n[1021 20:56:47 @_common.py:69] pool1 output: [None, 21, 21, 32]\n[1021 20:56:47 @_common.py:61] conv2 input: [None, 21, 21, 32]\n[1021 20:56:47 @_common.py:69] conv2 output: [None, 21, 21, 64]\n[1021 20:56:47 @_common.py:61] pool2 input: [None, 21, 21, 64]\n[1021 20:56:47 @_common.py:69] pool2 output: [None, 10, 10, 64]\n[1021 20:56:47 @_common.py:61] conv3 input: [None, 10, 10, 64]\n[1021 20:56:47 @_common.py:69] conv3 output: [None, 10, 10, 64]\n[1021 20:56:47 @_common.py:61] fc0 input: [None, 10, 10, 64]\n[1021 20:56:47 @_common.py:69] fc0 output: [None, 512]\n[1021 20:56:47 @_common.py:61] fc-pi input: [None, 512]\n[1021 20:56:47 @_common.py:69] fc-pi output: [None, 9]\n[1021 20:56:47 @sessinit.py:70] Restoring checkpoint from MsPacman-v0.tfmodel.\n[1021 20:56:47 @sessinit.py:132] WRN Variable fc-v/W:0 in checkpoint not found in the graph!\n[1021 20:56:47 @sessinit.py:132] WRN Variable fc-v/b:0 in checkpoint not found in the graph!\n('Total:', 5780.0)\n[2016-10-21 20:57:39,242] Starting new video recorder writing to /Users/me/PE11/gym/gym-submit/openaigym.video.0.2209.video000001.mp4\n('Total:', 2120.0)\n('Total:', 6300.0)\n('Total:', 5600.0)\n('Total:', 4330.0)\n('Total:', 7520.0)\n('Total:', 5870.0)\n('Total:', 6690.0)\n[2016-10-21 21:03:55,389] Starting new video recorder writing to /Users/me/PE11/gym/gym-submit/openaigym.video.0.2209.video000008.mp4\n('Total:', 5180.0)\n('Total:', 4990.0)\n('Total:', 6690.0)\n('Total:', 4450.0)\n('Total:', 4240.0)\n('Total:', 4990.0)\n('Total:', 5400.0)\n('Total:', 6770.0)\n('Total:', 6040.0)\n('Total:', 4290.0)\n('Total:', 5010.0)\n('Total:', 4990.0)\n('Total:', 6410.0)\n('Total:', 5230.0)\n('Total:', 5290.0)\n('Total:', 4810.0)\n('Total:', 6770.0)\n('Total:', 5590.0)\n('Total:', 7230.0)\n[2016-10-21 21:38:11,856] Starting new video recorder writing to /Users/me/PE11/gym/gym-submit/openaigym.video.0.2209.video000027.mp4\n('Total:', 5710.0)\n('Total:', 5180.0)\n('Total:', 7430.0)\n('Total:', 4970.0)\n('Total:', 7210.0)\n('Total:', 1520.0)\n('Total:', 5190.0)\n('Total:', 7900.0)\n('Total:', 4120.0)\n('Total:', 5610.0)\n('Total:', 8670.0)\n('Total:', 4720.0)\n('Total:', 6010.0)\n('Total:', 4550.0)\n('Total:', 3260.0)\n('Total:', 8050.0)\n('Total:', 4870.0)\n('Total:', 7160.0)\n('Total:', 3480.0)\n('Total:', 7120.0)\n('Total:', 7800.0)\n('Total:', 6190.0)\n('Total:', 6810.0)\n('Total:', 8520.0)\n('Total:', 6180.0)\n('Total:', 1690.0)\n('Total:', 6620.0)\n('Total:', 7160.0)\n('Total:', 1530.0)\n('Total:', 4610.0)\n('Total:', 5680.0)\n('Total:', 7790.0)\n('Total:', 7350.0)\n('Total:', 6910.0)\n('Total:', 6500.0)\n('Total:', 4990.0)\n('Total:', 6870.0)\n[2016-10-21 22:15:13,811] Starting new video recorder writing to /Users/me/PE11/gym/gym-submit/openaigym.video.0.2209.video000064.mp4\n('Total:', 7110.0)\n('Total:', 6730.0)\n('Total:', 8120.0)\n('Total:', 4290.0)\n('Total:', 7180.0)\n('Total:', 5790.0)\n('Total:', 6330.0)\n('Total:', 5880.0)\n('Total:', 4790.0)\n('Total:', 6680.0)\n('Total:', 8100.0)\n('Total:', 6710.0)\n('Total:', 4940.0)\n('Total:', 5760.0)\n('Total:', 6980.0)\n('Total:', 4790.0)\n('Total:', 7120.0)\n('Total:', 4980.0)\n('Total:', 6210.0)\n('Total:', 4410.0)\n('Total:', 6780.0)\n('Total:', 4880.0)\n('Total:', 8680.0)\n('Total:', 4690.0)\n('Total:', 7220.0)\n('Total:', 9290.0)\n('Total:', 4760.0)\n('Total:', 6580.0)\n('Total:', 7520.0)\n('Total:', 7430.0)\n('Total:', 5960.0)\n('Total:', 4930.0)\n('Total:', 7500.0)\n('Total:', 6770.0)\n('Total:', 6970.0)\n('Total:', 6110.0)\nException gym.error.Error: Error('env has been garbage collected. To keep using a monitor, you must keep around a reference to the env object. (HINT: try assigning the env to a variable in your code.)',) in > ignored\nException gym.error.Error: Error('env has been garbage collected. To keep using a monitor, you must keep around a reference to the env object. (HINT: try assigning the env to a variable in your code.)',) in > ignored\n$\n$\n$\n. ",
    "erichuang0771": "Done! All requested modifications are added and also tested : )\n. nvm.... delete please....\n. yes... you need..\n. ",
    "a-maci": "Thanks a lot. I will try out your code.\nClosing this issue for now. \n. Thanks\n. How many GPU cores did you use for training each of these ResNet configurations? \nI am not able to get to the same accuracy level that you mention for ResNet-34 with 2 GPUs.\nIn general, I have found the accuracy (which depends on learning rate schedule) to be quite sensitive to the number of GPUs. Is this your observation as well? Any workaround/tips/tricks you use to get around this issue?\nThanks. I see. \nI had changed the batch size to fit on 2 GPUs. I am getting 0.65% higher error rate than you reported. \n\nOn Jan 23, 2017, at 6:53 PM, Yuxin Wu notifications@github.com wrote:\nThe total batch size is fixed so it shouldn't be very sensitive to the number of GPUs.\nSome of the models are trained with 2 GPUs and some with 4. I couldn't remember which is which.\nWhat's the number you get?\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Thanks!\n\nOn Mon, Oct 31, 2016 at 7:44 PM, Yuxin Wu notifications@github.com wrote:\n\na ResNet-18 model with (W,A,G)=(1,4,32) should get 60% accuracy. But the\ntraining was done in a private framework and was not converted to\ntensorflow.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/ppwwyyxx/tensorpack/issues/40#issuecomment-257477329,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AWDdvzQ2WSGxoDUQLAm7chujfmGvM7Iaks5q5qeHgaJpZM4KluRh\n.\n\n\n-- \nAsit\n. Im going through the resnet-dorefa file. The model is similar but slightly\ndifferent compared to the model in imagenet-resnet file.\nYou mention some bugs in the architecture and workarounds in the code for\nresnet-dorefa. What are these bugs and could you explain little bit on the\nworkaround you have done:\n\nhandling pool1\nexplicit padding\nadding a tf.mul(49)\n\nThanks.\nOn Thu, Jan 5, 2017 at 12:42 AM, Yuxin Wu notifications@github.com wrote:\n\nWe've released the 1,4,32-ResNet18 model on DoReFa-Net http://dorefa.net\npage.\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/ppwwyyxx/tensorpack/issues/40#issuecomment-270591890,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AWDdv1eTpfFF1ftKxtaGlflygwenavXfks5rPK0AgaJpZM4KluRh\n.\n\n\n-- \nAsit\n. I see. Great. \nThanks. \n\nOn Jan 7, 2017, at 6:15 PM, Yuxin Wu notifications@github.com wrote:\nThe training was done not in tensorflow. There are some differences in the definition of Ops and also in architecture, e.g. we forgot to do the average in global average pooling (divide by 7x7) when we trained the model.\nThe current model file is just an equivalent in tensorflow. Ideally a clean model should just remove those hacks.\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub, or mute the thread.\n. protoc is in my path. The error is in compiling the caffe.proto file which I hadnt noticed before.\n\nI get this error message upon compiling:\nprotoc caffe.proto --python_out=.\ncaffe.proto:1:1: Expected top-level statement (e.g. \"message\").\nAny chance they updated the caffe.proto file (the file was last updated 4 months ago). Any chance you could upload your caffe.proto file in this repo?\nNeed help.\n. No, the link ends with \"Will update our parser to ignore BOM\". I dont know if protoc 2.6.1 incorporates this. I downloaded protoc 3.1.0 and I still get the above error. I wonder how it works for other people using protoc 2.6.1.\n. Cool, thanks.\nOne other item:\nWhen I want to print the training error curves I get val error.\ncat ../examples/DoReFa-Net/train_log/alexnet-dorefa/stat.json | jq '.[] | .train_error, .validation_error' | past\ne - - | python plot-point.py --legend 'train,val' --xlabel 'epoch' --ylabel 'error'\nTraceback (most recent call last):\n  File \"plot-point.py\", line 313, in \n    main()\n  File \"plot-point.py\", line 284, in main\n    val = float(val)\nValueError: could not convert string to float: null\n. Thanks.\nI will work on (1) and (3).\nQuestion: How should I include fg(.) and fa(.) in the implementation? E.g. is the below snippet correct (based on cifar10-resnet)?\nEx: \nc1 = Conv2D('conv1', b1, out_channel, stride=stride1, nl=BNReLU)\nc1 = activate(c1) #quantizing activations of conv1\nc1 = fg(c1)           #quantizing gradients for conv1 layer\nc2 = Conv2D('conv2', c1, out_channel)\nc2 = fg(c2)          #quantizing the gradients for conv2 layer, not quantizing the activations for this layer\nFor alexnet-dorefa I see this for example:\n .Conv2D('conv2', 384, 3)\n                .apply(fg)\n                .BatchNorm('bn2')\n                .MaxPooling('pool2', 3, 2, padding='SAME')\n                .apply(activate)\nI dont get how the activations of conv2 alone (before bn2) are quantized. \nDoes the above snippet quantize the output after maxpool? I.e.  CONV-->BN-->MXPOOL->QUANTIZE ?\nI was thinking of quantizing on a per layer basis: CONV->QUANTIZE->BN->QUANTIZE->MXPOOL->QUANTIZE. Is my understanding incorrect?\nThanks!. Thanks. You can close this PR. I will update when I have meaningful results/examples. . The folders downloaded from Imagenet website look something like this:\ntrain/\n    n01440764/ \n        n01440764_10026.JPEG\n        n01440764_10027.JPEG\n        ...\n    n01443537/\n        n01443537_10007.JPEG\n        n01443537_10014.JPEG\n        ...\ntest/\n   ILSVRC2012_test_00000001.JPEG\n   ILSVRC2012_test_00000002.JPEG\n   ...\nval/ \n   n01440764/\n      ILSVRC2012_val_00000293.JPEG\n      ILSVRC2012_val_00002138.JPEG\n      ...\n   n01443537/\n      ILSVRC2012_val_00000236.JPEG\n      ILSVRC2012_val_00000262.JPEG\n      ...\nIn alexnet-dorefa and elsewhere why do you club all the val images into one big folder? Just for convenience or some other reason?\nAlso, do you use the test data sets anywhere in your experiments?\nThanks.\n. I was following this tutorial and I missed one item (https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh), thats why my val datasets were organized in a folder!\n. Thanks. \nWill I have any name-scope clashing if I do and would the weights be shared between these two configs:?\nconfig1 = get_config()\nconfig2 = get_config() # again\nTrainer(config1).train() # max_epoch=15 Trainer(config2).train() #\nThe goal is to train a network with full precision for (say) 15 iterations and then switch to BWN after this. During this process I don't want to save/restore the model weights after the 15th iteration. \nThanks. \n\nOn Dec 29, 2016, at 1:21 AM, Yuxin Wu notifications@github.com wrote:\nIt's already quite easy to do transfer learning, compared to other frameworks. It's just a --load.\nThere are automatic ways within the framework, e.g. you can define both models and use an input to choose which model to use. But this is not easier.\nIf you just don't want to stop in the middle, you can write them together, with something like:\nTrainer(config1).train()  # max_epoch=15\nTrainer(config2).train()  # sess_init=....\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Thanks.. Hi -- I am following the method you described here to restore the inception_resnet_v2 checkpoint but getting some errors. See issue #148 \n\nDid you face any such problems? Were you able to train the network end-to-end?\nThanks. I noticed one thing - ParamRestore and SaverRestore dont output similar warning messages. \nParamRestore prints WRN messages for variables not present in the restored graph but the SaverRestore doesnt.\nWould be helpful if these are both consistent.\n. > One thing to note is that, a lot of variables that're in the checkpoint won't be in the npy dict.\n-- This makes sense.\nI can attach the entire log file for both the experiments if you want.\nHere is what I see when I re-start from a model trained on my machine (ResNet/imagenet-resnet.py with d=34)\nlog-model.log\nArgv: imagenet-resnet.py --gpu 0,1,2,3 --data /tank/imagenet-tensorpack-data --load /home/userID/TF/tensorpack/examples/ResNet/train_log/imagenet-resnet/model-550000 -d 34\n<the input and output tensorshapes messages>\n<applying regularizor messages>\n<filter shapes messages>\n<ModelSaver messages>\n\u001b[32m[0123 17:15:18 @base.py:120]\u001b[0m Initializing graph variables ...\n\u001b[32m[0123 17:15:23 @sessinit.py:82]\u001b[0m Restoring checkpoint from /home/userID/TF/tensorpack/examples/ResNet/train_log/imagenet-resnet/model-550000 ...\n\u001b[32m[0123 17:15:25 @concurrency.py:24]\u001b[0m Starting EnqueueThread\n\u001b[32m[0123 17:15:25 @base.py:139]\u001b[0m Start training with global_step=550000\n\u001b[32m[0123 17:42:18 @timer.py:46]\u001b[0m Epoch 1 (global_step 555000) finished, time:1612.92sec.\nSlightly different run (d=18) but this is what I see when I start with a pre-trained model (npy file) from modelzoo.\nlog-npy.log\nArgv: ttq-modified-imagenet-resnet.py --gpu 4,5,6,7 --data /tank/imagenet-tensorpack-data --load preTrainedModels/ImageNet-ResNet18.npy -d 18\n<the input and output tensorshapes messages>\n<applying regularizor messages>\n<filter shapes messages>\n<ModelSaver messages>\n\u001b[32m[0124 10:44:12 @sessinit.py:169]\u001b[0m Params to restore: group0/block1/conv1/bn/variance/EMA:0, group1/block1/preact/bn/mean/EMA:0, group1/block0/preact/bn/variance/EMA:0, group0/block1/conv1/bn/beta:0, group0/block1/preact/bn/variance/EMA:0, group3/block1/conv1/W:0, group0/block1/conv1/bn/gamma:0, group2/block0/conv1/bn/beta:0, group0/block0/conv1/bn/mean/EMA:0, bnlast/bn/mean/EMA:0, group1/block1/conv2/W:0, group3/block0/conv1/bn/gamma:0, group3/block0/convshortcut/W:0, group3/block1/conv2/W:0, group2/block1/preact/bn/beta:0, group0/block1/preact/bn/gamma:0, group0/block1/preact/bn/beta:0, linear/W:0, group1/block1/conv1/bn/variance/EMA:0, conv0/W:0, group1/block0/convshortcut/W:0, group3/block1/conv1/bn/variance/EMA:0, group2/block1/conv1/bn/variance/EMA:0, group0/block0/conv1/W:0, conv0/bn/gamma:0, group2/block1/preact/bn/variance/EMA:0, group1/block1/conv1/W:0, group2/block1/preact/bn/gamma:0, group0/block0/conv1/bn/variance/EMA:0, group3/block1/preact/bn/mean/EMA:0, group2/block0/convshortcut/W:0, group3/block0/conv1/W:0, bnlast/bn/variance/EMA:0, bnlast/bn/beta:0, group0/block1/conv1/W:0, group0/block1/conv2/W:0, group2/block0/conv1/bn/mean/EMA:0, group2/block1/conv1/W:0, group1/block0/conv1/bn/mean/EMA:0, group3/block0/conv2/W:0, group3/block0/conv1/bn/mean/EMA:0, group0/block0/conv2/W:0, group0/block1/conv1/bn/mean/EMA:0, group2/block0/conv1/bn/variance/EMA:0, group1/block1/preact/bn/variance/EMA:0, group1/block0/conv1/bn/gamma:0, group3/block0/preact/bn/variance/EMA:0, group2/block1/conv2/W:0, group3/block0/preact/bn/beta:0, group3/block0/preact/bn/gamma:0, conv0/bn/beta:0, group3/block1/preact/bn/variance/EMA:0, group2/block0/conv2/W:0, linear/b:0, conv0/bn/variance/EMA:0, group2/block0/preact/bn/variance/EMA:0, group1/block0/preact/bn/beta:0, bnlast/bn/gamma:0, group1/block0/preact/bn/gamma:0, group1/block1/preact/bn/gamma:0, group1/block0/conv1/bn/variance/EMA:0, group3/block0/conv1/bn/variance/EMA:0, group0/block1/preact/bn/mean/EMA:0, group3/block0/conv1/bn/beta:0, group1/block1/conv1/bn/gamma:0, group3/block1/conv1/bn/gamma:0, group2/block0/conv1/bn/gamma:0, group0/block0/conv1/bn/beta:0, group2/block1/conv1/bn/beta:0, group0/block0/conv1/bn/gamma:0, conv0/bn/mean/EMA:0, group1/block0/preact/bn/mean/EMA:0, group1/block1/preact/bn/beta:0, group1/block0/conv2/W:0, group2/block0/preact/bn/beta:0, group1/block1/conv1/bn/beta:0, group2/block1/conv1/bn/gamma:0, group2/block0/conv1/W:0, group2/block0/preact/bn/gamma:0, group3/block1/conv1/bn/mean/EMA:0, group2/block1/conv1/bn/mean/EMA:0, group1/block1/conv1/bn/mean/EMA:0, group3/block1/preact/bn/beta:0, group1/block0/conv1/W:0, group2/block1/preact/bn/mean/EMA:0, group2/block0/preact/bn/mean/EMA:0, group3/block1/conv1/bn/beta:0, group3/block0/preact/bn/mean/EMA:0, group3/block1/preact/bn/gamma:0, group1/block0/conv1/bn/beta:0\n\u001b[32m[0124 10:44:12 @sessinit.py:172]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable cost/EMA/biased:0 in the graph not found in the dict!\n\u001b[32m[0124 10:44:12 @sessinit.py:172]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable cost/EMA/local_step:0 in the graph not found in the dict!\n\u001b[32m[0124 10:44:12 @sessinit.py:172]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable cost/EMA:0 in the graph not found in the dict!\n\u001b[32m[0124 10:44:12 @sessinit.py:172]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable global_step:0 in the graph not found in the dict!\n\u001b[32m[0124 10:44:12 @sessinit.py:172]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group0/block0/conv1/WR:0 in the graph not found in the dict!\n\u001b[32m[0124 10:44:12 @sessinit.py:172]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group0/block0/conv2/WR:0 in the graph not found in the dict!\n\u001b[32m[0124 10:44:12 @sessinit.py:172]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group0/block1/conv1/WR:0 in the graph not found in the dict!\n\u001b[32m[0124 10:44:12 @sessinit.py:172]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group0/block1/conv2/WR:0 in the graph not found in the dict!\n\u001b[32m[0124 10:44:12 @sessinit.py:172]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group1/block0/conv1/WR:0 in the graph not found in the dict!\n...\n\u001b[32m[0124 10:44:12 @sessinit.py:172]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable input_queue_size/EMA/biased:0 in the graph not found in the dict!\n\u001b[32m[0124 10:44:12 @sessinit.py:172]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable input_queue_size/EMA/local_step:0 in the graph not found in the dict!\n\u001b[32m[0124 10:44:12 @sessinit.py:172]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable input_queue_size/EMA:0 in the graph not found in the dict!\n\u001b[32m[0124 10:44:12 @sessinit.py:172]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable l2_regularize_loss/EMA/biased:0 in the graph not found in the dict!\n\u001b[32m[0124 10:44:12 @sessinit.py:172]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable l2_regularize_loss/EMA/local_step:0 in the graph not found in the dict!\n...\n\u001b[32m[0124 10:44:12 @sessinit.py:179]\u001b[0m Restoring from dict ...\n\u001b[32m[0124 10:44:19 @concurrency.py:24]\u001b[0m Starting EnqueueThread\n\u001b[32m[0124 10:44:19 @base.py:139]\u001b[0m Start training with global_step=0\n\u001b[32m[0124 11:08:37 @timer.py:46]\u001b[0m Epoch 1 (global_step 5000) finished, time:1458.37sec.\nThe variables WR are some constant multipliers I added to the model in each experiment. But as you see the run with .npy loading prints it out and the one with model restore doesnt. \n. Ignore the ANSI sequences for color above.. I also noticed errors when running examples that use RNN/LSTM apis.\nFor instance, for PTB-LSTM example I am getting \nFile \"/home/user/TF/tpNew/tensorpack/examples/PennTreebank/reader.py\", line 118, in ptb_producer\n    [batch_size, (i + 1) * num_steps])\nTypeError: strided_slice() takes at least 4 arguments (3 given)\nI saw that the TF APIs related to RNNs/LSTMs changed recently. Are these errors related to these API changes?. I updated to TF 1.0. Still facing some difficulties with the PTB example.\nGetting this error now \n/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.py:2889: RuntimeWarning: Mean of empty slice.\n  out=out, **kwargs)\nTraceback (most recent call last):\n  File \"./PTB-LSTM.py\", line 158, in <module>\n    SimpleFeedfreeTrainer(config).train()\n  File \"/home/userID/TF/tensorpack/tensorpack/train/base.py\", line 65, in train\n    self.main_loop()\n  File \"/home/userID/TF/tensorpack/tensorpack/train/base.py\", line 188, in main_loop\n    self.trigger_epoch()\n  File \"/home/userID/TF/tensorpack/tensorpack/train/base.py\", line 87, in trigger_epoch\n    self.config.callbacks.trigger_epoch()\n  File \"/home/userID/TF/tensorpack/tensorpack/callbacks/base.py\", line 98, in trigger_epoch\n    self._trigger_epoch()\n  File \"/home/userID/TF/tensorpack/tensorpack/callbacks/group.py\", line 124, in _trigger_epoch\n    cb.trigger_epoch()\n  File \"/home/userID/TF/tensorpack/tensorpack/callbacks/base.py\", line 98, in trigger_epoch\n    self._trigger_epoch()\n  File \"/home/userID/TF/tensorpack/tensorpack/callbacks/base.py\", line 155, in _trigger_epoch\n    self.trigger()\n  File \"/home/userID/TF/tensorpack/tensorpack/callbacks/base.py\", line 147, in trigger\n    self._trigger()\n  File \"/home/userID/TF/tensorpack/tensorpack/callbacks/inference_runner.py\", line 245, in _trigger\n    self._write_summary_after_inference()\n  File \"/home/userID/TF/tensorpack/tensorpack/callbacks/inference_runner.py\", line 248, in _write_summary_after_inference\n    summary_inferencer(self.trainer, self.infs)\n  File \"/home/userID/TF/tensorpack/tensorpack/callbacks/inference_runner.py\", line 56, in summary_inferencer\n    ret = inf.after_inference()\n  File \"/home/userID/TF/tensorpack/tensorpack/callbacks/inference.py\", line 52, in after_inference\n    return self._after_inference()\n  File \"/home/userID/TF/tensorpack/tensorpack/callbacks/inference.py\", line 101, in _after_inference\n    assert len(self.stats) == len(self.names)\nTypeError: object of type 'numpy.float64' has no len(). BTW, the char-rnn example works fine for me with TF update to 1.0. Yes, that was mistake at my end. The dataset hadnt downloaded properly.. Is this still a problem or you have resolved this? If you do want to checkout an earlier version of resnet for TF 12.1 do you recollect which git commit number to checkout?\nThanks. This problem doesnt seem to be fixed. I am working with TF 1.0.0rc1 and HEAD version of this repo. \nHere is what I am getting after first epoch:\n236 ^[[32m[0227 18:27:56 @base.py:160]^[[0m Start Epoch 1 ...\n 237 ^[[32m[0227 18:48:56 @base.py:168]^[[0m Epoch 1 (global_step 5000) finished, time:1259.80 sec.\n 238 ^[[32m[0227 18:48:57 @saver.py:65]^[[0m Model saved to resnet34BaselineTrain/model-5000.\n 239 ^[[32m[0227 18:54:41 @group.py:43]^[[0m Callbacks took 345.025 sec in total. InferenceRunner: 343.555sec\n 240 ^[[32m[0227 18:54:41 @monitor.py:233]^[[0m input_queue_size: 50\n 241 ^[[32m[0227 18:54:41 @monitor.py:233]^[[0m l2_regularize_loss: 0.48794\n 242 ^[[32m[0227 18:54:41 @monitor.py:233]^[[0m learning_rate: 0.1\n 243 ^[[32m[0227 18:54:41 @monitor.py:233]^[[0m train-error-top1: 0.96621\n 244 ^[[32m[0227 18:54:41 @monitor.py:233]^[[0m train-error-top5: 0.87891\n 245 ^[[32m[0227 18:54:41 @monitor.py:233]^[[0m val-error-top1: 0.96124\n 246 ^[[32m[0227 18:54:41 @monitor.py:233]^[[0m val-error-top5: 0.87464\n 247 ^[[32m[0227 18:54:41 @monitor.py:233]^[[0m xentropy-loss: 5.6556\nThe final error is also off: \n1548 ^[[32m[0301 18:02:54 @base.py:160]^[[0m Start Epoch 110 ...\n1549 ^[[32m[0301 18:23:36 @base.py:168]^[[0m Epoch 110 (global_step 550000) finished, time:1241.72 sec.\n1550 ^[[32m[0301 18:23:37 @saver.py:65]^[[0m Model saved to resnet34BaselineTrain/model-550000.\n1551 ^[[32m[0301 18:29:02 @group.py:43]^[[0m Callbacks took 326.282 sec in total. InferenceRunner: 325.909sec\n1552 ^[[32m[0301 18:29:02 @monitor.py:233]^[[0m input_queue_size: 50\n1553 ^[[32m[0301 18:29:02 @monitor.py:233]^[[0m l2_regularize_loss: 0.55047\n1554 ^[[32m[0301 18:29:02 @monitor.py:233]^[[0m learning_rate: 1e-05\n1555 ^[[32m[0301 18:29:02 @monitor.py:233]^[[0m train-error-top1: 0.33629\n1556 ^[[32m[0301 18:29:02 @monitor.py:233]^[[0m train-error-top5: 0.1454\n1557 ^[[32m[0301 18:29:02 @monitor.py:233]^[[0m val-error-top1: 0.3068\n1558 ^[[32m[0301 18:29:02 @monitor.py:233]^[[0m val-error-top5: 0.11092\n1559 ^[[32m[0301 18:29:02 @monitor.py:233]^[[0m xentropy-loss: 1.414\n1560 ^[[32m[0301 18:29:02 @input_data.py:124]^[[0m EnqueueThread Exited.\nWhen I had done this run a while back using TF 0.12 this is what I was getting after the first epoch:\n254 ^[[32m[1227 12:08:31 @timer.py:46]^[[0m Epoch 1 (global_step 5000) finished, time:1432.42sec.\n 255 ^[[32m[1227 12:16:37 @stats.py:83]^[[0m cost: 4.6151\n 256 ^[[32m[1227 12:16:37 @stats.py:83]^[[0m input_queue_size: 48.462\n 257 ^[[32m[1227 12:16:37 @stats.py:83]^[[0m l2_regularize_loss: 0.62589\n 258 ^[[32m[1227 12:16:37 @stats.py:83]^[[0m learning_rate: 0.1\n 259 ^[[32m[1227 12:16:37 @stats.py:83]^[[0m train-error-top1: 0.80657\n 260 ^[[32m[1227 12:16:37 @stats.py:83]^[[0m train-error-top5: 0.55757\n 261 ^[[32m[1227 12:16:37 @stats.py:83]^[[0m val-error-top1: 0.76442\n 262 ^[[32m[1227 12:16:37 @stats.py:83]^[[0m val-error-top5: 0.5148\n 263 ^[[32m[1227 12:16:37 @stats.py:83]^[[0m xentropy-loss: 3.9892\n. Using tensorflow-gpu (1.0.0) and still getting val-error-top1: 0.98914 after Epoch 1.. I see. What's the solution for now? Use TF 0.12?\n\nOn Mar 2, 2017, at 9:05 PM, Yuxin Wu notifications@github.com wrote:\nSorry, looking at the comment in tensorflow/tensorflow#7038, it looks like 1.0 is still not new enough.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. I updated to TF 1.0.0.\nGetting a different error this time.\n\nTraceback (most recent call last):\n  File \"tpInception_resnet_v2.py\", line 204, in <module>\n    SyncMultiGPUTrainer(config).train()\n  File \"/home/akm/TF/tpNew/tensorpack/tensorpack/train/base.py\", line 64, in train\n    self.setup()\n  File \"/home/akm/TF/tpNew/tensorpack/tensorpack/train/base.py\", line 130, in setup\n    self._setup()   # subclass will setup the graph\n  File \"/home/akm/TF/tpNew/tensorpack/tensorpack/train/multigpu.py\", line 133, in _setup\n    self.config.tower, lambda: self._get_cost_and_grad()[1])\n  File \"/home/akm/TF/tpNew/tensorpack/tensorpack/train/multigpu.py\", line 41, in _multi_tower_grads\n    grad_list.append(get_tower_grad_func())\n  File \"/home/akm/TF/tpNew/tensorpack/tensorpack/train/multigpu.py\", line 133, in <lambda>\n    self.config.tower, lambda: self._get_cost_and_grad()[1])\n  File \"/home/akm/TF/tpNew/tensorpack/tensorpack/train/feedfree.py\", line 54, in _get_cost_and_grad\n    self.build_train_tower()\n  File \"/home/akm/TF/tpNew/tensorpack/tensorpack/train/feedfree.py\", line 43, in build_train_tower\n    f()\n  File \"/home/akm/TF/tpNew/tensorpack/tensorpack/train/feedfree.py\", line 36, in f\n    self.model.build_graph(inputs)\n  File \"/home/akm/TF/tpNew/tensorpack/tensorpack/models/model_desc.py\", line 113, in build_graph\n    self._build_graph(model_inputs)\n  File \"tpInception_resnet_v2.py\", line 53, in _build_graph\n    logits, end_points = inception_resnet_v2(image, is_training=is_training)\n  File \"/home/akm/TF/tpNew/tensorpack/examples/IRv2/inception_resnet_v2.py\", line 169, in inception_resnet_v2\n    net = tf.concat(3, [tower_conv, tower_conv1_1, tower_conv2_2, tower_pool_1])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 1047, in concat\n    dtype=dtypes.int32).get_shape(\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 651, in convert_to_tensor\n    as_ref=False)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 716, in internal_convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py\", line 176, in _constant_tensor_conversion_function\n    return constant(v, dtype=dtype, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py\", line 165, in constant\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py\", line 367, in make_tensor_proto\n    _AssertCompatible(values, dtype)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py\", line 302, in _AssertCompatible\n    (dtype.name, repr(mismatch), type(mismatch).__name__))\nTypeError: Expected int32, got list containing Tensors of type '_Message' instead.\nPrefetch process exited.\nPrefetch process exited.\nI guess something to do with the concat API. I did minor mod to the file. \n. Thanks. \nHow does one remove a variable from a checkpoint? Simply delete that line or its more involved?\n\nOn Feb 14, 2017, at 8:40 PM, Yuxin Wu notifications@github.com wrote:\nThe second problem is because TF 1.0 changes API of tf.concat. You should swap the argument order of tf.concat in inception_resnet_v2.py.\nThe first is because the checkpoint contains an unused variable \"global_step:0\" which happens to conflict with a variable tensorpack defined. In general there is no good way to solve this and it's best to remove unused variables from a checkpoint. But I'll push a change later to make it behave less strict in this case (print a warning instead of crash).\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. I am able to load the checkpoint. I see WRN messages like this for every variable in the graph model:\n\n```\n...\n...\nVariable InceptionResnetV2/Conv2d_1a_3x3/weights in the graph not found in checkpoint!\nVariable InceptionResnetV2/Conv2d_1a_3x3/BatchNorm/beta in the graph not found in checkpoint!\nVariable InceptionResnetV2/Conv2d_1a_3x3/BatchNorm/moving_mean in the graph not found in checkpoint!\nVariable InceptionResnetV2/Conv2d_1a_3x3/BatchNorm/moving_variance in the graph not found in checkpoint!\nVariable InceptionResnetV2/Conv2d_2a_3x3/weights in the graph not found in checkpoint!\nVariable InceptionResnetV2/Conv2d_2a_3x3/BatchNorm/beta in the graph not found in checkpoint!\nVariable InceptionResnetV2/Conv2d_2a_3x3/BatchNorm/moving_mean in the graph not found in checkpoint!\nVariable InceptionResnetV2/Conv2d_2a_3x3/BatchNorm/moving_variance in the graph not found in checkpoint!\n...\n...\nVariable InceptionResnetV2/Repeat_2/block8_9/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_mean in checkpoint not found in the graph!\nVariable InceptionResnetV2/Repeat_2/block8_9/Branch_1/Conv2d_0b_1x3/BatchNorm/moving_variance in checkpoint not found in the graph!\nVariable InceptionResnetV2/Repeat_2/block8_9/Branch_1/Conv2d_0b_1x3/weights in checkpoint not found in the graph!\nVariable InceptionResnetV2/Repeat_2/block8_9/Branch_1/Conv2d_0c_3x1/BatchNorm/beta in checkpoint not found in the graph!\nVariable InceptionResnetV2/Repeat_2/block8_9/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_mean in checkpoint not found in the graph!\nVariable InceptionResnetV2/Repeat_2/block8_9/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_variance in checkpoint not found in the graph!\nVariable InceptionResnetV2/Repeat_2/block8_9/Branch_1/Conv2d_0c_3x1/weights in checkpoint not found in the graph!\nVariable InceptionResnetV2/Repeat_2/block8_9/Conv2d_1x1/biases in checkpoint not found in the graph!\nVariable InceptionResnetV2/Repeat_2/block8_9/Conv2d_1x1/weights in checkpoint not found in the graph!\nVariable global_step in checkpoint not found in the graph!\n...\n```\nIs this the right behavior? I dont know if the checkpoint variables are being loaded/restored properly. Could you comment?\nAttached is the log file till the point the epochs start. \nlog.log.txt\n. Thanks a lot for looking into this and helping out. \n\nOn Feb 15, 2017, at 11:15 PM, Yuxin Wu notifications@github.com wrote:\nOne thing that may cause some training slow-down: google's training code handles the batch norm updates manually, by only using UPDATE_OPS from the first GPU.\nWith tensorpack models this was done automatically.\nBut with slim models, currently UPDATE_OPS from all GPUs will be executed. This was from PR #81. Now it looks like \"applying the UPDATE_OPS blindly\" is not always what a user would want. @PatWie for comments.\nOne possible solution is to use slim.arg_scope to change the updates_collections option of slim.batch_norm. But I'm not familiar with slim so I'm not sure does it work.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. What is the batch size that you are using and how many GPUs? . Closing this.\n\n\n\nIf you could look into why the checkpoint loading is slow that would be helpful. This is a performance aspect not functionality. I am able to get the experiment running.\n\n\nI had to change the num_classes to 1001 (line 94/95) to get this running. I thought imagenet has 1000 classes. The train folder has 1000 folders. Where is the additional 1 class/folder?\n\n\n. @ppwwyyxx Did you continue the training process for this network?\nI was re-training, starting from the checkpoint and (1) the process is very slow 0.70it/sec on 8 GPUs and (2) the error rate is quite bad - 0.98 top1 and 0.84 top5 after 2 epochs. \nThe only change to the code I did from the ones I attached last was to change 1000 to 1001 . Learning rate is 1e-6.\nAny idea what is going on?\n. \nI went with your advice of doing an eval on the model. The eval is giving me very bad results: Top1 Error: 0.98718, Top5 Error: 0.83158\nI dont know what is going wrong and where. I added the BGR to RGB and follow preprocessing items that tf/slim does but its not helping.\nSome related links: \nPreprocessing: https://github.com/tensorflow/models/blob/master/slim/preprocessing/inception_preprocessing.py#L237-L275\nOther efforts trying to do eval on IRv2: https://github.com/kentsommer/keras-inception-resnetV2\nThis dicussion also mentions something is wrong somewhere: https://github.com/kentsommer/keras-inception-resnetV2/issues/1\nCould you give this code a try when you get a chance. Thanks. I see now what you are saying. What should I do to fix this?. No, I didnt modify. Here is what it is:\nTOTAL_BATCH_SIZE = 512\nNR_GPU = 8\nBATCH_SIZE = TOTAL_BATCH_SIZE // NR_GPU\nINPUT_SHAPE = 299. I see. \nBut based on the above code, TOTAL_BATCH_SIZE is hard-coded and it is 512. BATCH_SIZE in turn is calculated based on total batch size and nr_gpus. \n. I see. Thanks.. Thanks.. Got it.\n. So, method 2 is correct then?\n\nOn Jul 29, 2017, at 11:00 AM, Yuxin Wu notifications@github.com wrote:\nYour method summarizes gradients w.r.t layer outputs, not variables.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Actually I need gradient stats for grads wrt variables(weights) and the grads that get back-propagated to the previous layer. \n\nGoal is to look at both the grad stats - grads that are involved with weight update and the grad stats that get back-prop'ed to the previous layer (to update weights in that layer). \nAny suggestions?\n\nOn Jul 29, 2017, at 11:00 AM, Yuxin Wu notifications@github.com wrote:\nYour method summarizes gradients w.r.t layer outputs, not variables.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Ok. Got it. \nOn Jul 29, 2017, at 11:28 AM, Yuxin Wu notifications@github.com wrote:\nThen just summarize both...what's the question?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. I see. \nWhy is this transformation not done for other networks in the examples - alexnet, VGG, inception?. Got it.. Is there a way to dump the gradient tensors for a batch of inputs?. Could you give an example? That would be helpful.\nSay, in the tensorpack imagenet-resnet.py example, what should I do to dump the two gradients (wrt input and wrt weights) for CONV2 layer?\n. Also, I was using DumpTensor in my callbacks. \nI put DumpTensor('conv0/output:0'), in callbacks but I get error:\nKeyError: \"The name 'c:0' refers to a Tensor which does not exist. The operation, 'c', does not exist in the graph.\"\n\nWhat should I put for the name of a tensor? I was trying this on mnist-convnet.py example.\nThanks\n. Thanks. I had somehow missed the mnist-vis example. Its a nice example to access the internal variables. I also saw you updated the docs to mention these. Thanks again.. One (stupid) question:\nHow did you figure out the string for the gradient tensor? For example this string -- gradients/conv1/Conv2D_grad/Conv2DBackpropInput\nI see such string in the events file in the log directory but for multi-gpu training they have \"towerXX\" in front of them. Is there a way to print the aggregate gradients?\nHere is what I have in the callbacks and I am getting errors:\nDumpTensor(['gradients/group2/block2/conv2/Conv2D_grad/Conv2DBackpropInput:0', 'gradients/group2/block2/conv2/Conv2D_grad/Conv2DBackpropFilter:0']),\nDumpTensor(['group2/block2/conv2/output:0']),\nKeyError: \"The name 'gradients/group2/block2/conv2/Conv2D_grad/Conv2DBackpropInput:0' refers to a Tensor which does not exist. The operation, 'gradients/group2/block2/conv2/Conv2D_grad/Conv2DBackpropInput', does not exist in the graph.\"\nThe error is produced even when I run with a very small batch size on 1 gpu.. Yes, got it.. Got it.\n. Is there no such thing as tower0/bnorm0/output:0, ie bnorm for tower0? \nThis is with respect to resnet topology (very first layer there).\nWhen I dump tower0/bnorm0/output:0 i get an error but not when I print tower1/bnorm0/output:0.. Ok, so its called InferenceTower instead of tower0.\nOne question to understand the framework: Each tower has a copy of the model and gets some number of inputs (=TotalBatchSize/NumGPUs). During inference or when checkpointing, do you store the average of all the model tensors across the towers?. Actually, no, its called tower0/bnorm0/FusedBatchnorm:0 for some reason.. > DumpTensors callback runs in every training step \n\nYes. That\u2019s my mistake. \n\nAny option to print tensor ops outputs during inference phase?\n\nOn Feb 20, 2018, at 5:10 PM, Yuxin Wu notifications@github.com wrote:\nDumpTensors callback runs in every training step so I don't see why it makes sense to evaluate inference tensors in it. That's up to you, though.\nThe tensor name is supposed to end with \"output:0\". Getting \"FusedBatchnorm:0\" is a bug. I'll fix it soon.\ndo you store the average of all the model tensors across the towers?\nNo. And as I said tensors on different towers, though have similar names, are not necessarily of the same shape at all.\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub, or mute the thread.\n. \n",
    "pawelsubko": "\nThat frequently? Just um... (it's complicated) sending some data to the server. Sending it every step is not the best solution.\n\nI. Ok, I may completely miss the point. Could you help me on the following problem:\nWhat is a best way to compute loss on a given datapoint, then run train_op on this datapoint? (otherwise it's a waste!)\nII. The way I \"normally\" do is I benchmark the whole loop body including the data_generator.next() call and all the other calls, and separately the train step only to get reliable (?) benchmark whether GPU train_op or some other calls/loading data/saving etc. is the bottleneck. (Last time inefficiently written Augmentation (?) caused 1s whole loop vs 450ms train_op, the problem may be somewhere else tho)\nby run_other_ops (that's what should have been) I meant computing predictions on current datapoint, or some sample of train set or anything else you want to compute or do (like saving) while training.\n. I. Given a datapoint I want to compute the loss of my model on this datapoint and make sure it is computed before the model was trained on this datapoint (in this epoch at least). Then, since the datapoint is already in RAM, I want to use it for training.\nII. I like this way of benchmarking dataloading and graph. I find one major problem tho:\nIt is not real time while training (might be important for many reasons for a cluster with multi-GPU setup where resources are shared - it's a real case scenario and I need to know if my 7 days working task is lagging for some reason). What is more when you experiment and change something in the graph, or add some augmentation should I run the tests again? Therefore at least some timing of body loop is a must. Ideally you should have also timer only for sess.run([train_op]) and timers should be normalized: time/batch_size (no matter what batch_size you get the same number).\nAgain - I may define shorter epoch (full epoch may last up to 12 hours in some experiments), but it becomes quite painful and not a very convenient workaround.\n. I. Great.\nII. I get what you mean. These statistics (relative time of occupancy or something like that) would be great. Although, the time of backpropagation alone (not matter if it's waiting for data or not) may still give you an Idea of how long other Callbacks take (sometimes it's as long as the backpropagation step alone).\nRegarding the design I'm not saying it's particularly bad but now to use my own timer (it sends statistics somewhere) I have to override the context manager - not sure if that's what's desirable.\n. I. That's great.\nII. Not necessary. I just found benchmarking on the fly any given parts of the pipeline useful.\nIII. I'll definitely use a modified version of it. thanks.. ",
    "weiHelloWorld": "OK, thanks!\n. Great, thank you!\n. ",
    "fferreres": "Awesome, thanks for the quick reply and the good news!\n. ",
    "KeyKy": "Thank!\n. ",
    "bigcat77": "Yes, it now runs thanks. By the way efbf256 didn't work either but efc74f2 did.\nThanks again.\n. I had a screwup in converting from gray-scale to RGB. \nThanks for the help.. ",
    "KaixiangLin": "If I restore from pretrained model, by this \nENV=Breakout-v0; ./run-atari.py --load \"$ENV\".tfmodel --env \"$ENV\" --episode 100 --output output_dir1\n it works well. The output is as follows. I didn't figure out why training from scratch doesn't work yet, it may be my problem...\n[1113 14:31:57 @sessinit.py:137] WRN Variable fc-v/W in checkpoint not found in the graph!\n[1113 14:31:57 @sessinit.py:137] WRN Variable fc-v/b in checkpoint not found in the graph!\n[1113 14:31:57 @run-atari.py:67] Start evaluation: \n('Total:', 20.0)\n[2016-11-13 14:32:14,458] Starting new video recorder writing to /output_dir1/openaigym.video.0.22993.video000001.mp4\n('Total:', 20.0)\n('Total:', 21.0)\n('Total:', 21.0)\n('Total:', 20.0)\n('Total:', 20.0)\n('Total:', 20.0)\n('Total:', 21.0)\n[2016-11-13 14:33:46,057] Starting new video recorder writing to /output_dir1/openaigym.video.0.22993.video000008.mp4\n. ",
    "jalfred479": "Right, I had been using the A3C example in OpenAIGym and hadn't used the --gpu argument, but got the following:\nTraceback (most recent call last):\n  File \"train-atari.py\", line 245, in <module>\n    nr_gpu = get_nr_gpu()\n  File \"/home/jalfred/Documents/tensorpack/tensorpack/utils/gpu.py\", line 19, in get_nr_gpu\n    assert env is not None  # TODO\nAssertionError\n. I just saw that now, thank you.\n. ",
    "thadpasce16": "@Junsong-Wang Were you able to get this working?\nHere is how I am implementing the TWN ternarization but I am not getting results.\n        shape             = x.get_shape()\n        weightAbs         = tf.stop_gradient(tf.abs(x))\n\n        threshold         = tf.stop_gradient(tf.reduce_mean(weightAbs) * 0.7)\n\n        absW_gt_threshold = tf.select( weightAbs > threshold, weightAbs, tf.zeros(shape))\n        nnz                           = tf.cast(tf.count_nonzero(absW_gt_threshold), dtype=tf.float32)\n        weightScale             = tf.stop_gradient(tf.reduce_sum(absW_gt_threshold)/(nnz))\n\n        weights_p         = tf.select( x >  threshold, tf.ones(shape) *  weightScale, tf.zeros(shape))\n        weights_n         = tf.select( x < -threshold, tf.ones(shape) * -weightScale, weights_p)\n\n        mask_z            = tf.select( (x < threshold) & (x > -threshold), tf.zeros(shape), tf.ones(shape))\n        with G.gradient_override_map({\"Sign\":\"Identity\", \"Mul\": \"Add\"}):\n            w = tf.sign(x) * tf.stop_gradient(mask_z)\n        w = w * weights_n\n\n        return w\n\n. (Newbie) question for clarification: why dont you have stop_gradient in the first line when calculating delta? As in \ndelta =  tf.stop_gradient(0.7 * tf.reduce_mean(tf.abs(x)))\nThanks to both of you for sharing your code. Its quite helpful.. The authors recently open sourced their implementation here: https://github.com/czhu95/ternarynet\nIt is based on tensorpack!\nI have a version but if you want to replicate the numbers in the paper it's better to go with their official version. \n\nOn Jan 19, 2017, at 10:41 PM, Yuxin Wu notifications@github.com wrote:\n@thadpasce16 has an implementation. I haven't got time to test it but I was told it had good results.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Thanks.\n\nIs there a way to specify the logdir to store the results/log files/snapshots etc rather than have the script pick up the log dir name itself?\nIf the user doesnt specify a logdir then default to the behavior currently implemented. If user specifies a logdir then use that.. Thanks.. I think I found it. Its already in the log file.. No, sorry. I didnt work on this. I was setting up for training rather than\nresuming from a checkpoint.\nI couldnt get the training process to work back then.\nOn Tue, Feb 14, 2017 at 12:48 PM, a-maci notifications@github.com wrote:\n\nHi -- I am following the method you described here to restore the\ninception_resnet_v2 checkpoint but getting some errors. See issue #148\nhttps://github.com/ppwwyyxx/tensorpack/issues/148\nDid you face any such problems? Were you able to train the network\nend-to-end?\nThanks\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/ppwwyyxx/tensorpack/issues/111#issuecomment-279831190,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AXspSaHmMg4b0_ZnSst5usZNkEHZIkyNks5rchMwgaJpZM4Lml0s\n.\n. I changed the BasicLSTMCell to LSTMBlockCell and it no longer complains. But I dont know what to replace for the MultiRNNCell.\nIt seems the TF API has changed.\n . Apologies for churn. I was able to make the code run by making these changes:\n\ncell = tf.nn.rnn_cell.BasicLSTMCell(num_units=param.rnn_size)\ncell = tf.nn.rnn_cell.MultiRNNCell([cell] * param.num_rnn_layer)\noutputs, last_state = tf.nn.rnn(cell, input_list, initial, scope='rnnlm')\nI am using TF version 0.12\nAfter training the model, how do I generate text? I am using this command line \npython char-rnn.py --gpu 0,1 --load train_log/char-rnn/model-XXXX sample\nand getting errors.. Error that I get is \nTraceback (most recent call last):\n  File \"./char-rnn.py\", line 191, in <module>\n    sample(args.load, args.start, args.num)\n  File \"./char-rnn.py\", line 149, in sample\n    state = model.initial.eval({input_vars[0]: dummy_input})\nAttributeError: 'tuple' object has no attribute 'eval'. You are correct. \nI re-checked and its a mistake on my part. I was comparing accuracies across different resnet depth runs.. The new one does this tf.transpose(image, [0, 3, 1, 2]) transformation. The old version didnt and the model was produced without this transformation. \nIn the new version, when reading the old model, is this transformation handled somewhere internally now?. ",
    "ghost": "Thank you for helping!\nI first did :  # git clone https://github.com/ppwwyyxx/tensorpack.git\nthen, export PYTHONPATH=$PYTHONPATH:readlink -f path/to/tensorpack , where I substituted path/to/tensorpack with/my/path/tensorpack\nIs it right?. Yes, you are absolutely right. I am learning how to do it right. I see what are you saying. Could you show me how to do it, please?. it looks like i am on the right way, if I do echo $PYTHONPATH it gives me :/ABC/DEF/tensorpack . But something is still wrong.. thank you, I think I went over it:) . Hi again,\nI made new version of Ubuntu14.04 on VB and I still not able to run the code. Here, i would like to show my path to tensorpack and PYTHONPATH :\nosboxes@osboxes:/opt/tensorpack/tensorpack$ ls -l\ntotal 40\ndrwxr-xr-x 2 root root 4096 Nov 26 03:14 callbacks\ndrwxr-xr-x 4 root root 4096 Nov 26 03:14 dataflow\n-rw-r--r-- 1 root root  578 Nov 26 03:14 init.py\ndrwxr-xr-x 2 root root 4096 Nov 26 03:14 models\ndrwxr-xr-x 2 root root 4096 Nov 26 03:14 predict\n-rw-r--r-- 1 root root  453 Nov 26 03:14 README.md\ndrwxr-xr-x 2 root root 4096 Nov 26 03:14 RL\ndrwxr-xr-x 2 root root 4096 Nov 26 03:14 tfutils\ndrwxr-xr-x 2 root root 4096 Nov 26 03:14 train\ndrwxr-xr-x 2 root root 4096 Nov 26 03:14 utils\nosboxes@osboxes:/opt/tensorpack/tensorpack$ echo $PYTHONPATH\n:/opt/tensorpack/tensorpack\nosboxes@osboxes:/opt/tensorpack/tensorpack$ . Thank you so much it helped. Then, I tried to reproduce the atari game on openAIGym it complained that gym is not installed. I installed but know I can not activate tensorflow environment wiht source ~/tensorflow/bin/activate . I tried to run without it and got the following : \nosboxes@osboxes:/opt/tensorpack/openAIgames-fedorov$ ./train-atari.py --env Breakout-v0 --gpu 0\n[1126 06:06:17 @bsds500.py:18] WRN Cannot import scipy. BSDS500 dataset won't be available!\n[1126 06:06:17 @svhn.py:18] WRN Cannot import scipy. SVHNDigit dataset won't be available!\n[1126 06:06:17 @format.py:16] WRN Error in 'import h5py'. HDF5Data won't be available.\n[1126 06:06:17 @format.py:24] WRN Error in 'import lmdb'. LMDBData won't be available.\n[1126 06:06:17 @format.py:31] WRN Error in 'import sklearn'. SVMLightData won't be available.\n[1126 06:06:18 @concurrency.py:24] WRN Cannot import Future in tornado.concurrent. MultiThreadAsyncPredictor won't be available.\nTraceback (most recent call last):\n  File \"./train-atari.py\", line 25, in \n    import common\n  File \"/opt/tensorpack/openAIgames-fedorov/common.py\", line 1\n    ../Atari2600/common.py\n    ^\nSyntaxError: invalid syntax\nosboxes@osboxes:/opt/tensor. I am using Ubuntu14.04 on VB which is installed on Windows10. But I use usb to transfer files from windows to ubuntu. May it affect it? Could you recommend something to resolve it?. It moved further but now it is OSError: [Errno 13] Permission denied: 'train_log'\nosboxes@osboxes:/opt/tensorpack/examples/Atari2600$ ls -l\ntotal 148\n-rw-r--r-- 1 root root   7255 Nov 26 07:41 atari.py\n-rw-r--r-- 1 root root   5972 Nov 26 07:41 breakout.jpg\n-rw-r--r-- 1 root root   3456 Nov 26 07:41 common.py\n-rw-r--r-- 1 root root 111444 Nov 26 07:41 curve-breakout.png\n-rwxr-xr-x 1 root root   8202 Nov 26 07:41 DQN.py\n-rw-r--r-- 1 root root   1714 Nov 26 07:41 README.md\nosboxes@osboxes:/opt/tensorpack/examples/OpenAIGym$ ls -l\ntotal 26500\n-rw-rw-r-- 1 osboxes osboxes 13561375 Nov 26 07:56 Asteroids-v0.tfmodel\n-rw-rw-r-- 1 osboxes osboxes 13544955 Nov 26 07:56 Breakout-v0.tfmodel\n-rw-r--r-- 1 root    root        3457 Nov 26 08:14 common.py\n-rw-r--r-- 1 root    root        4659 Nov 26 07:41 README.md\n-rwxr-xr-x 1 root    root        3278 Nov 26 07:41 run-atari.py\n-rwxr-xr-x 1 root    root        9580 Nov 26 07:41 train-atari.py\nosboxes@osboxes:/opt/tensorpack/examples/OpenAIGym$ ./train-atari.py --env Breakout-v0 --gpu 0\n[1126 08:20:41 @bsds500.py:18] WRN Cannot import scipy. BSDS500 dataset won't be available!\n[1126 08:20:41 @svhn.py:18] WRN Cannot import scipy. SVHNDigit dataset won't be available!\n[1126 08:20:41 @format.py:16] WRN Error in 'import h5py'. HDF5Data won't be available.\n[1126 08:20:41 @format.py:24] WRN Error in 'import lmdb'. LMDBData won't be available.\n[1126 08:20:41 @format.py:31] WRN Error in 'import sklearn'. SVMLightData won't be available.\n[1126 08:20:44 @concurrency.py:24] WRN Cannot import Future in tornado.concurrent. MultiThreadAsyncPredictor won't be available.\n[2016-11-26 08:20:44,722] Making new env: Breakout-v0\n[1126 08:20:45 @train-atari.py:249] [BA3C] Train on gpu 0 and infer on gpu 0\nTraceback (most recent call last):\n  File \"./train-atari.py\", line 255, in \n    config = get_config()\n  File \"./train-atari.py\", line 177, in get_config\n    logger.auto_set_dir()\n  File \"/opt/tensorpack/tensorpack/utils/logger.py\", line 117, in auto_set_dir\n    action=action)\n  File \"/opt/tensorpack/tensorpack/utils/logger.py\", line 91, in set_logger_dir\n    mkdir_p(dirname)\n  File \"/opt/tensorpack/tensorpack/utils/fs.py\", line 22, in mkdir_p\n    raise e\nOSError: [Errno 13] Permission denied: 'train_log'. Thank you so much. It completed training with some warnings. Started evaluation also with some warnings. . May I sent you an email with an output, then?. Thank you! yes, the cpu is not that great : https://www.newegg.com/Product/Product.aspx?Item=N82E16819103291\nHow the cpu might affect training?. In this case, is the bottle neck the transfer of data between CPU and GPU or it is something else?. Yuxin,\nmay I ask you what kind of hardware you currently use?. Just for information: it has being training for ~16 days (500 'Epoch's) on the following hardware (let me edit the post when it's done):\nGPU: GeForce GTX 1060 6Gb\nCPU: AMD Phenom 9950 Agena Quad-Core 2.6 GHz\nmemory: 8Gb. Yuxin,\nHow many 'Epoch' does it usually take to train the model? All 1000 (max val)? My last output:\n100%|####################################################|2/2[04:34<00:00, 0.01it/s]\n[0127 08:58:40 @common.py:76] Waiting for all the workers to finish the last run...\n[0127 08:58:42 @stats.py:101] SummaryGradient/conv0/W/rms: 0.0025302\n[0127 08:58:42 @stats.py:101] SummaryGradient/conv0/b/rms: 0.04538\n[0127 08:58:42 @stats.py:101] SummaryGradient/conv1/W/rms: 0.0013358\n[0127 08:58:42 @stats.py:101] SummaryGradient/conv1/b/rms: 0.023961\n[0127 08:58:42 @stats.py:101] SummaryGradient/conv2/W/rms: 0.0011286\n[0127 08:58:42 @stats.py:101] SummaryGradient/conv2/b/rms: 0.0077145\n[0127 08:58:42 @stats.py:101] SummaryGradient/conv3/W/rms: 0.0012831\n[0127 08:58:42 @stats.py:101] SummaryGradient/conv3/b/rms: 0.012985\n[0127 08:58:42 @stats.py:101] SummaryGradient/fc-pi/W/rms: 0.0047387\n[0127 08:58:42 @stats.py:101] SummaryGradient/fc-pi/b/rms: 0.0065827\n[0127 08:58:42 @stats.py:101] SummaryGradient/fc-v/W/rms: 0.028517\n[0127 08:58:42 @stats.py:101] SummaryGradient/fc-v/b/rms: 0.030862\n[0127 08:58:42 @stats.py:101] SummaryGradient/fc0/W/rms: 0.00021981\n[0127 08:58:42 @stats.py:101] SummaryGradient/fc0/b/rms: 0.0013471\n[0127 08:58:42 @stats.py:101] SummaryGradient/prelu/alpha/rms: 0.094278\n[0127 08:58:42 @stats.py:101] async_global_step: 2.772e+06\n[0127 08:58:42 @stats.py:101] cost: 0.014949\n[0127 08:58:42 @stats.py:101] input_queue_size: 2.3282e-37\n[0127 08:58:42 @stats.py:101] learning_rate: 0.0001\n[0127 08:58:42 @stats.py:101] max_score: 420\n[0127 08:58:42 @stats.py:101] mean_score: 407\n[0127 08:58:42 @stats.py:101] policy_loss: -2.8096\n[0127 08:58:42 @stats.py:101] predict_reward: 3.2488\n[0127 08:58:42 @stats.py:101] rms_advantage: 0.25385\n[0127 08:58:42 @stats.py:101] value_loss: 5.6258\n[0127 08:58:42 @stats.py:101] xentropy_loss: -180.54\n[0127 08:58:43 @group.py:42] Callbacks took 279.949 sec in total. Periodic-Evaluator: 277.974sec\n[0127 08:58:43 @timer.py:42] Start Epoch 463 (global_step 2778000) ...\n. The training is still running. Can I stop it and start using or I should wait until it finish/converge?\n. thank you, and in order to run the trained model (for 500 'Epoch's) and upload to the OpenAI Gym, I should replace 100 with 500 and use my api_key? Is it right?\nENV=Breakout-v0; ./run-atari.py --load \"$ENV\".tfmodel --env \"$ENV\" --episode 100 --output output_dir. I am sorry for asking but I looked through the train-atari.py and do not see any other convergence parameter/limit except max_epoch=1000. I would like to let it complete the training. How to estimate how much 'Epoch's left?. Yuxin,\nI noticed that for Epoch 478:\n[0127 22:31:02 @stats.py:101] max_score: 0\n[0127 22:31:02 @stats.py:101] mean_score: 0\nWhat could it mean?\n[0127 21:46:17 @timer.py:42] Start Epoch 478 (global_step 2868000) ...\n100%|##############################################|6000/6000[44:41<00:00, 2.24it/s]\n[0127 22:30:58 @timer.py:46] Epoch 478 (global_step 2868000) finished, time:2681.73sec.\n[2017-01-27 22:31:00,517] Making new env: Breakout-v0\n0it [00:00, ?it/s]\n[0127 22:31:01 @common.py:76] Waiting for all the workers to finish the last run...\n[2017-01-27 22:31:01,823] Making new env: Breakout-v0\n[0127 22:31:02 @stats.py:101] SummaryGradient/conv0/W/rms: 0.001804\n[0127 22:31:02 @stats.py:101] SummaryGradient/conv0/b/rms: 0.04172\n[0127 22:31:02 @stats.py:101] SummaryGradient/conv1/W/rms: 0.00080977\n[0127 22:31:02 @stats.py:101] SummaryGradient/conv1/b/rms: 0.014749\n[0127 22:31:02 @stats.py:101] SummaryGradient/conv2/W/rms: 0.00074902\n[0127 22:31:02 @stats.py:101] SummaryGradient/conv2/b/rms: 0.0053119\n[0127 22:31:02 @stats.py:101] SummaryGradient/conv3/W/rms: 0.00090987\n[0127 22:31:02 @stats.py:101] SummaryGradient/conv3/b/rms: 0.0082322\n[0127 22:31:02 @stats.py:101] SummaryGradient/fc-pi/W/rms: 0.003632\n[0127 22:31:02 @stats.py:101] SummaryGradient/fc-pi/b/rms: 0.0046092\n[0127 22:31:02 @stats.py:101] SummaryGradient/fc-v/W/rms: 0.023866\n[0127 22:31:02 @stats.py:101] SummaryGradient/fc-v/b/rms: 0.027606\n[0127 22:31:02 @stats.py:101] SummaryGradient/fc0/W/rms: 0.00017017\n[0127 22:31:02 @stats.py:101] SummaryGradient/fc0/b/rms: 0.0010221\n[0127 22:31:02 @stats.py:101] SummaryGradient/prelu/alpha/rms: 0.083858\n[0127 22:31:02 @stats.py:101] async_global_step: 2.868e+06\n[0127 22:31:02 @stats.py:101] cost: 0.0018653\n[0127 22:31:02 @stats.py:101] input_queue_size: 2.2872e-37\n[0127 22:31:02 @stats.py:101] learning_rate: 0.0001\n[0127 22:31:02 @stats.py:101] max_score: 0\n[0127 22:31:02 @stats.py:101] mean_score: 0\n[0127 22:31:02 @stats.py:101] policy_loss: -1.5109\n[0127 22:31:02 @stats.py:101] predict_reward: 2.9229\n[0127 22:31:02 @stats.py:101] rms_advantage: 0.19438\n[0127 22:31:02 @stats.py:101] value_loss: 2.6367\n[0127 22:31:02 @stats.py:101] xentropy_loss: -177.4\n[0127 22:31:03 @group.py:42] Callbacks took 3.605 sec in total. Periodic-Evaluator: 1.646sec; StatPrinter: 1.423sec\n. Thank you, understand.. Yuxin,\nI tried to run 500 epochs of trained model by ...$ ENV=train_log/train-atari/checkpoint; ./run-atari.py --load \"$ENV\".tfmodel --env \"$ENV\" --episode 500 --output output_dir\nbut I got the following error (I use my api_key from OpenAI Gym):\nraise error.Error('Attempted to look up malformed environment ID: {}. (Currently all IDs must be of the form {}.)'.format(id.encode('utf-8'), env_id_re.pattern))\ngym.error.Error: Attempted to look up malformed environment ID: train_log/train-atari/checkpoint. (Currently all IDs must be of the form ^(?:[\\w:-]+\\/)?([\\w:.-]+)-v(\\d+)$.. When I tried \"To run a pretrained Atari model for 100 episodes\":\nENV=Breakout-v0; ./run-atari.py --load \"$ENV\".tfmodel --env \"$ENV\" --episode 100 --output output_dir\nI got the following response (could you help to change the corresponding code, please?):\nigor@igorfedorov:~/tensorpack/examples/OpenAIGym$ ENV=Breakout-v0; ./run-atari.py --load \"$ENV\".tfmodel --env \"$ENV\" --episode 100 --output output_dir\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\n[0128 16:10:06 @run-atari.py:97] Environment Name: Breakout-v0\n[2017-01-28 16:10:06,053] Making new env: Breakout-v0\n[2017-01-28 16:10:06,898] Making new env: Breakout-v0\nTraceback (most recent call last):\n  File \"./run-atari.py\", line 109, in \n    run_submission(cfg, args.output, args.episode)\n  File \"./run-atari.py\", line 71, in run_submission\n    player = get_player(dumpdir=output)\n  File \"./run-atari.py\", line 30, in get_player\n    pl = GymEnv(ENV_NAME, dumpdir=dumpdir, auto_restart=False)\n  File \"/home/igor/tensorpack/tensorpack/RL/gymenv.py\", line 48, in init\n    self.gymenv.monitor.start(dumpdir)\n  File \"/home/igor/gym/gym/core.py\", line 92, in monitor\n    raise error.Error(\"env.monitor has been deprecated as of 12/23/2016. Remove your call to env.monitor.start(directory) and instead wrap your env with env = gym.wrappers.Monitor(env, directory) to record data.\")\ngym.error.Error: env.monitor has been deprecated as of 12/23/2016. Remove your call to env.monitor.start(directory) and instead wrap your env with env = gym.wrappers.Monitor(env, directory) to record data.\n. I am sorry, I tried:\nENV=Breakout-v0; ./run-atari.py --load train_log/train-atari/checkpoint --env \"$ENV\" --episode 500 --output output_dir\nNow, it says:\nraise error.Error(\"env.monitor has been deprecated as of 12/23/2016. Remove your call to env.monitor.start(directory) and instead wrap your env with env = gym.wrappers.Monitor(env, directory) to record data.\")\ngym.error.Error: env.monitor has been deprecated as of 12/23/2016. Remove your call to env.monitor.start(directory) and instead wrap your env with env = gym.wrappers.Monitor(env, directory) to record data.\n. I replaced tensorpack and ran ./run-atari.py but still have a problem with upload, it says at the end '...You can upload them to the scoreboard via gym.upload('/home/igor/tensorpack/examples/A3C-Gym/output_dir')...' but I am not sure where to add this line:\nigor@igorfedorov:~/tensorpack/examples/A3C-Gym$ ENV=Breakout-v0; ./run-atari.py --load train_log/train-atari/checkpoint --env \"$ENV\" --episode 500 --output output_dir\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\n[0128 20:50:22 @run-atari.py:96] Environment Name: Breakout-v0\n[2017-01-28 20:50:22,773] Making new env: Breakout-v0\n[2017-01-28 20:50:23,221] Making new env: Breakout-v0\n[2017-01-28 20:50:23,317] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to env.spec.timestep_limit with env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps'). This change was made 12/28/2016 and is included in version 0.7.0\n[2017-01-28 20:50:23,361] Starting new video recorder writing to /home/igor/tensorpack/examples/A3C-Gym/output_dir/openaigym.video.0.16841.video000000.mp4\n[0128 20:50:23 @common.py:101] conv0 input: [None, 84, 84, 12]\n[0128 20:50:23 @common.py:109] conv0 output: [None, 84, 84, 32]\n[0128 20:50:23 @common.py:101] pool0 input: [None, 84, 84, 32]\n[0128 20:50:23 @common.py:109] pool0 output: [None, 42, 42, 32]\n[0128 20:50:23 @common.py:101] conv1 input: [None, 42, 42, 32]\n[0128 20:50:23 @common.py:109] conv1 output: [None, 42, 42, 32]\n[0128 20:50:23 @common.py:101] pool1 input: [None, 42, 42, 32]\n[0128 20:50:23 @common.py:109] pool1 output: [None, 21, 21, 32]\n[0128 20:50:23 @common.py:101] conv2 input: [None, 21, 21, 32]\n[0128 20:50:23 @common.py:109] conv2 output: [None, 21, 21, 64]\n[0128 20:50:23 @common.py:101] pool2 input: [None, 21, 21, 64]\n[0128 20:50:23 @common.py:109] pool2 output: [None, 10, 10, 64]\n[0128 20:50:23 @common.py:101] conv3 input: [None, 10, 10, 64]\n[0128 20:50:23 @common.py:109] conv3 output: [None, 10, 10, 64]\n[0128 20:50:23 @common.py:101] fc0 input: [None, 10, 10, 64]\n[0128 20:50:23 @common.py:109] fc0 output: [None, 512]\n[0128 20:50:23 @common.py:101] fc-pi input: [None, 512]\n[0128 20:50:23 @common.py:109] fc-pi output: [None, 6]\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \nname: GeForce GTX 1060 6GB\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7845\npciBusID 0000:04:00.0\nTotal memory: 5.92GiB\nFree memory: 5.38GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:04:00.0)\n[0128 20:50:24 @sessinit.py:75] Restoring checkpoint from train_log/train-atari/model-3000000 ...\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv0/W/rms/EMA/biased:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv0/W/rms/EMA/local_step:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv0/W/rms/EMA:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv0/b/rms/EMA/biased:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv0/b/rms/EMA/local_step:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv0/b/rms/EMA:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv1/W/rms/EMA/biased:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv1/W/rms/EMA/local_step:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv1/W/rms/EMA:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv1/b/rms/EMA/biased:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv1/b/rms/EMA/local_step:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv1/b/rms/EMA:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv2/W/rms/EMA/biased:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv2/W/rms/EMA/local_step:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv2/W/rms/EMA:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv2/b/rms/EMA/biased:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv2/b/rms/EMA/local_step:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv2/b/rms/EMA:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv3/W/rms/EMA/biased:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv3/W/rms/EMA/local_step:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv3/W/rms/EMA:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv3/b/rms/EMA/biased:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv3/b/rms/EMA/local_step:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/conv3/b/rms/EMA:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc-pi/W/rms/EMA/biased:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc-pi/W/rms/EMA/local_step:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc-pi/W/rms/EMA:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc-pi/b/rms/EMA/biased:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc-pi/b/rms/EMA/local_step:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc-pi/b/rms/EMA:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc-v/W/rms/EMA/biased:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc-v/W/rms/EMA/local_step:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc-v/W/rms/EMA:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc-v/b/rms/EMA/biased:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc-v/b/rms/EMA/local_step:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc-v/b/rms/EMA:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc0/W/rms/EMA/biased:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc0/W/rms/EMA/local_step:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc0/W/rms/EMA:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc0/b/rms/EMA/biased:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc0/b/rms/EMA/local_step:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/fc0/b/rms/EMA:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/prelu/alpha/rms/EMA/biased:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/prelu/alpha/rms/EMA/local_step:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable SummaryGradient/prelu/alpha/rms/EMA:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable beta1_power:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable beta2_power:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable cost/EMA/biased:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable cost/EMA/local_step:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable cost/EMA:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable entropy_beta:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable explore_factor:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable fc-v/W:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable fc-v/b:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable global_step:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable input_queue_size/EMA/biased:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable input_queue_size/EMA/local_step:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable input_queue_size/EMA:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable learning_rate:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable policy_loss/EMA/biased:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable policy_loss/EMA/local_step:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable policy_loss/EMA:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable predict_reward/EMA/biased:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable predict_reward/EMA/local_step:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable predict_reward/EMA:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable rms_advantage/EMA/biased:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable rms_advantage/EMA/local_step:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable rms_advantage/EMA:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable value_loss/EMA/biased:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable value_loss/EMA/local_step:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable value_loss/EMA:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable xentropy_loss/EMA/biased:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable xentropy_loss/EMA/local_step:0 in checkpoint not found in the graph!\n[0128 20:50:24 @sessinit.py:137] WRN Variable xentropy_loss/EMA:0 in checkpoint not found in the graph!\n[0128 20:50:26 @run-atari.py:72] Start evaluation: \nTraceback (most recent call last):\n  File \"./run-atari.py\", line 108, in \n    run_submission(cfg, args.output, args.episode)\n  File \"./run-atari.py\", line 76, in run_submission\n    score = play_one_episode(player, predfunc)\n  File \"/home/igor/tensorpack/examples/A3C-Gym/common.py\", line 31, in play_one_episode\n    return np.mean(player.play_one_episode(f))\n  File \"/home/igor/tensorpack/tensorpack/RL/envbase.py\", line 71, in play_one_episode\n    r, isOver = self.action(act)\n  File \"/home/igor/tensorpack/tensorpack/RL/history.py\", line 42, in action\n    r, isOver = self.player.action(act)\n  File \"/home/igor/tensorpack/tensorpack/RL/envbase.py\", line 139, in action\n    return self.player.action(act)\n  File \"/home/igor/tensorpack/tensorpack/RL/gymenv.py\", line 65, in action\n    self.finish_episode()\n  File \"/home/igor/tensorpack/tensorpack/RL/gymenv.py\", line 52, in finish_episode\n    self.gymenv.monitor.flush()\n  File \"/home/igor/gym/gym/core.py\", line 92, in monitor\n    raise error.Error(\"env.monitor has been deprecated as of 12/23/2016. Remove your call to env.monitor.start(directory) and instead wrap your env with env = gym.wrappers.Monitor(env, directory) to record data.\")\ngym.error.Error: env.monitor has been deprecated as of 12/23/2016. Remove your call to env.monitor.start(directory) and instead wrap your env with env = gym.wrappers.Monitor(env, directory) to record data.\n[2017-01-28 20:51:23,061] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/igor/tensorpack/examples/A3C-Gym/output_dir')\n. thank you, is submission script basically adding of do_submit(output) at the end of run-atari.py?. Yuxin,\nI ran: \nigor@igorfedorov:~/tensorpack/examples/A3C-Gym$ ENV=Breakout-v0; ./run-atari.py --load train_log/train-atari/checkpoint --env \"$ENV\" --episode 500 --output output_dir\nat the end it says (Could you tell why it is only 1 episode, please and how to make complete submission  >=100 episodes?):\n...\n[2017-01-29 10:20:53,565] [Breakout-v0] Uploading 1 episodes of training data\n[2017-01-29 10:20:54,931] [Breakout-v0] Uploading videos of 1 training episodes (167686 bytes)\n[2017-01-29 10:20:55,357] [Breakout-v0] Creating evaluation object from /home/igor/tensorpack/examples/A3C-Gym/output_dir with learning curve and training video\n[2017-01-29 10:20:55,538] \n\nYou successfully uploaded your evaluation on Breakout-v0 to\nOpenAI Gym! You can find it at:\nhttps://gym.openai.com/evaluations/eval_XHvvWuSRWGK51wcRdrBw\n\n\n. Yuxin,\nthank you for guiding me, I ran run-atari.py again and received the following result:\n...\nTraceback (most recent call last):\n  File \"./run-atari.py\", line 111, in \n    do_submit('/home/igor/tensorpack/examples/A3C-Gym/output_dir')\n  File \"./run-atari.py\", line 83, in do_submit\n    gym.upload(output, api_key='sk_M2DFWRqlTFWSwmGSnjHGKw')\n  File \"/home/igor/gym/gym/scoreboard/api.py\", line 83, in upload\n    evaluation = _upload(training_dir, algorithm_id, writeup, benchmark_run_id, api_key, ignore_open_monitors)\n  File \"/home/igor/gym/gym/scoreboard/api.py\", line 102, in _upload\n    raise error.Error(\"Still have an open monitor on {}. You must run 'env.close()' before uploading.\".format(', '.join(envs)))\ngym.error.Error: Still have an open monitor on Breakout-v0. You must run 'env.close()' before uploading.\n[2017-01-29 21:32:43,780] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/igor/tensorpack/examples/A3C-Gym/output_dir')\n. you were right (https://gym.openai.com/evaluations/eval_vFQPgTFbRrCDijT8DDIZuA), thank you!. Just the DIR. Thanks for pointing out, cheers. Solved by upgrading setuptools\nThank you!. ",
    "dylanthomas": "Sorry.  I must have pushed something wrong.  My apologies.\nSeong-Joon Park, Managing Director\nMove   *sf. *\nOn Sun, Dec 11, 2016 at 1:44 PM, Yuxin Wu notifications@github.com wrote:\n\nThere are no changes in this PR. Is that a mistake?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/ppwwyyxx/tensorpack/pull/57#issuecomment-266263135,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABAN9cUpFYlSCWNnKirbJz84sL57L1Zdks5rG3-tgaJpZM4K8Njk\n.\n. \n",
    "ghostFaceKillah": "Aha, I see. \nThank you for your help!. ",
    "theKase": "What is the name of the file? I dont see any \"*.tfmodel\" file. The directory contains the following two files:\nevents.out.tfevents.1480812949.[Name of machine].local\nlog.log\nDo we have to pass the file path somewhere in the args?. ",
    "deepsemantic": "Hi, thanks for info and quick reply, I will try it later. I have another question regarding the implementation. According to what I understood from your paper, it needs to change the original convolution operation to bitwise operation in e.q (3) in your paper. It would be very interesting to know your implementation of this in forward and backward pass. Could you let me know where can I find the pieces of code in your implementation. Thanks again!. It works, thanks!. So, the examples you provided still use original conv-operation, not bit-op?  . I see, thanks!. Yes, I looked the code and understood when bitW=1 and bitW=32, but I am still wondering how the output looks like when bitW=2, i.e. quantize(x=0.002, k=2)? Is it still float32 number? or 2-bit integer number consist of 0 and 1? Thanks for your time!. Hi @Paseam, happy to hearing that you are implementing bit convolution kernel, I am also implementing it with same problem as your----too slow for training, could you give me some hints that how you did the implementation. Thanks in advance.\n. ",
    "jacky4323": "Hi  @ppwwyyxx .\nSorry , I confused about  2 bit would return values in {0,1/3,2/3,1}.\nin readme file , I saw the text below , However in hardware implementation it can't use 2 bits two represent 1/3,2/3(binary format can't use 2 bits to represent)\n\ntake 2.34669849e-01 for example:\nusing 2^k-1 :\nweight_bit=2 the value is equal to 0.3333333,but actually 0.33 can't use two bits to represent\nusing 2^k :\nweight_bit=2 the value is equal to 0.25, it can use two bits (0.11) to represent\nSo why don't use 2^k? Did I make a mistake? thanks.\n. ",
    "ahundt": "@ppwwyyxx you're right, I ran an r0.12 install script but it doesn't seem to have worked correctly. Thanks! Will reopen if the problem remains after updating. I forgot to run the pip install step. :-). looks like python is there and I don't think I'm running any other python code so I think that's the tensorpack example:\n\u2502\n+-----------------------------------------------------------------------------+                   \u2502\n| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |                   \u2502\n|-------------------------------+----------------------+----------------------+                   \u2502\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |                   \u2502\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |                   \u2502\n|===============================+======================+======================|                   \u2502\n|   0  GeForce GTX 1080    Off  | 0000:02:00.0     Off |                  N/A |                   \u2502\n| 27%   31C    P8    10W / 180W |    885MiB /  8112MiB |     13%      Default |                   \u2502\n+-------------------------------+----------------------+----------------------+                   \u2502\n                                                                                                  \u2502\n+-----------------------------------------------------------------------------+                   \u2502\n| Processes:                                                       GPU Memory |                   \u2502\n|  GPU       PID  Type  Process name                               Usage      |                   \u2502\n|=============================================================================|                   \u2502\n|    0      1758    G   /usr/lib/xorg/Xorg                             452MiB |                   \u2502\n|    0      2727    G   compiz                                         132MiB |                   \u2502\n|    0      3294    G   ...CTForProblematicRoots/disabled/ExtensionD    73MiB |                   \u2502\n|    0      5873    G   unity-control-center                            34MiB |                   \u2502\n|    0     11643    C   python                                         222MiB |                   \u2502\n+-----------------------------------------------------------------------------+\nThough on the other hand my CPU usage is very high as well, about 60% of all 44 logical cores are in use. (server xenon cpu). At startup it actually does say it is going to run on the gpu as well:\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so.8.0 locally\n[2016-12-08 15:33:51,572] Making new env: Breakout-v0\n[1208 15:33:52 @train-atari.py:248] [BA3C] Train on gpu 0 and infer on gpu 0\n[1208 15:33:52 @logger.py:69] WRN Directory train_log/train-atari exists! Please either backup/delete it, or use a new directory.\n[1208 15:33:52 @logger.py:71] WRN If you're resuming from a previous run you can choose to keep it.\n[1208 15:33:52 @logger.py:72] Select Action: k (keep) / b (backup) / d (delete) / n (new):\nd\n[1208 15:34:04 @logger.py:57] Argv: ./train-atari.py --env Breakout-v0 --gpu 0. Cool, I had the config wrong! Reinstall of tensorflow with CUDA version 6.1 flag set fixed it, looks like I'm getting 9-10 it/s now. I really appreciate the info about GPU performance levels, I didn't know those details.\nNow that I'm able to match your results I'm curious about another difference in iter/sec compared to a similar implementation. It looks like @traai has traai/async-deep-rl can achieve 80 million iterations in 53 hours of breakout which is 419 iterations per second just on 8 cpu cores and no GPU. Is this perhaps an architectural difference of some sort?\n. Ah so with comparable units then tensorpack is at ~1152 steps/sec on a GPU compared to 419 steps/sec on traai/async-deep-rl  with CPU. Yeah I figured there might be some key design/implementation differences too. . I suggest looking at http://pipeline.io/ but I haven't used it.. Sure, I was only trying to provide a helpful reference. :-). Cool do you have a link to where they mention this?\nAlso, one advantage of distributed training is you can amortize io to ease feeding the GPUs. In my case I have access to many 2 GPU machines and zero 8 GPU machines.. Just getting back to this now, I'm actually looking at multi-gpu training with segmentation. I can also run on a slurm cluster where each node has multiple gpus.. There is a TensorLayer example with Keras integration, also created when I asked the same question there, perhaps that might be helpful?\n. I took a look and it seems some additional work may be required to save out the weights in Keras format?\nAlso do you think there may be a problem if Keras supplies the logits portion as well?. @ppwwyyxx is there a change that would make it easier to integrate with fairly straightforward?. Interesting! Can networks like densenet and resnet can be constructed? These rely on the functional API.. Oh I see when I was reading the code I misunderstood tower to mean a cluster with gpus on separate machines. Slurm is like Kubernetes, it is for running in a datacenter or supercomputer with many physical machines, each of which can have different numbers of gpus. Basically you add some specially formatted comments to a shell script requesting CPUs, GPUs, and memory from a cluster then it will run the shell script as requested with environment variables filled out detailing what specific machines you are running on.\nDon't worry if multiple machines isn't something tensorpack supports. I wasn't sure, that's why I asked!. Nope, thanks!. Could you consider providing the clusterspec as a parameter to allow different mechanisms to initialize it?\nThis is because depending on the distributed system the most sensible manner to initialize the clusterspec will vary, as mentioned in https://github.com/ppwwyyxx/tensorpack/issues/193. ",
    "silverlining21": "\u592a\u611f\u8c22\u4e86\uff01\n\u5173\u4e8eTF\u7248\u672c\u7684\u95ee\u9898\uff0c\u6211\u4e5f\u67e5\u770b\u4e860.12\u7248\u672c\uff0c\u4e5f\u6ca1\u6709fused_batch_norm\u63a5\u53e3\nhttps://www.tensorflow.org/versions/r0.12/api_docs/python/nn.html#neural-network\n\u8fd9\u4e2a\u63a5\u53e3\u597d\u50cf\u53ea\u662f\u5728master\u5206\u652f\u4e2d\u6709\u3002release\u7248\u672c\u4e2d\u76ee\u524d\u662f\u6ca1\u6709\u7684\u3002. \u8c22\u8c22\u4e86\uff01\n1\u3001GPU\u5e94\u8be5\u662f\u6ca1\u95ee\u9898\u7684\uff0c\u6211\u6d4b\u8bd5mnist-convnet\u6837\u4f8b\u53ef\u4ee5\u6b63\u5e38\u8fd0\u884c\n2\u3001\u6211\u8dd1\u7684\u662f\u5355\u673a\u591a\u5361\uff0c\u6ca1\u7528\u8dd1\u5206\u5e03\u5f0f\n3\u3001\u4e0b\u9762\u8fd9\u4e2a\u662f\u6211\u673a\u5668\u4e0aGPU\u5360\u7528\u60c5\u51b5\uff0c\u957f\u65f6\u95f4\u4fdd\u6301\u4e0d\u53d8\u90fd\u662f\u8fd9\u4e2a\u5360\u7528\u60c5\u51b5\u3002\n\n\u4e0b\u9762\u8fd9\u4e2a\u662f\u7a0b\u5e8f\u521d\u59cb\u5316\u65e5\u5fd7\uff0c\u9664\u4e86\u6211\u6807\u7c97\u7684\u90e8\u5206\uff0c\u597d\u50cf\u6ca1\u6709\u9519\u8bef\u63d0\u793a\uff1a\n\nnrp@scs4450-SYS-7048GR-TR:~/program/tensorpack/examples/DoReFa-Net$ python idcard-dorefa.py --dorefa 1,2,6  --data /home/nrp/data/idcard/ --gpu 0,1,2,3\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\n/home/nrp/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n[1216 16:48:03 @idcard-dorefa.py:307] Batch per tower: 32\n[1216 16:48:03 @logger.py:69] WRN Directory train_log/idcard-dorefa exists! Please either backup/delete it, or use a new directory.\n[1216 16:48:03 @logger.py:71] WRN If you're resuming from a previous run you can choose to keep it.\n[1216 16:48:03 @logger.py:72] Select Action: k (keep) / b (backup) / d (delete) / n (new):\nd\n[1216 16:48:10 @logger.py:57] Argv: idcard-dorefa.py --dorefa 1,2,6 --data /home/nrp/data/idcard/ --gpu 0,1,2,3\n[1216 16:48:10 @utils.py:60] TENSORPACK_DATASET not set, using /home/nrp/program/tensorpack/tensorpack/dataflow/dataset for dataset.\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:02:00.0\nTotal memory: 11.92GiB\nFree memory: 11.81GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x80c1500\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: \nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:03:00.0\nTotal memory: 11.92GiB\nFree memory: 11.81GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x80c4e80\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 2 with properties: \nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:82:00.0\nTotal memory: 11.92GiB\nFree memory: 11.81GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x80c8800\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 3 with properties: \nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:83:00.0\nTotal memory: 11.92GiB\nFree memory: 11.81GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 2\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 3\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 2\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 3\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 2 and 0\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 2 and 1\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 3 and 0\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 3 and 1\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 2 3 \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y Y N N \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   Y Y N N \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 2:   N N Y Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 3:   N N Y Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:2) -> (device: 2, name: GeForce GTX TITAN X, pci bus id: 0000:82:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:3) -> (device: 3, name: GeForce GTX TITAN X, pci bus id: 0000:83:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:2) -> (device: 2, name: GeForce GTX TITAN X, pci bus id: 0000:82:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:3) -> (device: 3, name: GeForce GTX TITAN X, pci bus id: 0000:83:00.0)\n[1216 16:48:17 @multigpu.py:29] Training a model of 4 tower\n[1216 16:48:17 @multigpu.py:37] Building graph for training tower 0...\n[1216 16:48:17 @_common.py:72] conv0 input: [None, 32, 32, 3]\n[1216 16:48:17 @_common.py:80] conv0 output: [None, 6, 6, 96]\n[1216 16:48:17 @_common.py:72] conv1 input: [None, 6, 6, 96]\n[1216 16:48:17 @idcard-dorefa.py:90] Binarizing weight conv1/W\n[1216 16:48:17 @_common.py:80] conv1 output: [None, 6, 6, 256]\n[1216 16:48:17 @_common.py:72] pool1 input: [None, 6, 6, 256]\n[1216 16:48:17 @_common.py:80] pool1 output: [None, 3, 3, 256]\n[1216 16:48:17 @_common.py:72] conv2 input: [None, 3, 3, 256]\n[1216 16:48:17 @idcard-dorefa.py:90] Binarizing weight conv2/W\n[1216 16:48:17 @_common.py:80] conv2 output: [None, 3, 3, 384]\n[1216 16:48:17 @_common.py:72] pool2 input: [None, 3, 3, 384]\n[1216 16:48:17 @_common.py:80] pool2 output: [None, 2, 2, 384]\n[1216 16:48:17 @_common.py:72] conv3 input: [None, 2, 2, 384]\n[1216 16:48:17 @idcard-dorefa.py:90] Binarizing weight conv3/W\n[1216 16:48:17 @_common.py:80] conv3 output: [None, 2, 2, 384]\n[1216 16:48:17 @_common.py:72] conv4 input: [None, 2, 2, 384]\n[1216 16:48:17 @idcard-dorefa.py:90] Binarizing weight conv4/W\n[1216 16:48:17 @_common.py:80] conv4 output: [None, 2, 2, 256]\n[1216 16:48:17 @_common.py:72] pool4 input: [None, 2, 2, 256]\n[1216 16:48:17 @_common.py:80] pool4 output: [None, 1, 1, 256]\n[1216 16:48:17 @_common.py:72] fc0 input: [None, 1, 1, 256]\n[1216 16:48:17 @idcard-dorefa.py:90] Binarizing weight fc0/W\n[1216 16:48:17 @_common.py:80] fc0 output: [None, 4096]\n[1216 16:48:18 @_common.py:72] fc1 input: [None, 1, 1, 4096]\n[1216 16:48:18 @idcard-dorefa.py:90] Binarizing weight fc1/W\n[1216 16:48:18 @_common.py:80] fc1 output: [None, 4096]\n[1216 16:48:18 @_common.py:72] fct input: [None, 1, 1, 4096]\n[1216 16:48:18 @_common.py:80] fct output: [None, 6915]\n[1216 16:48:18 @regularize.py:17] Apply regularizer for fc0/W:0\n[1216 16:48:18 @regularize.py:17] Apply regularizer for fc1/W:0\n[1216 16:48:18 @regularize.py:17] Apply regularizer for fct/W:0\n[1216 16:48:19 @multigpu.py:37] Building graph for training tower 1...\n[1216 16:48:19 @idcard-dorefa.py:90] Binarizing weight conv1/W\n[1216 16:48:19 @idcard-dorefa.py:90] Binarizing weight conv2/W\n[1216 16:48:19 @idcard-dorefa.py:90] Binarizing weight conv3/W\n[1216 16:48:19 @idcard-dorefa.py:90] Binarizing weight conv4/W\n[1216 16:48:19 @idcard-dorefa.py:90] Binarizing weight fc0/W\n[1216 16:48:19 @idcard-dorefa.py:90] Binarizing weight fc1/W\n[1216 16:48:20 @multigpu.py:37] Building graph for training tower 2...\n[1216 16:48:20 @idcard-dorefa.py:90] Binarizing weight conv1/W\n[1216 16:48:21 @idcard-dorefa.py:90] Binarizing weight conv2/W\n[1216 16:48:21 @idcard-dorefa.py:90] Binarizing weight conv3/W\n[1216 16:48:21 @idcard-dorefa.py:90] Binarizing weight conv4/W\n[1216 16:48:21 @idcard-dorefa.py:90] Binarizing weight fc0/W\n[1216 16:48:21 @idcard-dorefa.py:90] Binarizing weight fc1/W\n[1216 16:48:22 @multigpu.py:37] Building graph for training tower 3...\n[1216 16:48:22 @idcard-dorefa.py:90] Binarizing weight conv1/W\n[1216 16:48:22 @idcard-dorefa.py:90] Binarizing weight conv2/W\n[1216 16:48:22 @idcard-dorefa.py:90] Binarizing weight conv3/W\n[1216 16:48:22 @idcard-dorefa.py:90] Binarizing weight conv4/W\n[1216 16:48:22 @idcard-dorefa.py:90] Binarizing weight fc0/W\n[1216 16:48:22 @idcard-dorefa.py:90] Binarizing weight fc1/W\n[1216 16:48:24 @modelutils.py:26] Model Parameters: \nconv0/W:0: shape=[12, 12, 3, 96], dim=41472\nconv1/W:0: shape=[5, 5, 48, 256], dim=307200\nbn1/beta:0: shape=[256], dim=256\nbn1/gamma:0: shape=[256], dim=256\nconv2/W:0: shape=[3, 3, 256, 384], dim=884736\nbn2/beta:0: shape=[384], dim=384\nbn2/gamma:0: shape=[384], dim=384\nconv3/W:0: shape=[3, 3, 192, 384], dim=663552\nbn3/beta:0: shape=[384], dim=384\nbn3/gamma:0: shape=[384], dim=384\nconv4/W:0: shape=[3, 3, 192, 256], dim=442368\nbn4/beta:0: shape=[256], dim=256\nbn4/gamma:0: shape=[256], dim=256\nfc0/W:0: shape=[256, 4096], dim=1048576\nbnfc0/beta:0: shape=[4096], dim=4096\nbnfc0/gamma:0: shape=[4096], dim=4096\nfc1/W:0: shape=[4096, 4096], dim=16777216\nbnfc1/beta:0: shape=[4096], dim=4096\nbnfc1/gamma:0: shape=[4096], dim=4096\nfct/W:0: shape=[4096, 6915], dim=28323840\nfct/b:0: shape=[6915], dim=6915\nTotal param=48514819 (185.069347 MB assuming all float32)\n[1216 16:48:24 @base.py:110] Setup callbacks ...\n[1216 16:48:24 @saver.py:63] [ModelSaver] tower0/train-error-top1/EMA:0 renamed to train-error-top1/EMA:0 when saving model.\n[1216 16:48:24 @saver.py:63] [ModelSaver] tower0/train-error-top5/EMA:0 renamed to train-error-top5/EMA:0 when saving model.\n[1216 16:48:24 @saver.py:63] [ModelSaver] tower0/cross_entropy_loss/EMA:0 renamed to cross_entropy_loss/EMA:0 when saving model.\n[1216 16:48:24 @saver.py:63] [ModelSaver] tower0/AddN/EMA:0 renamed to AddN/EMA:0 when saving model.\n[1216 16:48:24 @saver.py:63] [ModelSaver] tower0/cost/EMA:0 renamed to cost/EMA:0 when saving model.\n[1216 16:48:24 @saver.py:63] [ModelSaver] tower0/train-error-top1/EMA/biased:0 renamed to train-error-top1/EMA/biased:0 when saving model.\n[1216 16:48:24 @saver.py:63] [ModelSaver] tower0/train-error-top1/EMA/local_step:0 renamed to train-error-top1/EMA/local_step:0 when saving model.\n[1216 16:48:24 @saver.py:63] [ModelSaver] tower0/train-error-top5/EMA/biased:0 renamed to train-error-top5/EMA/biased:0 when saving model.\n[1216 16:48:24 @saver.py:63] [ModelSaver] tower0/train-error-top5/EMA/local_step:0 renamed to train-error-top5/EMA/local_step:0 when saving model.\n[1216 16:48:24 @saver.py:63] [ModelSaver] tower0/cross_entropy_loss/EMA/biased:0 renamed to cross_entropy_loss/EMA/biased:0 when saving model.\n[1216 16:48:24 @saver.py:63] [ModelSaver] tower0/cross_entropy_loss/EMA/local_step:0 renamed to cross_entropy_loss/EMA/local_step:0 when saving model.\n[1216 16:48:24 @saver.py:63] [ModelSaver] tower0/AddN/EMA/biased:0 renamed to AddN/EMA/biased:0 when saving model.\n[1216 16:48:24 @saver.py:63] [ModelSaver] tower0/AddN/EMA/local_step:0 renamed to AddN/EMA/local_step:0 when saving model.\n[1216 16:48:24 @saver.py:63] [ModelSaver] tower0/cost/EMA/biased:0 renamed to cost/EMA/biased:0 when saving model.\n[1216 16:48:24 @saver.py:63] [ModelSaver] tower0/cost/EMA/local_step:0 renamed to cost/EMA/local_step:0 when saving model.\n[1216 16:48:25 @base.py:111] Building graph for predictor tower 0...\n[1216 16:48:25 @idcard-dorefa.py:90] Binarizing weight conv1/W\n[1216 16:48:25 @idcard-dorefa.py:90] Binarizing weight conv2/W\n[1216 16:48:25 @idcard-dorefa.py:90] Binarizing weight conv3/W\n[1216 16:48:25 @idcard-dorefa.py:90] Binarizing weight conv4/W\n[1216 16:48:25 @idcard-dorefa.py:90] Binarizing weight fc0/W\n[1216 16:48:25 @idcard-dorefa.py:90] Binarizing weight fc1/W\n[1216 16:48:34 @base.py:120] Initializing graph variables ...\n[1216 16:48:36 @concurrency.py:24] Starting EnqueueThread\n[1216 16:48:36 @base.py:142] Start training with global_step=0\n  0%|                                                                                                                                                                 |0/10000[00:00<?,?it/s]\n\n. \u662f\u7684\u3002\n\u6211\u7684\u8bad\u7ec3\u6570\u636e\u683c\u5f0f\uff1anamelist + \u539f\u56fe\uff0cnamelsit\u7684\u6bcf\u4e00\u884c\u662f\u56fe\u7247 \u8def\u5f84+label\n\u53ef\u80fd\u786e\u5b9e\u662f\u6211\u5b9e\u73b0\u7684DataFLow\u6709\u95ee\u9898\uff0c\u53c2\u8003\u7684\u662f\u4f60\u7ed9\u7684lisvrc.py+alexnet-dorefa.py \u3002\u9488\u5bf9\u8fd9\u79cd\u683c\u5f0f\u6570\u636e\u4f60\u6709\u4ec0\u4e48\u6bd4\u8f83\u597d\u7684\u5efa\u8bae\u5417\uff1f\n\u8c22\u8c22\u4e86. here is my scripts:\n\nidcard.py modifed from ilvsrc.py\nidcard-dorefa.py modifed from alexnet-dorefa.py\n\nthat's all the changes I had made.\nand  i have just confirmed that dataset SVHN works well on my machine\n. \n\nRun:\npython idcard-dorefa.py --data ../../tensorpack/dataflow/dataset/idcard --dorefa 1,2,6 --gpu 0,1,2,3\nit reports an ERROR.  and  I have  checked  the  base class RNGDataFlow and DataFlow, seems that there is something  wrong with calling of rng method .\n\nnrp@scs4450-SYS-7048GR-TR:~/program/tensorpack/examples/DoReFa-Net$ python idcard-dorefa.py --data ../../tensorpack/dataflow/dataset/idcard --dorefa 1,2,6 --gpu 0,1,2,3\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\n/home/nrp/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n[1219 10:13:56 @idcard-dorefa.py:304] Batch per tower: 32\n[1219 10:13:56 @logger.py:69] WRN Directory train_log/idcard-dorefa exists! Please either backup/delete it, or use a new directory.\n[1219 10:13:56 @logger.py:71] WRN If you're resuming from a previous run you can choose to keep it.\n[1219 10:13:56 @logger.py:72] Select Action: k (keep) / b (backup) / d (delete) / n (new):\nd\n[1219 10:13:57 @logger.py:57] Argv: idcard-dorefa.py --data ../../tensorpack/dataflow/dataset/idcard --dorefa 1,2,6 --gpu 0,1,2,3\nTraceback (most recent call last):\n  File \"idcard-dorefa.py\", line 306, in \n    config = get_config()\n  File \"idcard-dorefa.py\", line 218, in get_config\n    data_train = get_data('train')\n  File \"idcard-dorefa.py\", line 158, in get_data\n    for dsp in ds.get_data():\n  File \"/home/nrp/program/tensorpack/tensorpack/dataflow/dataset/idcard.py\", line 163, in get_data\n    self.rng(idxs)\nAttributeError: 'IDCard12' object has no attribute 'rng'\n\n. Modefied the code as following: \n\nI got the result in the terminal like that, and it seems that code sutck in some  loop.........\n\n. > Oh wait.. do you mean that your data stop printing after your screenshot?... That's something you can debug on.\nNO, it's still  printing I guess cause my data list is too long .....\n.  I have fix the problems you mentioned above.  the  original batchsize=128 and I have more than 10000 items in both train set and val set.\nbut I still stuck in the same ouput....\n\n. hello there ?\nafter debuging the code by step, I found that the the code stuck in the place calling sess.run() by tf and I dont know how to trace it... could you help me out . thanks in advance. \n\n. I use anancoda, so I just simply use conda to install like this:\nconda install opencv=2.4.12\nand I have one thing need your confirmation that while reading images in you code you use relative path  or abs. path ?  in my understading this wont effect anything, I'm not sure about it. \nbecause in my implementation, I use the abs. path of the image to read it. here is the example of the name list of trainnig data:\n\n. how did  you fix it\uff0c finally \uff1f\nshould I reinstall opencv  that build by myself ..? . actually I use the official package of pre-built opencv provided byconda-forge\n\n. sorry.... I haven't update the changes ...\n\n. and here is the test code and result of using cv2 to read and resize image, then apply transpose by tf.\n```\nimport cv2\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfilename = \"/home/nrp/data/idcard/val/fdd103d0-b7ec-44b5-a02b-3550b9ab085d/37.jpg\"\nim = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\nplt.figure(1)\nplt.imshow(im)\nim = cv2.resize(im,(10,25))\nplt.figure(2)\nplt.imshow(im)\nx = tf.Variable(im, name='x')\nmodel = tf.initialize_all_variables()\nwith tf.Session() as session:\n    x = tf.transpose(x)\n    session.run(model)\n    result = session.run(x)\nplt.figure(3)\nplt.imshow(result)\nplt.show()\n```\nit seems that tf.transpose have change the orignal image.......... the result from left to right:\ntf output     |     resized result      |   orginal showed by plt       |      orignal showed linux image viewer|\n\n. NO exception raised\n\nthe result is the same.\n\n. yes\uff0cI have already fixed the input.\nand I did as you said add print len(dp) before op.run at input_data.py , but I cant not get the result you described..   ....\n. the input  after i fixed just like that.\n\n. i fixed code in _build_graph like that, but I sitll cant get print many 2  in treminal that you discribed\n\n. After removing the PrefetchDataZMQ  I got exception of feed failure and queue queue OutOfRangeError:\nhere is the log info.\n```\n.........................above is normal output of initialization....................................\n [1219 18:46:39 @idcard-dorefa.py:93] Binarizing weight conv1/W\n[1219 18:46:39 @idcard-dorefa.py:93] Binarizing weight conv2/W\n[1219 18:46:39 @idcard-dorefa.py:93] Binarizing weight conv3/W\n[1219 18:46:39 @idcard-dorefa.py:93] Binarizing weight fc0/W\n[1219 18:46:42 @base.py:120] Initializing graph variables ...\n[1219 18:46:43 @concurrency.py:24] Starting EnqueueThread\n[1219 18:46:43 @base.py:142] Start training with global_step=0\n  0%|                                                                                                                                                                 |0/10000[00:00<?,?it/s]2\n[1219 18:46:43 @input_data.py:86] ERR Exception in EnqueueThread:\nTraceback (most recent call last):\n  File \"/home/nrp/program/tensorpack/tensorpack/train/input_data.py\", line 82, in run\n    self.op.run(feed_dict=feed)\n  File \"/home/nrp/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1449, in run\n    _run_using_default_session(self, feed_dict, self.graph, session)\n  File \"/home/nrp/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3668, in _run_using_default_session\n    session.run(operation, feed_dict)\n  File \"/home/nrp/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 766, in run\n    run_metadata_ptr)\n  File \"/home/nrp/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 943, in _run\n    % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\nValueError: Cannot feed value of shape (128, 32, 32, 32) for Tensor u'input:0', which has shape '(?, 32, 32)'\n[1219 18:46:43 @input_data.py:93] Enqueue Thread Exited.\nTraceback (most recent call last):\n  File \"idcard-dorefa.py\", line 317, in \n    SyncMultiGPUTrainer(config).train()\n  File \"/home/nrp/program/tensorpack/tensorpack/train/base.py\", line 59, in train\n    self.main_loop()\n  File \"/home/nrp/program/tensorpack/tensorpack/train/base.py\", line 154, in main_loop\n    self.run_step() # implemented by subclass\n  File \"/home/nrp/program/tensorpack/tensorpack/train/multigpu.py\", line 109, in run_step\n    self.sess.run(self.train_op)\n  File \"/home/nrp/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 766, in run\n    run_metadata_ptr)\n  File \"/home/nrp/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 964, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/home/nrp/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\n    target_list, options, run_metadata)\n  File \"/home/nrp/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.OutOfRangeError: FIFOQueue '_0_input_queue' is closed and has insufficient elements (requested 1, current size 0)\n     [[Node: tower0/input_deque = QueueDequeue_class=[\"loc:@input_queue\"], component_types=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n     [[Node: tower0/InTopK_1/_15 = _Recvclient_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_557_tower0/InTopK_1\", tensor_type=DT_BOOL, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]]\nCaused by op u'tower0/input_deque', defined at:\n  File \"idcard-dorefa.py\", line 317, in \n    SyncMultiGPUTrainer(config).train()\n  File \"/home/nrp/program/tensorpack/tensorpack/train/base.py\", line 58, in train\n    self.setup()\n  File \"/home/nrp/program/tensorpack/tensorpack/train/base.py\", line 106, in setup\n    self._setup()\n  File \"/home/nrp/program/tensorpack/tensorpack/train/multigpu.py\", line 95, in _setup\n    self.config.tower, lambda: self._get_cost_and_grad()[1])\n  File \"/home/nrp/program/tensorpack/tensorpack/train/multigpu.py\", line 40, in _multi_tower_grads\n    grad_list.append(get_tower_grad_func())\n  File \"/home/nrp/program/tensorpack/tensorpack/train/multigpu.py\", line 95, in \n    self.config.tower, lambda: self._get_cost_and_grad()[1])\n  File \"/home/nrp/program/tensorpack/tensorpack/train/feedfree.py\", line 39, in _get_cost_and_grad\n    actual_inputs = self._get_input_tensors()\n  File \"/home/nrp/program/tensorpack/tensorpack/train/feedfree.py\", line 30, in _get_input_tensors\n    return self._input_method.get_input_tensors()\n  File \"/home/nrp/program/tensorpack/tensorpack/train/input_data.py\", line 44, in get_input_tensors\n    return self._get_input_tensors()\n  File \"/home/nrp/program/tensorpack/tensorpack/train/input_data.py\", line 122, in _get_input_tensors\n    ret = self.queue.dequeue(name='input_deque')\n  File \"/home/nrp/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py\", line 419, in dequeue\n    self._queue_ref, self._dtypes, name=name)\n  File \"/home/nrp/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 1057, in _queue_dequeue\n    timeout_ms=timeout_ms, name=name)\n  File \"/home/nrp/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/home/nrp/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/nrp/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1128, in init\n    self._traceback = _extract_stack()\nOutOfRangeError (see above for traceback): FIFOQueue '_0_input_queue' is closed and has insufficient elements (requested 1, current size 0)\n     [[Node: tower0/input_deque = QueueDequeue_class=[\"loc:@input_queue\"], component_types=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n     [[Node: tower0/InTopK_1/_15 = _Recvclient_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_557_tower0/InTopK_1\", tensor_type=DT_BOOL, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]]\n```. I tracing the bug as follow:\nI found that it failed to excute self.dataflow.get_data()\n\ninput_dada.py line 76  for dp in self.dataflow.get_data():. \nmultigpu.py line 120             self._input_method = QueueInput(config.dataset, input_queue)\nidcard-dorefa.py line 224   data_train = get_data('train')\nidcard.py line 157 which is the implement of function def get_data(self):\n\nAs it mentioned in above discussion, the result is quite weird after I read andresize grayscale image(in the example is digit '1') by cv2, then only applied transpose operation by tf, nothing more. \nthe result in the following image from left to right are:\ntf output     --|--    resized result   --|--   orginal showed by plt    --|--     orignal showed linux image viewer|\n\n. hi @ppwwyyxx \nfinally, I have fixed the problem of datafolw by followling your advices. and now I can train my model with my data noramlly.\nthe problem lies in that the Anaconda opencv=2.4.12(compiled by np111py27_1) do not raise exception when it's fail to read image which lead to the dataflow can not feed data to TF operations and keep it waiting that leads to process bar dead.\nfollow you hints, my solution is simply update the version of opencv from 2.4.12 to 3.1.0 (compiled by np111py27_1) using conda install, it turns out work for me.\nthanks again for you help!. After updating opencv to 3.1 and uncomment the line arr = np.transpose(arr, [1,2,0])in get_per_pixel_mean. , keep PrefetchDataZMQ there, the code work properly.. but before I do not change the shape of feeding tensor(using  opencv2.4.12), I also got the training speed 0 it/s in return. which mean the problemis not cause by the tensor shape changing. that's my opinion.....>..<. if there is anything, tiny tests or experiments,  that i can help you find out the reason, just let me know.... OK, thanks. I will try out.. I run these two experiment at the same time on different GUPs, does this effect anything ?. the first experiment, which  (W,A,G)=(32,32,32), after I change the learning rate to 0.01 and restore training and I got a much better convergence at top_1_error=0.06. but for the  (W,A,G)=(1,2,32) it's still the same. . thanks!. ",
    "yenchenlin": "Oops I make it work!\nI think the prerequisite should change to tf 0.12?. My original tf version is 0.11, and cv2 version is 2.4 (according to the dependencies on README). Hello @ppwwyyxx ,\nI use conda install -c https://conda.binstar.org/menpo opencv. Yes, I just reinstalled it and your code works! (maybe it's problem on my side)\nBTW, may I send a PR to specify that code here needs\ntensorflow 0.12\ncv2 3.1\nIt's different with the prerequisites in tensorpack's README.. Oh I thought you used some summary features which are not included in TF 0.11?\nSuch as this line, tf.summary.histogram can't be found in TF0.11's summary API document page.\n. ",
    "flydance0217": "Will it be possible to provide some suggestions on how you would implement that with minimal disruption to the original model in tensorpack. It seems to do the 10-crop test/validation, we need to support a 4D tensor where the tensor consists of the same validation image cropped at different location to the model, then average out the softmax output. \nHowever, it seems that imgaug.ImageAugmentor requiring the output to be a 3D array.. Thanks ppwwyyxx for the prompt response. I have been looking at the 2 solutions you mentioned over the weekend, but still unable to develop a working solution.\nI hope I can make this clear, just to briefly describe what I want to achieve\nIn every step, I want to look at the entire or subset (i.e., in the case of the weights for fully connected layer, say the first 10 rows) weights of one or more layers. I want to do some post-processing (i.e., by computing std, mean, median, etc). Depending on the post-processed info, I would then want to have the ability to change the weight value on the same/different layer at the specific location to specific values (i.e., last row of the fully-connected layer). \nFirst suggestion (create ops in _setup_graph):\nI don't seen to be able to access the weight value in this case(i.e., by calling tf.trainable_variables()[index].eval()). Can you please elaborate more on what you meant by \"create ops in _setup_graph\"? Does this mean that all the operations need to be in tf world?\nSecond suggestion:\nI looked at the Wasserstein GAN example you provided. It uses the tf.clip_by_value function provided by tensorflow, and works on the entire weight only. I traced through the code to gen_math_ops.py and op_def_library.py but its not straight-forward at this point, and is limited to tf world only. It would be nice if I can work using numpy (which I don't seen to be able to do it in this case)\nThe post-processing in numpy is key in this case I guess, as there are some complicated algorithms that have been developed in numpy world.\nThanks so much for the help!\n. Thanks. I didn't realize that you can do var.load(value) to update the value. This works but now it is extremely slow (as expected). I really need to consider rewriting everything in ops in the graph now...\nThanks again!. Sorry to bother you again. I was able to follow your Wasserstein GAN example and made the per-variable operation with VariableAssignmentOptimizer. However, is there a way to optionally perform this operation say after epoch 20? In the case of the Wasserstein GAN example, this means only perform clipping after epcoh 20.. Probably should have given more thoughts before posting. A colleague just pointed out I should able to accomplish what I described using ScaleGradient, but instead of passing in a scalar, pass in a tensor instead. \nWill come back if I figure out how it works.. Hi ppwwyyxx,\nAgree with you on the example I gave above,  but it might still be useful in some cases where one may want to manipulate the gradient on-the-fly based on some user-defined criteria. I was able to modify your ScaleGradient code so that the grad tensor is multiplied with a \"criteria\" tensor to achieve per weight gradient control. This way, I only need to modify the \"criteria\" tensor to do what I want.\nThanks for all the help. I have a follow up question. I can pass in a \"criteria\" tensor (built as part of the tf.global_variables) consists of only 0 or 1 to achieve per weight gradient control of any layer. However it seems that the ScaleGradient function only loads the content of this tensor once initially when the graph is built (which makes sense) and assume the content remains static.\nAs an example, initially the \"criteria\" tensor is set to 1 everywhere, meaning that every weight can be updated. Then at certain point I want to modify this tensor in my callback function so that say the first two rows (assuming its a 2D tensor) become 0, meaning the corresponding two rows of weight will no longer be updated. \nHow would you go about approaching this problem? \nThanks,. ",
    "kaihuchen": "@ppwwyyxx \nThanks for the quick reply. I have been using pix2pix/Torch for while, in fact all of the 400x400 training/testing datasets I used here were borrowed directly from my pix2pix/Torch. Now that I know that tensorpack requires 256x256 images I have adjusted my datasets accordingly. After adjusting for the model path as you mentioned above, I got the following error when running the test samples:\n\n[0101 00:14:08 @sessinit.py:71] Restoring checkpoint from ./train_log/Image2Image0101-000755/model-18 ...\n[0101 00:14:08 @sessinit.py:141] WRN Variable pos_acc/EMA/local_step:0 in checkpoint not found in the graph!\n[0101 00:14:08 @sessinit.py:141] WRN Variable input_queue_size/EMA:0 in checkpoint not found in the graph!\n[0101 00:14:08 @sessinit.py:141] WRN Variable global_step:0 in checkpoint not found in the graph!\n[0101 00:14:08 @sessinit.py:141] WRN Variable total_g_loss/EMA/biased:0 in checkpoint not found in the graph!\n[0101 00:14:08 @sessinit.py:141] WRN Variable d_CE_loss_neg/EMA:0 in checkpoint not found in the graph!\n[0101 00:14:08 @sessinit.py:141] WRN Variable g_CE_loss/EMA/biased:0 in checkpoint not found in the graph!\n[0101 00:14:08 @sessinit.py:141] WRN Variable g_CE_loss/EMA:0 in checkpoint not found in the graph!\n[0101 00:14:08 @sessinit.py:141] WRN Variable g_CE_loss/EMA/local_step:0 in checkpoint not found in the graph!\n[0101 00:14:08 @sessinit.py:141] WRN Variable total_g_loss/EMA/local_step:0 in checkpoint not found in the graph!\n[0101 00:14:08 @sessinit.py:141] WRN Variable d_CE_loss_pos/EMA/biased:0 in checkpoint not found in the graph!\n[0101 00:14:08 @sessinit.py:141] WRN Variable neg_acc/EMA/local_step:0 in checkpoint not found in the graph!\n[0101 00:14:08 @sessinit.py:141] WRN Variable d_CE_loss/EMA:0 in checkpoint not found in the graph!\n[0101 00:14:08 @sessinit.py:141] WRN Variable pos_acc/EMA:0 in checkpoint not found in the graph!\n[0101 00:14:08 @sessinit.py:141] WRN Variable beta1_power:0 in checkpoint not found in the graph!\n[0101 00:14:08 @sessinit.py:141] WRN Variable input_queue_size/EMA/biased:0 in checkpoint not found in the graph!\n[0101 00:14:08 @sessinit.py:141] WRN Variable L1_loss/EMA:0 in checkpoint not found in the graph!\n[0101 00:14:08 @sessinit.py:141] WRN Variable d_CE_loss_neg/EMA/local_step:0 in checkpoint not found in the graph!\n[0101 00:14:08 @sessinit.py:141] WRN Variable L1_loss/EMA/biased:0 in checkpoint not found in the graph!\n[0101 00:14:08 @sessinit.py:141] WRN Variable learning_rate:0 in checkpoint not found in the graph!\n[0101 00:14:08 @sessinit.py:141] WRN Variable total_g_loss/EMA:0 in checkpoint not found in the graph!\n[0101 00:14:08 @sessinit.py:141] WRN Variable d_CE_loss/EMA/local_step:0 in checkpoint not found in the graph!\n[0101 00:14:08 @sessinit.py:141] WRN Variable neg_acc/EMA/biased:0 in checkpoint not found in the graph!\n[0101 00:14:08 @sessinit.py:141] WRN Variable beta2_power:0 in checkpoint not found in the graph!\n[0101 00:14:08 @sessinit.py:141] WRN Variable neg_acc/EMA:0 in checkpoint not found in the graph!\n[0101 00:14:08 @sessinit.py:141] WRN Variable L1_loss/EMA/local_step:0 in checkpoint not found in the graph!\n[0101 00:14:08 @sessinit.py:141] WRN Variable pos_acc/EMA/biased:0 in checkpoint not found in the graph!\n[0101 00:14:08 @sessinit.py:141] WRN Variable input_queue_size/EMA/local_step:0 in checkpoint not found in the graph!\n[0101 00:14:08 @sessinit.py:141] WRN Variable d_CE_loss/EMA/biased:0 in checkpoint not found in the graph!\n[0101 00:14:08 @sessinit.py:141] WRN Variable d_CE_loss_pos/EMA:0 in checkpoint not found in the graph!\n[0101 00:14:08 @sessinit.py:141] WRN Variable d_CE_loss_pos/EMA/local_step:0 in checkpoint not found in the graph!\n[0101 00:14:08 @sessinit.py:141] WRN Variable d_CE_loss_neg/EMA/biased:0 in checkpoint not found in the graph!\n  0%|                                                                                                                      |0/1[00:00,?it/s]Failed to connect to Mir: Failed to connect to server socket: No such file or directory\nUnable to init server: Could not connect: Connection refused\n\n(random_window_name:20692): Gtk-WARNING **: cannot open display:\n</pre\n\nThe error seems to be about some GTK display setup which I could do without. Is there any way to just see the output images as files, or else see them in the Tensorboard?. @ppwwyyxx \nThat's very helpful info which got me over the problem I had earlier. There is only one hitch remains where the images I saved look off-color, like this:\n\nThe above is expected to be in full color instead. My code looks like this:\nfor i,img in enumerate(o):\n    img = Image.fromarray(img, \"RGB\")\n    path = os.path.join(\"somePathHere-\"+str(i)+\".jpg\")\n    img.save(path)\n\nI compared the pixel value in the array img with what got rendered in the file which appear to be the same, so I believe I am rendering exactly what got passed in o. Is there some pre-processing that I need to do with img here?  Thanks!\n. @ppwwyyxx \nThat was indeed the problem. Much appreciated for the help.\n. @ppwwyyxx \nOn the first question, I got it to work as you suggested, with the twist that I appended gen/conv8/output to output_names to get a 6x1x1x512 tensor, which does appear to be what I wanted. Much appreciated for your help!\nOne curious thing is that when I visualize the result of >1000 z vectors using t-SNE, I thought the diagram looks different (aside from coloring) from what I got with exactly the same training/testing datasets (which are frames from a celebrity interview video) using pix2pix/Torch.  Not sure what to make of it yet. Something interesting for me to look into.\nFollowing is the tensorpack version of the t-SNE diagram, where the test samples seem fairly uniformly distributed in the z space (projected from 512D to 3D for display):\n\nFollowing is the pix2pix/Torch version of the t-SNE diagram, which looks comparatively more lumpy:\n\n. ",
    "itmessager": "\n1. Loading a pre-trained model is just simply `--load model` handled in most examples. `--load` will try to load all parameters with the same name and print warning about the ummatched weights in the model, so to re-train some layers you should change their names before loading. [HED](https://github.com/ppwwyyxx/tensorpack/tree/master/examples/HED) is such an example which loads pre-trained vgg..\n\n2. [FAQ](http://tensorpack.readthedocs.io/en/latest/tutorial/faq.html#how-to-freeze-some-variables-in-training) about freezing variables.\n\n\ncan't find the FAQ page. > https://tensorpack.readthedocs.io/tutorial/faq.html\nthanks. > It can run now.Thank you very much!\nI face the same error ,could you tell me how to solve it?. ",
    "lightaime": "Since this error make the result imcomplete, the result won't be able to upload to gym. But it doesn't matter. thank you.. Thank you very much.\nOn Sat, Jan 14, 2017 at 8:03 PM, Yuxin Wu notifications@github.com wrote:\n\nI didn't know that would happen. I was ignoring this error all the time\nand things are fine.\nMaybe there are some changes in gym that make this error matter? I'll try\nit when I have time.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/ppwwyyxx/tensorpack/issues/97#issuecomment-272619956,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AWiZ4HEjyaaVdZDI6Ga0Ty0peZ3Ygsxgks5rSLmWgaJpZM4Lf02L\n.\n. \n",
    "Neltherion": "@ppwwyyxx\nThanks for the quick response... Changing the protocol to TCP (or inproc) avoids the first error but the second error (which I explained in the post above) still remains... I do believe its main problem resides here:\nFile \"C:\\Anaconda3\\lib\\multiprocessing\\reduction.py\", line 59, in dump\n    ForkingPickler(file, protocol).dump(obj)\nAttributeError: Can't pickle local object 'AugmentImageComponents.__init__.<locals>.func'\nwhich eventually results in this error:\nFile \"PATH_TO_PACKAGE\\Tensorpack\\tensorpack\\dataflow\\prefetch.py\", line 193, in __del__\n    x.terminate()\n  File \"C:\\Anaconda3\\lib\\multiprocessing\\process.py\", line 113, in terminate\n    self._popen.terminate()\nAttributeError: 'NoneType' object has no attribute 'terminate'. Ok... I commented this line ds = PrefetchDataZMQ(ds, 1) and had to change 2 parts of the code... \nthe first part was to use config.dataflow instead of config.dataset in tensorpack-master/examples/GAN/GAN.py:18 and the second part was to change labels to targets in sigmoid_cross_entropy_with_logits... I'm using Tensorflow 0.12.\nIf what I've done about the Prefetching is OK, then I'm closing this issue.... Thank you for your time... it's working right now and is pretty much faster than what yenchenlin has implemented... I only need time to get around the architecture...\nBut somehow, this project reminds me of Keras: abstracting some layers and providing easier to work interfaces... am I wrong?. Great! Thanks...\nby the way, I wanted to use steps_per_epoch so that I could see the image summaries in Tensorboard but this ends up pretty costly... for example after every 5 steps the logfile events.out.tfevents.148441180.... gets updated with the summaries but we also end up with huge model files like model-5.data-00000-of-00001 which is in my case a 670MB file... now this gets repeated and after 10 epochs we have 10x670MB less storage...\ncan we update the summaries just fine while just overwriting the model file so that in the end we end up with one model instead of one for every step of the way?. > A better solution will be there after step-callbacks are ready.\nThen I'll be waiting... Thanks!. Thanks... I'll take a look and hopefully will close the issue soon.... Right now I'm using the Image2Image example for colorization and just want to add a callback for converting the LAB images and saving them. I've made modifications so that the code accepts LAB images but the last part is to modify it for the callback...right now I don't know how to modify Image2Image to accept the Callback.... Sorry for dragging this but running self._setup_predictor_factory() results in calling self._predictor_factory = PredictorFactory(self.sess, self.model, self.config.predict_tower) but then I get this error : 'GANTrainer' object has no attribute 'sess'\n. Thanks so much... The problem is gone and the only thing that remains is that there's some sort of scope added to the prediction tensor's name based on the tower.\nHere's what I'm calling:\nself.trainer.get_predict_func(input_names=['luminance'], output_names=['gen/prediction'])\nand here's the naming error:\n\"The name 'towerp0/gen/prediction:0' refers to a Tensor which does not exist. The operation, 'towerp0/gen/prediction', does not exist in the graph.\"\nI read your tutorial on how to get the name of tensors in Tensorpack but how should we do it when they are combined in one scope like the code below:\nwith argscope(Deconv2D, nl=BNReLU, kernel_shape=4, stride=2):\n                return (LinearWrap(e8)\n                        .Deconv2D('deconv1', NF * 8)\n                        .Dropout()\n                        .ConcatWith(3, e7)\n                        .Deconv2D('deconv2', NF * 8)\n                        .Dropout()\n                        .ConcatWith(3, e6)\n                        .Deconv2D('deconv3', NF * 8)\n                        .Dropout()\n                        .ConcatWith(3, e5)\n                        .Deconv2D('deconv4', NF * 8)\n                        .ConcatWith(3, e4)\n                        .Deconv2D('deconv5', NF * 4)\n                        .ConcatWith(3, e3)\n                        .Deconv2D('deconv6', NF * 2)\n                        .ConcatWith(3, e2)\n                        .Deconv2D('deconv7', NF * 1)\n                        .ConcatWith(3, e1)\n                        .Deconv2D('prediction', OUT_CH)()). ",
    "Skylion007": "@PatWie I've noticed some difference on my own dataset with improved GAN. It seemed to help with my dataset in terms of yielding qualitatively better results at earlier epochs on their implementation. I maybe mistaken but isn't one important part of the paper you are missing is feature matching in section 3.1? Or have you called it something else? It also sounds like it will fix the issue you are having currently where the GAN is unstable. That sounds like possibly the most important improvement in the paper, but I could be mistaken.. Another issue with the wait infogan is written is that it's not easily configurable for nonmnist datasets. It would be nice to make it more flexible for other datasets. Other infogan repos even let you set the discriminator and generator via the command line. Expanding infogan to run on different datasets would really also show if tensorpack is flexible. Just a thought. Using infogan on the celebA dataset for instance would be a good exercise. \nAny updates on the improved GAN? Was it included in #107 or was that just the groundwork? Looking forward to trying out the improvements.. My colleague just recommended a paper on GANs: https://github.com/martinarjovsky/WassersteinGAN\nSupposedly this paper uses earth mover's distance to \"virtually guarantee\" that GANs will converge.  Would love to see this implemented in Tensorpack. @ppwwyyxx @PatWie . So aside from the reference implementation, I found an implementation that can use Tensorflow as backend. (It seems like it might be using a feed_dict or something because I get relatively low GPU utilization), and even one in Chainer.\nThe fact that this GAN is already implemented in so many frameworks less than a week after release bodes really well. Also I have tested at least one of the implementation and while it was unoptimized, I was impressed how quickly the loss dropped (even if the epochs did take a while).  So far it seems that the might have actually solved (most if not all) of the issues which would be really impressive!. Agreed. Amazing how fast the GAN research has been in recent months. This repo has some of the best most flexible GAN implementations out there. Keep up the good work. \ud83d\udc4d . @ppwwyyxx Here is the relevant paper: https://arxiv.org/pdf/1606.03498v1.pdf. > I have implemented these changes in my version:\n\nmini-batch discrimination\nswitched the labels 0 <-> 1\nlabel-smoothing\n\n@PatWie \n\nSecond comment on #105 . Like lets say I have some ground truth data. Every 10th epoch or so, I would like to print out the cross_entropy between the predicted output and the expected output. This would be data that is different from the training data and thus would not be part of the dataflow. I am little confused about how to do this in a high level sense restarting the entire python process and loading the data from the last saved model.  I would really appreciate if I could get this metric so I knew when my model is overfitting. Maybe that could even be used as a halting condition of training phase?\nTo clarify here is the scenario. You have a folder containing 256x512 ground truth images for the Image2Image GAN. How would you use that as a validation test set without killing the process and starting over every X epochs?. Also another point, I am actually not working with grayscale augments, but actually working on a dataset that has grayscale images. Any opencv function call appears to remove the last dimension (not just converting it to grayscale). I am well aware of ways to get the last dimension back and am using them, but I agree with @ppwwyyxx . Unless the operation explicitly changes the shape of an image, it should have the same behavior for multichannel and grayscale images.. As a user of this repo, I really hope that you start having a release branch as I have noticed a lot of almost daily breaking changes when using some the example code. \ud83d\udc4d . I just want a dataflow process to make it easy to scale up an algorithm to patch based input. For example, let's say you have a massive 10000x10000 px image (not uncommon for satellite imagery). \nDataflow would break image into small 256x256 patches. -> (break in batches of batchsize?) -> run network -> reassemble image by concatenating all patches into their proper place. \n^ (Also if patches overlap, it would be great to easily merge them by averaging the overlapping portions.)\nThe mapdata sounds like it might do what I want it to so I may try that. I was surprised that the repo didn't have any functions to do this since it's incredibly useful for Patch Based GANS.. To clarify it doesn't need to use the combined images in the loss function. I just want an easy way to output and reassemble the patches. I am asking because I do not know if the dataflow guarantees ordering within the batches and that they get placed in the same order for the output. \nI was using the GANs as an example. To be exact this is the paper I wanted to implement: http://www.ingentaconnect.com/content/ist/jist/2016/00000060/00000001/art00003 . (See Figure 4).\n\nTo be exact here is the figure from the paper that shows the process I want to implement.\nI apologize as I just recently got interested in deep learning so I am still learning all the appropriate terminology. I have tried using tf.extract_image_patches(), but it seems to run out of memory when I attempt on larger images in vanilla Tensorflow. I was hoping this repo could automate the process and make it easier to do the preprocessing on the fly. \nI also would like to complement you on your excellent support on this repo. You guys have had absolutely excellent response times and are willing to help people fully utilize the framework. I just wished to applaud you on that front.. Ah yes, that's what I was thinking. The issue is how to elegantly recombine them. Is there a post-callback I can use? I assume I'll need to collect the patches on the output and hold them in RAM so I was curious if there was a callback or runner that would help me do the patch recombination. (Hold N output values then recombine them perform \"some action\" on them). Bonus points if I could vary N as needed to compensate for different image sizes (not a requirement by any means, but it would be a nice addition).\nTLDR: The issue I have now is how to recombine them elegantly in the Tensorpack framework.. The staging contrib module does not appear to be in the pip build on Google's website. \nfrom tensorflow.contrib import staging is failing even though I have the required version of 1.1.0.\nFound the bug, StagingArea got moved and is in from tensorflow.python.ops.data_flow_ops import StagingArea instead of contrib.. Also a request would be if you could allow the callback to say save images of the validation set in a folder. That would be extremely useful is one of the main issues I have with the current repository. Is there a way to hack inference runner to do this currently? . @PatWie Could you exploit this fact to make a Sync / Async MultiGPU GAN trainer? I've been trying to figure out how to add one in, but I have had little luck figuring out the best way to go about it.. Seems to have caused a new issue. #276 . @ppwwyyxx This does the same thing as it being added to nightly. Check out the file they add to contrib. Since they do not appear to be moving the file and just referencing, this method would be preferable wouldn't it?\n. To underscore my point, here is the entirety of contrib staging module minus comments:\n```python\n\"\"\"contrib module containing StagingArea.\"\"\"\nfrom future import absolute_import\nfrom future import division\nfrom future import print_function\nfrom tensorflow.python.ops.data_flow_ops import StagingArea\n```. Ah, figured it out. It's the same bug the was encountered in the rate portion of DiscoGAN. get_global_step_var()'s return value needs to be wrapped in a tf.identity call.. @JesseYang I just officially added it to the Tensorpack Conv2D class with #625 so now we officially support it. . We can also try autorefactor tools to update all the examples and put deprecation warning under the old layers. Python rope seems like the best solution if we do that approach.. Any update on this front? The longer we wait to fix this, the more troublesome an issue it will become.. The question then begins if we create new layers should we support the Tensorflow arguments or the legacy Tensorpack ones? How can we easily mix the argscopes between Tensorpack and Tensorflow? Or is that what the custom getter is trying to achieve? And if Tensorpack layers are not being deprecated it becomes confusing when to use which one. Should the user register a new tf.layer or a Tensorpack layers? This seems like it is going to cause a lot of issues down the line if we don't eventually merge or deprecate the Tensorpack layers.. Also should mention that it needs to be updated every n epochs. Essentially, it would be like a scheduled hyperparam setter but the values used would be dynamically calculated based on the dataset. I could loop over it I suppose, but it's pretty inelegant and probably a good use case for a new type of callback. I was wondering if there were any callbacks that could be exploited to do this already.. Should of been more clear, it would not change. However, if you wanted to experiment about say dividing by the difference between your predictions and the actual dataset, that would be interesting. I just wanted to demonstrate their should be a way to have a different training regime happen every x epochs, possibly one that doesn't involve backprop. I guess the example I used could be hacked using the inferencerunner? Just wanted to see what you would recommend in that case.. I'd like to point out that the new paper TTUR also calculates stats across large portions of the dataset. It might be useful to integrate such a system as a feature in Tensorpack, especially if we can allow a user to define a temporary static graph to calculate it and a universal data format to store the preprocessed data.. Some other things to consider about how to potentially break and fix ModelDesc:\nPartial inference: DiscoGAN is a good example. For prediction you might want to only go from A->B.\nAdditionally, you may want to the graph to vary significantly in prediction (like running A->B generator multiple times recursively, this is currently rather cumbersome with the predictor).\nLoading in a predictor network: Networks that rely on things like Inception score for instance. PyTorch handles this really well by allowing you to just call the model inference of Inception from another model. \nInput Desc: Seems a little redundant to me, how is it different from place holders? Can I only send data to placeholder Tensors? It would be nice to be able to pump data into any part of the subgraph. In which case, what's the point of placeholders at all? Can we inject placeholders into the model on demand? Does this have negative performance implications? Does this violate the principle of least astonishment. \nEasy way to parameterize models. Additionally, well self.XXX is nonstandard, I think it can be extremely useful for building the model. CycleGAN example of using Resblocks come to mind. Defining the depth of the model should be easy to parameter meaning there should be some easy way to send model arguments. Maybe even trainers that can do parameter sweeps? Should trainers be part of the model.\nWe really should encourage people to use proper namespaces. Should we prefix entire ModelDesc with model names? What if a user want to do this.\nI know it's slightly outside the scope of Tensorpack, but can we make ModelDesc composable? Can I have models in my models? Can I have ModelDesc from stuff as simple as Resblocks, Predictor Nets? Should we have generic ModelDesc for different frameworks? Maybe we should have composer functions for common network types that don't appear in tf.layers and likely won't. Or even stuff that is higher level / more cutting edge. \nWhen and where to call collect_variables can be a little confusing. Should it be baked into the model? Should it not?\nIdea for accessing specific Tensors: Encourage Tensor named namespaces? Force users to use them in the model somehow. Maybe even draft a style guide. Will this hurt Tensorboard Visualization?\nI will say, one of my favorite things about Tensorpack is the ability to learn by example thanks to the very flushed out examples and limited symbolic functions. While I understand maintaining the latter is outside the scope of this project, implementing models can really help find flaws in stuff like trainers and ModelDesc. The example of WGAN comes to mind. We need to allow the models to be hackable. Maybe using stuff like self.XXX is fine as long it's used responsibly.\nWe might also want to mark the hacky stuff as experimental (with a decorator maybe?) and give warnings when people try to use it.  . I think I narrowed down the issue. Model saver + visualizing test set can easily take more than ten seconds. (Since they use different dataflows it thinks the training one has timed out since it hasn't received anything in 10+ seconds). Model saving alone can easily take 10 seconds if you are saving over the network. Maybe you need to add an exception to the timeout for callbacks?. Is there any easy way to add a replay buffer in Tensorpack? That seems like a useful feature that could be a nice dataflow augmentation.. Yeah, I wish there was a feature in Tensorflow for hidden namescopes or such. Regardless, I've figured out how to look at Generator and Discriminator based Tensorboards, but it can definitely be confusing at times. My biggest pete peave is the l2regularizers and conditionals which creates a long lines of disconnected ops and require zooming in a lot. Also a lot of the dataflow code could probably be a little better organized. All in all, the namescopes at last make it readable which is more than I can say for other libraries.. This stack overflow post seems relevant. More black magic with undocumented APIs and environment variables. \ud83d\udc4d . It would be really nice to have a version random resize that took in a range of pixels. Otherwise, you have to calculate the maximum size of the image, resize it to that and then calculate a downsize range to preserve as much information as possible and remove downsampling artifacts. . I am really curious how the regularizer will change since it looks like most tf.layers have a regularizer  as an argument? That seems like it would be more difficult to replace with simple function decorators. I definitely encourage using tf.layers for consistency, but I just want to make sure any regularizer I write will be future proof.. It really does deserve a ton of stars because it makes Tensorflow actually useful by adding much better support for experimentation. The dataflow and imgaug operators alone make it extremely useful and all the excellently well maintained examples make learning the code base easy. The multiGPU features are absolute icing on the cake and really help on large datasets. Also the documentation, quick bug fixes, and excellent examples are impeccable. \ud83d\udc4d . Layers are being deprecated. We are working on replacing them with tf.layers. . I think it would be a good idea to include it as it's useful documentation that would otherwise be hard to find.. Ah man, I was using those symbolic functions. They are actually incredibly useful for any type of Siamese networks, I would recommend commonly used metrics/model types should be better stored in Tensorpack. For instance, does GAN.py with it's useful and specific trainers really belong in examples? . Alternatively, someone could create that project and have it be a dependency of Tensorpack. While Tensorpack may not be the proper place for many of these loss functions, it has by far some of the cleanest and best implementations of them. I'd be happy to try to start that project if there is interest. @PatWie care to share a gist of the loss.py file?\nRegardless, I wouldn't mind seeing loss.py as part of say the example section.. I imagine posting your modified code in a GIST would help debug. . The experimental aggregation methods can be significantly more memory\nefficient. I also wrote a custom multiGPUGANTrainer that averages the\ngradient instead of the loss and found that rather difficult to do even\nwith _get_cost_and_grad.\nOn Wed, Oct 18, 2017 at 5:32 PM, Yuxin Wu notifications@github.com wrote:\n\nWhat's the reason you want to change the aggregation method? I don't\nexpect this to be changed as the default should be good.\nAfter some recent refactoring there is no _get_cost_and_grad any more (it\nwasn't a public method so I didn't take too much care on it) and the\nrelevant code to compute gradients is buried deeper.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/ppwwyyxx/tensorpack/issues/427#issuecomment-337734224,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AB9WXwBF_91rLOnRqWn-CMjDT9b9fdJIks5stm5YgaJpZM4PhlXt\n.\n. Not, it doesn't have to do with building the graph, but  it does have two\nseparate costs that both have to be optimized with custom variable lists.\nThat can be hard to access through the current interface.\n\nOn Wed, Oct 18, 2017 at 11:56 PM, Yuxin Wu notifications@github.com wrote:\n\nI'll try the aggregation methods. Writing a GAN trainer should have\nnothing to do with this method because you're supposed to build the graph\nby yourself anyway.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/ppwwyyxx/tensorpack/issues/427#issuecomment-337792796,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AB9WX1rNTLGVnvtl7ItJpoNJvxHVnUtYks5stshrgaJpZM4PhlXt\n.\n. Hmm, I haven't taken a look at the new interface. Last time I tried this was in July.. No but ModelDesc has and I really hacked it alot to get it do what I want.\nI ended up not pursuing it because the performance was really bad over\nmultiple GPUs and I just stuck with default multiGPU GAN trainer since then.\n\nOn Thu, Oct 19, 2017 at 5:12 PM, Yuxin Wu notifications@github.com wrote:\n\nWhat do you mean by new interface? GAN.py hasn't been changed much since\nMultiGPUGANTrainer was added.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/ppwwyyxx/tensorpack/issues/427#issuecomment-338039656,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AB9WXzcD1Furjo8Tblt7jyZVWUOnV0a2ks5st7tWgaJpZM4PhlXt\n.\n. But doesn't tensorpack now call tf.layers and tesnorpack naming is no longer used? or is there a backwards preserving translation somewhere I missing?. Ah, I see. NVM then, I got my local example working by modifying the script.. So I have a really hacky way I like to do this using tf.cond() and variable.assign. Basically you can either use the default value or assign the value based on whether the condition is true. This is what I have done whenever i need to decay the learning rate.. Ouch, on one hand great job for maintaining backwards compatibility. On the other hand, it's big blow to readability and documentation. ;-; Maintaining two separate keyword lists seems pretty awful and some of these hacks definitely don't follow the principle of least astonishment.\n\nMaybe a function decorator that would add an deprecation warning would be more readable? Perhaps something like the decorator in the top voted answer to this StackOverflow question?\nYou lose a lot of information and docs about what the default values are and such. It might be better just to explicitly rewrite all the args with a function decorator. You could have it take in a dict of value to remap and use a global dictionary to rewrite the args. That seems much more readable and transparent.. Sorry, that Stackoverflow comment has nothing to do with the click library, but rather adding functionality that doesn't exist within it. I'll post the relevant portions.\n```python\nimport functools\ndef rename_kwargs(replacements):\n    def actual_decorator(func):\n        @functools.wraps(func)\n        def decorated_func(*args, kwargs):\n            for internal_arg, external_arg in replacements.iteritems():\n                if external_arg in kwargs:\n                    kwargs[internal_arg] = kwargs.pop(external_arg)\n            return func(args, *kwargs)\n        return decorated_func\n    return actual_decorator\nif name == 'main':\n@rename_kwargs(different_arg='format')\ndef tester(different_arg):\n    print different_arg\n\ntester(format='test value')\n\n```\nIn the above snippet @rename_kwargs renames the keyword argument \"format\" to \"different_arg\" automatically. The decorator supports multiple key values passed in via a dict. You can augment the decorator to further modify the arguments as needed. That way when you decide to fully deprecate the old arguments you can just remove the decorators and the code will be readable and updated. Additionally, the decorator can be reused for all tf.layers argument translations and remove unnecessary code duplication. Isn't this how Tensorflow handles API changes anyhow?\n\nHow does this astonish a user who doesn't go and read source code?\nThis is more about astonishing contributors like myself and others. It just adds pretty nasty complexity and adds to the technical debt. I definitely agree we should change the arguments, but there are better ways to do that.\n\nFurthermore, supporting two separate versions of command arguments could get confusing. \nAt the very least, do we want our examples to break whenever Tensorflow changes a default option if we rely on them for the defaults? One of the best parts of Tensorpack are the clean and well maintained examples that have excellent default values for performance.. The new changes with the decorator look good. Minor style point is that parse_args might not be the most descriptive name for what the decorator does. The decorator definitely helps make it more readable.. It's pretty easy to monkey-patch Gradient Checkpointing in theory. We don't need any additional support to add it in. Gradient checkpointing is also is not a good option to have on by default because it will not work on all graphs.. Yeah it's a typo. Should be like so:\npython\nfrom tensorflow.python.ops import gradients\ndef gradients_memory(ys, xs, grad_ys=None, **kwargs):\n    return memory_saving_gradients.gradients(ys, xs, grad_ys, checkpoints='memory', **kwargs)\ngradients.__dict__[\"gradients\"] = gradients_memory\ncode is from this issue. Alternatively, are there anyway to get PyCharm to deal with Tensorpack in a friendly manner? From my preliminary research it looks PyCharm can correctly deal with all correctly. Does PyCharm really need static imports? The other option is to provide some kind of short hand to access Tensorpack like Tensorflow uses. Import tp and use tp.graph_builder.ModelDesc and such. That doesn't seem clean either though. I don't think generating all those static imports are the answer even if it's done dynamically.. Ah, so is the issue with the HAS_TF variable in the main init file? Seems like it is doing more harm than good. Surely there is a better way to separate out dataflow if that's the case or are there are constructs like try catch that editors can accurately interpret? . One solution to this would be to use the statica hack along with Werkzerg's lazy loading to have the IDE both be satisfied and also allow for the benefits of dynamic importing: inspired by this Reddit thread.. Hmm, I think you might need to use the more specific trick of deleting programmatically setting a boolean to false to fool some of the other IDEs. They may ignore constant false blocks. If it was that easy, I would expect more projects to use that instead of the statica hack. . Can't we just use an atexit hook to do this? Don't see why another linux specific dependency would be necessary.. Wouldn't signals be a more precise cross-platform way of handling it though?. https://stackoverflow.com/questions/40866576/run-atexit-when-python-process-is-killed. Can that library catch it though? I was under the impression that NOTHING can catch SIGKILL. That's the point of SIGKILL isn't it? . NVM, I understand it's a hook on the child. Still this would only solve the issue on Linux. There is a way of dealing with this on Windows, but it's far more complicated. The only reliable way would be to pole the parent from the subprocesses. The signal solution would solve pretty much ever other case though with the exception of SIGKILL and without the need for a Linux specific non-UNIX call for an edge case. Thoughts?\nAnother really clever method is the following: \n-    pass stdin pipe to child - you don't have to write any data into the stream.\n-    Child reads indefinitely from stdin until EOF. An EOF signals that the parent has gone.\n-    This is foolproof and portable way to detect when the parent has gone. \n. Are there any scenarios where it could get stuck in IO? I thought IO waits are by definition uninterruptible right? Process state 'D', if I am not mistaken?. Wouldn't the child receive an EOF? All file descriptors are closed when a process is terminated by any means right? Therefore the child would receive an EOF.. Can't we just adapt a way to pass all remaining keyword args straight to the tf.layers implementation? Would future proof it as well and we wouldn't have this issue arise again.. I have some code to this lying around somewhere. I can add it in at some point.. ",
    "nickfraser": "Thanks @PatWie for the example code. I ended up going with @ppwwyyxx 's approach, which worked after I removed references to EXTRA_SAVE_VARS_KEY in scripts/dump-model-params.py. The log got overwritten somehow, but I'm running another (also duplicate) experiment - if I end up with the same crash I'll attach the appropriate log files.. I did get the same crash. However, nothing useful appears in the logs (IMO). Also I'm using an old, modified version of tensorpack to do my own experiments. None-the-less, below is the output of the log file, in case it helps.\nlog.txt. I cannot unfortunately, it was running on an AWS machine and I wrongly assumed the log files would capture all errors. It was not a python related exception, rather something like \"unable to create thread\" or similar. If I train again, I'll pipe all of stdout/stderr to another log file. Apologies.. ",
    "andrewliao11": "hi @flashgm , I just implemented continuous a3c in pytorch here.\nLooking forward to your comments. ",
    "nesciemus": "$ python mnist-convnet.py\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\nTraceback (most recent call last):\n  File \"mnist-convnet.py\", line 19, in \n    from tensorpack import \n  File \"/home/yangchen/projects/work/binarized/tensorpack/tensorpack/init.py\", line 8, in \n    from tensorpack.train import \n  File \"/home/yangchen/projects/work/binarized/tensorpack/tensorpack/train/init.py\", line 29, in \n    global_import(module_name)\n  File \"/home/yangchen/projects/work/binarized/tensorpack/tensorpack/train/init.py\", line 13, in global_import\n    p = import(name, globals(), locals(), level=1)\n  File \"/home/yangchen/projects/work/binarized/tensorpack/tensorpack/train/base.py\", line 13, in \n    from .config import TrainConfig\n  File \"/home/yangchen/projects/work/binarized/tensorpack/tensorpack/train/config.py\", line 7, in \n    from ..callbacks.group import Callbacks\n  File \"/home/yangchen/projects/work/binarized/tensorpack/tensorpack/callbacks/init.py\", line 28, in \n    _global_import(module_name)\n  File \"/home/yangchen/projects/work/binarized/tensorpack/tensorpack/callbacks/init.py\", line 13, in _global_import\n    p = import(name, globals(), locals(), level=1)\n  File \"/home/yangchen/projects/work/binarized/tensorpack/tensorpack/callbacks/inference_runner.py\", line 11, in \n    from ..dataflow import DataFlow\n  File \"/home/yangchen/projects/work/binarized/tensorpack/tensorpack/dataflow/init.py\", line 29, in \n    _global_import(module_name)\n  File \"/home/yangchen/projects/work/binarized/tensorpack/tensorpack/dataflow/init.py\", line 16, in _global_import\n    p = import(name, globals(), locals(), level=1)\nImportError: No module named freeze\nI see there is no dependency for freeze.\nAnd it gives the same error even I installed the freeze package.\n. I see why.\nBecause I have installed the dataset package (a database interface package) before and there is a naming conflict.. ",
    "rohitgirdhar": "It seems to be fixed.\nWith the earlier version of TF, I was also getting 95% error after 1st epoch before:\n[0202 02:56:58 @stats.py:113] input_queue_size: 50\n[0202 02:56:58 @stats.py:113] l2_regularize_loss: 1.2286\n[0202 02:56:58 @stats.py:113] learning_rate: 0.1\n[0202 02:56:58 @stats.py:113] train-error-top1: 0.96814\n[0202 02:56:58 @stats.py:113] train-error-top5: 0.8822\n[0202 02:56:58 @stats.py:113] val-error-top1: 0.9553\n[0202 02:56:58 @stats.py:113] val-error-top5: 0.86338\n[0202 02:56:58 @stats.py:113] xentropy-loss: 5.7036\nBut with today's build, I get after 1st epoch:\n[0202 11:03:23 @stats.py:113] input_queue_size: 48.341\n[0202 11:03:23 @stats.py:113] l2_regularize_loss: 1.2029\n[0202 11:03:23 @stats.py:113] learning_rate: 0.1\n[0202 11:03:23 @stats.py:113] train-error-top1: 0.77943\n[0202 11:03:23 @stats.py:113] train-error-top5: 0.55875\n[0202 11:03:23 @stats.py:113] val-error-top1: 0.80062\n[0202 11:03:23 @stats.py:113] val-error-top5: 0.58646\n[0202 11:03:23 @stats.py:113] xentropy-loss: 3.9533\nThanks for figuring this, @ppwwyyxx !. I think it's just a standard SSD, I don't think we have any RAID configured on it. Though I'm far from an expert on this. PM me if you need specific model details, and I can try to figure that out.. So my images are resized to 256px, and stored as individual JPEGs on the disk (not in a tarfile). With 256 batch size over 4 GPUs, I was getting around 1 it/sec.. ",
    "akshata1391": "For Dropout, if I set isTraining = True, then during inference of the validation set, is the dropout set to isTraining=False ? . Thank you for the quick reply. ",
    "huihuiqu": "Thanks for your reply. After reading the human level control papaer, I think that in ddqn case. The frame means adding a new transition tuple   (s,a,r,s'). Thus, each step is 4 frames (action repeat). after each step(observation), the system updates itself by a minibatch(32). \nTherefore, in our case, I think that we still need to run 50,000,000 steps (5000 epochs ) to get  the result. Just to confirm that in tensorpack, we train a minibatch after obtaining a new single observation, am I right?. ",
    "pzz2011": "Sorry, I missed the requirement >= 1.0\nI switch to the tf1.0, all things goes well. \nthanks.. ",
    "mijung-kim": "Thanks! I already made it. . ",
    "duanLH": "I upgrade the tqdm, the problem solved. I think its better to add a choice to logger.auto_set_dir() to restore the last checkpoint. thanks, when i use --load, the parameters are restored by a checkpoint at 70 epoch, but  the start epoch is reset to 1. . Yes, I means that when I restore a model in epoch N, the program is start with N, but the print is start epoch 1, I think the print should be change to N. (sorry for my poor english). OK .  Tensorflow is a flexible tool,  but there are still not have a unified coding standard\u3002 thanks for your contributions very well,. ",
    "eyaler": "fastest response ever!\nbetter use your SHAPE parameter instead of hard-coding 256.\nbtw, do you think pix2pix could work well for larger images if we change say, 256->512 and 286->572. or would you also change something in the architecture?\n. For the second one we want random 90 deg rotations to 0, 90, 180, 270\nI will make a PR\nOn Mar 2, 2017 5:39 PM, \"Yuxin Wu\" notifications@github.com wrote:\n\nThe second one is just imgaug.MapImage(lambda x: cv2.transpose(x)) ?\nThe first one does look like something others may need, but this probably\nwon't come to top of my todo list very soon. Contributions are welcome.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/ppwwyyxx/tensorpack/issues/170#issuecomment-283688107,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AEOzC4T4Lqp6aQ3KKDxvhZwvblcdGlHJks5rhuKtgaJpZM4MRGK_\n.\n. i think that rotating (w,h) to (w,h) is indeed the expected behavior, since many algorithms assume a fixed input size. let me know your decision if you want a separate rot90, and if rot90 should have a different behavior. thanks!. i fixed the shifts to be random. also to make sure 90 deg rotations are pixel exact for odd and even canvas sizes, i made two small changes:\n\n\nuse np.round before int in largest_rotated_rect()\nsubtract 0.5 from center param of getRotationMatrix2D\n. thanks!. sorry if i was unclear. i wanted to get the model_file after training and pass to sample(). i ended up using:\n\n`\nmodel_file = tf.train.get_checkpoint_state(logger.LOG_DIR).model_checkpoint_path\nsample(args.data, model_file)\n`. so:\nmodel_file = os.path.join(logger.LOG_DIR, 'checkpoint'). yes. also seeing this.\nin image2image.py after \n        GANTrainer(config).train()\ni added:\n        model_file = os.path.join(logger.LOG_DIR, 'checkpoint')\n        sample(args.data, model_file)\nthis used to work. but after recent changes to master i am getting:\nTraceback (most recent call last):\n  File \"Image2Image.py\", line 230, in \n    sample(args.data, model_file)\n  File \"Image2Image.py\", line 199, in sample\n    pred = SimpleDatasetPredictor(pred, ds)\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorpack\\predict\\dataset.py\", line 65, in init\n    self.predictor = OfflinePredictor(config)\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorpack\\predict\\base.py\", line 141, in init\n    config.model.build_graph(input_placehdrs)\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorpack\\models\\model_desc.py\", line 116, in build_graph\n    self._build_graph(model_inputs)\n  File \"Image2Image.py\", line 118, in _build_graph\n    self.build_losses(real_pred, fake_pred)\n  File \"C:\\Users\\eyaler\\Dropbox\\python\\tensorpack\\examples\\GAN\\GAN.py\", line 54, in build_losses\n    add_moving_summary(self.g_loss, self.d_loss, d_accuracy, g_accuracy)\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorpack\\tfutils\\summary.py\", line 162, in add_moving_summary\n    avg_maintain_op = averager.apply(v)\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\moving_averages.py\", line 387, in apply\n    (1.0 + num_updates) / (10.0 + num_updates))\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 1392, in minimum\n    result = _op_def_lib.apply_op(\"Minimum\", x=x, y=y, name=name)\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 331, in apply_op\n    g = ops._get_graph_from_inputs(_Flatten(keywords.values()))\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3912, in _get_graph_from_inputs\n    _assert_same_graph(original_graph_element, graph_element)\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3851, in _assert_same_graph\n    \"%s must be from the same graph as %s.\" % (item, original_item))\nValueError: Tensor(\"truediv_2:0\", shape=(), dtype=float32) must be from the same graph as Tensor(\"EMA/decay:0\", shape=(), dtype=float32).. thanks. would be nice to have an option in auto_set_dir to change 'train_log' but still use the base_name logic. wow! how did you pinpoint the problem? I was debugging for hours...\nfor completeness, in my usecase i also have to do between runs:\nimport logging; logging.shutdown(). In windows i get error 32 file in use. this is when trying to write to the same log file with set_logger_dir(action='d'). i can confirm this fixed the logging issue.\n. the above merge fixes add border_value for shift and rotate image augmentation. please note that for rgb border_value should be a tuple (r,g,b). interesting... in the pytotch implementation its 1\nhttps://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/ee0a8292e2b87449c325bdb9439f90f911a0c0a1/models/networks.py#L94. they changed 0.9 to 1 in the original code, so for educational purposes would be a good idea to change as well . btw, when i played with saturation augmentation it seemed like it was changing the hue for some pixels (even when taking into account to BGR/RGB issue). Thanks. Could you provide some clues on how to apply freeze graph after using tensorpack?. i could not figure out how do i get the graphdef from the file? and how should i initialize the session?. i was stuck on this for too many days so i want to share my solution.\nrunning freeze_graph directly on the tensorpack save files yielded different results than before the freeze_graph. i had to first apply export and only then freeze. i ended up with the following solution which worked for me:\n`\n        from tensorpack.tfutils.export import ModelExport\n        from tensorflow.python.saved_model import tag_constants\n        from tensorflow.python.framework.graph_util import convert_variables_to_constants\n    tf.reset_default_graph()\n    e = ModelExport(Model(), input_node_list, output_node_list)\n    e.export(checkpoint_file, export_folder)\n\n    with tf.Session() as sess:\n        tf.saved_model.loader.load(sess, [tag_constants.SERVING], export_folder)\n        G = tf.get_default_graph().as_graph_def()\n        for node in G.node:\n            node.device = ''\n        G = convert_variables_to_constants(sess, G, output_node_list)\n        with tf.gfile.GFile(frozen_filename, 'wb') as f:\n            f.write(G.SerializeToString())\n\n`\nto infer:\n`\n        with tf.gfile.GFile(os.path.join(frozen_filename), 'rb') as f:\n            graph_def = tf.GraphDef()\n            graph_def.ParseFromString(f.read())\n    with tf.Graph().as_default() as graph:\n        tf.import_graph_def(graph_def)\n        input_tensor = graph.get_tensor_by_name('import/input_nodename:0')\n        output_tensor = graph.get_tensor_by_name('import/output_node_name:0')\n        with tf.Session(graph=graph) as sess:\n            output = sess.run(output_tensor, {input_tensor: input_image[None]})[0]\n\n`\nhopes this helps. if @ppwwyyxx is interested i can wrap this in some helper functions and make a PR\n. https://github.com/tensorflow/tensorflow/issues/13126. i edited the commits. thanks. i ended up using AugmentImageCoordinates. taking. one problem is that centerpaste only works with smaller images. we could change it to just return the image if it is larger or alternatively pad the smaller axis to equal the larger one. thanks! changing to get_variable() solved my issue. 1. the image augmentation methods do not accept batch dimension, and do not implement augment_with_params. so i would need to call get_params and then apply warpaffine myself. oops.. you could also do steps of 30, 45 etc\n. two reasons that relate to the fact that the integer division in not symmetric for positive and negative angles:\n\nif max_deg<180 and using step_deg, you could get angles with negative value > max_deg, which breaks user expectation.\nif max_deg<180 you dont get equal probabilities. say max_deg=90 and step_deg=90, you get only -90 and 0 angles, missing out on +90\n . -> catch_exceptions. \n",
    "yhlleo": "I tried the another example cifar-convnet.py, but the same problem occurred.  Before the cifar,  I have test the example HED, it runs well. \nI re-downloaded the file cifar-10-python.tar.gz  to exclude that the file is damaged.\n\nI have installed OpenCV 3.1, CUDA 8.0, cuDNN 5.1( without using anaconda ).. In fact, I traced debug the problem, it's just a problem of data reading. (The program was still in the data loading stage before the training phase, when it occurred.)\n\nI installed the OpenCV according to the guide: OpenCV: Installation in Linux.. I added some print in the original code:\n```\ntensorpack/dataflow/dataset/cifar.py\n...\n        for k in range(IMG_NUM):\n            img = data[k].reshape(3, 32, 32)\n            img = np.transpose(img, [1, 2, 0])\n            ret.append([img, label[k]])\n            #print k\n        print fname, len(ret)\n    print \"read_cifar done\"\n```\nOutput: \n...\n/home/swoda/tensorpack_data/cifar10_data/cifar-10-batches-py/data_batch_1 10000\n/home/swoda/tensorpack_data/cifar10_data/cifar-10-batches-py/data_batch_2 20000\n/home/swoda/tensorpack_data/cifar10_data/cifar-10-batches-py/data_batch_3 30000\n/home/swoda/tensorpack_data/cifar10_data/cifar-10-batches-py/data_batch_4 40000\nSegmentation fault (core dumped)\nThen, I printed the k:\n...\n5105\n5106\n5107\n5108\n5109\nSegmentation fault (core dumped)\nIt seems that the the location of the error is different sometimes.. It works by running:\npython2 -c \"from tensorpack import dataset; c = dataset.Cifar10('train')\"\nOutput:\n...\n9998\n9999\n/home/swoda/tensorpack_data/cifar10_data/cifar-10-batches-py/data_batch_5 50000\nread_cifar done. It happened when loading training data.\nIt made no effect with removing the prefetch and augmentors. . After adding import cv2 at the first line of cifar10-resnet.py and cifar1-convnet.py, the problem was solved!\nThank you very much!. I found an interesting thing that if I changed the input data:\ninput, output = input / 128.0 - 1, output / 128.0 - 1\nas:\noutput = output / 128.0 - 1.\nThen, both discrim/accuracy and gen/accuracy tended to be 0.5 during the training.\nTherefore, the operation input = input / 128.0 - 1 is unnecessary.. Thank you very much!. ",
    "asanakoy": "@ppwwyyxx, interestng. Is LMDB always faster than TFRecord? . ",
    "bradyz": "This is awesome!. perfect! exactly what i was looking for . ",
    "frolf": "running your pip command again from within the virtual environment i wish to use ( universe-starter-agent ), yielded an uninstall and reinstall, then again failing to import (even being run from the tensorpack/examples/A3C-Gym directory)\n\n(universe-starter-agent) shannon@frolfcuda: ~/tensorpack/examples/A3C-Gym$  pip install --user -U git+https://github.com/ppwwyyxx/tensorpack.git\nCollecting git+https://github.com/ppwwyyxx/tensorpack.git\n  Cloning https://github.com/ppwwyyxx/tensorpack.git to /tmp/pip-vtqoc3px-build\n  Ignoring subprocess32: markers 'python_version < \"3.0\"' don't match your environment\n  Ignoring functools32: markers 'python_version < \"3.0\"' don't match your environment\nRequirement already up-to-date: numpy in /home/shannon/anaconda3/envs/universe-starter-agent/lib/python3.5/site-packages (from tensorpack==0.1.6)\nRequirement already up-to-date: six in /home/shannon/anaconda3/envs/universe-starter-agent/lib/python3.5/site-packages (from tensorpack==0.1.6)\nRequirement already up-to-date: termcolor in /home/shannon/anaconda3/envs/universe-starter-agent/lib/python3.5/site-packages (from tensorpack==0.1.6)\nRequirement already up-to-date: tqdm>4.11.1 in /home/shannon/anaconda3/envs/universe-starter-agent/lib/python3.5/site-packages (from tensorpack==0.1.6)\nRequirement already up-to-date: msgpack-python in /home/shannon/anaconda3/envs/universe-starter-agent/lib/python3.5/site-packages (from tensorpack==0.1.6)\nRequirement already up-to-date: msgpack-numpy in /home/shannon/anaconda3/envs/universe-starter-agent/lib/python3.5/site-packages (from tensorpack==0.1.6)\nRequirement already up-to-date: pyzmq in /home/shannon/anaconda3/envs/universe-starter-agent/lib/python3.5/site-packages (from tensorpack==0.1.6)\nInstalling collected packages: tensorpack\n  Found existing installation: tensorpack 0.1.6\n    Uninstalling tensorpack-0.1.6:\n      Successfully uninstalled tensorpack-0.1.6\n  Running setup.py install for tensorpack ... done\nSuccessfully installed tensorpack-0.1.6\n(universe-starter-agent) shannon@frolfcuda:~/tensorpack/examples/A3C-Gym$ python\nPython 3.5.3 |Continuum Analytics, Inc.| (default, Feb 22 2017, 21:13:27) \n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport tensorpack\nTraceback (most recent call last):\n  File \"\", line 1, in \nImportError: No module named 'tensorpack'\n. Also I just created a new environment with conda, same error.\n\n\n\n\n\ncreated brand new environment using conda create -n packin python=3.5\n-launched environment\n\n\ninstalled opencv via 'conda install opencv'\n\n\n-installed tensorflow 1.0 via 'pip install tensorflow-gpu'\n\ninstalled tensorpack via 'pip install --user -U git+https://github.com/ppwwyyxx/tensorpack.git'  and received the following output:\n\n(packin) shannon@frolfcuda:~/tensorpack/examples/A3C-Gym$ pip install --user -U git+https://github.com/ppwwyyxx/tensorpack.git\nCollecting git+https://github.com/ppwwyyxx/tensorpack.git\n  Cloning https://github.com/ppwwyyxx/tensorpack.git to /tmp/pip-qf6uz9ca-build\n  Ignoring subprocess32: markers 'python_version < \"3.0\"' don't match your environment\n  Ignoring functools32: markers 'python_version < \"3.0\"' don't match your environment\nCollecting numpy (from tensorpack==0.1.6)\n  Using cached numpy-1.12.0-cp35-cp35m-manylinux1_x86_64.whl\nRequirement already up-to-date: six in /home/shannon/anaconda3/envs/packin/lib/python3.5/site-packages (from tensorpack==0.1.6)\nCollecting termcolor (from tensorpack==0.1.6)\nCollecting tqdm>4.11.1 (from tensorpack==0.1.6)\n  Using cached tqdm-4.11.2-py2.py3-none-any.whl\nCollecting msgpack-python (from tensorpack==0.1.6)\nCollecting msgpack-numpy (from tensorpack==0.1.6)\n  Using cached msgpack_numpy-0.3.9-py2.py3-none-any.whl\nCollecting pyzmq (from tensorpack==0.1.6)\n  Downloading pyzmq-16.0.2-cp35-cp35m-manylinux1_x86_64.whl (3.0MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.0MB 495kB/s \nInstalling collected packages: numpy, termcolor, tqdm, msgpack-python, msgpack-numpy, pyzmq, tensorpack\n  Found existing installation: tensorpack 0.1.6\n    Uninstalling tensorpack-0.1.6:\n      Successfully uninstalled tensorpack-0.1.6\n  Running setup.py install for tensorpack ... done\nSuccessfully installed msgpack-numpy-0.3.9 msgpack-python-0.4.8 numpy-1.12.0 pyzmq-16.0.2 tensorpack-0.1.6 termcolor-1.1.0 tqdm-4.11.2\n\nHowever when I run 'conda list' in this environment, tensorpack is not in the list now?\nAlso of course same general error of not importing with no module found.\n. I have since tried uninstalling both from within the environment and outside of it, reinstalling within, etc.   It claims to install successfully, but when using conda list, the package does not come up. And cannot be imported.\nBut it can be installed, but when it is uninstalled, it shows that the directory is /.local/lib/python3.5/site-packages/tensorpack-0.1.6-py3.5.egg-info\nWhich is not the virtual environment directory where other packages become installed to and can be uninstalled from.. I was using the proper pip that was associated with the virtual environment the whole time as evidenced from 'which pip' occasionally.\nI have managed to get tensorpack working properly in the virtual environment after a number of uninstalls and reinstalls.  I believe it was resolved by omitting the -user as recommended above.\nThanks for your help!, and you may consider appending the main readme re: installation with noting conda users of virtual environment should omit the --user. The following error was returned:\nshannon@frolfcuda:~/Desktop/tensorpack/examples/A3C-Gym$ ENV=Breakout-v0; ./run-atari.py --load model-24000.data-00000-of-00001 --env \"$ENV\" --episode 10 --output /home/shannon/Desktop/trashoutput\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\n[0307 14:04:13 @run-atari.py:97] Environment Name: Breakout-v0\n[2017-03-07 14:04:13,878] Making new env: Breakout-v0\n[0307 14:04:13 @varmanip.py:162] WRN [SaverRestore] ./model-24000.data-00000-of-00001 is corrected to ./model-24000 when restoring the model.\nTraceback (most recent call last):\n  File \"./run-atari.py\", line 106, in \n    session_init=SaverRestore(args.load),\n  File \"/home/shannon/anaconda3/envs/universe-starter-agent/lib/python3.5/site-packages/tensorpack/tfutils/sessinit.py\", line 90, in init\n    model_path = get_checkpoint_path(model_path)\n  File \"/home/shannon/anaconda3/envs/universe-starter-agent/lib/python3.5/site-packages/tensorpack/tfutils/varmanip.py\", line 164, in get_checkpoint_path\n    assert os.path.isfile(model_path) or os.path.isfile(model_path + '.index'), model_path\nAssertionError: ./model-24000\n. That fixed it and is running properly now. Thanks so much for your quick help and for your impressive contributions to the community. . ",
    "jeb2112": "Had a recent issue of this nature, and did not find any direct explanation so leaving this brief comment in case it may help someone. I had an Anaconda 2018.12 Python 3.7.1 environment (not virtual) and attempting to import tensorpack 0.9.1 that was installed via pip but not using --user option. tensorpack would import in python started from command line, and via jupyter notebook, but not in VS2017 community or VS code. A virtual env with Python 3.6.8 was the answer, though not sure if either or both. . ",
    "prashant905": "Also, I am simply trying to train resnet-dorefa.py with this command ./resnet-dorefa.py --dorefa 1,4,6 --data dorefa/ --gpu 0 .\nInside dorefa folder i have two folders train and val. train folder has small chunk of imagenet dataset. it has just two folders n02115913  and n02116738.  and similarly dorefa has val folder which have folder n02115913  and n02116738 containing annotations of train folders.\nThe command just gets executed without any output or errors.. My question is after running this command ./resnet-dorefa.py --data dorefa/ --gpu 0 on a CPU just to check the code with a small dataset of imagenet ILSVRC12 (like 15mb). It just run and finish without any output , so i am confused whether it should print something or show me the training status on the console. \nAlso , where does resent-dorefa.py it stores the npy files if it trains successfully? . ",
    "JesseYang": "Yes, this error only occurs during inference. I use the imagenet-resnet.py to test tf_debug in tensorpack. I try the first method, which uses SessionCreatorAdapter to create a session_creator and pass it to TrainConfig. Following the instructinos in tensorflow document, https://www.tensorflow.org/programmers_guide/debugger, I get an error when input 'run' command:\nTraceback (most recent call last):\n  File \"imagenet-resnet.py\", line 261, in <module>\n    SyncMultiGPUTrainer(config).train()\n  File \"/home/user/tensorflow_workspace/tensorpack/tensorpack/train/base.py\", line 91, in train\n    self.setup()\n  File \"/home/user/tensorflow_workspace/tensorpack/tensorpack/train/base.py\", line 127, in setup\n    self.sess.run(init_op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/wrappers/framework.py\", line 448, in run\n    run_end_resp = self.on_run_end(run_end_req)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py\", line 264, in on_run_end\n    self._dump_root, partition_graphs=partition_graphs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/lib/debug_data.py\", line 451, in __init__\n    raise IOError(\"Dump root directory %s does not exist\" % dump_root)\nIOError: Dump root directory /tmp/tfdbg_ri8gw6 does not exist\nI also tried with the tf_debug example given by tensorflow:\npython -m tensorflow.python.debug.examples.debug_mnist --debug\nand it works fine.\nThis issue https://github.com/tensorflow/tensorflow/issues/7615 mentions the similar problem, but with the example given by tensorflow and in Windows platform.. Thanks a lot! Now it works fine!!. @ppwwyyxx Thanks! The debug hook works fine.. Thanks! That works fine! Is it possible to assign different GPU to different predictors?. Thanks!. I see. Thanks! :-). Thanks!. Thanks!. Thanks. So when using multiple GPUs, the steps_per_epoch should always be set manually?. Thanks!. Thanks!. If PrefetchDataZMQ is used to start multiple processes to fetch data, will the get_data be called multiple times?. Do inferencer callback and the training graph use the same session?. I also tried to initialize all local variables by sess.run(tf.local_variables_initializer()). But it did not work.. You are right! I just find out the reason, the initializer is defined to initialize variables with scope 'metric_scope'. But the correct scope should be 'InferenceTower/metric_scope'. ",
    "neale": "Right, the model thing isn't really important. But for what its worth, it doesn't seem to register when just placed as a file as tensorpack/tensorpack/models/lstm.py\nI've seen that solution before, but it looked like it was ignoring the simulators entirely by just building a state matrix of each agent and keeping it outside of the graph. Is that going to be the best method? Or is there a better way to extract hidden states from the simulator, and control how they get fed back into the graph. \n. The probIem is that new layers don't seem to import past the top level. I did a fresh clone and pasted your example into tensorpack/tensorpack/models. \nAt the  top level I get : \n```\n\n\n\nfrom tensorpack import models\nmodels.Conv2D\n\nmodels.Lstm\n\nBut as soon as I drop into `examples` or `A3C-Gym` I get an error\nfrom tensorpack import models\nmodels.Conv2D\n\nmodels.Lstm\n'module' object has no attribute 'Lstm'\n```\n\n\n\nThanks for your help, I just have one more question. I noticed that in the LSTM example you linked to, the solution utilizes lots of inputs like this\npython\nself._inputvar_agent_indexs = InputDesc(tf.int32, [None], 'agent_index')\nself._inputvar_R = InputDesc(tf.float32, [None, None], 'R')\nself._inputvar_td = InputDesc(tf.float32, [None, None], 'td')\nself._inputvar_is_over = InputDesc(tf.int32, [None], 'is_over')\nself._inputvar_sequence_length = InputDesc(tf.int32, [None], 'sequence_length')\nAnd these are obviously done by feeding dicts with sess.run(states, feed_dict = {...})\nDoes this play nicely with the trainer/predictor thats already going in MySimulatorMaster? Or does it overwrite everything, and the workers will need to be rewritten. . \ud83d\udc4d got it, I had a virtualenv hanging around. I rebuilt the package and the layer registered fine. \nOk I'll work through that, thanks a lot. . ",
    "JamesChuanggg": "I type htop in terminal and it shows 64373mb.. Thanks! Disable tcmalloc solve the problem above.\nHowever I encountered another problem.\n[0319 01:09:44 @base.py:187] Building predictor graph towerp0 on gpu=0 ...\n[0319 01:09:44 @base.py:120] Finalize the graph, create the session ...\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0)\nTraceback (most recent call last):\n  File \"./DQN.py\", line 243, in <module>\n    QueueInputTrainer(config).train()\n  File \"/home/james847286/anaconda2/envs/tf/lib/python2.7/site-packages/tensorpack/train/base.py\", line 91, in train\n    self.setup()\n  File \"/home/james847286/anaconda2/envs/tf/lib/python2.7/site-packages/tensorpack/train/base.py\", line 123, in setup\n    self.sess = self._monitored_sess._tf_sess()  # expose the underlying session also\nAttributeError: 'MonitoredSession' object has no attribute '_tf_sess'\nSegmentation fault (core dumped)\nIt seems that we do not define _tf_sess() for self._monitored_sess.. Thanks, The fixes solve the problem! . Hi, \nThanks for solving the problem!\nMay I ask another personal question that what\u2019s the purpose for constructing \u201ctower\u201d since it occurs in several examples?\nDoes it relate to multi-thread training or something else?\nBest,\nJames\n. Thanks for replying.\nBtw Tensorpack is the best tensorflow repo I\u2019ve ever seen, great job!\n\nOn 20 Mar 2017, at 11:37 PM, Yuxin Wu notifications@github.com wrote:\nThat's the term TF guys use for data-parallel training: https://github.com/tensorflow/models/tree/master/inception#how-to-train-from-scratch https://github.com/tensorflow/models/tree/master/inception#how-to-train-from-scratch\nHere I extend the concept to use in inference as well.\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub https://github.com/ppwwyyxx/tensorpack/issues/194#issuecomment-287798369, or mute the thread https://github.com/notifications/unsubscribe-auth/ANoVprhLuidXsFvgMvacjOBqyLswJ_UTks5rnp1JgaJpZM4MhU4i.\n\n\n. This might be the last bug of DQN.\nCallback will set learning rate in designate epoch like this.\nScheduledHyperParamSetter('learning_rate',\n                          [(150, 4e-4), (250, 1e-4), (350, 5e-5)])\nHowever when it attempt to update the value, the following error occurs\n[0321 14:18:27 @param.py:144] learning_rate at epoch 151 will change to 0.00040000\nW tensorflow/core/kernels/queue_base.cc:294] _0_input_queue: Skipping cancelled enqueue attempt with queue not closed\nTraceback (most recent call last):\n[0321 14:18:27 @input_data.py:124] EnqueueThread Exited.\n  File \"./DQN.py\", line 250, in <module>\n    DQN_Trainer(config).train()\n  File \"/home/james847286/Work/RL/DeepQNetwork/tensorpack/train/dqn_trainer.py\", line 97, in train\n    self.main_loop()\n  File \"/home/james847286/Work/RL/DeepQNetwork/tensorpack/train/dqn_trainer.py\", line 171, in main_loop\n    self._callbacks.trigger_epoch()\n  File \"/home/james847286/Work/RL/DeepQNetwork/tensorpack/callbacks/base.py\", line 121, in trigger_epoch\n    self._trigger_epoch()\n  File \"/home/james847286/Work/RL/DeepQNetwork/tensorpack/callbacks/group.py\", line 104, in _trigger_epoch\n    cb.trigger_epoch()\n  File \"/home/james847286/Work/RL/DeepQNetwork/tensorpack/callbacks/base.py\", line 121, in trigger_epoch\n    self._trigger_epoch()\n  File \"/home/james847286/Work/RL/DeepQNetwork/tensorpack/callbacks/base.py\", line 181, in _trigger_epoch\n    self.trigger()\n  File \"/home/james847286/Work/RL/DeepQNetwork/tensorpack/callbacks/base.py\", line 168, in trigger\n    self._trigger()\n  File \"/home/james847286/Work/RL/DeepQNetwork/tensorpack/callbacks/param.py\", line 160, in _trigger\n    self._set_param()\n  File \"/home/james847286/Work/RL/DeepQNetwork/tensorpack/callbacks/param.py\", line 168, in _set_param\n    self.param.set_value(v)\n  File \"/home/james847286/Work/RL/DeepQNetwork/tensorpack/callbacks/param.py\", line 79, in set_value\n    self.var.load(v)\nAttributeError: 'Variable' object has no attribute 'load'\nSegmentation fault (core dumped). ",
    "samikha": "If in the ResNet example directory I run:\n ./imagenet-resnet.py --data dataset_ILSVRC2012/   --gpu 0\nI get the following:\n```\nFile \"/local/tensorflow350_gpu/lib/python3.5/site-packages/tensorpack/dataflow/dataset/ilsvrc.py\", line 157, in init\n    meta = ILSVRCMeta(meta_dir)\n  File \"/local/tensorflow350_gpu/lib/python3.5/site-packages/tensorpack/dataflow/dataset/ilsvrc.py\", line 33, in init\n    self.caffepb = get_caffe_pb()\n  File \"/local/tensorflow350_gpu/lib/python3.5/site-packages/tensorpack/utils/loadcaffe.py\", line 130, in get_caffe_pb\n    return imp.load_source('caffepb', caffe_pb_file)\n  File \"/local/python/3.5.0/lib/python3.5/imp.py\", line 172, in load_source\n    module = _load(spec)\n  File \"\", line 693, in _load\n  File \"\", line 673, in _load_unlocked\n  File \"\", line 662, in exec_module\n  File \"\", line 222, in _call_with_frames_removed\n  File \"/local/tensorpack_data/caffe/caffe_pb2.py\", line 17, in \n    serialized_pb='\\n\\x0b\\x63\\x61\\x66\\x66\\x65.proto\\x12\\x05\\x63\\x61\\x66\\x66\\x65\\\"\\x1c\\n\\tBlobShape\\x12\\x0f\\n\\x03\\x64im\\x18\\x01 \\x03(\\x03\\x42\\x02\\x10\\x01\\\"\\xcc\\x01\\n\\tBlobProto\\x12\\x1f\\n\\x05shape\\x18\\x07 \\x01(\\x0b\\x32\\x10.caffe.BlobShape\\x12\\x10\\n\\x04\\x64\\x61ta\\x18\\x05 \\x03(\\x02\\x42\\x02\\x10\\x01\\x12\\x10\\n\\x04\\x64iff\\x18\\x06 \\x03(\\x02\\x42\\x02\\x10\\x01\\x12\\x17\\n\\x0b\\x64ouble_data\\x18\\x08 \\x03(\\x01\\x42\\x02\\x10\\x01\\x12\\x17\\n\\x0b\\x64ouble_diff\\x18\\t \\x03(\\x01\\x42\\x02\\x10\\x01\\x12\\x0e\\n\\x03num\\x18\\x01 \\x01(\\x05:\\x01\\x30\\x12\\x13\\n\\x08\\x63hannels\\x18\\x02 \\x01(\\x05:\\x01\\x30\\x12\\x11\\n\\x06height\\x18\\x03 \\x01(\\x05:\\x01\\x30\\x12\\x10\\n\\x05width\\x18\\x04 \\x01(\\x05:\\x01\\x30\\\"2\\n\\x0f\\x42lobProtoVector\\x12\\x1f\\n\\x05\\x62lobs\\x18\\x01 \\x03(\\x0b\\x32\\x10.caffe.BlobProto\\\"\\x81\\x01\\n\\x05\\x44\\x61tum\\x12\\x10\\n\\x08\\x63hannels\\x18\\x01 \\x01(\\x05\\x12\\x0e\\n\\x06height\\x18\\x02 \\x01(\\x05\\x12\\r\\n\\x05width\\x18\\x03 \\x01(\\x05\\x12\\x0c\\n\\x04\\x64\\x61ta\\x18\\x04 \\x01(\\x0c\\x12\\r\\n\\x05label\\x18\\x05 \\x01(\\x05\\x12\\x12\\n\\nfloat_data\\x18\\x06 \\x03(\\x02\\x12\\x16\\n\\x07\\x65ncoded\\x18\\x07 \\x01(\\x08:\\x05\\x66\\x61lse\\\"\\x8a\\x02\\n\\x0f\\x46illerParameter\\x12\\x16\\n\\x04type\\x18\\x01 \\x01(\\t:\\x08\\x63onstant\\x12\\x10\\n\\x05value\\x18\\x02 \\x01(\\x02:\\x01\\x30\\x12\\x0e\\n\\x03min\\x18\\x03 \\x01(\\x02:\\x01\\x30\\x12\\x0e\\n\\x03max\\x18\\x04 \\x01(\\x02:\\x01\\x31\\x12\\x0f\\n\\x04mean\\x18\\x05 \\x01(\\x02:\\x01\\x30\\x12\\x0e\\n\\x03std\\x18\\x06 \\x01(\\x02:\\x01\\x31\\x12\\x12\\n\\x06sparse\\x18\\x07 \\x01(\\x05:\\x02-1\\x12\\x42\\n\\rvariance_norm\\x18\\x08 \\x01(\\x0e\\x32#.caffe.FillerParameter.VarianceNorm:\\x06\\x46\\x41N_IN\\\"4\\n\\x0cVarianceNorm\\x12\\n\\n\\x06\\x46\\x41N_IN\\x10\\x00\\x12\\x0b\\n\\x07\\x46\\x41N_OUT\\x10\\x01\\x12\\x0b\\n\\x07\\x41VERAGE\\x10\\x02\\\"\\x8e\\x02\\n\\x0cNetParameter\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05input\\x18\\x03 \\x03(\\t\\x12%\\n\\x0binput_shape\\x18\\x08 \\x03(\\x0b\\x32\\x10.caffe.BlobShape\\x12\\x11\\n\\tinput_dim\\x18\\x04 \\x03(\\x05\\x12\\x1d\\n\\x0e\\x66orce_backward\\x18\\x05 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x1e\\n\\x05state\\x18\\x06 \\x01(\\x0b\\x32\\x0f.caffe.NetState\\x12\\x19\\n\\ndebug_info\\x18\\x07 \\x01(\\x08:\\x05\\x66\\x61lse\\x12$\\n\\x05layer\\x18\\x64 \\x03(\\x0b\\x32\\x15.caffe.LayerParameter\\x12\\'\\n\\x06layers\\x18\\x02 \\x03(\\x0b\\x32\\x17.caffe.V1LayerParameter\\\"\\xc3\\n\\n\\x0fSolverParameter\\x12\\x0b\\n\\x03net\\x18\\x18 \\x01(\\t\\x12&\\n\\tnet_param\\x18\\x19 \\x01(\\x0b\\x32\\x13.caffe.NetParameter\\x12\\x11\\n\\ttrain_net\\x18\\x01 \\x01(\\t\\x12\\x10\\n\\x08test_net\\x18\\x02 \\x03(\\t\\x12,\\n\\x0ftrain_net_param\\x18\\x15 \\x01(\\x0b\\x32\\x13.caffe.NetParameter\\x12+\\n\\x0etest_net_param\\x18\\x16 \\x03(\\x0b\\x32\\x13.caffe.NetParameter\\x12$\\n\\x0btrain_state\\x18\\x1a \\x01(\\x0b\\x32\\x0f.caffe.NetState\\x12#\\n\\ntest_state\\x18\\x1b \\x03(\\x0b\\x32\\x0f.caffe.NetState\\x12\\x11\\n\\ttest_iter\\x18\\x03 \\x03(\\x05\\x12\\x18\\n\\rtest_interval\\x18\\x04 \\x01(\\x05:\\x01\\x30\\x12 \\n\\x11test_compute_loss\\x18\\x13 \\x01(\\x08:\\x05\\x66\\x61lse\\x12!\\n\\x13test_initialization\\x18  \\x01(\\x08:\\x04true\\x12\\x0f\\n\\x07\\x62\\x61se_lr\\x18\\x05 \\x01(\\x02\\x12\\x0f\\n\\x07\\x64isplay\\x18\\x06 \\x01(\\x05\\x12\\x17\\n\\x0c\\x61verage_loss\\x18! \\x01(\\x05:\\x01\\x31\\x12\\x10\\n\\x08max_iter\\x18\\x07 \\x01(\\x05\\x12\\x14\\n\\titer_size\\x18$ \\x01(\\x05:\\x01\\x31\\x12\\x11\\n\\tlr_policy\\x18\\x08 \\x01(\\t\\x12\\r\\n\\x05gamma\\x18\\t \\x01(\\x02\\x12\\r\\n\\x05power\\x18\\n \\x01(\\x02\\x12\\x10\\n\\x08momentum\\x18\\x0b \\x01(\\x02\\x12\\x14\\n\\x0cweight_decay\\x18\\x0c \\x01(\\x02\\x12\\x1f\\n\\x13regularization_type\\x18\\x1d \\x01(\\t:\\x02L2\\x12\\x10\\n\\x08stepsize\\x18\\r \\x01(\\x05\\x12\\x11\\n\\tstepvalue\\x18\\\" \\x03(\\x05\\x12\\x1a\\n\\x0e\\x63lip_gradients\\x18# \\x01(\\x02:\\x02-1\\x12\\x13\\n\\x08snapshot\\x18\\x0e \\x01(\\x05:\\x01\\x30\\x12\\x17\\n\\x0fsnapshot_prefix\\x18\\x0f \\x01(\\t\\x12\\x1c\\n\\rsnapshot_diff\\x18\\x10 \\x01(\\x08:\\x05\\x66\\x61lse\\x12K\\n\\x0fsnapshot_format\\x18% \\x01(\\x0e\\x32%.caffe.SolverParameter.SnapshotFormat:\\x0b\\x42INARYPROTO\\x12;\\n\\x0bsolver_mode\\x18\\x11 \\x01(\\x0e\\x32!.caffe.SolverParameter.SolverMode:\\x03GPU\\x12\\x14\\n\\tdevice_id\\x18\\x12 \\x01(\\x05:\\x01\\x30\\x12\\x17\\n\\x0brandom_seed\\x18\\x14 \\x01(\\x03:\\x02-1\\x12\\x11\\n\\x04type\\x18( \\x01(\\t:\\x03SGD\\x12\\x14\\n\\x05\\x64\\x65lta\\x18\\x1f \\x01(\\x02:\\x05\\x31\\x65-08\\x12\\x18\\n\\tmomentum2\\x18\\' \\x01(\\x02:\\x05\\x30.999\\x12\\x17\\n\\trms_decay\\x18& \\x01(\\x02:\\x04\\x30.99\\x12\\x19\\n\\ndebug_info\\x18\\x17 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\\"\\n\\x14snapshot_after_train\\x18\\x1c \\x01(\\x08:\\x04true\\x12;\\n\\x0bsolver_type\\x18\\x1e \\x01(\\x0e\\x32!.caffe.SolverParameter.SolverType:\\x03SGD\\x12\\x1f\\n\\x11layer_wise_reduce\\x18) \\x01(\\x08:\\x04true\\\"+\\n\\x0eSnapshotFormat\\x12\\x08\\n\\x04HDF5\\x10\\x00\\x12\\x0f\\n\\x0b\\x42INARYPROTO\\x10\\x01\\\"\\x1e\\n\\nSolverMode\\x12\\x07\\n\\x03\\x43PU\\x10\\x00\\x12\\x07\\n\\x03GPU\\x10\\x01\\\"U\\n\\nSolverType\\x12\\x07\\n\\x03SGD\\x10\\x00\\x12\\x0c\\n\\x08NESTEROV\\x10\\x01\\x12\\x0b\\n\\x07\\x41\\x44\\x41GRAD\\x10\\x02\\x12\\x0b\\n\\x07RMSPROP\\x10\\x03\\x12\\x0c\\n\\x08\\x41\\x44\\x41\\x44\\x45LTA\\x10\\x04\\x12\\x08\\n\\x04\\x41\\x44\\x41M\\x10\\x05\\\"l\\n\\x0bSolverState\\x12\\x0c\\n\\x04iter\\x18\\x01 \\x01(\\x05\\x12\\x13\\n\\x0blearned_net\\x18\\x02 \\x01(\\t\\x12!\\n\\x07history\\x18\\x03 \\x03(\\x0b\\x32\\x10.caffe.BlobProto\\x12\\x17\\n\\x0c\\x63urrent_step\\x18\\x04 \\x01(\\x05:\\x01\\x30\\\"N\\n\\x08NetState\\x12!\\n\\x05phase\\x18\\x01 \\x01(\\x0e\\x32\\x0c.caffe.Phase:\\x04TEST\\x12\\x10\\n\\x05level\\x18\\x02 \\x01(\\x05:\\x01\\x30\\x12\\r\\n\\x05stage\\x18\\x03 \\x03(\\t\\\"s\\n\\x0cNetStateRule\\x12\\x1b\\n\\x05phase\\x18\\x01 \\x01(\\x0e\\x32\\x0c.caffe.Phase\\x12\\x11\\n\\tmin_level\\x18\\x02 \\x01(\\x05\\x12\\x11\\n\\tmax_level\\x18\\x03 \\x01(\\x05\\x12\\r\\n\\x05stage\\x18\\x04 \\x03(\\t\\x12\\x11\\n\\tnot_stage\\x18\\x05 \\x03(\\t\\\"\\xa3\\x01\\n\\tParamSpec\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x31\\n\\nshare_mode\\x18\\x02 \\x01(\\x0e\\x32\\x1d.caffe.ParamSpec.DimCheckMode\\x12\\x12\\n\\x07lr_mult\\x18\\x03 \\x01(\\x02:\\x01\\x31\\x12\\x15\\n\\ndecay_mult\\x18\\x04 \\x01(\\x02:\\x01\\x31\\\"\\n\\x0c\\x44imCheckMode\\x12\\n\\n\\x06STRICT\\x10\\x00\\x12\\x0e\\n\\nPERMISSIVE\\x10\\x01\\\"\\x82\\x14\\n\\x0eLayerParameter\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x0c\\n\\x04type\\x18\\x02 \\x01(\\t\\x12\\x0e\\n\\x06\\x62ottom\\x18\\x03 \\x03(\\t\\x12\\x0b\\n\\x03top\\x18\\x04 \\x03(\\t\\x12\\x1b\\n\\x05phase\\x18\\n \\x01(\\x0e\\x32\\x0c.caffe.Phase\\x12\\x13\\n\\x0bloss_weight\\x18\\x05 \\x03(\\x02\\x12\\x1f\\n\\x05param\\x18\\x06 \\x03(\\x0b\\x32\\x10.caffe.ParamSpec\\x12\\x1f\\n\\x05\\x62lobs\\x18\\x07 \\x03(\\x0b\\x32\\x10.caffe.BlobProto\\x12\\x16\\n\\x0epropagate_down\\x18\\x0b \\x03(\\x08\\x12$\\n\\x07include\\x18\\x08 \\x03(\\x0b\\x32\\x13.caffe.NetStateRule\\x12$\\n\\x07\\x65xclude\\x18\\t \\x03(\\x0b\\x32\\x13.caffe.NetStateRule\\x12\\x37\\n\\x0ftransform_param\\x18\\x64 \\x01(\\x0b\\x32\\x1e.caffe.TransformationParameter\\x12(\\n\\nloss_param\\x18\\x65 \\x01(\\x0b\\x32\\x14.caffe.LossParameter\\x12\\x30\\n\\x0e\\x61\\x63\\x63uracy_param\\x18\\x66 \\x01(\\x0b\\x32\\x18.caffe.AccuracyParameter\\x12,\\n\\x0c\\x61rgmax_param\\x18g \\x01(\\x0b\\x32\\x16.caffe.ArgMaxParameter\\x12\\x34\\n\\x10\\x62\\x61tch_norm_param\\x18\\x8b\\x01 \\x01(\\x0b\\x32\\x19.caffe.BatchNormParameter\\x12)\\n\\nbias_param\\x18\\x8d\\x01 \\x01(\\x0b\\x32\\x14.caffe.BiasParameter\\x12,\\n\\x0c\\x63oncat_param\\x18h \\x01(\\x0b\\x32\\x16.caffe.ConcatParameter\\x12?\\n\\x16\\x63ontrastive_loss_param\\x18i \\x01(\\x0b\\x32\\x1f.caffe.ContrastiveLossParameter\\x12\\x36\\n\\x11\\x63onvolution_param\\x18j \\x01(\\x0b\\x32\\x1b.caffe.ConvolutionParameter\\x12)\\n\\ncrop_param\\x18\\x90\\x01 \\x01(\\x0b\\x32\\x14.caffe.CropParameter\\x12(\\n\\ndata_param\\x18k \\x01(\\x0b\\x32\\x14.caffe.DataParameter\\x12.\\n\\rdropout_param\\x18l \\x01(\\x0b\\x32\\x17.caffe.DropoutParameter\\x12\\x33\\n\\x10\\x64ummy_data_param\\x18m \\x01(\\x0b\\x32\\x19.caffe.DummyDataParameter\\x12.\\n\\reltwise_param\\x18n \\x01(\\x0b\\x32\\x17.caffe.EltwiseParameter\\x12\\'\\n\\telu_param\\x18\\x8c\\x01 \\x01(\\x0b\\x32\\x13.caffe.ELUParameter\\x12+\\n\\x0b\\x65mbed_param\\x18\\x89\\x01 \\x01(\\x0b\\x32\\x15.caffe.EmbedParameter\\x12&\\n\\texp_param\\x18o \\x01(\\x0b\\x32\\x13.caffe.ExpParameter\\x12/\\n\\rflatten_param\\x18\\x87\\x01 \\x01(\\x0b\\x32\\x17.caffe.FlattenParameter\\x12\\x31\\n\\x0fhdf5_data_param\\x18p \\x01(\\x0b\\x32\\x18.caffe.HDF5DataParameter\\x12\\x35\\n\\x11hdf5_output_param\\x18q \\x01(\\x0b\\x32\\x1a.caffe.HDF5OutputParameter\\x12\\x33\\n\\x10hinge_loss_param\\x18r \\x01(\\x0b\\x32\\x19.caffe.HingeLossParameter\\x12\\x33\\n\\x10image_data_param\\x18s \\x01(\\x0b\\x32\\x19.caffe.ImageDataParameter\\x12\\x39\\n\\x13infogain_loss_param\\x18t \\x01(\\x0b\\x32\\x1c.caffe.InfogainLossParameter\\x12\\x39\\n\\x13inner_product_param\\x18u \\x01(\\x0b\\x32\\x1c.caffe.InnerProductParameter\\x12+\\n\\x0binput_param\\x18\\x8f\\x01 \\x01(\\x0b\\x32\\x15.caffe.InputParameter\\x12\\'\\n\\tlog_param\\x18\\x86\\x01 \\x01(\\x0b\\x32\\x13.caffe.LogParameter\\x12&\\n\\tlrn_param\\x18v \\x01(\\x0b\\x32\\x13.caffe.LRNParameter\\x12\\x35\\n\\x11memory_data_param\\x18w \\x01(\\x0b\\x32\\x1a.caffe.MemoryDataParameter\\x12&\\n\\tmvn_param\\x18x \\x01(\\x0b\\x32\\x13.caffe.MVNParameter\\x12\\x33\\n\\x0fparameter_param\\x18\\x91\\x01 \\x01(\\x0b\\x32\\x19.caffe.ParameterParameter\\x12.\\n\\rpooling_param\\x18y \\x01(\\x0b\\x32\\x17.caffe.PoolingParameter\\x12\\n\\x0bpower_param\\x18z \\x01(\\x0b\\x32\\x15.caffe.PowerParameter\\x12+\\n\\x0bprelu_param\\x18\\x83\\x01 \\x01(\\x0b\\x32\\x15.caffe.PReLUParameter\\x12-\\n\\x0cpython_param\\x18\\x82\\x01 \\x01(\\x0b\\x32\\x16.caffe.PythonParameter\\x12\\x33\\n\\x0frecurrent_param\\x18\\x92\\x01 \\x01(\\x0b\\x32\\x19.caffe.RecurrentParameter\\x12\\x33\\n\\x0freduction_param\\x18\\x88\\x01 \\x01(\\x0b\\x32\\x19.caffe.ReductionParameter\\x12(\\n\\nrelu_param\\x18{ \\x01(\\x0b\\x32\\x14.caffe.ReLUParameter\\x12/\\n\\rreshape_param\\x18\\x85\\x01 \\x01(\\x0b\\x32\\x17.caffe.ReshapeParameter\\x12+\\n\\x0bscale_param\\x18\\x8e\\x01 \\x01(\\x0b\\x32\\x15.caffe.ScaleParameter\\x12.\\n\\rsigmoid_param\\x18| \\x01(\\x0b\\x32\\x17.caffe.SigmoidParameter\\x12.\\n\\rsoftmax_param\\x18} \\x01(\\x0b\\x32\\x17.caffe.SoftmaxParameter\\x12\\'\\n\\tspp_param\\x18\\x84\\x01 \\x01(\\x0b\\x32\\x13.caffe.SPPParameter\\x12\\n\\x0bslice_param\\x18~ \\x01(\\x0b\\x32\\x15.caffe.SliceParameter\\x12(\\n\\ntanh_param\\x18\\x7f \\x01(\\x0b\\x32\\x14.caffe.TanHParameter\\x12\\x33\\n\\x0fthreshold_param\\x18\\x80\\x01 \\x01(\\x0b\\x32\\x19.caffe.ThresholdParameter\\x12)\\n\\ntile_param\\x18\\x8a\\x01 \\x01(\\x0b\\x32\\x14.caffe.TileParameter\\x12\\x36\\n\\x11window_data_param\\x18\\x81\\x01 \\x01(\\x0b\\x32\\x1a.caffe.WindowDataParameter\\\"\\xb6\\x01\\n\\x17TransformationParameter\\x12\\x10\\n\\x05scale\\x18\\x01 \\x01(\\x02:\\x01\\x31\\x12\\x15\\n\\x06mirror\\x18\\x02 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x14\\n\\tcrop_size\\x18\\x03 \\x01(\\r:\\x01\\x30\\x12\\x11\\n\\tmean_file\\x18\\x04 \\x01(\\t\\x12\\x12\\n\\nmean_value\\x18\\x05 \\x03(\\x02\\x12\\x1a\\n\\x0b\\x66orce_color\\x18\\x06 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x19\\n\\nforce_gray\\x18\\x07 \\x01(\\x08:\\x05\\x66\\x61lse\\\"\\xc2\\x01\\n\\rLossParameter\\x12\\x14\\n\\x0cignore_label\\x18\\x01 \\x01(\\x05\\x12\\x44\\n\\rnormalization\\x18\\x03 \\x01(\\x0e\\x32&.caffe.LossParameter.NormalizationMode:\\x05VALID\\x12\\x11\\n\\tnormalize\\x18\\x02 \\x01(\\x08\\\"B\\n\\x11NormalizationMode\\x12\\x08\\n\\x04\\x46ULL\\x10\\x00\\x12\\t\\n\\x05VALID\\x10\\x01\\x12\\x0e\\n\\nBATCH_SIZE\\x10\\x02\\x12\\x08\\n\\x04NONE\\x10\\x03\\\"L\\n\\x11\\x41\\x63\\x63uracyParameter\\x12\\x10\\n\\x05top_k\\x18\\x01 \\x01(\\r:\\x01\\x31\\x12\\x0f\\n\\x04\\x61xis\\x18\\x02 \\x01(\\x05:\\x01\\x31\\x12\\x14\\n\\x0cignore_label\\x18\\x03 \\x01(\\x05\\\"M\\n\\x0f\\x41rgMaxParameter\\x12\\x1a\\n\\x0bout_max_val\\x18\\x01 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x10\\n\\x05top_k\\x18\\x02 \\x01(\\r:\\x01\\x31\\x12\\x0c\\n\\x04\\x61xis\\x18\\x03 \\x01(\\x05\\\"9\\n\\x0f\\x43oncatParameter\\x12\\x0f\\n\\x04\\x61xis\\x18\\x02 \\x01(\\x05:\\x01\\x31\\x12\\x15\\n\\nconcat_dim\\x18\\x01 \\x01(\\r:\\x01\\x31\\\"j\\n\\x12\\x42\\x61tchNormParameter\\x12\\x18\\n\\x10use_global_stats\\x18\\x01 \\x01(\\x08\\x12&\\n\\x17moving_average_fraction\\x18\\x02 \\x01(\\x02:\\x05\\x30.999\\x12\\x12\\n\\x03\\x65ps\\x18\\x03 \\x01(\\x02:\\x05\\x31\\x65-05\\\"]\\n\\rBiasParameter\\x12\\x0f\\n\\x04\\x61xis\\x18\\x01 \\x01(\\x05:\\x01\\x31\\x12\\x13\\n\\x08num_axes\\x18\\x02 \\x01(\\x05:\\x01\\x31\\x12&\\n\\x06\\x66iller\\x18\\x03 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\\"L\\n\\x18\\x43ontrastiveLossParameter\\x12\\x11\\n\\x06margin\\x18\\x01 \\x01(\\x02:\\x01\\x31\\x12\\x1d\\n\\x0elegacy_version\\x18\\x02 \\x01(\\x08:\\x05\\x66\\x61lse\\\"\\xfc\\x03\\n\\x14\\x43onvolutionParameter\\x12\\x12\\n\\nnum_output\\x18\\x01 \\x01(\\r\\x12\\x17\\n\\tbias_term\\x18\\x02 \\x01(\\x08:\\x04true\\x12\\x0b\\n\\x03pad\\x18\\x03 \\x03(\\r\\x12\\x13\\n\\x0bkernel_size\\x18\\x04 \\x03(\\r\\x12\\x0e\\n\\x06stride\\x18\\x06 \\x03(\\r\\x12\\x10\\n\\x08\\x64ilation\\x18\\x12 \\x03(\\r\\x12\\x10\\n\\x05pad_h\\x18\\t \\x01(\\r:\\x01\\x30\\x12\\x10\\n\\x05pad_w\\x18\\n \\x01(\\r:\\x01\\x30\\x12\\x10\\n\\x08kernel_h\\x18\\x0b \\x01(\\r\\x12\\x10\\n\\x08kernel_w\\x18\\x0c \\x01(\\r\\x12\\x10\\n\\x08stride_h\\x18\\r \\x01(\\r\\x12\\x10\\n\\x08stride_w\\x18\\x0e \\x01(\\r\\x12\\x10\\n\\x05group\\x18\\x05 \\x01(\\r:\\x01\\x31\\x12-\\n\\rweight_filler\\x18\\x07 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\x12+\\n\\x0b\\x62ias_filler\\x18\\x08 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\x12;\\n\\x06\\x65ngine\\x18\\x0f \\x01(\\x0e\\x32\\\".caffe.ConvolutionParameter.Engine:\\x07\\x44\\x45\\x46\\x41ULT\\x12\\x0f\\n\\x04\\x61xis\\x18\\x10 \\x01(\\x05:\\x01\\x31\\x12\\x1e\\n\\x0f\\x66orce_nd_im2col\\x18\\x11 \\x01(\\x08:\\x05\\x66\\x61lse\\\"+\\n\\x06\\x45ngine\\x12\\x0b\\n\\x07\\x44\\x45\\x46\\x41ULT\\x10\\x00\\x12\\t\\n\\x05\\x43\\x41\\x46\\x46\\x45\\x10\\x01\\x12\\t\\n\\x05\\x43UDNN\\x10\\x02\\\"0\\n\\rCropParameter\\x12\\x0f\\n\\x04\\x61xis\\x18\\x01 \\x01(\\x05:\\x01\\x32\\x12\\x0e\\n\\x06offset\\x18\\x02 \\x03(\\r\\\"\\xa4\\x02\\n\\rDataParameter\\x12\\x0e\\n\\x06source\\x18\\x01 \\x01(\\t\\x12\\x12\\n\\nbatch_size\\x18\\x04 \\x01(\\r\\x12\\x14\\n\\trand_skip\\x18\\x07 \\x01(\\r:\\x01\\x30\\x12\\x31\\n\\x07\\x62\\x61\\x63kend\\x18\\x08 \\x01(\\x0e\\x32\\x17.caffe.DataParameter.DB:\\x07LEVELDB\\x12\\x10\\n\\x05scale\\x18\\x02 \\x01(\\x02:\\x01\\x31\\x12\\x11\\n\\tmean_file\\x18\\x03 \\x01(\\t\\x12\\x14\\n\\tcrop_size\\x18\\x05 \\x01(\\r:\\x01\\x30\\x12\\x15\\n\\x06mirror\\x18\\x06 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\\"\\n\\x13\\x66orce_encoded_color\\x18\\t \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x13\\n\\x08prefetch\\x18\\n \\x01(\\r:\\x01\\x34\\\"\\x1b\\n\\x02\\x44\\x42\\x12\\x0b\\n\\x07LEVELDB\\x10\\x00\\x12\\x08\\n\\x04LMDB\\x10\\x01\\\".\\n\\x10\\x44ropoutParameter\\x12\\x1a\\n\\rdropout_ratio\\x18\\x01 \\x01(\\x02:\\x03\\x30.5\\\"\\xa0\\x01\\n\\x12\\x44ummyDataParameter\\x12+\\n\\x0b\\x64\\x61ta_filler\\x18\\x01 \\x03(\\x0b\\x32\\x16.caffe.FillerParameter\\x12\\x1f\\n\\x05shape\\x18\\x06 \\x03(\\x0b\\x32\\x10.caffe.BlobShape\\x12\\x0b\\n\\x03num\\x18\\x02 \\x03(\\r\\x12\\x10\\n\\x08\\x63hannels\\x18\\x03 \\x03(\\r\\x12\\x0e\\n\\x06height\\x18\\x04 \\x03(\\r\\x12\\r\\n\\x05width\\x18\\x05 \\x03(\\r\\\"\\xa5\\x01\\n\\x10\\x45ltwiseParameter\\x12\\x39\\n\\toperation\\x18\\x01 \\x01(\\x0e\\x32!.caffe.EltwiseParameter.EltwiseOp:\\x03SUM\\x12\\r\\n\\x05\\x63oeff\\x18\\x02 \\x03(\\x02\\x12\\x1e\\n\\x10stable_prod_grad\\x18\\x03 \\x01(\\x08:\\x04true\\\"\\'\\n\\tEltwiseOp\\x12\\x08\\n\\x04PROD\\x10\\x00\\x12\\x07\\n\\x03SUM\\x10\\x01\\x12\\x07\\n\\x03MAX\\x10\\x02\\\" \\n\\x0c\\x45LUParameter\\x12\\x10\\n\\x05\\x61lpha\\x18\\x01 \\x01(\\x02:\\x01\\x31\\\"\\xac\\x01\\n\\x0e\\x45mbedParameter\\x12\\x12\\n\\nnum_output\\x18\\x01 \\x01(\\r\\x12\\x11\\n\\tinput_dim\\x18\\x02 \\x01(\\r\\x12\\x17\\n\\tbias_term\\x18\\x03 \\x01(\\x08:\\x04true\\x12-\\n\\rweight_filler\\x18\\x04 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\x12+\\n\\x0b\\x62ias_filler\\x18\\x05 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\\"D\\n\\x0c\\x45xpParameter\\x12\\x10\\n\\x04\\x62\\x61se\\x18\\x01 \\x01(\\x02:\\x02-1\\x12\\x10\\n\\x05scale\\x18\\x02 \\x01(\\x02:\\x01\\x31\\x12\\x10\\n\\x05shift\\x18\\x03 \\x01(\\x02:\\x01\\x30\\\"9\\n\\x10\\x46lattenParameter\\x12\\x0f\\n\\x04\\x61xis\\x18\\x01 \\x01(\\x05:\\x01\\x31\\x12\\x14\\n\\x08\\x65nd_axis\\x18\\x02 \\x01(\\x05:\\x02-1\\\"O\\n\\x11HDF5DataParameter\\x12\\x0e\\n\\x06source\\x18\\x01 \\x01(\\t\\x12\\x12\\n\\nbatch_size\\x18\\x02 \\x01(\\r\\x12\\x16\\n\\x07shuffle\\x18\\x03 \\x01(\\x08:\\x05\\x66\\x61lse\\\"(\\n\\x13HDF5OutputParameter\\x12\\x11\\n\\tfile_name\\x18\\x01 \\x01(\\t\\\"^\\n\\x12HingeLossParameter\\x12\\x30\\n\\x04norm\\x18\\x01 \\x01(\\x0e\\x32\\x1e.caffe.HingeLossParameter.Norm:\\x02L1\\\"\\x16\\n\\x04Norm\\x12\\x06\\n\\x02L1\\x10\\x01\\x12\\x06\\n\\x02L2\\x10\\x02\\\"\\x97\\x02\\n\\x12ImageDataParameter\\x12\\x0e\\n\\x06source\\x18\\x01 \\x01(\\t\\x12\\x15\\n\\nbatch_size\\x18\\x04 \\x01(\\r:\\x01\\x31\\x12\\x14\\n\\trand_skip\\x18\\x07 \\x01(\\r:\\x01\\x30\\x12\\x16\\n\\x07shuffle\\x18\\x08 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x15\\n\\nnew_height\\x18\\t \\x01(\\r:\\x01\\x30\\x12\\x14\\n\\tnew_width\\x18\\n \\x01(\\r:\\x01\\x30\\x12\\x16\\n\\x08is_color\\x18\\x0b \\x01(\\x08:\\x04true\\x12\\x10\\n\\x05scale\\x18\\x02 \\x01(\\x02:\\x01\\x31\\x12\\x11\\n\\tmean_file\\x18\\x03 \\x01(\\t\\x12\\x14\\n\\tcrop_size\\x18\\x05 \\x01(\\r:\\x01\\x30\\x12\\x15\\n\\x06mirror\\x18\\x06 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x15\\n\\x0broot_folder\\x18\\x0c \\x01(\\t:\\x00\\\"\\'\\n\\x15InfogainLossParameter\\x12\\x0e\\n\\x06source\\x18\\x01 \\x01(\\t\\\"\\xcb\\x01\\n\\x15InnerProductParameter\\x12\\x12\\n\\nnum_output\\x18\\x01 \\x01(\\r\\x12\\x17\\n\\tbias_term\\x18\\x02 \\x01(\\x08:\\x04true\\x12-\\n\\rweight_filler\\x18\\x03 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\x12+\\n\\x0b\\x62ias_filler\\x18\\x04 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\x12\\x0f\\n\\x04\\x61xis\\x18\\x05 \\x01(\\x05:\\x01\\x31\\x12\\x18\\n\\ttranspose\\x18\\x06 \\x01(\\x08:\\x05\\x66\\x61lse\\\"1\\n\\x0eInputParameter\\x12\\x1f\\n\\x05shape\\x18\\x01 \\x03(\\x0b\\x32\\x10.caffe.BlobShape\\\"D\\n\\x0cLogParameter\\x12\\x10\\n\\x04\\x62\\x61se\\x18\\x01 \\x01(\\x02:\\x02-1\\x12\\x10\\n\\x05scale\\x18\\x02 \\x01(\\x02:\\x01\\x31\\x12\\x10\\n\\x05shift\\x18\\x03 \\x01(\\x02:\\x01\\x30\\\"\\xb8\\x02\\n\\x0cLRNParameter\\x12\\x15\\n\\nlocal_size\\x18\\x01 \\x01(\\r:\\x01\\x35\\x12\\x10\\n\\x05\\x61lpha\\x18\\x02 \\x01(\\x02:\\x01\\x31\\x12\\x12\\n\\x04\\x62\\x65ta\\x18\\x03 \\x01(\\x02:\\x04\\x30.75\\x12\\x44\\n\\x0bnorm_region\\x18\\x04 \\x01(\\x0e\\x32\\x1e.caffe.LRNParameter.NormRegion:\\x0f\\x41\\x43ROSS_CHANNELS\\x12\\x0c\\n\\x01k\\x18\\x05 \\x01(\\x02:\\x01\\x31\\x12\\x33\\n\\x06\\x65ngine\\x18\\x06 \\x01(\\x0e\\x32\\x1a.caffe.LRNParameter.Engine:\\x07\\x44\\x45\\x46\\x41ULT\\\"5\\n\\nNormRegion\\x12\\x13\\n\\x0f\\x41\\x43ROSS_CHANNELS\\x10\\x00\\x12\\x12\\n\\x0eWITHIN_CHANNEL\\x10\\x01\\\"+\\n\\x06\\x45ngine\\x12\\x0b\\n\\x07\\x44\\x45\\x46\\x41ULT\\x10\\x00\\x12\\t\\n\\x05\\x43\\x41\\x46\\x46\\x45\\x10\\x01\\x12\\t\\n\\x05\\x43UDNN\\x10\\x02\\\"Z\\n\\x13MemoryDataParameter\\x12\\x12\\n\\nbatch_size\\x18\\x01 \\x01(\\r\\x12\\x10\\n\\x08\\x63hannels\\x18\\x02 \\x01(\\r\\x12\\x0e\\n\\x06height\\x18\\x03 \\x01(\\r\\x12\\r\\n\\x05width\\x18\\x04 \\x01(\\r\\\"d\\n\\x0cMVNParameter\\x12 \\n\\x12normalize_variance\\x18\\x01 \\x01(\\x08:\\x04true\\x12\\x1e\\n\\x0f\\x61\\x63ross_channels\\x18\\x02 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x12\\n\\x03\\x65ps\\x18\\x03 \\x01(\\x02:\\x05\\x31\\x65-09\\\"5\\n\\x12ParameterParameter\\x12\\x1f\\n\\x05shape\\x18\\x01 \\x01(\\x0b\\x32\\x10.caffe.BlobShape\\\"\\xa2\\x03\\n\\x10PoolingParameter\\x12\\x35\\n\\x04pool\\x18\\x01 \\x01(\\x0e\\x32\\\".caffe.PoolingParameter.PoolMethod:\\x03MAX\\x12\\x0e\\n\\x03pad\\x18\\x04 \\x01(\\r:\\x01\\x30\\x12\\x10\\n\\x05pad_h\\x18\\t \\x01(\\r:\\x01\\x30\\x12\\x10\\n\\x05pad_w\\x18\\n \\x01(\\r:\\x01\\x30\\x12\\x13\\n\\x0bkernel_size\\x18\\x02 \\x01(\\r\\x12\\x10\\n\\x08kernel_h\\x18\\x05 \\x01(\\r\\x12\\x10\\n\\x08kernel_w\\x18\\x06 \\x01(\\r\\x12\\x11\\n\\x06stride\\x18\\x03 \\x01(\\r:\\x01\\x31\\x12\\x10\\n\\x08stride_h\\x18\\x07 \\x01(\\r\\x12\\x10\\n\\x08stride_w\\x18\\x08 \\x01(\\r\\x12\\x37\\n\\x06\\x65ngine\\x18\\x0b \\x01(\\x0e\\x32\\x1e.caffe.PoolingParameter.Engine:\\x07\\x44\\x45\\x46\\x41ULT\\x12\\x1d\\n\\x0eglobal_pooling\\x18\\x0c \\x01(\\x08:\\x05\\x66\\x61lse\\\".\\n\\nPoolMethod\\x12\\x07\\n\\x03MAX\\x10\\x00\\x12\\x07\\n\\x03\\x41VE\\x10\\x01\\x12\\x0e\\n\\nSTOCHASTIC\\x10\\x02\\\"+\\n\\x06\\x45ngine\\x12\\x0b\\n\\x07\\x44\\x45\\x46\\x41ULT\\x10\\x00\\x12\\t\\n\\x05\\x43\\x41\\x46\\x46\\x45\\x10\\x01\\x12\\t\\n\\x05\\x43UDNN\\x10\\x02\\\"F\\n\\x0ePowerParameter\\x12\\x10\\n\\x05power\\x18\\x01 \\x01(\\x02:\\x01\\x31\\x12\\x10\\n\\x05scale\\x18\\x02 \\x01(\\x02:\\x01\\x31\\x12\\x10\\n\\x05shift\\x18\\x03 \\x01(\\x02:\\x01\\x30\\\"g\\n\\x0fPythonParameter\\x12\\x0e\\n\\x06module\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05layer\\x18\\x02 \\x01(\\t\\x12\\x13\\n\\tparam_str\\x18\\x03 \\x01(\\t:\\x00\\x12 \\n\\x11share_in_parallel\\x18\\x04 \\x01(\\x08:\\x05\\x66\\x61lse\\\"\\xc0\\x01\\n\\x12RecurrentParameter\\x12\\x15\\n\\nnum_output\\x18\\x01 \\x01(\\r:\\x01\\x30\\x12-\\n\\rweight_filler\\x18\\x02 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\x12+\\n\\x0b\\x62ias_filler\\x18\\x03 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\x12\\x19\\n\\ndebug_info\\x18\\x04 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x1c\\n\\rexpose_hidden\\x18\\x05 \\x01(\\x08:\\x05\\x66\\x61lse\\\"\\xad\\x01\\n\\x12ReductionParameter\\x12=\\n\\toperation\\x18\\x01 \\x01(\\x0e\\x32%.caffe.ReductionParameter.ReductionOp:\\x03SUM\\x12\\x0f\\n\\x04\\x61xis\\x18\\x02 \\x01(\\x05:\\x01\\x30\\x12\\x10\\n\\x05\\x63oeff\\x18\\x03 \\x01(\\x02:\\x01\\x31\\\"5\\n\\x0bReductionOp\\x12\\x07\\n\\x03SUM\\x10\\x01\\x12\\x08\\n\\x04\\x41SUM\\x10\\x02\\x12\\t\\n\\x05SUMSQ\\x10\\x03\\x12\\x08\\n\\x04MEAN\\x10\\x04\\\"\\x8d\\x01\\n\\rReLUParameter\\x12\\x19\\n\\x0enegative_slope\\x18\\x01 \\x01(\\x02:\\x01\\x30\\x12\\x34\\n\\x06\\x65ngine\\x18\\x02 \\x01(\\x0e\\x32\\x1b.caffe.ReLUParameter.Engine:\\x07\\x44\\x45\\x46\\x41ULT\\\"+\\n\\x06\\x45ngine\\x12\\x0b\\n\\x07\\x44\\x45\\x46\\x41ULT\\x10\\x00\\x12\\t\\n\\x05\\x43\\x41\\x46\\x46\\x45\\x10\\x01\\x12\\t\\n\\x05\\x43UDNN\\x10\\x02\\\"Z\\n\\x10ReshapeParameter\\x12\\x1f\\n\\x05shape\\x18\\x01 \\x01(\\x0b\\x32\\x10.caffe.BlobShape\\x12\\x0f\\n\\x04\\x61xis\\x18\\x02 \\x01(\\x05:\\x01\\x30\\x12\\x14\\n\\x08num_axes\\x18\\x03 \\x01(\\x05:\\x02-1\\\"\\xa5\\x01\\n\\x0eScaleParameter\\x12\\x0f\\n\\x04\\x61xis\\x18\\x01 \\x01(\\x05:\\x01\\x31\\x12\\x13\\n\\x08num_axes\\x18\\x02 \\x01(\\x05:\\x01\\x31\\x12&\\n\\x06\\x66iller\\x18\\x03 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\x12\\x18\\n\\tbias_term\\x18\\x04 \\x01(\\x08:\\x05\\x66\\x61lse\\x12+\\n\\x0b\\x62ias_filler\\x18\\x05 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\\"x\\n\\x10SigmoidParameter\\x12\\x37\\n\\x06\\x65ngine\\x18\\x01 \\x01(\\x0e\\x32\\x1e.caffe.SigmoidParameter.Engine:\\x07\\x44\\x45\\x46\\x41ULT\\\"+\\n\\x06\\x45ngine\\x12\\x0b\\n\\x07\\x44\\x45\\x46\\x41ULT\\x10\\x00\\x12\\t\\n\\x05\\x43\\x41\\x46\\x46\\x45\\x10\\x01\\x12\\t\\n\\x05\\x43UDNN\\x10\\x02\\\"L\\n\\x0eSliceParameter\\x12\\x0f\\n\\x04\\x61xis\\x18\\x03 \\x01(\\x05:\\x01\\x31\\x12\\x13\\n\\x0bslice_point\\x18\\x02 \\x03(\\r\\x12\\x14\\n\\tslice_dim\\x18\\x01 \\x01(\\r:\\x01\\x31\\\"\\x89\\x01\\n\\x10SoftmaxParameter\\x12\\x37\\n\\x06\\x65ngine\\x18\\x01 \\x01(\\x0e\\x32\\x1e.caffe.SoftmaxParameter.Engine:\\x07\\x44\\x45\\x46\\x41ULT\\x12\\x0f\\n\\x04\\x61xis\\x18\\x02 \\x01(\\x05:\\x01\\x31\\\"+\\n\\x06\\x45ngine\\x12\\x0b\\n\\x07\\x44\\x45\\x46\\x41ULT\\x10\\x00\\x12\\t\\n\\x05\\x43\\x41\\x46\\x46\\x45\\x10\\x01\\x12\\t\\n\\x05\\x43UDNN\\x10\\x02\\\"r\\n\\rTanHParameter\\x12\\x34\\n\\x06\\x65ngine\\x18\\x01 \\x01(\\x0e\\x32\\x1b.caffe.TanHParameter.Engine:\\x07\\x44\\x45\\x46\\x41ULT\\\"+\\n\\x06\\x45ngine\\x12\\x0b\\n\\x07\\x44\\x45\\x46\\x41ULT\\x10\\x00\\x12\\t\\n\\x05\\x43\\x41\\x46\\x46\\x45\\x10\\x01\\x12\\t\\n\\x05\\x43UDNN\\x10\\x02\\\"/\\n\\rTileParameter\\x12\\x0f\\n\\x04\\x61xis\\x18\\x01 \\x01(\\x05:\\x01\\x31\\x12\\r\\n\\x05tiles\\x18\\x02 \\x01(\\x05\\\"\\n\\x12ThresholdParameter\\x12\\x14\\n\\tthreshold\\x18\\x01 \\x01(\\x02:\\x01\\x30\\\"\\xc1\\x02\\n\\x13WindowDataParameter\\x12\\x0e\\n\\x06source\\x18\\x01 \\x01(\\t\\x12\\x10\\n\\x05scale\\x18\\x02 \\x01(\\x02:\\x01\\x31\\x12\\x11\\n\\tmean_file\\x18\\x03 \\x01(\\t\\x12\\x12\\n\\nbatch_size\\x18\\x04 \\x01(\\r\\x12\\x14\\n\\tcrop_size\\x18\\x05 \\x01(\\r:\\x01\\x30\\x12\\x15\\n\\x06mirror\\x18\\x06 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x19\\n\\x0c\\x66g_threshold\\x18\\x07 \\x01(\\x02:\\x03\\x30.5\\x12\\x19\\n\\x0c\\x62g_threshold\\x18\\x08 \\x01(\\x02:\\x03\\x30.5\\x12\\x19\\n\\x0b\\x66g_fraction\\x18\\t \\x01(\\x02:\\x04\\x30.25\\x12\\x16\\n\\x0b\\x63ontext_pad\\x18\\n \\x01(\\r:\\x01\\x30\\x12\\x17\\n\\tcrop_mode\\x18\\x0b \\x01(\\t:\\x04warp\\x12\\x1b\\n\\x0c\\x63\\x61\\x63he_images\\x18\\x0c \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x15\\n\\x0broot_folder\\x18\\r \\x01(\\t:\\x00\\\"\\xeb\\x01\\n\\x0cSPPParameter\\x12\\x16\\n\\x0epyramid_height\\x18\\x01 \\x01(\\r\\x12\\x31\\n\\x04pool\\x18\\x02 \\x01(\\x0e\\x32\\x1e.caffe.SPPParameter.PoolMethod:\\x03MAX\\x12\\x33\\n\\x06\\x65ngine\\x18\\x06 \\x01(\\x0e\\x32\\x1a.caffe.SPPParameter.Engine:\\x07\\x44\\x45\\x46\\x41ULT\\\".\\n\\nPoolMethod\\x12\\x07\\n\\x03MAX\\x10\\x00\\x12\\x07\\n\\x03\\x41VE\\x10\\x01\\x12\\x0e\\n\\nSTOCHASTIC\\x10\\x02\\\"+\\n\\x06\\x45ngine\\x12\\x0b\\n\\x07\\x44\\x45\\x46\\x41ULT\\x10\\x00\\x12\\t\\n\\x05\\x43\\x41\\x46\\x46\\x45\\x10\\x01\\x12\\t\\n\\x05\\x43UDNN\\x10\\x02\\\"\\xe0\\x13\\n\\x10V1LayerParameter\\x12\\x0e\\n\\x06\\x62ottom\\x18\\x02 \\x03(\\t\\x12\\x0b\\n\\x03top\\x18\\x03 \\x03(\\t\\x12\\x0c\\n\\x04name\\x18\\x04 \\x01(\\t\\x12$\\n\\x07include\\x18  \\x03(\\x0b\\x32\\x13.caffe.NetStateRule\\x12$\\n\\x07\\x65xclude\\x18! \\x03(\\x0b\\x32\\x13.caffe.NetStateRule\\x12/\\n\\x04type\\x18\\x05 \\x01(\\x0e\\x32!.caffe.V1LayerParameter.LayerType\\x12\\x1f\\n\\x05\\x62lobs\\x18\\x06 \\x03(\\x0b\\x32\\x10.caffe.BlobProto\\x12\\x0e\\n\\x05param\\x18\\xe9\\x07 \\x03(\\t\\x12>\\n\\x0f\\x62lob_share_mode\\x18\\xea\\x07 \\x03(\\x0e\\x32$.caffe.V1LayerParameter.DimCheckMode\\x12\\x10\\n\\x08\\x62lobs_lr\\x18\\x07 \\x03(\\x02\\x12\\x14\\n\\x0cweight_decay\\x18\\x08 \\x03(\\x02\\x12\\x13\\n\\x0bloss_weight\\x18# \\x03(\\x02\\x12\\x30\\n\\x0e\\x61\\x63\\x63uracy_param\\x18\\x1b \\x01(\\x0b\\x32\\x18.caffe.AccuracyParameter\\x12,\\n\\x0c\\x61rgmax_param\\x18\\x17 \\x01(\\x0b\\x32\\x16.caffe.ArgMaxParameter\\x12,\\n\\x0c\\x63oncat_param\\x18\\t \\x01(\\x0b\\x32\\x16.caffe.ConcatParameter\\x12?\\n\\x16\\x63ontrastive_loss_param\\x18( \\x01(\\x0b\\x32\\x1f.caffe.ContrastiveLossParameter\\x12\\x36\\n\\x11\\x63onvolution_param\\x18\\n \\x01(\\x0b\\x32\\x1b.caffe.ConvolutionParameter\\x12(\\n\\ndata_param\\x18\\x0b \\x01(\\x0b\\x32\\x14.caffe.DataParameter\\x12.\\n\\rdropout_param\\x18\\x0c \\x01(\\x0b\\x32\\x17.caffe.DropoutParameter\\x12\\x33\\n\\x10\\x64ummy_data_param\\x18\\x1a \\x01(\\x0b\\x32\\x19.caffe.DummyDataParameter\\x12.\\n\\reltwise_param\\x18\\x18 \\x01(\\x0b\\x32\\x17.caffe.EltwiseParameter\\x12&\\n\\texp_param\\x18) \\x01(\\x0b\\x32\\x13.caffe.ExpParameter\\x12\\x31\\n\\x0fhdf5_data_param\\x18\\r \\x01(\\x0b\\x32\\x18.caffe.HDF5DataParameter\\x12\\x35\\n\\x11hdf5_output_param\\x18\\x0e \\x01(\\x0b\\x32\\x1a.caffe.HDF5OutputParameter\\x12\\x33\\n\\x10hinge_loss_param\\x18\\x1d \\x01(\\x0b\\x32\\x19.caffe.HingeLossParameter\\x12\\x33\\n\\x10image_data_param\\x18\\x0f \\x01(\\x0b\\x32\\x19.caffe.ImageDataParameter\\x12\\x39\\n\\x13infogain_loss_param\\x18\\x10 \\x01(\\x0b\\x32\\x1c.caffe.InfogainLossParameter\\x12\\x39\\n\\x13inner_product_param\\x18\\x11 \\x01(\\x0b\\x32\\x1c.caffe.InnerProductParameter\\x12&\\n\\tlrn_param\\x18\\x12 \\x01(\\x0b\\x32\\x13.caffe.LRNParameter\\x12\\x35\\n\\x11memory_data_param\\x18\\x16 \\x01(\\x0b\\x32\\x1a.caffe.MemoryDataParameter\\x12&\\n\\tmvn_param\\x18\\\" \\x01(\\x0b\\x32\\x13.caffe.MVNParameter\\x12.\\n\\rpooling_param\\x18\\x13 \\x01(\\x0b\\x32\\x17.caffe.PoolingParameter\\x12\\n\\x0bpower_param\\x18\\x15 \\x01(\\x0b\\x32\\x15.caffe.PowerParameter\\x12(\\n\\nrelu_param\\x18\\x1e \\x01(\\x0b\\x32\\x14.caffe.ReLUParameter\\x12.\\n\\rsigmoid_param\\x18& \\x01(\\x0b\\x32\\x17.caffe.SigmoidParameter\\x12.\\n\\rsoftmax_param\\x18\\' \\x01(\\x0b\\x32\\x17.caffe.SoftmaxParameter\\x12\\n\\x0bslice_param\\x18\\x1f \\x01(\\x0b\\x32\\x15.caffe.SliceParameter\\x12(\\n\\ntanh_param\\x18% \\x01(\\x0b\\x32\\x14.caffe.TanHParameter\\x12\\x32\\n\\x0fthreshold_param\\x18\\x19 \\x01(\\x0b\\x32\\x19.caffe.ThresholdParameter\\x12\\x35\\n\\x11window_data_param\\x18\\x14 \\x01(\\x0b\\x32\\x1a.caffe.WindowDataParameter\\x12\\x37\\n\\x0ftransform_param\\x18$ \\x01(\\x0b\\x32\\x1e.caffe.TransformationParameter\\x12(\\n\\nloss_param\\x18 \\x01(\\x0b\\x32\\x14.caffe.LossParameter\\x12&\\n\\x05layer\\x18\\x01 \\x01(\\x0b\\x32\\x17.caffe.V0LayerParameter\\\"\\xd8\\x04\\n\\tLayerType\\x12\\x08\\n\\x04NONE\\x10\\x00\\x12\\n\\n\\x06\\x41\\x42SVAL\\x10#\\x12\\x0c\\n\\x08\\x41\\x43\\x43URACY\\x10\\x01\\x12\\n\\n\\x06\\x41RGMAX\\x10\\x1e\\x12\\x08\\n\\x04\\x42NLL\\x10\\x02\\x12\\n\\n\\x06\\x43ONCAT\\x10\\x03\\x12\\x14\\n\\x10\\x43ONTRASTIVE_LOSS\\x10%\\x12\\x0f\\n\\x0b\\x43ONVOLUTION\\x10\\x04\\x12\\x08\\n\\x04\\x44\\x41TA\\x10\\x05\\x12\\x11\\n\\rDECONVOLUTION\\x10\\'\\x12\\x0b\\n\\x07\\x44ROPOUT\\x10\\x06\\x12\\x0e\\n\\nDUMMY_DATA\\x10 \\x12\\x12\\n\\x0e\\x45UCLIDEAN_LOSS\\x10\\x07\\x12\\x0b\\n\\x07\\x45LTWISE\\x10\\x19\\x12\\x07\\n\\x03\\x45XP\\x10&\\x12\\x0b\\n\\x07\\x46LATTEN\\x10\\x08\\x12\\r\\n\\tHDF5_DATA\\x10\\t\\x12\\x0f\\n\\x0bHDF5_OUTPUT\\x10\\n\\x12\\x0e\\n\\nHINGE_LOSS\\x10\\x1c\\x12\\n\\n\\x06IM2COL\\x10\\x0b\\x12\\x0e\\n\\nIMAGE_DATA\\x10\\x0c\\x12\\x11\\n\\rINFOGAIN_LOSS\\x10\\r\\x12\\x11\\n\\rINNER_PRODUCT\\x10\\x0e\\x12\\x07\\n\\x03LRN\\x10\\x0f\\x12\\x0f\\n\\x0bMEMORY_DATA\\x10\\x1d\\x12\\x1d\\n\\x19MULTINOMIAL_LOGISTIC_LOSS\\x10\\x10\\x12\\x07\\n\\x03MVN\\x10\\\"\\x12\\x0b\\n\\x07POOLING\\x10\\x11\\x12\\t\\n\\x05POWER\\x10\\x1a\\x12\\x08\\n\\x04RELU\\x10\\x12\\x12\\x0b\\n\\x07SIGMOID\\x10\\x13\\x12\\x1e\\n\\x1aSIGMOID_CROSS_ENTROPY_LOSS\\x10\\x1b\\x12\\x0b\\n\\x07SILENCE\\x10$\\x12\\x0b\\n\\x07SOFTMAX\\x10\\x14\\x12\\x10\\n\\x0cSOFTMAX_LOSS\\x10\\x15\\x12\\t\\n\\x05SPLIT\\x10\\x16\\x12\\t\\n\\x05SLICE\\x10!\\x12\\x08\\n\\x04TANH\\x10\\x17\\x12\\x0f\\n\\x0bWINDOW_DATA\\x10\\x18\\x12\\r\\n\\tTHRESHOLD\\x10\\x1f\\\"\\n\\x0c\\x44imCheckMode\\x12\\n\\n\\x06STRICT\\x10\\x00\\x12\\x0e\\n\\nPERMISSIVE\\x10\\x01\\\"\\xfd\\x07\\n\\x10V0LayerParameter\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x0c\\n\\x04type\\x18\\x02 \\x01(\\t\\x12\\x12\\n\\nnum_output\\x18\\x03 \\x01(\\r\\x12\\x16\\n\\x08\\x62iasterm\\x18\\x04 \\x01(\\x08:\\x04true\\x12-\\n\\rweight_filler\\x18\\x05 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\x12+\\n\\x0b\\x62ias_filler\\x18\\x06 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\x12\\x0e\\n\\x03pad\\x18\\x07 \\x01(\\r:\\x01\\x30\\x12\\x12\\n\\nkernelsize\\x18\\x08 \\x01(\\r\\x12\\x10\\n\\x05group\\x18\\t \\x01(\\r:\\x01\\x31\\x12\\x11\\n\\x06stride\\x18\\n \\x01(\\r:\\x01\\x31\\x12\\x35\\n\\x04pool\\x18\\x0b \\x01(\\x0e\\x32\\\".caffe.V0LayerParameter.PoolMethod:\\x03MAX\\x12\\x1a\\n\\rdropout_ratio\\x18\\x0c \\x01(\\x02:\\x03\\x30.5\\x12\\x15\\n\\nlocal_size\\x18\\r \\x01(\\r:\\x01\\x35\\x12\\x10\\n\\x05\\x61lpha\\x18\\x0e \\x01(\\x02:\\x01\\x31\\x12\\x12\\n\\x04\\x62\\x65ta\\x18\\x0f \\x01(\\x02:\\x04\\x30.75\\x12\\x0c\\n\\x01k\\x18\\x16 \\x01(\\x02:\\x01\\x31\\x12\\x0e\\n\\x06source\\x18\\x10 \\x01(\\t\\x12\\x10\\n\\x05scale\\x18\\x11 \\x01(\\x02:\\x01\\x31\\x12\\x10\\n\\x08meanfile\\x18\\x12 \\x01(\\t\\x12\\x11\\n\\tbatchsize\\x18\\x13 \\x01(\\r\\x12\\x13\\n\\x08\\x63ropsize\\x18\\x14 \\x01(\\r:\\x01\\x30\\x12\\x15\\n\\x06mirror\\x18\\x15 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x1f\\n\\x05\\x62lobs\\x18\\x32 \\x03(\\x0b\\x32\\x10.caffe.BlobProto\\x12\\x10\\n\\x08\\x62lobs_lr\\x18\\x33 \\x03(\\x02\\x12\\x14\\n\\x0cweight_decay\\x18\\x34 \\x03(\\x02\\x12\\x14\\n\\trand_skip\\x18\\x35 \\x01(\\r:\\x01\\x30\\x12\\x1d\\n\\x10\\x64\\x65t_fg_threshold\\x18\\x36 \\x01(\\x02:\\x03\\x30.5\\x12\\x1d\\n\\x10\\x64\\x65t_bg_threshold\\x18\\x37 \\x01(\\x02:\\x03\\x30.5\\x12\\x1d\\n\\x0f\\x64\\x65t_fg_fraction\\x18\\x38 \\x01(\\x02:\\x04\\x30.25\\x12\\x1a\\n\\x0f\\x64\\x65t_context_pad\\x18: \\x01(\\r:\\x01\\x30\\x12\\x1b\\n\\rdet_crop_mode\\x18; \\x01(\\t:\\x04warp\\x12\\x12\\n\\x07new_num\\x18< \\x01(\\x05:\\x01\\x30\\x12\\x17\\n\\x0cnew_channels\\x18= \\x01(\\x05:\\x01\\x30\\x12\\x15\\n\\nnew_height\\x18> \\x01(\\x05:\\x01\\x30\\x12\\x14\\n\\tnew_width\\x18? \\x01(\\x05:\\x01\\x30\\x12\\x1d\\n\\x0eshuffle_images\\x18@ \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x15\\n\\nconcat_dim\\x18\\x41 \\x01(\\r:\\x01\\x31\\x12\\x36\\n\\x11hdf5_output_param\\x18\\xe9\\x07 \\x01(\\x0b\\x32\\x1a.caffe.HDF5OutputParameter\\\".\\n\\nPoolMethod\\x12\\x07\\n\\x03MAX\\x10\\x00\\x12\\x07\\n\\x03\\x41VE\\x10\\x01\\x12\\x0e\\n\\nSTOCHASTIC\\x10\\x02\\\"W\\n\\x0ePReLUParameter\\x12&\\n\\x06\\x66iller\\x18\\x01 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\x12\\x1d\\n\\x0e\\x63hannel_shared\\x18\\x02 \\x01(\\x08:\\x05\\x66\\x61lse*\\x1c\\n\\x05Phase\\x12\\t\\n\\x05TRAIN\\x10\\x00\\x12\\x08\\n\\x04TEST\\x10\\x01'\n  File \"/local/tensorflow350_gpu/lib/python3.5/site-packages/google/protobuf/descriptor.py\", line 824, in new\n    return _message.default_pool.AddSerializedFile(serialized_pb)\nTypeError: expected bytes, str found\n```\n. Indeed, I have version 2.5.0. Thanks, works well indeed after downloading protoc 3.2.0. ",
    "leegenuine": "ok! thanks for your reply. ",
    "byronyi": "I am interested in providing peer direct DMA support to TF data feeding, following the same spirit of my GDR patch. It seems to be a bottleneck transferring tensors from either network or disk to GPU;\nas better and faster GPUs become available in-stock, it would become an important feature for actual deployment, which is often ignored during benchmarking with synthetic data.\nWith proper hardware (rNIC or NVMe SSD) peer direct DMA could bypass CPU and host memory and feed tensors to GPU directly. \nThe tentative design would be re-using the current ZMQ operator to setup control plane, and avoid serialisation and memory copies using direct DMA in the data plane. Batching and pipelining would be helpful to reduce the latency and increase the throughput.\nIf your cluster has IB/NVMe available, let me know! We are interested in deployments of peer direct DMA in realistic applications, and we'd love to help.. @PatWie Any comments? https://github.com/tensorflow/tensorflow/pull/11392 is our PR to improve model update efficiency during training. It improves RecvTensor's throughput of large tensors (>100MB) from 0.8GB/s to 3.6GB/s on a single machine. I am wondering if the same technique could be applied in data feeding as well.. While copying itself might not be the bottleneck, serialisation might be :) See https://github.com/tensorflow/tensorflow/issues/10530 for the rationale. Removing protobuf serialisation of tensors improves the throughput from 0.5GB/s to 1.5GB/s, so it could be quite significant. In fact, even the gRPC runtime has specialised implementation to avoid serialisation; see here.. ",
    "tmquan": "Hi @ppwwyyxx , \nI would like to revisit this one and am working on the image segmentation. \nSpecifically, I would like to implement a custom function on image augmentation which is so-called Elastic Deformation since the Gaussian Deformation is not robust to my problem and it requires the higher accuracy. \nIn Elastic Deformation, both 3-channel input and output images need to used the same seed number to generate the same vector flow from numpy.\nIs it straightforward to add such this customization? An example of interface could be helpful a lot. \nThanks so much. \n. Hi @ppwwyyxx ,\nI managed to complete the implementation ElasticDeformation augmentation. \nIt takes a random vector field prod(range(8, 16)) and truncates its boundary values. \nDeformation rate can be controlled via this range. \nNext, it performs warping the original image with this field. \nIt would be great if you can include in the next release.\nBests\n```Python\nclass ElasticDeform:\n    def init(self):\n        pass\ndef reset_state(self):\n    self.rng = get_rng(self)\n\ndef _augment(self, img, param):\n    assert img.ndim in [2, 3], img.ndim\n    du, dv = param\n    shape = img.shape\n\n    DU = cv2.resize(du, (shape[-2], shape[-2])) \n    DV = cv2.resize(dv, (shape[-2], shape[-2])) \n    X, Y = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]))\n    indices = np.reshape(Y+DV, (-1, 1)), np.reshape(X+DU, (-1, 1))\n\n    flow = img.copy()\n    from scipy.ndimage.interpolation    import map_coordinates\n    for k in range(3):\n        tmp = map_coordinates(img[...,k], indices, order=1)\n        flow[...,k] = tmp.reshape(shape[0:2])\n    flow = flow.reshape(shape)\n    return flow\ndef _get_augment_params(self, d):\n    \"\"\"\n    get the augmentor parameters\n    \"\"\"\n    size = self.rng.choice(range(8,16)) #8\n    ampl = self.rng.choice(range(8,16)) #8\n    du = self.rng.uniform(-ampl, ampl, size=(size, size))\n    dv = self.rng.uniform(-ampl, ampl, size=(size, size))\n\n    # Dont distort at the boundaries\n    du[ 0,:] = 0; du[-1,:] = 0; du[:, 0] = 0; du[:,-1] = 0;\n    dv[ 0,:] = 0; dv[-1,:] = 0; dv[:, 0] = 0; dv[:,-1] = 0;\n    return du, dv\ndef _augment_return_params(self, d):\n    \"\"\"\n    Augment the image and return both image and params\n    \"\"\"\n    prms = self._get_augment_params(d)\n    return (self._augment(d, prms), prms)\n\n```\n```python\nimport glob\nimgs = glob.glob(os.path.join('data/std/', 'lena.png'))\nds = ImageFromFile(imgs, channel=3, shuffle=False)\naugmentors = [\n    ElasticDeform(), \n    imgaug.ColorSpace(mode=cv2.COLOR_RGB2BGR),\n]\nds = AugmentImageComponent(ds, augmentors)\nds = PrintData(ds, num=2) # only for debugging\ngen = ds.get_data()\nfor dp in gen:\n    newImg = np.array(dp)\na = np.squeeze(cv2.imread(imgs[0]))\nb = np.squeeze(newImg)\nc = np.abs(a-b)\nconcat = np.concatenate((a,b,c), axis=-2)\nconcat = concat.astype(np.uint8)\nprint concat.shape\nplt.figure(figsize=(20, 60))\nplt.imshow(concat, cmap=plt.cm.gray)\nplt.axis('off')\nplt.show()\n\n```\n\n. ElasticDeform is quite common in biomedical image processing, especially in segmentation. \nIf you look into the original U-Net paper, the authors also leveraged it intensively. \nIt is good to know that the main library should be kept small and common :)\nI will close this issue for other people would like to perform their own custom augmentations. \nBests, . @PatWie : I am using the image2image translation for debugging the segmentation problem. \nHowever, I met the issue such that discriminator curve and generator curve are not going as expected. \nOnce it is done, I will make a pull request :)\n. I just reopened for more discussion. \nIf this is the case, we can not trust the result from the generator, can we?\nThe generated images are just qualitatively fine, not quantitatively. . In the GAN.py implementation, is it true that we need to minimize d_loss, as opposed to the minimizing -d_loss (maximizing d_loss) ops in GAN theory?\n```python\nby default, run one d_min after one g_min\n    self.g_min = opt.minimize(self.model.g_loss, var_list=self.model.g_vars, name='g_op')\n    with tf.control_dependencies([self.g_min]):\n        self.d_min = opt.minimize(self.model.d_loss, var_list=self.model.d_vars, name='d_op')\n    self.train_op = self.d_min\n\n```. Thank @ppwwyyxx ,\nI have another question. Why did you need tf.control_dependencies before d_min ops?\n. Resolved due to scale issue in tf.summary.image \nhttps://www.tensorflow.org/api_docs/python/tf/summary/image . ",
    "prashantsingh905": "Thanks for your reply. On the same note can you tell me how do i access the weights and biases from the previous layers define in linear warp.i need those as input for my next layers.\n I tried to find it in docs but didn't get any info on this.. Thanks ..i got the concept now.. ",
    "msbauer": "@PatWie: Thanks for the quick reply. Your proposed solution does not seem to work. . Thanks. I was also thinking about this workaround, however, then one has to chop up the datapoints again... can you explain to me what the problem with my original approach was?. I am not using AugmentImageComponents but AugmentImageComponent (without s). That only applies it to the first component.\nI meant more: why does my original RepeatedDataPoint not do what I expect it would do (which is: make 5 datapoints out of one datapoint which can be augmented independently)?. Unfortunately, I cannot do any testing until Tuesday. I currently use version 0.3.8 which works. A colleague has been using 0.3.9 and had problems; however, I am not certain that was the cause as we haven't checked that 0.3.8 solved the problem for him\nThe problem was that:\na) no error message when msgpack was not installed (initially I had msgpack-python and msgpack-numpy installed and didn't get an error but it also didn't work). I would expect an error message \"module missing\" or something similar in that case.\nb) The main problem for finding the error was that de-serialization did not return an error but a datastream that could not be interpreted by opencv. Thus, I was looking into issues with opencv for a while before realising that the de-serialization was actually the culprit.\nPossible solutions:\na) I will do testing for version specificity next week\nb) I think an exception should be raised if a module is missing instead of just returning something that cannot be interpreted (even if msgpack is in the requirement; I used the pip installation recommended in the readme and it did not install msgpack on its own).. Ok, now I can reproduce the error. I uninstalled msgpack (in conda) and only installed msgpack-python/msgpack-numpy with pip.\nFor msgpack-numpy latest version (0.4.0) I get the following error\nFile \"mini_imagenet.py\", line 340, in <module>\n    df = get_data('test', 'predict', print_datapoints=1, classes=classes, n_split=len(classes), shuffle=False)\n  File \"mini_imagenet.py\", line 267, in get_data\n    ds = PrintData(ds, num=print_datapoints)\n  File \"~/lib/tensorpack/tensorpack/dataflow/common.py\", line 626, in __init__\n    self.print_info()\n  File \"~/lib/tensorpack/tensorpack/dataflow/common.py\", line 696, in print_info\n    for i, dummy in enumerate(cutoff(ds.get_data(), self.num)):\n  File \"~/lib/tensorpack/tensorpack/dataflow/common.py\", line 686, in cutoff\n    for el in gen:\n  File \"~/lib/tensorpack/tensorpack/dataflow/common.py\", line 231, in get_data\n    for dp in self.ds.get_data():\n  File \"~/lib/tensorpack/tensorpack/dataflow/common.py\", line 231, in get_data\n    for dp in self.ds.get_data():\n  File \"~/lib/tensorpack/tensorpack/dataflow/common.py\", line 231, in get_data\n    for dp in self.ds.get_data():\n  File \"~/lib/tensorpack/tensorpack/dataflow/common.py\", line 232, in get_data\n    ret = self.func(dp)\n  File \"~/lib/tensorpack/tensorpack/dataflow/common.py\", line 254, in f\n    r = func(dp[index])\n  File \"mini_imagenet.py\", line 242, in <lambda>\n    ds = MapDataComponent(ds, lambda x: cv2.imdecode(x, cv2.IMREAD_COLOR), 0)   # [[w, h, c], [class]]; c is in BGR\nTypeError: buf is not a numpy array, neither a scalar\nIf I use msgpack-numpy==0.3.8 everything works smoothly. Thus, v0.4.0 seems to de-serialise the lmdb-file differently to v0.3.8 (everything else stayed the same). . Thanks. Maybe also mention somewhere in the \"efficient data loading\" documentation that this incompatibility exists and that old LMDB files don't work anymore when upgrading? The dependence might not be obvious to everyone and it took me a long time to figure out what the problem was (probably also as I am less experienced). Ok, I have switched to tensorflow 1.3 and cudnn/6.0. Now it seems to work perfectly. It is a bit strange that cudnn 5.1 does not work at all, whereas cudnn/6 works fine. . ",
    "iridescent303": "Thanks! Let me try it.. ",
    "zhyiwei": "thanks, i changed version to 1.0\n@ppwwyyxx . @ppwwyyxx \nwhen i ran \"./create-lmdb.py stat --db train.mdb\", it printed:\nTraceback (most recent call last):\n  File \"./create-lmdb.py\", line 136, in \n    compute_mean_std(args.db, args.output)\n  File \"./create-lmdb.py\", line 105, in compute_mean_std\n    ds = LMDBDataPoint(db, shuffle=False)\nTypeError: init() got an unexpected keyword argument 'shuffle'\n\nso i changed \"ds = LMDBDataPoint(db, shuffle=False)\" --> \"ds = LMDBDataPoint(db)\" (create-lmdb.py), then run \"./create-lmdb.py stat --db train.mdb\" again, but now it print:\nTraceback (most recent call last):\n  File \"./create-lmdb.py\", line 137, in \n    compute_mean_std(args.db, args.output)\n  File \"./create-lmdb.py\", line 108, in compute_mean_std\n    with get_tqdm(total=ds.size()) as bar:\n  File \"/usr/local/lib/python2.7/dist-packages/tensorpack/dataflow/base.py\", line 73, in size\n    return self.ds.size()\nAttributeError: 'str' object has no attribute 'size'\n=========================================\ni dont know what's wrong with it\n. @ppwwyyxx thank you\nand I also changed \"loss = tf.nn.ctc_loss(logits, label, seqlen, time_major=False)\" to \"loss = tf.nn.ctc_loss( label, logits, seqlen, time_major=False)\" in train_timit.py  ----  I found  that \"label\" should be the first arg ,because in ctc_ops.py, the function is defined as : \ndef ctc_loss(labels, inputs, sequence_length,\n             preprocess_collapse_repeated=False,\n             ctc_merge_repeated=True, time_major=True):\n\nand now I ran \"./train-timit.py --train train.mdb --test test.mdb --stat stats.data\", it printed :\n[0402 13:01:56 @logger.py:69] Argv: ./train-timit.py --train train.mdb --test test.mdb --stat stats.data\n[0402 13:01:56 @format.py:82] Found 111 entries in train.mdb\n[0402 13:01:57 @format.py:82] Found 11 entries in test.mdb\n[0402 13:01:57 @param.py:190] Use train_log/train-timit/hyper.txt to control hyperparam learning_rate.\n[0402 13:01:57 @common.py:94] fc input: [None, 128]\n[0402 13:01:57 @common.py:102] fc output: [None, 62]\n\nis that right? where should i find the \"edit distance\"?\n. @ppwwyyxx \nhttps://github.com/ppwwyyxx/tensorpack/tree/4a30d18dd9512319ddbe2e7f1503abe45e1e80cb/examples/CTC-TIMIT\n\"\nResults:\nGet 0.28 LER (normalized edit distance) after about 40 epochs.\n\"\nbut i didn't find this \"normalized edit distance\". @ppwwyyxx \nso it looks like my print contents were not correct, i think there may be somgthing wrong in my train.mdb and test.mdb files\nwould you mind uploading your .mdb files? i'll be very thankful for your help. @ppwwyyxx \noh, the dataset I used is just a small one\nTIMIT_speech_database.zip\n. ",
    "byangderek": "Get it.\nSo in the example case, I just need one AugmentImageComponents for geometry and one AugmentImageComponent for imgproc.. thx!. ",
    "FabiEder": "Yes, I just wanted to let you know. \nIt worked fine with larger batch sizes! Thanks.. ",
    "rhofour": "Here's a tiny patch that shows what I'm trying to do. Running DQN.py with this patch I still see almost all of the memory on my GPU being reserved.\ncode.txt\n. Does it work with DQN? I'm wondering if it's not getting passed all the way\nthrough the trainers.\nOn Apr 7, 2017 20:29, \"Yuxin Wu\" notifications@github.com wrote:\n\nIt works for me.\ndiff --git i/examples/cifar-convnet.py w/examples/cifar-convnet.py\nindex a8e18dc..8761bb2 100755\n--- i/examples/cifar-convnet.py\n+++ w/examples/cifar-convnet.py\n@@ -10,6 +10,7 @@ import os\nimport tensorpack.tfutils.symbolic_functions as symbf\n from tensorpack.tfutils.summary import \n+from tensorpack.tfutils.sesscreate import \n from tensorpack.utils.gpu import get_nr_gpu\n\"\"\"\n@@ -114,6 +115,13 @@ def get_config(cifar_classnum):\n     dataset_train = get_data('train', cifar_classnum)\n     dataset_test = get_data('test', cifar_classnum)\n\ntf_conf = tf.ConfigProto()\ntf_conf.gpu_options.per_process_gpu_memory_fraction = 0.05\ntf_conf.allow_soft_placement = True\nsession_creator = NewSessionCreator(config=tf_conf)\n+\n+\n     def lr_func(lr):\n         if lr < 3e-5:\n             raise StopTraining()\n@@ -127,6 +135,7 @@ def get_config(cifar_classnum):\n             StatMonitorParamSetter('learning_rate', 'val_error', lr_func,\n                                    threshold=0.001, last_k=10),\n         ],\nsession_creator=session_creator,\n         max_epoch=150,\n     )\n\nThen I can see it consumes only a little memory\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/ppwwyyxx/tensorpack/issues/220#issuecomment-292681287,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAyIwjiTYh5AKv8WS7rHBHcMcanyNvGLks5rttTTgaJpZM4M20HX\n.\n. Ok, just tried on cifar and that seems to work while DQN (still) doesn't. Hopefully later today I can figure out what's different about the two that's causing this.. Aha, it seems this is a TF issue: https://github.com/tensorflow/tensorflow/issues/8136\n\nGPU settings appear to be set on the first call to tf.Session or tf.Server. Thus, simply making a new session before anything else runs and setting the correct GPU memory limit fixes my problem. I haven't yet figured out where the earlier call to tf.Session or tf.Server is though.. Huh, that's odd. It's easily reproducible for me and happens right from the first epoch. I'm running a version of TF I compiled, but not that recently. I'll switch to the latest RC in the next few days and see if it's fixed for me.. Ah, looking closer at how sample is called I see that's the case.\nLooks like the real bug is in my prioritized experience replay implementation which handles this case incorrectly.. ",
    "tillahoffmann": "We do most of our decoding in python (for now) but intend to migrate to using [tf.train.Example](https://www.tensorflow.org/api_docs/python/tf/train/Example) which support variable size tensors and are natively supported by tensorflow. \nLet me know if you want to collaborate on this issue. Would be great to hear your requirements and use cases.. ",
    "agupta74": "Will it be possible to share your Vgg16.npy file? I am just wondering if there are issues with converting the caffe model.\nThanks !\nSent from Yahoo Mail for iPhone\n. Thanks Yuxin ! I did print it but the value dp[0] was a dict object with raw bytes as keys and values but should instead be a numpy array. Is it possible that the decoding of the input data has some issues ?\nCan you please let me know the versions of the following packages you are using ? I installed all these packages using conda install .- numpy- msgpack-Python\u00a0- msgpack-numpy\nThanks,Ajay\nSent from Yahoo Mail for iPhone\n.  Also can you please let me know the dimensions and data type of input data fed into feed_dict ?\n. Thanks ! Sure I will let you know which package helps.\n. It looks like after I updated my numpy, opencv and msg-pack numpy packages, it worked. I used the versions you listed and for opencv I used version 3.1.0\nThanks for your help ! \u00a0\nI will let you know if I run into any other issues.. I was able to complete the training. In order to make sure that my training process went OK, can you please verify if the following graphs look reasonable (graphs below were generated using tensorboard)?\n\n\n\n. ",
    "tangabc": "I was using an older version of tensorpack (dated Feb '17). Let me try the latest version and see. Thanks.. ",
    "ZhenghaoFei": "pip install tornado can solve the issue. ",
    "jyh2986": "I'm sorry I gave you a little clue.\nSame problem in other machines (I used same docker env.)\nI thought that the segfault is always occurred on ModelSaver, but this is not true.\nIf I add meaningless code(e.g. add dummy argument), it stops immediately like belows.\n[0421 09:09:53 @base.py:116] Setup callbacks graph ...\n[0421 09:09:53 @base.py:187] Building predictor graph towerp0 on gpu=0 ...\nSegmentation fault (core dumped)\nOne of the other phenomena is that segfault always occurs at the end, even if it is not interrupted.\nI think I have to find more clues to explain you.\nOne of the suspects is that I didn't install TF & TP from scratch. \n. This is the failure procedures.\n1. Install tensorflow 0.12.1 using docker and installed opencv(make install), tensorpack\n  - There's no problem.\n2. Upgrade tensorflow using \"pip install -upgrade\" to v1.0.1, and tensorpack 0.1.8\n  - Segfault occurred. \n3. I removed opencv and make install again.\n  - Segfault occurred.\nAfter that, I re-installed tensorflow(using docker), opencv and tensorpack from scratch. \nAnd there's no problem. It might be caused by upgrading tensorflow.\nI tried to identify the problem with existing repository, but it failed.\nAnyway, it is solved.\nThank you.\n. ",
    "Seraphli": "No, I mean I cannot reproduce using my own code. When I visit openai gym website, your report score of a3c is on the list. So I check out your code and want to learn how you implement DQN. \nSo do using maxpool layer improve the performance?\nBTW, I saw people were talking about reproducing score at here.. So your implement change:\n1. using maxpool\n2. using different relu\n3. padding (I'm not sure. It is not mentioned in paper)\n4. double q\n5. initializer (Some said deepmind were using xavier)\nIs this list correct?\nAnd you said the original architecture worked well. Do you still remember the details? I have several questions:\n1. In the README, you said training DQN only take about one day, so the original network only took half of the day? If so, what's the gpu? Do you still remember how much frame rate in training phase and evaluation phase?\n2. Is the score still >300 pts in original setup?. Thank you for your replies. I find a result generated by simple_dqn. I don't know if you have checked out this repository. Although the max score is >400, the average score is ~200. I am not quite understand why there is a different.. Alright, thank you anyway.. I mean you should fix README. ok. ",
    "Sirius083": "Hello, the mnist example link is not available, can you reput the link?\nI cannot find the example of this usage\nThanks in advance. ",
    "andreas128": "Thank you very much, that solved the Issue.. ",
    "abhijitnathwani": "Hello,\nEven I faced the same error, and on solving with the method as suggested by @ppwwyyxx I solved that error. However, i guess changing the data_format also changed some shapes.\nNow I'm getting error:\nValueError: Dimensions must be equal, but are 3 and 16 for 'tower0/res1.0/add' (op: 'Add') with input shapes: [?,3,32,3], [?,3,32,16].\nPlease help me for the same. I need to train cifar10-resnet.py on CPU. I don't have GPU with me.. Can you please help me with changes @ppwwyyxx ? I'm stuck on this for long now. Don't know where to make the changes for this to run on CPU. All the help is appreciated!. @ppwwyyxx I already made the necessary changes in build_graph to bypass GPU checks. I still cannot get why the\nValueError: Dimensions must be equal, but are 3 and 16 for 'tower0/res1.0/add' (op: 'Add') with input shapes: [?,3,32,3], [?,3,32,16].\nerror occurs.\nShapes should have nothing to do with GPU/CPU I guess.\n. @ppwwyyxx Thanks a ton! You're a savior. Works well now.. ",
    "Superlee506": "@ppwwyyxx \nWhen I tested the \"roi_align\" function, I also meet this problem and I used GPU.\n2018-05-08 17:24:44.425156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:85:00.0, compute capability: 3.7)\n2018-05-08 17:24:44.624317: E tensorflow/core/common_runtime/executor.cc:643] Executor failed to create kernel. Invalid argument: Default AvgPoolingOp only supports NHWC.\n     [[Node: roi_align/AvgPool = AvgPool[T=DT_FLOAT, data_format=\"NCHW\", ksize=[1, 1, 2, 2], padding=\"SAME\", strides=[1, 1, 2, 2], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](roi_align/crop_and_resize/transpose_1)]]. @ppwwyyxx Ok. @vqdang I face the same question, do you find a solution?. @ppwwyyxx @chunfuchen How can you achieve 60~200s per epoch? I used 4 K80, and after 3K steps,  it still takes 1000s per epoch.  I fine-tuned the mask RCNN mode with the follow codes:\npython train.py --gpu 0,1,2,3 --load ImageNet-ResNet50.npz\n\n. @ppwwyyxx Copy that~~Thanks for your patience~. @ppwwyyxx @sharpstill Hi, I know about the \"generate_anchors\" function, but I have a little confused about Line 68 in \"get_all_anchors\". Why do we need to add 1 as below:\n\nIs it related with the float box you mentioned\uff1f But I'm still confused.  Any suggestion would be appreciated.\n. @ppwwyyxx Thanks for your timely reply. I remove the prefetch method as you say, but still have the following errors. Do I need to change the trainer method at the same time?\n\"Traceback (most recent call last):\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorpack\\train\\base.py\", line 256, in main_loop\n    self._callbacks.before_train()\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorpack\\callbacks\\base.py\", line 63, in before_train\n    self._before_train()\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorpack\\callbacks\\group.py\", line 70, in _before_train\n    cb.before_train()\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorpack\\callbacks\\base.py\", line 63, in before_train\n    self._before_train()\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorpack\\callbacks\\prof.py\", line 60, in _before_train\n    start_proc_mask_signal(self._proc)\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorpack\\utils\\concurrency.py\", line 212, in start_proc_mask_signal\n    p.start()\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\multiprocessing\\process.py\", line 105, in start\n    self._popen = self._Popen(self)\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\multiprocessing\\context.py\", line 212, in _Popen\n    return _default_context.get_context().Process._Popen(process_obj)\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\multiprocessing\\context.py\", line 313, in _Popen\n    return Popen(process_obj)\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\multiprocessing\\popen_spawn_win32.py\", line 66, in init\n    reduction.dump(process_obj, to_child)\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\multiprocessing\\reduction.py\", line 59, in dump\n    ForkingPickler(file, protocol).dump(obj)\n_pickle.PicklingError: Can't pickle : attribute lookup module on builtins failed\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorpack\\callbacks\\group.py\", line 76, in _after_train\n    cb.after_train()\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorpack\\callbacks\\base.py\", line 171, in after_train\n    self._after_train()\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorpack\\callbacks\\prof.py\", line 80, in _after_train\n    self._proc.join()\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\multiprocessing\\process.py\", line 120, in join\n    assert self._popen is not None, 'can only join a started process'\nAssertionError: can only join a started process\nTraceback (most recent call last):\n  File \"D:/Github/Tensorpack_Examples/FasterRCNN/train.py\", line 401, in \n    launch_train_with_config(cfg, trainer)\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorpack\\train\\interface.py\", line 91, in launch_train_with_config\n    extra_callbacks=config.extra_callbacks)\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorpack\\train\\base.py\", line 331, in train_with_defaults\n    steps_per_epoch, starting_epoch, max_epoch)\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorpack\\train\\base.py\", line 303, in train\n    self.main_loop(steps_per_epoch, starting_epoch, max_epoch)\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorpack\\utils\\argtools.py\", line 182, in wrapper\n    return func(args, *kwargs)\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorpack\\train\\base.py\", line 256, in main_loop\n    self._callbacks.before_train()\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorpack\\callbacks\\base.py\", line 63, in before_train\n    self._before_train()\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorpack\\callbacks\\group.py\", line 70, in _before_train\n    cb.before_train()\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorpack\\callbacks\\base.py\", line 63, in before_train\n    self._before_train()\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorpack\\callbacks\\prof.py\", line 60, in _before_train\n    start_proc_mask_signal(self._proc)\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorpack\\utils\\concurrency.py\", line 212, in start_proc_mask_signal\n    p.start()\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\multiprocessing\\process.py\", line 105, in start\n    self._popen = self._Popen(self)\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\multiprocessing\\context.py\", line 212, in _Popen\n    return _default_context.get_context().Process._Popen(process_obj)\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\multiprocessing\\context.py\", line 313, in _Popen\n    return Popen(process_obj)\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\multiprocessing\\popen_spawn_win32.py\", line 66, in init\n    reduction.dump(process_obj, to_child)\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\multiprocessing\\reduction.py\", line 59, in dump\n    ForkingPickler(file, protocol).dump(obj)\n_pickle.PicklingError: Can't pickle : attribute lookup module on builtins failed\n[0405 16:54:43 @input_source.py:149] EnqueueThread QueueInput/input_queue Exited.\nD:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\site-packages\\h5py__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from float to np.floating is deprecated. In future, it will be treated as np.float64 == np.dtype(float).type.\n  from ._conv import register_converters as _register_converters\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"D:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\multiprocessing\\spawn.py\", line 106, in spawn_main\n    exitcode = _main(fd)\n  File \"D:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\multiprocessing\\spawn.py\", line 116, in _main\n    self = pickle.load(from_parent)\nEOFError: Ran out of input\n\". @ppwwyyxx It works, thanks for your patience.  It has higher GPU utilization than Keras. I think keras can really play for fun and f I should abandon keras from now on and . Thanks again.. @ppwwyyxx  Thanks.. When I just used the TF's train function to train my model, the errors are as follows, it seems not related to tensorpack, but to some related tf errors as in https://github.com/CharlesShang/FastMaskRCNN/issues/159\nhttps://github.com/tensorflow/serving/issues/627\nSo I will close this issue, if someone knows how to solve it, I will appreciate.\n```\n2018-05-13 00:55:05.452042: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: invalid configuration argument\n2018-05-13 00:55:05.452913: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: invalid configuration argument\n     [[Node: fpn_maskrcnn_head/PyramidROIAlign/Where = Where_device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]]\n2018-05-13 00:55:05.453128: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: invalid configuration argument\n     [[Node: fpn_maskrcnn_head/PyramidROIAlign/Where = Where_device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]]\n2018-05-13 00:55:05.453190: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: invalid configuration argument\n     [[Node: fpn_maskrcnn_head/PyramidROIAlign/Where = Where_device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]]\n2018-05-13 00:55:05.453238: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: invalid configuration argument\n     [[Node: fpn_maskrcnn_head/PyramidROIAlign/Where = Where_device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]]\nTraceback (most recent call last):\n  File \"/home/chaoli/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1323, in _do_call\n    return fn(*args)\n  File \"/home/chaoli/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn\n    status, run_metadata)\n  File \"/home/chaoli/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 473, in exit\n    c_api.TF_GetCode(self.status.status))\ntensorflow.python.framework.errors_impl.InternalError: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: invalid configuration argument\n     [[Node: fpn_maskrcnn_head/PyramidROIAlign/Where = Where_device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]]\n     [[Node: fpn_maskrcnn_head/PyramidROIAlign/Where/_779 = _Recvclient_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2167_fpn_maskrcnn_head/PyramidROIAlign/Where\", tensor_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]]\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/home/chaoli/PycharmProjects/SuperCode/tensorpack-master/Tensorpack_Examples/Humanpose/test.py\", line 153, in \n    print(sess.run(mrcnn_loss, feed_dict=feed_datas))\n  File \"/home/chaoli/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 889, in run\n    run_metadata_ptr)\n  File \"/home/chaoli/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"/home/chaoli/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\n    options, run_metadata)\n  File \"/home/chaoli/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InternalError: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: invalid configuration argument\n     [[Node: fpn_maskrcnn_head/PyramidROIAlign/Where = Where_device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]]\n     [[Node: fpn_maskrcnn_head/PyramidROIAlign/Where/_779 = _Recvclient_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2167_fpn_maskrcnn_head/PyramidROIAlign/Where\", tensor_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]]\nCaused by op 'fpn_maskrcnn_head/PyramidROIAlign/Where', defined at:\n  File \"/home/chaoli/PycharmProjects/SuperCode/tensorpack-master/Tensorpack_Examples/Humanpose/test.py\", line 113, in \n    config.MASK_POOL_SIZE, config.NUM_CLASS, config.ANCHOR_STRIDES)\n  File \"/home/chaoli/tensorflow/lib/python3.5/site-packages/tensorpack/tfutils/scope_utils.py\", line 113, in wrapper\n    return func(args, kwargs)\n  File \"/home/chaoli/tensorflow/lib/python3.5/site-packages/tensorpack/tfutils/scope_utils.py\", line 52, in wrapper\n    return func(*args, kwargs)\n  File \"/home/chaoli/PycharmProjects/SuperCode/tensorpack-master/Tensorpack_Examples/Humanpose/model.py\", line 617, in fpn_maskrcnn_head\n    roi_features = PyramidROIAlign(rois, fpn_features, pool_size, features_strides)\n  File \"/home/chaoli/tensorflow/lib/python3.5/site-packages/tensorpack/tfutils/scope_utils.py\", line 84, in wrapper\n    return func(args, **kwargs)\n  File \"/home/chaoli/PycharmProjects/SuperCode/tensorpack-master/Tensorpack_Examples/Humanpose/model.py\", line 572, in PyramidROIAlign\n    index = tf.where(tf.equal(leves,level))[:,0]\n  File \"/home/chaoli/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 2439, in where\n    return gen_array_ops.where(input=condition, name=name)\n  File \"/home/chaoli/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 5930, in where\n    \"Where\", input=input, name=name)\n  File \"/home/chaoli/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/chaoli/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/chaoli/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1470, in init\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\nInternalError (see above for traceback): WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 1, status: invalid configuration argument\n     [[Node: fpn_maskrcnn_head/PyramidROIAlign/Where = Where_device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]]\n     [[Node: fpn_maskrcnn_head/PyramidROIAlign/Where/_779 = _Recvclient_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2167_fpn_maskrcnn_head/PyramidROIAlign/Where\", tensor_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]]\nProcess finished with exit code 1\n```. ",
    "kdqzzxxcc": "@PatWie \nChange this line to what? I can't figure out what's the purpose of this line, it's just get the same size of image?. @PatWie I get it. Thank you very much!. @PatWie Sorry to bother you again, I still want to know does the above mentioned error occurred due to the memory limitation of single TitanX (12GB)? so we can't solve it by multiple GPUs?. @PatWie @ppwwyyxx I get it. Thank you again!. ",
    "chengdazhi": "Thanks for your prompt reply. I am using tensorflow 1.0.1 and the newest version of tensorpack. Should I upgrade tensorflow to 1.1.0 to fix this problem?. ",
    "dralves": "Sorry if I wasn't clear, I guess my wondering is whether that line should instead be something like:\ncost = tf.reduce_mean(tf.where(tf.equal(y_true, 1.), cost * (1 - beta), cost))\n. ",
    "freegyp": "In my comprehension, it means all trainable weights in every layer. Is that right?. ",
    "deeplearningmachine": "Sorry. I thought the k (keep) selection below would perform the \"--load\" action to restore the latest saved model.\n[0602 12:16:41 @logger.py:86] WRN If you're resuming from a previous run you can choose to keep it.\n[0602 12:16:41 @logger.py:87] Select Action: k (keep) / b (backup) / d (delete) / n (new):\nIt works perfectly now when I use this one:  python nips-cifar10-resnet.py --gpu 0,1,2,3 -n 5 --load train_log_pretrain_10epoch390_/model-1170\nThanks a lot for your prompt help!. Thanks. \nSo how could I collect the real data from the lastLayerRepresentation of the ResNet? I need to collect several steps(epochs) of the lastLayerRepresentation  data and then run a clustering on the collection. \nI need to do that periodically during the training of the ResNet. . I meant the projected embeddings of the training data at the last layer (before the FC) of the ResNet. I need to collect all min-batches of one epoch before I can run my clustering.\nIt  was caused by the added eval() call in the middle of  of the main_loop(self) in the base.py. \nThe complaint came from the InferenceRunner(dataset_test,[ScalarStats('cost'), ClassificationError()]) function. If I remove the InferenceRunner, everything works fine. Any example of using tf.train.SessionRunHook? \n. Finalize the graph, create the session ...\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:0a:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:09:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:2) -> (device: 2, name: GeForce GTX TITAN X, pci bus id: 0000:06:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:3) -> (device: 3, name: GeForce GTX TITAN X, pci bus id: 0000:05:00.0)\nI tensorflow/core/common_runtime/simple_placer.cc:669] Ignoring device specification /GPU:2 for node 'tower2/input_deque' because the input edge from 'input_queue' is a reference connection and already has a device field set to /GPU:3\nI tensorflow/core/common_runtime/simple_placer.cc:669] Ignoring device specification /GPU:1 for node 'tower1/input_deque' because the input edge from 'input_queue' is a reference connection and already has a device field set to /GPU:3\nI tensorflow/core/common_runtime/simple_placer.cc:669] Ignoring device specification /GPU:0 for node 'tower0/input_deque' because the input edge from 'input_queue' is a reference connection and already has a device field set to /GPU:3\n[0602 13:49:13 @base.py:132] Graph variables initialized.\n[0602 13:49:13 @concurrency.py:36] Starting EnqueueThread ...\n[0602 13:49:13 @base.py:167] Start Epoch 1 ...\n  0%|                                                                                                              |0/390[00:00<?,?it/s]\nW tensorflow/core/kernels/queue_base.cc:294] _0_input_queue: Skipping cancelled enqueue attempt with queue not closed\nTraceback (most recent call last):\n[0602 13:49:16 @input_data.py:125] EnqueueThread Exited.\n  File \"cifar10-resnet.py\", line 599, in \n    SyncMultiGPUTrainer(config).train()\n  File \"/data/datapk/tensorpack/tensorpack/train/base.py\", line 96, in train\n    self.main_loop()\n  File \"/data/datapk/tensorpack/tensorpack/train/base.py\", line 240, in main_loop\nlast_layer_rep.append(self.model.lastLayerRepresentation.eval())\nFile \"/data/datapk/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 567, in eval\n    return _eval_using_default_session(self, feed_dict, self.graph, session)\n  File \"/data/datapk/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3729, in _eval_using_default_session\n    return session.run(tensors, feed_dict)\n  File \"/data/datapk/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 767, in run\n    run_metadata_ptr)\n  File \"/data/datapk/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 965, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/data/datapk/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\n    target_list, options, run_metadata)\n  File \"/data/datapk/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'input' with dtype float\n         [[Node: input = Placeholderdtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]]\n         [[Node: towerp0/gap/output/_3953 = _Recvclient_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_683_towerp0/gap/output\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\nCaused by op u'input', defined at:\n  File \"cifar10-resnet.py\", line 599, in \n    SyncMultiGPUTrainer(config).train()\n  File \"/data/datapk/tensorpack/tensorpack/train/base.py\", line 95, in train\n    self.setup()\n  File \"/data/datapk/tensorpack/tensorpack/train/base.py\", line 110, in setup\n    self._setup()   # subclass will setup the graph\n  File \"/data/datapk/tensorpack/tensorpack/train/multigpu.py\", line 112, in _setup\n    super(SyncMultiGPUTrainer, self)._setup()\n  File \"/data/datapk/tensorpack/tensorpack/train/feedfree.py\", line 39, in _setup\n    self._input_method.setup_training(self)\n  File \"/data/datapk/tensorpack/tensorpack/train/input_data.py\", line 158, in setup_training\n    super(QueueInput, self).setup_training(trainer)\n  File \"/data/datapk/tensorpack/tensorpack/train/input_data.py\", line 39, in setup_training\n    self.setup(trainer.model)\n  File \"/data/datapk/tensorpack/tensorpack/train/input_data.py\", line 148, in setup\n    self.input_placehdrs = model.get_reused_placehdrs()\n  File \"/data/datapk/.local/lib/python2.7/site-packages/functools32/functools32.py\", line 378, in wrapper\n    result = user_function(args, *kwds)\n  File \"/data/datapk/tensorpack/tensorpack/models/model_desc.py\", line 64, in get_reused_placehdrs\n    return self.build_placeholders()\n  File \"/data/datapk/tensorpack/tensorpack/models/model_desc.py\", line 88, in build_placeholders\n    name=prefix + v.name))\n  File \"/data/datapk/local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1502, in placeholder\n    name=name)\n  File \"/data/datapk/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2149, in _placeholder\n    name=name)\n  File \"/data/datapk/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/data/datapk/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/data/datapk/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1226, in init\n    self._traceback = _extract_stack()\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'input' with dtype float\n         [[Node: input = Placeholderdtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]]\n         [[Node: towerp0/gap/output/_3953 = _Recvclient_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_683_towerp0/gap/output\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n. Thanks a lot for the help!. Would the tensors in different towers return the same results for the _after_run? \nThanks!\n. many thanks!. before_run is just able to collect the current mini-batch data or can collect all the mini batches in a epoch? In order to have the projected representations of all the training data, I will need to collect the last layer's representations of all batches of an epoch in all the GPU towers, is it correct?. yeah, a tutorial with some examples about callbacks will be greatly beneficial for sure! . Is the Graph finalized in the callback?. any example code for outputting the last layer's representation across the entire training and testing datasets of all batches?. yes, the code works perfectly.\n100%|#####################################################################################|390/390[01:11<00:00, 4.76it/s]\n[0623 12:07:18 @base.py:254] Epoch 11 (global_step 4290) finished, time:71.23 sec.\n[0623 12:07:19 @saver.py:66] Model saved to test/model-4290.\n100%|#######################################################################################|79/79[00:05<00:00,14.95it/s]\n[0623 12:07:24 @monitor.py:294] cross_entropy_loss: 0.39552\n[0623 12:07:24 @monitor.py:294] input_queue_size: 50\n[0623 12:07:24 @monitor.py:294] learning_rate: 0.1\n[0623 12:07:24 @monitor.py:294] train_error: 0.13705\n[0623 12:07:24 @monitor.py:294] val_error: 0.1673\n[0623 12:07:24 @monitor.py:294] validation_cost: 0.85421\n[0623 12:07:24 @monitor.py:294] wd_cost: 0.33984\n[0623 12:07:24 @group.py:43] Callbacks took 5.392 sec in total. InferenceRunner: 5.286sec\n[0623 12:07:24 @base.py:167] Start Epoch 12 ...\n100%|#####################################################################################|390/390[01:11<00:00, 4.82it/s]\n[0623 12:08:36 @base.py:254] Epoch 12 (global_step 4680) finished, time:71.73 sec.\n[0623 12:08:36 @saver.py:66] Model saved to test/model-4680.\n100%|#######################################################################################|79/79[00:05<00:00,15.07it/s]\n[0623 12:08:41 @monitor.py:294] cross_entropy_loss: 0.38523\n[0623 12:08:41 @monitor.py:294] input_queue_size: 50\n[0623 12:08:41 @monitor.py:294] learning_rate: 0.1\n[0623 12:08:41 @monitor.py:294] train_error: 0.13363\n[0623 12:08:41 @monitor.py:294] val_error: 0.1769\n[0623 12:08:41 @monitor.py:294] validation_cost: 0.87989\n[0623 12:08:41 @monitor.py:294] wd_cost: 0.3224\n[0623 12:08:41 @group.py:43] Callbacks took 5.347 sec in total. InferenceRunner: 5.243sec\n[0623 12:08:41 @base.py:167] Start Epoch 13 ...\n100%|#####################################################################################|390/390[01:11<00:00, 4.95it/s]\n[0623 12:09:53 @base.py:254] Epoch 13 (global_step 5070) finished, time:71.64 sec.\n[0623 12:09:53 @saver.py:66] Model saved to test/model-5070.\n100%|#######################################################################################|79/79[00:05<00:00,14.82it/s]\n[0623 12:09:58 @monitor.py:294] cross_entropy_loss: 0.34247\n[0623 12:09:58 @monitor.py:294] input_queue_size: 50\n[0623 12:09:58 @monitor.py:294] learning_rate: 0.1\n[0623 12:09:58 @monitor.py:294] train_error: 0.11957\n[0623 12:09:58 @monitor.py:294] val_error: 0.1642\n[0623 12:09:58 @monitor.py:294] validation_cost: 0.80837\n[0623 12:09:58 @monitor.py:294] wd_cost: 0.30838\n[0623 12:09:58 @group.py:43] Callbacks took 5.416 sec in total. InferenceRunner: 5.334sec\n[0623 12:09:58 @base.py:167] Start Epoch 14 ...\n. Issue related to my browser? Got \"Rats! WebGL hits a snag!\" error from Chrome.. Thanks!. Thanks. I meant the implementation that achieves similar performance as that of the paper.. Thanks! but it seems I still got the same error message. Also, on another machine I got a different message: /lib/python2.7/site-packages/tensorpack/dataflow/common.py\", line 142, in _aggregate_batch\n    raise TypeError(\"Unsupported type to batch: {}\".format(type(dt)))\nTypeError: Unsupported type to batch: \nI use TF1.4+Python2.7. \nESC[32m[0426 10:25:23 @logger.py:74]ESC[0m Argv: imagenet-resnet.py --data ../../../imageNet2012/ --gpu 2,3 -d 18\nESC[32m[0426 10:25:23 @fs.py:89]ESC[0m ESC[5mESC[31mWRNESC[0m Env var $TENSORPACK_DATASET not set, using /home/testing/tensorpack_data for datasets.\nESC[32m[0426 10:25:24 @parallel.py:290]ESC[0m [PrefetchDataZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.\nESC[32m[0426 10:25:24 @param.py:195]ESC[0m Use train_log/imagenet-resnet/hyper.txt to set hyperparam: 'learning_rate'.\nESC[32m[0426 10:25:24 @config.py:166]ESC[0m ESC[5mESC[31mWRNESC[0m TrainConfig.nr_tower was deprecated! Set the number of GPUs on the trainer instead!\nESC[32m[0426 10:25:24 @config.py:167]ESC[0m ESC[5mESC[31mWRNESC[0m See https://github.com/ppwwyyxx/tensorpack/issues/458 for more information.\nESC[32m[0426 10:25:24 @base.py:345]ESC[0m ESC[5mESC[31mWRNESC[0m You're calling new trainers with old trainer API!\nESC[32m[0426 10:25:24 @base.py:346]ESC[0m ESC[5mESC[31mWRNESC[0m Now it returns the old trainer for you, please switch to use new trainers soon!\nESC[32m[0426 10:25:24 @base.py:347]ESC[0m ESC[5mESC[31mWRNESC[0m See https://github.com/ppwwyyxx/tensorpack/issues/458 for more information.\nESC[32m[0426 10:25:25 @input_source.py:194]ESC[0m Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...\nESC[32m[0426 10:25:25 @training.py:42]ESC[0m [DataParallel] Training a model of 2 towers.\nESC[32m[0426 10:25:25 @training.py:102]ESC[0m Building graph for training tower 0 ...\nESC[32m[0426 10:25:25 @registry.py:121]ESC[0m conv0 input: [None, 3, 448, 224]\nESC[32m[0426 10:25:25 @registry.py:129]ESC[0m conv0 output: [None, 64, 224, 112]\nESC[32m[0426 10:25:25 @registry.py:121]ESC[0m pool0 input: [None, 64, 224, 112]\nESC[32m[0426 10:25:25 @registry.py:129]ESC[0m pool0 output: [None, 64, 112, 56]\nESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group0/block0/conv1 input: [None, 64, 112, 56]\nESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group0/block0/conv1 output: [None, 64, 112, 56]\nESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group0/block0/conv2 input: [None, 64, 112, 56]\nESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group0/block0/conv2 output: [None, 64, 112, 56]\nESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group0/block1/conv1 input: [None, 64, 112, 56]\nESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group0/block1/conv1 output: [None, 64, 112, 56]\nESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group0/block1/conv2 input: [None, 64, 112, 56]\nESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group0/block1/conv2 output: [None, 64, 112, 56]\nESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group1/block0/conv1 input: [None, 64, 112, 56]\nESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group1/block0/conv1 output: [None, 128, 56, 28]\nESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group1/block0/conv2 input: [None, 128, 56, 28]\nESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group1/block0/conv2 output: [None, 128, 56, 28]\nESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group1/block0/convshortcut input: [None, 64, 112, 56]\nESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group1/block0/convshortcut output: [None, 128, 56, 28]\nESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group1/block1/conv1 input: [None, 128, 56, 28]\nESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group1/block1/conv1 output: [None, 128, 56, 28]\nESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group1/block1/conv2 input: [None, 128, 56, 28]\nESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group1/block1/conv2 output: [None, 128, 56, 28]\nESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group2/block0/conv1 input: [None, 128, 56, 28]\nESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group2/block0/conv1 output: [None, 256, 28, 14]\nESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group2/block0/conv2 input: [None, 256, 28, 14]\nESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group2/block0/conv2 output: [None, 256, 28, 14]\nESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group2/block0/convshortcut input: [None, 128, 56, 28]\nESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group2/block0/convshortcut output: [None, 256, 28, 14]\nESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group2/block1/conv1 input: [None, 256, 28, 14]\nESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group2/block1/conv1 output: [None, 256, 28, 14]\nESC[32m[0426 10:25:25 @registry.py:121]ESC[0m group2/block1/conv2 input: [None, 256, 28, 14]\nESC[32m[0426 10:25:25 @registry.py:129]ESC[0m group2/block1/conv2 output: [None, 256, 28, 14]\nESC[32m[0426 10:25:26 @registry.py:121]ESC[0m group3/block0/conv1 input: [None, 256, 28, 14]\nESC[32m[0426 10:25:26 @registry.py:129]ESC[0m group3/block0/conv1 output: [None, 512, 14, 7]\nESC[32m[0426 10:25:26 @registry.py:121]ESC[0m group3/block0/conv2 input: [None, 512, 14, 7]\nESC[32m[0426 10:25:26 @registry.py:129]ESC[0m group3/block0/conv2 output: [None, 512, 14, 7]\nESC[32m[0426 10:25:26 @registry.py:121]ESC[0m group3/block0/convshortcut input: [None, 256, 28, 14]\nESC[32m[0426 10:25:26 @registry.py:129]ESC[0m group3/block0/convshortcut output: [None, 512, 14, 7]\nESC[32m[0426 10:25:26 @registry.py:121]ESC[0m group3/block1/conv1 input: [None, 512, 14, 7]\nESC[32m[0426 10:25:26 @registry.py:129]ESC[0m group3/block1/conv1 output: [None, 512, 14, 7]\nESC[32m[0426 10:25:26 @registry.py:121]ESC[0m group3/block1/conv2 input: [None, 512, 14, 7]\nESC[32m[0426 10:25:26 @registry.py:129]ESC[0m group3/block1/conv2 output: [None, 512, 14, 7]\nESC[32m[0426 10:25:26 @registry.py:121]ESC[0m gap input: [None, 512, 14, 7]\nESC[32m[0426 10:25:26 @registry.py:129]ESC[0m gap output: [None, 512]\nESC[32m[0426 10:25:26 @registry.py:121]ESC[0m linear input: [None, 512]\nESC[32m[0426 10:25:26 @registry.py:129]ESC[0m linear output: [None, 1000]\nESC[32m[0426 10:25:26 @registry.py:121]ESC[0m linear_2 input: [None, 512]\nESC[32m[0426 10:25:26 @registry.py:129]ESC[0m linear_2 output: [None, 1000]\nESC[32m[0426 10:25:26 @regularize.py:88]ESC[0m regularize_cost() found 22 variables to regularize.\nESC[32m[0426 10:25:26 @regularize.py:19]ESC[0m The following tensors will be regularized: conv0/W:0, group0/block0/conv1/W:0, group0/block0/conv2/W:0, group0/block1/conv1/W:0, group0/block1/conv2/W:0, group1/block0/conv1/W:0, group1/bloc\nk0/conv2/W:0, group1/block0/convshortcut/W:0, group1/block1/conv1/W:0, group1/block1/conv2/W:0, group2/block0/conv1/W:0, group2/block0/conv2/W:0, group2/block0/convshortcut/W:0, group2/block1/conv1/W:0, group2/block1/conv2/W:0, group3/bl\nock0/conv1/W:0, group3/block0/conv2/W:0, group3/block0/convshortcut/W:0, group3/block1/conv1/W:0, group3/block1/conv2/W:0, linear/W:0, linear_2/W:0\nESC[32m[0426 10:25:26 @training.py:102]ESC[0m Building graph for training tower 1 ...\ngroup3/block0/conv2/W:0          [3, 3, 512, 512]  2359296\ngroup3/block0/convshortcut/W:0   [1, 1, 256, 512]   131072\ngroup3/block1/preact/bn/gamma:0  [512]                 512\ngroup3/block1/preact/bn/beta:0   [512]                 512\ngroup3/block1/conv1/W:0          [3, 3, 512, 512]  2359296\ngroup3/block1/conv1/bn/gamma:0   [512]                 512\ngroup3/block1/conv1/bn/beta:0    [512]                 512\ngroup3/block1/conv2/W:0          [3, 3, 512, 512]  2359296\nbnlast/bn/gamma:0                [512]                 512\nbnlast/bn/beta:0                 [512]                 512\nlinear/W:0                       [512, 1000]        512000\nlinear/b:0                       [1000]               1000\nlinear_2/W:0                     [512, 1000]        512000\nlinear_2/b:0                     [1000]               1000ESC[36m\nTotal #vars=58, #params=12200720, size=46.54MBESC[0m\nESC[32m[0426 10:25:27 @base.py:142]ESC[0m Setup callbacks graph ...\nESC[32m[0426 10:25:27 @predict.py:42]ESC[0m Building predictor tower 'InferenceTower' on device /gpu:0 ...\nESC[32m[0426 10:25:27 @collection.py:165]ESC[0m These collections were modified but restored in InferenceTower: (tf.GraphKeys.SUMMARIES: 7->8)\nESC[32m[0426 10:25:27 @summary.py:39]ESC[0m Maintain moving average summary of 4 tensors in collection MOVING_SUMMARY_OPS.\nESC[32m[0426 10:25:27 @summary.py:76]ESC[0m Summarizing collection 'summaries' of size 7.\nESC[32m[0426 10:25:27 @graph.py:92]ESC[0m Applying collection UPDATE_OPS of 34 ops.\nESC[32m[0426 10:25:32 @base.py:147]ESC[0m Creating the session ...\nESC[32m[0426 10:25:39 @base.py:151]ESC[0m Initializing the session ...\nESC[32m[0426 10:25:39 @base.py:158]ESC[0m Graph Finalized.\nESC[32m[0426 10:25:40 @inference_runner.py:100]ESC[0m InferenceRunner will eval 782 iterations\nESC[32m[0426 10:25:40 @concurrency.py:38]ESC[0m Starting EnqueueThread QueueInput/input_queue ...\nESC[32m[0426 10:25:40 @base.py:192]ESC[0m Start Epoch 1 ...\nESC[32m[0426 10:25:40 @input_source.py:502]ESC[0m Pre-filling StagingArea ...\nESC[32m[0426 10:25:40 @input_source.py:143]ESC[0m ESC[4mESC[5mESC[31mERRESC[0m Exception in EnqueueThread QueueInput/input_queue:\nTraceback (most recent call last):\n  File \"/home/testing/condaEnv2/lib/python2.7/site-packages/tensorpack/input_source/input_source.py\", line 133, in run\n    dp = next(self._itr)\n  File \"/home/testing/condaEnv2/lib/python2.7/site-packages/tensorpack/dataflow/common.py\", line 339, in get_data\n    for dp in self.ds.get_data():\n  File \"/home/testing/condaEnv2/lib/python2.7/site-packages/tensorpack/dataflow/common.py\", line 274, in get_data\n    for dp in self.ds.get_data():\n  File \"/home/testing/condaEnv2/lib/python2.7/site-packages/tensorpack/dataflow/common.py\", line 119, in get_data\n    yield BatchData._aggregate_batch(holder, self.use_list)\n  File \"/home/testing/condaEnv2/lib/python2.7/site-packages/tensorpack/dataflow/common.py\", line 142, in _aggregate_batch\n    raise TypeError(\"Unsupported type to batch: {}\".format(type(dt)))\nTypeError: Unsupported type to batch: \nESC[32m[0426 10:25:40 @input_source.py:149]ESC[0m EnqueueThread QueueInput/input_queue Exited.\nESC[32m[0426 10:25:40 @base.py:208]ESC[0m Training was stopped.\n. Sorry, I used some old codes. It works perfectly now.. If I use four GPUs each with batch size of 16. Will that be similar to run the network on one GPU with batch size of 64?. any build-in function in Tensorpack?. doing so, the permutation for the mixup could be incorrect? That is, you will have smaller BATCH_SIZE for the permutation?. If the batch size is 128 and 2 GPUs are used, then each GPU will has a batchsize of 64. That is, the permutation of mixup will use a batch of 64 images. Is that correct? Any example of \"write a dataflow that mixes more images than it yields every time\"? Thanks a lot!. Did you mean the following?\nds = BatchData(ds, batch, remainder=not isTrain)\nds=split(ds)\n. Any build-in function for the split? Also, is it possible to have two batches during test phase: one from BatchData(testing data) and the other from BatchData(training data)?. During the test phase, when I have a batch of data points for testing, I would like to randomly add some data points from the training set (ideally, a small batch from the training data). . Can I code something like this?\ndef get_data(train_or_test, isMixup, alpha):\n    ds_test = dataset.Cifar10(test)\n    ds_train = dataset.Cifar10(train)\n   ds_test = BatchData(ds_test, batch, remainder=not isTrain)\n   ds_train = BatchData(ds_train, batch, remainder=not isTrain)\n   mix(ds_test,ds_train). Thanks!  Any chance I can have two batches of data points when calling BatchData? \nThat is, when I call  ds = BatchData(ds, batch, remainder=not isTrain), I would like the ds returns two random batches of data points.\n. Can I retrieve two batches of data points from BatchData(ds, batch)?\n. Thanks a lot!!. Can I define that \"itr\" within the get_data() function?\ndef get_data(train_or_test, isMixup, alpha):\n    ds = dataset.Cifar10(train_or_test)\n    batch = BATCH_SIZE\n    ds = BatchData(ds, batch, remainder=not isTrain)\nSpecifically, I want the following codes in mixup to mix two batches of data:\n    ds = BatchData(ds, batch, remainder=not isTrain)\n    def f(dp):\n        images, labels = dp\n        one_hot_labels = np.eye(CLASS_NUM)[labels]  # one hot coding\n        if not isTrain or not isMixup:\n            return [images, one_hot_labels]\n    # mixup implementation:\n    # Note that for larger images, it's more efficient to do mixup on GPUs (i.e. in the graph)\n    weight = np.random.beta(alpha, alpha, BATCH_SIZE)\n    x_weight = weight.reshape(BATCH_SIZE, 1, 1, 1)\n    y_weight = weight.reshape(BATCH_SIZE, 1)\n    index = np.random.permutation(BATCH_SIZE)\n\n    x1, x2 = images, images[index]\n    x = x1 * x_weight + x2 * (1 - x_weight)\n    y1, y2 = one_hot_labels, one_hot_labels[index]\n    y = y1 * y_weight + y2 * (1 - y_weight)\n    return [x, y]\n\nds = MapData(ds, f)\n\n. Many thanks!. I did use some codes like below, but it seemed to use too much memory to load the data. I wonder if there is a way using less memory?\n\ndef get_data(train_or_test, fake=False):\n    if fake:\n        return FakeData([[64, INPUT_SHAPE, INPUT_SHAPE, 3], [64]], 1000, random=False, dtype='uint8')\n    isTrain = train_or_test == 'train'\n    datadir = args.data\n    ds = dataset.ILSVRC12(datadir, train_or_test,\n                          shuffle=True if isTrain else False, dir_structure='original')\nds = FixedSizeData(ds,100000)\n\n---. No, it seemed it used too much GPU memory for that.. I see. so, it may be caused by the image size? \nwhen I crop the image to smaller size, say 200 instead of 224 as your default value, the whole net can fit into my GPU. But with smaller image size, all the Resnet parameters such as learning rate may need to be retuned. . on https://github.com/tensorpack/tensorpack/tree/master/examples/ResNet\nyou highlight one of the baselines:\nResNet18 | 10.50% | 29.66%\nwhich was obtained with 8 GPUs and a total batch size of 256.\nDo you have a benchmark accuracy for only one GPU? \nAlso, I wonder if you have an idea on what the optimal batchsize will be for only one GPU on ResNet18?\nWhen I ran with one GPU/ batch size of 64. I got: 13.1% and 33.8% for the top5 and top1, respectively. Those numbers are 3% lower than what you got. \n. would \"8 GPUs and a total batch size of 256 \" be similar to  \"1 GPUs and a total batch size of 32 \"?. Sorry; just tried to reproduce your benchmark accuracy with my limited GPU power. \nI will look into how you synchronize the results of the 8 GPUs and how you distribute the 256 samples in the batch to the 8 GPUs in your implementation. Thanks!\n. ",
    "lispc": "ok~ I will close the pull request~ . ",
    "stephen-song": "@PatWie hi \uff0cCan you re-deliver a link for more details (why and how) ?The previous link is no longer accessible.. sorry to bother , I did not read the relevant issues carefully \uff0cNow I found some references here.\n. ",
    "mikeun": "Hi,\nI'd like to initialize some variables after each epoch,\nI can set up op while _setup_graph() with init_op=tf.variables_initializer(varlist)  but how do I run it in _trigger_epoch()?. Thank you. \nI see that it is just tensorflow graph, but your queue to plug new data into it - it is not just \"feed\" ).\nI use your tensorpack and it works well to push my GPU to 100% - \nI added stat function like confusion matrix.\n'''\n\u001b[32m[1007 15:52:17 @monitor.py:355]\u001b[0m confusion matrix of class 0: | 38643|  2818|   431|    86|, Recall:92.06, Precision: 84.61\n\u001b[32m[1007 15:52:17 @monitor.py:355]\u001b[0m confusion matrix of class 1: |  6174| 12156|  1510|    59|, Recall:61.09, Precision: 72.79\n\u001b[32m[1007 15:52:17 @monitor.py:355]\u001b[0m confusion matrix of class 2: |   795|  1662|  6974|   313|, Recall:71.57, Precision: 76.09\n\u001b[32m[1007 15:52:17 @monitor.py:355]\u001b[0m confusion matrix of class 3: |    58|    63|   250|  1736|, Recall:82.39, Precision: 79.12\n'''\nbut it adds new line with new epoch in the tensorboard scalars. \n\nQuestion:\n1) how can I turn off output my confusion matrix to tensorboard\n2) why my test step (inference with test data) is not the same (quantities of data-points in the confusion matrix inside one class are not the same)?\n'''\n[0m confusion matrix of class 0: | 39992|  2136|   128|   104|\n[0m confusion matrix of class 1: |  6744| 11696|   704|    80|\n[0m confusion matrix of class 2: |   896|  1568|  7160|   344|\n[0m confusion matrix of class 3: |    48|    40|   192|  1896|\n'''\ncompare with the same test set but another run:\n'''\n[0m confusion matrix of class 0: | 38643|  2818|   431|    86|\n[0m confusion matrix of class 1: |  6174| 12156|  1510|    59|\n[0m confusion matrix of class 2: |   795|  1662|  6974|   313|\n[0m confusion matrix of class 3: |    58|    63|   250|  1736|\n'''. 2. My DataFlow = DataPoints are dumped to tfrecords by your dump_dataflow_to_tfrecord() to make  data injection faster (I added compress option to make result files much smaller). So test set is the same all times:\n'''\ntest = TFRecordData('./test_.tfrecords', size=73768)\ntest = PrefetchDataZMQ(test, nr_proc=4, hwm=100)\ntest = BatchData(test, BATCH_SIZE)\n'''\nmaybe that is consequent of \"nr_proc=4\"?. Thank you.. Hi,\nI'd like to implement NN model with two cost functions.\nFirst part - network will classify input data - tune all weights (W) by minimizing weighted logloss.\nSecond part will take some of these weights (W), apply linear algebra functions and minimize result (L2 or L1 loss). Second part should work on the (W) also.\nWhat kind of approach (your examples) will you recommend?. I use BatchData(remainder=True). \nThis error was eliminated when I setup BatchData(remainder=False).\none another question:\n- if I use eval() method - does it force new batch of data from input queue?\n. I meant tensor.eval() method - just call to get an value of tensor.\nprogram logic is -\n- it runs train step of current batch. \n- collect step: I'd like to collect labels and predictions of current batch to save them into corresponded dictionary with lists inside. So I use tensor_of_prediction.eval() and tensor_of_labels.eval() method to get values of labels and predictions. This .eval() method should use current data batch to do such step properly.\nthe question is - does .eval() method use current input data batch to calculate forward run or it reads next batch of data? . Now I use   \ndef _trigger_step(self):\n        outputs = [tensor.eval() for tensor in self.outputs]\n        self.on_fetches(outputs)\nwhere is self.outputs - list of tensors. How can I get values of this tensors in after_run method?. I've got it works \nhere is callback:\n`# -- coding: UTF-8 --\nimport numpy as np\nimport six\nfrom tensorpack.callbacks.base import Callback\nfrom tensorpack.utils import logger\nfrom tensorpack.utils.stats import ConfusionMatrix\nfrom tensorpack.tfutils.common import get_op_tensor_name\nimport tensorflow as tf\nall = ['AfterTrainEpoch','ConfusionMatrixAfterEpoch']\nclass AfterTrainEpoch(Callback):\ndef _setup_graph(self):\n    train_tower = self.trainer.tower_func.towers[0]\n    self.outputs = [train_tower.get_tensor(name) for name in self.get_fetches()]\n\ndef _before_train(self):\n    pass\n\ndef _before_epoch(self):\n    self._before_train_epoch()\n\ndef _before_train_epoch(self):\n    pass\n\ndef _before_run(self, _):\n    return tf.train.SessionRunArgs(fetches=self.outputs)\n\ndef _after_run(self, _, run_values):\n    self.on_fetches(run_values.results)\n\ndef on_fetches(self, outputs):\n    self._on_fetches(outputs)\n\ndef _on_fetches(self, outputs):\n    raise NotImplementedError()\n\ndef _trigger_epoch(self):\n    ret = self._after_train_epoch()\n    if ret is None:\n        return\n    for k, v in six.iteritems(ret):\n        try:\n            v = float(v)\n        except ValueError:\n            logger.warn(\"{} returns a non-scalar statistics!\".format(type(self).__name__))\n            continue\n        else:\n            self.trainer.monitors.put_scalar(k, v)\n\ndef _after_train_epoch(self):\n    pass\n\ndef get_fetches(self):\n    try:\n        ret = self._get_fetches()\n    except NotImplementedError:\n        logger.warn(\"Inferencer._get_output_tensors was deprecated and renamed to _get_fetches\")\n        ret = self._get_output_tensors()\n\n    return [get_op_tensor_name(n)[1] for n in ret]\n\ndef _get_fetches(self):\n    raise NotImplementedError()\n\ndef _get_output_tensors(self):\n    pass\n\nclass ConfusionMatrixAfterEpoch(AfterTrainEpoch):\n    \"\"\"\n    Compute precision / recall of classification, given the\n    prediction vector and the label vector.\n    \"\"\"\ndef __init__(self, pred_tensor_name, label_tensor_name, labels_list):\n    \"\"\"\n    Args:\n        pred_tensor_name(str): name of the 0/1 prediction tensor.\n        label_tensor_name(str): name of the 0/1 label tensor.\n    \"\"\"\n    self.label_list = labels_list\n    self.pred_tensor_name = pred_tensor_name\n    self.label_tensor_name = label_tensor_name\n    self.stat = ConfusionMatrix()\n    self.labels = {\n            'labels': [],\n            'predic': []\n            }\n\ndef _before_train_epoch(self):\n    self.labels['labels']=[]\n    self.labels['predic']=[]\n\ndef _get_fetches(self):\n    return [self.pred_tensor_name, self.label_tensor_name]\n\ndef _on_fetches(self, outputs):\n    pred, label = outputs\n    self.labels['labels'].append(label)\n    self.labels['predic'].append(pred)\n\ndef _after_train_epoch(self):\n    self.stat.feed(np.concatenate(self.labels['labels'], axis=0),\n                   np.concatenate(self.labels['predic'], axis=0), self.label_list)\n    out_dict = {}\n    for i in range(self.stat.get_conf_matr.shape[0]):\n        sum_recall =  self.stat.get_conf_matr[i,:].sum()\n        if not bool(sum_recall):\n            recall = -1\n        else:\n            recall = round(self.stat.get_conf_matr[i,i] / sum_recall * 100, 2)\n        sum_precision = self.stat.get_conf_matr[:,i].sum()\n        if not bool(sum_precision):\n            precision = -1\n        else:\n            precision = round(self.stat.get_conf_matr[i,i] / sum_precision * 100, 2)\n        tab='|'\n        for j in range(self.stat.get_conf_matr.shape[1]):\n            tab += '{: =6.0f}|'.format(self.stat.get_conf_matr[i,j])\n        name = 'train conf matrix of class {}: {}, Recall:{: =5.2f}, Precision'.format(i,tab, recall)\n        out_dict[name] = precision\n    return out_dict\n\n`\nand I added function to utils.stats\n`\nfrom sklearn.metrics import confusion_matrix\nclass ConfusionMatrix(object):\n    \"\"\"\n    Statistics for one_hot mutliclass binary decision,\n    including precision, recall, false positive, false negative\n    \"\"\"\ndef __init__(self):\n    self.reset()\n\ndef reset(self):\n    self.conf_matr = None  # positive label\n\ndef feed(self, pred, label, labels):\n    \"\"\"\n    Args:\n        pred (np.ndarray): binary array.\n        label (np.ndarray): binary array of the same size.\n    \"\"\"\n    assert pred.shape == label.shape, \"{} != {}\".format(pred.shape, label.shape)\n    self.conf_matr = confusion_matrix(label, pred, labels)\n\n@property\ndef get_conf_matr(self):\n    return self.conf_matr\n\n`. 1. here is log of error:\n```\n[0605 18:05:11 @graph.py:91] Applying collection UPDATE_OPS of 8 ops.\n[libprotobuf FATAL external/protobuf_archive/src/google/protobuf/message_lite.cc:68] CHECK failed: (byte_size_before_serialization) == (byte_size_after_serialization): tensorflow.GraphDef was modified concurrently during serialization.\nterminate called after throwing an instance of 'google::protobuf::FatalException'\n  what():  CHECK failed: (byte_size_before_serialization) == (byte_size_after_serialization): tensorflow.GraphDef was modified concurrently during serialization.\nProcess finished with exit code 134 (interrupted by signal 6: SIGABRT)`\n```\nthere is no \"creating session ....\"\n\n\nMy program is crashed consistently. I have not any other version of TF.\n\n\nconfig in this case is:\n\n\ndataset_train, dataset_test, _ = get_data(command=command)\n        steps_per_epoch = dataset_train.size()\n        dataset_train = StagingInput(QueueInput(dataset_train))\n        dataset_test = QueueInput(dataset_test)\noutput_config = {\n                'steps_per_epoch': steps_per_epoch,\n                'callbacks': [\n                    ModelSaver(max_to_keep=10, keep_checkpoint_every_n_hours=0.33),\n                    MaxSaver('MAF_loss', filename='max_log_prob'),\n                    LogProbAfterEpoch(\"EstimatePDF/MAF_loss\"),\n                    InferenceRunner(\n                        dataset_test,\n                        [\n                            LogProbSum(\"EstimatePDF/MAF_loss\")\n                        ]\n                    ),\n                    ScheduledHyperParamSetter('optimizer/learning_rate', [\n                        (a.pdf_estimator['drop0'], a.pdf_estimator['lr0']),\n                        (a.pdf_estimator['drop1'], a.pdf_estimator['lr1'])\n                    ])\n                ],\n                'max_epoch': a.pdf_estimator['hm_epoch']\n        }\n\nhere is code of collections:\n\n```\n        analyze_variables = tf.get_collection(\n            tf.GraphKeys.TRAINABLE_VARIABLES,\n            scope='.analyze'\n        )\n        pdf_variables = tf.get_collection(\n            tf.GraphKeys.TRAINABLE_VARIABLES,\n            scope='.EstimatePDF'\n        )\n        train_variables = tf.get_collection(\n            tf.GraphKeys.TRAINABLE_VARIABLES,\n            scope='^(?!(.(EstimatePDF|analyze))).$'\n        )\n        databus['analyze_variables'] = analyze_variables\n        databus['pdf_variables'] = pdf_variables\n        databus['train_variables'] = train_variables\n    G = tf.get_default_graph()\n    for tensor in databus['analyze_variables']:\n        G.add_to_collection(name='analyze_variables', value=tensor)\n    for tensor in databus['pdf_variables']:\n        G.add_to_collection(name='pdf_variables', value=tensor)\n    for tensor in databus['train_variables']:\n        G.add_to_collection(name='train_variables', value=tensor)\n\n```\nI don't think I put any non variable into the collections.\nhere are callbacks I used:\nclass AfterTrainEpoch(Callback):\n    def _setup_graph(self):\n        self.train_tower = self.trainer.tower_func.towers[0]\n        self.outputs = [self.train_tower.get_tensor(name) for name in self.get_fetches()]\n    def _before_train(self):\n        pass\n    def _before_epoch(self):\n        self._before_train_epoch()\n    def _before_train_epoch(self):\n        pass\n    def _before_run(self, _):\n        return tf.train.SessionRunArgs(fetches=self.outputs)\n    def _after_run(self, _, run_values):\n        self.on_fetches(run_values.results)\n    def on_fetches(self, outputs):\n        self._on_fetches(outputs)\n    def _on_fetches(self, outputs):\n        raise NotImplementedError()\n    def _trigger_epoch(self):\n        ret = self._after_train_epoch()\n        if ret is None:\n            return\n        for k, v in six.iteritems(ret):\n            try:\n                v = float(v)\n            except ValueError:\n                logger.warn(\"{} returns a non-scalar statistics!\".format(type(self).__name__))\n                continue\n            else:\n                self.trainer.monitors.put_scalar(k, v)\n    def _after_train_epoch(self):\n        pass\n    def get_fetches(self):\n        try:\n            ret = self._get_fetches()\n        except NotImplementedError:\n            logger.warn(\"Inferencer._get_output_tensors was deprecated and renamed to _get_fetches\")\n            ret = self._get_output_tensors()\n        return [get_op_tensor_name(n)[1] for n in ret]\n    def _get_fetches(self):\n        raise NotImplementedError()\n    def _get_output_tensors(self):\n        pass\nclass LogProbAfterEpoch(AfterTrainEpoch):\n    def __init__(self, log_prob_tensor_name):\n        \"\"\"\n        Args:\n            log_prob_tensor_name(str): name of the LogProb tensor.\n        \"\"\"\n        self.log_prob_tensor_name = log_prob_tensor_name\n    def _before_train_epoch(self):\n        self.collector = list()\n    def _get_fetches(self):\n        return [self.log_prob_tensor_name]\n    def _on_fetches(self, outputs):\n        log_prob = outputs[0]\n        self.collector.append(log_prob)\n    def _after_train_epoch(self):\n        log_prob = np.concatenate(self.collector, axis=0)\n        return {\n            'train_MAF_loss': np.sum(log_prob)\n        }\nclass LogProbSum(Inferencer):\n    def __init__(self, log_prob_tensor_name):\n        \"\"\"\n        Args:\n            log_prob_tensor_name(str): name of the LogProb tensor.\n        \"\"\"\n        self.log_prob_tensor_name = log_prob_tensor_name\n    def _before_inference(self):\n        self.collector = list()\n    def _get_fetches(self):\n        return [self.log_prob_tensor_name]\n    def _on_fetches(self, outputs):\n        log_prob = outputs[0]\n        self.collector.append(log_prob)\n    def _after_inference(self):\n        log_prob = np.concatenate(self.collector, axis=0)\n        return {\n            'MAF_loss': np.sum(log_prob)\n        }\n. Also I found this issue:  https://github.com/tensorflow/tensorflow/issues/19657\nabout the same problem.. Does it mean - I have value ~ 2**32 in my dataflow?\nI made all my data normalized from 0 to 1 and saved it to TFrecords with compression.\nDataflow:\n```\n        train_size, test_size = get_train_test_size()\n    train = TFRecordData(\n        a.folders['inp'] + a.tf_records['train_file'],\n        size=train_size,\n        compress=True\n    )\n    train = BatchData(\n        train,\n        a.start_args['train_batch_size'],\n        remainder=True)\n\n    test = TFRecordData(\n        a.folders['inp'] + a.tf_records['test_file'],\n        size=test_size,\n        compress=True\n    )\n    test = BatchData(\n        test,\n        a.start_args['test_batch_size'],\n        remainder=True)\n\n. I don't understand what does it mean exactly?\n[0606 10:50:47 @collection.py:151] Size of these collections were changed in InferenceTower: (analyze_variables: 1->2), (pdf_variables: 72->144), (train_variables: 174->348)\n[0606 10:50:47 @collection.py:164] These collections were modified but restored in InferenceTower: (tf.GraphKeys.UPDATE_OPS: 8->16)\n```\nwhy does it change collections size (*2) during InferenceTower?\nI can collect variables without creating collection - maybe this will resolve the problem?. I reduced number of parameters and the model goes on. Maybe there is number of parameters limit.. I print >>>>>>>>>>>>>>>>>>>>>>>>><><><><>< -------------------- epoch_N\nin get_data() of Dataflow each time it should start new epoch.\nSettings are:\nepoch size is 10240\nbatch size is 1024\nso epoch is 10 batches by 1024 instances.\nbut I have got epoch  = 410624 instances = 401 times by 1024 instances (1024 batch size). BatchQueueInput should make batches by 1024 instances 10 times per epoch only - but result is 401 times per epoch.. my Dataflow generate 10240 datapoints in \"for\" cycle - it should be epoch.\nit prints \">>>>>>>>>>>>>>>>>>>>>>>>><><><><>< -------------------- epoch_counter\" before this \"for\" cycle. So I see this print each time it should start new epoch and generate 10240 dataponts.\nBut I see this print many (41) times inside one epoch. \n410624 - this is from confusion matrix - it collects all instances of current epoch (each step generates batch_size of results - this callback collects them after each step and print confusion matrix after epoch for all results in current epoch) and shows them in a table - so sum of them equals number of all instances.\nWhere is place to stop batches and finish an epoch?\nWhy BatchQueueInput gathers much more datapoints?. All this about to change datapoints values of different epochs. I feed different values in different epochs. . ```\n                'callbacks': [\n                    ConfusionMatrixAfterEpoch('prediction/predicted_class', 'InputBlock/label/label', label_list),\n                    ModelSaver(max_to_keep=3, keep_checkpoint_every_n_hours=6),\n                    AnalyzeTensors(\n                        save_tensors=['^InputBlock/email_features$'],  # ['^InputBlock/analyze/mix_tensor'],\n                        save_ids=['^InputBlock/label/customer$'],  # ['^InputBlock/analyze/mix_tensor'],\n                        save_prob=['^prediction/predicted_prob$'],  # ['^InputBlock/analyze/mix_tensor'],\n                        init_variables=['analyze'],\n                        features_names=get_features(select='email_history')\n                    ),\n                    ScheduledHyperParamSetter(\n                        'optimizer/learning_rate',\n                        [\n                            (a.analyze['drop0'], a.analyze['lr0'])\n                        ]\n                    )\n                ]\n```\n```\nclass AfterTrainEpoch(Callback):\ndef _setup_graph(self):\n    self.train_tower = self.trainer.tower_func.towers[0]\n    self.outputs = [self.train_tower.get_tensor(name) for name in self.get_fetches()]\n\ndef _before_train(self):\n    pass\n\ndef _before_epoch(self):\n    self._before_train_epoch()\n\ndef _before_train_epoch(self):\n    pass\n\ndef _before_run(self, _):\n    return tf.train.SessionRunArgs(fetches=self.outputs)\n\ndef _after_run(self, _, run_values):\n    self.on_fetches(run_values.results)\n\ndef on_fetches(self, outputs):\n    self._on_fetches(outputs)\n\ndef _on_fetches(self, outputs):\n    raise NotImplementedError()\n\ndef _trigger_epoch(self):\n    ret = self._after_train_epoch()\n    if ret is None:\n        return\n    for k, v in six.iteritems(ret):\n        try:\n            v = float(v)\n        except ValueError:\n            logger.warn(\"{} returns a non-scalar statistics!\".format(type(self).__name__))\n            continue\n        else:\n            self.trainer.monitors.put_scalar(k, v)\n\ndef _after_train_epoch(self):\n    pass\n\ndef get_fetches(self):\n    try:\n        ret = self._get_fetches()\n    except NotImplementedError:\n        logger.warn(\"Inferencer._get_output_tensors was deprecated and renamed to _get_fetches\")\n        ret = self._get_output_tensors()\n\n    return [get_op_tensor_name(n)[1] for n in ret]\n\ndef _get_fetches(self):\n    raise NotImplementedError()\n\ndef _get_output_tensors(self):\n    pass\n\n```\n```\nclass AnalyzeTensors(AfterTrainEpoch):\n    \"\"\"\n    Save calculated tensors and initialize feature variables\n    \"\"\"\ndef __init__(self,\n             save_tensors: list,\n             save_ids: list,\n             save_prob: list,\n             init_variables: list,\n             features_names: list\n             ):\n    \"\"\"\n    Args:\n        save_tensors(str): pattern of name tensor to save into file.\n        init_variables(str): pattern of tensors to init.\n    \"\"\"\n    super().__init__()\n    self.save_ids = save_ids\n    self.save_prob = save_prob\n    self.save_tensors = save_tensors\n    self.init_variables = init_variables\n    self.features_names = features_names\n    self.tensor_values_output_list = list()\n    self.prob_output_list = list()\n    self.tensor_values_output = None\n    self.save_tensors_list = None\n\ndef _setup_graph(self):\n    G = tf.get_default_graph()\n    self.save_tensors_list = list()\n    patterns_list = list()\n    patterns_list.extend(self.save_tensors)\n    patterns_list.extend(self.save_ids)\n    patterns_list.extend(self.save_prob)\n\n    for pattern in patterns_list:\n        self.save_tensors_list.extend([\n            x.name for x in G.get_operations() if re.search(pattern, x.name)\n        ])\n\n    self.ids_output_list = list()\n\n    init_variables_list = list()\n    for pattern in self.init_variables:\n        init_variables_list.extend([\n            x for x in tf.trainable_variables() if re.search(pattern, x.name)\n        ])\n    self.init_op = tf.variables_initializer(init_variables_list, name='init_analyze_vars')\n\n    super()._setup_graph()\n\ndef _get_fetches(self):\n    return self.save_tensors_list\n\ndef _on_fetches(self, outputs):\n    self.tensor_values_output, self.ids_output, self.prob_output = outputs\n\ndef _trigger_epoch(self):\n    self.tensor_values_output_list.append(self.tensor_values_output)\n    self.ids_output_list.append(self.ids_output)\n    self.prob_output_list.append(self.prob_output)\n    self.init_op.run()\n\ndef _after_train(self):\n    a = GetConfig()\n    index_values = np.concatenate(self.ids_output_list, axis=0)\n    prob_values = np.concatenate(self.prob_output_list, axis=0)\n    prob_values = pd.DataFrame(\n        prob_values,\n        index=index_values.flatten(),\n        columns=[\"prob_{:0>4d}\".format(i) for i in range(a.hm_labels)],\n        dtype=np.float32)\n    all_values = np.concatenate(tuple(self.tensor_values_output_list), axis=0)\n    all_values = pd.DataFrame(all_values, index=index_values.flatten(), columns=self.features_names)\n    all_values = pd.concat([all_values, prob_values], axis=1)\n    folder_to_save = a.folders['analyze']  #  + \"{}/\".format(self.analyze_id)\n    create_folders([folder_to_save])\n    table_file = folder_to_save + a.analyze['prediction_file']\n    check_files([table_file])\n    with pd.HDFStore(table_file, chunkshape='auto', complevel=9, complib='blosc', mode='w') as store:\n        store.append(a.analyze['res_table_name'], all_values)\n    print(\"result is saved to \\\"{}\\\"\".format(table_file))\n\n.\ndef _setup_graph(self):\n    self.init_op = tf.variables_initializer(init_variables_list, name='init_analyze_vars')\n\ndef _trigger_epoch(self):\n...\n    self.init_op.run() <--- here is problem??\n```\n. I've got same bad results - \nI didn't use simple trainer. \nI use trainer with many optimizer ops:\n```\nclass AnalyzeTrainer(TowerTrainer):\n    def init(self, input, model, period_class=1, period_pdf=1, period_ohe_l1=1):\n        \"\"\"\n        Args:\n            input (InputSource):\n            model (AnalyzeModelDesc):\n        \"\"\"\n        super(AnalyzeTrainer, self).init()\n        assert isinstance(model, AnalyzeModelDesc), model\n        assert isinstance(model, AnalyzeModelDesc), model\n        assert min(period_class, period_pdf, period_ohe_l1) == 1\n        self.period_class = period_class\n        self.period_pdf = period_pdf\n        self.period_ohe_l1 = period_ohe_l1\n        self.period_ohe_l2 = period_ohe_l2\n    inputs_desc = model.get_inputs_desc()\n    # Setup input\n    cbs = input.setup(inputs_desc)\n    self.register_callback(cbs)\n\n    self.tower_func = TowerFuncWrapper(model.build_graph, inputs_desc)\n    with TowerContext('', is_training=True):\n        self.tower_func(*input.get_input_tensors())\n    opt = model.get_optimizer()\n\n    # Define the training iteration\n    with tf.name_scope('optimize'):\n            self.train_minimize_class = opt.minimize(\n                model.analyze_class_cost,\n                var_list=model.varlist_analyze,\n                name='minimize_analyze_class_cost'\n            )\n\n            self.train_minimize_pdf = opt.minimize(\n                model.analyze_pdf_cost,\n                var_list=model.varlist_analyze,\n                name='minimize_analyze_pdf_cost'\n            )\n\n            self.train_minimize_ohe_l1 = list()\n            for tensor in model.ohe_cost_l1:\n                self.train_minimize_ohe_l1.append(\n                    opt.minimize(\n                        tensor,\n                        var_list=model.varlist_analyze,\n                        name='minimize_ohe_l1/' + get_feature_name(tensor.name)\n                    )\n                )\n\ndef run_step(self):\n        run_op = list()\n        if self.global_step % self.period_class == 0:\n            run_op.append(self.train_minimize_class)\n        if self.global_step % self.period_pdf == 0:\n            run_op.append(self.train_minimize_pdf)\n        if self.global_step % self.period_ohe_l1 == 0:\n            run_op.extend(self.train_minimize_ohe_l1)\n\n        for op in run_op:\n            self.hooked_sess.run(op)\n\n```. I've found out that it runs get_data() for each self.hooked_sess.run(op).\nHow can I made Dataflow to be able to control trigger of new epoch? or can you show example of Dataflow like Callback.. ",
    "chuber1986": "Wrapping the predictor in 'with tf.device(...)' doesn't work because the OfflinePredictor builds the graph within an TowerContex which uses an '\\gpu:0' as default.\nI build want to use a Resnet to preprocess the input for another  Resnet (OfflinePredictor as target function of an ThreadedMapData). Because of there size of these networks I need to put them in distinct GPUs. (Writing the activations to a file isn't feasible ether)\n\nChristian. Thanks! Works like a charm!. \n",
    "xingyul": "Thanks a lot! It works.. Is there a way to freeze only part of batchnorm layers? My project requires freezing part of the model and fine-tuning the rest of it. Thanks.. I think batch normalization layer is updated through moving averages. So even if gradient is zero, batch normalization parameters will still change. Is there a way to only apply RunUpdateOps() to some of batch normalzation layers?. ",
    "vqdang": "@ppwwyyxx  Thanks\nI also have some more questions to ask.\n\n\nIs there anyway to set the buffer size for PrefetchDataZMQ ? Setting more nr_proc didn't seem to better utilize my CPUs while the GPUS still starved\n\n\nget_current_tower_context().is_training, allows InferenceRunner to perform validation. I also need to run testing after training like validation (inputs at _build_graph only has image, no label). Basically at each epoch it will be training -> validation -> testing. How can I achieve that?\n. I don't understand how the run part can help me achieve that. From what I have seen, the run part is not executed at the same session as training. What I need is preferable like this\n\n\nconfig = TrainConfig(\n            dataflow=train_generator,\n            callbacks=[\n                      InferenceRunner(valid_generator, BinaryClassificationStats('prediction', 'edgemap4d')),\n                      InferenceRunner(test_generator, TestInferencer('prediction', 'edgemap4d'))\n                      ],\n            model=Model(),\n            steps_per_epoch=steps_per_epoch,\n            max_epoch=100)\ntest_generatorwill also provide different shape from from the valid_generator. So far, I have sub-classed Inferencer to create the TestInferencer. But it seems that I could not even provide input with different shapes to the Inferencer. This is the error when valid_generator output shape [102, 102, 3] and test_generator output shape [478, 478, 3]\ntensorflow/core/framework/tensor_shape.cc:240] Check failed: size >= 0 (-3800563712 vs. 0)\n. Thanks, I understand.. Running TestDataSpeed got me at least 90 iter/sec both on server and local machine. I rechecked my operation in the graph and changed cropping using slicing to tf.image.resize_image_with_crop_or_pad,  (I need to crop output at some bottom layers and then concatenate them at some upper layers, like Unet)   \nTesting on single gpu for both server and local machine (same augmentations)\n\nlocal machine (nr_proc = 16):  iter/sec stable around 1.84 iter/sec (it improved alot) (just 16 logical processor)\nthe server: reach higher peak but dropping to low at some following epochs. nr_proc > 16 just made it dropping faster (it has 32 logical processor). I then removed the augmentation but it changed nothing.\n\nI also used resizing in the network, do all tensorflow operations operate on GPU? I honestly don't know how should I go with this. But I highly suspect that something CPU-related are my bottleneck, I just dont know how to pinpoint it.\nI plan to try the following but I am not sure how I can apply to my case.\n```\nds0 = dataset.ILSVRC12('/path/to/ILSVRC12', 'train', shuffle=True)\naugmentor = AugmentorList(lots_of_augmentors)\nds1 = ThreadedMapData(\n    ds0, nr_thread=25,\n    map_func=lambda x: augmentor.augment(x), buffer_size=1000)\nds1 = PrefetchDataZMQ(ds1, nr_proc=1)\nds = BatchData(ds1, 256)\n```\nThis is my get_data()\n```\n\"\"\" data: numpy array for images\n    label: numpy array for images' labels\n\"\"\"\ndef get_data_generator(data, data_shape, label, label_shape, \n                                batch_size = 16, is_train = False):\n    ds = Dataset(data, label)\n    ### augmentation for both the input and label\n    if is_train:\n        shape_aug = [\n            ElasticDeform(size = (4, 8), ampl = (6, 10)),\n            RandomRotation(45),\n            imgaug.Flip(horiz=True),\n            imgaug.Flip(vert=True),\n            imgaug.ToUint8()\n        ]\n    else:\n        shape_aug = [\n            imgaug.CenterCrop(data_shape),\n        ]\n    ds = AugmentImageComponents(ds, shape_aug, (0, 1), copy=True)\n### augmentation for just the output\naugmentors = [\n    # finally crop mask size to output shape\n    LabelProc(False, True), # to fix label value or shape\n    imgaug.CenterCrop(label_shape),\n]\nds = AugmentImageComponent(ds, augmentors, index = 1, copy=True)\n\n### augmentation for just the input\nif is_train:\n    augmentors = [\n        imgaug.CenterCrop(data_shape),\n    ]\n    ds = AugmentImageComponent(ds, augmentors, index = 0, copy=False)\n    ds = BatchDataByShape(ds, batch_size, idx=0)\n    ds = PrefetchDataZMQ(ds, 16)\nelse:\n    ds = BatchData(ds, batch_size)\n\nreturn ds\n\n``\nThanks\n. Do you know anyway too track which tensorflow operation running on GPU/CPU? I'm doing brute force by rerunning the graph to check it for now but this way is a bit too counter-intuitive.. @ppwwyyxx \nIt's abit troublesome because I need both upsampling and concatenation (viatf.concatandtf.image.resize_nearest_neighbor` or conv2d transpose) but the existence of either resize or conv2d transpose still make iter/sec goes down the same way. I will look it through again today.\n@PatWie  Thanks for your nice code! I haven't known such thing exist.\nI will close this because this is now out of tensorpack scope. Thanks everyone.\n. Could you also add a method to just dump out the base graph to Tensorboard? (No training yet, just general structure debugging) I'm using something like below but it requires to comment/uncomment  some part in batch_norm.py every time between debugging/training (mostly the ctx).\n```\ndef _visualize_graph(self, outdir):\n    x = tf.placeholder(tf.uint8, shape=(None, None, None, 3))\n    y = tf.placeholder(tf.uint8, shape=(None, None, None, None))\n    self._build_graph([x, y])\nwith tf.Session() as sess:\n    writer = tf.summary.FileWriter(outdir, sess.graph)\n    print(sess.run(self.graph_tree))\n    writer.close()\n\n```. @ppwwyyxx Sorry for not being clear enough. It's not about all of the ops in the whole graph but the main portion itself. For example:\n```\ndef _build_graph(self, inputs):\n    image, label = inputs\n### Main Network - simple neural network\nwith argscope(Conv2D, out_channel=32, kernel_shape=3, nl=tf.nn.relu):\n    l = Conv2D('conv0', image, 32, 3, nl=tf.nn.relu)\n    l = MaxPooling('pool0', l, 2)\n    l = FullyConnected('fc0', l, 512, nl=tf.nn.relu)\n    l = Dropout('dropout', l, 0.5)\n    l = FullyConnected('fc1', l, 10, nl=tf.identity)\n\n### Main Network - complex network with lots of interconnections between layers\nwith argscope(Conv2D, out_channel=32, kernel_shape=3, nl=tf.nn.relu):\n    l = Conv2D('conv0', image, 32, 3, nl=tf.nn.relu)\n    l1, l2, l3 = layer_a('complex_layer_a', l)\n    l = layer_b('complex_layer_b', [l, l1, l2])\n    l = tf.concat([l, l1, l2, l3], 3)\n    l = FullyConnected('fc1', l, 10, nl=tf.identity)\n\n### Others stuff in graph for whole training process\nloss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\nloss = tf.reduce_mean(loss, name='xentropy-loss')\n\nadd_moving_summary(loss)\nself.cost = loss\n\n```\nAnd I want to debug the l tree, in a simple case, there is no need for visualization before actual training. But in a more complex case, visualization is needed if I want to make sure all the layers (or components) are linked as intended before actually run the trainer. For now, I do this via\n```\ndef _visualize_graph(self, outdir):\n    x = tf.placeholder(tf.uint8, shape=(None, None, None, 3))\n    y = tf.placeholder(tf.uint8, shape=(None, None, None, None))\n    self._build_graph([x, y])\nwith tf.Session() as sess:\n    writer = tf.summary.FileWriter(outdir, sess.graph)\n    print(sess.run(self.graph_tree))\n    writer.close()\n\nWith `self.graph_tree = l` in `_build_graph`. However, for `_visualize_graph` to run just by\nmodel = Model()\nmodel._visual_graph('model_graph')\n``\nI need to modify whenever variablectxwithctx = get_current_tower_context()is used in all the ops inl(in my case, it is theBatchNorm`). Because in case above, there clearly no context of the tower and I'm not sure how to set the context manually. \nIn short, some placeholder for similar situation or an official support to visualize part of the whole graph tree. Beside, to actually get to the graph tree in Tensorboard via running the trainer is also taking much longer time and not suit to debug just the main structure I think.\n. I'm using the default InferenceRunner. Some more details:\ndef get_valid_data_generator(data, data_shape, label, label_shape, batch_size = 16):\n    ds = Dataset(data, label)\n    shape_aug = [\n        imgaug.CenterCrop(data_shape),\n    ]\n    ds = AugmentImageComponents(ds, shape_aug, (0, 1), copy=True)\n    augmentors = [\n        imgaug.CenterCrop(label_shape),\n    ]        \n    ds = AugmentImageComponent(ds, augmentors, index = 1, copy=True)\n    ds = BatchData(ds, batch_size)\n    return ds\nAnd the setup in TrainConfig:\nInferenceRunner(\n      get_valid_data_generator(x_valid, base_shape,\n                                                y_valid, mask_shape,\n                                                batch_size = 32),\n      ClassificationStats('predmap', 'segmapx')). I solved the problem. Using '//' in the path broke the tensorflow loader, I don't know why and it seems silly. Here an example:\nAbove: /media/vqdang/Data_2/dang/output/miccai/v1.3.0_b//model-8.index\nFixed : /media/vqdang/Data_2/dang/output/miccai/v1.3.0_b/model-8.index\nAbout the dataflow, is the queue reset every epoch? I haven't thought too much about that but the dataflow-callback way as in https://github.com/ppwwyyxx/tensorpack/blob/2c129dede46b8eb76e3f8a137d80d6accd4b48ff/examples/DeepQNetwork/expreplay.py#L109\nlooks more elegant and I want to try that. However, I need to make sure that from the next period, every samples used for training must be generated from the new distribution.. @ppwwyyxx I forgot to ask, after finishing SyncMultiGPUTrainer(config).train(), the existing dataflow will stop and existing queue will be removed right? Sorry to bring this up after closing the issue.. @ppwwyyxx  Could you provide an example on how to use this feature? Thanks.. ",
    "haamoon": "What is your status on this project? I spent some time on this while ago. The accuracy I got was ~10% less than the original tf-faster-rcnn implementation I used. I never found some time to review the code and find the problem. I can share the code with you if you are willing to work on that.. I can work on it. I wouldn\u2019t be able to finish until CVPR submission deadline though.. Thanks! I am trying to port faster-rcnn implementation (https://github.com/endernewton/tf-faster-rcnn) to tensorpack. It seems they have fixed BatchNorm layers during training. Any suggestion for freezing their params? . I am reading the documents for freezing several layers in FAQ. It seems that in cases we are also doing l2_regularization using stop_gradient may not be sufficient to freeze some weights. Since they get gradients from l2_regularizer too. Is that right?  . My bad. I think we don't need to pass the parent scope down to other layers. We just need to pass down different names for each layer. Correct me if it is wrong.\n . I'd be happy to do this. Tell me what was the original design you had in mind. . So for crop it should looks like this:\ndef _augment_coords(self, coords, param):\n   h0, w0 = param\n   #coords = np.copy(coords)\n   # change x\n   coords[:, 0] = coords[:, 0] - w0\n   # change y\n   coords[:, 1] = coords[:, 1] - h0    \n   return coords\nI assume we can directly change coords and similar to AugmentImageComponents copying data will be handled in AugmentImageCoordinates. Can we assume coords are always numpy arrays? Any need to do a type check? And I agree with the notes.. How do you want to change MapImage? Should it receive two functions? one for mapping image array and one (optional) for mapping coordinates? In case second function is None should we do identity mapping or throw not implemented exception? \n. Take a look at #335. Let me know what you think.. I used a simple layer to convert the point coordinates to the box coordinates. It also makes sure box coordinates are inside the image:\n```\n@layer_register()\ndef convert_bboxes(bottoms):\n    box_coords, w, h = bottoms\n    shape = tf.shape(box_coords)\n    xs = tf.reshape(box_coords[:, :, 0], [shape[0], -1, 4], name = 'reshape-xs')\n    ys = tf.reshape(box_coords[:, :, 1], [shape[0], -1, 4], name = 'reshape-ys')\n# Change coordinates to what rcnn uses\n# Equivalent to tf.round(xs - 0.5)\nxs = tf.round(xs - .5)\nys = tf.round(ys - .5)\n\nx1 = tf.reduce_min(xs, axis=-1, keep_dims=True, name='minx')\nx2 = tf.reduce_max(xs, axis=-1, keep_dims=True, name='maxx')\ny1 = tf.reduce_min(ys, axis=-1, keep_dims=True, name='miny')\ny2 = tf.reduce_max(ys, axis=-1, keep_dims=True, name='maxy')\n\nx1 = tf.clip_by_value(x1, .0, tf.to_float(w))\nx2 = tf.clip_by_value(x2, .0, tf.to_float(w))\ny1 = tf.clip_by_value(y1, .0, tf.to_float(h))\ny2 = tf.clip_by_value(y2, .0, tf.to_float(h))\nbbox = tf.concat(axis = 2, values=[x1,y1,x2,y2])\nreturn bbox\n\n```\nSince the same function could be used for any type of augmentation (including rotation and more complicated ones), I think having a separate helper function for that would be more convenient. It could be also implemented as a helper augmenter that does the conversion at the end.. Yes, that is what I did.. Just tested AugmentImageCoordinates. Use attached code to test all.\nTest.zip\n . Absolutely. Let me know if you need help on other stuff. I would like to collaborate with you.. Does InferenceRunner call _build_graph with ctx.is_training == False? I though _build_graph is called only once when the training starts and the inference is also run on the training graph.... Thanks! That is exactly what I needed.. I am still not sure how to use the dataflow with 'slim.learning.train' where we don\u2019t have direct access to the session object. Maybe we need to write an operation which reads from ds (using py_func?) and enqueues it and makes a QueueRunner for that?. Does it make sense if we start the thread in a function and pass the function to slim.learning.train as the init_fn argument?\n```\ndef init_fn(sess):\n  with sess.as_default():\n    thread.start()\nslim.learning.train(...., init_fn=init_fn)\n```. I might need this in faster rcnn :). Just deleted corresponding TODOs.. Yes, that makes sense.. ",
    "kdplus": "@PatWie @ppwwyyxx Thx! It works. GPU util seems good after we fixed a disk mounting problem (by moving data from network-mounted to local). Thanks :p. Well, there are still some problems with the ds in multi-GPUs. It do work on the first several hours of training, but it will slow down after a few hours(GPU waiting for the data)\n\nfinally it will look like this\n\nand for the single gpu case it looks like this\n\nAnd today morning I even found it got stuck in the get_data() here\uff0c when executing InferenceRunner. But it works well in the previous epoch.\ndef run(self):\n        with self.default_sess():\n            try:\n                self.dataflow.reset_state()\n                while True:\n                    for dp in self.dataflow.get_data():\n                        feed = dict(zip(self.placehdrs, dp))\n                        # print 'qsize:', self.sess.run([self.op, self.size_op], feed_dict=feed)[1]\n                        self.op.run(feed_dict=feed)\n            except (tf.errors.CancelledError, tf.errors.OutOfRangeError):\n                pass\n            except Exception:\n                logger.exception(\"Exception in EnqueueThread:\")\n            finally:\n                try:\n                    self.close_op.run()\n                except Exception:\n                    pass\n                logger.info(\"EnqueueThread Exited.\")\n. I think it might be a special problem on my machine or data, when I use the batch size = 1, it won't slow down anymore. Thx. @ppwwyyxx Thx, I tried append a savename_prefix=tower0 in get_savename_from_varname. It works for me. \nI'd like to try to add a tf.name_scope('tower0') It should work well.\nAnd another question is that the model is restored but due to the Keras model callback\n```\nclass KerasCallback(Callback):\n    def init(self, isTrain):\n        self._isTrain = isTrain\n        self._learning_phase = KB.learning_phase()\ndef _before_run(self, ctx):\n    return tf.train.SessionRunArgs(\n        fetches=[], feed_dict={self._learning_phase: int(self._isTrain)})\n\nit give me an error when I try to  `pred = OfflinePredictor(pred)`\n[0706 20:08:47 @sessinit.py:102] Restoring checkpoint from /mnt/ficusspain/yxwang/research_lung/08_all_e2e2/506_resume-95/cls_3d/src/train_log/model_kp:./train_keras/model-74478 ...\nTraceback (most recent call last):\n  File \"/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1039, in _do_call\n    return fn(*args)\n  File \"/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1021, in _run_fn\n    status, run_metadata)\n  File \"/home/yxwang/.pyenv/versions/threed/lib/python3.6/contextlib.py\", line 89, in exit\n    next(self.gen)\n  File \"/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\n    pywrap_tensorflow.TF_GetCode(status))\ntensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'batch_normalization_1/keras_learning_phase' with dtype bool\n         [[Node: batch_normalization_1/keras_learning_phase = Placeholderdtype=DT_BOOL, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]]\n         [[Node: output/_101 = _Recvclient_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_239_output\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"src/evaluate.py\", line 470, in \n    main()\n  File \"src/evaluate.py\", line 463, in main\n    scores = evaluate(nodules, arguments[''], arguments[''], n_gpus=n_gpus, load=arguments[''])\n  File \"src/evaluate.py\", line 386, in evaluate\n    result = pred([batch])[0]\n  File \"/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorpack/predict/base.py\", line 55, in call\n    output = self._do_call(dp)\n  File \"/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorpack/predict/base.py\", line 122, in _do_call\n    output = self.sess.run(self.output_tensors, feed_dict=feed)\n  File \"/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 778, in run\n    run_metadata_ptr)\n  File \"/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 982, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1032, in _do_run\n    target_list, options, run_metadata)\n  File \"/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1052, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'batch_normalization_1/keras_learning_phase' with dtype bool\n         [[Node: batch_normalization_1/keras_learning_phase = Placeholderdtype=DT_BOOL, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]]\n         [[Node: output/_101 = _Recvclient_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_239_output\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\nCaused by op 'batch_normalization_1/keras_learning_phase', defined at:\n  File \"src/evaluate.py\", line 470, in \n    main()\n  File \"src/evaluate.py\", line 463, in main\n    scores = evaluate(nodules, arguments[''], arguments[''], n_gpus=n_gpus, load=arguments[''])\n  File \"src/evaluate.py\", line 384, in evaluate\n    pred = OfflinePredictor(pred)\n  File \"/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorpack/predict/base.py\", line 139, in init\n    config.model.build_graph(input_placehdrs)\n  File \"/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorpack/models/model_desc.py\", line 105, in build_graph\n    self._build_graph(model_inputs)\n  File \"/mnt/ficusspain/yxwang/research_lung/08_all_e2e2/000_nodule_workspace_template/cls_3d/src/model_kp.py\", line 238, in _build_graph\n    M = self._build_keras_model()\n  File \"/mnt/ficusspain/yxwang/research_lung/08_all_e2e2/000_nodule_workspace_template/cls_3d/src/model_kp.py\", line 74, in _build_keras_model\n    model.add(normalization.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n  File \"/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/keras/models.py\", line 476, in add\n    output_tensor = layer(self.outputs[0])\n  File \"/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/keras/engine/topology.py\", line 596, in call\n    output = self.call(inputs, **kwargs)\n  File \"/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/keras/layers/normalization.py\", line 190, in call\n    training=training)\n  File \"/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2600, in in_train_phase\n    training = learning_phase()\n  File \"/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 116, in learning_phase\n    name='keras_learning_phase')\n  File \"/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1507, in placeholder\n    name=name)\n  File \"/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1997, in _placeholder\n    name=name)\n  File \"/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1228, in init\n    self._traceback = _extract_stack()\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'batch_normalization_1/keras_learning_phase' with dtype bool\n         [[Node: batch_normalization_1/keras_learning_phase = Placeholderdtype=DT_BOOL, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]]\n         [[Node: output/_101 = _Recvclient_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_239_output\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\nFile \"/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1228, in init\n    self._traceback = _extract_stack()\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'batch_normalization_1/keras_learning_phase' with dtype bool\n         [[Node: batch_normalization_1/keras_learning_phase = Placeholderdtype=DT_BOOL, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]]\n         [[Node: output/_101 = _Recvclient_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_239_output\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\nIs there any good way to handle this placeholder problem, I am not very familiar with tensorflow. \nThx!. https://github.com/fchollet/keras/issues/2310  I found this but it doesn't work for me. `keras.backend.learning_phase()` seems to be an int, hence does not have a `name`. Is there a variable or    tensor things to get a correct name?. I called **K.set_learning_phase(1)** this before keras.backend.learning_phase().name. Now I comment this line, and get the correct name ''keras_learning_phase\"\nBut it seems does not exist in the graph\nTraceback (most recent call last):\n  File \"src/evaluate.py\", line 486, in \n    main()\n  File \"src/evaluate.py\", line 479, in main\n    scores = evaluate(nodules, arguments[''], arguments[''], n_gpus=n_gpus, load=arguments[''])\n  File \"src/evaluate.py\", line 397, in evaluate\n    pred = OfflinePredictor(pred)\n  File \"/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorpack/predict/base.py\", line 141, in init\n    input_tensors = get_tensors_by_names(config.input_names)\n  File \"/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorpack/tfutils/common.py\", line 120, in get_tensors_by_names\n    ret.append(G.get_tensor_by_name(varn))\n  File \"/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2563, in get_tensor_by_name\n    return self.as_graph_element(name, allow_tensor=True, allow_operation=False)\n  File \"/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2414, in as_graph_element\n    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\n  File \"/home/yxwang/.pyenv/versions/threed/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2456, in _as_graph_element_locked\n    \"graph.\" % (repr(name), repr(op_name)))\nKeyError: \"The name 'keras_learning_phase:0' refers to a Tensor which does not exist. The operation, 'keras_learning_phase', does not exist in the graph.\"\n``\n. In my case the learning_phase's name actually isbatch_normalization_1/keras_learning_phasewhich is different withkeras.backend.learning_phase().namehence after I add this name toinput_namesofPredictConfigand useresult = pred([batch, 0])[0]when predicting\nIt works well now!. A strange thing happens to me, theOfflinePredictoralways produce the same inference results for every batch and even same result for every single data in the batch. \nI checked the batch data are different and my model value is correct. Do you have any idea for this kind of error? thx\nBy the way, there still some name scope related problems when I try to use theDataParallelOfflinePredictor`.. thx, I also tried to use the basic tensorflow layer (because I need use conv3d) but it's training accuracy was stuck at 50%  for a binary classification problem...\nbut, after I used the keras to build the model everything works fine.(but followed by lots of other issues . ",
    "sxjzwq": "UPDATE: It works on other model and env, such as WizardOfWor-v0. Thanks! \nDowngrade the gym to 0.8.2, it works now.  . Thanks.\nI find it can be diverge easily on some difficult games (such as WizardOfWor). Did you have similar situation? How did you deal with that? continue training until it converge at some point or restart the training process?\nYou mentioned that you train 2 days to get those models in your Model Zoo, how many epochs of those model?. ",
    "NoobFang": "Specifically, I have trained the resnet50 by your example \"imagenet-resnet.py\", then I want to build a new model upon this pretrained resnet, say, add some layers after the output of some conv layer of resnet50. However, when I import the meta graph which saved by the exmaple \"imagenet-resnet.py\" and I print the imported model by \"decscribe_model\" function but did not find the input placeholder for feeding new data. \nDo I need to create new placeholders for new data? If so, how to connect (which I said redirect) these new placeholder to the pretrained resnet? Or is there some elegant api to do this without build the whole model from scratch(like input placeholders -> every blocks of resnet-> my new layers -> ...)?. OK, I got it. \nThanks for your prompt reply!. ",
    "ricky1203": "stack of crash\n(gdb) bt\n0  0x00007fff73b104ee in google::protobuf::Arena::AllocateAligned(std::type_info const*, unsigned long) ()\nfrom /home/zbhuang/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n1  0x00007fff739c2f13 in tensorflow::TensorProto::New(google::protobuf::Arena*) const ()\nfrom /home/zbhuang/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n2  0x00007fff39c4c038 in google::protobuf::MessageLite::ParseFromArray(void const*, int) () from /usr/lib/x86_64-linux-gnu/libprotobuf.so.9\n3  0x00007fff39f18c61 in ZMQConnection::recv_tensor_list (this=0x5e2fff0, tlist=0x7ffd397f8d30) at zmq_conn.h:57\n4  0x00007fff39f1948e in ZMQRecvOp::Compute (this=0x5e30ab0, ctx=0x7ffd397f98a0) at zmq_recv_op.cc:44\n. ",
    "tonyw": "yes\uff0ctensorflow can support hdfs,but tensorpack     not.by specific\uff0cthere is an assert in the \"callback.saver\" class,in tensorpack souce  code. So I think the  source  keeper shuold remove the assert statement or use gfile interface. I checked it. Its runs smoothly. Thank you respected job. Or Is there any sample for SyncMultiGPUTrainerReplicated?. @ppwwyyxx But it's alway failed,and throw exception like:\nValueError: Variable conv0/weights does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\nyou can test with mnist-slim or anyone other's. after I use newest code,it works too.\nThank you \n. when I run with 1 GPU,the progress bar print the throughput like this \"99/590[00:50<03:23,2.41it/s\". when I use 2 GPU's by changing the 'CUDA_VISIBLE_DEVICES',the progress bar print the same value.\nAt the begin, there are some log like this:\n[0731 19:01:07 @model_utils.py:47] Model Parameters: \nname         shape                   dim  device\n\nconv1_1/W:0  [3, 3, 3, 64]          1728  /device:GPU:0\nconv1_1/b:0  [64]                     64  /device:GPU:1\nconv1_2/W:0  [3, 3, 64, 64]        36864  /device:GPU:1\nconv1_2/b:0  [64]                     64  /device:GPU:0\nconv2_1/W:0  [3, 3, 64, 128]       73728  /device:GPU:0\nconv2_1/b:0  [128]                   128  /device:GPU:1\nconv2_2/W:0  [3, 3, 128, 128]     147456  /device:GPU:1\nconv2_2/b:0  [128]                   128  /device:GPU:0\nconv3_1/W:0  [3, 3, 128, 256]     294912  /device:GPU:0\nconv3_1/b:0  [256]                   256  /device:GPU:1\nconv3_2/W:0  [3, 3, 256, 256]     589824  /device:GPU:1\nconv3_2/b:0  [256]                   256  /device:GPU:0\nconv3_3/W:0  [3, 3, 256, 256]     589824  /device:GPU:0\nconv3_3/b:0  [256]                   256  /device:GPU:1\nconv4_1/W:0  [3, 3, 256, 512]    1179648  /device:GPU:1\nconv4_1/b:0  [512]                   512  /device:GPU:0\nconv4_2/W:0  [3, 3, 512, 512]    2359296  /device:GPU:0\nconv4_2/b:0  [512]                   512  /device:GPU:1\nconv4_3/W:0  [3, 3, 512, 512]    2359296  /device:GPU:1\nconv4_3/b:0  [512]                   512  /device:GPU:0\nconv5_1/W:0  [3, 3, 512, 512]    2359296  /device:GPU:0\nconv5_1/b:0  [512]                   512  /device:GPU:1\nconv5_2/W:0  [3, 3, 512, 512]    2359296  /device:GPU:1\nconv5_2/b:0  [512]                   512  /device:GPU:0\nconv5_3/W:0  [3, 3, 512, 512]    2359296  /device:GPU:0\nconv5_3/b:0  [512]                   512  /device:GPU:1\nfc6/W:0      [25088, 4096]     102760448  /device:GPU:1\nfc6/b:0      [4096]                 4096  /device:GPU:0\nfc7/W:0      [4096, 4096]       16777216  /device:GPU:0\nfc7/b:0      [4096]                 4096  /device:GPU:0\nfc8/W:0      [4096, 2]              8192  /device:GPU:0\nfc8/b:0      [2]                       2  /device:GPU:0\nTotal #vars=32, #param=134268738 (512.19 MB assuming all float32)\nI think there are must be someting wrong\n. log.log.zip\nthere is the raw log.\nI want to see a clearly performance raise by the progress bar.for example: if I use 1 GPU,the progress bar print 1it/s,when I use 2 GPU, the pregress bar print 2it/s. like this.\nIn that case, I did not change the batch size.. Oh, you mean the tensorpack would fetch double data , If I use two tower? \nIn that case , if I set the max_epoch = 1, each tower run half epoch or each tower will run all epoch?. Thank you.\nIs there anyway to evaluate the total throughput around all Gpu cards in real time?. I red the ProgressBar class. I think you're right.The throughput is not a common definition for sync and async training. But In sync Multi-GPU Trainer,the throughput is just mean in a sync iterator,all gpu process data counter.. So I think if tensorpack supply a specific progressbar for sync Multi-GPU Trainer, it's will be a better job.And will avoid make other users confused like me.. thanks for your feedback. Thank you for your comment. I got it. I will try it.. It's runs ok,thank you for your comments.. I think this line should be:\n\nif reg_cost is not None:. But I run it ok when I use QueueInputTrainer.\nthis is my code https://gist.github.com/tonyw/e9d1ec7f0debd58b7b1433207adc1894\nI set the lost like this \nself.cost = tf.reduce_mean(self.cost) + l2_penalty + l1_penalty\nIs there something wrong with me?. Thank you for your suggestion,I will try it.. \n",
    "chunfuchen": "my bad. my usage is correct. just my file on server is not up-to-date... Thanks. I do not notice this detail. . Here are my logs on resnet-18 over the ImageNet dataset. (I tested on P100)\nw/o DataParallelInfereneRunner: ~50 seconds\nw/  DataParallelInfereneRunner (4 gpus): ~26 seconds\nEven if it is not linear speedup, it still helps a lot.\nBtw, to achieve the above results, I need to enable data prefetch for inference as well (in your example codes, the prefetch is disable.)\nexamples/ResNet/imagenet-resnet.py line 79\nif isTrain:\n    ds = PrefetchDataZMQ(ds, min(20, multiprocessing.cpu_count())). One more results, resneXt-50 over the ImageNet dataset. (ResNeXt is implemented by using split in Conv2d.)\nw/o DataParallelInfereneRunner: ~180 seconds\nw/ DataParallelInfereneRunner: ~60 seconds\n. Thanks. I see. Nonetheless, in this case, for training data, after completing one epoch, it also does not guarantee all images are processed once, right? (It might be okay for training.). Thanks for rapid reply. Then, I think the above codes can be used for DataParallelInfereneRunner, right? \nWith this improvement, I can ~2x by using DataParallelInfereneRunner ( 4 gpus are used.)\n. Here is my code gist:\nhttps://gist.github.com/chunfuchen/1df7224432cd53fbd88bdd9ce143f589\nThe modification I did are commented with \"modified\" keyword, only three lines.\nThanks.. I can not find out the issue... I just re-train it and now it looks well... I will close this issue. Thanks.. Thanks. I see. Does it discuss in any paper? Do you mind pointing me?. got it and thanks.. Thanks. I do not encounter the error but I get out of memory issue if I start param server and worker at the same gpu. (launch order: param server --> worker)\nThe param server will occupy all memory as tensorflow did by default. \nSo, in practice, I need to set up the upper bound memory utilzation ratio for parameter server, right?\nOr for distributed learning, the parameter server is allocated at cpu?\n. okay, thanks. \nKeep getting:\n2017-09-26 14:18:27.912036: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session 1dc3a76f13e926d3 with config: intra_op_parallelism_threads: 1 gpu_options { per_process_gpu_memory_fraction: 0.99 allocator_type: \"BFC\" allow_growth: true force_gpu_compatible: true } allow_soft_placement: true\nI think it should be my fault in setting up cluster spec. Let me take a look first.\nThanks for your help.. :( still can not train the model, \nMy clusterSpec is \npython\n    cluster_spec = tf.train.ClusterSpec({\n            'ps': ['localhost:2222', 'localhost:2232'],\n            'worker': ['localhost:2223', 'localhost:2233']\n        })\nI start 4 processes: 2 param servers, 2 workers (start 2 workers first, and then start 2 param sersers.)\nAfter 4 processes are started, I do see 2 param servers show the message:\n[0926 15:04:30 @distributed.py:72] My role in the cluster: job=ps, task=0\n[0926 15:04:30 @distributed.py:196] Running ps 0\n[0926 15:04:30 @distributed.py:197] Kill me with 'kill 29729'\nand\n[0926 15:03:41 @distributed.py:72] My role in the cluster: job=ps, task=1\n[0926 15:03:41 @distributed.py:196] Running ps 1\n[0926 15:03:41 @distributed.py:197] Kill me with 'kill 29473'\nOne of workers displayed already\n0926 15:05:34 @concurrency.py:36] Starting EnqueueThread QueueInput/input_queue ...\n[0926 15:05:35 @base.py:228] Start Epoch 1 ...\n0%|                                                         |0/5000[00:00<?,?it/s]\nhowever, the other worker keep displaying  (I assume that this worker should show the same message to the other worker but it did not, so I guess there is something wrong.)\nStart master session 6c3a06a21ffc4486 with config: intra_op_parallelism_threads: 1 gpu_options { per_process_gpu_memory_fraction: 0.99 allocator_type: \"BFC\" allow_growth: true force_gpu_compatible: true } allow_soft_placement: true\n2017-09-26 15:24:24.040698: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session a27bc07052114c4c with config: intra_op_parallelism_threads: 1 gpu_options { per_process_gpu_memory_fraction: 0.99 allocator_type: \"BFC\" allow_growth: true force_gpu_compatible: true } allow_soft_placement: true\n2017-09-26 15:24:54.368830: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session ede2e745ee062fe4 with config: intra_op_parallelism_threads: 1 gpu_options { per_process_gpu_memory_fraction: 0.99 allocator_type: \"BFC\" allow_growth: true force_gpu_compatible: true } allow_soft_placement: true\n2017-09-26 15:25:24.735236: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session f5989ba1e0196365 with config: intra_op_parallelism_threads: 1 gpu_options { per_process_gpu_memory_fraction: 0.99 allocator_type: \"BFC\" allow_growth: true force_gpu_compatible: true } allow_soft_placement: true\nMay I use single machine to mimic distributed learning? I am testing my script and then I will deploy on real cluster.\nThanks.\n. okay, I need to ask admin about the machine configuration, I will try to find another machine to test it. Thanks.. No luck, by using my laptop as a single server, I still keep getting the above message... (the non-chief worker can not start working.)\nMay I know that by using your example codes, cifar10-resnet.py, I think I only need to replace original SyncMultiGPUTrainerParameterServer to DistributedTrainerReplicated and setup tf.train.ClusterSpec and tf.train.Server, right? Do I need to change Optimizer or model definition?\nHere is codes examples [cifar10-resnet-dist.py]. (I modify the codes to run on CPU, replace AvgPooling with Stride Conv.) (https://gist.github.com/chunfuchen/158434a1d86f44d77666ec685f57e501)\nAny suggestion would be appreciated. . Thanks for your great help, it is really helpful for me. I can train it now.\nBtw, I saw a warning message as below:\nWARNING:tensorflow:From ~/tensorpack/callbacks/param.py:69: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\nInstructions for updating:\nPlease use tf.global_variables instead.\nIs it okay? I think you made this change for distributed trainer yesterday.(584e9cd4cb2812c722b65a79309cc78ee9d5b552). Thanks. Will close this issue after I validate on the cluster.. Everything works well on a cluster, I have tested on the following configurations:\n- 4 nodes, each with one gpus\n- 2 nodes, each with two gpus\nThanks for your help.. My machine has 8 v100 GPU and it is a bare metal machine.\nAll are Compute Mode: Default.. Got it and thanks.. Do you mind sharing the performance speed on 8 P100 when training Faster RCNN?\nWhen using 8 gpus, I can only get ~200-300 seconds per epoch, and utilization of each one is about 50%-60%.\nThe QueueInput/queue_size is 46.96 in the log. Do you think it is related to prefetch? \nThanks.\n. Thanks. It seems that the speed per epoch is varied on my machine even after 30 epoch (9k steps)\nIt can be from 90 seconds to 200 seconds per epoch (with 8 gpus, no other users use the gpus), I guess that because the number of proposals on each image might be varied which affects the speed of each image.\nFurthermore, may I have few questions about training log?\n1. The Warning message of training image:\n  COCO_val2014_000000251330.jpg is invalid for training: No valid foreground/background for RPN!\nI think it is okay since the image did not provide FG/BG for RPN\n\nPerformance metric:\nI observed that I will get nan on certain metrics, is it normal? Here is log\n\n[1117 10:48:45 @monitor.py:363] rpn_losses/label_metrics/precision_th0.1: 0.39033\n[1117 10:48:45 @monitor.py:363] rpn_losses/label_metrics/precision_th0.2: nan\n[1117 10:48:45 @monitor.py:363] rpn_losses/label_metrics/precision_th0.5: nan\n[1117 10:48:45 @monitor.py:363] rpn_losses/label_metrics/recall_th0.1: 0.98191\n[1117 10:48:45 @monitor.py:363] rpn_losses/label_metrics/recall_th0.2: 0.95139\n[1117 10:48:45 @monitor.py:363] rpn_losses/label_metrics/recall_th0.5: 0.67927\nThanks.\n. Thanks for your help. After about 400 epochs, the speed is more stable (~105 seconds per epoch). \nAfter pull the newest changes, got an error about mismtached data type.\nAt model.py, line 88:\nprecision = tf.truediv(pos_prediction_corr, nr_pos_prediction)\nError message:\nTypeError: x and y must have the same dtype, got tf.int64 != tf.int32\nsince you cast the valid_prediction to tf.int32 at line 80:\nvalid_prediction = tf.cast(valid_label_prob > th, tf.int32)\nHowever, the tf.count_nonzero will return the tf.int64 by default, I should set the dtype for tf.count_nonzero to tf.int32, right?\nNote: I am using python3.6.\n. Another error :( \nI just pull the newest changes.\n2017-11-18 10:44:54.781565: E tensorflow/stream_executor/cuda/cuda_driver.cc:406] failed call to cuInit: CUDA_ERROR_NO_DEVICE\n2017-11-18 10:44:54.781715: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: xxxx\n2017-11-18 10:44:54.781731: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: xxx\n2017-11-18 10:44:54.781960: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 384.81.0\n2017-11-18 10:44:54.782014: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:369] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  384.81  Sat Sep  2 02:43:11 PDT 2017\nGCC version:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.5)\n\"\"\"\n2017-11-18 10:44:54.782060: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 384.81.0\n2017-11-18 10:44:54.782072: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 384.81.0\n^. hmm, weird, after I comment out the line 78 in utils/box_ops.py\nos.environ['CUDA_VISIBLE_DEVICES'] = ''  # we don't want the dataflow process to touch CUDA\nIt works again. maybe I system need to keep this variable existed even when we would set Ops in CPU.\n. Hi, this might be related to the general tensorflow question, if you think stackoverflow is a better place to ask, please just ignore it.\nNow, the input image size is [None, None, 3] for dynamic image size in faster rcnn; however, my basemodel needs to do upsampling to dynamically change the image resolution on-the-fly (like encoder-decoder architecture); however, with None type in image size, I can not build the graph since the upsampling needs deterministic shape to perform upsample (I have tried BilinearUpsampling and FixedUnpooling in tensorpack (specify the ratio to 2), and 'tf.image.resize' in tensorflow (it requires final size of an image).)\nDo you have any suggestion? Thanks.. Thanks for your reply.  Yes, tf.image.resize_images (or more specific tf.image.resize_bilinear) support target size as a \"tensor\", but it is a 1-D tensor with [new_height, new_width]; however, when the size of the input of graph is [None,None,3], I do not find out a way to derive the [new_height, new_width] for tf.image.resize_images during building the graph.\nE.g. in my graph, there are two tensors, A and B, and the size of A is larger than B; during the running time, I would like to resize B to the size of A (the size of A is varied); however, I can not infer the size of A since I create a placeholder [None,None,3] for A for dynamic input size. Hence, during building the graph, when I try to use get_shape().as_list() to get A's shape, I would only get [None, None, 3] and then I can not derive the size of B for tf.image._resize_images function.\nThanks.. Sorry, my bad, never mind. Thanks for reminding to use \"tensor\"... I used to use \"list\" to specify the targeted tensor size.. . Noted and thanks.. A bug of resuming of faster RCNN:\nAfter I resume a trained model, the learning rate of the first epoch will be 0.003.\n[1119 22:52:36 @param.py:144] After epoch 0, learning_rate will change to 0.00300000\n[1119 22:52:36 @monitor.py:262] Found training history from JSON, now starting from epoch number 117.\n[1119 22:52:36 @base.py:209] Start Epoch 117 ...\n[1119 22:58:19 @argtools.py:142] WRN Input /home/chenrich/dataset/COCO14/train2014/COCO_train2014_000000273046.jpg is filtered for training: No valid foreground/background for RPN!\n[1119 23:06:41 @base.py:219] Epoch 117 (global_step 34800) finished, time:844.78 sec.\n[1119 23:06:41 @graph.py:70] Running Op sync_variables_from_main_tower ...\n[1119 23:06:41 @monitor.py:363] learning_rate: 0.003\nOn the other hand, I can understand the speed per epoch might be varied for each epoch since the number of positive proposals might be increased after more epochs; however, when I resume model, the speed will become as slow as training from scratch and then it gradually increases its speed.   However, the number of positive proposals after resume should be identical or similar to previous one, why the speed is slow?  (As you can see that above log shows finishing time are 844 sec, I can get about 200 seconds on average before resuming.)\n. Okay, got it. Thanks.. Thanks for sharing your experience. :). Just want to share the trained results, basemodel is ResNet-50:\nEvaluation on minival set, FASTRCNN_BATCH=256, ~33h on 8 v100.\nAverage speed: after epoch 300, it costs ~115 seconds per epcoh.\nAverage Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.344\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.555\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.365\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.158\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.391\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.498\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.301\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.462\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.480\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.250\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.534\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.66\n. Please ignore it now... something wrong from my side..... okay, my bad. but the start epoch is read from stat.json rather than deriving from global_step and step_num\n. ",
    "VladMiryaha": "Problems disappeared after updating to Tensorflow ver. 1.0. But am I right that:\nsteps_per_epoch = dataset_train.size() (default value)\nand increase number of epoches in 40 times?\nis the same as code in current HED implementation?\nThank you.\n. I see. Thank you so much.. ",
    "maciejjaskowski": "Ok, I just tested adding GraphProfiler to imagenet-resnet.py from examples and figured out that\npython imagenet-res.py --fake --gpu=0\nCrashes on tensorflow-1.1.0 but seems to run ok on tensorflow-1.2.1-rc. Ok, let me then ask these questions:\nWhat does an iteration mean in the case of multi-gpu training? Is it\nbatch_size*nr_tower or batch_size ?\nAre there any requirements when it comes to shuffling data in dataflow when\nusing multigpu trainer?\nBest,\nMaciej\nOn 2 August 2017 at 08:29, Yuxin Wu notifications@github.com wrote:\n\nClosed #359 https://github.com/ppwwyyxx/tensorpack/issues/359.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/ppwwyyxx/tensorpack/issues/359#event-1188938584, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAiVLZrFZc2BthsdzilWcgUKIgVi6P-qks5sUBdRgaJpZM4OqpwW\n.\n. You can't create a directory in GCS. \n\nIn GCS, just like in S3 there is no real tree structure, it's all flat. A file gs://mybucket/a/b/c/d/myfile is just a file and gs://mybucket/a/b is NOT a directory. You can do with this address many things you'd do with a directory, e.g. you can \"list a directory\". However, you can't create a directory unless you place a file in it The closest analogy that should be familiar to github user is... git :) You can't git add an (empty) directory, can you?\nOn the other hand, if you have an empty bucket gs://mybucket, unlike on typical filesystems, nothing stops you from writing a file into gs://mybucket/a/b/c/d/myfile. It's 100% legal.. How about recursive_create_dir? Seems to me like a more suitable :). MaxSaver and MinSaver are for now useless on GCS as well, as shutil.copy and glob.glob are used instead of respectively tf.gfile.Copy and tf.gfile.Glob. shutil and glob will fail on paths starting with \"gs://\". Awesome, thanks for tackling it so quickly! What is the best way to install tensorpack from a commit like that?. A non-appendable filesystem is what GCS, S3 etc. provide as object based filesystems scale much better without sacrificing resiliency (more).\nAfter applying this patch, our average cost of uploading data to GCS dropped over 5 times.\nAs a demagogical argument: I heard from friends at Google, that's what people at Google do :-)\nIndeed, I can't find specific example of such use in Supervisor. \nStill, this PR provides new feature.\n. I am not sure if setting logdir to sth on gcs works right for tensorpack now although this could be a viable option. It does work with pure tensorflow.\nAssuming it works (which is likely), if you set logdir to gs:// each time a file is flushed the whole file is transferred to gcs. \nYou can test it for yourself if you create a bucket YOUR_VERSIONED_BUCKET,\nenable object versioning on GCS: gsutil versioning set on YOUR_VERSIONED_BUCKET\nand use the code below:\n```\nimport tensorflow as tf\nimport time\nfw = tf.summary.FileWriter(YOUR_VERSIONED_BUCKED)\nfor i in range(5):\n  value = i\n  s = tf.Summary(value=[tf.Summary.Value(tag=\"summary_tag\", simple_value=value),\n])\n  fw.add_summary(s)\n  fw.flush()\n  print(\"Sleeping\")\n  time.sleep(2)```\nand then when you verify: gsutil ls -a YOUR_VERSIONED_BUCKET you get sth like:\nYOUR_VERSIONED_BUCKET/test-event/events.out.tfevents.1523609555.Maciejs-MBP-2.waw.nomagic.io#1523609556432534\nYOUR_VERSIONED_BUCKET/test-event/events.out.tfevents.1523609555.Maciejs-MBP-2.waw.nomagic.io#1523609557845069\n.... Yes, each flush creates a new copy. However, it does not matter whether versioning is on or not.\nI turned on versioning only to highlight the problem. . It does not take space but it does take money for transfer :)\nEffectively, if the size (N) of a file grows linearly, the total transfer cost grows with O(N^2) in the current version of tensorpack. \nThe option above lowers the cost back to O(N).\nFlushing the file every now and than if no changes actually occur to the content of the file should not be a problem. I can check it if you wish. \nI was thinking, maybe I should rename the argument to sth like: rotate_logs ?. Probably not a big deal if your model's performance does not variate too much epoch to epoch :-). ",
    "lezhang-thu": "Thanks for the instant reply.\nWhat I mean is what is the gradient of tf.nn.l2_loss?\nThe answer seems to be just the advantage * \\nabla self.value.\nSo  one really needs the stop_gradient of advantage to make the gradient be correct.\nI'm not sure of its correctness. So I'm just asking you for the advice.. Thanks! Now I am sure of it.. EVALUATE_PROC in train-atari.py is of no use. Maybe it's a good idea to remove it.. Thanks.\nSo what is done here is that these variables might be trained, but within that context, these variables would finally be restored to its original value. So anyway, implicitly, they are not trained, right?\nI still have a doubt. In the code, there are the following:\nbest_v = tf.reduce_max(targetQ_predict_value, 1)    # N,\nor\nbest_v = tf.reduce_sum(targetQ_predict_value * predict_onehot, 1), depending on whether the DQN variant is double or not. But I feel the point locates at the following code:\ntarget = reward + (1.0 - tf.cast(isOver, tf.float32)) * self.gamma * tf.stop_gradient(best_v)\nWe can notice the tf.stop_gradient operation. So is it still necessary to have the collection.freeze_collection when creating the target network? I mean the variables within the target network are all stop_gradient to some extent.\nSo my new question is whether collection.freeze_collection is still necessary.\nI am not questioning the correctness of the code. I know it is correct. I just want to make sure I understand the underlying logic. Thanks a lot!\n. Good! Thanks a lot. Your words are very clear.\nThanks!. You are so helpful. Thanks a lot!. I found a new API of Python 3 threading.Barrier, with the parameter parties of 2.\nSo now my question is straightforward, if in _trigger_step(self) and run(), the threading.Barrier is called,  would these two functions wait for each other? Thanks. . So you mean they are in different threads?\nHence your answer to the threading.Barrier question is yes?\nI think if run() (by threading.Thread) and _trigger_step() (by callbacks) are called in different threads, the answer should be yes. So these two threads can be synchronized.\nThanks!\n. ",
    "cyrsis": "ZMQ is good for  message, Do you want to do reactive version of tensorpack?. Thanks man\ngit clone again , everything seems working but I dont even have GPU in my Mac need a new machine for training, from your benchmark, you done really well. Yes, ZMQ can works well with windows and mac.\nbut the latency between machine is the biggest issues as far as I can tell\n. ",
    "Huddolly": "Actually, I think the number is correct here. Since you are resuming the interrupted training process, not retraining a pretrained model, the epoch number should follow previous process. As in your case, the model is stopped at No. 475000 iteration (475000/5000 = 95 epoch)\uff0c the resumed epoch number should be 96. @chunfuchen . ",
    "vfdev-5": "Guys, congrats with 2000 stars ! . I didn't tried pickle mostly because, actually, yes it is better (at least for me) to have human-readable presentation too. My idea behind is to be able to log into a file the whole pipeline used for training/validation and repeat a certain experiment after.\nSo, yes, I reimplement __str__ of Augmentor and print parameters without rng and any callable. \nI do not quite understand how is done module importing in dataflow.imgaug and that's why serialize method code I couldn't put near deserialize in __init__.py.\nAnd do you plan to cover imgaug with tests ? \n. Yes, I do this here. However, it is not enough. Okay, I see. Thank you !. I rerun my code with the latest tensorpack and the bug disappeared \ud83d\udc4d\nHowever, at the end there is an IndexError that is thrown after the messge Train is finished!. Here is the traceback:\n[0830 21:45:55 @base.py:242] Training has finished!\nTraceback (most recent call last):\n  File \"cifar-convnet-cross-validation.py\", line 190, in <module>\n    trainer.train()\n  File \"/usr/lib/python3.5/contextlib.py\", line 66, in __exit__\n    next(self.gen)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3815, in get_controller\n    if self.stack[-1] is not default:\nIndexError: list index out of range\n[0830 21:45:55 @prefetch.py:174] [Prefetch Master] Context terminated.\nPrefetch process exited.\n[0830 21:45:55 @input_source.py:203] EnqueueThread QueueInput/input_queue Exited.\nPrefetch process exited.\nPrefetch process exited.\nPrefetch process exited.\nPrefetch process exited.\n@ppwwyyxx  could you hint how to fix this? Thanks\n. Yes, there are nice features as iaa.Sometimes, iaa.SomeOf etc (similar to RandomApplyAug and RandomChooseAug), keypoints transformations, iaa.ElasticTransformation and iaa.PiecewiseAffine, and other color augmentations, dropouts etc. Image augmentation randomness is implemented in a developed way, various sampling provided.\nHowever, it mostly uses skimage vs opencv (in tensorpack) and bounded by uint8 and range 0-255.  In my experience, transformations like iaa.PiecewiseAffine can slow down dataflow.\n@ppwwyyxx I do not know what kind of wrapping you would like to provide. Probably, the simplest way to use aleju/imgaug within tensorpack is with MapData. And it would be up to user to handle type of transformations, sampling, determinism/repeatability of aleju/imgaug transformations. \nHTH\n. Okay, I can propose a PR . Yes, I see what you mean. So every epoch I will get the same sequence of images. Sure that this is not good.. @ppwwyyxx actually my initial problem before facing FixedSizeData is related to PrefetchData and what you mention in documentation : \n\ne.g. you are likely to see duplicated datapoints at the beginning\n\nIt is very strange to see several identical batchs. Do you think we can do something with that or we can keep it like this ? \n. Hi @ppwwyyxx \nSorry to revive this closed issue, I would like to comeback to PrefetchData function and duplicated datapoints. As you said, duplicates are explained by forking the dataflow, i.e. dataflow iterator.\nComparing to other frameworks, for example, PyTorch, their Dataset implementation requires similar methods: __len__ and __getitem__. Another class DataLoader provides similarly a prefetch with multiprocessing. Their implementation does not have duplicates in the beginning.\nI wonder why you chose to provide iterator with get_data and not a similar to pytorch API ?\nThanks\n. @ppwwyyxx thank you !\nActually, following your comment and the documentation here :\n```\nds = LMDBData(db, shuffle=False)\nds = LocallyShuffleData(ds, 50000)\nds = PrefetchData(ds, 5000, 1) # !!! ADDING THIS !!!\nds = LMDBDataPoint(ds)\nds = MapDataComponent(ds, lambda x: cv2.imdecode(x, cv2.IMREAD_COLOR), 0)\nds = AugmentImageComponent(ds, lots_of_augmentors)\nds = PrefetchDataZMQ(ds, 25)\nds = BatchData(ds, 256)\n```\nit helps also to avoid duplicates in my case too without using indices ! \nIn my case it looks like :\ntrain_ds = Cifar10('train')\npref_train_ds = PrefetchData(train_ds, nr_prefetch=5*batch_size, nr_proc=1)\naug_train_ds = AugmentImageComponent(pref_train_ds, geom_augmenters, index=0, copy=False)\naug_train_ds = AugmentImageComponent(aug_train_ds, color_augmenters, index=0, copy=False)  \ntrain_batches = BatchData(aug_train_ds, batch_size=batch_size, use_list=True, remainder=False)\ntrain_batches = PrefetchDataZMQ(train_batches, nr_proc=15)\nWhy in your examples you have (main process makes batches) :\n... -> PrefetchDataZMQ -> BatchData\nand not \n... -> BatchData -> PrefetchDataZMQ\nis there a particular reason ? \n. Should it be like this (import pyarrow before sys.modules['torch'] = None) ?\n```\ntry:\n    # https://github.com/apache/arrow/pull/1223#issuecomment-359895666\n    import sys\nimport pyarrow as pa\n\nold_mod = sys.modules.get('torch', None)\nsys.modules['torch'] = None\n# import pyarrow as pa\nif old_mod is not None:\n    sys.modules['torch'] = old_mod\nelse:\n    del sys.modules['torch']\n\nexcept ImportError:\npa = None\nIn case of no pyarrow package `sys.modules['torch']` is `None` . Yes, but if no pyarrow package installed ?. Actually, I have already updated to the master. Try a smaller size, for example (10, 10, 3) in FakeData. @ppwwyyxx could you reproduce the issue ?. Could you try please this code too:python\nfrom tensorpack.dataflow import MultiProcessMapDataZMQ, RNGDataFlow, MultiThreadMapData\ndataset = [(\"file_%i\" % i, i % 3) for i in range(500)]\nclass TestDataset(RNGDataFlow):\ndef __init__(self, dataset):\n    super(TestDataset, self).__init__()\n    self.dataset = dataset\n\ndef size(self):\n    return len(self.dataset)\n\ndef get_data(self):\n    for k in range(len(self.dataset)):\n        yield self.dataset[k]\n\ntest_dataset = TestDataset(dataset)\ntest_dataset.size()\ndef data_transform(dp):\n    return dp\nmp_test_dataset = MultiProcessMapDataZMQ(test_dataset, nr_proc=10, map_func=data_transform, strict=True)\nmp_test_dataset.reset_state()\nfor i, (fp, _) in enumerate(mp_test_dataset.get_data()):\n    if i % 100 == 0:\n        print(i, end=\" . \", flush=True)\n```\nOkay, I'll try to upgrade msgpack, which version do you use ?\nVersion of msgpack I use is msgpack (0.5.6)  . Strange configuration I have. Here is a list of versions of some of packages I have:\nmsgpack (0.5.6)\nmsgpack-numpy (0.4.3)\nmsgpack-python (0.5.5)\nnumpy (1.14.1)\npickleshare (0.7.4)\npip (9.0.1)\npyzmq (17.0.0)\ntensorpack (0.8.1)\non Ubuntu 16.04. I could reproduce the issue using this docker file. Could you try it please?\nDockerfile.txt\ntest_tensorpack_inf_loop.txt\nPlease remove .txt extensions from Dockerfile and replace it for test_tensorpack_inf_loop.txt by py. Github does not allow include py and dockerfile as is.\n. Awesome ! \ud83c\udf89 . The lines 227 and 228:\ntransform_matrix[0, 1] = m01 * cos_shear + m00 * sin_shear\ntransform_matrix[1, 1] = m11 * cos_shear + m10 * sin_shear\ninserts shear into rotational part : \n[[ scale * cos(a),    scale*sin(a+shear)]\n[-scale*sin(a),       scale*cos(a+shear)]]\nand yes, lines 229 and 230 define tx and ty to keep the center unchanged with shear\n. ",
    "huangzehao": "@ppwwyyxx Hi, yuxin. Would you explain why DataFlow with prefetchDataZMQ will forks the ImageNet reader? Can we avoid this problem while keep it fast? Thanks!. ",
    "tarvaina": "Thanks.\nThe EC2 distributed benchmark at the bottom of https://www.tensorflow.org/performance/benchmarks indicates good scalability. Am I reading it wrong or is that an unrealistic setting? I would like to understand the current situation better before choosing a solution.. ",
    "yg320": "Thanks. great,\nthanks for the quick response!. Hi!\nI was wondering about your comment:\n\"I personally think tf.Estimator is not well designed and therefore not flexible enough. But I also think it does have something that tensorpack trainers can learn from.\"\nCan you please elaborate about the flexibility issues on tf.Estimator that you observed? \nAnd also, what do you think that Tensorpack trainers can learn from tf.Estimators?\nI'm using Tensorpack for a while now, and was considering giving tf.Estimators a try (especially because of the community)\nThanks!\n. ",
    "chrisyeh96": "Thanks for the clarification!. Thanks for responding so quickly! Does this mean that the Tensorpack ResNet models expect inputs whose bands across the entire dataset have mean 0 and standard deviation 1?. ",
    "cctgem": "Sorry, I didn't explain it clearly. \nMy question is how to calculate the derivatives (\u2202ro/\u2202ri) in equation 10. In other words, how to calculate the derivatives (\u2202Wb/\u2202W), Wb is the weight after quantization and W is the origin float-point weight.\nMy result is tanh(W)'==(1-tanh(W) * tanh(W)), but i see the other result in other realization.. ",
    "ouceduxzk": "I have the same problem, currently investigating it, want to make it work since tensorpack is such a great library and currently i have to use windows. ",
    "BRougeHub": "@vqdang Were you able to find a way around this problem?. ",
    "skoppula": "The sort of lame way of doing validation tracking would be to scrape the log files which output the validation statistics at the end of every epoch -- is there a Tensorpack script to do this already?. Ah, yup, of course. Thanks for the quick response. Hi there! I've created a self-contained working example of the issue:\nhttps://gist.github.com/skoppula/93320039610b0d2d4332bb18ce70ff19\nThis example is almost entirely Tensorpack's regular CIFAR10 ResNet example, with about 30 extra lines of code adding a new layer (that includes moving averages). Adding this layer to the graph produces the error (line 118: l = RescaleActivationLayer('rescale', l)). Most of this new layer code is based off the Tensorpack's BatchNorm implementation.\nAny ideas?\nThanks.. Thanks! Fixed the problem.. Great, thanks. Should be workable.. ",
    "tworuler": "how to use tf dataset in InferenceRunner and Predictor? \nTFDatasetInput didn't implement size(), it cannot be used in InferenceRunner.\nPredictors like SimpleDatasetPredictor only accept DataFlow, is there any way to use tf dataset?. The returned value of test_perf(tfrecord_dataset) is smaller than test_perf(image_dataset), but using tfrecord_dataset, IO is the bottleneck, image_dataset not. How is TFDatasetInput._iterator used in training progress?. it's caused by prefetch and should be nothing about tensorpack. I changed the code as below and the issue was gone.\ntfrecord_dataset = (\n    tf.data.Dataset.from_tensor_slices(tfrecord_list)\n      .shuffle(buffer_size=1024)\n      .flat_map(tf.data.TFRecordDataset)\n      .map(record_parser, num_parallel_calls=10)\n      .shuffle(buffer_size=5000)\n      .repeat()\n      .batch(batch_size)\n      .prefetch(5)\n). ",
    "TomorrowIsAnOtherDay": "@ppwwyyxx \n\nIt doesn't work,  error information like this:\n\n. Thanks for your reply.I am trying to do something for current model. Could you please give me some advice?\nIs there a faster way to communicate among different GPUs? I supposed the current communication like this:GPU0->cpu->GPU1(e.g. cifar10_multi_gpu_train.py)\nIs there a TF API for me to communicate among different GPUs directly? . Thanks for your advice, I will have a try ~. It worked as expected after pulling the newest code  \uff1a\uff09. additional infomation:\n\n. https://github.com/ppwwyyxx/tensorpack/blob/a3cc3a18c492ac16e6d678e1f26c076c3ae5f70f/tensorpack/train/tower.py#L185\nAfter setting colocate_gradients_with_ops = False, It ran faster as it was.\nIt's this configuration which make embedding layer poor performance.\nAny advice on this problem?\nI think whether to open this configuration should depend on whether  an embedding layer was inside graph or not?. It worked as expected, now I can used SingleCostTrainer. COLOCATE_GRADIENTS_WITH_OPS=False to set it:). Thanks for quick fixing!. Thanks for your rapid reply, I will have a try\uff1a\uff09. It worked as expected.\nShould it be a argument when constructing a new callback ?\ndata_shuffle_cb = CallbackFactory(trigger=lambda self: [data_reader.shuffle_train_dataset()])\ndata_shuffle_cb.set_chief_only(False)\n\nreturn TrainConfig(\n        data=TFDatasetInput(training_dataset),\n        model=Policy(),\n        callbacks=[\n            data_shuffle_cb,\n            InferenceRunner(TFDatasetInput(test_dataset), ScalarStats('cost')),\n            ],  \n        steps_per_epoch=train_num // args.batch * 4,\n        max_epoch=80). Ok~ \ud83d\udcaf\n\nI will make it work in this way\uff1a\uff09 . Sorry for not describing problem clearly.\nI  defined a tensor with name pred_value\ndef _build_value_loss(self, state, val):\n    with tf.variable_scope('val_fun'):\n      out = self._basic_model(state)\n      out = FullyConnected('val', out, 1)\n      pred_val = tf.identity(out, name='pred_value')\n      loss = tf.nn.l2_loss(pred_val - val)\n    return loss\nand KerError was raised when I pass pred_value as a argument of self.trainer.get_predictor(*self.predictor_io_names)\nFile \"/home/robot/Firework/tensorgo/trpo/simulator.py\", line 32, in _setup_graph\n    self.value_pred = self.trainer.get_predictor(*self.value_predictor_io_names)\n  File \"/home/disk1/Firework/py35/lib/python3.5/site-packages/tensorpack/train/tower.py\", line 106, in get_predictor\n    output_tensors = tower.get_tensors(output_names)\n  File \"/home/disk1/Firework/py35/lib/python3.5/site-packages/tensorpack/tfutils/tower.py\", line 320, in get_tensors\n    return [self.get_tensor(name) for name in names]\n  File \"/home/disk1/Firework/py35/lib/python3.5/site-packages/tensorpack/tfutils/tower.py\", line 320, in <listcomp>\n    return [self.get_tensor(name) for name in names]\n  File \"/home/disk1/Firework/py35/lib/python3.5/site-packages/tensorpack/tfutils/tower.py\", line 306, in get_tensor\n    ret = get_op_or_tensor_by_name(name_with_ns)\n  File \"/home/disk1/Firework/py35/lib/python3.5/site-packages/tensorpack/tfutils/common.py\", line 131, in get_op_or_tensor_by_name\n    return f(name)\n  File \"/home/disk1/Firework/py35/lib/python3.5/site-packages/tensorpack/tfutils/common.py\", line 126, in f\n    return G.get_tensor_by_name(n)\n  File \"/home/disk1/Firework/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3654, in get_tensor_by_name\n    return self.as_graph_element(name, allow_tensor=True, allow_operation=False)\n  File \"/home/disk1/Firework/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3478, in as_graph_element\n    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\n  File \"/home/disk1/Firework/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3520, in _as_graph_element_locked\n    \"graph.\" % (repr(name), repr(op_name)))\nKeyError: \"The name 'tower-pred-0/pred_value:0' refers to a Tensor which does not exist. The operation, 'tower-pred-0/pred_value', does not exist in the graph.\"\nand if I removed the variable_scope, it worked correctly.\ndef _build_value_loss(self, state, val):\n    out = self._basic_model(state)\n    out = FullyConnected('val', out, 1)\n    pred_val = tf.identity(out, name='pred_value')\n    loss = tf.nn.l2_loss(pred_val - val)\n    return loss\nTF_version: v1.7.0-3-g024aecf414 1.7.0\nTensorpack_version: master branch. It work:)\nThanks for your reply.. ",
    "tranorrepository": "Any progress for this question?. I got OOM error too. More information is below: \nfname, boxes, klass, is_crowd = img['file_name'], img['boxes'], img['class'], img['is_crowd']\nKeyError: 'file_name'\nP.S, python 3.4, tf 1.4 . @ppwwyyxx \nCOCO annotations are download from http://cocodataset.org/#home\nprint the img,\n{..., b'license': 4, b'file_name': b'../data/COCO/train2014/COCO_train2014_000000158420.jpg'} \nwhen I modified the img['file_name'] to img[b'file_name'], the error disappear.  . My msgpack is 0.4.8, it seems to be the latest.. ",
    "jongchoi": "The average queue size during the first training epoch appears to be very low indeed: 2.32e-37. Also, because I was able to draw ~7.8 it/s with the --fake option on, so tensorflow seems good.\nIn the experiment, there seems to be some difference in performance (it/s) between train and predict phases: 0.19 it/s during training vs. 1.68 it/s during prediction.\nI first thought that this could be explained by the difference in the amount of computation needed for the two phases, but I'm a little confused now because the input queue seems to be full (average queue size is 50?) during prediction.\n\u001b\n[0905 03:44:38 @monitor.py:355] DataParallelInferenceRunner/QueueInput/queue_size: 50\n[0905 03:44:38 @monitor.py:355] QueueInput/queue_size: 2.3199e-37\nI'll look into further what may cause slow data in my system. Do you have any suggestion on how to best profile the system with the imagenet example?. ",
    "AkhilSinghRana": "Hi could you resolve visualization on test set after every 3 epochs.  \nThanks,. Hi thanks I will try again. \nCheers. ",
    "ipalit1981": "Thanks. I created a directory and set the $TENSORPACK_PIPEDIR to this location, however it made no difference in output. So, does it mean that Tensorpack can't be run on AFS?. Thanks for the suggestion. When I use PrefetchData instead, the code is running fine on AFS. For the nr_prefetch, nr_proc arguments I am using 128, 4 respectively. Is there any recommended values for these parameters?\nHowever, PrefetchDataZMQ is still generating error on AFS after setting $TENSORPACK_PIPEDIR\nI am using the following command to set it on shell:\nexport TENSORPACK_PIPEDIR=$TENSORPACK_PIPEDIR:THE_DIR_PATH\nbash-4.1$ echo $TENSORPACK_PIPEDIR\n/afs/nd.edu/user19/ipalit/Segmentation/dorefa_net/pipedir/\nI use the following command for running the code:\nbash-4.1$ python3 svhn-digit-dorefa.py --dorefa 1,2,4 --gpu 0\nThe log is attached. Thanks again.\nlog.txt\n. Setting TENSORPACK_PIPEDIR to /tmp resolves the issue. Now, I can run the original code with PrefetchDataZMQ on AFS. Thanks.. Thanks. The saved models reflect the actual weights, not the quantized one. As such, when using such pretrained models for inference, one should pass the weights through the quantization function and then use it. Please correct me if I am wrong.. Thanks. I am closing this issue.. Hi, coming back with another question that just crossed my mind. The reported accuracy numbers (from the code) after each epoch -- generated via inference runner -- are based-on the quantized weights. Right? . Thanks.. ",
    "Openning07": "Thank you for such a quick response!\nI would check even further to try to solve it...\nThanks anyway!. ",
    "stecklin": "Thanks for the quick response. I was in fact missing to adjust the labels, results are much better now.\nYet still the performance is notably (-15%) worse than it should be. Are there more common pitfalls you know of? I paid attention to the value ranges and BGR/RGB space as previously mentioned by you.. ",
    "ildoonet": "@ppwwyyxx Thanks, I already did the same way you do for this problem, It works properly.\nI will pull your code to use it. Thanks! . I only use tensorpack's dataflow.\nI don't use tensorpack's wrapper for tensorflow.\nIf I create a model by pure tensorflow, can dataflow like below be converted to tensorflow queue easily?\n```\ndef get_mnist_data(is_train, image_size, batchsize):\n    ds = MNISTCh('train' if is_train else 'test', shuffle=True)\nif is_train:\n    augs = [\n        imgaug.RandomApplyAug(imgaug.RandomResize((0.8, 1.2), (0.8, 1.2)), 0.3),\n        imgaug.RandomApplyAug(imgaug.RotationAndCropValid(15), 0.5),\n        imgaug.RandomApplyAug(imgaug.SaltPepperNoise(white_prob=0.01, black_prob=0.01), 0.25),\n        imgaug.Resize((224, 224), cv2.INTER_AREA)\n    ]\n    ds = AugmentImageComponent(ds, augs)\n    ds = PrefetchData(ds, 128*10, multiprocessing.cpu_count())\n    ds = BatchData(ds, batchsize)\n    ds = PrefetchData(ds, 256, 4)\nelse:\n    # no augmentation, only resizing\n    augs = [\n        imgaug.Resize((image_size, image_size), cv2.INTER_CUBIC),\n    ]\n    ds = AugmentImageComponent(ds, augs)\n    ds = BatchData(ds, batchsize)\n    ds = PrefetchData(ds, 20, 2)\nreturn ds\n\n```\nI have created a simple class to convert, and used it for me.... Very nice. Thanks.\n. ",
    "xiaosongwang": "OK, thanks. ",
    "swiatkowski": "Thanks for your quick replay. Yes, you are right that this warning is printed to the console. However, it is not stored in the log file (e.g. log.log). I used the log file for debugging the problem with no tfevents file being generated and couldn't find the warnings there. I believe that having the warnings in the log would make debugging easier.\nThe first line of the log is stored only after the log file handler is set in [2]. The TFEventWriter warnings are generated before this file handler is set. Additionally, the InferenceRunner log in [3] is similarly printed only to the console and not to the log file.\nBest,\nJakub\n[2] https://github.com/ppwwyyxx/tensorpack/blob/a36ad1807632060824a0cb156e644315216085ad/tensorpack/utils/logger.py#L74\n[3] https://github.com/ppwwyyxx/tensorpack/blob/d5f3350de60974424feb08057af84fbb82dff21c/tensorpack/callbacks/inference_runner.py#L81. Ok, I might have misunderstood the concept of the log file that is automatically generated during training. I thought it would contain any logs generated by the TensorPack code. However, if understand correctly now, this log is meant to only store the logs generated during the training process (and not during setting up the TrainConfig etc). This makes sense to me and as far as I am concern this issue be closed. Thanks for addressing this concern and keep up the good work :). ",
    "Szy-Young": "Thanks for your reply. I think I really need further looking into the framework of tensorflow and make better use of it to meet my requests.. ",
    "adeagle": "thanks,problem solved.\nmore question:\ni want to train with 4 gpus use Between-graph mode,how to do?. multi-gpu training,beacuase io overload. thank you. ok,thanks. ok, I see,thanks a lot. ok,works,it\u2018s very thoughtful.. pip uninstall tensorpack\npython setup.py install\nproblem solved!!. ",
    "bor2093": "The problem solves by changing intra_op_parallelism_threads from 1 to 0 in tfutils/common.py. ",
    "HongyangGao": "Thank you. But I am running the same code. When the server is just rebooted, this can work.  But if I use CTRL-C to stop the program, it will show this problem. Actually I am using the way in imagenet-resnet example to prepare the data. I am wondering if the queue in the last run is not exiting. Can you give me some advice? or do you need my code?. Actually, sometimes it will show this error. Sometimes, it will enter ipython.. Yes, I not running this code in local. I can have a try. Thank you very much.. The error becomes sqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. Can you help with it? Thank you.\n. Sure. I didn't use sqlite reader. I just follow the imagenet-resnet and give the dataflow the address of my data.\n\u001b[32m[1027 00:20:33 @logger.py:74]\u001b[0m Argv: main.py\n\u001b[32m[1027 00:20:33 @tensor_net.py:46]\u001b[0m Running on 2 towers. Batch size per tower: 64\n\u001b[32m[1027 00:20:33 @fs.py:89]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Env var $TENSORPACK_DATASET not set, using /home/hgao/tensorpack_data for datasets.\n\u001b[32m[1027 00:20:34 @prefetch.py:263]\u001b[0m [PrefetchDataZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.\n\u001b[32m[1027 00:20:34 @ilsvrc.py:118]\u001b[0m Assuming directory /tempspace2/hgao/data/imagenet/val has original structure.\n\u001b[32m[1027 00:20:34 @param.py:189]\u001b[0m Use ./logdir1/hyper.txt to set hyperparam: 'learning_rate'.\n\u001b[32m[1027 00:20:34 @inference_runner.py:83]\u001b[0m InferenceRunner will eval on an InputSource of size 782\n\u001b[32m[1027 00:21:08 @input_source.py:178]\u001b[0m Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...\n\u001b[32m[1027 00:21:08 @input_source.py:459]\u001b[0m Setting up StagingArea for GPU prefetching ...\n\u001b[32m[1027 00:21:08 @training.py:41]\u001b[0m Training a model of 2 towers\n\u001b[32m[1027 00:21:08 @training.py:92]\u001b[0m Building graph for training tower 0 on device LeastLoadedDeviceSetter-/gpu:0...\n\u001b[32m[1027 00:21:09 @regularize.py:108]\u001b[0m Add REGULARIZATION_LOSSES of 58 tensors on the total cost.\n\u001b[32m[1027 00:21:11 @training.py:92]\u001b[0m Building graph for training tower 1 on device LeastLoadedDeviceSetter-/gpu:1...\n\u001b[32m[1027 00:21:11 @regularize.py:108]\u001b[0m Add REGULARIZATION_LOSSES of 58 tensors on the total cost.\n\u001b[32m[1027 00:21:13 @model_utils.py:47]\u001b[0m \u001b[36mModel Parameters: \n\u001b[0mname                                            shape                   dim  device\n\nconv_s/weights:0                                [3, 3, 3, 32]           864  /device:GPU:0\nconv_s/batch_norm/gamma:0                       [32]                     32  /device:GPU:1\nconv_s/batch_norm/beta:0                        [32]                     32  /device:GPU:1\nconv_1_0/conv1/conv/weights:0                   [3, 3, 32, 1]           288  /device:GPU:1\nconv_1_0/conv1/batch_norm/gamma:0               [32]                     32  /device:GPU:1\nconv_1_0/conv1/batch_norm/beta:0                [32]                     32  /device:GPU:1\nconv_1_0/conv2/weights:0                        [1, 1, 32, 64]         2048  /device:GPU:1\nconv_1_0/conv2/batch_norm/gamma:0               [64]                     64  /device:GPU:0\nconv_1_0/conv2/batch_norm/beta:0                [64]                     64  /device:GPU:0\nconv_1_1/conv1/conv/weights:0                   [3, 3, 64, 1]           576  /device:GPU:0\nconv_1_1/conv1/batch_norm/gamma:0               [64]                     64  /device:GPU:0\nconv_1_1/conv1/batch_norm/beta:0                [64]                     64  /device:GPU:0\nconv_1_1/conv2/weights:0                        [1, 1, 64, 128]        8192  /device:GPU:0\nconv_1_1/conv2/batch_norm/gamma:0               [128]                   128  /device:GPU:1\nconv_1_1/conv2/batch_norm/beta:0                [128]                   128  /device:GPU:1\nconv_1_2/conv1/conv/weights:0                   [3, 3, 128, 1]         1152  /device:GPU:1\nconv_1_2/conv1/batch_norm/gamma:0               [128]                   128  /device:GPU:1\nconv_1_2/conv1/batch_norm/beta:0                [128]                   128  /device:GPU:1\nconv_1_2/conv2/weights:0                        [1, 1, 128, 128]      16384  /device:GPU:1\nconv_1_2/conv2/batch_norm/gamma:0               [128]                   128  /device:GPU:0\nconv_1_2/conv2/batch_norm/beta:0                [128]                   128  /device:GPU:0\nconv_1_3/conv1/conv/weights:0                   [3, 3, 128, 1]         1152  /device:GPU:0\nconv_1_3/conv1/batch_norm/gamma:0               [128]                   128  /device:GPU:0\nconv_1_3/conv1/batch_norm/beta:0                [128]                   128  /device:GPU:0\nconv_1_3/conv2/weights:0                        [1, 1, 128, 256]      32768  /device:GPU:0\nconv_1_3/conv2/batch_norm/gamma:0               [256]                   256  /device:GPU:1\nconv_1_3/conv2/batch_norm/beta:0                [256]                   256  /device:GPU:1\nconv_1_4/conv1/conv/weights:0                   [3, 3, 256, 1]         2304  /device:GPU:1\nconv_1_4/conv1/batch_norm/gamma:0               [256]                   256  /device:GPU:1\nconv_1_4/conv1/batch_norm/beta:0                [256]                   256  /device:GPU:1\nconv_1_4/conv2/weights:0                        [1, 1, 256, 256]      65536  /device:GPU:1\nconv_1_4/conv2/batch_norm/gamma:0               [256]                   256  /device:GPU:0\nconv_1_4/conv2/batch_norm/beta:0                [256]                   256  /device:GPU:0\nconv_1_5/conv1/conv/weights:0                   [3, 3, 256, 1]         2304  /device:GPU:0\nconv_1_5/conv1/batch_norm/gamma:0               [256]                   256  /device:GPU:0\nconv_1_5/conv1/batch_norm/beta:0                [256]                   256  /device:GPU:0\nconv_1_5/conv2/weights:0                        [1, 1, 256, 512]     131072  /device:GPU:0\nconv_1_5/conv2/batch_norm/gamma:0               [512]                   512  /device:GPU:1\nconv_1_5/conv2/batch_norm/beta:0                [512]                   512  /device:GPU:1\nconv_2/group_0_conv0/conv/weights:0             [1, 1, 4, 1, 1]           4  /device:GPU:1\nconv_2/group_0/conv_0/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:1\nconv_2/group_0/conv_0/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:1\nconv_2/group_0/conv_0/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:1\nconv_2/group_0/conv_0/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:1\nconv_2/group_0/conv_0/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:1\nconv_2/group_0/conv_0/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:1\nconv_2/group_0/conv_1/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:1\nconv_2/group_0/conv_1/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:1\nconv_2/group_0/conv_1/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:1\nconv_2/group_0/conv_1/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:1\nconv_2/group_0/conv_1/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:1\nconv_2/group_0/conv_1/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:1\nconv_2/group_0/conv_2/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:1\nconv_2/group_0/conv_2/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:1\nconv_2/group_0/conv_2/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:1\nconv_2/group_0/conv_2/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:1\nconv_2/group_0/conv_2/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:1\nconv_2/group_0/conv_2/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:1\nconv_2/group_0/conv_3/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:1\nconv_2/group_0/conv_3/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:1\nconv_2/group_0/conv_3/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:1\nconv_2/group_0/conv_3/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:1\nconv_2/group_0/conv_3/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:1\nconv_2/group_0/conv_3/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:1\nconv_2/group_0/conv_4/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:1\nconv_2/group_0/conv_4/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:1\nconv_2/group_0/conv_4/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:1\nconv_2/group_0/conv_4/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:1\nconv_2/group_0/conv_4/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:0\nconv_2/group_0/conv_4/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:0\nconv_2/group_1_conv0/conv/weights:0             [1, 1, 4, 1, 1]           4  /device:GPU:0\nconv_2/group_1/conv_0/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:0\nconv_2/group_1/conv_0/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:0\nconv_2/group_1/conv_0/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:0\nconv_2/group_1/conv_0/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:0\nconv_2/group_1/conv_0/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:1\nconv_2/group_1/conv_0/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:1\nconv_2/group_1/conv_1/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:1\nconv_2/group_1/conv_1/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:1\nconv_2/group_1/conv_1/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:1\nconv_2/group_1/conv_1/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:1\nconv_2/group_1/conv_1/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:0\nconv_2/group_1/conv_1/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:0\nconv_2/group_1/conv_2/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:0\nconv_2/group_1/conv_2/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:0\nconv_2/group_1/conv_2/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:0\nconv_2/group_1/conv_2/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:0\nconv_2/group_1/conv_2/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:1\nconv_2/group_1/conv_2/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:1\nconv_2/group_1/conv_3/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:1\nconv_2/group_1/conv_3/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:1\nconv_2/group_1/conv_3/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:1\nconv_2/group_1/conv_3/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:1\nconv_2/group_1/conv_3/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:0\nconv_2/group_1/conv_3/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:0\nconv_2/group_1/conv_4/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:0\nconv_2/group_1/conv_4/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:0\nconv_2/group_1/conv_4/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:0\nconv_2/group_1/conv_4/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:0\nconv_2/group_1/conv_4/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:1\nconv_2/group_1/conv_4/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:1\nconv_2/group_2_conv0/conv/weights:0             [1, 1, 4, 1, 1]           4  /device:GPU:1\nconv_2/group_2/conv_0/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:1\nconv_2/group_2/conv_0/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:1\nconv_2/group_2/conv_0/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:1\nconv_2/group_2/conv_0/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:1\nconv_2/group_2/conv_0/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:0\nconv_2/group_2/conv_0/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:0\nconv_2/group_2/conv_1/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:0\nconv_2/group_2/conv_1/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:0\nconv_2/group_2/conv_1/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:0\nconv_2/group_2/conv_1/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:0\nconv_2/group_2/conv_1/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:1\nconv_2/group_2/conv_1/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:1\nconv_2/group_2/conv_2/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:1\nconv_2/group_2/conv_2/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:1\nconv_2/group_2/conv_2/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:1\nconv_2/group_2/conv_2/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:1\nconv_2/group_2/conv_2/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:0\nconv_2/group_2/conv_2/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:0\nconv_2/group_2/conv_3/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:0\nconv_2/group_2/conv_3/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:0\nconv_2/group_2/conv_3/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:0\nconv_2/group_2/conv_3/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:0\nconv_2/group_2/conv_3/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:1\nconv_2/group_2/conv_3/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:1\nconv_2/group_2/conv_4/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:1\nconv_2/group_2/conv_4/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:1\nconv_2/group_2/conv_4/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:1\nconv_2/group_2/conv_4/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:1\nconv_2/group_2/conv_4/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:0\nconv_2/group_2/conv_4/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:0\nconv_2/group_3_conv0/conv/weights:0             [1, 1, 4, 1, 1]           4  /device:GPU:0\nconv_2/group_3/conv_0/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:0\nconv_2/group_3/conv_0/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:0\nconv_2/group_3/conv_0/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:0\nconv_2/group_3/conv_0/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:0\nconv_2/group_3/conv_0/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:1\nconv_2/group_3/conv_0/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:1\nconv_2/group_3/conv_1/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:1\nconv_2/group_3/conv_1/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:1\nconv_2/group_3/conv_1/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:1\nconv_2/group_3/conv_1/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:1\nconv_2/group_3/conv_1/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:0\nconv_2/group_3/conv_1/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:0\nconv_2/group_3/conv_2/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:0\nconv_2/group_3/conv_2/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:0\nconv_2/group_3/conv_2/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:0\nconv_2/group_3/conv_2/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:0\nconv_2/group_3/conv_2/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:1\nconv_2/group_3/conv_2/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:1\nconv_2/group_3/conv_3/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:1\nconv_2/group_3/conv_3/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:1\nconv_2/group_3/conv_3/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:1\nconv_2/group_3/conv_3/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:1\nconv_2/group_3/conv_3/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:0\nconv_2/group_3/conv_3/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:0\nconv_2/group_3/conv_4/conv1/conv/weights:0      [3, 3, 128, 1]         1152  /device:GPU:0\nconv_2/group_3/conv_4/conv1/batch_norm/gamma:0  [128]                   128  /device:GPU:0\nconv_2/group_3/conv_4/conv1/batch_norm/beta:0   [128]                   128  /device:GPU:0\nconv_2/group_3/conv_4/conv2/weights:0           [1, 1, 128, 128]      16384  /device:GPU:0\nconv_2/group_3/conv_4/conv2/batch_norm/gamma:0  [128]                   128  /device:GPU:1\nconv_2/group_3/conv_4/conv2/batch_norm/beta:0   [128]                   128  /device:GPU:1\nconv_3_0/conv1/conv/weights:0                   [3, 3, 512, 1]         4608  /device:GPU:1\nconv_3_0/conv1/batch_norm/gamma:0               [512]                   512  /device:GPU:1\nconv_3_0/conv1/batch_norm/beta:0                [512]                   512  /device:GPU:1\nconv_3_0/conv2/weights:0                        [1, 1, 512, 1024]    524288  /device:GPU:1\nconv_3_0/conv2/batch_norm/gamma:0               [1024]                 1024  /device:GPU:0\nconv_3_0/conv2/batch_norm/beta:0                [1024]                 1024  /device:GPU:0\nconv_3_1/conv1/conv/weights:0                   [3, 3, 1024, 1]        9216  /device:GPU:0\nconv_3_1/conv1/batch_norm/gamma:0               [1024]                 1024  /device:GPU:0\nconv_3_1/conv1/batch_norm/beta:0                [1024]                 1024  /device:GPU:0\nconv_3_1/conv2/weights:0                        [1, 1, 1024, 1024]  1048576  /device:GPU:0\nconv_3_1/conv2/batch_norm/gamma:0               [1024]                 1024  /device:GPU:1\nconv_3_1/conv2/batch_norm/beta:0                [1024]                 1024  /device:GPU:1\nout/pool/batch_norm/gamma:0                     [1024]                 1024  /device:GPU:1\nout/pool/batch_norm/beta:0                      [1024]                 1024  /device:GPU:1\nout/dense/weights:0                             [1024, 1000]        1024000  /device:GPU:1\nout/dense/biases:0                              [1000]                 1000  /device:GPU:0\u001b[36m\nTotal #vars=179, #param=3251000 (12.40 MB assuming all float32)\u001b[0m\n\u001b[32m[1027 00:21:13 @base.py:207]\u001b[0m Setup callbacks graph ...\n\u001b[32m[1027 00:21:15 @input_source.py:178]\u001b[0m Setting up the queue 'DataParallelInferenceRunner/QueueInput/input_queue' for CPU prefetching ...\n\u001b[32m[1027 00:21:15 @predictor_factory.py:54]\u001b[0m Building predictor tower 'InferenceTower0' on device /gpu:0 ...\n\u001b[32m[1027 00:21:15 @predictor_factory.py:54]\u001b[0m Building predictor tower 'InferenceTower1' on device /gpu:1 ...\n\u001b[32m[1027 00:21:16 @summary.py:34]\u001b[0m Maintain moving average summary of 4 tensors.\n\u001b[32m[1027 00:21:16 @graph.py:91]\u001b[0m Applying collection UPDATE_OPS of 232 ops.\n\u001b[32m[1027 00:21:19 @base.py:212]\u001b[0m Creating the session ...\n\u001b[32m[1027 00:21:22 @base.py:216]\u001b[0m Initializing the session ...\n\u001b[32m[1027 00:21:22 @base.py:223]\u001b[0m Graph Finalized.\n\u001b[32m[1027 00:21:24 @concurrency.py:36]\u001b[0m Starting EnqueueThread DataParallelInferenceRunner/QueueInput/input_queue ...\n\u001b[32m[1027 00:21:24 @concurrency.py:36]\u001b[0m Starting EnqueueThread QueueInput/input_queue ...\n\u001b[32m[1027 00:21:24 @input_source.py:418]\u001b[0m Pre-filling staging area ...\n\u001b[32m[1027 00:21:25 @common.py:140]\u001b[0m \u001b[4m\u001b[5m\u001b[31mERR\u001b[0m Cannot batch data. Perhaps they are of inconsistent shape?\nTraceback (most recent call last):\n  File \"/tempspace/hgao/py3.6/lib/python3.6/site-packages/tensorpack/dataflow/common.py\", line 136, in _aggregate_batch\n    np.asarray([x[k] for x in data_holder], dtype=tp))\n  File \"/tempspace/hgao/py3.6/lib/python3.6/site-packages/numpy/core/numeric.py\", line 531, in asarray\n    return array(a, dtype, copy=False, order=order)\nValueError: setting an array element with a sequence.\n\u001b[32m[1027 00:21:25 @common.py:143]\u001b[0m \u001b[4m\u001b[5m\u001b[31mERR\u001b[0m Shape of all arrays to be batched: [(64, 224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3),\n (224, 224, 3)]\n\u001b[32m[1027 00:22:20 @common.py:140]\u001b[0m \u001b[4m\u001b[5m\u001b[31mERR\u001b[0m Cannot batch data. Perhaps they are of inconsistent shape?\nTraceback (most recent call last):\n  File \"/tempspace/hgao/py3.6/lib/python3.6/site-packages/tensorpack/dataflow/common.py\", line 136, in _aggregate_batch\n    np.asarray([x[k] for x in data_holder], dtype=tp))\n  File \"/tempspace/hgao/py3.6/lib/python3.6/site-packages/numpy/core/numeric.py\", line 531, in asarray\n    return array(a, dtype, copy=False, order=order)\nValueError: setting an array element with a sequence.\n\u001b[32m[1027 00:22:20 @input_source.py:140]\u001b[0m \u001b[4m\u001b[5m\u001b[31mERR\u001b[0m Exception in EnqueueThread QueueInput/input_queue:\nTraceback (most recent call last):\n  File \"/tempspace/hgao/py3.6/lib/python3.6/site-packages/tensorpack/dataflow/common.py\", line 136, in _aggregate_batch\n    np.asarray([x[k] for x in data_holder], dtype=tp))\n  File \"/tempspace/hgao/py3.6/lib/python3.6/site-packages/numpy/core/numeric.py\", line 531, in asarray\n    return array(a, dtype, copy=False, order=order)\nValueError: setting an array element with a sequence.\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/tempspace/hgao/py3.6/lib/python3.6/site-packages/tensorpack/input_source/input_source.py\", line 130, in run\n    for dp in self.dataflow.get_data():\n  File \"/tempspace/hgao/py3.6/lib/python3.6/site-packages/tensorpack/dataflow/common.py\", line 110, in get_data\n    yield BatchData._aggregate_batch(holder, self.use_list)\n  File \"/tempspace/hgao/py3.6/lib/python3.6/site-packages/tensorpack/dataflow/common.py\", line 142, in _aggregate_batch\n    s = pprint.pformat([x[k].shape for x in data_holder])\n  File \"/tempspace/hgao/py3.6/lib/python3.6/site-packages/tensorpack/dataflow/common.py\", line 142, in \n    s = pprint.pformat([x[k].shape for x in data_holder])\nAttributeError: 'int' object has no attribute 'shape'\n\u001b[32m[1027 00:22:20 @base.py:273]\u001b[0m Training was stopped.\n\u001b[32m[1027 00:22:20 @input_source.py:146]\u001b[0m EnqueueThread QueueInput/input_queue Exited.\n\u001b[32m[1027 00:22:20 @prefetch.py:56]\u001b[0m [PrefetchDataZMQ] Context terminated.\n\u001b[32m[1027 00:22:20 @input_source.py:146]\u001b[0m EnqueueThread DataParallelInferenceRunner/QueueInput/input_queue Exited.\n. but I have use this code os.environ['TENSORPACK_PIPEDIR'] = '/tmp'\nand also export TENSORPACK_PIPEDIR=/tmp in bash\nShould I do something else?. I think it is the problem of tensorflow versions. I tried tf1.3. It is working now. But one more question, two GPU training is the fastest. When I try more gpus, the speed decreases significantly. Is there anything I can do to promote more gpus?. \nThis is training progress. Currently I am using 3 gpus and training a MobileNet like model on k80.. Thank you. It works.. Thanks. I have been using tensorpack for more than one year. It's a really good one for multi-gpu training. The thing confuses me is this:\n\nI train on 4 gpus. But it seems all are allocated to one gpu instead of 4. Is it possible for some configuration. On the server where code runs well, the gpus are Titan XP. But on this trouble server, they are GTX 1080 ti. I mean does the multi-gpu allocation depend on something? Thank you very much.\n. Actually I tried to reduce the batch size such that OOM does not happen. In that case, gpu 6 was allocated with all tensors while the other three remained 215MiB. It seems the tensor allocation only allocate tensors on the first gpu.. Thanks. I can try to debug somehow.. Nope, I debugged several days but didn't find any clue. Since the deadline\nis comming soom, I may try to fix it later. I am not sure it's the problem\nof tensoeflow or cuda.\nOn Thu, Dec 20, 2018, 16:46 Yuxin Wu notifications@github.com wrote:\n\nHave you solved this issue?\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/tensorpack/tensorpack/issues/1014#issuecomment-449160177,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGGcrw-pgKb9J_h2XqHGTh_-gRAqQWjSks5u7BMugaJpZM4ZVjp9\n.\n-- \n\nHongyang\n. Thank you very much. I will try it.\nOn Thu, Dec 20, 2018 at 4:54 PM Yuxin Wu notifications@github.com wrote:\n\nThe easiest way to debug is to remove your model and replace it with a\nsimple model of several FullyConnected layers and see if the wrong behavior\nstill exists.\nIf the error is gone, that means your model may have override the device\nsetting somewhere and you can debug from there by adding parts of your\nmodel back.\nIf the wrong behavior is still there, remove the data and replace it with\nFakeData. Remove all the custom callbacks.\nIf the wrong behavior is still there then, you should have already created\na small enough example that you can share.\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/tensorpack/tensorpack/issues/1014#issuecomment-449161913,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGGcrwHOteR-eSp44o2gxAks6CuOLzDGks5u7BUZgaJpZM4ZVjp9\n.\n-- \n\nHongyang\n. ",
    "PeisenZhao": "Hello, @ppwwyyxx . I have met the same problem as HongyangGao mentioned. I tried:\n1 Add   export TENSORPACK_PIPEDIR=/tmp   to bashrc\n2 Changed the tensorflow version to 1.3\nbut they did not help.\nI found an another interesting phenomenon. If I kept the input data list as the first time I run the code(after reboot), the code could run. But if I changed the input data list, the error appeared. \nI think there are something fixed when I run the code first time. \n. Here are the log, the error is same as HongyangGao provided. The part of the model loading log is omitted.\n[0127 12:27:00 @sessinit.py:219] Restoring from dict ...\n[0127 12:27:09 @base.py:157] Graph Finalized.\n[0127 12:27:13 @concurrency.py:36] Starting EnqueueThread DataParallelInferenceRunner/QueueInput/input_queue ...\n[0127 12:27:13 @concurrency.py:36] Starting EnqueueThread QueueInput/input_queue ...\n[0127 12:27:13 @base.py:191] Start Epoch 1 ...\n  0%|                                                                                                                                  |0/1788[00:00<?,?it/s][0127 12:27:13 @input_source.py:493] Pre-filling staging area ...\n[0127 12:27:14 @common.py:138] ERR Cannot batch data. Perhaps they are of inconsistent shape?\nTraceback (most recent call last):\n  File \"/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorpack/dataflow/common.py\", line 136, in _aggregate_batch\n    np.asarray([x[k] for x in data_holder], dtype=tp))\n  File \"/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/numpy/core/numeric.py\", line 492, in asarray\n    return array(a, dtype, copy=False, order=order)\nValueError: setting an array element with a sequence.\n[0127 12:27:14 @common.py:141] ERR Shape of all arrays to be batched: [(224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3),\n (224, 224, 3),\n (128, 224, 224, 3)]\n[0127 12:27:14 @common.py:138] ERR Cannot batch data. Perhaps they are of inconsistent shape?\nTraceback (most recent call last):\n  File \"/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorpack/dataflow/common.py\", line 136, in _aggregate_batch\n    np.asarray([x[k] for x in data_holder], dtype=tp))\n  File \"/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/numpy/core/numeric.py\", line 492, in asarray\n    return array(a, dtype, copy=False, order=order)\nValueError: setting an array element with a sequence.\n[0127 12:27:14 @input_source.py:142] ERR Exception in EnqueueThread QueueInput/input_queue:\nTraceback (most recent call last):\n  File \"/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorpack/input_source/input_source.py\", line 135, in run\n    self.op.run(feed_dict=feed)\n  File \"/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1744, in run\n    _run_using_default_session(self, feed_dict, self.graph, session)\n  File \"/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 4120, in _run_using_default_session\n    session.run(operation, feed_dict)\n  File \"/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 895, in run\n    run_metadata_ptr)\n  File \"/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1124, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\n    options, run_metadata)\n  File \"/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\n    raise type(e)(node_def, op, message)\nInvalidArgumentError: You must feed a value for placeholder tensor 'label' with dtype int32 and shape [?]\n     [[Node: label = Placeholderdtype=DT_INT32, shape=[?], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\nCaused by op u'label', defined at:\n  File \"video-pretrain-resnet-ft.py\", line 269, in \n    SyncMultiGPUTrainerParameterServer(config).train()\n  File \"/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorpack/train/base.py\", line 327, in new\n    return old_trainer(args, kwargs)\n  File \"/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorpack/trainv1/multigpu.py\", line 66, in init\n    super(SyncMultiGPUTrainerParameterServer, self).init(config)\n  File \"/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorpack/trainv1/base.py\", line 73, in init\n    self._setup()   # subclass will setup the graph and InputSource\n  File \"/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorpack/trainv1/multigpu.py\", line 69, in _setup\n    callbacks = self._input_source.setup(self.model.get_inputs_desc())\n  File \"/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorpack/utils/argtools.py\", line 171, in wrapper\n    return func(*args, kwargs)\n  File \"/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorpack/input_source/input_source_base.py\", line 97, in setup\n    self._setup(inputs_desc)\n  File \"/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorpack/input_source/input_source.py\", line 524, in _setup\n    self._input.setup(inputs)\n  File \"/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorpack/utils/argtools.py\", line 171, in wrapper\n    return func(args, **kwargs)\n  File \"/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorpack/input_source/input_source_base.py\", line 97, in setup\n    self._setup(inputs_desc)\n  File \"/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorpack/input_source/input_source.py\", line 185, in _setup\n    self._input_placehdrs = [v.build_placeholder_reuse() for v in inputs]\n  File \"/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorpack/graph_builder/model_desc.py\", line 68, in build_placeholder_reuse\n    return self.build_placeholder()\n  File \"/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorpack/graph_builder/model_desc.py\", line 53, in build_placeholder\n    self.type, shape=self.shape, name=self.name)\n  File \"/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1548, in placeholder\n    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2094, in _placeholder\n    name=name)\n  File \"/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/DB/rhome/pszhao/anaconda2/envs/condaEnv/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1204, in init\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'label' with dtype int32 and shape [?]\n     [[Node: label = Placeholderdtype=DT_INT32, shape=[?], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n[0127 12:27:14 @input_source.py:148] EnqueueThread QueueInput/input_queue Exited.\n[0127 12:27:14 @base.py:207] Training was stopped.\nPrefetchDataZMQ successfully cleaned-up.\n[0127 12:27:14 @parallel.py:62] [PrefetchDataZMQ] Context terminated.\n[0127 12:27:14 @input_source.py:148] EnqueueThread DataParallelInferenceRunner/QueueInput/input_queue Exited.\nPrefetchDataZMQ successfully cleaned-up.. Here are the dataset I used. If I changed the data list, the error appeared.\nds = VideoDataset(datadir, name, shuffle=True)\nds = AugmentImageComponent(ds, augmentors, copy=False)\nds = PrefetchDataZMQ(ds, cpu)\nds = BatchData(ds, batch_size, remainder=False). Here are the implemented. Change it back, it can run correctly. The \"data list\" is a txt file of lines of \"image_path + label\".(\"self.full_dir\" in the code) I think it can remember the first data list's information. When I change another data list, it goes wrong.\nMay I ask another question? In this implementation, how can I keep the images' order in one batch if I use more cpus in this code \"ds = PrefetchDataZMQ(ds, cpu)\". Maybe cpu=4 or more.\n```python\nclass VideoDatasetFiles(RNGDataFlow):\ndef init(self, dir, name, shuffle=None):\n    assert name in ['train', 'test', 'val', 'simple_test'], name\n    assert os.path.isdir(dir), dir\n    self.full_dir = os.path.join(dir, '{}_videofolder.txt'.format(name))\n    self.name = name\n    if shuffle is None:\n        shuffle = name == 'train'\n    self.shuffle = shuffle\n    imglist = []\n    with open(self.full_dir) as f:\n        lines = f.readlines()\n    for line in lines:\n        video, frames, label = line.strip().split()\n        frames_list = ['{:05}.jpg'.format(i+1) for i in [int(j*(float(frames)/24)) for j in xrange(24)]]\n        label = int(label)\n        for img in frames_list:\n            imglist.append((os.path.join(dir, '20bn-something-something-v1', video, img), label))\n    self.imglist = imglist\n\ndef size(self):\n    return len(self.imglist)\n\ndef get_data(self):\n    idx = np.arange(self.size()/24)\n    if self.shuffle:\n        self.rng.shuffle(idx)\n    idxs = [tt*24+jj for tt in idx for jj in xrange(24)]\n    for k in idxs:\n        fname, label = self.imglist[k]\n        yield [fname, label]\n\nclass VideoDataset(VideoDatasetFiles):\ndef init(self, dir, name, shuffle=None):\n    super(VideoDataset, self).init(dir, name, shuffle)\n\ndef get_data(self):\n    for fname, label in super(VideoDataset, self).get_data():\n        im = cv2.imread(fname, cv2.IMREAD_COLOR)\n        assert im is not None, fname\n        yield [im, label]\n\n```. Thank you very much! Finally, I run it with PrefetchData successfully.. Filesystem            Size  Used Avail Use% Mounted on\nudev                   63G     0   63G   0% /dev\ntmpfs                  13G   12M   13G   1% /run\n/dev/sda3             880G   23G  813G   3% /\ntmpfs                  63G  380K   63G   1% /dev/shm\ntmpfs                 5.0M  4.0K  5.0M   1% /run/lock\ntmpfs                  63G     0   63G   0% /sys/fs/cgroup\n/dev/sda1             453M   62M  364M  15% /boot\n/dev/sdb1             3.6T  2.5G  3.4T   1% /home\n/dev/sdc1             3.6T   68M  3.4T   1% /home1\n192.168.28.10:/DB     6.0T  5.5T  141G  98% /DB\nDB7:/DATA3_DB7        9.1T  9.0T   96G  99% /DATA3_DB7\n192.168.28.10:/DATA   9.9T  9.1T  255G  98% /DATA\n192.168.28.10:/DATA2  8.2T  8.1T  121G  99% /DATA2\ntmpfs                  13G   16K   13G   1% /run/user/557\ntmpfs                  13G     0   13G   0% /run/user/546\ntmpfs                  13G     0   13G   0% /run/user/593\ntmpfs                  13G     0   13G   0% /run/user/563\ntmpfs                  13G     0   13G   0% /run/user/572\ntmpfs                  13G     0   13G   0% /run/user/565\ntmpfs                  13G     0   13G   0% /run/user/566\ntmpfs                  13G     0   13G   0% /run/user/544\ntmpfs                  13G     0   13G   0% /run/user/552. Thanks for your patient! These directories do not help. I have to use PrefetchData.. Amazing, it can work by using abstract socket. Maybe it is the filesystem problem. I am so happy to solve this problem!. ",
    "jiefengpeng": "But many other platforms implementing ResNets use tf.moments + batch_normalization instead of fused_batch_norm, such as the code training from scratch in Tensorflow official webpage, their speed is not so slow. Any communication delay problem in your code when using tf.moments + batch_normalization?. Ok, I got it that fused_batch_norm is faster than batch_normalization, but my point is that in other resnet implemented framework, which use moment + batch_normalization, their speed is 2 more times faster than your code with moment + batch_normalization.. By the way, we train on 8 TITAN X.. Ah, actually it is multi-GPU training with 8 TITAN X. And we've checked that the batch size and data layout are as same as yours. The only difference is they use moving_mean and moving_var in training for which they need to use moment&batch_normalization. I'm really confused that fused_batch_norm is significantly 6 more times faster than moment&batch_normalization. What's more, we observed a poor GPU utilization with moment&batch_normalization which is around 100% when using fused_batch_norm.. With nn.moments they calculate bm and bv, and update bm and bv with moving_mean and moving_var, after which they are fed to non-fused batch norm to output xn in training.\nOk, we try to collect details later and thanks a lot to your patience.. Actually we try to do something like renormalization in tensorpack.. As you said, the reason is the data format with NCHW. https://github.com/tensorflow/tensorflow/issues/9141\nIs there any problem occurred if we transform the data format at data layer to NHWC in tensorpack?. Sorry! I mistake the data layout with other implementation.\nSo there is no issue of \"tensorpack vs the other implementation\", and thanks again for your kindness!. ",
    "henrykmichalewski": "But wait, what is the fix? I tried to import other callbacks from base.py and trigger.py instead of Triggerable, but so far the learning seems to be stuck at  Pre-filling staging area .... ",
    "BichenWuUCB": "Hi Yuxin,\nYes, upgrading to the latest master solved this problem. Thanks, and great work! . Seems that it has something to do with auto tuning. I commented this line \nos.environ['TF_CUDNN_USE_AUTOTUNE'] = '0'\nto enable tensorflow auto tuning and the issue is gone. Otherwise, the GPU memory keeps growing with number of evaluation steps until OOM. \nMy current tensorflow version is 1.4.0 and cudnn version is 6.0.21  . Thanks Yuxin, this is very helpful. I'll look into it and let you know. . ",
    "lbin": "I got OOM exactly after 127 epoch on Titan xp GPUs.. Oh yes, it's my mistake. It is in traing.\n\n. ",
    "anthony123": "@ppwwyyxx   That's exactly what i want, I need  to know whether I am in training mode or testing mode ,  and when I am in testing mode, I need change the network a little bit, Just like what dropout does! . firstly, i do half the batch size from 64 to 32 correctly when using two gpus.  \nThe results are of  the FIRST epoch under two conditions(one gpu & two gpus). When I ran the code twice on one gpu, then the results of The FIRST EPOCH are almost the same. \nAll the other codes are the same under one gpu versus two gpus. Is it possible \"Pre-filling staging area\" affect the result?\nIn the line 205 of Inference_runnner.py, it says that \"hooks from StagingInput will force the consumption of nr_tower datapoints in every run.\". When i enable both InferenceRunner and DataParallelInferenceRunner(by pushing both to callbacks), there is only one output. Maybe I do it wrong?  . The validation(test) error are exactly the same !   I do not think this is right!. I do use it in the get_data() function, codes are as followings:\n```\n    ds = AugmentImageComponent(ds, augmentors)\n    nr_tower = args.gpu.split(',')\n    BATCH_SIZE = 64\n    BATCH_SIZE  /= len(nr_tower)\n    ds = BatchData(ds, BATCH_SIZE, remainder=not isTrain)\n    if isTrain:\n        ds = PrefetchData(ds, 3, 2)\n    return ds\n```\nThe iterations (For one epoch) change from 781 to 1562 for training, change from 157 to 313 for testing(two gpus versus one gpu).\nAnd I find one strange thing, \ncallbacks.append(DataParallelInferenceRunner(\n                dataset_test, [ScalarStats('cost'), ClassificationError()], [0,1,2]))\nstills work, but I only have two gpus on my computer. So How should I fix the #steps_per_epoch? should I pass 64(as before) as the second param for the BatchData function?. I change the code as following:\nfrom \nsteps_per_epoch = dataset_train.size()\nto\nsteps_per_epoch = dataset_train.size()//nr_tower\nand pass it to funciton:\nreturn TrainConfig(\n        dataflow=dataset_train,\n        callbacks=callbacks,\n        model=Model(args.k, args.num_block, args.layers_per_block, args.path, args.increasing_rate, args.layer_increasing_rate),\n        steps_per_epoch=steps_per_epoch,\n        max_epoch=args.max_epoch,\n        nr_tower=nr_tower,\n    )\nthe step is kept the same, but the problem is still there :(. Maybe you can give me your e-mail, and i can send you my code . the code is less than 300 lines. Keep steps_per_epoch constant solved my problem. \nchange the code\nfrom\nsteps_per_epoch = dataset_train.size()\nto\nsteps_per_epoch = dataset_train.size()//nr_tower\n. When i upgrade tensorpack to 0.8.1, there is another error has appeared. the error traces are as follows:\n\nNext i will try horovod trainer ,see if it can give me a reasonable speedup. ",
    "mrastegari": "Sorry I mean a quantized version (binary) for DoReFa-Net.. ",
    "tschnz": "Setting GPU 2,3 on a cluster with 4 GPUs starts training and building /gpu:0 and /gpu:1. I'm not admin for these machines. Would be also nice to have a choice on which device to train when using only one GPU on a multi-GPU cluster.. My bad. Can be closed. Works.. ",
    "AngusG": "I'm using the alexnet-dorefa.py training script with data in LMDB format and seeing up to 4.099TB of virtual memory usage causing things to crash approx 30% of the way into Epoch 1 on a 4 GPU cluster with 24 cores and 125GB CPU memory. I adapted get_imagenet_dataflow.py from imagenet_utils.py according to http://tensorpack.readthedocs.io/en/latest/tutorial/efficient-dataflow.html#sequential-read like so:\ndef get_imagenet_dataflow(\n        datadir, name, batch_size,\n        augmentors):\n    \"\"\"\n    See explanations in the tutorial:\n    http://tensorpack.readthedocs.io/en/latest/tutorial/efficient-dataflow.html\n    \"\"\"\n    assert name in ['train', 'val', 'test']\n    assert datadir is not None\n    assert isinstance(augmentors, list)\n    isTrain = name == 'train'\n    cpu = min(30, multiprocessing.cpu_count())\n    if isTrain:\n        ds = LMDBData(os.path.join(datadir, 'ILSVRC12-train.lmdb'), shuffle=False)\n        ds = LocallyShuffleData(ds, 50000)\n        ds = PrefetchData(ds, 5000, 1)\n        ds = LMDBDataPoint(ds)\n        ds = MapDataComponent(ds, lambda x: cv2.imdecode(x, cv2.IMREAD_COLOR), 0)\n        ds = AugmentImageComponent(ds, augmentors, copy=False)\n        ds = PrefetchDataZMQ(ds, cpu)\n        ds = BatchData(ds, batch_size, remainder=False)\n    else:\n        ds = LMDBData(os.path.join(datadir, 'ILSVRC12-val.lmdb'), shuffle=False)\n        aug = imgaug.AugmentorList(augmentors)\n        ds = LMDBDataPoint(ds)\n        ds = MapDataComponent(ds, lambda x: cv2.imdecode(x, cv2.IMREAD_COLOR), 0)\n        ds = AugmentImageComponent(ds, augmentors, copy=False)\n        ds = BatchData(ds, batch_size, remainder=True)\n        ds = PrefetchDataZMQ(ds, 1)\n    return ds\nI have tried reducing buffer_size in LocallyShuffleData() but that doesn't seem to have an effect on the virtual memory allocation. Full output is attached if you have any suggestions as to how I can address this issue.\ncdr344-3799739.txt\n. ",
    "dongzhuoyao": "I will check this \"5.6 vs 3.9\" problem.. if I set alpha=1, I can obtain a 4.74 result. could you merge the code first?  \nI will setup a README file to clarify the difference between the paper. hope me or someone else interested can fix it later.\n. the new preact18 architecture achieves 4.08, still exists a 1.08 difference.   =\u3002=. haven't done it, will do it later.. preact18-origin: 5.08\npreact18-mixup:3.7\nis this result ok?\n. yes, the code now  is a little messy,  I will clean it up. ok. updated. done. sorry for my carelessness, even though got a 3.7 result.  is this code ok? I will retrain it later.. already training. :smile:. updated. may be you can try that. I think it will be ok for github.. how about this https://mega.nz/#!YU1FWJrA!O1ywiCS2IiOlUCtCpI6HTJOMrneN-Qdv3ywQP5poecM  ?. also luckily, the vgg16.npy can be used for HED initialization.. \"--load model-xxx\"    is ok, don't need postfix.. check whether or not this file \"/train_log/alexnet-dorefa/model-660000.index\"  exists.. it may be better discuss the function first,  then start coding.. \naccording to https://github.com/kuangliu/pytorch-cifar/blob/master/models/preact_resnet.py, the above implementation in tensorpack: line 26 and line 27 should be exchanged.  @ppwwyyxx \n. A little clarification:\n\nThe PreResnet have two different implementations.   the official one is https://github.com/facebook/fb.resnet.torch/blob/master/models/preresnet.lua, and the unofficial one: https://github.com/kuangliu/pytorch-cifar/blob/master/models/preact_resnet.py.  what the tensorpack implemented is the official PreResnet. however, the mixup paper just choosed the unofficial one.\nthe unofficial one is \"bn-relu first and then  diverge\".\nour mixup is just an example for tensorpack, and followed the mixup authors' implementation(an unofficial preresnet).  so I think other examples in tensorpack based on official preresnet had better keep unchanged.\nThere is one mistake that @yselivonchyk corrected for me:\n\n\nResNet-18 with preactivation as by https://github.com/kuangliu/pytorch-cifar is using ResNet with preactivation block with 2 consecutive convolution layers in the block. Existing implementation was using 3 convolutional blocks.\n\n\nmore depths choice for mixup implementations are welcome.  but just keep it only suitable for this mixup example(should not influence other examples. because I think mixup example is a special case).. maybe this can help you: #488 . maybe my implementation is wrong, I contrast with your imagenet-resnet.py.\n\nTOTAL_BATCH_SIZE=256,\nin your implementations, it seems that no matter how many GPUS, the total batch size is always 256.\nI doubt:  since 1-gpu can hold 256, if you have 4-GPU, why not set batch size as 1024. in your implementation, if 4-gpu, single-gpu batch size is 256/4=64.\n. my writing about multi-gpu training is wrong, now fixed, thank you. hi, you can check any example in tensorpack, such as resnet.\njust follow them as convention.. sorry, typo.. \nI really want is ResNet101-SE.. \ud83d\ude1e . @JasonHanG hi, I current use pycharm, this phenomenon is normal, just dont need to care about it. it really import it, just pycharm doesn't recognise it.\nthis is mine:\n\n. @michaelklachko \nhi,  the params should include weight, bias(in conv), beta, gamma(in bn), those parameter number is unrelated to the dataset. \nplease kindly correct me if my understanding is wrong.. \ud83d\ude25 thank you @ppwwyyxx @michaelklachko . yes, i can do that, but in that way I still need hack the MapData method in the \"common.py\", because mixup needs two data, however, the MapData only returns one. any good suggestion?. yes , \nif not isTrain:\n       return [images, one_hot_labels]\n can make sure mixup only happen in training.. it may be better to use logger.info(\"...\"). still work on documentation. if possible, I will send a seperate pr.. got it. there exists following problems:\n\n\nbn/mean/EMA,bn/variance/EMA cannot load from dict.\n\n\nin tf.layers, kernel,bias is correspond to W,b in tensorpack. to make it fine-tune sucessfully, I should also pass the kernel_initializer,bias_initializer, it seemed that a AtrousConvolution is a must method to fine-tune.\n\n\nrelated log is as follows:\n[0105 14:48:46 @sessinit.py:206] Variables to restore from dict: group2/block9/conv3/bn/beta:0, group2/block11/conv1/bn/variance/EMA:0, group0/block1/conv1/bn/beta:0, group2/block11/conv3/bn/gamma:0, group3/block0/conv3/W:0, group2/block17/conv1/bn/mean/EMA:0, group0/block0/conv3/bn/beta:0, group2/block15/conv1/bn/beta:0, group0/block2/conv3/W:0, group2/block10/conv1/bn/gamma:0, group2/block22/conv1/bn/gamma:0, group2/block6/conv3/bn/beta:0, group2/block3/conv3/bn/variance/EMA:0, group3/block1/conv1/bn/variance/EMA:0, group2/block1/conv1/bn/variance/EMA:0, group3/block1/conv1/W:0, group2/block15/conv3/bn/variance/EMA:0, group2/block14/conv1/bn/gamma:0, group2/block4/conv3/bn/beta:0, group3/block1/conv3/bn/mean/EMA:0, group2/block13/conv3/bn/gamma:0, group0/block0/conv1/bn/variance/EMA:0, group2/block14/conv3/bn/variance/EMA:0, group2/block15/conv1/W:0, group2/block20/conv3/W:0, group2/block18/conv3/bn/variance/EMA:0, group2/block22/conv1/bn/beta:0, group2/block0/convshortcut/bn/beta:0, group1/block2/conv3/W:0, group2/block19/conv3/bn/mean/EMA:0, group0/block0/convshortcut/bn/beta:0, group2/block15/conv1/bn/mean/EMA:0, group0/block1/conv3/bn/variance/EMA:0, group2/block21/conv3/bn/variance/EMA:0, group3/block1/conv3/bn/gamma:0, group3/block2/conv1/bn/mean/EMA:0, group2/block5/conv3/W:0, group2/block16/conv1/bn/mean/EMA:0, group2/block4/conv1/bn/mean/EMA:0, group2/block6/conv1/bn/variance/EMA:0, group2/block3/conv3/bn/beta:0, group1/block0/conv1/bn/gamma:0, group2/block7/conv1/bn/mean/EMA:0, group2/block3/conv1/W:0, group2/block22/conv1/bn/mean/EMA:0, group2/block10/conv1/bn/variance/EMA:0, group2/block21/conv3/bn/gamma:0, group2/block11/conv1/W:0, group2/block19/conv1/bn/beta:0, group2/block4/conv1/W:0, group2/block12/conv3/bn/gamma:0, group3/block2/conv1/W:0, group3/block1/conv1/bn/gamma:0, group2/block10/conv1/bn/beta:0, group2/block11/conv3/bn/variance/EMA:0, group2/block10/conv3/bn/beta:0, group1/block2/conv3/bn/mean/EMA:0, group0/block2/conv3/bn/beta:0, group2/block4/conv3/W:0, group2/block12/conv3/W:0, group2/block11/conv1/bn/beta:0, group2/block1/conv1/bn/beta:0, group2/block6/conv1/W:0, group2/block13/conv3/W:0, group2/block0/conv3/bn/gamma:0, group2/block9/conv1/W:0, group1/block3/conv3/bn/variance/EMA:0, conv0/bn/mean/EMA:0, group1/block3/conv1/bn/variance/EMA:0, group2/block4/conv3/bn/mean/EMA:0, group1/block0/convshortcut/bn/variance/EMA:0, group2/block3/conv1/bn/variance/EMA:0, group2/block21/conv1/W:0, group2/block9/conv1/bn/gamma:0, group3/block2/conv1/bn/variance/EMA:0, group3/block0/convshortcut/bn/variance/EMA:0, group2/block14/conv3/W:0, group1/block0/convshortcut/bn/beta:0, group1/block1/conv1/bn/mean/EMA:0, group2/block19/conv3/bn/variance/EMA:0, group2/block9/conv1/bn/mean/EMA:0, group2/block17/conv3/bn/gamma:0, group1/block1/conv3/bn/gamma:0, group0/block2/conv1/bn/beta:0, group2/block16/conv3/bn/beta:0, group1/block2/conv3/bn/variance/EMA:0, group1/block3/conv3/bn/mean/EMA:0, group2/block10/conv3/W:0, group0/block1/conv3/bn/beta:0, group2/block20/conv3/bn/mean/EMA:0, group3/block2/conv1/bn/gamma:0, group2/block9/conv1/bn/variance/EMA:0, group2/block22/conv1/bn/variance/EMA:0, group2/block17/conv3/bn/variance/EMA:0, group2/block11/conv3/W:0, group2/block11/conv1/bn/mean/EMA:0, group0/block0/convshortcut/W:0, group2/block12/conv3/bn/mean/EMA:0, group2/block3/conv1/bn/beta:0, group2/block0/convshortcut/bn/variance/EMA:0, group2/block0/conv3/bn/mean/EMA:0, group2/block19/conv1/bn/mean/EMA:0, group2/block14/conv3/bn/mean/EMA:0, group2/block15/conv1/bn/gamma:0, group1/block1/conv3/bn/beta:0, group2/block19/conv3/W:0, group2/block4/conv3/bn/gamma:0, group1/block2/conv1/bn/mean/EMA:0, group2/block21/conv1/bn/mean/EMA:0, group2/block22/conv3/W:0, group2/block18/conv3/bn/gamma:0, group2/block16/conv1/W:0, group2/block2/conv3/bn/variance/EMA:0, group2/block18/conv1/W:0, group2/block17/conv1/bn/variance/EMA:0, group2/block5/conv3/bn/gamma:0, group2/block10/conv1/W:0, group2/block16/conv3/bn/mean/EMA:0, group2/block5/conv1/bn/gamma:0, group2/block10/conv1/bn/mean/EMA:0, group2/block8/conv3/bn/beta:0, group2/block9/conv3/W:0, group2/block12/conv3/bn/variance/EMA:0, group1/block0/convshortcut/bn/gamma:0, group3/block0/conv3/bn/gamma:0, group2/block7/conv3/bn/mean/EMA:0, group0/block1/conv1/W:0, group1/block0/conv3/bn/mean/EMA:0, group2/block6/conv3/W:0, group2/block16/conv1/bn/gamma:0, group2/block0/conv1/bn/mean/EMA:0, group2/block5/conv1/bn/variance/EMA:0, group1/block0/conv1/bn/mean/EMA:0, group1/block2/conv1/bn/beta:0, group2/block21/conv1/bn/variance/EMA:0, group3/block2/conv3/bn/gamma:0, group3/block2/conv3/bn/beta:0, group2/block20/conv1/bn/beta:0, group2/block22/conv3/bn/mean/EMA:0, group2/block8/conv3/bn/mean/EMA:0, group2/block14/conv3/bn/beta:0, group2/block18/conv1/bn/mean/EMA:0, group2/block16/conv3/bn/gamma:0, group2/block10/conv3/bn/gamma:0, group1/block3/conv3/bn/gamma:0, group2/block21/conv3/bn/mean/EMA:0, group0/block2/conv1/bn/variance/EMA:0, group0/block2/conv1/bn/mean/EMA:0, group1/block0/convshortcut/bn/mean/EMA:0, group1/block0/conv1/bn/variance/EMA:0, group2/block6/conv3/bn/variance/EMA:0, group0/block1/conv3/W:0, group2/block11/conv3/bn/beta:0, group2/block13/conv1/bn/beta:0, group2/block4/conv1/bn/variance/EMA:0, group2/block5/conv3/bn/beta:0, group3/block0/conv3/bn/variance/EMA:0, group0/block2/conv1/W:0, group2/block2/conv1/bn/gamma:0, group2/block18/conv1/bn/variance/EMA:0, group1/block1/conv1/bn/beta:0, group2/block2/conv1/W:0, group2/block0/conv1/W:0, group2/block13/conv1/W:0, group2/block20/conv1/bn/gamma:0, group3/block1/conv1/bn/mean/EMA:0, group1/block2/conv3/bn/beta:0, group2/block17/conv3/W:0, group3/block0/convshortcut/bn/mean/EMA:0, group2/block21/conv3/bn/beta:0, group3/block1/conv1/bn/beta:0, group3/block0/convshortcut/bn/beta:0, group2/block17/conv1/W:0, group2/block3/conv1/bn/gamma:0, group2/block7/conv1/bn/beta:0, group2/block2/conv1/bn/mean/EMA:0, group2/block15/conv1/bn/variance/EMA:0, group0/block2/conv3/bn/gamma:0, group2/block8/conv3/bn/variance/EMA:0, group2/block4/conv3/bn/variance/EMA:0, group2/block20/conv3/bn/variance/EMA:0, group3/block0/conv3/bn/mean/EMA:0, group3/block0/convshortcut/bn/gamma:0, group0/block0/conv1/bn/mean/EMA:0, group2/block17/conv3/bn/beta:0, group0/block0/convshortcut/bn/gamma:0, group2/block18/conv1/bn/gamma:0, group2/block0/conv1/bn/gamma:0, group2/block2/conv3/bn/beta:0, conv0/W:0, group1/block0/convshortcut/W:0, group2/block9/conv3/bn/variance/EMA:0, group2/block0/conv3/W:0, group2/block8/conv1/W:0, group2/block7/conv1/bn/gamma:0, group2/block14/conv1/W:0, group3/block0/convshortcut/W:0, group1/block1/conv1/W:0, group2/block9/conv1/bn/beta:0, group1/block3/conv1/bn/mean/EMA:0, group2/block7/conv3/W:0, group2/block5/conv1/bn/mean/EMA:0, group3/block2/conv3/W:0, group2/block0/convshortcut/W:0, group2/block1/conv3/bn/gamma:0, group1/block0/conv3/bn/variance/EMA:0, group2/block16/conv1/bn/variance/EMA:0, group2/block21/conv1/bn/beta:0, group2/block2/conv3/W:0, group2/block3/conv3/W:0, group0/block0/conv3/bn/mean/EMA:0, group3/block0/conv1/bn/mean/EMA:0, group1/block0/conv3/bn/gamma:0, group2/block7/conv1/W:0, group2/block0/conv1/bn/variance/EMA:0, group1/block2/conv3/bn/gamma:0, group2/block2/conv3/bn/gamma:0, group2/block22/conv3/bn/variance/EMA:0, group2/block8/conv1/bn/mean/EMA:0, group2/block12/conv3/bn/beta:0, group2/block15/conv3/bn/gamma:0, group1/block0/conv3/bn/beta:0, group2/block22/conv3/bn/beta:0, conv0/bn/beta:0, group3/block1/conv3/bn/beta:0, conv0/bn/variance/EMA:0, group3/block0/conv3/bn/beta:0, group2/block15/conv3/bn/beta:0, group2/block13/conv3/bn/variance/EMA:0, group0/block2/conv3/bn/mean/EMA:0, group1/block1/conv3/W:0, group3/block0/conv1/bn/beta:0, group2/block16/conv1/bn/beta:0, group1/block1/conv1/bn/gamma:0, group2/block10/conv3/bn/variance/EMA:0, group2/block6/conv1/bn/gamma:0, group0/block0/conv3/bn/gamma:0, group2/block12/conv1/bn/gamma:0, group2/block0/conv3/bn/beta:0, group1/block3/conv3/bn/beta:0, group0/block0/convshortcut/bn/variance/EMA:0, group0/block0/convshortcut/bn/mean/EMA:0, group2/block1/conv3/bn/variance/EMA:0, group2/block16/conv3/bn/variance/EMA:0, group2/block6/conv1/bn/beta:0, group1/block1/conv3/bn/variance/EMA:0, group2/block15/conv3/W:0, group2/block1/conv1/bn/gamma:0, group2/block4/conv1/bn/beta:0, group0/block0/conv3/W:0, group3/block2/conv3/bn/variance/EMA:0, group2/block17/conv1/bn/gamma:0, group1/block3/conv3/W:0, group2/block1/conv1/bn/mean/EMA:0, group2/block8/conv1/bn/gamma:0, group2/block12/conv1/bn/mean/EMA:0, group3/block1/conv3/W:0, group3/block2/conv3/bn/mean/EMA:0, group2/block2/conv3/bn/mean/EMA:0, group2/block19/conv3/bn/beta:0, group2/block6/conv3/bn/gamma:0, group2/block22/conv1/W:0, group2/block18/conv3/bn/mean/EMA:0, group2/block14/conv3/bn/gamma:0, group2/block18/conv1/bn/beta:0, group0/block1/conv1/bn/variance/EMA:0, group1/block3/conv1/bn/gamma:0, group2/block12/conv1/W:0, group0/block1/conv1/bn/gamma:0, group2/block11/conv3/bn/mean/EMA:0, group2/block12/conv1/bn/beta:0, group2/block5/conv1/W:0, group2/block0/conv1/bn/beta:0, group1/block2/conv1/bn/gamma:0, group2/block20/conv3/bn/beta:0, group2/block13/conv1/bn/mean/EMA:0, group1/block3/conv1/bn/beta:0, group2/block1/conv3/W:0, group3/block0/conv1/bn/gamma:0, group2/block8/conv3/bn/gamma:0, group0/block1/conv3/bn/mean/EMA:0, group1/block2/conv1/W:0, group1/block1/conv1/bn/variance/EMA:0, group2/block2/conv1/bn/beta:0, group2/block4/conv1/bn/gamma:0, group2/block14/conv1/bn/mean/EMA:0, group0/block0/conv1/W:0, group1/block1/conv3/bn/mean/EMA:0, conv0/bn/gamma:0, group2/block1/conv3/bn/mean/EMA:0, group2/block7/conv1/bn/variance/EMA:0, group2/block0/conv3/bn/variance/EMA:0, group2/block20/conv3/bn/gamma:0, group3/block0/conv1/W:0, group2/block11/conv1/bn/gamma:0, group2/block13/conv3/bn/beta:0, group2/block19/conv3/bn/gamma:0, group2/block7/conv3/bn/gamma:0, group2/block14/conv1/bn/variance/EMA:0, group2/block1/conv1/W:0, group2/block6/conv1/bn/mean/EMA:0, group2/block0/convshortcut/bn/gamma:0, group0/block1/conv1/bn/mean/EMA:0, group2/block17/conv1/bn/beta:0, group0/block0/conv3/bn/variance/EMA:0, group2/block14/conv1/bn/beta:0, group2/block8/conv1/bn/variance/EMA:0, group2/block20/conv1/bn/variance/EMA:0, group2/block5/conv3/bn/mean/EMA:0, group2/block18/conv3/W:0, group2/block8/conv1/bn/beta:0, group2/block18/conv3/bn/beta:0, group2/block2/conv1/bn/variance/EMA:0, group0/block1/conv3/bn/gamma:0, group1/block0/conv3/W:0, group2/block7/conv3/bn/variance/EMA:0, group2/block20/conv1/W:0, group0/block2/conv3/bn/variance/EMA:0, group2/block13/conv1/bn/variance/EMA:0, group3/block0/conv1/bn/variance/EMA:0, group2/block13/conv1/bn/gamma:0, group2/block5/conv1/bn/beta:0, group2/block21/conv3/W:0, group2/block7/conv3/bn/beta:0, group2/block20/conv1/bn/mean/EMA:0, group0/block0/conv1/bn/beta:0, group2/block17/conv3/bn/mean/EMA:0, group0/block0/conv1/bn/gamma:0, group2/block0/convshortcut/bn/mean/EMA:0, group2/block10/conv3/bn/mean/EMA:0, group0/block2/conv1/bn/gamma:0, group2/block15/conv3/bn/mean/EMA:0, group2/block1/conv3/bn/beta:0, group2/block6/conv3/bn/mean/EMA:0, group2/block12/conv1/bn/variance/EMA:0, group2/block9/conv3/bn/gamma:0, group2/block13/conv3/bn/mean/EMA:0, group3/block1/conv3/bn/variance/EMA:0, group2/block16/conv3/W:0, group3/block2/conv1/bn/beta:0, group2/block3/conv1/bn/mean/EMA:0, group1/block0/conv1/bn/beta:0, group2/block19/conv1/bn/variance/EMA:0, group2/block22/conv3/bn/gamma:0, group1/block0/conv1/W:0, group2/block5/conv3/bn/variance/EMA:0, group1/block2/conv1/bn/variance/EMA:0, group2/block3/conv3/bn/mean/EMA:0, group1/block3/conv1/W:0, group2/block9/conv3/bn/mean/EMA:0, group2/block19/conv1/bn/gamma:0, group2/block19/conv1/W:0, group2/block21/conv1/bn/gamma:0, group2/block8/conv3/W:0, group2/block3/conv3/bn/gamma:0\n[0105 14:48:46 @sessinit.py:89] WRN The following variables are in the graph, but not found in the dict: aspp6_conv/bias:0, aspp6_conv/kernel:0, beta1_power:0, beta2_power:0, bn/beta:0, bn/gamma:0, bn/mean/EMA:0, bn/variance/EMA:0, global_step:0, group0/block0/bn/beta:0, group0/block0/bn/gamma:0, group0/block0/bn/mean/EMA:0, group0/block0/bn/variance/EMA:0, group0/block0/conv2/bias:0, group0/block0/conv2/kernel:0, group0/block1/bn/beta:0, group0/block1/bn/gamma:0, group0/block1/bn/mean/EMA:0, group0/block1/bn/variance/EMA:0, group0/block1/conv2/bias:0, group0/block1/conv2/kernel:0, group0/block2/bn/beta:0, group0/block2/bn/gamma:0, group0/block2/bn/mean/EMA:0, group0/block2/bn/variance/EMA:0, group0/block2/conv2/bias:0, group0/block2/conv2/kernel:0, group1/block0/bn/beta:0, group1/block0/bn/gamma:0, group1/block0/bn/mean/EMA:0, group1/block0/bn/variance/EMA:0, group1/block0/conv2/bias:0, group1/block0/conv2/kernel:0, group1/block1/bn/beta:0, group1/block1/bn/gamma:0, group1/block1/bn/mean/EMA:0, group1/block1/bn/variance/EMA:0, group1/block1/conv2/bias:0, group1/block1/conv2/kernel:0, group1/block2/bn/beta:0, group1/block2/bn/gamma:0, group1/block2/bn/mean/EMA:0, group1/block2/bn/variance/EMA:0, group1/block2/conv2/bias:0, group1/block2/conv2/kernel:0, group1/block3/bn/beta:0, group1/block3/bn/gamma:0, group1/block3/bn/mean/EMA:0, group1/block3/bn/variance/EMA:0, group1/block3/conv2/bias:0, group1/block3/conv2/kernel:0, group2/block0/bn/beta:0, group2/block0/bn/gamma:0, group2/block0/bn/mean/EMA:0, group2/block0/bn/variance/EMA:0, group2/block0/conv2/bias:0, group2/block0/conv2/kernel:0, group2/block1/bn/beta:0, group2/block1/bn/gamma:0, group2/block1/bn/mean/EMA:0, group2/block1/bn/variance/EMA:0, group2/block1/conv2/bias:0, group2/block1/conv2/kernel:0, group2/block10/bn/beta:0, group2/block10/bn/gamma:0, group2/block10/bn/mean/EMA:0, group2/block10/bn/variance/EMA:0, group2/block10/conv2/bias:0, group2/block10/conv2/kernel:0, group2/block11/bn/beta:0, group2/block11/bn/gamma:0, group2/block11/bn/mean/EMA:0, group2/block11/bn/variance/EMA:0, group2/block11/conv2/bias:0, group2/block11/conv2/kernel:0, group2/block12/bn/beta:0, group2/block12/bn/gamma:0, group2/block12/bn/mean/EMA:0, group2/block12/bn/variance/EMA:0, group2/block12/conv2/bias:0, group2/block12/conv2/kernel:0, group2/block13/bn/beta:0, group2/block13/bn/gamma:0, group2/block13/bn/mean/EMA:0, group2/block13/bn/variance/EMA:0, group2/block13/conv2/bias:0, group2/block13/conv2/kernel:0, group2/block14/bn/beta:0, group2/block14/bn/gamma:0, group2/block14/bn/mean/EMA:0, group2/block14/bn/variance/EMA:0, group2/block14/conv2/bias:0, group2/block14/conv2/kernel:0, group2/block15/bn/beta:0, group2/block15/bn/gamma:0, group2/block15/bn/mean/EMA:0, group2/block15/bn/variance/EMA:0, group2/block15/conv2/bias:0, group2/block15/conv2/kernel:0, group2/block16/bn/beta:0, group2/block16/bn/gamma:0, group2/block16/bn/mean/EMA:0, group2/block16/bn/variance/EMA:0, group2/block16/conv2/bias:0, group2/block16/conv2/kernel:0, group2/block17/bn/beta:0, group2/block17/bn/gamma:0, group2/block17/bn/mean/EMA:0, group2/block17/bn/variance/EMA:0, group2/block17/conv2/bias:0, group2/block17/conv2/kernel:0, group2/block18/bn/beta:0, group2/block18/bn/gamma:0, group2/block18/bn/mean/EMA:0, group2/block18/bn/variance/EMA:0, group2/block18/conv2/bias:0, group2/block18/conv2/kernel:0, group2/block19/bn/beta:0, group2/block19/bn/gamma:0, group2/block19/bn/mean/EMA:0, group2/block19/bn/variance/EMA:0, group2/block19/conv2/bias:0, group2/block19/conv2/kernel:0, group2/block2/bn/beta:0, group2/block2/bn/gamma:0, group2/block2/bn/mean/EMA:0, group2/block2/bn/variance/EMA:0, group2/block2/conv2/bias:0, group2/block2/conv2/kernel:0, group2/block20/bn/beta:0, group2/block20/bn/gamma:0, group2/block20/bn/mean/EMA:0, group2/block20/bn/variance/EMA:0, group2/block20/conv2/bias:0, group2/block20/conv2/kernel:0, group2/block21/bn/beta:0, group2/block21/bn/gamma:0, group2/block21/bn/mean/EMA:0, group2/block21/bn/variance/EMA:0, group2/block21/conv2/bias:0, group2/block21/conv2/kernel:0, group2/block22/bn/beta:0, group2/block22/bn/gamma:0, group2/block22/bn/mean/EMA:0, group2/block22/bn/variance/EMA:0, group2/block22/conv2/bias:0, group2/block22/conv2/kernel:0, group2/block3/bn/beta:0, group2/block3/bn/gamma:0, group2/block3/bn/mean/EMA:0, group2/block3/bn/variance/EMA:0, group2/block3/conv2/bias:0, group2/block3/conv2/kernel:0, group2/block4/bn/beta:0, group2/block4/bn/gamma:0, group2/block4/bn/mean/EMA:0, group2/block4/bn/variance/EMA:0, group2/block4/conv2/bias:0, group2/block4/conv2/kernel:0, group2/block5/bn/beta:0, group2/block5/bn/gamma:0, group2/block5/bn/mean/EMA:0, group2/block5/bn/variance/EMA:0, group2/block5/conv2/bias:0, group2/block5/conv2/kernel:0, group2/block6/bn/beta:0, group2/block6/bn/gamma:0, group2/block6/bn/mean/EMA:0, group2/block6/bn/variance/EMA:0, group2/block6/conv2/bias:0, group2/block6/conv2/kernel:0, group2/block7/bn/beta:0, group2/block7/bn/gamma:0, group2/block7/bn/mean/EMA:0, group2/block7/bn/variance/EMA:0, group2/block7/conv2/bias:0, group2/block7/conv2/kernel:0, group2/block8/bn/beta:0, group2/block8/bn/gamma:0, group2/block8/bn/mean/EMA:0, group2/block8/bn/variance/EMA:0, group2/block8/conv2/bias:0, group2/block8/conv2/kernel:0, group2/block9/bn/beta:0, group2/block9/bn/gamma:0, group2/block9/bn/mean/EMA:0, group2/block9/bn/variance/EMA:0, group2/block9/conv2/bias:0, group2/block9/conv2/kernel:0, group3/block0/bn/beta:0, group3/block0/bn/gamma:0, group3/block0/bn/mean/EMA:0, group3/block0/bn/variance/EMA:0, group3/block0/conv2/bias:0, group3/block0/conv2/kernel:0, group3/block1/bn/beta:0, group3/block1/bn/gamma:0, group3/block1/bn/mean/EMA:0, group3/block1/bn/variance/EMA:0, group3/block1/conv2/bias:0, group3/block1/conv2/kernel:0, group3/block2/bn/beta:0, group3/block2/bn/gamma:0, group3/block2/bn/mean/EMA:0, group3/block2/bn/variance/EMA:0, group3/block2/conv2/bias:0, group3/block2/conv2/kernel:0, learning_rate:0\n[0105 14:48:46 @sessinit.py:89] WRN The following variables are in the dict, but not found in the graph: group0/block0/conv2/W:0, group0/block0/conv2/bn/beta:0, group0/block0/conv2/bn/gamma:0, group0/block0/conv2/bn/mean/EMA:0, group0/block0/conv2/bn/variance/EMA:0, group0/block1/conv2/W:0, group0/block1/conv2/bn/beta:0, group0/block1/conv2/bn/gamma:0, group0/block1/conv2/bn/mean/EMA:0, group0/block1/conv2/bn/variance/EMA:0, group0/block2/conv2/W:0, group0/block2/conv2/bn/beta:0, group0/block2/conv2/bn/gamma:0, group0/block2/conv2/bn/mean/EMA:0, group0/block2/conv2/bn/variance/EMA:0, group1/block0/conv2/W:0, group1/block0/conv2/bn/beta:0, group1/block0/conv2/bn/gamma:0, group1/block0/conv2/bn/mean/EMA:0, group1/block0/conv2/bn/variance/EMA:0, group1/block1/conv2/W:0, group1/block1/conv2/bn/beta:0, group1/block1/conv2/bn/gamma:0, group1/block1/conv2/bn/mean/EMA:0, group1/block1/conv2/bn/variance/EMA:0, group1/block2/conv2/W:0, group1/block2/conv2/bn/beta:0, group1/block2/conv2/bn/gamma:0, group1/block2/conv2/bn/mean/EMA:0, group1/block2/conv2/bn/variance/EMA:0, group1/block3/conv2/W:0, group1/block3/conv2/bn/beta:0, group1/block3/conv2/bn/gamma:0, group1/block3/conv2/bn/mean/EMA:0, group1/block3/conv2/bn/variance/EMA:0, group2/block0/conv2/W:0, group2/block0/conv2/bn/beta:0, group2/block0/conv2/bn/gamma:0, group2/block0/conv2/bn/mean/EMA:0, group2/block0/conv2/bn/variance/EMA:0, group2/block1/conv2/W:0, group2/block1/conv2/bn/beta:0, group2/block1/conv2/bn/gamma:0, group2/block1/conv2/bn/mean/EMA:0, group2/block1/conv2/bn/variance/EMA:0, group2/block10/conv2/W:0, group2/block10/conv2/bn/beta:0, group2/block10/conv2/bn/gamma:0, group2/block10/conv2/bn/mean/EMA:0, group2/block10/conv2/bn/variance/EMA:0, group2/block11/conv2/W:0, group2/block11/conv2/bn/beta:0, group2/block11/conv2/bn/gamma:0, group2/block11/conv2/bn/mean/EMA:0, group2/block11/conv2/bn/variance/EMA:0, group2/block12/conv2/W:0, group2/block12/conv2/bn/beta:0, group2/block12/conv2/bn/gamma:0, group2/block12/conv2/bn/mean/EMA:0, group2/block12/conv2/bn/variance/EMA:0, group2/block13/conv2/W:0, group2/block13/conv2/bn/beta:0, group2/block13/conv2/bn/gamma:0, group2/block13/conv2/bn/mean/EMA:0, group2/block13/conv2/bn/variance/EMA:0, group2/block14/conv2/W:0, group2/block14/conv2/bn/beta:0, group2/block14/conv2/bn/gamma:0, group2/block14/conv2/bn/mean/EMA:0, group2/block14/conv2/bn/variance/EMA:0, group2/block15/conv2/W:0, group2/block15/conv2/bn/beta:0, group2/block15/conv2/bn/gamma:0, group2/block15/conv2/bn/mean/EMA:0, group2/block15/conv2/bn/variance/EMA:0, group2/block16/conv2/W:0, group2/block16/conv2/bn/beta:0, group2/block16/conv2/bn/gamma:0, group2/block16/conv2/bn/mean/EMA:0, group2/block16/conv2/bn/variance/EMA:0, group2/block17/conv2/W:0, group2/block17/conv2/bn/beta:0, group2/block17/conv2/bn/gamma:0, group2/block17/conv2/bn/mean/EMA:0, group2/block17/conv2/bn/variance/EMA:0, group2/block18/conv2/W:0, group2/block18/conv2/bn/beta:0, group2/block18/conv2/bn/gamma:0, group2/block18/conv2/bn/mean/EMA:0, group2/block18/conv2/bn/variance/EMA:0, group2/block19/conv2/W:0, group2/block19/conv2/bn/beta:0, group2/block19/conv2/bn/gamma:0, group2/block19/conv2/bn/mean/EMA:0, group2/block19/conv2/bn/variance/EMA:0, group2/block2/conv2/W:0, group2/block2/conv2/bn/beta:0, group2/block2/conv2/bn/gamma:0, group2/block2/conv2/bn/mean/EMA:0, group2/block2/conv2/bn/variance/EMA:0, group2/block20/conv2/W:0, group2/block20/conv2/bn/beta:0, group2/block20/conv2/bn/gamma:0, group2/block20/conv2/bn/mean/EMA:0, group2/block20/conv2/bn/variance/EMA:0, group2/block21/conv2/W:0, group2/block21/conv2/bn/beta:0, group2/block21/conv2/bn/gamma:0, group2/block21/conv2/bn/mean/EMA:0, group2/block21/conv2/bn/variance/EMA:0, group2/block22/conv2/W:0, group2/block22/conv2/bn/beta:0, group2/block22/conv2/bn/gamma:0, group2/block22/conv2/bn/mean/EMA:0, group2/block22/conv2/bn/variance/EMA:0, group2/block3/conv2/W:0, group2/block3/conv2/bn/beta:0, group2/block3/conv2/bn/gamma:0, group2/block3/conv2/bn/mean/EMA:0, group2/block3/conv2/bn/variance/EMA:0, group2/block4/conv2/W:0, group2/block4/conv2/bn/beta:0, group2/block4/conv2/bn/gamma:0, group2/block4/conv2/bn/mean/EMA:0, group2/block4/conv2/bn/variance/EMA:0, group2/block5/conv2/W:0, group2/block5/conv2/bn/beta:0, group2/block5/conv2/bn/gamma:0, group2/block5/conv2/bn/mean/EMA:0, group2/block5/conv2/bn/variance/EMA:0, group2/block6/conv2/W:0, group2/block6/conv2/bn/beta:0, group2/block6/conv2/bn/gamma:0, group2/block6/conv2/bn/mean/EMA:0, group2/block6/conv2/bn/variance/EMA:0, group2/block7/conv2/W:0, group2/block7/conv2/bn/beta:0, group2/block7/conv2/bn/gamma:0, group2/block7/conv2/bn/mean/EMA:0, group2/block7/conv2/bn/variance/EMA:0, group2/block8/conv2/W:0, group2/block8/conv2/bn/beta:0, group2/block8/conv2/bn/gamma:0, group2/block8/conv2/bn/mean/EMA:0, group2/block8/conv2/bn/variance/EMA:0, group2/block9/conv2/W:0, group2/block9/conv2/bn/beta:0, group2/block9/conv2/bn/gamma:0, group2/block9/conv2/bn/mean/EMA:0, group2/block9/conv2/bn/variance/EMA:0, group3/block0/conv2/W:0, group3/block0/conv2/bn/beta:0, group3/block0/conv2/bn/gamma:0, group3/block0/conv2/bn/mean/EMA:0, group3/block0/conv2/bn/variance/EMA:0, group3/block1/conv2/W:0, group3/block1/conv2/bn/beta:0, group3/block1/conv2/bn/gamma:0, group3/block1/conv2/bn/mean/EMA:0, group3/block1/conv2/bn/variance/EMA:0, group3/block2/conv2/W:0, group3/block2/conv2/bn/beta:0, group3/block2/conv2/bn/gamma:0, group3/block2/conv2/bn/mean/EMA:0, group3/block2/conv2/bn/variance/EMA:0, linear/W:0, linear/b:\n  . final solution:   I move the atrous convolution implementation into deeplab example, so that it will not pollute the tensorpack kernel.. because the image size in pascalVOC2012 are all different, some are large, some are very small,  RandomCropWithPadding differs from\n\nCenterPaste(background_filler=constant) + RandomCrop()\n\nin :\n\nif the image size larger than crop_size,  it will not do padding, then cropping.\nelse  it will do padding, then cropping.\n\nanyway, I just move RandomCropWithPadding into the deeplabv2 example in case of pollution. if we finally find an alternative solution based on current tensorpack kernel, or this method really necessary for some dataset, we can move it into tensorpack kernel again.\n  . ",
    "xmfbit": "Thanks! I misunderstood it.. @sharpstill  \n\n-24 only happens in the last conv of each unit.\n\nThe channels of previous units are 384\nSee here: (Ine 73-79): https://github.com/ppwwyyxx/tensorpack/blob/master/examples/ShuffleNet/shufflenet.py#L73\n  . ",
    "sharpstill": "@ppwwyyxx Really Sorry to bother you again,Why you say 384 % 16 == 0 is enough.  I notice that l = Conv2D('conv2', l,\n               out_channel if stride == 1 else out_channel - in_channel,\n               1, split=group, nl=BN) , but in the caller l = shufflenet_unit(l, channels[0], group, 2 if i == 0 else 1), thus the channel=24must satisfy formular: (out_channel - in_channel)%group == 0? (384-24)%8 == 0\nAm I understand correct?\n. @ppwwyyxx oh really sorry, I think the tensorflow cannot print anything in the running process because the sess.run magic call hides everything. Maybe I am wrong, I am not familar with tf, I use chainer before.\n  . @ppwwyyxx two proof:\n1.  because fm_anchors is define on 1/16 feature map,\n2. and in\nhttps://github.com/ppwwyyxx/tensorpack/blob/master/examples/FasterRCNN/model.py#L43\n ,your comment also says the box_logits is defined on 1/16 feature map.\nI think the decoded_boxes  is defined on 1/16 feature map . @ppwwyyxx \nabout 1: https://github.com/ppwwyyxx/tensorpack/blob/master/examples/FasterRCNN/train.py#L86  and https://github.com/ppwwyyxx/tensorpack/blob/master/examples/FasterRCNN/data.py#L50 prove that the fm_anchor is defined on 1/16.0 smaller scale feature map.\nAm I correct or I am missing something?. @ppwwyyxx  I am confused, in get_all_anchors, yes, I found it return anchor defined on the original size of image, but in https://github.com/ppwwyyxx/tensorpack/blob/master/examples/FasterRCNN/train.py#L86 , You only use tf.shape(image)[0] // config.ANCHOR_STRIDE to slice the returned anchor ,why ?\nand what is difference between shape and scale, for example, shape=[H/16, W/16] means the image size is 1/16 of original image(H,W)??\n  . I have wechat, and email:sharpstill@163.com, If you can speak chinese , we can contact with each other more convenient , I am using your tensorpack to finish my PhD thesis, So it is very important for me. @ppwwyyxx \none point: \nin fm_anchors = self._get_anchors(image) the returned anchor box coordinates is height/16.0 and width/16.0 of original image height & width? Am I right? so decoded_boxes box coordinates is height/16.0 and width/16.0 of original image height & width??? why you say it is defined on original image?. but in my knowledge, tensorflow cannot add break point and op cannot break point to debug, because sess.run magic hide the background actual flow. for  complex code such as Faster R-CNN, You use IPython as IP; IP.embed(). to debug? or log? I am using pycharm before, and using chainer which can debug like usual python code. But this time I use your tensorpack , how to best practice of debug ?. tf 1.5 has already released , which fully support dynamic graph( like in PyTorch), which will ease the burden of debug breakpoint tf. Will tensorpack supply a switch argument to open dynamic graph let us to easy debug?\n  . Thanks a lot , your example code sames also support group conv original from paper ShuffleNet(Face++),  You just show tf.nnsupport it, the trick is you just pass the filter_shape to W.\nWithout you , I cannot get the point and image how to implement it yet, can you put the example tutorial in Your Official Document?\n  . one question please, I see the tf document abouttf.nn.depthwise_conv2d,\nwhich is located in https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d\n What is filter's last  dimension channel_multiplier, I cannot understand why the doment says output has in_channels * channel_multiplier channels.\nIf I am not understand wrongly, the ShuffleNet paper's group convolution is output channel is equals with the input channel????\nDoes the ShuffleNet paper's group convolution mean each input feature map's channel will be convolved by multiple kernel channel? and this will cause the output channel is in_channels * channel_multiplier ?\n. I just cannot get the ShuffleNet paper's  group convolution output channel == input channel? in your example, it seems not like this?  I read the paper, and did not found the useful information about how to does group convolve exactly?\nCan you write just some information to help me please?. In your  models.Conv2D: there is split parameter:\nsplit (int): Split channels as used in Alexnet. Defaults to 1 (no split).\nWhat does this splitmean, does this actually do 1x1 group conv in ShuffleNet?\n  . Thanks a lot, I read source code, split does actually a lot of individual conv layer here.. Another question, If I want to use ShuffleNet as Faster R-CNN backbone base net, How to use it? Which layers can I used as RPN and RoI Pooling? Which layers can I use after RoI pooling?\n  . @ppwwyyxx Thank you anyway. Your tensorpack give me a lot of help already. @ppwwyyxx In your tutorial document , You write the conception of tower, What is tower concept, you didn't give any information in earlier section of your document? Can you give some information about it?. Maybe stackoverflow is better place for these questions? Chainer group move these type of question to stackoverflow  which can answer by other developer who are willing to do as well.. Maybe I put a tag = \"tensorpack\" on stackoverflow . You may watch it, You can answer it or not based on your mood.. Really sorry to bother you , I am not for fun to do it, My job is modify your faster r-cnn code.\n I have deep learning experience and have a vision paper in CVPR, but I used chainer framework before.\nI am new to tensorpack . So sometimes I am stuck for your code and really need your help.. It is tensorflow official model file, so it is should not have problem file. I found the solution in https://github.com/openai/pixel-cnn/issues/9. tf.train.import_meta_graph(args.meta, clear_devices=True). ",
    "charliememory": "This link seems not work now. Could you help to address this issue? Thanks.. @PatWie  This link still does not work for me  http://models.tensorpack.com/caffe/ \nAnyway, I find the one cached by google search engine working for me. http://webcache.googleusercontent.com/search?q=cache:ox2CCzCHrPkJ:models.tensorpack.com/+&cd=1&hl=en&ct=clnk&gl=be. Well, now it works. Thanks for your help.. ",
    "Forwil": "thanks ppwwyyxx! the reference is very useful.\nI found a workaround method by add two line below __name__ == '__main__':\ngpu_options = tf.GPUOptions(allow_growth=True)\nsess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\njust create a useless session with gpu_options in the begging of sample.\nIt seems like gpu_options is a global setting and not a session setting.. ",
    "Ancho5515": "@erdollar hello! I want to know how does your net work with prelu? can you share some results?. ",
    "lliimsft": "Thanks. \nWe switched to v1 and that works. Will test v2 trainer later on.. ",
    "chongyang-xu": "hi,@ppwwyyxx  I have the same question,could you give me any advice? \nI read the docs http://tensorpack.readthedocs.io/en/latest/modules/train.html?highlight=DistributedTrainerReplicated#tensorpack.train.DistributedTrainerReplicated\nand  I modified last lines of tensorpack/examples/ResNet/cifar10-resnet.py\n166    config = TrainConfig(\n167         model=Model(n=NUM_UNITS),\n168         dataflow=dataset_train,\n169         callbacks=  [\n170             ModelSaver(),\n171             InferenceRunner(dataset_test,\n172                             [ScalarStats('cost'), ClassificationError('wrong_vector')]),\n173              ScheduledHyperParamSetter('learning_rate',\n174                                       [(1, 0.1), (82, 0.01), (123, 0.001), (300, 0.0002)]),\n175        ],\n176         max_epoch=164,\n177         session_init=SaverRestore(args.load) if args.load else None,\n178 \n179     )\n180     config.session_config=None\n181     print(config.data,config.model)\n182     nr_gpu = max(get_nr_gpu(), 1)\n183 \n184     hosts = ['gpu2', 'gpu3']\n185     cluster_spec = tf.train.ClusterSpec({\n186             'ps': [h + ':2222' for h in hosts],\n187             'worker': [h + ':2223' for h in hosts]\n188          })\n189 \n190     server = tf.train.Server(\n191                 cluster_spec, job_name=args.job, task_index=args.task,\n192                     config = get_default_sess_config()  )\n193 \n194     #DistributedTrainerReplicated(config, server).train()\n195 \n196     #launch_train_with_config(config, SyncMultiGPUTrainerParameterServer(nr_gpu))\n197     #launch_train_with_config(config, SyncMultiGPUTrainer(nr_gpu))\n198     launch_train_with_config(config, DistributedTrainerReplicated(nr_gpu, server))\nand I launch it like this:\n(tf-py27) [chongyang@gpu2 ResNet]$ python cifar10-resnet.py --job worker --task 0\nand part of the error message\n[1206 14:16:28 @training.py:90] Building graph for training tower 1 on device /job:worker/task:0/gpu:1...\n2017-12-06 14:16:28.115375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:06:00.0, compute capability: 6.0)\n2017-12-06 14:16:28.115407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:84:00.0, compute capability: 6.0)\n[1206 14:16:47 @trainers.py:193] WRN For efficiency, local MODEL_VARIABLES are only synced to PS once every epoch. Be careful if you save the model more frequently than this.\nTraceback (most recent call last):\n  File \"cifar10-resnet.py\", line 198, in <module>\n    launch_train_with_config(config, DistributedTrainerReplicated(nr_gpu, server))\n  File \"/home/chongyang/.local/lib/python2.7/site-packages/tensorpack/train/interface.py\", line 88, in launch_train_with_config\n    model._build_graph_get_cost, model.get_optimizer)\n  File \"/home/chongyang/.local/lib/python2.7/site-packages/tensorpack/utils/argtools.py\", line 171, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chongyang/.local/lib/python2.7/site-packages/tensorpack/train/tower.py\", line 148, in setup_graph\n    self.register_callback(input_callbacks + train_callbacks)\n  File \"/home/chongyang/.local/lib/python2.7/site-packages/tensorpack/train/base.py\", line 146, in _register_callback\n    self._register_callback(x)\n  File \"/home/chongyang/.local/lib/python2.7/site-packages/tensorpack/train/base.py\", line 149, in _register_callback\n    assert not isinstance(self._callbacks, Callbacks), \\\nAttributeError: 'DistributedTrainerReplicated' object has no attribute '_callbacks'\n. ",
    "magentay": "\ncovert the  imagenet resnet18 checkpoints file to graphdef file resnet.pb using \n      output_node = ['InferenceTower/linear/output']\n\n```python\n   with tf.Session() as sess:\n        new_saver = tf.train.import_meta_graph(\n            metafile, clear_devices=True)\n        new_saver.restore(sess, ckpt.model_checkpoint_path)\n    output_graph_def = tf.graph_util.convert_variables_to_constants(\n        sess,  # The session is used to retrieve the weights\n        tf.get_default_graph().as_graph_def(),  # The graph_def is used to retrieve the nodes\n        output_node  # The output node names are used to select the usefull nodes\n    )\n\n    # Finally we serialize and dump the output graph to the filesystem\n    with tf.gfile.GFile(output_graph, \"wb\") as f:\n        f.write(output_graph_def.SerializeToString())\n    print(\"%d ops in the final graph.\" % len(output_graph_def.node))\n\n```\n2  try to use summarize_graph \n bazel-bin/tensorflow/tools/graph_transforms/summarize_graph \\\n    >   --in_graph= resnet.pb\n    No inputs spotted.\n    No variables spotted.\n    Found 1 possible outputs: (name=InferenceTower/linear/output, op=Identity)\n    Found 11699467 (11.70M) const parameters, 0 (0) variable parameters, and 0 control_edges\n    Op types used: 208 Const, 124 Identity, 81 Reshape, 61 Mul, 48 Add, 21 Sub, 20 Conv2D, 20 Rsqrt, 17 \n    Relu, 1 Mean, 1 Transpose, 1 RealDiv, 1 QueueDequeueV2, 1 MaxPool, 1 MatMul, 1 FIFOQueueV2, 1 Cast, 1 BiasAdd\nI also tried output_node = ['tower0/linear/output'] and had the same result as \"No inputs spotted.\". \nThanks!\n. I am expecting the summarize_graph can find both \"input\" and \"output\". \nIf I convert this graphdef file to tflite file\n```\nbazel-bin/tensorflow/contrib/lite/toco/toco -- \\\n  --input_file=resnet.pb \\\n  --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE \\\n  --output_file=resnet.lite --inference_type=FLOAT \\\n  --input_type=FLOAT --input_arrays=input \\\n  --output_arrays=InferenceTower/linear/output  --input_shapes=1,224,224,3\n2017-11-29 18:45:39.848975: I tensorflow/contrib/lite/toco/import_tensorflow.cc:937] Converting unsupported operation: FIFOQueueV2\n2017-11-29 18:45:39.849072: I tensorflow/contrib/lite/toco/import_tensorflow.cc:937] Converting unsupported operation: QueueDequeueV2\n2017-11-29 18:45:39.849104: I tensorflow/contrib/lite/toco/import_tensorflow.cc:937] Converting unsupported operation: Transpose\n2017-11-29 18:45:39.849122: F tensorflow/contrib/lite/toco/import_tensorflow.cc:286] Check failed: GetStringAttr(node, \"data_format\") == \"NHWC\" (NCHW vs. NHWC)\nAborted (core dumped)\n```\n. python\n def _get_inputs(self):\n        return [InputDesc(self.image_dtype, [None, self.image_shape, self.image_shape, 3], 'input'),\n                InputDesc(tf.int32, [None], 'label')]\nThe \"input\" in the resnet model is very similar as other example models,  How to modify it with \"placeholders\"?  \n. So the solution is: \n1. generate simple inference graph by pure tensorflow ( or tensorpack symbolic)\n2. attach the checkpoint file to freeze the graph\n3. convert the freeze graph to tflite.\nI will try it.\nThanks for your help! \n. 2017-11-30 00:22:17.240715: F tensorflow/contrib/lite/toco/tflite/export.cc:192] Unsupported operator: ReorderAxes\nAborted (core dumped)\nI used the tensorpack's LinearWrap to generate the graph and combined the checkpoint, but I got this error by  toco tool. I did not find this operator \"ReorderAxes\" in the graphdef ops. What is this  \"ReorderAxes\" operator? Is it related with LinearWrap?\nThanks!. If I write a graph with data_format \" NHWC\", but freeze with the checkpoint trained by \"NCHW\". Is it still right? or I have to retrain the checkpoint with data_format \"NHWC\"?. ",
    "maraghuram": "Sorry my bad. They are identical. \nI was doing\nwget https://github.com/openai/atari-py/blob/master/atari_py/atari_roms/ms_pacman.bin instead of wget https://github.com/openai/atari-py/raw/master/atari_py/atari_roms/ms_pacman.bin\nThanks for the quick response.\n. ",
    "zuokai": "I want to change the label shape to (128,), when i load data by tensorpack, how can I change the data shape manually\uff1f. thanks. I just change the batch_size form 1 to 128, everything is ok. but I don\u2018t know why.... thanks. if I set the label = -1 of a picture\uff0c that mean this picture does not calculate the loss. ",
    "fanyun-sun": "Just to inform you that there are still many unfixed errors in the function visualize in train.py.. ",
    "jwnsu": "Forgot to updated tensorpack, just updated (install update) to latest tensorpack 0.8 (from 0.7.1), still got the same error. Also I made change to NHWC (so it could run inference on CPU-only). Will do a fresh repository pull to see whether it's fine.. Yeah, I simply did global replacement and commented out transpose in train.py. Let me revert back to clean repository to see if the issue disappears. Thx for quick comment.. After reverted NHWC changes, it now starting normal training. So the error was from NHWC changes. thx.\nAny plan to add NHWC support in tensorpack? Most of frameworks support test/inference on CPU.\nAnother question is on data augmentation. Some dataset needs to turn off flip (in data.py: \"imgaug.Flip(horiz=True)\"). However, after turn-off flip, there's an error from tensorpack that one of vertical or horiz needs to be on). It will be good for tensorpack to remove this restriction.\nWill close the issue shortly. . After remove that line, training aborted with an error complaining either vertical or horiz flip needs to be on.. confirmed it works fine by simply removing it, by default tensorpack has the flag as false, thx for the info.. Thx. Update to tensorpack 0.8.1 resolved the issue.. Thx for quick response, all files in examples/FasterRCNN were updated/refreshed (have followed this code repository for past a couple of years).. all codes are directly from tensorpack, our changes are limited to dataset configuration only (our own dataset are coco format too, so changes are only to COCO_id_to_category_id and class_names). \nRESNET_NUM_BLOCKS in config.py is intact as latest tensorpack config.py file.. ok, we did a fresh pull from tensorpack, then applies our changes on top, the issue now goes away. . resolved issue by upgrading msgpack.. If I remember it correctly, was running 0.5.8, which produced the error, upgrade to 0.6.0 resolved the issue.. ",
    "yuxwind": "Do you mean in one epoch, 4 GPUs will go through the dataset one time instead of 4 times? \n\nOn Dec 8, 2017, at 23:19, Yuxin Wu notifications@github.com wrote:\nTOTAL_BATCH_SIZE is always 256 no matter how many GPUs you use. So one epoch is always one epoch. The number of GPUs only affects how many samples per GPU.\nThe original resnet paper means the same thing. It may differ a little bit (i.e. 5004 vs 5000) but that's negligible.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub https://github.com/ppwwyyxx/tensorpack/issues/544#issuecomment-350427331, or mute the thread https://github.com/notifications/unsubscribe-auth/AFNYNMBeBa6zSg7k2S7IlTdl3yw6bIyBks5s-iZlgaJpZM4Q76Pw.\n\n\n. When I set the TOTAL_BATCH_SIZE = 192, the progress bar will show something like 4908/26690. What does that mean? Is this the progress of iteration?\nIt seems to make sense that 26690 = 1281167/48 (the size of training is 1281167). \n\nOn Dec 8, 2017, at 23:54, Yuxin Wu notifications@github.com wrote:\nYes.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub https://github.com/ppwwyyxx/tensorpack/issues/544#issuecomment-350428750, or mute the thread https://github.com/notifications/unsubscribe-auth/AFNYNNq9bEYK4kN-oCP9Xw-4RaQZxwwdks5s-i6NgaJpZM4Q76Pw.\n\n\n. The log is like this:\n[1208 22:46:33 @base.py:164] Start Epoch 45 \u2026\n24%|##3       |6316/26690[1:30:25<4:54:41, 1.15it/s]\n\nOn Dec 9, 2017, at 00:18, Yuxin Wu notifications@github.com wrote:\nPlease always paste your observation instead of describing it. I do not understand what you observed.\nAs far as I know you will not see anything like 4908/26690 if changing TOTAL_BATCH_SIZE is the only thing you did.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub https://github.com/ppwwyyxx/tensorpack/issues/544#issuecomment-350429653, or mute the thread https://github.com/notifications/unsubscribe-auth/AFNYNK9kD-e2V9HzjN_81-9xH4yC1npSks5s-jQtgaJpZM4Q76Pw.\n\n\n. I checked my code and found the problem is caused by deleting 'steps_per_epoch=5000' in TrainConfig in imagenet-resnet.py. When this config is added, it shows the correct running progress now. Thanks a lot! . ",
    "jiang1st": "Can I check out an old tensorpack to get the training started?. I tried one built-in example with sync bn, and it works fine. It should be my problem when replacing slim.batch_norm with tensorpack's batch norm. Will check it.. Thank you for your suggestion. I think the operations in each gpu are the same. I used SyncMultiGPUTrainerReplicated from tensorpack for multi-gpu training.\nWhen would the BatchNorm be executed more than once? I tried to print the tensor name of the input at the beginning of BatchNorm, it looks like each BatchNorm is called only once.. Thank you for the suggestions. I think I have put the batch_norm in a for loop ( slim mobilenet v2 ). If a put batch_norm outside the for loop, then it works well.\nWhat I don't understand is that, in the for loop, layers all have different names (including batch_norm). Why would batch_norm hang? Thank you.. Thank you for the suggestions. I think I have found the reason: I only used the first few layers of mobilenet backbone for training. The last several layers are not used for training, but were created by default when loading mobilenet_base(). After removing the batch_norm in these layers, the training works.. Thanks for the update!. ",
    "hhxjzyr": "Thank you very much, I will just try to use model._build_graph . I found a solution by using\n tensorflow/tensorflow/python/tools/freeze_graph.py to get a pure frozen inference graph after training a model from tenorpack (or any other trainining method)\nHere is my usage \npython\nwith tf.Session() as sess:\n    model_path = tf.train.latest_checkpoint(CHECKPOINT_DIR)\n    # get metagraphDef which contains graphDef, clear devices placement in training stage\n    tf.train.import_meta_graph(os.path.join(CHECKPOINT_DIR,'graph-1214-001754.meta'),clear_devices=True)\n    # write GraphDef to a GraphDef protocol buffer file which suitable for freeze_graph.py input \n    tf.train.write_graph(graph_or_graph_def=tf.get_default_graph().as_graph_def(),logdir='train_log',name='original.pbtxt',as_text=True)\nThen use freeze.graph tool in shell command\npython tensorflow/tensorflow/python/tools/freeze_graph.py \\ \n--input_graph /home/yaren/densenet-tensorflow/train_log/original.pbtxt \\ \n--input_checkpoint /home/yaren/densenet-tensorflow/train_log/cifar10-single-fisrt150-second225-max3001214-001743/model-234300 \\\n--output_node_names \"InferenceTower/in_top_k/InTopKV2\" \\\n--output_graph /home/yaren/densenet-tensorflow/train_log/frozen.pb\nIn the above shell command, the output_node_names is very critical, the script based on it to decide which part of Graph is retained. \nThere is a related issue #386\nI haven't tested the frozen graph for inference  If I found some new information I would continue to reply in this issue. Thank you very much! . I have solved this issue if I use allow_soft_placement=True , \n```python\nwith tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\nrestorer.restore(sess=sess,save_path='/home/yaren/densenet-tensorflow/train_log/cifar10-single-fisrt150-second225-max3001214-001743/model-234300')\n```\nAnd I just find a solution and still can't figure out why ....... I just understood and many thanks to you!. ",
    "nameless-Chatoyant": "I have changed get_data to yield one by one, it seems BatchData() still reproduce above 2nd mistake\n```python\nfrom tensorpack import *\nimport numpy as np\nshape = (5,8,8,1)\nclass Data(RNGDataFlow):\n    def size(self):\n        return 10\n    def get_data(self):\n        for i in range(10):\n            yield np.ones(shape)\npython\nds = Data()\nds.reset_state()\nfor i in ds.get_data():\n    print(i.shape)\n``\nit will output(5,8,8,1)` 10 times.\npython\nds = Data()\nds = BatchData(ds, 8, use_list = False)\nds.reset_state()\nfor i in ds.get_data():\n    print(type(i))\n    print(len(i))\n    print(*[j.shape for j in i])\noutput\npython\n<class 'list'>\n5\n(8, 8, 8, 1) (8, 8, 8, 1) (8, 8, 8, 1) (8, 8, 8, 1) (8, 8, 8, 1). It works.Thank you so much for correct my mistakes, and apologize for such a issue. ",
    "wanghuok": "Thanks. it is a great work.. So, such as 'tower-pred-0' is tensorpack add, and there is no control to it?. ",
    "nikoliazekter": "ubuntu windows?. ",
    "Zarnluz": "Okay, thank you for your reply.\nI have to find a workaround then. . ",
    "jbescudie": "Same here. The following code reproduces the error under Ubuntu 16.04, python 2.7, with tensorpack updated from pip today :\nimport tensorpack as tp\nds=tp.DataFromList(range(10))\nds=tp.PrefetchDataZMQ(ds, 6)\ntp.TestDataSpeed(ds).start_test()\n. the code is run in an IPython console (in PyCharm), I'm left with 7 python2.7 processes : the original one and the 6 forks. And 6 forks remains after exiting the main process. I tried in a virtualenv and without.. After further testing, I agree it works as it should: processes are closed after correctly exiting the main process with exit(). It doesn't work with the way pycharm closes the session though, so that is not a tensorpack issue.\nSo I have a feature request regarding this behaviour: add a method to manually close processes created by PrefetchData and PrefetchDataZMQ. It would be really useful during development for those who don't restart the interpreter all the time.\nPS: I had an unexpected behaviour: having a file called zmq.py in my path messed up loading tensorpack module. I can live with that, if you want more info, I'll open another issue.\n. Thanks for your anwser. The ds.__del__() method doesn't close processes when run from command line. I used this code:\nimport tensorpack as tp\nds=tp.DataFromList(range(10))\nds=tp.PrefetchDataZMQ(ds, 6)\ntp.TestDataSpeed(ds).start_test()\nraw_input(\"Enter to clean\")\nds.__del__()\nraw_input(\"Continue\")  # processes are still there \nexit()\nAlthough, if run in debug mode with a breakpoint on the 'Continue' line, the processes are gone (and marked as TERMINATED in ds._procs).\nFor the naming conflict, I meant my file for the first code I submitted in this issue was named zmq.py. Maybe I'm wrong, but I don't think it should mix with another import inside a module.. ",
    "flyers": "I see. I now understand I have to write my own trainer by inheriting the Trainer class or TowerTrainer class. The GAN tutorial is a good example to start with.\nThere is one thing more. For my case, there are several dataflows instead of one dataflow in the GAN example. Let's say we have two different dataflows d1 and d2. When d1 is fed, it is used to train the sub-network. When d2 is fed, it is used to train the master network. The current example trainer does not show how to get the different dataflows and the corresponding optimizing loss in a train loop.\nCould you please provide me some hints on this?. Thanks for your quick reply. I will try to implement this and update this issue for reference when it works.. I have some questions for sharing layers with different data batches. Let's say my model looks like follows:\n```python\nclass MyModel(ModelDescBase):\n    \"\"\" A custom model which consists of two sub-networks on two different tasks with sharing layer\"\"\"\ndef _get_inputs(self):\n    return [InputDesc(tf.float32, (None, 2), 'input'),\n            InputDesc(tf.float32, (None,), 'label'),\n            InputDesc(tf.float32, (None, 2), 'input'),\n            InputDesc(tf.float32, (None,), 'label'),]\n\ndef _build_graph(self, inputs):\n    data1, label1, data2, label2 = inputs\n    # sharing layer definition\n\ndef _get_optimizer(self):\n    lr = tf.get_variable(\n        'learning_rate', initializer=1e-4, trainable=False)\n    opt = tf.train.AdamOptimizer(lr, epsilon=1e-3)\n    return opt\n\nIt takes two different dataflows. So in the build_graph function, the received inputs will be two different data batch. I want to define the graph so that the two networks share common conv or fc layers. It seems that the provide FullyConnected or Conv2D does not support this. How should I achieve this in an elegant way? Does the following code with mixed primitive tf variables work?python\ndef _build_graph(self, inputs):\n    data1, label1, data2, label2 = inputs\n    # sharing layer definition\n    sharing_w = tf.get_variable('w', shape=[2, 10], dtype=tf.float32)\ny1 = tf.nn.relu(tf.matmul(data1, sharing_w))\ny1 = FullyConnected('net1', y1, 1, nl=tf.identity)\n\ny2 = tf.nn.relu(tf.matmul(data2, sharing_w))\ny2 = FullyConnected('net2', y2, 1, nl=tf.identity)\n\ny1_loss = tf.nn.l2_loss(y1 - label1, name='net1_loss')\ny2_loss = tf.nn.l2_loss(y2 - label2, name='net2_loss')\n\n. I see. That's pretty elegant.. I wrote the following simple example where the the trainer is optimizing two sub-networks using two different dataflows. The two dataflows are generated by a separate thread. However, I found that in the training process, the program sometimes get stuck. It seems that the dataflows are blocked. Any idea on why that happens?python\nimport tensorflow as tf\nfrom tensorpack.graph_builder.model_desc import ModelDescBase, InputDesc\nfrom tensorpack import FullyConnected, BatchData, DataFromQueue, StartProcOrThread, FeedInput, ModelSaver\nfrom tensorpack.train.base import Trainer\nfrom tensorpack.train.tower import TowerTrainer\nfrom tensorpack.tfutils import optimizer, summary\nfrom tensorpack.tfutils.tower import TowerContext, TowerFuncWrapper, get_current_tower_context\nfrom tensorpack.utils.argtools import memoized\nfrom tensorpack.utils import logger\nimport threading\nfrom six.moves import queue\nimport numpy as np\nclass MyModel(ModelDescBase):\n    \"\"\" A custom model which consists of two sub-networks\"\"\"\ndef _get_inputs(self):\n    l = self.get_inputs_desc_list()\n    res = []\n    for x in l:\n        res += x\n    return res\n\ndef get_inputs_desc_list(self):\n    ds1 = [InputDesc(tf.float32, (None, 2), 'input'),\n           InputDesc(tf.float32, (None,), 'label')]\n    ds2 = [InputDesc(tf.float32, (None, 2), 'input'),\n           InputDesc(tf.float32, (None,), 'label')]\n    return [ds1, ds2]\n\ndef _build_graph(self, inputs):\n    data1, label1, data2, label2 = inputs\n    with tf.variable_scope(\"share\") as share_scope:\n        y1 = FullyConnected('fc0', data1, 10, nl=tf.nn.relu)\n\n    with tf.variable_scope('net1'):\n        y1 = FullyConnected('fc_out', y1, 1, nl=tf.identity)\n\n    with tf.variable_scope('net2'):\n        with tf.variable_scope(share_scope, reuse=True):\n            y2 = FullyConnected('fc0', data2, 10, nl=tf.nn.relu)\n        y2 = FullyConnected('fc_out', y2, 1, nl=tf.identity)\n\n    self.y1_loss = tf.nn.l2_loss(y1 - label1)\n    self.y1_loss = tf.truediv(self.y1_loss, tf.cast(\n        tf.shape(label1)[0], tf.float32), name='net1_loss')\n    self.y2_loss = tf.nn.l2_loss(y2 - label2)\n    self.y2_loss = tf.truediv(self.y2_loss, tf.cast(\n        tf.shape(label2)[0], tf.float32), name='net2_loss')\n\n    self.vars1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'share') + \\\n        tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'net1')\n    self.vars2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'share') + \\\n        tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'net2')\n\n    summary.add_moving_summary(self.y1_loss, self.y2_loss)\n\n@memoized\ndef get_optimizer(self):\n    lr = tf.get_variable(\n        'learning_rate', initializer=1e-4, trainable=False)\n    opt = tf.train.AdamOptimizer(lr, epsilon=1e-3)\n    return opt\n\nclass MyTrainer(TowerTrainer):\n    \"\"\"\n    A custom trainer which optimize several costs from different dataflows simutaneously\n    \"\"\"\ndef __init__(self, inputs, model):\n    \"\"\"\n    inputs ([InputSource]): a list of several InputSources\n    model (ModelDescBase): a model\n    \"\"\"\n    super(MyTrainer, self).__init__()\n    inputs_desc = model.get_inputs_desc()\n    inputs_desc_list = model.get_inputs_desc_list()\n\n    # setup input callbacks for each InputSource\n    for dataflow, desc in zip(inputs, inputs_desc_list):\n        cbs = dataflow.setup(desc)\n        self.register_callback(cbs)\n\n    # build the graph\n    self.tower_func = TowerFuncWrapper(model.build_graph, inputs_desc)\n    tensors = []\n    for dataflow in inputs:\n        tensors += dataflow.get_input_tensors()\n    with TowerContext('', is_training=True):\n        self.tower_func(*tensors)\n\n    opt = model.get_optimizer()\n    with tf.name_scope('optimize'):\n        self.opt_y1 = opt.minimize(\n            model.y1_loss, var_list=model.vars1, name='op1')\n        self.opt_y2 = opt.minimize(\n            model.y2_loss, var_list=model.vars2, name='op2')\n\ndef run_step(self):\n    self.hooked_sess.run(self.opt_y1)\n    self.hooked_sess.run(self.opt_y2)\n\nclass DataThread(threading.Thread):\n    \"\"\"\n    A thread to generate data for different dataflows\n    \"\"\"\ndef __init__(self, batch_size):\n    super(DataThread, self).__init__()\n    self.queue1 = queue.Queue(maxsize=batch_size * 8 * 2)\n    self.queue2 = queue.Queue(maxsize=batch_size * 8 * 2)\n\n    self.daemon = True\n    self.name = 'DataThread'\n\ndef run(self):\n    while True:\n        if np.random.rand() < 0.5:\n            x = np.random.rand(2)\n            y = np.sum(x * x)\n            self.queue1.put([x, y])\n        else:\n            x = np.random.rand(2)\n            y = np.sum(x * x) * 0.001\n            self.queue2.put([x, y])\n\nif name == 'main':\n    # logger.set_logger_dir('/home/sliay/models/trainer')\n    BATCH_SIZE = 128\n    data_gen = DataThread(BATCH_SIZE)\n    data1 = BatchData(DataFromQueue(data_gen.queue1), BATCH_SIZE)\n    data2 = BatchData(DataFromQueue(data_gen.queue2), BATCH_SIZE)\n    M = MyModel()\n    MyTrainer([FeedInput(data1), FeedInput(data2)], M).train_with_defaults(\n        callbacks=[StartProcOrThread(data_gen),\n                   ],\n        steps_per_epoch=100,\n        max_epoch=100,\n    )\n. 1. Thanks for your explanation. So if I have a master thread which maintains several queues, I can increase the maxsize to reduce the probability it is blocked.\n2. As for the data feeding process, my current implementation will waste some data which will not be used for training. Is my understanding correct? That's not what I want. Can I avoid this by using QueueInput instead of FeedInput?\n3. I did not get how to use different collection name for this. Will the following do or I also need to modify the other part?python\nsummary.add_moving_summary(self.y1_loss, collection='loss1')\nsummary.add_moving_summary(self.y2_loss, collection='loss2')\n. I also have another question in case of creating predictors in `TowerTrainer`. In my case I want to create different predictors for evaluating `y1` and `y2` separately. Do I need to create several different `tower_func` or I just need to call `SimplePredictBuilder` with different names?. I have updated the code to address the above three issues. It seems that they are all solved. However, when I tried to create separate online predictors in the trainer, some error happens. The following is my code. It seems that the tower function is having issues finding the tensor in the graph.python\nimport tensorflow as tf\nfrom tensorpack import ModelDescBase, InputDesc, FullyConnected, \\\n    BatchData, DataFromQueue, StartProcOrThread, QueueInput, ModelSaver, PeriodicTrigger\nfrom tensorpack.callbacks import Callback\nfrom tensorpack.train.base import Trainer\nfrom tensorpack.train.tower import TowerTrainer\nfrom tensorpack.tfutils import optimizer, summary\nfrom tensorpack.tfutils.tower import TowerContext, TowerFuncWrapper, get_current_tower_context\nfrom tensorpack.utils.argtools import memoized\nfrom tensorpack.utils import logger\nimport threading\nfrom six.moves import queue\nimport numpy as np\nclass MyModel(ModelDescBase):\n    \"\"\" A custom model which consists of two sub-networks\"\"\"\ndef _get_inputs(self):\n    l = self.get_inputs_desc_list()\n    res = []\n    for x in l:\n        res += x\n    return res\n\ndef get_inputs_desc_list(self):\n    ds1 = [InputDesc(tf.float32, (None, 2), 'input'),\n           InputDesc(tf.float32, (None,), 'label')]\n    ds2 = [InputDesc(tf.float32, (None, 2), 'input'),\n           InputDesc(tf.float32, (None,), 'label')]\n    return [ds1, ds2]\n\ndef _build_graph(self, inputs):\n    data1, label1, data2, label2 = inputs\n    with tf.variable_scope(\"share\") as share_scope:\n        y1 = FullyConnected('fc0', data1, 10, nl=tf.nn.relu)\n\n    with tf.variable_scope('net1'):\n        y1 = FullyConnected('fc_out', y1, 1, nl=tf.identity)\n\n    with tf.variable_scope('net2'):\n        with tf.variable_scope(share_scope, reuse=True):\n            y2 = FullyConnected('fc0', data2, 10, nl=tf.nn.relu)\n        y2 = FullyConnected('fc_out', y2, 1, nl=tf.identity)\n\n    self.y1_loss = tf.nn.l2_loss(y1 - label1)\n    self.y1_loss = tf.truediv(self.y1_loss, tf.cast(\n        tf.shape(label1)[0], tf.float32), name='net1_loss')\n    self.y2_loss = tf.nn.l2_loss(y2 - label2)\n    self.y2_loss = tf.truediv(self.y2_loss, tf.cast(\n        tf.shape(label2)[0], tf.float32), name='net2_loss')\n\n    self.vars1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'share') + \\\n        tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'net1')\n    self.vars2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'share') + \\\n        tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'net2')\n\n    summary.add_moving_summary(self.y1_loss, collection='loss1')\n    summary.add_moving_summary(self.y2_loss, collection='loss2')\n\n@memoized\ndef get_optimizer(self):\n    lr = tf.get_variable(\n        'learning_rate', initializer=1e-4, trainable=False)\n    opt = tf.train.AdamOptimizer(lr, epsilon=1e-3)\n    return opt\n\nclass MyTrainer(TowerTrainer):\n    \"\"\"\n    A custom trainer which optimize several costs from different dataflows simutaneously\n    \"\"\"\ndef __init__(self, inputs, model):\n    \"\"\"\n    inputs ([InputSource]): a list of several InputSources\n    model (ModelDescBase): a model\n    \"\"\"\n    super(MyTrainer, self).__init__()\n    inputs_desc = model.get_inputs_desc()\n    inputs_desc_list = model.get_inputs_desc_list()\n\n    # setup input callbacks for each InputSource\n    for dataflow, desc in zip(inputs, inputs_desc_list):\n        cbs = dataflow.setup(desc)\n        self.register_callback(cbs)\n\n    # build the graph\n    self.tower_func = TowerFuncWrapper(model.build_graph, inputs_desc)\n    tensors = []\n    for dataflow in inputs:\n        tensors += dataflow.get_input_tensors()\n    with TowerContext('', is_training=True):\n        self.tower_func(*tensors)\n\n    opt = model.get_optimizer()\n    with tf.name_scope('optimize'):\n        self.opt_y1 = opt.minimize(\n            model.y1_loss, var_list=model.vars1, name='op1')\n        self.opt_y2 = opt.minimize(\n            model.y2_loss, var_list=model.vars2, name='op2')\n\ndef run_step(self):\n    self.hooked_sess.run([self.opt_y1, tf.get_collection('loss1')])\n    self.hooked_sess.run([self.opt_y2, tf.get_collection('loss2')])\n\ndef get_predictor1(self):\n    return self.get_predictor(['input'], ['net1/fc_out'])\n\ndef get_predictor2(self):\n    return self.get_predictor(['input'], ['net2/fc_out'])\n\nclass DataThread1(threading.Thread):\n    def init(self, batch_size):\n        super(DataThread1, self).init()\n        self.queue = queue.Queue(maxsize=batch_size * 8 * 2)\n        self.daemon = True\n        self.name = 'DataThread1'\ndef run(self):\n    while True:\n        x = np.random.rand(2)\n        y = np.sum(x * x)\n        self.queue.put([x, y])\n\nclass DataThread2(threading.Thread):\n    def init(self, batch_size):\n        super(DataThread2, self).init()\n        self.queue = queue.Queue(maxsize=batch_size * 8 * 2)\n        self.daemon = True\n        self.name = 'DataThread2'\ndef run(self):\n    while True:\n        x = np.random.rand(2)\n        y = np.sum(x * x) * 0.001\n        self.queue.put([x, y])\n\nclass Evaluator(Callback):\n    def _setup_graph(self):\n        self.pred_func = self.trainer.get_predictor1()\ndef _trigger(self):\n    x = np.array([1, 2])\n    pred = self.pred_func(x[None])[0][0]\n    self.trainer.monitors.put_scalar('pred', pred)\n\nif name == 'main':\n    logger.set_logger_dir('/home/sliay/models/trainer')\n    BATCH_SIZE = 128\n    data_gen1 = DataThread1(BATCH_SIZE)\n    data_gen2 = DataThread2(BATCH_SIZE)\n    data1 = BatchData(DataFromQueue(data_gen1.queue), BATCH_SIZE)\n    data2 = BatchData(DataFromQueue(data_gen2.queue), BATCH_SIZE)\n    M = MyModel()\n    MyTrainer([QueueInput(data1), QueueInput(data2)], M).train_with_defaults(\n        callbacks=[\n            StartProcOrThread(data_gen1),\n            StartProcOrThread(data_gen2),\n            PeriodicTrigger(Evaluator(),\n                every_k_epochs=1),\n        ],\n        steps_per_epoch=100,\n        max_epoch=100,\n    )\n. Thanks very much for your help. I will paste my final runnable code here in case anyone else want to do similar thing such as multitask training.python\nimport tensorflow as tf\nfrom tensorpack import ModelDescBase, InputDesc, FullyConnected, \\\n    BatchData, DataFromQueue, StartProcOrThread, QueueInput, ModelSaver, PeriodicTrigger\nfrom tensorpack.callbacks import Callback\nfrom tensorpack.train.tower import TowerTrainer\nfrom tensorpack.tfutils import optimizer, summary\nfrom tensorpack.tfutils.tower import TowerContext, TowerFuncWrapper, get_current_tower_context\nfrom tensorpack.utils.argtools import memoized\nfrom tensorpack.utils import logger\nimport threading\nfrom six.moves import queue\nimport numpy as np\nclass MyModel(ModelDescBase):\n    \"\"\" A custom model which consists of two sub-networks\"\"\"\ndef _get_inputs(self):\n    l = self.get_inputs_desc_list()\n    res = []\n    for x in l:\n        res += x\n    return res\n\ndef get_inputs_desc_list(self):\n    ds1 = [InputDesc(tf.float32, (None, 2), 'input1'),\n           InputDesc(tf.float32, (None,), 'label1')]\n    ds2 = [InputDesc(tf.float32, (None, 2), 'input2'),\n           InputDesc(tf.float32, (None,), 'label2')]\n    return [ds1, ds2]\n\ndef _build_graph(self, inputs):\n    data1, label1, data2, label2 = inputs\n    with tf.variable_scope(\"share\") as share_scope:\n        y1 = FullyConnected('fc0', data1, 10, nl=tf.nn.relu)\n\n    with tf.variable_scope('net1'):\n        y1 = FullyConnected('fc_out', y1, 1, nl=tf.identity)\n\n    with tf.variable_scope('net2'):\n        with tf.variable_scope(share_scope, reuse=True):\n            y2 = FullyConnected('fc0', data2, 10, nl=tf.nn.relu)\n        y2 = FullyConnected('fc_out', y2, 1, nl=tf.identity)\n\n    self.y1_loss = tf.nn.l2_loss(y1 - label1)\n    self.y1_loss = tf.truediv(self.y1_loss, tf.cast(\n        tf.shape(label1)[0], tf.float32), name='net1_loss')\n    self.y2_loss = tf.nn.l2_loss(y2 - label2)\n    self.y2_loss = tf.truediv(self.y2_loss, tf.cast(\n        tf.shape(label2)[0], tf.float32), name='net2_loss')\n\n    self.vars1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'share') + \\\n        tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'net1')\n    self.vars2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'share') + \\\n        tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'net2')\n\n    summary.add_moving_summary(self.y1_loss, collection='loss1')\n    summary.add_moving_summary(self.y2_loss, collection='loss2')\n\n@memoized\ndef get_optimizer(self):\n    lr = tf.get_variable(\n        'learning_rate', initializer=1e-4, trainable=False)\n    opt = tf.train.AdamOptimizer(lr, epsilon=1e-3)\n    return opt\n\nclass MyTrainer(TowerTrainer):\n    \"\"\"\n    A custom trainer which optimize several costs from different dataflows simutaneously\n    \"\"\"\ndef __init__(self, inputs, model):\n    \"\"\"\n    inputs ([InputSource]): a list of several InputSources\n    model (ModelDescBase): a model\n    \"\"\"\n    super(MyTrainer, self).__init__()\n    inputs_desc = model.get_inputs_desc()\n    inputs_desc_list = model.get_inputs_desc_list()\n\n    # setup input callbacks for each InputSource\n    for dataflow, desc in zip(inputs, inputs_desc_list):\n        cbs = dataflow.setup(desc)\n        self.register_callback(cbs)\n\n    # build the graph\n    self.tower_func = TowerFuncWrapper(model.build_graph, inputs_desc)\n    tensors = []\n    for dataflow in inputs:\n        tensors += dataflow.get_input_tensors()\n    with TowerContext('', is_training=True):\n        self.tower_func(*tensors)\n\n    opt = model.get_optimizer()\n    with tf.name_scope('optimize'):\n        self.opt_y1 = opt.minimize(\n            model.y1_loss, var_list=model.vars1, name='op1')\n        self.opt_y2 = opt.minimize(\n            model.y2_loss, var_list=model.vars2, name='op2')\n\ndef run_step(self):\n    self.hooked_sess.run([self.opt_y1, tf.get_collection('loss1')])\n    self.hooked_sess.run([self.opt_y2, tf.get_collection('loss2')])\n\ndef get_predictor1(self):\n    return self.get_predictor(['input1'], ['net1/fc_out/output'])\n\ndef get_predictor2(self):\n    return self.get_predictor(['input2'], ['net2/fc_out/output'])\n\nclass DataThread1(threading.Thread):\n    def init(self, batch_size):\n        super(DataThread1, self).init()\n        self.queue = queue.Queue(maxsize=batch_size * 8 * 2)\n        self.daemon = True\n        self.name = 'DataThread1'\ndef run(self):\n    while True:\n        x = np.random.rand(2)\n        y = np.sum(x)\n        self.queue.put([x, y])\n\nclass DataThread2(threading.Thread):\n    def init(self, batch_size):\n        super(DataThread2, self).init()\n        self.queue = queue.Queue(maxsize=batch_size * 8 * 2)\n        self.daemon = True\n        self.name = 'DataThread2'\ndef run(self):\n    while True:\n        x = np.random.rand(2)\n        y = np.sum(x) * 0.01\n        self.queue.put([x, y])\n\nclass Evaluator(Callback):\n    def _setup_graph(self):\n        self.pred_func1 = self.trainer.get_predictor1()\n        self.pred_func2 = self.trainer.get_predictor2()\ndef _trigger(self):\n    x = np.array([1, 2])\n    pred1 = self.pred_func1(x[None])[0][0]\n    pred2 = self.pred_func2(x[None])[0][0]\n    self.trainer.monitors.put_scalar('pred1', np.asscalar(pred1))\n    self.trainer.monitors.put_scalar('pred2', np.asscalar(pred2))\n\nif name == 'main':\n    logger.set_logger_dir('/home/sliay/models/trainer')\n    BATCH_SIZE = 128\n    data_gen1 = DataThread1(BATCH_SIZE)\n    data_gen2 = DataThread2(BATCH_SIZE)\n    data1 = BatchData(DataFromQueue(data_gen1.queue), BATCH_SIZE)\n    data2 = BatchData(DataFromQueue(data_gen2.queue), BATCH_SIZE)\n    M = MyModel()\n    MyTrainer([QueueInput(data1), QueueInput(data2)], M).train_with_defaults(\n        callbacks=[\n            StartProcOrThread(data_gen1),\n            StartProcOrThread(data_gen2),\n            PeriodicTrigger(Evaluator(),\n                every_k_epochs=1),\n        ],\n        steps_per_epoch=100,\n        max_epoch=100,\n    )\n```. ",
    "sgzqc": "Hi all, i have run the example code(from flyers) above, the result is not normal just because the QueueInput/queue_size has data while the QueueInput_1/queue_size is almost empty, the log is as below:\n[0717 18:49:04 @base.py:272] Epoch 90 (global_step 18000) finished, time:1.47 seconds.\n[0717 18:49:05 @monitor.py:428] QueueInput/queue_size: 34.267\n[0717 18:49:05 @monitor.py:428] QueueInput_1/queue_size: 1.5595e-38\n[0717 18:49:05 @monitor.py:428] net1_loss: 10.731\n[0717 18:49:05 @monitor.py:428] net2_loss: 0.004886\n[0717 18:49:05 @monitor.py:428] pred1: 0.94957\n[0717 18:49:05 @monitor.py:428] pred2: 0.015798\n[0717 18:49:05 @base.py:262] Start Epoch 91 ...\n100%|##########|100/100[00:01<00:00,63.74it/s]\n[0717 18:49:06 @base.py:272] Epoch 91 (global_step 18200) finished, time:1.57 seconds.\n[0717 18:49:06 @monitor.py:428] QueueInput/queue_size: 35.001\n[0717 18:49:06 @monitor.py:428] QueueInput_1/queue_size: 1.5303e-38\n[0717 18:49:06 @monitor.py:428] net1_loss: 10.888\n[0717 18:49:06 @monitor.py:428] net2_loss: 0.0055581\n[0717 18:49:06 @monitor.py:428] pred1: 0.95158\n[0717 18:49:06 @monitor.py:428] pred2: 0.0138\n[0717 18:49:06 @base.py:262] Start Epoch 92 ...\n100%|##########|100/100[00:01<00:00,68.13it/s]\n[0717 18:49:08 @base.py:272] Epoch 92 (global_step 18400) finished, time:1.47 seconds.\n[0717 18:49:08 @monitor.py:428] QueueInput/queue_size: 34.065\n[0717 18:49:08 @monitor.py:428] QueueInput_1/queue_size: 1.4937e-38\n[0717 18:49:08 @monitor.py:428] net1_loss: 10.866\n[0717 18:49:08 @monitor.py:428] net2_loss: 0.0048307\n[0717 18:49:08 @monitor.py:428] pred1: 0.92544\n[0717 18:49:08 @monitor.py:428] pred2: 0.015397\n[0717 18:49:08 @base.py:262] Start Epoch 93 ...\n100%|##########|100/100[00:01<00:00,63.75it/s]\n[0717 18:49:09 @base.py:272] Epoch 93 (global_step 18600) finished, time:1.57 seconds.\n[0717 18:49:09 @monitor.py:428] QueueInput/queue_size: 38.195\n[0717 18:49:09 @monitor.py:428] QueueInput_1/queue_size: 1.42e-38\n[0717 18:49:09 @monitor.py:428] net1_loss: 10.45\n[0717 18:49:09 @monitor.py:428] net2_loss: 0.0047563\n[0717 18:49:09 @monitor.py:428] pred1: 0.94389\n[0717 18:49:09 @monitor.py:428] pred2: 0.014586\n[0717 18:49:09 @base.py:262] Start Epoch 94 ...\n  0%|          |0/100[00:00<?,?it/s][0717 18:49:11 @base.py:272] Epoch 94 (global_step 18800) finished, time:1.57 seconds.\n100%|##########|100/100[00:01<00:00,63.74it/s]\n[0717 18:49:11 @monitor.py:428] QueueInput/queue_size: 36.5\n[0717 18:49:11 @monitor.py:428] QueueInput_1/queue_size: 1.5501e-38\n[0717 18:49:11 @monitor.py:428] net1_loss: 10.427\n[0717 18:49:11 @monitor.py:428] net2_loss: 0.004415\n[0717 18:49:11 @monitor.py:428] pred1: 0.95424\n[0717 18:49:11 @monitor.py:428] pred2: 0.014196\n[0717 18:49:11 @base.py:262] Start Epoch 95 ...\n  0%|          |0/100[00:00<?,?it/s][0717 18:49:13 @base.py:272] Epoch 95 (global_step 19000) finished, time:1.47 seconds.\n100%|##########|100/100[00:01<00:00,68.15it/s]\n[0717 18:49:13 @monitor.py:428] QueueInput/queue_size: 39.004\n[0717 18:49:13 @monitor.py:428] QueueInput_1/queue_size: 1.2729e-38\n[0717 18:49:13 @monitor.py:428] net1_loss: 10.679\n[0717 18:49:13 @monitor.py:428] net2_loss: 0.004388\n[0717 18:49:13 @monitor.py:428] pred1: 0.94648\n[0717 18:49:13 @monitor.py:428] pred2: 0.014553\n[0717 18:49:13 @base.py:262] Start Epoch 96 ...\n  0%|          |0/100[00:00<?,?it/s][0717 18:49:14 @base.py:272] Epoch 96 (global_step 19200) finished, time:1.47 seconds.\n100%|##########|100/100[00:01<00:00,68.10it/s]\n[0717 18:49:14 @monitor.py:428] QueueInput/queue_size: 43.999\n[0717 18:49:14 @monitor.py:428] QueueInput_1/queue_size: 1.5625e-38\n[0717 18:49:14 @monitor.py:428] net1_loss: 10.688\n[0717 18:49:14 @monitor.py:428] net2_loss: 0.0045032\n[0717 18:49:14 @monitor.py:428] pred1: 0.95173\n[0717 18:49:14 @monitor.py:428] pred2: 0.013164\n[0717 18:49:14 @base.py:262] Start Epoch 97 ...\n  0%|          |0/100[00:00<?,?it/s][0717 18:49:16 @base.py:272] Epoch 97 (global_step 19400) finished, time:1.47 seconds.\n100%|##########|100/100[00:01<00:00,68.08it/s]\n[0717 18:49:16 @monitor.py:428] QueueInput/queue_size: 46.185\n[0717 18:49:16 @monitor.py:428] QueueInput_1/queue_size: 2.1594e-38\n[0717 18:49:16 @monitor.py:428] net1_loss: 10.591\n[0717 18:49:16 @monitor.py:428] net2_loss: 0.0041702\n[0717 18:49:16 @monitor.py:428] pred1: 0.93017\n[0717 18:49:16 @monitor.py:428] pred2: 0.012632\n[0717 18:49:16 @base.py:262] Start Epoch 98 ...\n  0%|          |0/100[00:00<?,?it/s][0717 18:49:17 @base.py:272] Epoch 98 (global_step 19600) finished, time:1.52 seconds.\n100%|##########|100/100[00:01<00:00,65.84it/s]\n[0717 18:49:17 @monitor.py:428] QueueInput/queue_size: 46.516\n[0717 18:49:17 @monitor.py:428] QueueInput_1/queue_size: 1.6361e-38\n[0717 18:49:17 @monitor.py:428] net1_loss: 10.789\n[0717 18:49:17 @monitor.py:428] net2_loss: 0.0041644\n[0717 18:49:17 @monitor.py:428] pred1: 0.95202\n[0717 18:49:17 @monitor.py:428] pred2: 0.01548\n[0717 18:49:17 @base.py:262] Start Epoch 99 ...\n  0%|          |0/100[00:00<?,?it/s][0717 18:49:19 @base.py:272] Epoch 99 (global_step 19800) finished, time:1.52 seconds.\n100%|##########|100/100[00:01<00:00,65.88it/s]\n[0717 18:49:19 @monitor.py:428] QueueInput/queue_size: 48.972\n[0717 18:49:19 @monitor.py:428] QueueInput_1/queue_size: 1.5584e-38\n[0717 18:49:19 @monitor.py:428] net1_loss: 11.04\n[0717 18:49:19 @monitor.py:428] net2_loss: 0.0040718\n[0717 18:49:19 @monitor.py:428] pred1: 0.96595\n[0717 18:49:19 @monitor.py:428] pred2: 0.015411\n[0717 18:49:19 @base.py:262] Start Epoch 100 ...\n  0%|          |0/100[00:00<?,?it/s][0717 18:49:21 @base.py:272] Epoch 100 (global_step 20000) finished, time:1.47 seconds.\n100%|##########|100/100[00:01<00:00,68.09it/s]\n[0717 18:49:21 @monitor.py:428] QueueInput/queue_size: 50\n[0717 18:49:21 @monitor.py:428] QueueInput_1/queue_size: 1.5499e-38\n[0717 18:49:21 @monitor.py:428] net1_loss: 10.796\n[0717 18:49:21 @monitor.py:428] net2_loss: 0.0042789\n[0717 18:49:21 @monitor.py:428] pred1: 0.96815\n[0717 18:49:21 @monitor.py:428] pred2: 0.015265\n[0717 18:49:21 @base.py:276] Training has finished!\n[0717 18:49:21 @input_source.py:149] EnqueueThread QueueInput/input_queue Exited.\n2018-07-17 18:49:21.051904: W tensorflow/core/kernels/queue_base.cc:295] _1_QueueInput/input_queue: Skipping cancelled enqueue attempt with queue not closed\nAnyone can give me some suggest ? @flyers @ppwwyyxx . ",
    "hao522": "I set 'TEST_POST_NMS_TOPK' option in config.py to 600, it worked in this way, thx. ",
    "RadZaeem": "That was fast.\nThank you!\nBy the way, any recommendation or guide on how to use tfdbg or other techniques to 'trace' value of gradients flow? I use gradient_map_override, modified gradients, and want to make sure things go correctly.\nCurrently I am porting code from bit-rnn (quantized RNN, He et al)to tensorpack. Thanks again. ",
    "youkaichao": "thanks! problem solved!\nBut i still think that since reset_state can be called only once, it's better if it's called automatically when constructed ^_^. ",
    "yselivonchyk": "Moving BnRelu into the residual branch (the correct way) leads to diverging training. figuring out why. @ppwwyyxx @dongzhuoyao \nRegarding your comments about design of the residual block. I ran the model by @kuangliy and figured out next correct way of applying BnRelu on residual branch: apply BnRelu only before convolution in identity branch. Differences: my existing implementation always applies BnRelu.\nThe reference code expressing the ides from https://github.com/kuangliu/pytorch-cifar:\ndef forward(self, x):\n        out = F.relu(self.bn1(x))\n        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n        ...\nFor example, for PreActResNet18 BnRelu before residual branch would be applied exactly 3 times: for the first block of the second, third and forth module (whenever filter depth changes).\nPreActResNet18_Cifar correct schema:\ninput---c------id--------id----br-----c-------id-----br-----c-------id-----br-----c-------id-----mp-fc10-sm\n................\\brcbrc/ \\brcbrc/....\\cbrc/..\\brcbrc/.....\\cbrc/..\\brcbrc/.....\\cbrc/  \\brcbrc/ \nI will add this changes to the pull request later.\nIt might also beg some changes in other Cirar+ResNet implementations as mentioned by @dongzhuoyao  but I am not that familiar with the code there to justify the changes.\n. All done.\nNumbers:\nwd=0.0001 (as in mixup paper): 6.0% no mixup 4.3% with mixup\nwd=0.0005 (as in preactresnet paper): 5.7% no mixup 4.1% with mixup\n. Figured, that i can achieve what i want with tp.PeriodicTrigger(RunOp(...)). . ",
    "bilipa": "thanks. Thanks, I know. ",
    "csnemes2": "Thanks for the quick and clear answer, I am already downloading the 1.4 :)\nAny chance to update the readme? \nI have just checked it and no hints, just something tf>=1.2 which is true but not very tight :). ",
    "liuhu-bigeye": "One question, note that TF r1.5 support cudnn 7.0 which highlighted with a faster group convolution implementation, will tensorpack add group conv based on cudnn 7.0 instead?. ",
    "kelvict": "I define my get data as below.\ndef get_data(self):\n        self.rng.shuffle(self.filenames)\n        for idx, fname in enumerate(self.filenames):\n            with open(fname) as f:\n                for l_idx, l in enumerate(f):\n                    yield [os.getpid(), fname.split('.')[-1], l_idx] + self.extract_features_from_a_line(l)\nAnd use prefetch as below \ndf = MDPRankDataFlow()\n    #df = BatchData(df, 3)\n    #cpu_cnt = cpu_count()\n    cpu_cnt = 4\n    df = PrefetchDataZMQ(df, cpu_cnt, 50)\n    df.reset_state()\nBut I still get the same example from different process. I just want to get a example one time. . ",
    "pvoigtlaender": "It indeed only rarely happened, I was processing a lot of images and it only crashed after around 200.\nPlease let me know, if you get further insights on this.\nFor now I implemented the following workaround (I hope it won't mess up the masks too much)\n# rounding errors could happen here, because masks were not originally computed for this shape.\n    # but it's hard to do better, because the network does not know the \"original\" scale\n    mask = (cv2.resize(mask, (w, h)) > 0.5).astype('uint8')\n    ret = np.zeros(shape, dtype='uint8')\n    if x1 >= ret.shape[1]:\n        x1_old = x1\n        x1 = ret.shape[1] - 1\n        print(\"warning, size mismatch (x), changing x1 from {} to {}\".format(x1_old, x1))\n        x0 = x1 + 1 - w\n    if y1 >= ret.shape[0]:\n        y1_old = y1\n        y1 = ret.shape[0] - 1\n        print(\"warning, size mismatch (y), chaing y1 from {} to {}\".format(y1_old, y1))\n        y0 = y1 + 1 - h\n    ret[y0:y1 + 1, x0:x1 + 1] = mask. The code changes should really not be related, it is mainly just logic on which images to load and where to store the results.\nWhat I did change, though is the config, I set\nTEST_PRE_NMS_TOPK = 15000\nTEST_POST_NMS_TOPK = 1000\nFASTRCNN_NMS_THRESH = 0.8\nRESULT_SCORE_THRESH = 0.0\nRESULTS_PER_IM = 1000\nI'm currently testing on images from KITTI. Thanks for your patch! I can confirm, that the error does not occur anymore!\n  . ",
    "junhaenglee": "Could yo elaborate a little bit more?\nCan I add the code somewhere under _build_graph() as follows?\nclass Model(ModelDesc):\n    def _build_graph(self, inputs):\n       ...\n      callbacks=[\nRunOp(lambda: tf.add_check_numerics_ops(), run_before=False, run_as_trigger=False, run_step=True),\n ...]`.\n       .... Thank you very much.. ",
    "sirotenko": "The workaround is to run this before running tensorpack:\npip install 'opencv-python==3.3.0.9'\nBased on this https://github.com/skvark/opencv-python/issues/44. ",
    "tkuanlun350": "Turns out it is not related to the cudnn bugs. Thanks for your help. The rest of the problem should better ask on stackoverflow.. @qinhuan Yes, but performance dropped to 0.39 using 4 gpus. There maybe some bugs in my implementation :(. @ppwwyyxx  The solution to tf.image.imresize is awesome. Thanks! great job.\nI still have an question about fpn's  rpn loss, if you can kindly explain to me will be very appreciated.\nMay I ask you why rpn loss is computed level-wise and what is the meaning of placeholder in rpn loss?. I asked because I just concat all the boxes and anchors across every level and compute the loss for rpn without placeholder and (1./ RPN_BATCH_PER_IM) term. \nI am wondering whether this will be the potential bugs in my previous implementation.\nThanks for clarification.. ",
    "qinhuan": "@tkuanlun350  Hi, Have you finished the code of FPN? Could you share the code? Thanks.. ",
    "amcinto": "Thanks for getting back so quickly! What was it recently changed to?. Okay thanks! I\u2019ll give that a shot and let you know what happens. Have a great day!. Just wanted to let you know that it does in fact work! Thanks. ",
    "HiKapok": "Hi, here is one TensorFlow SE-ResNet with pre-trained weights converted from Caffe. You can try this if you are interested in.. ",
    "giampierosalvi": "update: running\nconda install tensorflow-gpu\ninstead of the above pip install, seems to work.. Hi,\nthank you for the fast answer and commit. The script now goes past the previous error, but I get the following problem (very long log, I am afraid). I know it's hard to debug without the data, but if you see anything that can be easily fixed I would appreciate any hints.\nThank you!\n[0119 17:45:21 @logger.py:74] Argv: ./train-timit.py --train train.mdb --test test.mdb --stat stats.data\n[0119 17:45:21 @format.py:86] Found 4620 entries in train.mdb\n[0119 17:45:21 @format.py:86] Found 1680 entries in test.mdb\n[0119 17:45:21 @param.py:189] Use train_log/train-timit/hyper.txt to set hyperparam: 'learning_rate'.\n[0119 17:45:21 @inference_runner.py:80] InferenceRunner will eval 26 iterations\n[0119 17:45:21 @input_source.py:193] Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...\n[0119 17:45:22 @registry.py:122] fc input: [None, 128]\n[0119 17:45:22 @registry.py:130] fc output: [None, 62]\n[0119 17:45:22 @model_utils.py:49] Model Parameters: \nname                                          shape          dim\n\nrnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0  [167, 512]   85504\nrnn/multi_rnn_cell/cell_0/lstm_cell/bias:0    [512]          512\nrnn/multi_rnn_cell/cell_1/lstm_cell/kernel:0  [256, 512]  131072\nrnn/multi_rnn_cell/cell_1/lstm_cell/bias:0    [512]          512\nfc/W:0                                        [128, 62]     7936\nfc/b:0                                        [62]            62\nTotal #vars=6, #params=225598, size=0.86MB\n[0119 17:45:22 @base.py:196] Setup callbacks graph ...\n[0119 17:45:22 @predict.py:42] Building predictor tower 'InferenceTower' on device /gpu:0 ...\n[0119 17:45:22 @collection.py:140] Size of these collections were changed in InferenceTower: (tf.GraphKeys.WHILE_CONTEXT: 1->2)\n[0119 17:45:22 @summary.py:34] Maintain moving average summary of 8 tensors.\n[0119 17:45:22 @base.py:212] Creating the session ...\n2018-01-19 17:45:22.933029: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n2018-01-19 17:45:22.933055: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n2018-01-19 17:45:22.933062: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n2018-01-19 17:45:23.141723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-01-19 17:45:23.142214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: \nname: TITAN Xp\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.582\npciBusID 0000:02:00.0\nTotal memory: 11.90GiB\nFree memory: 11.74GiB\n2018-01-19 17:45:23.142233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 \n2018-01-19 17:45:23.142240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y \n2018-01-19 17:45:23.142249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN Xp, pci bus id: 0000:02:00.0)\n[0119 17:45:23 @base.py:220] Initializing the session ...\n[0119 17:45:23 @base.py:227] Graph Finalized.\n[0119 17:45:23 @concurrency.py:36] Starting EnqueueThread QueueInput/input_queue ...\n[0119 17:45:23 @base.py:247] Start Epoch 1 ...\n  0%|                                                                                          |0/72[00:00<?,?it/s]2018-01-19 17:45:24.240321: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.240328: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.240344: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.240358: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.240364: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.240542: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.240563: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.240553: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.240578: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.240596: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.240601: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.240780: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.240786: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.240803: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.240815: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.240822: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.240967: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.240973: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.240981: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.240985: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.240996: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.241156: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.241162: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.241175: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.241192: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.241200: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.241395: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.241414: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.241423: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.241441: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.241446: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.241600: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.241610: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.241621: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.241623: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.241633: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.241779: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.241791: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.241801: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.241817: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.241824: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.241983: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.241992: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.242000: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.242009: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.242010: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.242148: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.242156: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.242166: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.242179: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.242186: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.242329: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.242341: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.242350: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.242364: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.242370: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.242477: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.242506: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.242529: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.242552: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.242573: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n2018-01-19 17:45:24.243335: W tensorflow/core/kernels/queue_base.cc:295] _0_QueueInput/input_queue: Skipping cancelled enqueue attempt with queue not closed\nTraceback (most recent call last):\n  File \"./train-timit.py\", line 126, in \n[0119 17:45:24 @input_source.py:148] EnqueueThread QueueInput/input_queue Exited.\n    launch_train_with_config(config, SimpleTrainer())\n  File \"/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorpack/train/interface.py\", line 96, in launch_train_with_config\n    config.steps_per_epoch, config.starting_epoch, config.max_epoch)\n  File \"/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorpack/train/base.py\", line 288, in train\n    self.main_loop(steps_per_epoch, starting_epoch, max_epoch)\n  File \"/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorpack/utils/argtools.py\", line 171, in wrapper\n    return func(args, kwargs)\n  File \"/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorpack/train/base.py\", line 253, in main_loop\n    self.run_step()  # implemented by subclass\n  File \"/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorpack/train/base.py\", line 172, in run_step\n    self.hooked_sess.run(self.train_op)\n  File \"/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 518, in run\n    run_metadata=run_metadata)\n  File \"/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 862, in run\n    run_metadata=run_metadata)\n  File \"/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 818, in run\n    return self._sess.run(*args, kwargs)\n  File \"/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 972, in run\n    run_metadata=run_metadata)\n  File \"/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 818, in run\n    return self._sess.run(args, **kwargs)\n  File \"/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 895, in run\n    run_metadata_ptr)\n  File \"/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1124, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\n    options, run_metadata)\n  File \"/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InvalidArgumentError: label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\nCaused by op u'CTCLoss', defined at:\n  File \"./train-timit.py\", line 126, in \n    launch_train_with_config(config, SimpleTrainer())\n  File \"/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorpack/train/interface.py\", line 92, in launch_train_with_config\n    model._build_graph_get_cost, model.get_optimizer)\n  File \"/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorpack/utils/argtools.py\", line 171, in wrapper\n    return func(args, kwargs)\n  File \"/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorpack/train/tower.py\", line 161, in setup_graph\n    train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)\n  File \"/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorpack/train/trainers.py\", line 52, in _setup_graph\n    grads = self._make_get_grad_fn(input, get_cost_fn, get_opt_fn)()\n  File \"/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorpack/train/tower.py\", line 188, in get_grad_fn\n    cost = get_cost_fn(input.get_input_tensors())\n  File \"/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorpack/tfutils/tower.py\", line 206, in call\n    output = self._tower_fn(args)\n  File \"/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorpack/graph_builder/model_desc.py\", line 168, in _build_graph_get_cost\n    self.build_graph(inputs)\n  File \"/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorpack/graph_builder/model_desc.py\", line 116, in build_graph\n    self._build_graph(inputs)\n  File \"./train-timit.py\", line 54, in _build_graph\n    loss = tf.nn.ctc_loss(label, logits, seqlen, time_major=False)\n  File \"/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorflow/python/ops/ctc_ops.py\", line 152, in ctc_loss\n    ignore_longer_outputs_than_inputs=ignore_longer_outputs_than_inputs)\n  File \"/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorflow/python/ops/gen_ctc_ops.py\", line 168, in _ctc_loss\n    name=name)\n  File \"/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/giampi/anaconda2/envs/tensorpack2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1204, in init\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\nInvalidArgumentError (see above for traceback): label SparseTensor is not valid: indices[16] = [0,16] is out of bounds: need 0 <= index < [64,16]\n     [[Node: CTCLoss = CTCLossctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\nPrefetchDataZMQ successfully cleaned-up.. If it can help, I ran the following with my train.mdb file:\n```\nfrom tensorpack import *\nds = LMDBDataPoint('train.mdb', shuffle=False)\nds.reset_state()\nget the first utterance\nfor dp in ds.get_data():\n   break\n``\nThe I looked intodp:len(dp)returns 2,dp[0].shapereturns(16, 39)and dp[1].shape returns(36,)`.\nIt does indeed look strange if dp[1] is supposed to hold the labels for the feature vectors in dp[0]. But I am not sure how to fix it.. I went a bit further: it seems each data point returned by get_data() has two arrays, the first always has the same size and the second varies in size. So, perhaps I have misinterpreted the data points returned by get_data(), thinking they would be utterances whereas they are fixed windows of features vectors, instead. I still can't explain the variable size of the second element of dp.\n```\nget all the utterances\ndps = [dp for dp in ds.get_data()]\nprint the shapes for the first 10\nfor i in range(10):\n   print(dps[i][0].shape, dps[i][1].shape)\n```\n((16, 39), (36,))\n((16, 39), (37,))\n((16, 39), (32,))\n((16, 39), (35,))\n((16, 39), (44,))\n((16, 39), (19,))\n((16, 39), (35,))\n((16, 39), (30,))\n((16, 39), (36,))\n((16, 39), (37,))\n. I believe I found the problem: as I mentioned earlier, my database is stored in lower case file names. When I ran sox to convert from NIST to RIFF waveform, this ment that the input and output file names where identical. I was aware of this, but assumed that the conversion would work nevertheless. However, now that I look at the wav files, they all have the same size, and probably contained no audio data at all. I will correct that. Thank you for your help!. ",
    "sagarwaghmare69": "Thank you for quick response. When you say not use PrefetchDataZMQ does that mean any of the parallel dataflows ? \nI will try to redesign the whole flow to avoid using train_ds = BatchData(train_ds, batch, use_list=True) twice. . Ok, cool. Thank you very much for your help.. ",
    "arrowrowe": "\nSyncMultiGPUTrainerParameterServer works fine.\nAgree. Will look into this later.\n\nClose for now. Will reopen if new clues found.\nThanks for the reply! :p @ppwwyyxx . ",
    "JasonHanG": "Oh, I see!\nThanks for your information.\nBut the red wavy lines are really annoying.... ^_^\nI'm explicitly import every class just for the time being, which works ok.\nMaybe I shouldn't waste too much time on this little inconvinent ,It's time to move on.\n. ",
    "dpkingma": "Yeah, looks like uint8 overflow/underflow.. Awesome! Thanks for the quick fix!. ",
    "pandasMX": "sorry, i just found it. I fixed the problem because i noticed that the default buffer_size was set to be 2000. Howerver, my validation set has only 1500 images. That's why it failed.. the mobilenet code is from official keras implementation, the only change i made is the following function:\n`def mobilenet(inputs):\n    input = tf.layers.Input(tensor=inputs[0])\ndef image_preprocess(image):\n    image = ImageNetModel.image_preprocess(image)\n    image = tf.transpose(image, [0, 3, 1, 2])\n    return image\n\nx = Lambda(image_preprocess)(input)\n\nx = _conv_block(x, 32, ALPHA, strides=(2, 2))\nx = _depthwise_conv_block(x, 64, ALPHA, DEPTH_MULTIPLIER, block_id=1)\n\nx = _depthwise_conv_block(x, 128, ALPHA, DEPTH_MULTIPLIER,\n                          strides=(2, 2), block_id=2)\nx = _depthwise_conv_block(x, 128, ALPHA, DEPTH_MULTIPLIER, block_id=3)\n\nx = _depthwise_conv_block(x, 256, ALPHA, DEPTH_MULTIPLIER,\n                          strides=(2, 2), block_id=4)\nx = _depthwise_conv_block(x, 256, ALPHA, DEPTH_MULTIPLIER, block_id=5)\n\nx = _depthwise_conv_block(x, 512, ALPHA, DEPTH_MULTIPLIER,\n                          strides=(2, 2), block_id=6)\nx = _depthwise_conv_block(x, 512, ALPHA, DEPTH_MULTIPLIER, block_id=7)\nx = _depthwise_conv_block(x, 512, ALPHA, DEPTH_MULTIPLIER, block_id=8)\nx = _depthwise_conv_block(x, 512, ALPHA, DEPTH_MULTIPLIER, block_id=9)\nx = _depthwise_conv_block(x, 512, ALPHA, DEPTH_MULTIPLIER, block_id=10)\nx = _depthwise_conv_block(x, 512, ALPHA, DEPTH_MULTIPLIER, block_id=11)\n\nx = _depthwise_conv_block(x, 1024, ALPHA, DEPTH_MULTIPLIER,\n                          strides=(2, 2), block_id=12)\nx = _depthwise_conv_block(x, 1024, ALPHA, DEPTH_MULTIPLIER, block_id=13)\n\nshape = (int(1024 * ALPHA), 1, 1)\n\nx = GlobalAveragePooling2D()(x)\nx = Reshape(shape, name='reshape_1')(x)\nx = Dropout(0.001, name='dropout')(x)\nx = Conv2D(NUM_CLASSES, (1, 1),\n           padding='same', name='conv_preds')(x)\nx = Activation('softmax', name='act_softmax')(x)\nx = Reshape((NUM_CLASSES,), name='reshape_2')(x)\n\nM = tf.keras.models.Model(input, x, name='mobilenet_0.75_160')\nreturn M`\n\n. Ohhh, thanks! I just realise that 'fc1000/kernel' is a variable. So the output node should be 'fc1000'? But it says AssertionError: fc1000 is not in graph after trying. . Thanks for your advices. However, I found more than 50k node names in the graph and I have no idea which are the input and output node names. . ",
    "MrWanter": "I changed it to read from single lmdb file, with the lines from your tutorial:\nds = LMDBData(db, shuffle=False)\nds = LocallyShuffleData(ds, 50000)\nds = PrefetchData(ds, 5000, 1)\nds = LMDBDataPoint(ds)\nds = MapDataComponent(ds, lambda x: cv2.imdecode(x, cv2.IMREAD_COLOR), 0)\nds = AugmentImageComponent(ds, lots_of_augmentors)\nds = PrefetchDataZMQ(ds, 25)\nds = BatchData(ds, 256)\nnow it is around 2.4 it/s, and cpu usage is around 80%for each core, RAM usage is around 18G/64G, but GPU usage is not stable, sometimes all 70%-90%, sometimes drop down to half of that, although on average keep in high usage.\nfor the first epoch, I got \n[0201 09:48:18 @monitor.py:363] DataParallelInferenceRunner/QueueInput/queue_size: 50\n[0201 09:48:18 @monitor.py:363] QueueInput/queue_size: 0.85\nfor the second epoch, I got\n[0201 10:30:16 @monitor.py:363] DataParallelInferenceRunner/QueueInput/queue_size: 2.5103\n[0201 10:30:16 @monitor.py:363] QueueInput/queue_size: 2.2759e-37\nfor the third epoch, I got\n[0201 11:11:54 @monitor.py:363] DataParallelInferenceRunner/QueueInput/queue_size: 2.5153\n[0201 11:11:54 @monitor.py:363] QueueInput/queue_size: 2.35e-37\nbut sorry, I'm not much understand those numbers and features, about the efficient dataflow, there is explanations:\n```\nOne process reads LMDB file, shuffle them in a buffer and put them into a multiprocessing.Queue (used by PrefetchData).\n25 processes take items from the queue, decode and process them into [image, label] pairs, and send them through ZMQ IPC pipe.\nThe main process takes data from the pipe, makes batches.\ndoes it mean read `50000` images from the `1.28` million and cached them in memory in a buffer(which corresponds to the `18G/64G` memory usage), then one process(corresponding to the `PrefetchData(ds, 5000, 1)`) read `5000` of them, next `25` processes do parallel decoding and preprocessing for the `5000` images, last, batch them into `256` chanks.\nI may lack some basic knowledge to understand dataflow processes, and by look into source of functions  like `LMDBData`, `LocallyShuffleData`, I cannot understand them to know the details of the process, I'm wondering if you can point me to some references, Thanks for your attention!. could you suggest some possible modification on my case? e.g. since memory is `64G`, can I set some larger memory usage to get higher data speed?. the lines for creating the lmdb file is:\nfrom tensorpack.dataflow import *\nclass BinaryILSVRC12(dataset.ILSVRCFiles):\n    def get_data(self):\n        for fname, label in super(BinaryILSVRC12, self).get_data():\n            with open(fname, 'rb') as f:\n                jpeg = f.read()\n            jpeg = np.asarray(bytearray(jpeg), dtype='uint8')\n            yield [jpeg, label]\nds0 = BinaryILSVRC12()\nds1 = PrefetchDataZMQ(ds0, nr_proc=1)\ndftools.dump_dataflow_to_lmdb(ds1, '/path/to/ILSVRC-train.lmdb')\nbut `dataset.ILSVRCFiles` is not found, I used `class dataflow.dataset.ILSVRC12Files`, and initialize the class as:\n`ds0 = BinaryILSVRC12('/temp/wangtao/ILSVRC/Data/CLS-LOC/', 'train', shuffle=False)`\n is it the same ? Am I right?\nThanks. I see. I tested the raw reading speed with\nds = LMDBData('/path/to/ILSVRC-train.lmdb', shuffle=False)\nds = BatchData(ds, 256, use_list=True)\nTestDataSpeed(ds).start()\n``\nwhich is strange that, at first the speed is around 100 it/s, then to20%of reading, it quickly drop down to3 it/s, and maintain3-8 it/s` to the end.\nFor locally shuffle data with \nds = LMDBData('/path/to/ILSVRC-train.lmdb', shuffle=False)\nds = LocallyShuffleData(ds, 50000)\nds = BatchData(ds, 256, use_list=True)\nTestDataSpeed(ds).start()\nwhich I got speed of 2-3 it/s when reading is stable.\nFor the parallelized version adding all augmentors for Shufflenet :\nds = LMDBData(db, shuffle=False)\nds = LocallyShuffleData(ds, 50000)\nds = PrefetchData(ds, 5000, 1)\nds = LMDBDataPoint(ds)\nds = MapDataComponent(ds, lambda x: cv2.imdecode(x, cv2.IMREAD_COLOR), 0)\nds = AugmentImageComponent(ds, lots_of_augmentors)\nds = PrefetchDataZMQ(ds, 25)\nds = BatchData(ds, 256)\nwhich I got speed of 2.7 it/s when reading is stable, and 12cores of CPU are all around 70-80% usage, but it is still not fast enough, I'm wondering if you can suggest how to further boost the dataflow, I tried change the local shuffle buffer size to 100000(I have 64G RAM), or change the PrefetchDataZMQ from 25 to 40, but seems not much change in reading speed, all around 2-3 it/s.. [0201 11:11:54 @monitor.py:363] DataParallelInferenceRunner/QueueInput/queue_size: 2.5153\n[0201 11:11:54 @monitor.py:363] QueueInput/queue_size: 2.35e-37\nfor the DataParallelInferenceRunner/QueueInput/queue_size I used lmdb, but for  QueueInput/queue_size I didn't change it and just used the original random read, so it is much more slow.\ncould it be possible that my queue ratio (2.5) is too low that layer PrefetchDataZMQ processes cannot keep a busy state, and is it safely to conclude that it is caused by low disk reading speed as the raw reading speed with \nds = LMDBData('/path/to/ILSVRC-train.lmdb', shuffle=False)\nds = BatchData(ds, 256, use_list=True)\nTestDataSpeed(ds).start()\nonly got stable reading speed of around 3 it/s, hope you can give me some advice, thank you!. I tested with sudo hdparm -Tt /dev/sdX, the cached reading speed is around 1200 M/s, uncached disk reading is 150M/s, with iotop, I observed the disk reading speed is around 75M/s when training on 3 GPU, a half of the tested reading speed of 150M/s. \nI'm a bit confused here about the locally shuffle data, does it mean maintaining a 50000 image sized memory chunk on the RAM, and only fetch batches from it, then after 50000/3(default shuffle interval) continuous images are fetched(batch by batch), we shuffle the whole 50000 image chunk, then do next 50000/3 fetch, but how does this machinism assure that all images are visited, and how do we update the 50000 image chunk, is it updated like a queue that newly readed image are put onto the first and olded one kick off the queue(LMDBData(db, shuffle=False) returns one image a time). with vmstat 1 4 on the machine for 3 GPU training, I get \n```\nprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----\n r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st\n11  0  45548 684776 770364 32992844    0    0   286    21    1    1 34  4 61  0  0\n19  0  45548 697964 770364 32980384    0    0 66176     0 38472 55734 69  6 24  1  0\n11  0  45548 760188 770372 32910624    0    0 65984    12 36339 50278 70  5 24  1  0\n12  0  45548 703368 770372 32965052    0    0 61628     0 22334 60357 72  3 24  0  0\nthe `wa` is almost 0 all the time, means cpu are not waiting for disk reading and `bi` is around `60000-65000`, which is reading around `60-65 M/s`. I think the bottleneck lies both in `cpu` performance and `disk` reading speed.\n. But could you please explain a little about my confusion, thank you. Yes, I think it is the best that the CPU and disk can do, as even all `6` cores with `12` threads are busy, they don't wait for disk reading, and reduce CPU preprocessing I still cannot get full disk reading upper bound. Have estimated or tested that in total like `100` epochs, that all images are visited?. Am I right that each data point contain `TOTAL_BATCH_SIZE / nr_tower` images? so in total, all gpus together compute `TOTAL_BATCH_SIZE` of images?. Thanks for your quick response, my confusion is that for training with `3` gpu and `4` gpu, training progress bar is `xxx/5000`, but validation bar for `3` gpu is `xxx/589`, while for `4` gpu is `xxx/782`, not the same, for training, `256` batch times `5000` is just `128` million, but what about validation? . I see, `782*(256/4) ~= 589*(256/3) ~=50000`, but why not make it the same like training bar, as `(782/4)*256 ~= (589/3)*256 ~= (196)*256`, so we always see `xxx/196` in validation bar. I see:). by the way, I oberseve there is\n[0203 17:27:40 @fs.py:89] WRN Env var $TENSORPACK_DATASET not set, using /home/wangtao/tensorpack_data for dataset\ns.\nwhat is the meaning of `TENSORPACK_DATASET` ?. I suspect may be the training `lmdb` file is not correct, would it be wrong if I generate the `lmdb` file with\nclass BinaryILSVRC12(dataset.ILSVRC12Files):\n    def get_data(self):\n        for fname, label in super(BinaryILSVRC12, self).get_data():\n            with open(fname, 'rb') as f:\n                jpeg = f.read()\n            jpeg = np.asarray(bytearray(jpeg), dtype='uint8')\n            yield [jpeg, label]\nds0 = BinaryILSVRC12('/temp/wangtao/ILSVRC/Data/CLS-LOC/', 'train', shuffle = False)\nds1 = PrefetchDataZMQ(ds0, nr_proc=1)\ndftools.dump_dataflow_to_lmdb(ds1, '/temp/wangtao/ILSVRC-train.lmdb')\nI dont remember if my correct generate was \n`ds0 = BinaryILSVRC12('/temp/wangtao/ILSVRC/Data/CLS-LOC/', 'train', shuffle = True)`. after regenerating the training `lmdb`, the problem solved, but still not clean why it happened. Yes, when I tried on terminal, it works fine, but seems when I get a `tfdbg -> run`, it dumps so many things out, then root directory is filled, before run, there is 11G left, with a run, prompts no space left:\n2018-02-07 10:23:07.433343: E ./tensorflow/core/kernels/debug_ops.h:186] Debug node of watch key tower1/gradients/tower1/group2/block3/add_grad/Shape:0:DebugIdentityfailed to publish debug tensor data to all URLs file:///var/tmp/tfdbg_2DnmZj, due to: Publishing to 1 of 1 debug target URLs failed, due to the following errors: Failed to create directory  /var/tmp/tfdbg_2DnmZj/tfdbg_device,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/add_grad, due to: Failed to create directory  /var/tmp/tfdbg_2DnmZj/tfdbg_device,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3;\n2018-02-07 10:23:07.443635: E ./tensorflow/core/kernels/debug_ops.h:186] Debug node of watch key tower1/gradients/tower1/group2/block3/conv1/Conv2D_2_grad/ShapeN:0:DebugIdentityfailed to publish debug tensor data to all URLs file:///var/tmp/tfdbg_2DnmZj, due to: Publishing to 1 of 1 debug target URLs failed, due to the following errors: Failed to create directory  /var/tmp/tfdbg_2DnmZj/tfdbg_device,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1/Conv2D_2_grad, due to: Failed to create directory  /var/tmp/tfdbg_2DnmZj/tfdbg_device,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1;\n2018-02-07 10:23:07.443804: E ./tensorflow/core/kernels/debug_ops.h:186] Debug node of watch key tower1/gradients/tower1/group2/block3/conv1/Conv2D_2_grad/ShapeN:1:DebugIdentityfailed to publish debug tensor data to all URLs file:///var/tmp/tfdbg_2DnmZj, due to: Publishing to 1 of 1 debug target URLs failed, due to the following errors: Failed to create directory  /var/tmp/tfdbg_2DnmZj/tfdbg_device,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1/Conv2D_2_grad, due to: Failed to create directory  /var/tmp/tfdbg_2DnmZj/tfdbg_device,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1;\n2018-02-07 10:23:07.445778: E ./tensorflow/core/kernels/debug_ops.h:186] Debug node of watch key tower1/gradients/tower1/group2/block3/conv1/Conv2D_1_grad/ShapeN:0:DebugIdentityfailed to publish debug tensor data to all URLs file:///var/tmp/tfdbg_2DnmZj, due to: Publishing to 1 of 1 debug target URLs failed, due to the following errors: Failed to create directory  /var/tmp/tfdbg_2DnmZj/tfdbg_device,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1/Conv2D_1_grad, due to: Failed to create directory  /var/tmp/tfdbg_2DnmZj/tfdbg_device,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1;\n2018-02-07 10:23:07.445894: E ./tensorflow/core/kernels/debug_ops.h:186] Debug node of watch key tower1/gradients/tower1/group2/block3/conv1/Conv2D_1_grad/ShapeN:1:DebugIdentityfailed to publish debug tensor data to all URLs file:///var/tmp/tfdbg_2DnmZj, due to: Publishing to 1 of 1 debug target URLs failed, due to the following errors: Failed to create directory  /var/tmp/tfdbg_2DnmZj/tfdbg_device,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1/Conv2D_1_grad, due to: Failed to create directory  /var/tmp/tfdbg_2DnmZj/tfdbg_device,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1;\n2018-02-07 10:23:07.446192: E ./tensorflow/core/kernels/debug_ops.h:186] Debug node of watch key tower1/gradients/tower1/group2/block3/conv1/Conv2D_grad/ShapeN:0:DebugIdentityfailed to publish debug tensor data to all URLs file:///var/tmp/tfdbg_2DnmZj, due to: Publishing to 1 of 1 debug target URLs failed, due to the following errors: Failed to create directory  /var/tmp/tfdbg_2DnmZj/tfdbg_device,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1/Conv2D_grad, due to: Failed to create directory  /var/tmp/tfdbg_2DnmZj/tfdbg_device,job_localhost,replica_0,task_0,device_GPU_1/tower1/gradients/tower1/group2/block3/conv1;\n```\nHave you  tried this?. I changed this two lines in param.py: https://github.com/ppwwyyxx/tensorpack/blob/bf4d89389b508844280d2bc6826441fe5dcd8243/tensorpack/callbacks/param.py#L281-L282\nto  \ndef _get_value_to_set(self):\n        return self.f(self.global_step, self.get_current_value())\nthen replace the original schedualer :\nScheduledHyperParamSetter('learning_rate',\n                                   [(0, 3e-1), (30, 3e-2), (60, 3e-3), (90, 3e-4)]),\nwith \nHyperParamSetterWithFunc('learning_rate',\n                                 lambda e, x: 0.5*(1-e/3e5)),\nAm I right? but seems not right, after 10 epochs(global_step 12500, total step 3e5), lr still 0.4999, not decrease to around 0.48, by the way, seems the summary is also updated epoch based, can we modify it to global_step based?. I change the line:\nhttps://github.com/ppwwyyxx/tensorpack/blob/bf4d89389b508844280d2bc6826441fe5dcd8243/tensorpack/callbacks/param.py#L161\nto  def _trigger_step(self): and it works properly!. @Skylion007 Thanks for sharing! I'll try it. as in the http://tensorpack.readthedocs.io/en/latest/tutorial/summary.html :\n\nAll the \"what, when, where\" can be customized in either the graph or with the callbacks/monitors setting.\n\nsay,  I only want to change the lr to be step based, keep other summary epoch based, \nfor\n\nwith the callbacks/monitors setting \n\nseems tensorpack put all summaries writing together:\nhttps://github.com/ppwwyyxx/tensorpack/blob/bf4d89389b508844280d2bc6826441fe5dcd8243/tensorpack/callbacks/monitor.py#L120-L141\nis it difficult to modify the code since it is wrapped deeply?\nfor\n\ncan be customized in either the graph\n\nI dont understand what you mean by customize it graph, do you mean call tf.summary.xxx when we construct the graph? \ncan you share a simple example of using tf.train.get_or_create_global_step()? . Thank you, a great example for customizing!. and for tf.train.get_or_create_global_step, do you mean write a lr decay function like tf.train.exponential_decay and pass it to optimizer?. I see, thank you. But not found where inference called _build_graph or build_graph, could you pls point out? Thank you. https://github.com/ppwwyyxx/tensorpack/blob/54c5a42db3c33df7310dc80106dd123e4e04d1b4/tensorpack/train/interface.py#L90-L92\nbut seems this is only for training, while for validation, where does it being called ? . I see, but in_trigger():\nhttps://github.com/ppwwyyxx/tensorpack/blob/9bbdf94da8be9976725fb131bfb5c9f5523b25ac/tensorpack/callbacks/inference_runner.py#L261-L278\nseems _set_graph() or set_graph is not called, where am I wrong?. I see, will try it!. I see. I see, thank you for pointing out, have you tried remove some of the preprocessing so that cpu burden could be reduced a bit?\n. could you suggest for like shufflenet example, which preprocessing would more safe to remove? I mean just based on experience, thank you. I see, will try to remove some. sorry, but where is mean substraction in preprocessing? \nif isTrain:\n        augmentors = [\n            GoogleNetResize(),\n            imgaug.RandomOrderAug(\n                [imgaug.BrightnessScale((0.6, 1.4), clip=False),\n                 imgaug.Contrast((0.6, 1.4), clip=False),\n                 imgaug.Saturation(0.4, rgb=False),\n                 # rgb-bgr conversion for the constants copied from fb.resnet.torch\n                 imgaug.Lighting(0.1,\n                                 eigval=np.asarray(\n                                     [0.2175, 0.0188, 0.0045][::-1]) * 255.0,\n                                 eigvec=np.array(\n                                     [[-0.5675, 0.7192, 0.4009],\n                                      [-0.5808, -0.0045, -0.8140],\n                                      [-0.5836, -0.6948, 0.4203]],\n                                     dtype='float32')[::-1, ::-1]\n                                 )]),\n            imgaug.Flip(horiz=True),\n        ]\n    else:\n        augmentors = [\n            imgaug.ResizeShortestEdge(256, cv2.INTER_CUBIC),\n            imgaug.CenterCrop((224, 224)),\n        ]. I see, it is intergreted into the graph, since it's in the graph, does it performed on gpu?. is it possible to move more preprocessing into GPU graph?. :smile: . I see.. By the way, have you reproduced the result of 140 Mflop, I only got 34.6, around 2 points away from the paper's results, I used linear decay schedule with other settings as described in the paper.. @PatWie @ppwwyyxx , thank you both. but showing code is also more precise, and help us learning programming skills. turns out updated op needs to be added with training . what if I want to let some layer's batchnorm act as testing state(use trained moving averages), while other layers as training state(use batch statistics)? . can I do:\nwith TowerContext('', is_training=True):\n        Conv2D(...)\nwith TowerContext('', is_training=False):\n        Conv2D(...). but I only use the Conv2D which usednl=BNRelu or nl= BN\nfor the \nwith TowerContext('', is_training=False):\n        Conv2D(...)\npart, I want to load pretrained batchnorm params(moving average)\nfor the \nwith TowerContext('', is_training=True):\n        Conv2D(...)\npart, I want to train it as usual(use the batch statistics and keep moving average)\nwould the my way of \nwith TowerContext('', is_training=True):\n        Conv2D(...)\nwith TowerContext('', is_training=False):\n        Conv2D(...)\nwork? Thank you . but I didn't see an error, will this work:\nwith argscope([Conv2D, BatchNorm], is_training=False):\n      Conv2D(...,nl=BNRelu)\nwith argscope([Conv2D, BatchNorm], is_training=True):\n      Conv2D(...,nl=BNRelu). could you pls point out a way to do this? Thanks . I see, so this will work?\nwith argscope([Conv2D, BatchNorm], training=False):\n      Conv2D(...,nl=BNRelu)\nwith argscope([Conv2D, BatchNorm], training=True):\n      Conv2D(...,nl=BNRelu). but seems Conv2D also dont accept training argument\ndef Conv2D(x, out_channel, kernel_shape,\n           padding='SAME', stride=1,\n           W_init=None, b_init=None,\n           nl=tf.identity, split=1, use_bias=True,\n           data_format='NHWC'):. I see, thank you. but seems batchnorm also dont accept training argument:\ndef BatchNorm(x, use_local_stat=None, decay=0.9, epsilon=1e-5,\n              use_scale=True, use_bias=True,\n              gamma_init=tf.constant_initializer(1.0), data_format='NHWC',\n              internal_update=False):. so my use of \nwith TowerContext('', is_training=True):\n        Conv2D(...)\nwith TowerContext('', is_training=False):\n        Conv2D(...)\nin the old version will work? as \nuse_local_stat (bool): whether to use mean/var of the current batch or the moving average.\n            Defaults to True in training and False in inference.. Yes, I dont use tensorpack trainner. I modify some thing and got it work in py2, but it stucked here:\n[0712 14:41:04 @logger.py:74] Argv: /home/user/prj/tensorpack-fpn/train.py --config  MODE_MASK=True MODE_FPN=True  DATA.BASEDIR=/home/user/prj/mscoco  BACKBONE.WEIGHTS=./ImageNet-ResNet50.npz\n[0712 14:41:06 @config.py:195] Config: ------------------------------------------\n{'BACKBONE': {'FREEZE_AFFINE': False,\n              'NORM': 'FreezeBN',\n              'RESNET_NUM_BLOCK': [3, 4, 6, 3],\n              'STRIDE_1X1': False,\n              'TF_PAD_MODE': False,\n              'WEIGHTS': './ImageNet-ResNet50.npz'},\n 'DATA': {'BASEDIR': '/home/user/prj/mscoco',\n          'CLASS_NAMES': [],\n          'NUM_CATEGORY': 80,\n          'NUM_CLASS': 81,\n          'TRAIN': ['train2014', 'valminusminival2014'],\n          'VAL': 'minival2014'},\n 'FPN': {'ANCHOR_STRIDES': (4, 8, 16, 32, 64),\n         'FRCNN_CONV_HEAD_DIM': 256,\n         'FRCNN_FC_HEAD_DIM': 1024,\n         'FRCNN_HEAD_FUNC': 'fastrcnn_2fc_head',\n         'NUM_CHANNEL': 256,\n         'PROPOSAL_MODE': 'Level',\n         'RESOLUTION_REQUIREMENT': 32},\n 'FRCNN': {'BATCH_PER_IM': 512,\n           'BBOX_REG_WEIGHTS': [10.0, 10.0, 5.0, 5.0],\n           'FG_RATIO': 0.25,\n           'FG_THRESH': 0.5},\n 'MODE_FPN': True,\n 'MODE_MASK': True,\n 'MRCNN': {'HEAD_DIM': 256},\n 'PREPROC': {'MAX_SIZE': 1344.0,\n             'PIXEL_MEAN': [123.675, 116.28, 103.53],\n             'PIXEL_STD': [58.395, 57.12, 57.375],\n             'SHORT_EDGE_SIZE': 800},\n 'RPN': {'ANCHOR_RATIOS': (0.5, 1.0, 2.0),\n         'ANCHOR_SIZES': (32, 64, 128, 256, 512),\n         'ANCHOR_STRIDE': 16,\n         'BATCH_PER_IM': 256,\n         'CROWD_OVERLAP_THRES': 0.7,\n         'FG_RATIO': 0.5,\n         'HEAD_DIM': 1024,\n         'MIN_SIZE': 0,\n         'NEGATIVE_ANCHOR_THRES': 0.3,\n         'NUM_ANCHOR': 15,\n         'POSITIVE_ANCHOR_THRES': 0.7,\n         'PROPOSAL_NMS_THRESH': 0.7,\n         'TEST_PER_LEVEL_NMS_TOPK': 1000,\n         'TEST_POST_NMS_TOPK': 1000,\n         'TEST_PRE_NMS_TOPK': 6000,\n         'TRAIN_PER_LEVEL_NMS_TOPK': 2000,\n         'TRAIN_POST_NMS_TOPK': 2000,\n         'TRAIN_PRE_NMS_TOPK': 12000},\n 'TEST': {'FRCNN_NMS_THRESH': 0.5,\n          'RESULTS_PER_IM': 100,\n          'RESULT_SCORE_THRESH': 0.05,\n          'RESULT_SCORE_THRESH_VIS': 0.3},\n 'TRAIN': {'BASE_LR': 0.01,\n           'LR_SCHEDULE': [240000, 320000, 360000],\n           'NUM_GPUS': 4,\n           'STEPS_PER_EPOCH': 500,\n           'WARMUP': 1000,\n           'WEIGHT_DECAY': 0.0001},\n 'TRAINER': 'replicated'}\n[0712 14:41:06 @train.py:566] Warm Up Schedule (steps, value): [(0, 0.0033333333333333335), (2000.0, 0.01)]\n[0712 14:41:06 @train.py:567] LR Schedule (epochs, value): [(4, 0.01), (960.0, 0.001), (1280.0, 0.00010000000000000002)]\n[0712 14:41:07 @prof.py:45] WRN [GPUUtilizationTracker] Both devices and CUDA_VISIBLE_DEVICES are None! Will monitor all 4 visible GPUs!\nloading annotations into memory...\nDone (t=10.54s)\ncreating index...\nindex created!\n[0712 14:41:24 @coco.py:77] Instances loaded from /home/user/prj/mscoco/annotations/instances_train2014.json.\n. https://github.com/tensorpack/tensorpack/blob/ccda379035165c1192f253b04af1fb3e73f1faf4/examples/FasterRCNN/eval.py#L75\none modification is change this line to boxes, probs, labels, masks = model_func(resized_img) dont know if it is right for py2\n. OK it is not stuck, just loading the data. everytime for running, it stucks at \nloading annotations into memory...\nDone (t=10.44s)\ncreating index...\nindex created!\n[0712 15:45:35 @coco.py:77] Instances loaded from /home/wangtao/prj/mscoco/annotations/instances_train2014.json.\nfor so long(more than 10 mins), then val data for 260 secs ,can we mitigate this issue?\n. I see, thank you.. Sorry, I misunderstood the table.. Thanks for your quick response!. I see, thank you. Is it safe to change \n```\n        ret = crop_and_resize(\n            featuremap, boxes,\n            tf.zeros([tf.shape(boxes)[0]], dtype=tf.int32),\n            resolution * 2)\n    ret = tf.nn.avg_pool(ret, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME', data_format='NHWC')\n\nto\n        ret = crop_and_resize(\n            featuremap, boxes,\n            tf.zeros([tf.shape(boxes)[0]], dtype=tf.int32),\n            resolution)\n``. I really cant feature out why first crop2*resolusionthen avg pool, is there reason specifically fortf.image.crop_and_resize` ?. I see, thank you, I'll test it. I see, thanks!. ",
    "leix28": "It works. Thank you!. Thank you for your reply. Seems the issue has been fixed in recently. . ",
    "John1231983": "Thank you. I have read it. But the importance is that my dataset does not follow the coco style, it means no bounding box available, only masks for each object. It likes shape example in matterport's implementation. Could you tell me what should I do to make my data loader?. Do I need convert my dataset to coco annotation format to use your code?. Thanks so much. I am reading your guide and let you know when it completed. I have one more question about your implementation. As you know, the Batch Norm training is important for segmentation and detection. The paper used the batch size of 16. I found that you used the batch size of 256. During the training, do you consider to train the Batch Norm statistic? In my opinion, it looks that you did not train BN parameters\n. Sorrry, I forget the lines that you have mentioned\n\nWe only support single image per GPU.\nBecause of (3), BatchNorm statistics are not supposed to be updated during fine-tuning. This specific kind of BatchNorm will need\u00a0my kernel\u00a0which is included since TF 1.4. If using an earlier version of TF, it will be either slow or wrong\n\nIt means that the BN statistic will not learn during training. On other hands, it is frozen. Am I right?\n. Hi ppwwppxx, could you tell me any reason why your performance (32.3%) is better than keras based (29.5%) using same resnet-50?. Yes, I am talking about matterport\u00a0implementation. Actually, I am using your code and matterport\u00a0to work with DSB2018 dataset challence. I was successful to run it with matterport\u00a0but I was not successful with your project. My dataset is not similar coco format ( only contains raw image and its masks). In matterport, I based on example of training shape  https://github.com/matterport/Mask_RCNN/blob/master/train_shapes.ipynb\nI am very happy if you can make an example like that, because it may be helpful to work in difference dataset. Thanks. ",
    "YanWang2014": "I am also trying to use own data for training. Although it's not difficult to provide masks etc to train.py (by modify data.py and coco.py),  I see many dependencies on pycocotools in eval.py, which makes further work required to make the code run on other dataset. Cool. ",
    "letuantang": "@John1231983 can I ask you some question? you said that you have successfully run the matterpot code on your own data set and mentioned the link to train_shapes.ipynb? Can you explain more detail about it? It's mean that base on the train_shapes.ipynb you edit the code to train for your own dataset (to recognize an object like a clock and do the segmentation, for example). Is it right? Or you just successfully run the code on train_shapes.ipynb on your computer? I also want to run the Mask RCNN on my own dataset but I don't know exactly what need I do with my trainning images and what format it is or just put the raw image inside. I'm still focusing on this point. Thanks. ",
    "cpoptic": "@John1231983 @letuantang   Facebook AI Research released Maskrcnn_benchmark, it's allegedly a much faster version of Matterport's Mask_RCNN.  Would recommend trying that out:  https://github.com/facebookresearch/maskrcnn-benchmark. I can confirm getting this error message as well.\n\ntensorflow.python.framework.errors_impl.NotFoundError: /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/horovod/tensorflow/mpi_lib.cpython-36m-x86_64-linux-gnu.so: undefined symbol: _ZN10tensorflow6StatusC1ENS_5error4CodeENS_11StringPieceE\n\nI would disagree about this issue not being super interesting.  It may not be interesting, but it's clearly a bug and therefore should be addressed.  Also the original poster made a great request for \"some instructions on how to prepare custom datasets. Also how to do transfer learning with the pretrained official tensorpack models.\"\nI'm going to try creating a new conda environment and do a fresh install of tensorflow-gpu, opencv-python and tensorpack.. ",
    "raingo": "Could you please give an example?\nI have used \nlogger.set_logger_dir(exp_dir, 'k')\nto setup the experiment directory. The experiment maybe cancelled, and I want to restart from the same directory.. I think I've got it. http://tensorpack.readthedocs.io/en/latest/tutorial/save-load.html\nThanks.. ",
    "sicnarf1a": "Thank you for your prompt reply. Is it possible to use util such as tfutils.optimizer.PostProcessOptimizer to solve this problem?. Thank you. I will try to do it myself and ask again if another issue arises.\nBest Regards.. Thank you so much for your help! It is clear now.. ",
    "yaroslavvb": "oh, oops, good point, that's in their code. I see, thanks\nSomewhat related, is there an easy way to add to tensorboard duration of each train step? Perhaps it needs adding something like this inside tensorpack somewhere?\n    start_time = time.perf_counter()\n     ....run(train_op)\n    duration = time.perf_counter() - start_time\n    trainer.monitors.put_scalar('step_time', duration)\n\n. ok looks like shutil called rmtree which gives that error\nthe actual error was due to NFS/EFS misbehaving. It seems impossible to delete NFS directories sometimes, but it seems possible to move them to /efs/trash as a workaround\n```\n\n\n\nshutil.rmtree('/efs/runs/yuxin_numpy/1gpu')\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/shutil.py\", line 480, in rmtree\n    _rmtree_safe_fd(fd, path, onerror)\n  File \"/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/shutil.py\", line 438, in _rmtree_safe_fd\n    onerror(os.unlink, fullname, sys.exc_info())\n  File \"/home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/shutil.py\", line 436, in _rmtree_safe_fd\n    os.unlink(name, dir_fd=topfd)\nOSError: [Errno 16] Device or resource busy: '.nfsb3d9a8ed434c603a00000002'\n```. I kept hitting this error, and it turned out to be due to TensorBoard keeping files open. So the trick is to always kill tensorboard before deleting files. Some tricks for future reference\n\n\n\n```\nThe first command to try is a lazy unmount of the EFS volume. Below is the lazy unmount command.\n$ umount -l \"EFS mount path\" \nThe command below will find the process that is using the EFS share and output only the process id. Using \"ps aux\" you can find out more about the process. When ready to kill the process parse the below command to \"kill -9\".\n$ lsof -t \"efs/share\"\nThe command below is another way of finding and killing the process in one command. Remove \"k\" to view the process without killing it. \n$ fuser -km \"EFS mount path\"\n```\n. The umount has the same error as rm, those recipes help track down who is\nkeeping files open\nOn Feb 9, 2018 21:18, \"Yuxin Wu\" notifications@github.com wrote:\n\nI'm not quite following. Why do you umount it? Do you need to mount it\nback so you can use it?\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/ppwwyyxx/tensorpack/issues/644#issuecomment-364627256,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AABaHLFqqWAQESD6G2sQ3vQV7-7AsjqDks5tTSasgaJpZM4R_VhP\n.\n. I mean the values don't make sense to me, shouldn't they be integers? Are they being smoothed outside of tensorboard as well?\n\n. OK, makes sense. There's slight possibility that frequent event writing will have higher CPU utilization due to  protobuf encoding.  The actual flush interval is pretty long (30 seconds?), so probably it doesn't add much to slowness on the disk side.\n\nI've had to increase the flush interval sometimes for interactive debugging\n```\n  from tensorflow.python.summary.writer.writer import FileWriter\n  old_init = FileWriter.init\n  def newinit(*args, kwargs):\n    print(\"Overriding FileWriter flush_secs to 1\")\n    kwargs['flush_secs']=1\n    old_init(*args, kwargs)\n  FileWriter.init=newinit\n```. http://34.239.138.159:6006/#scalars\nAll steps are multiples of 6\nSome of those runs are just just the default mnist-conv.py example,  https://github.com/ppwwyyxx/tensorpack/blob/master/examples/mnist-convnet.py\n. Where does 468 come from? Is it set by tensorpack somewhere?. ah, just found http://tensorpack.readthedocs.io/en/latest/tutorial/summary.html?highlight=add_moving_summary, looks like it's every epoch, I was getting \"every 6\" because I used larger batch size. OK, for reference for myself, I ended up doing below. Couldn't figure out the setting for adjusting flush frequency, will dig into that later if that becomes a problem\nreturn TrainConfig(\n    model=Model(),\n    ....\n    extra_callbacks=train.DEFAULT_CALLBACKS()+[\n      MergeAllSummaries(period=1),\n    ],\n\n. @ppwwyyxx yes. It works on graph_defs (proto in -> proto out) and also uses heuristics to determine which nodes to copy. It's not exactly checkpointing -- it'll find nodes with multiple consumers and duplicate them to allow to forget their value.  There's an update from @allenlavoie here, it sounds like it'll be a couple of quarters before it's ready https://github.com/openai/gradient-checkpointing/issues/13#issuecomment-360211530. you can do something like this\nimport tensorflow as tf\ntf.__dict__[\"gradients\"] = memory_saving_gradients.gradients_memory\nHowever, that only overrides tf.gradients method called from tensorflow namespace. If you call something like Optimizer.minimize, internally, that's going to call gradients method from a different namespace, probably this one\nfrom tensorflow.python.ops import gradients\nSo to cover those usages, you may have to do\nimport tensorflow.python.ops as ops\nops.__dict__['gradients'] = memory_saving_gradients.gradients_memory\nThe underlying lesson is that this library is not quite production quality. Before going down this route, are you sure your use case needs it? IE, you could use smaller batch size for your resnet. For monkey-patching you don't need to find the place where it's called, just put it in the beginning of the file somewhere.\nBut if you wanted to make sure you are monkey patching the right thing, you may need to find how it gets called. The trick is to add import pdb; pdb.set_trace() into gradients.py file in the beginning of call, ie here. Then type \"bt\" to see who called it.\nYou can to modify this file in local installation, ie, on my computer it's this file\n/Users/yaroslav/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\n. I think you need to do the other one, since tf.gradients is never called directly by TensorPack, it's probably called indirectly as ops.gradients.\nimport tensorflow.python.ops as ops\nops.__dict__['gradients'] = memory_saving_gradients.gradients_memory. @michaelklachko most things are cheap except matmul/conv, because you have low ratio of compute to volume of data needed. The automatic mode failed, so you have to provide checkpoint nodes manually. For manual nodes to save memory, you have to pick sets nodes which separate your computation graph, like described here. Here's an example of using manual checkpoints:\nhttps://github.com/openai/gradient-checkpointing/blob/65bf6a97dd5244b5e0b5130cd2cc9d10575c224c/test/resnet_correctness_test.py#L247\nYou use the collection to store checkpoint tensors, and then use strategy=\"collection\". You could use pdb to debug it, but I suggest first trying to specfy checkpoints it in the same was as done in the test -- https://github.com/openai/gradient-checkpointing/blob/65bf6a97dd5244b5e0b5130cd2cc9d10575c224c/test/resnet_correctness_test.py#L247. ",
    "michaelklachko": "Where should I put the line tf.__dict__[\"gradients\"] = memory_saving_gradients.gradients_memory to enable checkpointing for the imagenet-resnet model? . In my case, I designed a new type of a convolutional layer, however to test it, I have to compute 9x more feature maps in each layer. I want to test this on ResNet model, so basically I have to run 9x wider ResNet-18, and I only have 4 GPUs. I already reduced batch size from 256 to 32, and also reduced the default number of feature maps in half. Now it barely fits in memory. I'm afraid further reduction of batch size might lead to issues with batchnorm, and the hyperparams were probably not optimized for such small batches.\nThat's why I think gradient-checkpointing would be an ideal method for me. \nI'm trying to figure out the right place to override tf.gradients in tensorpack, so I followed the path of optimizers:\nhttps://github.com/ppwwyyxx/tensorpack/blob/master/examples/ResNet/imagenet-resnet.py#L143\nhttps://github.com/ppwwyyxx/tensorpack/blob/master/tensorpack/train/trainers.py#L132\nhttps://github.com/ppwwyyxx/tensorpack/blob/master/tensorpack/train/tower.py#L115\nAfter that I'm lost. \n. @yaroslavvb, so just to clarify, if I put\nimport memory_saving_gradients\ntf.__dict__[\"gradients\"] = memory_saving_gradients.gradients_memory\nhere: https://github.com/ppwwyyxx/tensorpack/blob/master/examples/ResNet/resnet_model.py#L6\nIt will enable checkpointing for the code launched in \nhttps://github.com/ppwwyyxx/tensorpack/blob/master/examples/ResNet/imagenet-resnet.py ?\n. @allenlavoie, I'm doing tf.reduce_sum op on the output from a normal conv2d layer:\nhttps://github.com/michaelklachko/tensorpack/blob/learning/tensorpack/models/conv2d.py#L163\nis that a cheap op to recompute? What would be its name string to add to your list (e.g. \"ReduceSum\"?) . @yaroslavvb, ok thanks, I'll try that as soon as my current test is finished.  \nBy the way, my modification of conv2d is the following chain of ops:\n\nnormal conv2d (but with 9x more output feature maps)  <<<that's what consumes all that RAM\nmultiplying the result by a constant mask\nreshaping the result into groups of feature maps (9 maps per group)\ndoing tf.reduce_sum on each group to combine 9 maps into one.\nthe result is treated as the output of conv2d layer\n\nI just noticed that my code uses different variables to hold the result from step 1 (ret, conv_masked, and conv_grouped):\nhttps://github.com/michaelklachko/tensorpack/blob/learning/tensorpack/models/conv2d.py#L195-L197\nDoes it mean that I'm reserving 3x times as much memory as I really need?. @allenlavoie, I just checked and I don't see https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/python/grappler/memory_optimizer_test.py file on my machine:\n\nmichael@Pascal:/usr/local/lib/python2.7/dist-packages/tensorflow/python/grappler$ ll\ntotal 40K\n-rw-r--r-- 1 staff 4.9K Feb 15 16:40 cluster.py\n-rw-r--r-- 1 staff 6.2K Feb 15 16:40 cluster.pyc\n-rw-r--r-- 1 staff    0 Feb 15 16:40 init.py\n-rw-r--r-- 1 staff  199 Feb 15 16:40 init.pyc\n-rw-r--r-- 1 staff 3.3K Feb 15 16:40 item.py\n-rw-r--r-- 1 staff 4.1K Feb 15 16:40 item.pyc\n-rw-r--r-- 1 staff 1.8K Feb 15 16:40 tf_optimizer.py\n-rw-r--r-- 1 staff 1.5K Feb 15 16:40 tf_optimizer.pyc\n\nI installed TF 1.6rc1 with pip.. @yaroslavvb I'm getting import error:\n\n\n\nimport tensorflow.python.ops as ops\nTraceback (most recent call last):\n  File \"\", line 1, in \nAttributeError: 'module' object has no attribute 'python'\n. Thanks! Now I'm getting this error:\n\n\n\nTraceback (most recent call last):\n  File \"/home/michael/tensorpack/examples/ResNet/imagenet-resnet.py\", line 144, in <module>\n    launch_train_with_config(config, trainer)\n  File \"/home/michael/tensorpack/tensorpack/train/interface.py\", line 87, in launch_train_with_config\n    model._build_graph_get_cost, model.get_optimizer)\n  File \"/home/michael/tensorpack/tensorpack/utils/argtools.py\", line 182, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/michael/tensorpack/tensorpack/train/tower.py\", line 161, in setup_graph\n    train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)\n  File \"/home/michael/tensorpack/tensorpack/train/trainers.py\", line 155, in _setup_graph\n    self._make_get_grad_fn(input, get_cost_fn, get_opt_fn), get_opt_fn)\n  File \"/home/michael/tensorpack/tensorpack/graph_builder/training.py\", line 217, in build\n    use_vs=[False] + [True] * (len(self.towers) - 1))\n  File \"/home/michael/tensorpack/tensorpack/graph_builder/training.py\", line 113, in build_on_towers\n    ret.append(func())\n  File \"/home/michael/tensorpack/tensorpack/train/tower.py\", line 199, in get_grad_fn\n    aggregation_method=self.AGGREGATION_METHOD)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 460, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/home/michael/tensorpack/examples/ResNet/resnet_model.py\", line 13, in gradients_memory\n    return memory_saving_gradients.gradients(ys, xs, grad_ys, checkpoints='memory', **kwargs)\n  File \"/usr/lib/python2.7/memory_saving_gradients.py\", line 141, in gradients\n    raise Exception('unable to find bottleneck tensors! please provide checkpoint nodes manually, or use checkpoints=\"speed\".')\nException: unable to find bottleneck tensors! please provide checkpoint nodes manually, or use checkpoints=\"speed\".\nI tried changing to checkpoints='speed', but then it seems like no checkpoints are being made (the model runs out of memory). \nNot sure if this is a gradient-checkpointing issue, or a tensorpack issue. . @yaroslavvb but why did it fail? I'm trying to run default Resnet-18, similar to your deep_imagenet_benchmark.py setup. \nWhat would be a list of proper nodes to checkpoint for ResNet?. @yaroslavvb Ok, I just specified a list of tensors manually like this:\nimport memory_saving_gradients\nfrom tensorflow.python.ops import gradients\ndef gradients_memory(ys, xs, grad_ys=None, **kwargs):\n    tensors = ['tower0/conv0/Relu:0', 'tower0/group0/block0/conv1/Relu:0']\n    return memory_saving_gradients.gradients(ys, xs, grad_ys, checkpoints=tensors, **kwargs)\ngradients.__dict__[\"gradients\"] = gradients_memory\nHowever, it ignores them:\nTraceback (most recent call last):\n  File \"/home/michael/tensorpack/examples/ResNet/imagenet-resnet.py\", line 150, in <module>\n    launch_train_with_config(config, trainer)\n  File \"/home/michael/tensorpack/tensorpack/train/interface.py\", line 87, in launch_train_with_config\n    model._build_graph_get_cost, model.get_optimizer)\n  File \"/home/michael/tensorpack/tensorpack/utils/argtools.py\", line 182, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/michael/tensorpack/tensorpack/train/tower.py\", line 161, in setup_graph\n    train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)\n  File \"/home/michael/tensorpack/tensorpack/train/trainers.py\", line 155, in _setup_graph\n    self._make_get_grad_fn(input, get_cost_fn, get_opt_fn), get_opt_fn)\n  File \"/home/michael/tensorpack/tensorpack/graph_builder/training.py\", line 217, in build\n    use_vs=[False] + [True] * (len(self.towers) - 1))\n  File \"/home/michael/tensorpack/tensorpack/graph_builder/training.py\", line 113, in build_on_towers\n    ret.append(func())\n  File \"/home/michael/tensorpack/tensorpack/train/tower.py\", line 199, in get_grad_fn\n    aggregation_method=self.AGGREGATION_METHOD)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 460, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/home/michael/tensorpack/examples/ResNet/resnet_model.py\", line 27, in gradients_memory\n    return memory_saving_gradients.gradients(ys, xs, grad_ys, checkpoints=tensors, **kwargs)\n  File \"/usr/lib/python2.7/memory_saving_gradients.py\", line 184, in gradients\n    raise Exception('no checkpoints nodes found or given as input! ')\nException: no checkpoints nodes found or given as input!\n. @yaroslavvb I tried it and got exactly the same error as above. Why the above method does not work (supplying tensors list to checkpoints arg)? . OK, let's change the message then. Also, the checkpoint names are not very descriptive. It would probably be better to let a user enter a name for a backup/new checkpoint.. Yes. For example, right now:\nif act == 'b':\n            backup_name = dirname + _get_time_str()\n            shutil.move(dirname, backup_name)\nAfter multiple runs with different hyperparams it's hard to determine which backup refers to which run. I have to look into log.log.\nIt would be better if instead I could simply enter by hand the name of the directory that reflects this particular choice of hyperparams, so that when I want to load a model I can immediately see which checkpoint has it.\nNot a big deal though, just a thought.. I agree in general, but in this case you already ask for user input. My suggestion is to ask for the dir name, if the action answer is \"backup\" or \"new\". If you don't think that's useful, fine, let's just change the \"starting from epoch number n\" message and close this ticket.\nBy the way, I'm curious, what issues did you mean when you said: \"I'm not a big fan of this epoch-loading feature since it has caused other troubles in the past.\". Actually, the option \"keep\" does not make sense to me. Why is it needed? \n. Oh, I see. This should probably be mentioned somewhere in documentation.. Seems like you already extract features as 'prelogits' tensor. Now you just need to write a callback where you do something with that tensor after each k iterations or each k epochs.. 1.7M number refers to CIFAR10 architecture, which is a lot smaller than Imagenet version.. ",
    "allenlavoie": "FWIW Grappler's memory optimizer gets reasonable memory savings on many models already (20%ish?) from recomputing \"cheap\" ops like batch norm. May be worth flipping the config option in the RunConfig passed to your Session to check.\nIf you're using ops which aren't in my beautiful hard-coded list you might need to either add \"_recompute_hint\" or append to the list.\nNot full checkpointing, though, certainly.. @michaelklachko looks like that's just \"Sum\" and is not currently on the list. Feel free to add it and ping me on a PR.. That's a unit test file, so presumably it gets excluded from the pip package to save space. You should be able to download / check it out and run it outside the repo if you want.. ",
    "Arturus": "This is non-trivial amount of imports just to make barebone example IDE-friendly:\npython\nfrom tensorpack.graph_builder.model_desc import ModelDesc\nfrom tensorpack.graph_builder.model_desc import InputDesc\nfrom tensorpack.dataflow.raw import FakeData\nfrom tensorpack.dataflow.common import BatchData\nfrom tensorpack.dataflow.parallel import PrefetchDataZMQ\nfrom tensorpack.train.config import TrainConfig\nfrom tensorpack.input_source.input_source import QueueInput\nfrom tensorpack.callbacks.saver import ModelSaver\nfrom tensorpack.callbacks.inference_runner import InferenceRunner\nfrom tensorpack.callbacks.inference import ScalarStats\nfrom tensorpack.tfutils.sessinit import SaverRestore\nfrom tensorpack.train.trainers import SimpleTrainer\nfrom tensorpack.train.interface import launch_train_with_config\nimport tensorpack.utils.logger as logger\nimport tensorpack.tfutils.summary as summary. This static hack really made my PyCharm happy. Single line from tensorpack import * is enough. \nThank you!. ",
    "ai2010": "Thanks. Right I can only set the interval of checkpoints in terms of numbers of epochs... right?. thanks!. ",
    "A-Specker": "Your trainer can ignore loaded variables, it could look something like:\nsession_init=SaverRestore(args.load, ignore=['global_step']). ",
    "LandyGuo": "box_loss is defined at: https://github.com/ppwwyyxx/tensorpack/blob/master/examples/FasterRCNN/model.py#L429\nsince huber loss in tensorflow is defined as :\n\nit's robust and  will never cause NaN issues.\nand tf.to_float(tf.shape(labels)[0]) in L431 is constant 256 since config.FASTRCNN_BATCH_PER_IM is set as 256.\nso it will nerver be NaN whether with or without  valid foreground boxes, am I right ?  Is it possible the NaN issue is caused by moving average calculation?\n. Thanks I see. Close it now. thanks , I'll share if I get better results while playing with parameters. ",
    "crazysal": "Could you please comment on \"this is how tensorflow works\" ? \nI understood you want to take a relative floating point coordinate with an origin wrt the current cell, but how does tf know the relative cell from the original position in the grid ?  Or just direct me to the part of the source code. Do you mean the void CropAndResizePerBox() in the C src ? \nI'm a bit confused on how the geometry works in calculating the y_in and x_in values in the nested loop.. ",
    "tjulyz": "I didn't change other codes but only the codes for reading training data from LMDB file. The codes are copyed from your tutorials and shown as above. \nI use the commond:  ./imagenet-resnet.py --data /path/to/original/ILSVRC --gpu 0,1,2,3 -d 50 --mode resnet. Sorry about that. In my running codes,        ds = BatchData(ds, batch_size). \n1. \n\n\n\nI run the two on the same computer with same environment: TF1.40, Python2.7, and install the tensorpack use: pip install -U git+https://github.com/ppwwyyxx/tensorpack.git.\n. What the manner do you choose for training your resnet models? With LMDB file or reading from list randomly? \nBesides, is the performance reported in your README of Resnet evaluated on validation dataset without pre-activation?\nThanks a lot!. I have checked the generated LMDB. It is about 149.7GB.\nI will change the environment and try it again.\nThank you for your share and kind reply!\nThanks a lot!. ",
    "junsukchoe": "I update my tensorflow, and it works well.\nThanks you!. Thank you!. Thank you for reply!\nI have another question.\nI am confused with 'LocallyShuffleData'. What does 'buffer_size' mean?\nDoes it mean the number of images? \nSo, If I set the 'buffer_size' of LocallyShuffleData to the number of training data (e.g. 1,281,167 for ImageNet), then the shuffle occurs every one epoch?\n. Thanks a lot! I finally understand that.\nUnfortunately, I found that LMDB deserialization is the bottleneck.\nSo, I try to implement the dataflow in a parallel way.\nFor example:\n        from tensorpack.utils.compatible_serialize import loads\n\n        ds = LMDBData(datadir, shuffle=False)\n        ds = MultiThreadMapData(ds, nr_thread=25,\n                                map_func = lambda dp: loads(dp[1]))\n        ds = LocallyShuffleData(ds, 50000)\n        ds = PrefetchData(ds, 5000, 1)\n        ds = MapDataComponent(ds, lambda x: cv2.imdecode(x, cv2.IMREAD_COLOR), 0)\n        ds = AugmentImageCoordinates(ds, augmentors, coords_index=2, copy=False)\n        ds = PrefetchDataZMQ(ds, 40)\n        ds = BatchData(ds, 256, remainder=not isTrain)\n\nAt this time, I am worried about this multithread implementation. \nIf the MultiThreadMapData forks the LMDBData, then all threads are expected to output the same datapoint. \nIs this true? \nSorry for my poor background about parallel implementation.\n. Yes, I also have found that mutli thread implementation is not much faster than single thread implementation.\nThanks for kind reply! It really helps a lot.. ",
    "shdyn": "Thank you for your advice. Is there an example about this? This will be\nmore helpful.\nOn Tue, Feb 27, 2018 at 11:38 AM, Yuxin Wu notifications@github.com wrote:\n\nIf you want to do something during training, see above comments.\nIf you want to do something after training, see\nhttp://tensorpack.readthedocs.io/en/latest/tutorial/inference.html.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/ppwwyyxx/tensorpack/issues/677#issuecomment-368941586,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AfQ3Tq1y0Q-NwsSG8KyN_7mBKzz5GgZvks5tZC-ggaJpZM4SVCG0\n.\n. Thank you very much.\n\nOn Tue, Feb 27, 2018 at 4:09 PM, Yuxin Wu notifications@github.com wrote:\n\nTo do inference during training, https://github.com/ppwwyyxx/\ntensorpack/blob/26d792d490cea55855f10e9f02da738726eb664b/examples/GAN/\nCycleGAN.py#L185-L201.\nTo do inference after training, https://github.com/ppwwyyxx/\ntensorpack/blob/26d792d490cea55855f10e9f02da738726eb664b/examples/HED/hed.\npy#L217-L228\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/ppwwyyxx/tensorpack/issues/677#issuecomment-369026786,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AfQ3TneipVgikcJnVjEhCo68wvEu46q4ks5tZG8GgaJpZM4SVCG0\n.\n. Sorry for the basic questions.\nFrom tensorboard, I can clearly see there is a node named \"gap\" in the graph. But when I print the available nodes in the checkpoint. There is no \"gap\" node. The following is the available variable names in the graph.\n/usr/bin/python2.7 /home/dk/james/tf-based/ls-checkpoint-tensorpack.py\nFailed to load OpenCL runtime\n[0301 20:09:14 @varmanip.py:165] WRN Checkpoint path /home/dk/james/tf-based/resnet50/20180216-182737/model-2321408.data-00000-of-00001 is auto-corrected to /home/dk/james/tf-based/resnet50/20180216-182737/model-2321408.\n{'EMA/QueueInput/queue_size': [],\n 'EMA/QueueInput/queue_size/biased': [],\n 'EMA/QueueInput/queue_size/local_step': [],\n 'EMA/cost': [],\n 'EMA/cost/biased': [],\n 'EMA/cost/local_step': [],\n 'EMA/cross_entropy': [],\n 'EMA/cross_entropy/biased': [],\n 'EMA/cross_entropy/local_step': [],\n 'EMA/train_error': [],\n 'EMA/train_error/biased': [],\n 'EMA/train_error/local_step': [],\n 'Logits/biases': [10575],\n 'Logits/biases/Momentum': [10575],\n 'Logits/weights': [2048, 10575],\n 'Logits/weights/Momentum': [2048, 10575],\n 'conv1/BatchNorm/beta': [64],\n 'conv1/BatchNorm/beta/Momentum': [64],\n 'conv1/BatchNorm/moving_mean': [64],\n 'conv1/BatchNorm/moving_variance': [64],\n 'conv1/weights': [7, 7, 3, 64],\n 'conv1/weights/Momentum': [7, 7, 3, 64],\n 'conv2_1_branch1/BatchNorm/beta': [256],\n 'conv2_1_branch1/BatchNorm/beta/Momentum': [256],\n 'conv2_1_branch1/BatchNorm/moving_mean': [256],\n 'conv2_1_branch1/BatchNorm/moving_variance': [256],\n 'conv2_1_branch1/weights': [1, 1, 64, 256],\n 'conv2_1_branch1/weights/Momentum': [1, 1, 64, 256],\n 'conv2_1_branch2_a/BatchNorm/beta': [64],\n 'conv2_1_branch2_a/BatchNorm/beta/Momentum': [64],\n 'conv2_1_branch2_a/BatchNorm/moving_mean': [64],\n 'conv2_1_branch2_a/BatchNorm/moving_variance': [64],\n 'conv2_1_branch2_a/weights': [1, 1, 64, 64],\n 'conv2_1_branch2_a/weights/Momentum': [1, 1, 64, 64],\n 'conv2_1_branch2_b/BatchNorm/beta': [64],\n 'conv2_1_branch2_b/BatchNorm/beta/Momentum': [64],\n 'conv2_1_branch2_b/BatchNorm/moving_mean': [64],\n 'conv2_1_branch2_b/BatchNorm/moving_variance': [64],\n 'conv2_1_branch2_b/weights': [3, 3, 64, 64],\n 'conv2_1_branch2_b/weights/Momentum': [3, 3, 64, 64],\n 'conv2_1_branch2_c/BatchNorm/beta': [256],\n 'conv2_1_branch2_c/BatchNorm/beta/Momentum': [256],\n 'conv2_1_branch2_c/BatchNorm/moving_mean': [256],\n 'conv2_1_branch2_c/BatchNorm/moving_variance': [256],\n 'conv2_1_branch2_c/weights': [1, 1, 64, 256],\n 'conv2_1_branch2_c/weights/Momentum': [1, 1, 64, 256],\n 'conv2_2/conv2_2_1_identity_a/BatchNorm/beta': [64],\n 'conv2_2/conv2_2_1_identity_a/BatchNorm/beta/Momentum': [64],\n 'conv2_2/conv2_2_1_identity_a/BatchNorm/moving_mean': [64],\n 'conv2_2/conv2_2_1_identity_a/BatchNorm/moving_variance': [64],\n 'conv2_2/conv2_2_1_identity_a/weights': [1, 1, 256, 64],\n 'conv2_2/conv2_2_1_identity_a/weights/Momentum': [1, 1, 256, 64],\n 'conv2_2/conv2_2_1_identity_b/BatchNorm/beta': [64],\n 'conv2_2/conv2_2_1_identity_b/BatchNorm/beta/Momentum': [64],\n 'conv2_2/conv2_2_1_identity_b/BatchNorm/moving_mean': [64],\n 'conv2_2/conv2_2_1_identity_b/BatchNorm/moving_variance': [64],\n 'conv2_2/conv2_2_1_identity_b/weights': [3, 3, 64, 64],\n 'conv2_2/conv2_2_1_identity_b/weights/Momentum': [3, 3, 64, 64],\n 'conv2_2/conv2_2_1_identity_c/BatchNorm/beta': [256],\n 'conv2_2/conv2_2_1_identity_c/BatchNorm/beta/Momentum': [256],\n 'conv2_2/conv2_2_1_identity_c/BatchNorm/moving_mean': [256],\n 'conv2_2/conv2_2_1_identity_c/BatchNorm/moving_variance': [256],\n 'conv2_2/conv2_2_1_identity_c/weights': [1, 1, 64, 256],\n 'conv2_2/conv2_2_1_identity_c/weights/Momentum': [1, 1, 64, 256],\n 'conv2_2/conv2_2_2_identity_a/BatchNorm/beta': [64],\n 'conv2_2/conv2_2_2_identity_a/BatchNorm/beta/Momentum': [64],\n 'conv2_2/conv2_2_2_identity_a/BatchNorm/moving_mean': [64],\n 'conv2_2/conv2_2_2_identity_a/BatchNorm/moving_variance': [64],\n 'conv2_2/conv2_2_2_identity_a/weights': [1, 1, 256, 64],\n 'conv2_2/conv2_2_2_identity_a/weights/Momentum': [1, 1, 256, 64],\n 'conv2_2/conv2_2_2_identity_b/BatchNorm/beta': [64],\n 'conv2_2/conv2_2_2_identity_b/BatchNorm/beta/Momentum': [64],\n 'conv2_2/conv2_2_2_identity_b/BatchNorm/moving_mean': [64],\n 'conv2_2/conv2_2_2_identity_b/BatchNorm/moving_variance': [64],\n 'conv2_2/conv2_2_2_identity_b/weights': [3, 3, 64, 64],\n 'conv2_2/conv2_2_2_identity_b/weights/Momentum': [3, 3, 64, 64],\n 'conv2_2/conv2_2_2_identity_c/BatchNorm/beta': [256],\n 'conv2_2/conv2_2_2_identity_c/BatchNorm/beta/Momentum': [256],\n 'conv2_2/conv2_2_2_identity_c/BatchNorm/moving_mean': [256],\n 'conv2_2/conv2_2_2_identity_c/BatchNorm/moving_variance': [256],\n 'conv2_2/conv2_2_2_identity_c/weights': [1, 1, 64, 256],\n 'conv2_2/conv2_2_2_identity_c/weights/Momentum': [1, 1, 64, 256],\n 'conv3_1_branch1/BatchNorm/beta': [512],\n 'conv3_1_branch1/BatchNorm/beta/Momentum': [512],\n 'conv3_1_branch1/BatchNorm/moving_mean': [512],\n 'conv3_1_branch1/BatchNorm/moving_variance': [512],\n 'conv3_1_branch1/weights': [1, 1, 256, 512],\n 'conv3_1_branch1/weights/Momentum': [1, 1, 256, 512],\n 'conv3_1_branch2_a/BatchNorm/beta': [128],\n 'conv3_1_branch2_a/BatchNorm/beta/Momentum': [128],\n 'conv3_1_branch2_a/BatchNorm/moving_mean': [128],\n 'conv3_1_branch2_a/BatchNorm/moving_variance': [128],\n 'conv3_1_branch2_a/weights': [1, 1, 256, 128],\n 'conv3_1_branch2_a/weights/Momentum': [1, 1, 256, 128],\n 'conv3_1_branch2_b/BatchNorm/beta': [128],\n 'conv3_1_branch2_b/BatchNorm/beta/Momentum': [128],\n 'conv3_1_branch2_b/BatchNorm/moving_mean': [128],\n 'conv3_1_branch2_b/BatchNorm/moving_variance': [128],\n 'conv3_1_branch2_b/weights': [3, 3, 128, 128],\n 'conv3_1_branch2_b/weights/Momentum': [3, 3, 128, 128],\n 'conv3_1_branch2_c/BatchNorm/beta': [512],\n 'conv3_1_branch2_c/BatchNorm/beta/Momentum': [512],\n 'conv3_1_branch2_c/BatchNorm/moving_mean': [512],\n 'conv3_1_branch2_c/BatchNorm/moving_variance': [512],\n 'conv3_1_branch2_c/weights': [1, 1, 128, 512],\n 'conv3_1_branch2_c/weights/Momentum': [1, 1, 128, 512],\n 'conv3_2/conv3_2_1_identity_a/BatchNorm/beta': [128],\n 'conv3_2/conv3_2_1_identity_a/BatchNorm/beta/Momentum': [128],\n 'conv3_2/conv3_2_1_identity_a/BatchNorm/moving_mean': [128],\n 'conv3_2/conv3_2_1_identity_a/BatchNorm/moving_variance': [128],\n 'conv3_2/conv3_2_1_identity_a/weights': [1, 1, 512, 128],\n 'conv3_2/conv3_2_1_identity_a/weights/Momentum': [1, 1, 512, 128],\n 'conv3_2/conv3_2_1_identity_b/BatchNorm/beta': [128],\n 'conv3_2/conv3_2_1_identity_b/BatchNorm/beta/Momentum': [128],\n 'conv3_2/conv3_2_1_identity_b/BatchNorm/moving_mean': [128],\n 'conv3_2/conv3_2_1_identity_b/BatchNorm/moving_variance': [128],\n 'conv3_2/conv3_2_1_identity_b/weights': [3, 3, 128, 128],\n 'conv3_2/conv3_2_1_identity_b/weights/Momentum': [3, 3, 128, 128],\n 'conv3_2/conv3_2_1_identity_c/BatchNorm/beta': [512],\n 'conv3_2/conv3_2_1_identity_c/BatchNorm/beta/Momentum': [512],\n 'conv3_2/conv3_2_1_identity_c/BatchNorm/moving_mean': [512],\n 'conv3_2/conv3_2_1_identity_c/BatchNorm/moving_variance': [512],\n 'conv3_2/conv3_2_1_identity_c/weights': [1, 1, 128, 512],\n 'conv3_2/conv3_2_1_identity_c/weights/Momentum': [1, 1, 128, 512],\n 'conv3_2/conv3_2_2_identity_a/BatchNorm/beta': [128],\n 'conv3_2/conv3_2_2_identity_a/BatchNorm/beta/Momentum': [128],\n 'conv3_2/conv3_2_2_identity_a/BatchNorm/moving_mean': [128],\n 'conv3_2/conv3_2_2_identity_a/BatchNorm/moving_variance': [128],\n 'conv3_2/conv3_2_2_identity_a/weights': [1, 1, 512, 128],\n 'conv3_2/conv3_2_2_identity_a/weights/Momentum': [1, 1, 512, 128],\n 'conv3_2/conv3_2_2_identity_b/BatchNorm/beta': [128],\n 'conv3_2/conv3_2_2_identity_b/BatchNorm/beta/Momentum': [128],\n 'conv3_2/conv3_2_2_identity_b/BatchNorm/moving_mean': [128],\n 'conv3_2/conv3_2_2_identity_b/BatchNorm/moving_variance': [128],\n 'conv3_2/conv3_2_2_identity_b/weights': [3, 3, 128, 128],\n 'conv3_2/conv3_2_2_identity_b/weights/Momentum': [3, 3, 128, 128],\n 'conv3_2/conv3_2_2_identity_c/BatchNorm/beta': [512],\n 'conv3_2/conv3_2_2_identity_c/BatchNorm/beta/Momentum': [512],\n 'conv3_2/conv3_2_2_identity_c/BatchNorm/moving_mean': [512],\n 'conv3_2/conv3_2_2_identity_c/BatchNorm/moving_variance': [512],\n 'conv3_2/conv3_2_2_identity_c/weights': [1, 1, 128, 512],\n 'conv3_2/conv3_2_2_identity_c/weights/Momentum': [1, 1, 128, 512],\n 'conv3_2/conv3_2_3_identity_a/BatchNorm/beta': [128],\n 'conv3_2/conv3_2_3_identity_a/BatchNorm/beta/Momentum': [128],\n 'conv3_2/conv3_2_3_identity_a/BatchNorm/moving_mean': [128],\n 'conv3_2/conv3_2_3_identity_a/BatchNorm/moving_variance': [128],\n 'conv3_2/conv3_2_3_identity_a/weights': [1, 1, 512, 128],\n 'conv3_2/conv3_2_3_identity_a/weights/Momentum': [1, 1, 512, 128],\n 'conv3_2/conv3_2_3_identity_b/BatchNorm/beta': [128],\n 'conv3_2/conv3_2_3_identity_b/BatchNorm/beta/Momentum': [128],\n 'conv3_2/conv3_2_3_identity_b/BatchNorm/moving_mean': [128],\n 'conv3_2/conv3_2_3_identity_b/BatchNorm/moving_variance': [128],\n 'conv3_2/conv3_2_3_identity_b/weights': [3, 3, 128, 128],\n 'conv3_2/conv3_2_3_identity_b/weights/Momentum': [3, 3, 128, 128],\n 'conv3_2/conv3_2_3_identity_c/BatchNorm/beta': [512],\n 'conv3_2/conv3_2_3_identity_c/BatchNorm/beta/Momentum': [512],\n 'conv3_2/conv3_2_3_identity_c/BatchNorm/moving_mean': [512],\n 'conv3_2/conv3_2_3_identity_c/BatchNorm/moving_variance': [512],\n 'conv3_2/conv3_2_3_identity_c/weights': [1, 1, 128, 512],\n 'conv3_2/conv3_2_3_identity_c/weights/Momentum': [1, 1, 128, 512],\n 'conv4_1_branch1/BatchNorm/beta': [1024],\n 'conv4_1_branch1/BatchNorm/beta/Momentum': [1024],\n 'conv4_1_branch1/BatchNorm/moving_mean': [1024],\n 'conv4_1_branch1/BatchNorm/moving_variance': [1024],\n 'conv4_1_branch1/weights': [1, 1, 512, 1024],\n 'conv4_1_branch1/weights/Momentum': [1, 1, 512, 1024],\n 'conv4_1_branch2_a/BatchNorm/beta': [256],\n 'conv4_1_branch2_a/BatchNorm/beta/Momentum': [256],\n 'conv4_1_branch2_a/BatchNorm/moving_mean': [256],\n 'conv4_1_branch2_a/BatchNorm/moving_variance': [256],\n 'conv4_1_branch2_a/weights': [1, 1, 512, 256],\n 'conv4_1_branch2_a/weights/Momentum': [1, 1, 512, 256],\n 'conv4_1_branch2_b/BatchNorm/beta': [256],\n 'conv4_1_branch2_b/BatchNorm/beta/Momentum': [256],\n 'conv4_1_branch2_b/BatchNorm/moving_mean': [256],\n 'conv4_1_branch2_b/BatchNorm/moving_variance': [256],\n 'conv4_1_branch2_b/weights': [3, 3, 256, 256],\n 'conv4_1_branch2_b/weights/Momentum': [3, 3, 256, 256],\n 'conv4_1_branch2_c/BatchNorm/beta': [1024],\n 'conv4_1_branch2_c/BatchNorm/beta/Momentum': [1024],\n 'conv4_1_branch2_c/BatchNorm/moving_mean': [1024],\n 'conv4_1_branch2_c/BatchNorm/moving_variance': [1024],\n 'conv4_1_branch2_c/weights': [1, 1, 256, 1024],\n 'conv4_1_branch2_c/weights/Momentum': [1, 1, 256, 1024],\n 'conv4_2/conv4_2_1_identity_a/BatchNorm/beta': [256],\n 'conv4_2/conv4_2_1_identity_a/BatchNorm/beta/Momentum': [256],\n 'conv4_2/conv4_2_1_identity_a/BatchNorm/moving_mean': [256],\n 'conv4_2/conv4_2_1_identity_a/BatchNorm/moving_variance': [256],\n 'conv4_2/conv4_2_1_identity_a/weights': [1, 1, 1024, 256],\n 'conv4_2/conv4_2_1_identity_a/weights/Momentum': [1, 1, 1024, 256],\n 'conv4_2/conv4_2_1_identity_b/BatchNorm/beta': [256],\n 'conv4_2/conv4_2_1_identity_b/BatchNorm/beta/Momentum': [256],\n 'conv4_2/conv4_2_1_identity_b/BatchNorm/moving_mean': [256],\n 'conv4_2/conv4_2_1_identity_b/BatchNorm/moving_variance': [256],\n 'conv4_2/conv4_2_1_identity_b/weights': [3, 3, 256, 256],\n 'conv4_2/conv4_2_1_identity_b/weights/Momentum': [3, 3, 256, 256],\n 'conv4_2/conv4_2_1_identity_c/BatchNorm/beta': [1024],\n 'conv4_2/conv4_2_1_identity_c/BatchNorm/beta/Momentum': [1024],\n 'conv4_2/conv4_2_1_identity_c/BatchNorm/moving_mean': [1024],\n 'conv4_2/conv4_2_1_identity_c/BatchNorm/moving_variance': [1024],\n 'conv4_2/conv4_2_1_identity_c/weights': [1, 1, 256, 1024],\n 'conv4_2/conv4_2_1_identity_c/weights/Momentum': [1, 1, 256, 1024],\n 'conv4_2/conv4_2_2_identity_a/BatchNorm/beta': [256],\n 'conv4_2/conv4_2_2_identity_a/BatchNorm/beta/Momentum': [256],\n 'conv4_2/conv4_2_2_identity_a/BatchNorm/moving_mean': [256],\n 'conv4_2/conv4_2_2_identity_a/BatchNorm/moving_variance': [256],\n 'conv4_2/conv4_2_2_identity_a/weights': [1, 1, 1024, 256],\n 'conv4_2/conv4_2_2_identity_a/weights/Momentum': [1, 1, 1024, 256],\n 'conv4_2/conv4_2_2_identity_b/BatchNorm/beta': [256],\n 'conv4_2/conv4_2_2_identity_b/BatchNorm/beta/Momentum': [256],\n 'conv4_2/conv4_2_2_identity_b/BatchNorm/moving_mean': [256],\n 'conv4_2/conv4_2_2_identity_b/BatchNorm/moving_variance': [256],\n 'conv4_2/conv4_2_2_identity_b/weights': [3, 3, 256, 256],\n 'conv4_2/conv4_2_2_identity_b/weights/Momentum': [3, 3, 256, 256],\n 'conv4_2/conv4_2_2_identity_c/BatchNorm/beta': [1024],\n 'conv4_2/conv4_2_2_identity_c/BatchNorm/beta/Momentum': [1024],\n 'conv4_2/conv4_2_2_identity_c/BatchNorm/moving_mean': [1024],\n 'conv4_2/conv4_2_2_identity_c/BatchNorm/moving_variance': [1024],\n 'conv4_2/conv4_2_2_identity_c/weights': [1, 1, 256, 1024],\n 'conv4_2/conv4_2_2_identity_c/weights/Momentum': [1, 1, 256, 1024],\n 'conv4_2/conv4_2_3_identity_a/BatchNorm/beta': [256],\n 'conv4_2/conv4_2_3_identity_a/BatchNorm/beta/Momentum': [256],\n 'conv4_2/conv4_2_3_identity_a/BatchNorm/moving_mean': [256],\n 'conv4_2/conv4_2_3_identity_a/BatchNorm/moving_variance': [256],\n 'conv4_2/conv4_2_3_identity_a/weights': [1, 1, 1024, 256],\n 'conv4_2/conv4_2_3_identity_a/weights/Momentum': [1, 1, 1024, 256],\n 'conv4_2/conv4_2_3_identity_b/BatchNorm/beta': [256],\n 'conv4_2/conv4_2_3_identity_b/BatchNorm/beta/Momentum': [256],\n 'conv4_2/conv4_2_3_identity_b/BatchNorm/moving_mean': [256],\n 'conv4_2/conv4_2_3_identity_b/BatchNorm/moving_variance': [256],\n 'conv4_2/conv4_2_3_identity_b/weights': [3, 3, 256, 256],\n 'conv4_2/conv4_2_3_identity_b/weights/Momentum': [3, 3, 256, 256],\n 'conv4_2/conv4_2_3_identity_c/BatchNorm/beta': [1024],\n 'conv4_2/conv4_2_3_identity_c/BatchNorm/beta/Momentum': [1024],\n 'conv4_2/conv4_2_3_identity_c/BatchNorm/moving_mean': [1024],\n 'conv4_2/conv4_2_3_identity_c/BatchNorm/moving_variance': [1024],\n 'conv4_2/conv4_2_3_identity_c/weights': [1, 1, 256, 1024],\n 'conv4_2/conv4_2_3_identity_c/weights/Momentum': [1, 1, 256, 1024],\n 'conv4_2/conv4_2_4_identity_a/BatchNorm/beta': [256],\n 'conv4_2/conv4_2_4_identity_a/BatchNorm/beta/Momentum': [256],\n 'conv4_2/conv4_2_4_identity_a/BatchNorm/moving_mean': [256],\n 'conv4_2/conv4_2_4_identity_a/BatchNorm/moving_variance': [256],\n 'conv4_2/conv4_2_4_identity_a/weights': [1, 1, 1024, 256],\n 'conv4_2/conv4_2_4_identity_a/weights/Momentum': [1, 1, 1024, 256],\n 'conv4_2/conv4_2_4_identity_b/BatchNorm/beta': [256],\n 'conv4_2/conv4_2_4_identity_b/BatchNorm/beta/Momentum': [256],\n 'conv4_2/conv4_2_4_identity_b/BatchNorm/moving_mean': [256],\n 'conv4_2/conv4_2_4_identity_b/BatchNorm/moving_variance': [256],\n 'conv4_2/conv4_2_4_identity_b/weights': [3, 3, 256, 256],\n 'conv4_2/conv4_2_4_identity_b/weights/Momentum': [3, 3, 256, 256],\n 'conv4_2/conv4_2_4_identity_c/BatchNorm/beta': [1024],\n 'conv4_2/conv4_2_4_identity_c/BatchNorm/beta/Momentum': [1024],\n 'conv4_2/conv4_2_4_identity_c/BatchNorm/moving_mean': [1024],\n 'conv4_2/conv4_2_4_identity_c/BatchNorm/moving_variance': [1024],\n 'conv4_2/conv4_2_4_identity_c/weights': [1, 1, 256, 1024],\n 'conv4_2/conv4_2_4_identity_c/weights/Momentum': [1, 1, 256, 1024],\n 'conv4_2/conv4_2_5_identity_a/BatchNorm/beta': [256],\n 'conv4_2/conv4_2_5_identity_a/BatchNorm/beta/Momentum': [256],\n 'conv4_2/conv4_2_5_identity_a/BatchNorm/moving_mean': [256],\n 'conv4_2/conv4_2_5_identity_a/BatchNorm/moving_variance': [256],\n 'conv4_2/conv4_2_5_identity_a/weights': [1, 1, 1024, 256],\n 'conv4_2/conv4_2_5_identity_a/weights/Momentum': [1, 1, 1024, 256],\n 'conv4_2/conv4_2_5_identity_b/BatchNorm/beta': [256],\n 'conv4_2/conv4_2_5_identity_b/BatchNorm/beta/Momentum': [256],\n 'conv4_2/conv4_2_5_identity_b/BatchNorm/moving_mean': [256],\n 'conv4_2/conv4_2_5_identity_b/BatchNorm/moving_variance': [256],\n 'conv4_2/conv4_2_5_identity_b/weights': [3, 3, 256, 256],\n 'conv4_2/conv4_2_5_identity_b/weights/Momentum': [3, 3, 256, 256],\n 'conv4_2/conv4_2_5_identity_c/BatchNorm/beta': [1024],\n 'conv4_2/conv4_2_5_identity_c/BatchNorm/beta/Momentum': [1024],\n 'conv4_2/conv4_2_5_identity_c/BatchNorm/moving_mean': [1024],\n 'conv4_2/conv4_2_5_identity_c/BatchNorm/moving_variance': [1024],\n 'conv4_2/conv4_2_5_identity_c/weights': [1, 1, 256, 1024],\n 'conv4_2/conv4_2_5_identity_c/weights/Momentum': [1, 1, 256, 1024],\n 'conv5_1_branch1/BatchNorm/beta': [2048],\n 'conv5_1_branch1/BatchNorm/beta/Momentum': [2048],\n 'conv5_1_branch1/BatchNorm/moving_mean': [2048],\n 'conv5_1_branch1/BatchNorm/moving_variance': [2048],\n 'conv5_1_branch1/weights': [1, 1, 1024, 2048],\n 'conv5_1_branch1/weights/Momentum': [1, 1, 1024, 2048],\n 'conv5_1_branch2_a/BatchNorm/beta': [512],\n 'conv5_1_branch2_a/BatchNorm/beta/Momentum': [512],\n 'conv5_1_branch2_a/BatchNorm/moving_mean': [512],\n 'conv5_1_branch2_a/BatchNorm/moving_variance': [512],\n 'conv5_1_branch2_a/weights': [1, 1, 1024, 512],\n 'conv5_1_branch2_a/weights/Momentum': [1, 1, 1024, 512],\n 'conv5_1_branch2_b/BatchNorm/beta': [512],\n 'conv5_1_branch2_b/BatchNorm/beta/Momentum': [512],\n 'conv5_1_branch2_b/BatchNorm/moving_mean': [512],\n 'conv5_1_branch2_b/BatchNorm/moving_variance': [512],\n 'conv5_1_branch2_b/weights': [3, 3, 512, 512],\n 'conv5_1_branch2_b/weights/Momentum': [3, 3, 512, 512],\n 'conv5_1_branch2_c/BatchNorm/beta': [2048],\n 'conv5_1_branch2_c/BatchNorm/beta/Momentum': [2048],\n 'conv5_1_branch2_c/BatchNorm/moving_mean': [2048],\n 'conv5_1_branch2_c/BatchNorm/moving_variance': [2048],\n 'conv5_1_branch2_c/weights': [1, 1, 512, 2048],\n 'conv5_1_branch2_c/weights/Momentum': [1, 1, 512, 2048],\n 'conv5_2/conv5_2_1_identity_a/BatchNorm/beta': [512],\n 'conv5_2/conv5_2_1_identity_a/BatchNorm/beta/Momentum': [512],\n 'conv5_2/conv5_2_1_identity_a/BatchNorm/moving_mean': [512],\n 'conv5_2/conv5_2_1_identity_a/BatchNorm/moving_variance': [512],\n 'conv5_2/conv5_2_1_identity_a/weights': [1, 1, 2048, 512],\n 'conv5_2/conv5_2_1_identity_a/weights/Momentum': [1, 1, 2048, 512],\n 'conv5_2/conv5_2_1_identity_b/BatchNorm/beta': [512],\n 'conv5_2/conv5_2_1_identity_b/BatchNorm/beta/Momentum': [512],\n 'conv5_2/conv5_2_1_identity_b/BatchNorm/moving_mean': [512],\n 'conv5_2/conv5_2_1_identity_b/BatchNorm/moving_variance': [512],\n 'conv5_2/conv5_2_1_identity_b/weights': [3, 3, 512, 512],\n 'conv5_2/conv5_2_1_identity_b/weights/Momentum': [3, 3, 512, 512],\n 'conv5_2/conv5_2_1_identity_c/BatchNorm/beta': [2048],\n 'conv5_2/conv5_2_1_identity_c/BatchNorm/beta/Momentum': [2048],\n 'conv5_2/conv5_2_1_identity_c/BatchNorm/moving_mean': [2048],\n 'conv5_2/conv5_2_1_identity_c/BatchNorm/moving_variance': [2048],\n 'conv5_2/conv5_2_1_identity_c/weights': [1, 1, 512, 2048],\n 'conv5_2/conv5_2_1_identity_c/weights/Momentum': [1, 1, 512, 2048],\n 'conv5_2/conv5_2_2_identity_a/BatchNorm/beta': [512],\n 'conv5_2/conv5_2_2_identity_a/BatchNorm/beta/Momentum': [512],\n 'conv5_2/conv5_2_2_identity_a/BatchNorm/moving_mean': [512],\n 'conv5_2/conv5_2_2_identity_a/BatchNorm/moving_variance': [512],\n 'conv5_2/conv5_2_2_identity_a/weights': [1, 1, 2048, 512],\n 'conv5_2/conv5_2_2_identity_a/weights/Momentum': [1, 1, 2048, 512],\n 'conv5_2/conv5_2_2_identity_b/BatchNorm/beta': [512],\n 'conv5_2/conv5_2_2_identity_b/BatchNorm/beta/Momentum': [512],\n 'conv5_2/conv5_2_2_identity_b/BatchNorm/moving_mean': [512],\n 'conv5_2/conv5_2_2_identity_b/BatchNorm/moving_variance': [512],\n 'conv5_2/conv5_2_2_identity_b/weights': [3, 3, 512, 512],\n 'conv5_2/conv5_2_2_identity_b/weights/Momentum': [3, 3, 512, 512],\n 'conv5_2/conv5_2_2_identity_c/BatchNorm/beta': [2048],\n 'conv5_2/conv5_2_2_identity_c/BatchNorm/beta/Momentum': [2048],\n 'conv5_2/conv5_2_2_identity_c/BatchNorm/moving_mean': [2048],\n 'conv5_2/conv5_2_2_identity_c/BatchNorm/moving_variance': [2048],\n 'conv5_2/conv5_2_2_identity_c/weights': [1, 1, 512, 2048],\n 'conv5_2/conv5_2_2_identity_c/weights/Momentum': [1, 1, 512, 2048],\n 'global_step': [],\n 'learning_rate': []}\n\nProcess finished with exit code 0\nThe network configuration can be seen previously. It should be noticed that I use  keras.layers.GlobalAveragePooling2D() to define global average pooling layer. So how should I extract the features in \"gap\" layer? . ",
    "souxun2015": "But my msgpack's version is 0.5.6\nRequirement already up-to-date: msgpack in ./anaconda2/lib/python2.7/site-packages/msgpack-0.5.6-py2.7-linux-x86_64.egg. Thanks a lot, I found a lower version of msgpack. I uninstall it, and it works. but the speed is very slow, my computer has two 1080s. The util of GPU is always 0.\n[0301 10:05:46 @graph.py:72] Running Op sync_variables/sync_variables_from_main_tower ...\n[0301 10:05:46 @concurrency.py:38] Starting EnqueueThread DataParallelInferenceRunner/QueueInput/input_queue ...\n[0301 10:05:47 @base.py:255] Start Epoch 1 ...\n  0%|          |0/5000[00:00<?,?it/s][0301 10:05:47 @input_source.py:496] Pre-filling StagingArea ...\n[0301 10:05:51 @input_source.py:500] Successfully put 1 element to StagingArea.\n  0%|          |12/5000[02:03<13:40:50, 0.10it/s]. OK ,thank you.. I have test the code by the fake data, the speed can reach 5.99 it/s. However, it is only 0.53 it/s using the ILSVRC12. And in this case, i set 'ds = dataset.ILSVRC12(datadir, name, shuffle=False)'.\nIt makes full use of CPU, but the util of GPU is 0 in most of time.\nI  use imagenet-resnet.py without changing.\n. The training speed.\n'ds = dataset.ILSVRC12(datadir, name, shuffle=False)'.\nWhen shuffle = False, the result is the following:\n[0301 12:56:30 @concurrency.py:38] Starting EnqueueThread QueueInput/input_queue ...\n[0301 12:56:30 @graph.py:72] Running Op sync_variables/sync_variables_from_main_tower ...\n[0301 12:56:31 @concurrency.py:38] Starting EnqueueThread DataParallelInferenceRunner/QueueInput/input_queue ...\n[0301 12:56:31 @base.py:255] Start Epoch 1 ...\n  0%|          |0/5000[00:00<?,?it/s][0301 12:56:31 @input_source.py:496] Pre-filling StagingArea ...\n[0301 12:56:32 @input_source.py:500] Successfully put 1 element to StagingArea.\n  1%|1         |66/5000[01:56<2:50:12, 0.48it/s]\n'ds = dataset.ILSVRC12(datadir, name, shuffle=True)'.\nWhen shuffle = True, the result is the following:\n[0301 12:59:26 @concurrency.py:38] Starting EnqueueThread QueueInput/input_queue ...\n[0301 12:59:26 @graph.py:72] Running Op sync_variables/sync_variables_from_main_tower ...\n[0301 12:59:27 @concurrency.py:38] Starting EnqueueThread DataParallelInferenceRunner/QueueInput/input_queue ...\n[0301 12:59:27 @base.py:255] Start Epoch 1 ...\n  0%|          |0/5000[00:00<?,?it/s][0301 12:59:27 @input_source.py:496] Pre-filling StagingArea ...\n[0301 12:59:29 @input_source.py:500] Successfully put 1 element to StagingArea.\n  0%|          |10/5000[01:06<8:40:35, 0.16it/s]\nShuffle will cost more time, I can understand this situation. But the speed is too slow. This is my computer information.\n\nThe imagenet dataset is saved in HDD disk.. Sorry for such issue. I have tested the code as you mentioned above.\n`\nds = LMDBData('/path/to/ILSVRC-train.lmdb', shuffle=False)\nds = BatchData(ds, 256, use_list=True)\n`\nThe output is:\n\nMy cpu is: one CPU E5-1620 v4, 3.5Ghz, memory is 64G.\nDoes this result indicate that my hardware is the bottleneck?\n. Ok, I get it. Thank you very much.\nAre there some suggestion for accelerating the code in this situation? . I have done these. It seems that the cpu in my computer is the bottleneck.\nWhat is suitable CPU configuration for this situation? Can you give me some advice?. Ok, thanks very much. It is very helpful. . ",
    "tingxingdong": "I want to do the reverse, from npz to ckpt. @sharpstill , Do you know is there any tool? . Why the pretrained model here is npz format (http://models.tensorpack.com/)\nWhile, TF hosted ones are pb (https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md). \nIs there anyway convert from npz to pb format? Thankyou.. We want to convert TF format to TF Lite format. According to TOCO, *npz is not supported yet, as I know. . Thanks so much. I read the file: https://github.com/tensorpack/tensorpack/blob/master/examples/basics/export-model.py\nBut I do not quite get the comments\n\ntrain the model by\n    python export.py\nexport the model by\n    python export.py --export serving --load train_log/export/checkpoint\n    python export.py --export compact --load train_log/export/checkpoint\n\nWhich export.py it refers to (maybe export-model.py)?\nSuppose I have already download the \"ShuffleNetV2-1x.npz\" ( I think I can skip step 1 training, right? but I do not have dir train_log), what should I do next in order to obtain *.pb ?  Can you give more instructions. Thank you. \n. Thanks so much for quick reply and patience. I insert the following code after line 249 of \nhttps://github.com/tensorpack/tensorpack/blob/master/examples/ImageNetModels/shufflenet.py\npred_config = PredictConfig(\n    session_init=get_model_loader(args.load),\n    model = Model()\n    #, input_names=['input']\n    #, output_names=['output']\n    )\nModelExporter(pred_config).export_compact('/tmp/compact_graph.pb')\n\nI used command\" python ./shufflenet.py --load ShuffleNetV2-1x.npz --v2 -r=1\"\nIt gvies me AssertionError:  which is model=Model() on line 252 , \nHowever, line 249 is exactly the model=Model() which passed before this block. \nAnother question is what is the input_names & and output_names should I put? I can view the *npz file by \nx.np.load()\nprint name = x.files\nIt gives me such names like \"stage/4/block3/dconv_bn/variance/EMA:0\", etc \nbut I do not know who are input names and who are output names. \n. Thanks so much. I updated my local repo by checking in your new commits. \nI worked on linux machine which blocked the github.com so it is hard to paste all my log. But that is exactly the same error like you pasted here. \nI tried several words in the ['  '], like 'linear' (from line  153, fully connected operation), 'gap' (line 153, for Pooling), , 'linear output', or 'output', etc.   from the shufflenet.py.\n, output_names=[' ']\nbut They all gives me similar error:\nKeyError: \"The name 'linear:0' refers to a Tensor which does not exist. The operation, 'linear', does not exist in the graph.\nSo I do not know what is the output_names? \n. Thanks, linear/output works. But this time, it dies in line 259. \n```\n[0103 15:08:42 @sessinit.py:217] Restoring from dict ...\nConverted 282 variables to const ops.\nTraceback (most recent call last):\n  File \"./shufflenet.py\", line 259, in \n    ModelExporter(pred_config).export_compact('/tmp/compact_graph.pb')\n  File \"/usr/local/lib/python2.7/dist-packages/tensorpack/tfutils/export.py\", line 74, in export_compact\n    False)\nTypeError: optimize_for_inference() takes exactly 4 arguments (5 given)\n```\n. I uninstall and reinstall again\nsudo pip show tensorflow \nName: tensorflow\nVersion: 1.12.0\nNow, I have different errors\n```\nTraceback (most recent call last):\n  File \"./shufflenet.py\", line 260, in \n    ModelExporter(pred_config).export_compact('./compact_graph.pb')\n  File \"/usr/local/lib/python2.7/dist-packages/tensorpack/tfutils/export.py\", line 74, in export_compact\n    False)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/tools/optimize_for_inference_lib.py\", line 111, in optimize_for_inference\n    placeholder_type_enum)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/tools/strip_unused_lib.py\", line 83, in strip_unused\n    raise KeyError(\"The following input nodes were not found: %s\\n\" % not_found)\nKeyError: \"The following input nodes were not found: set([u'label'])\\n\"\n```\nCan you upload .ckpt & pb file somewhere? Maybe it is more easier. \nThank you. \n. By ,input_names=['input'], it works, and I can get ./compact_graph.pb. \nYet, I am not able to use TOCO (which is tflite_convert, the same thing just renamed)\n```\ntflite_convert --output_file=./shufflenet.tflite --graph_def_file=./compact_graph.pb --input_arrays=input --output_arrays=linear/output\n2019-01-03 17:21:35.382450: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\nTraceback (most recent call last):\n  File \"/home/tim/tensorflow/bin/tflite_convert\", line 11, in \n    sys.exit(main())\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 412, in main\n    app.run(main=run_main, argv=sys.argv[:1])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\n    _sys.exit(main(argv))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 408, in run_main\n    _convert_model(tflite_flags)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 162, in _convert_model\n    output_data = converter.convert()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lite/python/lite.py\", line 404, in convert\n    \"'{0}'.\".format(_tensor_name(tensor)))\nValueError: Provide an input shape for input array 'input'.\n```\nI am able to visualize *.pb by tensorboard, which shows that input is a placeholder with shapes not defined.  \n. Still many many thanks!. one more question, is the obtained pb file is a frozen one (frozen with trained weights in ckpt) or it is merely a graph without weights in it?  Thanks again. I see the trained npz (and thus converted pb) store weights in NCHW format. Is there anyway to change from NCHW into NHWC format when converting from npz to pb? . Simply change shufflenet.py line  37, line 117 to NHWC and \"channle_last\" won't work as it will report Dimension not equal error.  . Thanks, I changed the imagenet_utility.py data_format from NCHW and NHWC. I also change a couple of hard coded location of in_channel = in_shape[1] to in_shape[3] in shufflenet.py. I can proceed a little bit but died in shortcut_channel.\n```\n\n0104 13:06:37 @shufflenet.py:144] #Channels: [24, 116, 232, 464]\n[0104 13:06:37 @registry.py:121] conv1 input: [None, 224, 224, 3]\n[0104 13:06:37 @registry.py:129] conv1 output: [None, 112, 112, 24]\n[0104 13:06:37 @registry.py:121] pool1 input: [None, 112, 112, 24]\n[0104 13:06:37 @registry.py:129] pool1 output: [None, 56, 56, 24]\n[0104 13:06:37 @registry.py:121] stage2 input: [None, 56, 56, 24]\n[0104 13:06:37 @registry.py:121] stage2/block0/conv1 input: [None, 56, 56, 24]\n[0104 13:06:37 @registry.py:129] stage2/block0/conv1 output: [None, 56, 56, 58]\n[0104 13:06:37 @registry.py:121] stage2/block0/dconv input: [None, 56, 56, 58]\n[0104 13:06:37 @registry.py:129] stage2/block0/dconv output: [None, 56, 28, 58]\n[0104 13:06:37 @registry.py:121] stage2/block0/conv2 input: [None, 56, 28, 58]\n[0104 13:06:37 @registry.py:129] stage2/block0/conv2 output: [None, 56, 28, 60]\n[0104 13:06:37 @registry.py:121] stage2/block0/shortcut_dconv input: [None, 56, 56, 24]\nTraceback (most recent call last):\n  File \"./shufflenet.py\", line 263, in \n    ModelExporter(pred_config).export_compact('./new_compact_graph-nhwc.pb')\n  File \"/usr/local/lib/python2.7/dist-packages/tensorpack/tfutils/export.py\", line 48, in export_compact\n    self.config.tower_func(input.get_input_tensors())\n  File \"/usr/local/lib/python2.7/dist-packages/tensorpack/tfutils/tower.py\", line 286, in call\n    output = self._tower_fn(args)\n  File \"/home/tim/tensorpack/examples/ImageNetModels/imagenet_utils.py\", line 333, in build_graph\n    logits = self.get_logits(image)\n  File \"./shufflenet.py\", line 149, in get_logits\n    l = shufflenet_stage('stage2', l, channels[0], 4, group)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorpack/models/registry.py\", line 124, in wrapped_func\n    outputs = func(args, actual_args)\n  File \"./shufflenet.py\", line 109, in shufflenet_stage\n    l = shufflenet_unit_v2(name, l, channel, 2 if i == 0 else 1)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorpack/models/registry.py\", line 124, in wrapped_func\n    outputs = func(*args, actual_args)\n  File \"./shufflenet.py\", line 95, in shufflenet_unit_v2\n    shortcut = DepthConv('shortcut_dconv', shortcut, shortcut_channel, 3, stride=2)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorpack/models/registry.py\", line 124, in wrapped_func\n    outputs = func(args, **actual_args)\n  File \"./shufflenet.py\", line 29, in DepthConv\n    assert out_channel % in_channel == 0, (out_channel, in_channel)\nAssertionError: (56, 24)\n\n\nFor shortcut is complicated for me, as it is tangled with shape transposes. \nThanks, but isline 36 shufflenet.py [1, 1, stride, stride]a bug ?\naccording to the API (https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d), should it be [1, stride, stride, 1]?\n. I think I fixed most of the places in shufflent.py in order to generate NHWC graph and thus an NHWC format pb. Now it proceeds to end a pb, but the dimension starts to be wrong after the 1st channel_shuffle. So I think this is the last obstacle. Can you help to refactor it? After this, I can contribute my changed code to the repo, so everyone can see if OK.   Thank you. \n```\n@under_name_scope()\ndef channel_shuffle(l, group):\n    in_shape = l.get_shape().as_list()\n    in_channel = in_shape[1]  // (changed to in_shape[3])\n    assert in_channel % group == 0, in_channel\n    l = tf.reshape(l, [-1, in_channel // group, group] + in_shape[-2:]) //guess the reshape is critical\n    l = tf.transpose(l, [0, 2, 1, 3, 4]) //guess, this tranpose need to change \n    l = tf.reshape(l, [-1, in_channel] + in_shape[-2:])\n    return l\n```. Thanks so much. Now I can get a NHWC *pb successfully with everything is correct. I modified shufflenet.py a lot, and imagenet_utils.py by one line change from NCHW to NHWC.  I can contribute my code if needed. \n. ",
    "yazdanbakhsh": "But this is just for inference and I forced the batch size to be one. Will check that. ",
    "cjerry1243": "thanks!\n. ",
    "liuxiaowei199345": "I add 'from tensorpack.dataflow import LMDBData' ,but it still have error:\nValueError:invalid literal for float():00000000_n02088632_1176.JPEG\nTraining was stopped.\nEnqueueThread QueueInput/input_queue Exited.\nPrefetDataZMQ successfully cleaned-up.\nso ,what I should do?\n. I don't know how to use other data format. Can you help me ?. Thanks. \nHow to  add the names of tensors to OfflinePredictor ? The intermediate-results is conv0 , conv1 ,bn1, pool1....? How should I establish contact with them?. I use \npython\n            logits = (LinearWrap(image)\n                .Conv2D('conv0', 96, 12, stride=4, padding='VALID')\n                .print_tensor()  # conv0/output\n                .apply(activate)\n                #.......\ngot name of tensor. But I don't know that where add to ProcessTensor() .\nIs it add to OfflinePredictor?. ProcessTensors() doesn't instance list names of tensor is exist or not?\nDo I need use callbacks() in OfflinePredictor?. Thanks very much.\nAnd, I find some questions:\nold:\n```python\nclass Model(ImageNetModel):\n    weight_decay = 5e-6\n    weight_decay_pattern = 'fc.*/W'\ndef get_logits(self, image):\n    if BITW == 't':\n        fw, fa, fg = get_dorefa(32, 32, 32)\n        fw = ternarize\n    else:\n        fw, fa, fg = get_dorefa(BITW, BITA, BITG)\n#....\n\n```\nI found the accuracy to be low. So I guess there may be some problems. I add some code:\n```python\nclass Model(ImageNetModel):\n    weight_decay = 5e-6\n    weight_decay_pattern = 'fc.*/W'\ndef inputs(self):\n    return [tf.placeholder(tf.float32, [None, 224, 224, 3], 'input'),\n            tf.placeholder(tf.int32, [None], 'label')]\n\ndef get_logits(self, image):\n    if BITW == 't':\n        fw, fa, fg = get_dorefa(32, 32, 32)\n        fw = ternarize\n    else:\n        fw, fa, fg = get_dorefa(BITW, BITA, BITG)\n#...\n\n```\nThe accuracy rate has changed. How about it?\n. ",
    "JiandongMu": "DoReFa was designed for ultra-low bits. But I believe it also works for 8-8-32 right? By the way, dorefa will generate some checkpoints, however, how can I resume training from the checkpoints? Thanks,. ",
    "Mistobaan": "The project uses flake8. see tox.ini for the configuration. . few questions:\n- does flake8 auto-format ?\n- do you mind sharing the flake8 configuration in the project ? \n. oh my bad. ",
    "MrLinNing": "Thank you very much,  Yuxin Wu! I used your method, and it works well!\nI have read your paper DoReFa-Net, and I run the python script  svhn-digit-dorefa.py --dorefa 1,2,4. It seems that  trained weights is not 1 bits which can represent 2 different real values but have many different real values in the first figure. Can you detail more about it?\nBesides, what does the W-rms: 1.1019  mean in  param-summary/conv2/W-rms: 1.1019 ?\nI am looking forward to your reply, thank your :) !\n\n\n. Thank you very much for your patience! \nThere is a problem that has troubled me for a long time. When bitW=2, the quantized output is 4 \uff082^2\uff09 different real numbers instead of 4bitfixed-point integers, which cannot satisfy the condition of Formula 3 in the paper. Can you help me understand how to use Formula 3? Thank you\uff01\n\n\n\n. Sorry,  I didn't understand your reply. What I want to say is that the final quantized output  Wb is not fixed-point integer according to your code. But the input value X,Y of formula 3 is the fixed point number.. ",
    "Minotaur-CN": "cool,\nProblem solved.\nThanks. Thanks for your reply!\n1 . What will happen if you don't press ctrl-c ?\nRun normal, no error happends.\n2. What will happen if you press ctrl-c a lot of times?\nChange LearningRate or want to re-training another model, using ctrl-c to interupt previous trainning code. Can not start the training precedure again, must kill the tensorpack thread by command.. 1. press ctrl-c can quit training process as normal. \n2. all error just as I posted before.  Using linux bash 'top', could see the training thread are not terminated, had to kill the thread by command\nkill ps -ef|grep python | grep -v grep|awk '{print $2}'. cool.\nThanks for your reply!. ",
    "KindleHe": "Another problem is that the accuracy of the discriminator keeps about 0.6~0.7 after 217 epochs on train2017.zip. The loss don't decrease for many epochs. Here is my output parameters:\nEpoch 217 (global_step 601524) finished, time:10 minutes 27 seconds.\n[0322 16:36:23 @saver.py:84] Model saved to train_log/enet-pat/model-601524.\n[0322 16:36:23 @monitor.py:428] GAN_loss/discrim/accuracy: 0.61338\n[0322 16:36:23 @monitor.py:428] GAN_loss/discrim/loss: 0.64238\n[0322 16:36:23 @monitor.py:428] GAN_loss/gen/accuracy: 0.17531\n[0322 16:36:23 @monitor.py:428] GAN_loss/gen/loss: 0.91403\n[0322 16:36:23 @monitor.py:428] QueueInput/queue_size: 50\n[0322 16:36:23 @monitor.py:428] additional_losses/loss_LA: 1.8281\n[0322 16:36:23 @monitor.py:428] additional_losses/loss_LP1: 0.3446\n[0322 16:36:23 @monitor.py:428] additional_losses/loss_LP2: 0.26865\n[0322 16:36:23 @monitor.py:428] additional_losses/loss_LT1: 0.026572\n[0322 16:36:23 @monitor.py:428] additional_losses/loss_LT2: 0.026721\n[0322 16:36:23 @monitor.py:428] additional_losses/loss_LT3: 0.05257\nI find your experiment records in the #527 (a copy is below), I have two doubts: \n1)why 40 epoches achieves 0.99414 discrim/accuracy while my 217get only 0.61338? \n2)why the loss_PA between your (8.9464) and my (1.8281) is much different?\n3) why your LT1 LT2 LT3 is so small whiel my is not?\n4) Could you tell me your training config and machine config?\n100%|#######################|19714/19714[59:02<00:00, 5.56it/s]          \n[1206 04:49:54 @base.py:255] Epoch 40 (global_step 788560) finished, time:3542.78 sec.                                                             \n[1206 04:49:55 @saver.py:82] Model saved to train_log/enet-pat/model-788560.                                                                       \n[1206 04:49:55 @monitor.py:363] GAN_loss/discrim/accuracy: 0.99414       \n[1206 04:49:55 @monitor.py:363] GAN_loss/discrim/loss: 0.019299          \n[1206 04:49:55 @monitor.py:363] GAN_loss/gen/accuracy: 0.0026996         \n[1206 04:49:55 @monitor.py:363] GAN_loss/gen/loss: 8.9464\n[1206 04:49:55 @monitor.py:363] QueueInput/queue_size: 49.585\n[1206 04:49:55 @monitor.py:363] add: 19.204\n[1206 04:49:55 @monitor.py:363] additional_losses/loss_LA: 8.9464\n[1206 04:49:55 @monitor.py:363] additional_losses/loss_LP1: 1.3098\n[1206 04:49:55 @monitor.py:363] additional_losses/loss_LP2: 0.0015531\n[1206 04:49:55 @monitor.py:363] additional_losses/loss_LT1: 2.3112e-37\n[1206 04:49:55 @monitor.py:363] additional_losses/loss_LT2: 2.2718e-37\n[1206 04:49:55 @monitor.py:363] additional_losses/loss_LT3: 2.3154e-37\n[1206 04:49:55 @base.py:245] Start Epoch 41 ...\n100%|#######################|19714/19714[59:03<00:00, 5.56it/s]\n[1206 05:48:58 @base.py:255] Epoch 41 (global_step 808274) finished, time:3543.57 sec.. According to your reply in #541 , the below [website] (https://github.com/ppwwyyxx/tensorpack/pull/541) cannot open with remind \"404 error\", I am curious how you always obtain a model better than baseline, and what the baseline here is ?(the paper's model?)\n```\nDid the following in https://gist.github.com/ppwwyyxx/6da15f2d9087373635254d4e6fbe4891:\nUsed the paper's normalization\nFixed VGG scale\nUsed same learning rate for G and D\nUsed SeparateGANTrainer\nAnd now I can always obtain a model better than baseline (in all the 3 times I've tried).\n```. @ppwwyyxx yes, the latter one is also SR, and I found the re-implementation works not bad for the high-resolution input such as 512 but cannot achieve the paper's effect. I think it is fair that the SR algorithms all works on the same downsampling datasets such as  Set5, Set14, SunHays80, Urban100, BSD100.. @ppwwyyxx Yes, I did not get the reason yet. Could you please give me some advice?. ",
    "planetceres": "Since the current version does not condition for the os, would it be reasonable to specify somewhere in the docs or readme that training isn't meant for osx? . ",
    "EchoWho": "I realized that in the group conv case, you are assuming the data_format is of the tf.nn 'NHWC' style consistently. However, the interface can still be more clear, since now you need to change the data_format input based on split=1 or split>1. . Thanks for the prompt reply. Do you mind pointing me to references of why modifying metagraph for training is not a good idea? . ",
    "KaiyuYue": "Sorry to bother, data_format='channels_last' will control the dimensions order. I will close.. ",
    "pimpke": "I wrote a wrong version in the issue description, I actually used the latest version 0.8.5 when I was testing. Corrected.. I forgot to update the error in the issue description when I returned to master from mask_rcnn branch. The error is the same on master (description updated).\nWere there any recent changes to 'COCO-ResNet50-MaskRCNN.npz' on http://models.tensorpack.com, because I couldn't find a recent commit in the commit history that works?. Hi, I didn't properly reverted some code changes (the changes were about running inference on a CPU) on a remote computer (problems with git --git-dir command). Loading of the weights works fine on a GPU.. ",
    "TKZC0615": "OK. Thank you very much for your reply.. Okay. Thank you anyway.. ",
    "chaowang1994": "Thanks for your reply.\nAnd I change command :\npython train.py --load '/path/to/ImageNet-ResNet50.npz' --gpu 0\nError is same as above.\n\nDefault MaxPoolingOp only supports NHWC.. Sorry,There is a problem with my GPU.\nThanks again. \ud83d\udc4d . I meet the same thing too.\n\npython 2.7 \ntensorflow 1.7.0 by pip install tensorflow==1.7.0\ninstall tensorpack using pip install tensorpack. You should using a GPU to run it.. ",
    "whj363636": "I will try. Thanks very much for your help.. ",
    "yzpick": "Yes, I tried, but I don't specifically know what to put in the \"output_names\", since the variables are in Keras mode. I tried directly use the Keras layer's name, but it said:\nKeyError: \"The name 'label:0' refers to a Tensor which does not exist. The operation, 'label', does not exist in the graph.\"\n. so I used the mnist-keras-v2.py to do the test, i print the model summary:\nconv2d_1 (Conv2D)            (None, 28, 28, 32)        320       \n\nmax_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n\nconv2d_2 (Conv2D)            (None, 14, 14, 32)        9248      \n\nconv2d_3 (Conv2D)            (None, 14, 14, 32)        9248      \n\nmax_pooling2d_2 (MaxPooling2 (None, 7, 7, 32)          0         \n\nconv2d_4 (Conv2D)            (None, 7, 7, 32)          9248      \n\nflatten_1 (Flatten)          (None, 1568)              0         \n\ndense_1 (Dense)              (None, 512)               803328    \n\ndropout_1 (Dropout)          (None, 512)               0         \n\ndense_2 (Dense)              (None, 10)                5130      \n\nactivation_1 (Activation)    (None, 10)                0         \nTotal params: 836,522\nTrainable params: 836,522\nNon-trainable params: 0\n\nThen I directly use the \"activation_1\" layer name as the output name, but I still got the error:\nKeyError: \"The name 'activation_1:0' refers to a Tensor which does not exist. The operation, 'activation_1', does not exist in the graph.\"\nBut you can see activation_1 is in the model.\nAny ideas why?\n. sorry I am a bit confused, so you mean print(M.summary()) does not necessarily print the corresponding tensor name? I can use print(M.layers[10].name) but it still outputs \"activation_1\", what do you mean by print the tensor name? can you please directly give an offlinepredictor example that can be used in the mnist-v2? . Thanks a lot Yuxin!. xxxxxxxxxxxxxx\n[0504 13:37:58 @training.py:100] Building graph for training tower 1 on device /gpu:1 ...\nxxxxxxxxxxxxxxx\n[0504 13:39:42 @training.py:100] Building graph for training tower 2 on device /gpu:2 ...\nxxxxxxxxxxxxxxx\n[0504 13:42:36 @training.py:284] WRN [SyncMultiGPUReplicatedBuilder] variable tower0/bidirectional/Variable:0 has prefix 'tower0', this is unexpected.\nxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n[0504 13:42:36 @training.py:284] WRN [SyncMultiGPUReplicatedBuilder] variable tower0/bidirectional/Variable/Adam:0 has prefix 'tower0', this is unexpected.\nxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n[0504 13:42:36 @training.py:297] WRN [ReplicatedTrainer] Cannot find bidirectional/conv_lst_m2d_16/kernel:0 in the graph!\n[0504 13:42:36 @training.py:297] WRN [ReplicatedTrainer] Cannot find bidirectional/conv_lst_m2d_16/recurrent_kernel:0 in the graph!\nxxxxxxxxxxxxxxxxxxxxxxxx\n[0504 13:42:36 @training.py:299] 'sync_variables_from_main_tower' includes 606 operations.\ni use xxxxxxxxxxx to replace similar messages. i am using Keras, it is the bidirectional layer wrapper: https://keras.io/layers/wrappers/. I checked a bit, it looks like it is not the bidirectional wrapper that has problem, it is the convlstm2d layer in keras, i am not sure if you can reproduce the error using this one https://github.com/keras-team/keras/blob/master/examples/conv_lstm.py\nIf the variables are incompatible, then why can I still get results? is there any problem with the results I got? . Thanks for the clarifying. then what does it actually do when i use multigpu to run the script? is it only tower0 works properly and tower1 and tower2 actually do nothing useful?. Sure. Have a nice weekend :). ",
    "tals": "See https://github.com/zeromq/pyzmq/blob/master/zmq/backend/cython/message.pyx#L322. ",
    "ninetailskim": "http://tensorpack.readthedocs.io/en/latest/tutorial/index.html\n(on the homepage of this repo, in the README.md). ",
    "bzamecnik": "No worry. I was studying the lib and IntelliJ Idea spelchecker was nagging from time to time. Anyway, awesome work!. Thanks. Unfortunately, keras.py is not covered by tests and CI is not set up in the GitHub repo. Existing tests are green. I only tried that in my Jupyter notebook and went fine - it just uncovered an unrelated exception that caused the variable being unassigned.. I don't say you have to do it. It's just a reminder for me to write it in\nfuture if needed. We're still considering whether to use just DataFlow or\nalso trainers in our project since Keras support is still quite\nexperimental.\nDne \u00fat, 15. 5. 2018 18:53 u\u017eivatel Yuxin Wu notifications@github.com\nnapsal:\n\nI don't think I'll be able to spend any more time on supporting Keras.\nPlus I think having sample_weight is a very ugly design..\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/ppwwyyxx/tensorpack/issues/764#issuecomment-389237679,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAbOrDf0WgiGYCkz1_ZfIK-P0vz7vksJks5tywgSgaJpZM4T_WkG\n.\n. Well, thanks for the awesome work. At Rossum we'll certainly need to\nimprove our research infrastructure to be both easy to use and high\nperformance. So far Tensorpack DataFlow seems to be a really good step -\nwell composable and faster than our home grown multi-processing generators.\nOn the other hand leaving all the Keras tools like model saving/loading,\ncallbacks and other stuff may be just a too big step for less marginal\nbenefit. Let's see if we'll be able to improve Tensorpack to support Keras\nbetter, or improve Keras for TF Queue/Staging/Multi GPU or just switch to\nTensorflow completely. Anyway if there's any improvement we'll be happy to\nprovide PR's.\n\nDne \u00fat, 15. 5. 2018 22:08 u\u017eivatel Yuxin Wu notifications@github.com\nnapsal:\n\nYes. Keras support is just a proof-of-concept. And obviously it is\nunlikely to be a focus of this project.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/ppwwyyxx/tensorpack/issues/764#issuecomment-389296734,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAbOrLx861Fdwh9x9WYR6vgQu7-Z6ozzks5tyzXGgaJpZM4T_WkG\n.\n. And I prepared a DataFlow that compares two others like zip, checks\nrecursively equality of their nested components and reports differences.\nUseful regression testing to quickly see we broke anything. I'll try to\nclean it up and make a PR. I wish I had it earlier today. :)\n\nDne \u010dt, 17. 5. 2018 19:00 u\u017eivatel Yuxin Wu notifications@github.com\nnapsal:\n\nNow most DataFlow assumes list. I feel some inconvenience with it as well.\nAllowing dict will be great!\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/ppwwyyxx/tensorpack/issues/768#issuecomment-389937808,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAbOrIjRJQuURF3kNNILiWqPqgCo3eWWks5tzazFgaJpZM4UC5k7\n.\n. I agree with @ppwwyyxx that this is a big change to the existing object model.\n\nSo far a forking operation just clones the given data flow graph and runs multiple instances of each.\nWhat we'd like to achieve is to fork a dataflow into multiple parallel map operations, each working on a different slice. I can imagine slicing on infinite generators (done by a cooperative way by predefined rule) - keep sample if index % total_workers == worker_index. For that to work efficiently the pipeline would need to be evaluated in a lazy way, so that we'd skip computation for skipped samples. This assumes that complexity of mapping function is uniform. Otherwise we'd need some queue to balance the computation.\nAnyway I'm starting to think whether Dask with it's task graph being a full DAG would be more suitable for our input pipeline.. ",
    "samson-wang": "It's weird. Pytorch and Caffe2 works well.\nThe TF was installed through pip. I will try to compile from source.. Yes. pip install tensorflow. Any suggestions?. Another thing.\nI have tried to train imagenet on caffe2/caffe/pytorch. And never get close to the reported accuracy. The learning curve is different.\nI can only get 72% top 1 accuracy. The inception-style data augmentation is used. I only have 4 titanX, so I adjusted the learning rate according to the linear scaling rule. No help either.\nThe tensorpack seems to be the chance. Hope it works great.. ",
    "lengstrom": "Thank you, that did it.. ",
    "Jarlan1207": "Could you help me ?. So, should I use Ubuntu to run it?. Ok, I got it, Thank you. I run it on  tensorflow-gpu. i run these code\nimport tensorflow as tf\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\nThen I  got \n2018-05-20 17:45:16.481366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y \n2018-05-20 17:45:16.481375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0)\n2018-05-20 17:45:16.482564: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 133.81M (140312576 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\n2018-05-20 17:45:16.484438: I tensorflow/core/common_runtime/direct_session.cc:265] Device mapping:\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0\nDevice mapping:\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0\nPyDev console: starting.\nPython 3.5.2 (default, Nov 23 2017, 16:37:01) \n[GCC 5.4.0 20160609] on linux\nIsn't  tensorflow-gpu. Thank you. I run it on tensorflow-gpu-1.3. and now the problem was solved.. ",
    "XiangyuWu": "\nAnd this is where I implement .print_tensor().. \nThe initialization logs of gpu were ignored. As you can see, the project would begin to process the epochs after I run python svhn-digit-dorefa.py, but no tensor values were printed. What's more, though I added sys.exit() sentence in your definition of print_tensor() under tensorpack/models/linearwrap.py, it also seemed to make no sense.\n\nI am not familiar with your framework, so thanks for your help.. I got it!\nWhat indeed help to print tensor values easily in your framework is ProcessTensors(). \nAnd for sys.exit() problem, probably I made some mistakes.. ",
    "Darthholi": "Upvote!. So far I think it can be done in the code using:\ndef _aggregate_batch(data_holder, use_list=False):\n        if isinstance(data_holder[0], map):\n            iterat_indices = data_holder[0].keys()\n        else:\n            size = len(data_holder[0])\n            iterat_indices = range(size)\n        result = []\n        for k in iterat_indices:\n...or even better having a separate function, that would give us iterator of indices to iterate over (indices for list, keys for map, anything for anything else ...). (And if You know how to make the checks pass, tell me, i would love to fix it of course! :) ). Thanks! \nI will just ask, for clarity - the base dataflow producing different set of indices needs to know to which process it belongs and the total number of processes.\nWhere, in the code, would You prefer to give it the information?\nMy take would be to use reset_data(adding the slicing params here) called from ZMQ that would propagate to the base dataflow.\n(I did write the new classes to avoid cluttering reset_data with params, because I was afraid You would not like the solution, but maybe I was wrong :) )\n(Edit: Iam reading the mentioned #401 again and again and Iam having the feeling, that maybe I just do not get it if the prefetch is enough or if the dataflows need to know which process out of what number of processes they are. ... If I shuffle the data and prefetch some number of them, is there any guarantee that on 1 epoch (defined by the dataset size), that it will see each datapoint only once?). Thank You!\nWe had a conversation with @bzamecnik and we would like to ask You how is it achieved without giving the generator the information about the processID (and total number of processes)...?\nBecause if, in the end, this information would be needed, Iam ready to code it for us all.. Thx, it works! :). ",
    "npow": "How much faster is the new FeedfreePredictor?\n# This does not have a visible improvement over naive predictor,\n    # but will have an improvement if image_dtype is set to float32.. I put a breakpoint in the constructor of tf.Graph, and it turns out that the tf.data.TFRecordDataset is creating another graph instead of using the default one. \nimport tensorflow as tf\nTRAIN_FILE = '...'\nds = tf.data.TFRecordDataset(TRAIN_FILE, compression_type='GZIP')\n```\n-> ds = tf.data.TFRecordDataset(TRAIN_FILE, compression_type='GZIP')\n  /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/readers.py(194)init()\n-> filenames = ops.convert_to_tensor(filenames, dtype=dtypes.string)\n  /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py(998)convert_to_tensor()\n-> as_ref=False)\n  /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py(1094)internal_convert_to_tensor()\n-> ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py(217)_constant_tensor_conversion_function()\n-> return constant(v, dtype=dtype, name=name)\n  /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py(192)constant()\n-> g = ops.get_default_graph()\n  /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py(5327)get_default_graph()\n-> return _default_graph_stack.get_default()\n  /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py(5002)get_default()\n-> ret = self._GetGlobalDefaultGraph()\n  /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py(5011)_GetGlobalDefaultGraph()\n-> self._global_default_graph = Graph()\n\n/home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py(2677)init()\n-> self._lock = threading.RLock()\n(Pdb) c\n-> ds = tf.data.TFRecordDataset(TRAIN_FILE, compression_type='GZIP')\n  /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/readers.py(207)init()\n-> self._impl = filenames.flat_map(read_one_file)\n  /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py(1001)flat_map()\n-> return FlatMapDataset(self, map_func)\n  /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py(2257)init()\n-> experimental_nested_dataset_support=True)\n  /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py(1454)init()\n-> self._function.add_to_graph(ops.get_default_graph())\n  /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/function.py(481)add_to_graph()\n-> self._create_definition_if_needed()\n  /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/function.py(337)_create_definition_if_needed()\n-> self._create_definition_if_needed_impl()\n  /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/function.py(346)_create_definition_if_needed_impl()\n-> self._capture_by_value, self._caller_device)\n  /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/function.py(845)func_graph_from_py_func()\n-> func_graph = _FuncGraph(name, capture_by_value)\n  /home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/function.py(631)init()\n-> super(_FuncGraph, self).init(args, *kwargs)\n/home/npow/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py(2677)init()\n-> self._lock = threading.RLock()\n``. The_GetGlobalDefaultGraphcall creates the default one - I verified withtf.get_default_graph(). The second one created infunc_graph_from_py_func` is different.. Sorry, I tried creating a minimal example with some fake data and couldn't reproduce it. I'll close the issue for now and re-open when I have more time to investigate.. \n",
    "jonkoi": "Sorry for being confusing.\nAs i understand it, and correct me if i am wrong, the final weights are still stores in floats and have floating point gemm performed on them. I wanted to ask if you know of a way or a framewiork of compressing these weights to int8 or lower and have low precision gemm performed on them.. ",
    "jwook1004": "Thanks for the prompt response! Confirmed that the pretrained COCO-ResNet50-MaskRCNN.npz network can achieve mAP of 0.377/0.330 (bbox/segm), which is same as the baseline. Trying \"train-from-scratch\" with the pretrained ResNet50 model (ImageNet-ResNet50.npz) downloaded from the website (http://models.tensorpack.com/ResNet/). Will update the status.\n. Confirmed that train with \"ImageNet-ResNet50.npz\" pretrained network achieved the baseline as well. Thanks for the help! Closing the issue.. I think the case I illustrated is different from simply customizing ops in the back-prop.\ntf.custom_gradient allows change of computation in back-prop given the same input/output Tensors as arguments. The case I illustrated requires different input argument (i.e., full-prec weights)  to be used for the same computation in the back-prop.\nI was wondering if there's any tensorpack functions / wrapper that can be used/extended to achieve the required change? \n. Hi Yuxin,\nThanks for your input. Just to confirm my understanding, I think what you suggested would be similar to the following pseudo code?\n@tf.custom_gradient\ndef convquant(x,w,wq):\n  y = conv2d(x,wq) #  Use quantized weight in the forward pass\n  def convquant_grad(dy):\n    dx = conv2d_backpropInput(dy,w) #  Use original weight in the backward pass\n    return dx\n  return y, convquant_grad\n. \nYeah... I think the above pseudo code still has several problems. For example, it is not clear how to handle weight gradient computation. The above pseudo code does not compute weight gradients (dW), whereas typically instantiation of Conv2D would automatically call Conv2D_backpropInput and Conv2D_backpropFilter. So unless handled separately, the training would not work properly. Also, it would require separate instantiation of variables, which is tricky unless being very careful.\nSo, it seems to me that Tensorflow might have fundamental limitations with respect to flexibility in manipulating backward-pass computation. I was curious about your thoughts on this point. Would there be a good workaround in Tensorflow to enhance such flexibility?\n. Thanks for your input. I agree with you that backward pass of TF can become much flexible with tf.gradients and tf.custom_gradient. \nRegarding one of your comments:\n\nOn this point I think TF is more flexible than any other frameworks. Others only allow you to do (1).\n\nHow do you think of Pytorch? I saw that some people also implement quantization schemes on Pytorch by manipulating the back-prop computation (e.g., link). My naive understanding is that one can explicitly define \"backward()\" function for any op, which is somewhat similar to tf.custom_gradients. I was wondering which one fundamentally allows more flexibility (or maybe they are equally good)?\n. Thanks for a quick response!! Let us also try TF 1.10. \n. Confirmed that TF 1.10 works without the problems reported here. . ",
    "yogurfrul": "it works when I pass config=get_default_sess_config() to tf.Server\nThen I got error  Master init: Unavailabel: OS Error when I run worker before ps,while it's ok when I run ps firstly.\n[0608 05:55:08 @base.py:197] Creating the session ...\n2018-06-08 05:55:09.075000: E tensorflow/core/distributed_runtime/master.cc:269] Master init: Unavailable: OS Error\n2018-06-08 05:55:09.147089: E tensorflow/core/distributed_runtime/master.cc:269] Master init: Unavailable: OS Error\nTraceback (most recent call last):\n  File \"distributed-ps-mnist-convnet.py\", line 145, in \n    launch_train_with_config(config, DistributedTrainerParameterServer([int(x) for x in args.gpu.split(\",\")], server))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorpack-0.8.5-py2.7.egg/tensorpack/train/interface.py\", line 90, in launch_train_with_config\n    extra_callbacks=config.extra_callbacks)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorpack-0.8.5-py2.7.egg/tensorpack/train/base.py\", line 302, in train_with_defaults\n    steps_per_epoch, starting_epoch, max_epoch)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorpack-0.8.5-py2.7.egg/tensorpack/train/base.py\", line 273, in train\n    self.initialize(session_creator, session_init)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorpack-0.8.5-py2.7.egg/tensorpack/train/trainers.py\", line 199, in initialize\n    get_distributed_session_creator(self.server), session_init)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorpack-0.8.5-py2.7.egg/tensorpack/utils/argtools.py\", line 181, in wrapper\n    return func(args, *kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorpack-0.8.5-py2.7.egg/tensorpack/train/base.py\", line 200, in initialize\n    self.sess = session_creator.create_session()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorpack-0.8.5-py2.7.egg/tensorpack/tfutils/distributed.py\", line 40, in create_session\n    return sm.prepare_session(master=server.target, init_op=init_op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 281, in prepare_session\n    sess.run(init_op, feed_dict=init_feed_dict)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 905, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1140, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\n    run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.UnavailableError: OS Error\n. thx for reply, I will try horovod later\u3002\n. 1.I did pass bind=False to RemoteDataZMQ and the revicer holding when get_data .while the sender still got the same error\n\n\nit works~\n\n\nI pass format='zmq_op' to send_dataflow_zmq. I also Installed  zmq-ops in sender and revicer /usr/local/lib/python2.7/dist-packages/zmq_ops-0.1.0-py2.7.egg\nhow do I use zmq_ops on the receiver side?\n\n\nI can not test it becase error 1 above\n\n\n. 1.I also it's a network issue,but I don't how to fix it.\nWhile if I use the default bind value ,the test runs ok\nbind=False to send_dataflow_zmq and bind=True to RemoteDataZMQ\nand I run the test succ with ipc send and bind =True\n2.I run the benchmark.py send and recv ok.\npyarrow 0.9.0 do not support for deserialize the metadata version of  reviced dataflow BatchData(dataset.Mnist('train'), 128) \n. All rank would dump its graph .\nI read the doc of horovod \nI got that only broadcast the variable at the begin of train from root_rank.\nOther ranks get the grad after ring allreduce and apply it on its own and begin next step.\nI need the graph to check if I'right. I set the logger dir by \nlogger.set_logger_dir(os.path.join('train_log', 'distributed-horovod',str(hvd.rank())), action='d')\nand only log.log file in the dir \n. ^[[32m[0614 06:03:51 @logger.py:74]^[[0m Argv: distributed-horovod-mnist-convnet.py\n^[[32m[0614 06:03:51 @fs.py:88]^[[0m ^[[5m^[[31mWRN^[[0m Env var $TENSORPACK_DATASET not set, using /root/tensorpack_data for datasets.\n^[[32m[0614 06:03:52 @trainers.py:346]^[[0m Horovod local rank=3\n^[[32m[0614 06:03:52 @interface.py:31]^[[0m Automatically applying QueueInput on the DataFlow.\n^[[32m[0614 06:03:52 @input_source.py:193]^[[0m Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...\n^[[32m[0614 06:03:52 @registry.py:121]^[[0m conv0 input: [None, 28, 28, 1]\n^[[32m[0614 06:03:52 @registry.py:129]^[[0m conv0 output: [None, 28, 28, 32]\n^[[32m[0614 06:03:52 @registry.py:121]^[[0m pool0 input: [None, 28, 28, 32]\n^[[32m[0614 06:03:52 @registry.py:129]^[[0m pool0 output: [None, 14, 14, 32]\n^[[32m[0614 06:03:52 @registry.py:121]^[[0m conv1 input: [None, 14, 14, 32]\n^[[32m[0614 06:03:52 @registry.py:129]^[[0m conv1 output: [None, 14, 14, 32]\n^[[32m[0614 06:03:52 @registry.py:121]^[[0m conv2 input: [None, 14, 14, 32]\n^[[32m[0614 06:03:52 @registry.py:129]^[[0m conv2 output: [None, 14, 14, 32]\n^[[32m[0614 06:03:52 @registry.py:121]^[[0m pool1 input: [None, 14, 14, 32]\n^[[32m[0614 06:03:52 @registry.py:129]^[[0m pool1 output: [None, 7, 7, 32]\n^[[32m[0614 06:03:52 @registry.py:121]^[[0m conv3 input: [None, 7, 7, 32]\n^[[32m[0614 06:03:52 @registry.py:129]^[[0m conv3 output: [None, 7, 7, 32]\n^[[32m[0614 06:03:52 @registry.py:121]^[[0m fc0 input: [None, 7, 7, 32]\n^[[32m[0614 06:03:52 @registry.py:129]^[[0m fc0 output: [None, 512]\n^[[32m[0614 06:03:52 @registry.py:121]^[[0m fc1 input: [None, 512]\n^[[32m[0614 06:03:52 @registry.py:129]^[[0m fc1 output: [None, 10]\n^[[32m[0614 06:03:52 @regularize.py:88]^[[0m regularize_cost() found 2 variables to regularize.\n^[[32m[0614 06:03:52 @regularize.py:19]^[[0m The following tensors will be regularized: fc0/W:0, fc1/W:0\n^[[32m[0614 06:03:52 @model_utils.py:63]^[[0m ^[[36mTrainable Variables:\n^[[0mname       shape              dim\n\nxxxx\nTotal #vars=12, #params=836522, size=3.19MB^[[0m\n^[[32m[0614 06:03:52 @base.py:131]^[[0m ^[[5m^[[31mWRN^[[0m Callback MovingAverageSummary is chief-only, skipped.\n^[[32m[0614 06:03:52 @base.py:131]^[[0m ^[[5m^[[31mWRN^[[0m Callback MergeAllSummaries_RunWithOp is chief-only, skipped.\n^[[32m[0614 06:03:52 @base.py:131]^[[0m ^[[5m^[[31mWRN^[[0m Callback TFEventWriter is chief-only, skipped.\n^[[32m[0614 06:03:52 @base.py:131]^[[0m ^[[5m^[[31mWRN^[[0m Callback JSONWriter is chief-only, skipped.\n^[[32m[0614 06:03:52 @base.py:179]^[[0m Setup callbacks graph ...\n^[[32m[0614 06:03:52 @base.py:197]^[[0m Creating the session ...\n^[[32m[0614 06:03:58 @base.py:212]^[[0m Graph Finalized.\n^[[32m[0614 06:03:58 @concurrency.py:37]^[[0m Starting EnqueueThread QueueInput/input_queue ...\n^[[32m[0614 06:03:58 @graph.py:73]^[[0m Running Op horovod_broadcast/group_deps ...\n^[[32m[0614 06:03:58 @common.py:774]^[[0m ^[[36mDataFlow Info:^[[0m\n^[[32m[0614 06:04:16 @base.py:232]^[[0m Start Epoch 1 ...\n^[[32m[0614 06:05:22 @base.py:242]^[[0m Epoch 1 (global_step 468) finished, time:1 minute 5 seconds.\n^[[32m[0614 06:05:22 @base.py:232]^[[0m Start Epoch 2 ...\n^[[32m[0614 06:08:16 @base.py:242]^[[0m Epoch 2 (global_step 936) finished, time:2 minutes 53 seconds.\n^[[32m[0614 06:08:16 @base.py:246]^[[0m Training has finished!\nI think the reason is in  **^[[32m[0614 06:03:52 @base.py:131]^[[0m ^[[5m^[[31mWRN^[[0m Callback TFEventWriter is chief-only, skipped.. it works~\nI think it't right to have a default,but the code is useless to check the chief_only of callbacks on train/base.py. \nis_chief = True\n\"\"\"\nWhether this process is the chief worker in distributed training.\nCertain callbacks will only be run by chief worker.\n\"\"\"\n\nxxx\nif not self.is_chief and cb.chief_only:\n    logger.warn(\"Callback {} is chief-only, skipped.\".format(str(cb)))\n    return False\nelse:\n    self._callbacks.append(cb)\n    return True. oh I'm confused .Never mind~\n\nI get the events of a non-master process\nI still find horovod_broadcast on tensorboard,and I choose one Session runs option ,the op disappear.\nNow I can see the broadcast only run once at the variable init process\n. so there is an option --gpu ,I think the example support GPU training.\nI want to run an lstm example training on multi gpu , how would I change it? or there is another example I can use. I set session_config=get_default_sess_config() to TrainConfig don't work. oh  yes, \nI change the trainer to HorovodTrainer ,it works .. I have changed the mnist-convnet example trainer to the horovodtrainer and mpirun it on 2 gpus .\nchange the TrainConfig as:\nmodel=Model(),\ndataflow=dataset_train,\ncallbacks=[\nGPUUtilizationTracker(),\nGraphProfiler(dump_metadata=True, dump_tracing=True, dump_event=True),\n],\nsteps_per_epoch=10,\nmax_epoch=3,\nit works fine. Then I try profiler\ntry 1:\nadd profiler in the train script as \nwith tf.contrib.tfprof.ProfileContext(os.path.join('./train_log', str(hvd.rank())),trace_steps=range(3, 6),\n                              dump_steps=[6]) as pctx:\nlaunch_train_with_config(config, HorovodTrainer())\nit runs ok,but no profiler file generated .\ntry2:\nadd profiler in the tensorpack source tensorpack/train/base.py as\ndef main_loop(self, steps_per_epoch, starting_epoch, max_epoch):\nwith tf.contrib.tfprof.ProfileContext(logget_logger_dir()) as pctx:\nwith self.sess.as_default():\nit generate profiler file but an exception \ntensorflow.python.framework.errors_impl.UnavailableError: CUPTI subcriber limit reached.\ntry3:\nadd profiler in the tensorpack source tensorpack/train/base.py as\ndef main_loop(self, steps_per_epoch, starting_epoch, max_epoch):\nwith tf.contrib.tfprof.ProfileContext(logger.get_logger_dir(),trace_steps=range(3, 6), dump_steps=[6]) as pctx:\nwith self.sess.as_default():\nno exception found, but hold stuck when fisrt epoch of rank=0 end .\nrank=0 logdir has 10 chrome-trace-.json and 10 runmetadata-.pb and one events file and one log.log\nrank=1 logdir has only one events file and one log.log and the profile_6!\n. the profile_6 up is imcompeleted.\nI change the PennTreebank example in the same way ,and try as third way\nit runs ok and the profile file generated right.\nso this problem may be between the QueueInput and ProfileContext. can it  be used with StagingInput? I think yes\nand How orignal tensorflow profiler prefetch data\nAnd must I change the tensorpack source code tensorpack/train/base.py to add profiler?. as try way 1: the training runs ok,but no profiler file generated .\nthen I try it on PennTreebank example ,it works ok.\nso it's about the QueueInput ,not the tensorpack source code.\nI would try TFDatasetInput\n. TFDatasetInput works, and profiler runs ok.\nTFDatasetInput coverts dataflow to tf.dataset and enable prefetch from disk to cpu using orignal tensorflow way\uff1f\nCan I say TFDatasetInput do the same work with QueueInput just in different way by different speed.. I did it in this way:\ninit an _profile_context in GraphProfiler  in init\nself._profile_context = tf.contrib.tfprof.ProfileContext(self._dir, debug=True)\nthen add the run metadata of this step to the profiler in _after_run\nself._profile_context.profiler.add_step(self.global_step, meta)\nfinally dump the profiler on specify step of first epoch\nself._profile_context.profiler._write_profile(filename)\nit runs ok with QueueInput and dump the profiler\n. yes ,it can be reproducible.\nthe problem  happened in one step of about 40~50 step,\nthe trace of step before it  are normal, after it the trace are abnormal.. For any unexpected problems, PLEASE ALWAYS INCLUDE:\n1. What you did:\n    + If you're using examples:\n        examples/ResNet/imagenet-resnet.py\n        + What's the command you run:\n        ./imagenet-resnet.py --data path/to/data --gpu 3,4 --batch 64\n        + Have you made any changes to code? Paste them if any:\n        just add GraphProfiler to callbacks in line 76 :\n        callbacks = [\n            ModelSaver(),\n            EstimatedTimeLeft(),\n            ScheduledHyperParamSetter(\n                'learning_rate', [\n                    (0, min(START_LR, BASE_LR)), (30, BASE_LR * 1e-1), (60, BASE_LR * 1e-2),\n                    (90, BASE_LR * 1e-3), (100, BASE_LR * 1e-4)]),\n            **GraphProfiler()**\n        ]\n    https://github.com/tensorpack/tensorpack/blob/master/examples/ResNet/imagenet-resnet.py#L76\n2. What you observed, including but not limited to the entire logs.\nI found this in chrome-trace-50.json, normal nodestats start timestamp are all startwith 15xxx while some nodestats start timestamp are startwith 19xxx,the timeline duration are so many year!\n'''\n       {\n            \"args\": {\n                \"op\": \"MEMCPYDtoH\",\n                \"name\": \"edge_7572_tower0/linear/BiasAdd\"\n            },\n            \"tid\": 0,\n            \"name\": \"MEMCPYDtoH\",\n            \"ph\": \"X\",\n            \"pid\": 5,\n            \"dur\": 359,\n            \"ts\": 19978045970716470,\n            \"cat\": \"Op\"\n        },\n        {\n            \"args\": {\n                \"op\": \"MEMCPYDtoH\",\n                \"name\": \"edge_1079_tower0/gradients/tower0/gap/output_grad/DynamicStitch\"\n            },\n            \"tid\": 1,\n            \"name\": \"MEMCPYDtoH\",\n            \"ph\": \"X\",\n            \"pid\": 5,\n            \"dur\": 2,\n            \"ts\": 1531302016698591,\n            \"cat\": \"Op\"\n        },\n        {\n            \"args\": {\n                \"op\": \"Sub\",\n                \"name\": \"tower0/group2/block0/conv3/bn/AssignMovingAvg/sub_1\"\n            },\n            \"tid\": 0,\n            \"name\": \"Sub\",\n            \"ph\": \"X\",\n            \"pid\": 17,\n            \"dur\": 4,\n            \"ts\": 19978045970621550,\n            \"cat\": \"Op\"\n        },\n        {\n            \"args\": {\n                \"op\": \"Sub\",\n                \"name\": \"tower0/group2/block0/conv3/bn/AssignMovingAvg_1/sub_1\"\n            },\n            \"tid\": 0,\n            \"name\": \"Sub\",\n            \"ph\": \"X\",\n            \"pid\": 17,\n            \"dur\": 67,\n            \"ts\": 19978045970621559,\n            \"cat\": \"Op\"\n        },\n'''\n\n\nYour environment:\nPython version.\n'python3.5'\nTF version: python -c 'import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)'.\n'tensorflow 1.8.0'\nTensorpack version: python -c 'import tensorpack; print(tensorpack.__version__)'.\n'0.8.6(I download the master source code and install tensorpack by python setup.py install)'\n. varlist2 is wout and bout in fact ,so I use apply_grad_processors and ScaleGradient as below:\n\n\n\ndef optimizer(self):\n        opt = tf.train.AdamOptimizer(1e-4)\n        return optimizer.apply_grad_processors(\n            opt, [gradproc.ScaleGradient(\n                [('.*/wout', 10), ('.*/bout', 10)],log=True)])\nI found this print in log\n[0727 06:38:03 @gradproc.py:251] Apply lr multiplier 10 for var_name/wout\n[0727 06:38:03 @gradproc.py:251] Apply lr multiplier 10 for var_name/bout\nI think it works~\nthx. Can I join two different optimizer to one in ModelDesc?\nopt1 = tf.train.AdamOptimizer(1e-4)\nopt2 = tf.train.GradientDescentOptimizer(1e-3). I see~\nthx. I want to show op output shape  so I \nimage_shape2d = tf.shape(image,name='imageshape')[2:]\nand I add monitors in TrainConfig\nmonitors=[ScalarPrinter(enable_step=True,whitelist=['.*/Conv2D','imageshape'])],\nBut I get noting to print. Thx\nand I get wrong result after TF_CUDNN_USE_AUTOTUNE=0 above\nI retry set shuffle=False in get_train_dataflow ,then print the image shape and obeject number in the image and step time cost\nI just try one epoch of 210 step ,\nthe epoch cost time are 4min 54s(enable autotune) and 1min 44s(disable autotune)\nthen I get the log below:\nunset TF_CUDNN_USE_AUTOTUNE\n```\nimage shape:[1201 800 3]\nobeject number:[2]\n\u001b[32m[0810 06:27:14 @base.py:254]\u001b[0m hvd rank 0, (global_step 86)  time:0.2963137626647949.\nimage shape:[800 1199 3]\nobeject number:[8]\n\u001b[32m[0810 06:27:14 @base.py:254]\u001b[0m hvd rank 0, (global_step 87)  time:0.33693432807922363.\nimage shape:[800 1261 3]\nobeject number:[10]\n\u001b[32m[0810 06:27:17 @base.py:254]\u001b[0m hvd rank 0, (global_step 88)  time:2.603098154067993.\nimage shape:[800 808 3]\nobeject number:[2]\n\u001b[32m[0810 06:27:17 @base.py:254]\u001b[0m hvd rank 0, (global_step 89)  time:0.5877795219421387.\nimage shape:[800 1278 3]obeject number:[4]\n\u001b[32m[0810 06:27:20 @base.py:254]\u001b[0m hvd rank 0, (global_step 90)  time:2.456442356109619.\nimage shape:[800 1199 3]\nobeject number:[1]\n\u001b[32m[0810 06:27:22 @base.py:254]\u001b[0m hvd rank 0, (global_step 91)  time:2.151698350906372.\n```\nexport TF_CUDNN_USE_AUTOTUNE=0\n```\nimage shape:[1201 800 3]\nobeject number:[2]\n\u001b[32m[0810 06:16:50 @base.py:254]\u001b[0m hvd rank 0, (global_step 86)  time:0.379549503326416.\nimage shape:[800 1199 3]obeject number:[8]\n\u001b[32m[0810 06:16:51 @base.py:254]\u001b[0m hvd rank 0, (global_step 87)  time:0.40617823600769043.\nimage shape:[800 1261 3]\nobeject number:[10]\n\u001b[32m[0810 06:16:51 @base.py:254]\u001b[0m hvd rank 0, (global_step 88)  time:0.36014246940612793.\nimage shape:[800 808 3]obeject number:[2]\n\u001b[32m[0810 06:16:52 @base.py:254]\u001b[0m hvd rank 0, (global_step 89)  time:0.44893598556518555.\nimage shape:[800 1278 3]\nobeject number:[4]\n\u001b[32m[0810 06:16:52 @base.py:254]\u001b[0m hvd rank 0, (global_step 90)  time:0.32912302017211914.\nimage shape:[800 1199 3]\nobeject number:[1]\n\u001b[32m[0810 06:16:52 @base.py:254]\u001b[0m hvd rank 0, (global_step 91)  time:0.3986361026763916.\n. If each data has different size,  , export TF_CUDNN_USE_AUTOTUNE=0 may improve train performence\nIt would be very nice to let others know that  disable cudnn autotune in  FasterRCNN  example. I  can only find this in fastrcnn notes\nThe training will start very slow due to convolution warmup, until about 10k steps to reach a maximum speed. Then the training speed will slowly decrease due to more accurate proposals.\n```\n1,.You mean AUTOTUNE mainly do the convolution warmup?\n2.why 10K would reach a maximum speed , there are about 10k kinds of image shape in the COCO?\n3.If I want to check the performence of trainning by step cost time and performence  of gradient allreduce , do you have some suggestion?. When I use  SyncMultiGPUTrainerReplicated trainer by the command\nCUDA_VISIBLE_DEVICES=0,1 python train.py (all the default config)\nthen it come to 3~4 it/s after 100 step, \nit's incredible becase when I run CUDA_VISIBLE_DEVICES=0  python train.py it always come to about 0.5 it/s\nthen I print the step time cost ,:\n[0814 12:15:17 @base.py:247] hvd rank 0, (global_step 61) begin at 1534248917.7014177\n 29%|############################1                                                                    |61/210[01:37<01:42, 1.46it/s][0814 12:15:18 @base.py:254] hvd rank 0, (global_step 61) finished at 1534248918.3272123, time:0.6257946491241455.\n[0814 12:15:18 @base.py:247] hvd rank 0, (global_step 62) begin at 1534248918.3275385\n 30%|############################6                                                                    |62/210[01:38<01:55, 1.28it/s][0814 12:15:19 @base.py:254] hvd rank 0, (global_step 62) finished at 1534248919.20141, time:0.8738715648651123.\n[0814 12:15:19 @base.py:247] hvd rank 0, (global_step 63) begin at 1534248919.2017226\n 30%|#############################1                                                                   |63/210[01:39<01:48, 1.35it/s][0814 12:15:19 @base.py:254] hvd rank 0, (global_step 63) finished at 1534248919.9030836, time:0.7013609409332275.\n[0814 12:15:19 @base.py:247] hvd rank 0, (global_step 64) begin at 1534248919.9034119\n[0814 12:15:20 @base.py:254] hvd rank 0, (global_step 64) finished at 1534248920.0661807, time:0.16276884078979492.\n[0814 12:15:20 @base.py:247] hvd rank 0, (global_step 65) begin at 1534248920.0665429\n[0814 12:15:20 @base.py:254] hvd rank 0, (global_step 65) finished at 1534248920.2433767, time:0.1768338680267334.\n[0814 12:15:20 @base.py:247] hvd rank 0, (global_step 66) begin at 1534248920.243729\n 31%|##############################4                                                                  |66/210[01:40<01:15, 1.91it/s][0814 12:15:20 @base.py:254] hvd rank 0, (global_step 66) finished at 1534248920.8267455, time:0.5830163955688477.\n[0814 12:15:20 @base.py:247] hvd rank 0, (global_step 67) begin at 1534248920.8270683\n[0814 12:15:20 @base.py:254] hvd rank 0, (global_step 67) finished at 1534248920.9875398, time:0.16047143936157227.\n[0814 12:15:20 @base.py:247] hvd rank 0, (global_step 68). my question is  2 gpu  run faster  than 1 gpu without gradient allreduce\nI can not find some way to debug it.\nCUDA_VISIBLE_DEVICES=0,1 python train.py ,I run many times and speed up from about 60 step ,then come to 4 it/s.\n(global_step 137) finished at 1534294900.3835046, time:0.11500763893127441.\n[0815 01:01:40 @base.py:247]  (global_step 138) begin at 1534294900.3839097\n 66%|###############################################################                                 |138/210[01:33<00:15, 4.53it/s][0815 01:01:40 @base.py:254]  (global_step 138) finished at 1534294900.504368, time:0.12045836448669434.\n[0815 01:01:40 @base.py:247]  (global_step 139) begin at 1534294900.5047371\n[0815 01:01:40 @base.py:254]  (global_step 139) finished at 1534294900.6229901, time:0.11825299263000488.\n[0815 01:01:40 @base.py:247]  (global_step 140) begin at 1534294900.6234102\n[0815 01:01:40 @base.py:254]  (global_step 140) finished at 1534294900.7403178, time:0.11690759658813477.\n[0815 01:01:40 @base.py:247]  (global_step 141) begin at 1534294900.7407358\n[0815 01:01:40 @base.py:254]  (global_step 141) finished at 1534294900.8584886, time:0.1177527904510498.\n[0815 01:01:40 @base.py:247]  (global_step 142) begin at 1534294900.8589025\n 68%|################################################################9                               |142/210[01:34<00:16, 4.12it/s][0815 01:01:41 @base.py:254]  (global_step 142) finished at 1534294901.5618732, time:0.7029707431793213.\n[0815 01:01:41 @base.py:247]  (global_step 143) begin at 1534294901.5622423\n[0815 01:01:41 @base.py:254]  (global_step 143) finished at 1534294901.6763937, time:0.1141514778137207.\n[0815 01:01:41 @base.py:247]  (global_step 144) begin at 1534294901.676972\n[0815 01:01:42 @base.py:254]  (global_step 144) finished at 1534294902.0140057, time:0.337033748626709.\n[0815 01:01:42 @base.py:247]  (global_step 145) begin at 1534294902.0145013\n 69%|##################################################################2                             |145/210[01:34<00:14, 4.58it/s][0815 01:01:42 @base.py:254]  (global_step 145) finished at 1534294902.143973, time:0.1294717788696289.\n[0815 01:01:42 @base.py:247]  (global_step 146) begin at 1534294902.1443706\n[0815 01:01:42 @base.py:254]  (global_step 146) finished at 1534294902.8292904, time:0.6849198341369629.\n[0815 01:01:42 @base.py:247]  (global_step 147) begin at 1534294902.829791\n[0815 01:01:43 @base.py:254]  (global_step 147) finished at 1534294903.529799, time:0.7000079154968262.\n[0815 01:01:43 @base.py:247]  (global_step 148) begin at 1534294903.5302541\n 70%|###################################################################6                            |148/210[01:36<00:22, 2.78it/s][0815 01:01:43 @base.py:254]  (global_step 148) finished at 1534294903.6478417, time:0.11758756637573242.\n[0815 01:01:43 @base.py:247]  (global_step 149) begin at 1534294903.6482408\n[0815 01:01:43 @base.py:254]  (global_step 149) finished at 1534294903.772947, time:0.12470626831054688.\n[0815 01:01:43 @base.py:247]  (global_step 150) begin at 1534294903.7733433\n[0815 01:01:43 @base.py:254]  (global_step 150) finished at 1534294903.88845, time:0.11510658264160156.\n[0815 01:01:43 @base.py:247]  (global_step 151) begin at 1534294903.8888514\n 72%|#####################################################################                           |151/210[01:36<00:15, 3.75it/s][0815 01:01:44 @base.py:254]  (global_step 151) finished at 1534294904.1664782, time:0.2\nCUDA_VISIBLE_DEVICES=0 python train.py \n66%|###############################################################5                                |139/210[03:49<01:43, 0.69it/s][0815 01:09:38 @base.py:254]  (global_step 139) finished at 1534295378.650176, time:2.45257830619812.\n[0815 01:09:38 @base.py:247]  (global_step 140) begin at 1534295378.650516\n 67%|################################################################                                |140/210[03:51<02:08, 0.55it/s][0815 01:09:40 @base.py:254]  (global_step 140) finished at 1534295380.852188, time:2.201672077178955.\n[0815 01:09:40 @base.py:247]  (global_step 141) begin at 1534295380.8525262\n[0815 01:09:41 @base.py:254]  (global_step 141) finished at 1534295381.1603434, time:0.3078172206878662.\n[0815 01:09:41 @base.py:247]  (global_step 142) begin at 1534295381.1607199\n 68%|################################################################9                               |142/210[03:54<01:46, 0.64it/s][0815 01:09:43 @base.py:254]  (global_step 142) finished at 1534295383.447481, time:2.2867610454559326.\n[0815 01:09:43 @base.py:247]  (global_step 143) begin at 1534295383.447814\n[0815 01:09:43 @base.py:254]  (global_step 143) finished at 1534295383.7949831, time:0.3471691608428955.\n[0815 01:09:43 @base.py:247]  (global_step 144) begin at 1534295383.7953558\n 69%|#################################################################8                              |144/210[03:55<01:02, 1.05it/s][0815 01:09:44 @base.py:254]  (global_step 144) finished at 1534295384.1309144, time:0.3355586528778076.\n[0815 01:09:44 @base.py:247]  (global_step 145) begin at 1534295384.1312623\n[0815 01:09:44 @base.py:254]  (global_step 145) finished at 1534295384.4696224, time:0.338360071182251.\n[0815 01:09:44 @base.py:247]  (global_step 146) begin at 1534295384.4699774\n 70%|##################################################################7                             |146/210[03:57<01:08, 0.93it/s][0815 01:09:46 @base.py:254]  (global_step 146) finished at 1534295386.5130866, time:2.043109178543091.\n[0815 01:09:46 @base.py:247]  (global_step 147) begin at 1534295386.5134234\n[0815 01:09:47 @base.py:254]  (global_step 147) finished at 1534295387.00269, time:0.48926663398742676.\n[0815 01:09:47 @base.py:247]  (global_step 148) begin at 1534295387.0030763\n 70%|###################################################################6                            |148/210[03:58<00:46, 1.34it/s][0815 01:09:47 @base.py:254]  (global_step 148) finished at 1534295387.3571424, time:0.3540661334991455.\n[0815 01:09:47 @base.py:247]  (global_step 149) begin at 1534295387.3574877\n 71%|####################################################################1                           |149/210[04:00<01:39, 0.62it/s][0815 01:09:49 @base.py:254]  (global_step 149) finished at 1534295389.8615568, time:2.5040690898895264.\n[0815 01:09:49 @base.py:247]  (global_step 150) begin at 1534295389.8618948\n[0815 01:09:50 @base.py:254]  (global_step 150) finished at 1534295390.2188885, time:0.3569936752319336.\n[0815 01:09:50 @base.py:247]  (global_step 151) begin at 1534295390.219254\n 72%|#####################################################################                           |151/210[04:03<01:25, 0.69it/s]. I disable warmup by export TF_CUDNN_USE_AUTOTUNE=0 \nthe problem is still here.\nHow can multi gpu with allreduce run faster than 1 gpu. why  not add a control dependence between Stage and Unstage  to make unstage run before stage .\nso we can set the StagingArea  capacity to 1, it can work as the same way using smaller GPU memory. glad to hear that my suggestion is accepted~. I make a mistake in question 3, the MultiGPUGANTrainer just add and mean of all the loss ,not the variable gradient. I think it's a dangerous trick ,if this is right, why we allreduce the gradient instead of loss?\n. It's faster ,but may be wrong.\nIf reduce the loss is equal to reduce the gradient, why all other distribute trainning do the reduce of gradient?. you mean that  reduce the loss is equal to reduce the gradient?\nAnd the reduce of gradient is faster than reduce of loss?\nSize of gradient  is same with the shape of tranning variable , bigger than the loss,which is just scalar in common model. How can reduce of gradient is faster?\n. ",
    "asit03in": "Thanks!\nThings work for me with your edits. \nThe baseline network was getting to 90% validation accuracy.\nWith 90% sparsity I am getting very close to it (89.4%).\nAttached is the training script and log file with the results.\nlog.log.txt\nsparse-cifar-convnet.py.txt\n. A small note.\nThings work if I use self.trainer.sess.run(self.mask_update_op)\nIf I use, self.mask_update_op.run(), I get the following error.\n2018-06-07 17:04:47.019623: W tensorflow/core/kernels/queue_base.cc:277] _0_QueueInput/input_queue: Skipping cancelled enqueue attempt with queue not closed\nTraceback (most recent call last):\n  File \"sparse-cifar-convnet.py\", line 176, in <module>\n    launch_train_with_config(config, trainer)\n  File \"/home/akmishr1/sparse_lowprecision/tensorpack/tensorpack/train/interface.py\", line 90, in launch_train_with_config\n    extra_callbacks=config.extra_callbacks)\n[0607 17:04:47 @input_source.py:148] EnqueueThread QueueInput/input_queue Exited.\n  File \"/home/akmishr1/sparse_lowprecision/tensorpack/tensorpack/train/base.py\", line 301, in train_with_defaults\n    steps_per_epoch, starting_epoch, max_epoch)\n  File \"/home/akmishr1/sparse_lowprecision/tensorpack/tensorpack/train/base.py\", line 273, in train\n    self.main_loop(steps_per_epoch, starting_epoch, max_epoch)\n  File \"/home/akmishr1/sparse_lowprecision/tensorpack/tensorpack/utils/argtools.py\", line 181, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/akmishr1/sparse_lowprecision/tensorpack/tensorpack/train/base.py\", line 239, in main_loop\n    self._callbacks.trigger_step()\n  File \"/home/akmishr1/sparse_lowprecision/tensorpack/tensorpack/callbacks/group.py\", line 85, in trigger_step\n    cb.trigger_step()\n  File \"/home/akmishr1/sparse_lowprecision/tensorpack/tensorpack/callbacks/base.py\", line 138, in trigger_step\n    self._trigger_step()\n  File \"sparse-cifar-convnet.py\", line 101, in _trigger_step\n    self.mask_update_op.run()\nAttributeError: 'Tensor' object has no attribute 'run'\nClosing this thread for now.. ",
    "windid": "If you need, I will glad to share my TPU quota with you.. ",
    "xuke225": "It is indeed this reason\uff0cThanks a lot!. ",
    "brisker": "@ppwwyyxx \nif I want to train the model from float-32 full-precision model, what param should the \"args.load\" be? \nalexnet.npz?. @ppwwyyxx \ntwo questions:\n1. in here why did you quantize the activations after batchnorm,  but not before batchnorm?\n2.  in here, it seems that there is no quantization on the activations(or called \"outputs\") of the element-wise-adding operation in resnet, why?\nthanks a lot!. @ppwwyyxx \nin here it seems that there is no quantization on the activations(or called \"outputs\") of the element-wise-adding operation in resnet,  but this can be the input of the first conv layer of the next resblock, right?. @ppwwyyxx \nhere : https://github.com/tensorpack/tensorpack/blob/master/examples/DoReFa-Net/resnet-dorefa.py#L60\noutput of  get_stem_full function is not quanitzed, so here the return of function resblock: https://github.com/tensorpack/tensorpack/blob/master/examples/DoReFa-Net/resnet-dorefa.py#L80\n and the return of function group:\nhttps://github.com/tensorpack/tensorpack/blob/master/examples/DoReFa-Net/resnet-dorefa.py#L88\n are not quantized. So the group output is the input of the next group--- But the first layer in a group is a conv layer.\n. @ppwwyyxx \nI have downloaded this and put it into the  ~/tensorpack_data/ilsvrc_metadata, but it seems that the code will delete what I have manually downloaded.. @ppwwyyxx \nstill this error\n\nbetween two \"ls\" , I run the alex_dorefa.py the folder caffe_ilsvrc12 exists.. @ppwwyyxx \nnew error:\nroot@997991b14e71:/data/home/users/ccc/projects/tensorpack/examples/DoReFa-Net# ./alexnet-dorefa.py --dorefa 1,2,6 --data /data/data/ImageNetOrigin --gpu 4,5,6,7\n/root/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n[0703 07:15:20 @logger.py:109] WRN Log directory train_log/alexnet-dorefa-1,2,6 exists! Use 'd' to delete it. \n[0703 07:15:20 @logger.py:112] WRN If you're resuming from a previous run, you can choose to keep it.\nPress any other key to exit. \nSelect Action: k (keep) / d (delete) / q (quit):d\n[0703 07:15:22 @logger.py:74] Argv: ./alexnet-dorefa.py --dorefa 1,2,6 --data /data/data/ImageNetOrigin --gpu 4,5,6,7\n[0703 07:15:22 @alexnet-dorefa.py:222] Batch per tower: 64\n[0703 07:15:22 @fs.py:88] WRN Env var $TENSORPACK_DATASET not set, using /root/tensorpack_data for datasets.\n[0703 07:15:23 @parallel.py:290] [PrefetchDataZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.\n[0703 07:15:23 @ilsvrc.py:128] [ILSVRC12] Assuming directory /data/data/ImageNetOrigin/val has 'original' structure.\n[0703 07:15:23 @training.py:51] [DataParallel] Training a model of 4 towers.\n[0703 07:15:23 @interface.py:31] Automatically applying QueueInput on the DataFlow.\n[0703 07:15:23 @interface.py:42] Automatically applying StagingInput on the DataFlow.\n[0703 07:15:23 @input_source.py:195] Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...\n[0703 07:15:23 @training.py:111] Building graph for training tower 0 on device /gpu:0 ...\n[0703 07:15:23 @registry.py:121] conv0 input: [None, 3, 224, 224]\n[0703 07:15:23 @registry.py:129] conv0 output: [None, 96, 54, 54]\n[0703 07:15:23 @registry.py:121] conv1 input: [None, 96, 54, 54]\n[0703 07:15:23 @alexnet-dorefa.py:79] Quantizing weight conv1/W\n[0703 07:15:23 @registry.py:129] conv1 output: [None, 256, 54, 54]\n[0703 07:15:23 @registry.py:121] pool1 input: [None, 256, 54, 54]\n[0703 07:15:23 @registry.py:129] pool1 output: [None, 256, 27, 27]\n[0703 07:15:23 @registry.py:121] conv2 input: [None, 256, 27, 27]\n[0703 07:15:23 @alexnet-dorefa.py:79] Quantizing weight conv2/W\n[0703 07:15:23 @registry.py:129] conv2 output: [None, 384, 27, 27]\n[0703 07:15:23 @registry.py:121] pool2 input: [None, 384, 27, 27]\n[0703 07:15:23 @registry.py:129] pool2 output: [None, 384, 14, 14]\n[0703 07:15:23 @registry.py:121] conv3 input: [None, 384, 14, 14]\n[0703 07:15:23 @alexnet-dorefa.py:79] Quantizing weight conv3/W\n[0703 07:15:24 @registry.py:129] conv3 output: [None, 384, 14, 14]\n[0703 07:15:24 @registry.py:121] conv4 input: [None, 384, 14, 14]\n[0703 07:15:24 @alexnet-dorefa.py:79] Quantizing weight conv4/W\n[0703 07:15:24 @registry.py:129] conv4 output: [None, 256, 14, 14]\n[0703 07:15:24 @registry.py:121] pool4 input: [None, 256, 14, 14]\n[0703 07:15:24 @registry.py:129] pool4 output: [None, 256, 6, 6]\n[0703 07:15:24 @registry.py:121] fc0 input: [None, 256, 6, 6]\n[0703 07:15:24 @alexnet-dorefa.py:79] Quantizing weight fc0/W\n[0703 07:15:24 @registry.py:129] fc0 output: [None, 4096]\nTraceback (most recent call last):\n  File \"./alexnet-dorefa.py\", line 227, in <module>\n    launch_train_with_config(config, SyncMultiGPUTrainerReplicated(nr_tower))\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorpack/train/interface.py\", line 81, in launch_train_with_config\n    model._build_graph_get_cost, model.get_optimizer)\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorpack/utils/argtools.py\", line 181, in wrapper\n    return func(*args, **kwargs)\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorpack/train/tower.py\", line 172, in setup_graph\n    train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorpack/train/trainers.py\", line 166, in _setup_graph\n    self._make_get_grad_fn(input, get_cost_fn, get_opt_fn), get_opt_fn)\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorpack/graph_builder/training.py\", line 222, in build\n    use_vs=[False] + [True] * (len(self.towers) - 1))\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorpack/graph_builder/training.py\", line 118, in build_on_towers\n    ret.append(func())\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorpack/train/tower.py\", line 199, in get_grad_fn\n    cost = get_cost_fn(*input.get_input_tensors())\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorpack/tfutils/tower.py\", line 255, in __call__\n    output = self._tower_fn(*args)\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorpack/graph_builder/model_desc.py\", line 235, in _build_graph_get_cost\n    ret = self.build_graph(*inputs)\n  File \"/data/home/users/ccc/projects/tensorpack/examples/DoReFa-Net/imagenet_utils.py\", line 181, in build_graph\n    logits = self.get_logits(image)\n  File \"./alexnet-dorefa.py\", line 122, in get_logits\n    .BatchNorm('bnfc0')\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorpack/models/linearwrap.py\", line 47, in layer_func\n    ret = layer(name, self._t, *args, **kwargs)\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorpack/models/registry.py\", line 124, in wrapped_func\n    outputs = func(*args, **actual_args)\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorpack/models/tflayer.py\", line 64, in decorated_func\n    return func(inputs, **ret)\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorpack/models/batch_norm.py\", line 176, in BatchNorm\n    xn = layer.apply(inputs, training=training, scope=tf.get_variable_scope())\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 503, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 443, in __call__\n    self.build(input_shapes[0])\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/normalization.py\", line 150, in build\n    'to its original shape. Got input rank: ', ndim)\nValueError: ('Only 4D inputs are currently supported with fused batch norm. Consider reshaping the input to 4D and reshape the output back to its original shape. Got input rank: ', 2)\n. @ppwwyyxx \nenvironment:\ntensorflow1.13.0(in docker) cuda8.0 cudnn6 anaconda2. @ppwwyyxx \nstill error\n\n. @ppwwyyxx \nsorry, I forgort to modify the batch_norm.py in the anaconda/lib/python2.7/xxxxx ..\nafter modifying that file, the error changes to:\n``\nroot@997991b14e71:/data/home/users/wangdu/projects/tensorpack/examples/DoReFa-Net# ./alexnet-dorefa.py --dorefa 1,2,6 --data /data/data/ImageNetOrigin --gpu 4,5,6,7\n/root/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype fromfloattonp.floatingis deprecated. In future, it will be treated asnp.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n[0703 07:51:08 @logger.py:109] WRN Log directory train_log/alexnet-dorefa-1,2,6 exists! Use 'd' to delete it. \n[0703 07:51:08 @logger.py:112] WRN If you're resuming from a previous run, you can choose to keep it.\nPress any other key to exit. \nSelect Action: k (keep) / d (delete) / q (quit):d\n[0703 07:51:10 @logger.py:74] Argv: ./alexnet-dorefa.py --dorefa 1,2,6 --data /data/data/ImageNetOrigin --gpu 4,5,6,7\n[0703 07:51:10 @alexnet-dorefa.py:222] Batch per tower: 64\n[0703 07:51:10 @fs.py:88] WRN Env var $TENSORPACK_DATASET not set, using /root/tensorpack_data for datasets.\n[0703 07:51:12 @parallel.py:290] [PrefetchDataZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.\n[0703 07:51:12 @ilsvrc.py:128] [ILSVRC12] Assuming directory /data/data/ImageNetOrigin/val has 'original' structure.\n[0703 07:51:12 @training.py:51] [DataParallel] Training a model of 4 towers.\n[0703 07:51:12 @interface.py:31] Automatically applying QueueInput on the DataFlow.\n[0703 07:51:12 @interface.py:42] Automatically applying StagingInput on the DataFlow.\n[0703 07:51:12 @input_source.py:195] Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...\n[0703 07:51:12 @training.py:111] Building graph for training tower 0 on device /gpu:0 ...\n[0703 07:51:12 @registry.py:121] conv0 input: [None, 3, 224, 224]\n[0703 07:51:12 @registry.py:129] conv0 output: [None, 96, 54, 54]\n[0703 07:51:12 @registry.py:121] conv1 input: [None, 96, 54, 54]\n[0703 07:51:12 @alexnet-dorefa.py:79] Quantizing weight conv1/W\n[0703 07:51:12 @registry.py:129] conv1 output: [None, 256, 54, 54]\n[0703 07:51:12 @registry.py:121] pool1 input: [None, 256, 54, 54]\n[0703 07:51:12 @registry.py:129] pool1 output: [None, 256, 27, 27]\n[0703 07:51:12 @registry.py:121] conv2 input: [None, 256, 27, 27]\n[0703 07:51:12 @alexnet-dorefa.py:79] Quantizing weight conv2/W\n[0703 07:51:12 @registry.py:129] conv2 output: [None, 384, 27, 27]\n[0703 07:51:12 @registry.py:121] pool2 input: [None, 384, 27, 27]\n[0703 07:51:12 @registry.py:129] pool2 output: [None, 384, 14, 14]\n[0703 07:51:12 @registry.py:121] conv3 input: [None, 384, 14, 14]\n[0703 07:51:12 @alexnet-dorefa.py:79] Quantizing weight conv3/W\n[0703 07:51:12 @registry.py:129] conv3 output: [None, 384, 14, 14]\n[0703 07:51:12 @registry.py:121] conv4 input: [None, 384, 14, 14]\n[0703 07:51:12 @alexnet-dorefa.py:79] Quantizing weight conv4/W\n[0703 07:51:12 @registry.py:129] conv4 output: [None, 256, 14, 14]\n[0703 07:51:12 @registry.py:121] pool4 input: [None, 256, 14, 14]\n[0703 07:51:12 @registry.py:129] pool4 output: [None, 256, 6, 6]\n[0703 07:51:12 @registry.py:121] fc0 input: [None, 256, 6, 6]\n[0703 07:51:12 @alexnet-dorefa.py:79] Quantizing weight fc0/W\n[0703 07:51:12 @registry.py:129] fc0 output: [None, 4096]\n[0703 07:51:12 @registry.py:121] fc1 input: [None, 4096]\n[0703 07:51:12 @alexnet-dorefa.py:79] Quantizing weight fc1/W\n[0703 07:51:12 @registry.py:129] fc1 output: [None, 4096]\n[0703 07:51:12 @registry.py:121] fct input: [None, 4096]\n[0703 07:51:12 @registry.py:129] fct output: [None, 1000]\n[0703 07:51:12 @regularize.py:88] regularize_cost() found 3 variables to regularize.\n[0703 07:51:12 @regularize.py:19] The following tensors will be regularized: fc0/W:0, fc1/W:0, fct/W:0\n[0703 07:51:13 @training.py:111] Building graph for training tower 1 on device /gpu:1 ...\n[0703 07:51:13 @alexnet-dorefa.py:79] Quantizing weight tower1/conv1/W\n[0703 07:51:13 @alexnet-dorefa.py:79] Quantizing weight tower1/conv2/W\n[0703 07:51:13 @alexnet-dorefa.py:79] Quantizing weight tower1/conv3/W\n[0703 07:51:13 @alexnet-dorefa.py:79] Quantizing weight tower1/conv4/W\n[0703 07:51:13 @alexnet-dorefa.py:79] Quantizing weight tower1/fc0/W\n[0703 07:51:13 @alexnet-dorefa.py:79] Quantizing weight tower1/fc1/W\n[0703 07:51:13 @regularize.py:88] regularize_cost() found 3 variables to regularize.\n[0703 07:51:14 @training.py:111] Building graph for training tower 2 on device /gpu:2 ...\n[0703 07:51:14 @alexnet-dorefa.py:79] Quantizing weight tower2/conv1/W\n[0703 07:51:14 @alexnet-dorefa.py:79] Quantizing weight tower2/conv2/W\n[0703 07:51:14 @alexnet-dorefa.py:79] Quantizing weight tower2/conv3/W\n[0703 07:51:14 @alexnet-dorefa.py:79] Quantizing weight tower2/conv4/W\n[0703 07:51:14 @alexnet-dorefa.py:79] Quantizing weight tower2/fc0/W\n[0703 07:51:14 @alexnet-dorefa.py:79] Quantizing weight tower2/fc1/W\n[0703 07:51:15 @regularize.py:88] regularize_cost() found 3 variables to regularize.\n[0703 07:51:15 @training.py:111] Building graph for training tower 3 on device /gpu:3 ...\n[0703 07:51:15 @alexnet-dorefa.py:79] Quantizing weight tower3/conv1/W\n[0703 07:51:15 @alexnet-dorefa.py:79] Quantizing weight tower3/conv2/W\n[0703 07:51:16 @alexnet-dorefa.py:79] Quantizing weight tower3/conv3/W\n[0703 07:51:16 @alexnet-dorefa.py:79] Quantizing weight tower3/conv4/W\n[0703 07:51:16 @alexnet-dorefa.py:79] Quantizing weight tower3/fc0/W\n[0703 07:51:16 @alexnet-dorefa.py:79] Quantizing weight tower3/fc1/W\n[0703 07:51:16 @regularize.py:88] regularize_cost() found 3 variables to regularize.\n[0703 07:51:18 @training.py:318] 'sync_variables_from_main_tower' includes 243 operations.\n[0703 07:51:18 @model_utils.py:63] Trainable Variables: \nname           shape                  dim\n\nconv0/W:0      [12, 12, 3, 96]      41472\nconv0/b:0      [96]                    96\nconv1/W:0      [5, 5, 48, 256]     307200\nbn1/beta:0     [256]                  256\nbn1/gamma:0    [256]                  256\nconv2/W:0      [3, 3, 256, 384]    884736\nbn2/beta:0     [384]                  384\nbn2/gamma:0    [384]                  384\nconv3/W:0      [3, 3, 192, 384]    663552\nbn3/beta:0     [384]                  384\nbn3/gamma:0    [384]                  384\nconv4/W:0      [3, 3, 192, 256]    442368\nbn4/beta:0     [256]                  256\nbn4/gamma:0    [256]                  256\nfc0/W:0        [9216, 4096]      37748736\nfc0/b:0        [4096]                4096\nbnfc0/beta:0   [4096]                4096\nbnfc0/gamma:0  [4096]                4096\nfc1/W:0        [4096, 4096]      16777216\nbnfc1/beta:0   [4096]                4096\nbnfc1/gamma:0  [4096]                4096\nfct/W:0        [4096, 1000]       4096000\nfct/b:0        [1000]                1000\nTotal #vars=23, #params=60985416, size=232.64MB\n[0703 07:51:18 @base.py:174] Setup callbacks graph ...\n[0703 07:51:18 @parallel.py:52] WRN TENSORPACK_PIPEDIR is not used on Linux any more! Abstract sockets will be used.\nTraceback (most recent call last):\n  File \"./alexnet-dorefa.py\", line 227, in \n    launch_train_with_config(config, SyncMultiGPUTrainerReplicated(nr_tower))\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorpack/train/interface.py\", line 90, in launch_train_with_config\n    extra_callbacks=config.extra_callbacks)\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorpack/train/base.py\", line 306, in train_with_defaults\n    steps_per_epoch, starting_epoch, max_epoch)\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorpack/train/base.py\", line 276, in train\n    self.setup_callbacks(callbacks, monitors)\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorpack/utils/argtools.py\", line 181, in wrapper\n    return func(args, *kwargs)\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorpack/train/base.py\", line 176, in setup_callbacks\n    self._callbacks.setup_graph(weakref.proxy(self))\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorpack/callbacks/base.py\", line 52, in setup_graph\n    self._setup_graph()\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorpack/callbacks/group.py\", line 66, in _setup_graph\n    cb.setup_graph(self.trainer)\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorpack/callbacks/base.py\", line 52, in setup_graph\n    self._setup_graph()\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorpack/input_source/input_source.py\", line 507, in _setup_graph\n    unstage_op = tf.group(unstage_ops, name='unstage_all')\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2895, in group\n    dev = inp.device\nAttributeError: 'list' object has no attribute 'device'\nPrefetchDataZMQ successfully cleaned-up.\n```\n. @ppwwyyxx \nI will give a try instantly. Before this, I want to ask that, I have installed the tensorpack using \"pip install -U git+https://github.com/tensorpack/tensorpack.git\" , in the anaconda2 environment. So if I  want to manually make modifications based on your new commits, I need to modified the code files in the anaconda/lib/python2.7/tensorpack/xxxxxx ?. @ppwwyyxx \nno training error now . thanks a lot\n1. why is the training so slow?( in caffe, alexnet for imagenet should be around  4 iter/s in speed) I am using 4 titan xp.\n\n\nbesides,  if I want to train the quantized model based on the 32-bit full precision model, will simply \"--load alexnet.npz\" work? I tried, but error occurs.\nhow to train dorefa on cifar100? I tried the cifar-convnet.py, and download the dataset manullay and then unzip into the right folder, like this: (just like what I did in the alexnet-imagenet.py)\n\nbut why the code still download the cifar-100-python.tar.gz automatically, which leads to error?\ncan not find where to define \"how many epoches do an inference during training\". where is these parameters? the code is a little confusing to me. @ppwwyyxx \nfor the second point, the alexnet.npz is downloaded from here:\nhttp://models.tensorpack.com/DoReFa-Net/. @ppwwyyxx \nbut still error after I downloaded the cifar-100 data manully and run the cifar-convnet in the /examples/basics foler.\n\n. @ppwwyyxx \ngiven that there is clip operation to [0,1] for the activations in dorefa-net, what is the pixel value range of the  network input?\n[-1,1] or [-255,255]?? (since you have substracted mean). Will this influence the result?. @ppwwyyxx \nIn dorefa-net, most of the convoluion and fully connected layers have no bias term, is this true? If it is, why?\nhttps://github.com/tensorpack/tensorpack/blob/master/examples/DoReFa-Net/alexnet-dorefa.py#L93\n. @ppwwyyxx \nin the paper the author mentioned the training-from-scratch and fineuning-from-full-precision experiments. If I set --dorefa 32,32,32, then the model is full-precision , right? It can be used to be finetuning?\nIf I simply want to do evaluation every 1 epoch, and save model every 5 epoches, how to write the .PeriodicTrigger?\nGiven that there is clip operation to [0,1] on the activations, we need to zero the gradients corresponding to these 0s and 1s. But I can not find this zero-gradient operation in the code. Is this due to the auto-grad mechanism of tensorflow?. @ppwwyyxx \nGiven that there is clip operation to [0,1] on the activations, we need to zero the gradients corresponding to these 0s and 1s. But I can not find this zero-gradient operation in the code. Is this due to the auto-grad mechanism of tensorflow?\nIt seems that there is no \"de-normalization\" process in the code, since there is the following formula to normalize the weight to range[-1,1]\n\nIf there is no de-normalization (or call it de-quantization), the data distribution seems to be different from the original weights. Is this correct? Or have I understood something wrong?\n\n. @ppwwyyxx \nBut what I am refering to is the weights of convolutions, not activations. Why can batchnorm bring the data distribution of convolution weights back?. @ppwwyyxx \nSo did you mean that this implementation does not have the de-normalization process? This does not influence the accuracy\uff1f. @ppwwyyxx \nI have trained a model and save it like this:\n\nBut how to load this for finetuning? The alexnet-dorefa.py seems not to generate the ckpt model files.. @ppwwyyxx \nI run --load 2925, and\nthe log has the following hints:\n[0707 06:52:15 @steps.py:126] Start training with global_step=2925\nso this restore just load the weights, not any more information loaded, right? (for example load checkpoint  learning rate stored in the model-2925). @ppwwyyxx \nwhat if I only want to load the weights?\nDoes tensorpack support this?. @ppwwyyxx \nso did you mean I can simply run the \npython dump-model-params.py --input model-2925 --output weights.npz --meta graph.meta\nand then I can load this weights.npz file for only loading the weights?. @ppwwyyxx \na little confused \ndump-model-params.py [-h] --meta META input output\nI just run\npython dump-model-params.py --meta graph.meta model-2925 weights.npz\nnot correct?. @ppwwyyxx \nI have successfully generate this weights.npz, but when I load this for finetuning, error is like this:\ntensorflow.python.framework.errors_impl.DataLossError: Unable to open table file ~/projects/tensorpack/examples/DoReFa-Net/float32/cifar100/train_log/alexnet-dorefa-\n32,32,32/model-7800-weights.npz: Data loss: not an sstable (bad magic number): \nperhaps your file is in a different file format and you need to use a different restore operator?\nPrefetchDataZMQ successfully cleaned-up.. @ppwwyyxx \n--load \"weights.npz\"\nchanging to DictRestore() still error......\nTraceback (most recent call last):\n  File \"alexnet-dorefa.py\", line 279, in <module>\n    config.session_init = DictRestore(args.load)\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorpack-0.8.6-py2.7.egg/tensorpack/tfutils/sessinit.py\", line 195, in __init__\n    assert isinstance(variable_dict, dict), type(variable_dict)\nAssertionError: <type 'str'>. @ppwwyyxx \nBut this weights.npz file is created from \npython dump-model-params.py --meta graph.meta model-2925 weights.npz.... \nThis should be a dict, right?. \n@ppwwyyxx \nIf I train a dorefa-32-32-32 model, can I regard this model as the full-precision model, as described in your paper?. @ppwwyyxx\nI run a --dorefa 8,8,8 experiment on imagenet, but why the val-error-top1 is still >90% after 45 epoches? I just set the default settings in the alexnet_dorefa.py file\n[0710 01:38:19 @base.py:237] Start Epoch 45 ...\n100%|################################################################################################|5000/5000[16:57<00:00, 4.92it/s]\n[0710 01:55:16 @base.py:247] Epoch 45 (global_step 225000) finished, time:16 minutes 57 seconds.\n[0710 01:55:16 @graph.py:73] Running Op sync_variables/sync_variables_from_main_tower ...\n[0710 01:55:28 @saver.py:84] Model saved to train_log/alexnet-dorefa-8,8,8/model-225000.\n100%|##################################################################################################|782/782[01:53<00:00, 6.90it/s]\n[0710 01:57:21 @monitor.py:440] QueueInput/queue_size: 49.05\n[0710 01:57:21 @monitor.py:440] l2_regularize_loss: 1.0566\n[0710 01:57:21 @monitor.py:440] param-summary/conv0/W-rms: 0.093548\n[0710 01:57:21 @monitor.py:440] param-summary/conv1/W-rms: 0.10669\n[0710 01:57:21 @monitor.py:440] param-summary/conv2/W-rms: 0.1022\n[0710 01:57:21 @monitor.py:440] param-summary/conv3/W-rms: 0.1015\n[0710 01:57:21 @monitor.py:440] param-summary/conv4/W-rms: 0.10491\n[0710 01:57:21 @monitor.py:440] param-summary/fc0/W-rms: 0.0076856\n[0710 01:57:21 @monitor.py:440] param-summary/fc1/W-rms: 0.0070158\n[0710 01:57:21 @monitor.py:440] param-summary/fct/W-rms: 0.016896\n[0710 01:57:21 @monitor.py:440] train-error-top1: 0.46866\n[0710 01:57:21 @monitor.py:440] train-error-top5: 0.2167\n[0710 01:57:21 @monitor.py:440] val-error-top1: 0.95656\n[0710 01:57:21 @monitor.py:440] val-error-top5: 0.87892\n[0710 01:57:21 @monitor.py:440] xentropy-loss: 2.0036\n. @ppwwyyxx \nI tried to modify the resnet-dorefa.py to train the resnet18-dorefanet, and before train the quantization model, I just want to pretrain a full-precision model. So firstly I run python resnet-dorefa.py --dorefa 32,32,32 ..., but the loss just do not drop after several epoches.  Any advice is appreciated. The modified scripts are: \nresnet-dorefa.py: \n```\n!/usr/bin/env python\n-- coding: utf-8 --\nFile: resnet-dorefa.py\nimport cv2\nimport tensorflow as tf\nimport argparse\nimport numpy as np\nimport os\nfrom tensorpack import *\nfrom tensorpack.dataflow import dataset\nfrom tensorpack.tfutils.varreplace import remap_variables\nfrom tensorpack.tfutils.summary import add_param_summary\nfrom imagenet_utils import get_imagenet_dataflow, fbresnet_augmentor, ImageNetModel\nfrom imagenet_utils import ImageNetModel, eval_on_ILSVRC12, fbresnet_augmentor\nfrom dorefa import get_dorefa\nfrom tensorpack.utils.gpu import get_num_gpu\n\"\"\"\nThis script loads the pre-trained ResNet-18 model with (W,A,G) = (1,4,32)\nIt has 59.2% top-1 and 81.5% top-5 validation error on ILSVRC12 validation set.\nTo run on images:\n    ./resnet-dorefa.py --load ResNet-18-14f.npz --run a.jpg b.jpg\nTo eval on ILSVRC validation set:\n    ./resnet-dorefa.py --load ResNet-18-14f.npz --eval --data /path/to/ILSVRC\n\"\"\"\nBITW = 1\nBITA = 4\nBITG = 32\nTOTAL_BATCH_SIZE = 256\nclass Model(ImageNetModel):\n    weight_decay = 0.0001\n    weight_decay_pattern = 'fc.*/W'\ndef get_logits(self, image):\n\n    if BITW == 't':\n        fw, fa, fg = get_dorefa(32, 32, 32)\n        fw = ternarize\n    else:\n        fw, fa, fg = get_dorefa(BITW, BITA, BITG)\n\n    # monkey-patch tf.get_variable to apply fw\n    def new_get_variable(v):\n        name = v.op.name\n        # don't binarize first and last layer\n        if not name.endswith('W'): #or 'conv0' in name or 'fct' in name:\n            return v\n        else:\n            logger.info(\"Quantizing weight {}\".format(v.op.name))\n            return fw(v)\n\n    def nonlin(x):\n        if BITA == 32:\n            return tf.nn.relu(x)    # still use relu for 32bit cases\n        return tf.clip_by_value(x, 0.0, 1.0)\n\n    def activate(x):\n        return fa(nonlin(x))\n    def resblock(x, channel, stride):\n        def get_stem_full(x):\n            return (LinearWrap(x)\n                    .Conv2D('c3x3a', channel, 3)\n                    .BatchNorm('stembn')\n                    .apply(activate)\n                    .Conv2D('c3x3b', channel, 3)())\n        channel_mismatch = channel != x.get_shape().as_list()[3]\n        if stride != 1 or channel_mismatch or 'pool1' in x.name:\n            # handling pool1 is to work around an architecture bug in our model\n            if stride != 1 or 'pool1' in x.name:\n                x = AvgPooling('pool', x, stride, stride)\n            x = BatchNorm('bn', x)\n            x = activate(x)\n            shortcut = Conv2D('shortcut', x, channel, 1)\n            stem = get_stem_full(x)\n        else:\n            shortcut = x\n            x = BatchNorm('bn', x)\n            x = activate(x)\n            stem = get_stem_full(x)\n        return shortcut + stem\n\n    def group(x, name, channel, nr_block, stride):\n        with tf.variable_scope(name + 'blk1'):\n            x = resblock(x, channel, stride)\n        for i in range(2, nr_block + 1):\n            with tf.variable_scope(name + 'blk{}'.format(i)):\n                x = resblock(x, channel, 1)\n        return x\n    with remap_variables(new_get_variable), \\\n            argscope([Conv2D, BatchNorm, MaxPooling], data_format='channels_first'), \\\n            argscope(BatchNorm, momentum=0.9, epsilon=1e-4), \\\n            argscope(Conv2D, use_bias=False):\n        logits = (LinearWrap(image)\n                  # use explicit padding here, because our private training framework has\n                  # different padding mechanisms from TensorFlow\n                  .tf.pad([[0, 0], [3, 2], [3, 2], [0, 0]])\n                  .Conv2D('conv1', 64, 7, stride=2, padding='VALID', use_bias=True)\n                  .tf.pad([[0, 0], [1, 1], [1, 1], [0, 0]], 'SYMMETRIC')\n                  .MaxPooling('pool1', 3, 2, padding='VALID')\n                  .apply(group, 'conv2', 64, 2, 1)\n                  .apply(group, 'conv3', 128, 2, 2)\n                  .apply(group, 'conv4', 256, 2, 2)\n                  .apply(group, 'conv5', 512, 2, 2)\n                  .BatchNorm('lastbn')\n                  .apply(nonlin)\n                  .GlobalAvgPooling('gap')\n                  .tf.multiply(49)  # this is due to a bug in our model design\n                  .FullyConnected('fct', 1000)())\n    add_param_summary(('.*/W', ['histogram', 'rms']))\n    tf.nn.softmax(logits, name='output')  # for prediction\n    return logits\n#def optimizer(self):\n    #lr = tf.get_variable('learning_rate', initializer=0.001, trainable=False)\n    #return tf.train.AdamOptimizer(lr, epsilon=1e-5)        \ndef optimizer(self):\n    lr = tf.get_variable('learning_rate', initializer=0.01, trainable=False)\n    return tf.train.MomentumOptimizer(lr, 0.9, use_nesterov=False)\n\ndef get_config():\n    data_train = get_data('train')\n    data_test = get_data('val')\nreturn TrainConfig(\n    dataflow=data_train,\n    callbacks=[\n        PeriodicTrigger(ModelSaver(), every_k_epochs=15),\n        ScheduledHyperParamSetter(\n            'learning_rate', [(30, 0.001), (60, 0.0001)]),\n        InferenceRunner(data_test,\n                        [ClassificationError('wrong-top1', 'val-error-top1'),\n                         ClassificationError('wrong-top5', 'val-error-top5')])\n    ],\n    model=Model(),\n    steps_per_epoch=1280000 // TOTAL_BATCH_SIZE,\n    max_epoch=91,\n)\n\ndef get_data(dataset_name):\n    isTrain = dataset_name == 'train'\n    augmentors = fbresnet_augmentor(isTrain)\n    return get_imagenet_dataflow(\n        args.data, dataset_name, BATCH_SIZE, augmentors)\nif name == 'main':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--gpu', help='the physical ids of GPUs to use')\n    parser.add_argument('--load', help='load a checkpoint, or a npz (given as the pretrained model)')\n    parser.add_argument('--data', help='ILSVRC dataset dir')\n    parser.add_argument('--dorefa', required=True,\n                        help='number of bits for W,A,G, separated by comma. W=\"t\" means TTQ')\n    parser.add_argument('--run', help='run on a list of images with the pretrained model', nargs='*')\n    args = parser.parse_args()\ndorefa = args.dorefa.split(',')\nif dorefa[0] == 't':\n    assert dorefa[1] == '32' and dorefa[2] == '32'\n    BITW, BITA, BITG = 't', 32, 32\nelse:\n    BITW, BITA, BITG = map(int, dorefa)\n\nif args.gpu:\n    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n\nif args.run:\n    assert args.load.endswith('.npz')\n    run_image(Model(), DictRestore(dict(np.load(args.load))), args.run)\n    sys.exit()\n\nnr_tower = max(get_num_gpu(), 1)\nBATCH_SIZE = TOTAL_BATCH_SIZE // nr_tower\nlogger.set_logger_dir(os.path.join(\n    'train_log', 'resnet18-dorefa-{}'.format(args.dorefa)))\nlogger.info(\"Batch per tower: {}\".format(BATCH_SIZE))\n\nconfig = get_config()\nif args.load:\n    config.session_init = SaverRestore(args.load)\nlaunch_train_with_config(config, SyncMultiGPUTrainerReplicated(nr_tower))\n\n```\nimagenet_utils.py:\n```\nimport cv2\nimport numpy as np\nimport multiprocessing\nimport tensorflow as tf\nfrom abc import abstractmethod\nfrom tensorpack import imgaug, dataset, ModelDesc\nfrom tensorpack.dataflow import (\n    AugmentImageComponent, PrefetchDataZMQ,\n    BatchData, MultiThreadMapData)\nfrom tensorpack.predict import PredictConfig, SimpleDatasetPredictor\nfrom tensorpack.utils.stats import RatioCounter\nfrom tensorpack.models import regularize_cost\nfrom tensorpack.tfutils.summary import add_moving_summary\nfrom tensorpack.utils import logger\nclass GoogleNetResize(imgaug.ImageAugmentor):\n    \"\"\"\n    crop 8%~100% of the original image\n    See Going Deeper with Convolutions by Google.\n    \"\"\"\n    def init(self, crop_area_fraction=0.08,\n                 aspect_ratio_low=0.75, aspect_ratio_high=1.333,\n                 target_shape=224):\n        self._init(locals())\ndef _augment(self, img, _):\n    h, w = img.shape[:2]\n    area = h * w\n    for _ in range(10):\n        targetArea = self.rng.uniform(self.crop_area_fraction, 1.0) * area\n        aspectR = self.rng.uniform(self.aspect_ratio_low, self.aspect_ratio_high)\n        ww = int(np.sqrt(targetArea * aspectR) + 0.5)\n        hh = int(np.sqrt(targetArea / aspectR) + 0.5)\n        if self.rng.uniform() < 0.5:\n            ww, hh = hh, ww\n        if hh <= h and ww <= w:\n            x1 = 0 if w == ww else self.rng.randint(0, w - ww)\n            y1 = 0 if h == hh else self.rng.randint(0, h - hh)\n            out = img[y1:y1 + hh, x1:x1 + ww]\n            out = cv2.resize(out, (self.target_shape, self.target_shape), interpolation=cv2.INTER_CUBIC)\n            return out\n    #out = imgaug.ResizeShortestEdge(self.target_shape, interp=cv2.INTER_CUBIC).augment(img)\n    out = imgaug.Resize(self.target_shape,interp=cv2.INTER_CUBIC).augment(img)\n\n    out = imgaug.CenterCrop(self.target_shape).augment(out)\n    return out\n\ndef fbresnet_augmentor(isTrain):\n    \"\"\"\n    Augmentor used in fb.resnet.torch, for BGR images in range [0,255].\n    \"\"\"\n    if isTrain:\n        augmentors = [\n            #GoogleNetResize(),\n            #imgaug.ResizeShortestEdge(256, cv2.INTER_CUBIC),\n            imgaug.Resize((256,256)),\n            imgaug.RandomCrop((224, 224)),\n            # It's OK to remove these augs if your CPU is not fast enough.\n            # Removing brightness/contrast/saturation does not have a significant effect on accuracy.\n            # Removing lighting leads to a tiny drop in accuracy.\n        imgaug.Flip(horiz=True),\n    ]\nelse:\n    augmentors = [\n        imgaug.Resize((256,256)),\n        imgaug.CenterCrop((224, 224)),\n    ]\nreturn augmentors\n\ndef get_imagenet_dataflow(\n        datadir, name, batch_size,\n        augmentors, parallel=None):\n    \"\"\"\n    See explanations in the tutorial:\n    http://tensorpack.readthedocs.io/en/latest/tutorial/efficient-dataflow.html\n    \"\"\"\n    assert name in ['train', 'val', 'test']\n    assert datadir is not None\n    assert isinstance(augmentors, list)\n    isTrain = name == 'train'\n    if parallel is None:\n        parallel = min(40, multiprocessing.cpu_count() // 2)  # assuming hyperthreading\n    if isTrain:\n        ds = dataset.ILSVRC12(datadir, name, shuffle=True)\n        ds = AugmentImageComponent(ds, augmentors, copy=False)\n        if parallel < 16:\n            logger.warn(\"DataFlow may become the bottleneck when too few processes are used.\")\n        ds = PrefetchDataZMQ(ds, parallel)\n        ds = BatchData(ds, batch_size, remainder=False)\n    else:\n        ds = dataset.ILSVRC12Files(datadir, name, shuffle=False)\n        aug = imgaug.AugmentorList(augmentors)\n    def mapf(dp):\n        fname, cls = dp\n        im = cv2.imread(fname, cv2.IMREAD_COLOR)\n        im = aug.augment(im)\n        return im, cls\n    ds = MultiThreadMapData(ds, parallel, mapf, buffer_size=2000, strict=True)\n    ds = BatchData(ds, batch_size, remainder=True)\n    ds = PrefetchDataZMQ(ds, 1)\nreturn ds\n\ndef eval_on_ILSVRC12(model, sessinit, dataflow):\n    pred_config = PredictConfig(\n        model=model,\n        session_init=sessinit,\n        input_names=['input', 'label'],\n        output_names=['wrong-top1', 'wrong-top5']\n    )\n    pred = SimpleDatasetPredictor(pred_config, dataflow)\n    acc1, acc5 = RatioCounter(), RatioCounter()\n    for top1, top5 in pred.get_result():\n        batch_size = top1.shape[0]\n        acc1.feed(top1.sum(), batch_size)\n        acc5.feed(top5.sum(), batch_size)\n    print(\"Top1 Error: {}\".format(acc1.ratio))\n    print(\"Top5 Error: {}\".format(acc5.ratio))\nclass ImageNetModel(ModelDesc):\n    image_shape = 224\n\"\"\"\nuint8 instead of float32 is used as input type to reduce copy overhead.\nIt might hurt the performance a liiiitle bit.\nThe pretrained models were trained with float32.\n\"\"\"\nimage_dtype = tf.uint8\n\n\"\"\"\nEither 'NCHW' or 'NHWC'\n\"\"\"\ndata_format = 'NCHW'\n\n\"\"\"\nWhether the image is BGR or RGB. If using DataFlow, then it should be BGR.\n\"\"\"\nimage_bgr = True\n\nweight_decay = 1e-4\n\n\"\"\"\nTo apply on normalization parameters, use '.*/W|.*/gamma|.*/beta'\n\"\"\"\nweight_decay_pattern = '.*/W'\n\n\"\"\"\nScale the loss, for whatever reasons (e.g., gradient averaging, fp16 training, etc)\n\"\"\"\nloss_scale = 1.\n\ndef inputs(self):\n    return [tf.placeholder(self.image_dtype, [None, self.image_shape, self.image_shape, 3], 'input'),\n            tf.placeholder(tf.int32, [None], 'label')]\n\ndef build_graph(self, image, label):\n    image = ImageNetModel.image_preprocess(image, bgr=self.image_bgr)\n    assert self.data_format in ['NCHW', 'NHWC']\n    if self.data_format == 'NCHW':\n        image = tf.transpose(image, [0, 3, 1, 2])\n\n    logits = self.get_logits(image)\n    loss = ImageNetModel.compute_loss_and_error(logits, label)\n\n    if self.weight_decay > 0:\n        wd_loss = regularize_cost(self.weight_decay_pattern,\n                                  tf.contrib.layers.l2_regularizer(self.weight_decay),\n                                  name='l2_regularize_loss')\n        add_moving_summary(loss, wd_loss)\n        total_cost = tf.add_n([loss, wd_loss], name='cost')\n    else:\n        total_cost = tf.identity(loss, name='cost')\n        add_moving_summary(total_cost)\n\n    if self.loss_scale != 1.:\n        logger.info(\"Scaling the total loss by {} ...\".format(self.loss_scale))\n        return total_cost * self.loss_scale\n    else:\n        return total_cost\n\n@abstractmethod\ndef get_logits(self, image):\n    \"\"\"\n    Args:\n        image: 4D tensor of ``self.input_shape`` in ``self.data_format``\n\n    Returns:\n        Nx#class logits\n    \"\"\"\n\ndef optimizer(self):\n    lr = tf.get_variable('learning_rate', initializer=0.1, trainable=False)\n    tf.summary.scalar('learning_rate-summary', lr)\n    return tf.train.MomentumOptimizer(lr, 0.9, use_nesterov=True)\n\n@staticmethod\ndef image_preprocess(image, bgr=True):\n    with tf.name_scope('image_preprocess'):\n        if image.dtype.base_dtype != tf.float32:\n            image = tf.cast(image, tf.float32)\n        image = image * (1.0 / 255)\n\n        mean = [0.485, 0.456, 0.406]    # rgb\n        std = [0.229, 0.224, 0.225]\n\n        if bgr:\n            mean = mean[::-1]\n            std = std[::-1]\n        image_mean = tf.constant(mean, dtype=tf.float32)\n        image_std = tf.constant(std, dtype=tf.float32)\n        image = (image - image_mean) / image_std\n        return image\n\n@staticmethod\ndef compute_loss_and_error(logits, label):\n    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n    loss = tf.reduce_mean(loss, name='xentropy-loss')\n\n    def prediction_incorrect(logits, label, topk=1, name='incorrect_vector'):\n        with tf.name_scope('prediction_incorrect'):\n            x = tf.logical_not(tf.nn.in_top_k(logits, label, topk))\n        return tf.cast(x, tf.float32, name=name)\n\n    wrong = prediction_incorrect(logits, label, 1, name='wrong-top1')\n    add_moving_summary(tf.reduce_mean(wrong, name='train-error-top1'))\n\n    wrong = prediction_incorrect(logits, label, 5, name='wrong-top5')\n    add_moving_summary(tf.reduce_mean(wrong, name='train-error-top5'))\n    return loss\n\nif name == 'main':\n    import argparse\n    from tensorpack.dataflow import TestDataSpeed\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data', required=True)\n    parser.add_argument('--batch', type=int, default=32)\n    parser.add_argument('--aug', choices=['train', 'val'], default='val')\n    args = parser.parse_args()\nif args.aug == 'val':\n    augs = fbresnet_augmentor(False)\nelif args.aug == 'train':\n    augs = fbresnet_augmentor(True)\ndf = get_imagenet_dataflow(\n    args.data, 'train', args.batch, augs)\n# For val augmentor, Should get >100 it/s (i.e. 3k im/s) here on a decent E5 server.\nTestDataSpeed(df).start()\n\n. @ppwwyyxx \nbut why did the resnet-dorefa have a `tf.multiply(49)` operation before the last full connected layer, in your implementation?. @ppwwyyxx \nafter training and getting the quantized model, how to print all the weights values of a specific layer, given the quantized model? (or save them into a .npy file). @ppwwyyxx \nI want to modify dorefa-net, but  I met a problem on how to define a learned variable which is like : `[alpha, a_1*alpha,a_2*alpha,...,a_k*alpha]`, in which `alpha`  and` a_1 ~ a_k` are all learned numbers in this variable.  Is this kind of variable supported in tensorpack?. @ppwwyyxx \nin tensorpack , how to print  the values of  a specific tensor?\nin standard tensorflow, it is like this:\nwith tf.Session() as sess:   \n           print(sess.run(product))\n           print (product.eval())\nor\n`sess.run(tf.Print(.....,[....]))`\nbut in tensorpack, I can not find where the current session is.. @ppwwyyxx \ndid you mean directly insert the following code into the training sciprt?\n `ProcessTensors(['tensor1', 'tensor2'], lambda tensor1, tensor2: print(tensor1, tensor2, tensor1 + tensor2))`. @ppwwyyxx \nGiven this custom layer:\n@layer_register()\ndef CustomOp(x):\n   ........\n```\nand x is the activations between convolutional layers. But if I print the shape of x, the results are usually\nsomething like :(none, 64,56,56) , which means the first dimension--batch_size dimension is unknown. But in my code, I need to reshape x to dimension[-1,1] and do other related operations. But after reshaping, the first dimension is still \"none\" and this causes error in my following operations due to slicing. So is there any methods that can solve my issues? If total_batch_size is 256, and I am using 8 gpus, is this \"none\"--batch_size-dimension 256 or 256/8=32? \n\nI noticed that this problem is cause by https://github.com/Microsoft/LQ-Nets/blob/master/imagenet_utils.py#L194\ndef _get_inputs(self):\n        return [InputDesc(self.image_dtype, [None, self.image_shape, self.image_shape, 3], 'input'),\n                InputDesc(tf.int32, [None], 'label')]\nSo if my batch_size is 256 and I am using 8 gpus,  what number should I set in the above \"None\"? \n256 or 32?. So if my batch_size is 256 and I am using 8 gpus, what number should I set in the above \"None\"?\n256 or 32?\n@ppwwyyxx . @ppwwyyxx \nafter changing that to 32, this error happens:\n2018-09-18 16:50:16.397043: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: FIFOQueue '_1_DataParallelInferenceRunner/QueueInput/input_queue' is closed and has insufficient elements (requested 1, current size 0)\n     [[Node: DataParallelInferenceRunner/QueueInput/input_deque_7 = QueueDequeueV2[component_types=[DT_UINT8, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](DataParallelInferenceRunner/QueueInput/input_queue)]]. @ppwwyyxx \nif I want to create a variable with L2-regularizer, but I want to let the weight_decay to decreases over global_steps (not simply a constant number value):\nnew_v = tf.get_variable(\n                'new_v', [1,1], tf.float32,\n                initializer=new_v_init,\n                trainable=True)\n                regularizer=tf.contrib.layers.l2_regularizer(weight_decay))\nIs this supported in tensorpack?. @ppwwyyxx \ndoes the callbacks.DumpTensors() only support dump the tensors with names? \nwhat if an intermmediate variable calculated by other tensors with names?\nsuppose a=b*2, and I want to dump the 'a' variable into npz file, but a has no names. How to dump it into npz file?. @ppwwyyxx \ncan it be a Variable, not Tensor?. @ppwwyyxx \nhow to save dump the tensors into npz file every 100 iters?\ncurrently, directly writing callbacks= [DumpTensors(*)] will save the tensors every iteration, which is too much regarding to the disk storage. How to save it every 100 or 1000 iters?. @ppwwyyxx \nif using PeriodicCallback, will still the tensors of a total epoch be saved to disk? Can I save the tensors of only 10 iters ?. @ppwwyyxx \nwhat I need to do is get the current global_step by tf.train.get_or_create_global_step and manipulate the graph variables according to the current global_steps., for example, \nif global_step==100, \nvariable_1= variable_1 * 2, \nif global_step==200, \nvariable_1=variable_1 + variable_2\nif global_step==300, \nvariable_1=variable_1 * variable_3\n(every 100 steps, the operations are not the same) But how to get the exact value of global_step in the code, and use this value instantly?. @ppwwyyxx \nI am using tensorpack-0.8.6, and when I run global_step = get_global_step_value(), the code gives me an error:\nFile \"/root/anaconda2/lib/python2.7/site-packages/tensorpack-0.8.6-py2.7.egg/tensorpack/tfutils/common.py\", line 76, in get_global_step_value\n    get_global_step_var())\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/training_util.py\", line 58, in global_step\n    return int(sess.run(global_step_tensor))\nAttributeError: 'NoneType' object has no attribute 'run'\nI just use it in a newly defined layer function.\nIs there any method that can make me directly use this global_step value in a newly defined layer function? In the standard tensorpack-imagenet training code, I can not find where the tensorflow-session is. How to use this  get_global_step_value() function?. @ppwwyyxx \nso given an imagenet-resnet training task, here :https://github.com/tensorpack/tensorpack/blob/master/examples/ResNet/imagenet-resnet.py\n, in this python script, where can I get the global_step_value, and feed this value as a parameter to the definition of a layer definition , for example ,feed it as a parameter of this function:https://github.com/tensorpack/tensorpack/blob/master/examples/ResNet/resnet_model.py#L39\nor a newly defined layer function.. @ppwwyyxx \nIf so, if I am using get_global_step_var in a newly defined layer function, \n after the training has started, the value of this tensor will keep changing with the training process, right? And if I am using this tensor as the parameter of this function, it is valid, right?. @ppwwyyxx \nthanks for your advice.\nActually, I want to do this:\nafter every 2 epoches**, I add my newly defined operation on one layer. So in the 1 to 2 epoch, only the first layer is added this operation, in the 3 to 4 epoch, the first and the second layer is added this operation. etc. until all the layers are added this operation. So I need to include the global_step value into the layers's definition (for example resnet). So did you have any advice on how to use this global_step_var tensor?. @ppwwyyxx \nsorry. Thanks for your patience.. @ppwwyyxx \nI have pulled a new issue about this problem , related to tensorpack.  https://github.com/tensorpack/tensorpack/issues/974. this is my callbacks:\ncallbacks = [PeriodicTrigger(ModelSaver(), every_k_epochs=15)\n]\nSo I think the problem comes from the tf.summary callback , which is  a default tensorpack operation after each epoch. In the function Son_QuantizedActive there is tf.summary operation, indeed. So how can I avoid this issue? Any advice?\n@ppwwyyxx . @ppwwyyxx \nBesides, If I simply remove my own tf.summaryoperation, it seems that the tensorpack itself will trigger tf.summary after each epoch, is this true?. @ppwwyyxx \nyes. Thanks!. @ppwwyyxx \n1. \nin your  example code:\n+        def f(x):\n+            return tf.cond(get_global_step_var() < 100,\n+                           lambda: Conv2D('convtrue', x, filters=64),\n+                           lambda: Conv2D('convfalse', x, filters=64))\nif global_step<100, the computation graph contains the layer 'convture', else  it contains the layer 'convfalse'\n. So in the end,  will the computation graph be a little weird? Both the weight of convtrue and convfalse will be saved?\n\nBesides, if this is an issue related to summary, if I convert to a higher tensorflow version (currently 1.3.0), will this issue be also solved, without removing the tf.summary operation? If converting to a higher tensorflow version does not work, will exist any other valid solutions, except removing summary operation?\n\n. @ppwwyyxx \nI solved this issue by either removing tf.smmary or replace tf.summary with tf.contrib.summary. @ppwwyyxx \nI will give it a try after the current experiment ends.\nBesides, If I replace tf.summary withtf.contrib.summary, no error occurs, but after every epoch, the printed information actually does not contain the summary value I have included in the tf.contrib.summary. Why this happens?. @ppwwyyxx \nI tried to change the tf.condfunction to tf.case function to get a 3-pipeline graph like this:\ndef f1(x): return tf.identity(x+1)\nr = case({tf.less(global_step, y): f1(3), tf.greater(global_step, z): f1(4)},\n         default=f1(5), exclusive=True)\nbut the code is keeping giveing this error:\n```\n  File \"imagenet.py\", line 78, in get_logits\n    group_func, self.block_func, self.qw, self.pretrained_weights)\n  File \"/data/projects/resnet_model.py\", line 155, in resnet_backbone\n    .Conv2DQuant('conv0', 64, 7, stride=2, nl=BNReLUQuant, is_quant=True)\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorpack-0.8.6-py2.7.egg/tensorpack/models/linearwrap.py\", line 47, in layer_func\n    ret = layer(name, self._t, *args, kwargs)\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorpack-0.8.6-py2.7.egg/tensorpack/models/registry.py\", line 124, in wrapped_func\n    outputs = func(*args, actual_args)\n  File \"/data/learned_quantization.py\", line 549, in Conv2DQuant\n    quantized_weight = QuantizedWeight('weight_quant', kernel_in, n, nbit=nbit,pretrained_weights=pretrained_weights)\n  File \"/data/learned_quantization.py\", line 477, in QuantizedWeight\n    default=Son_QuantizedWeight(name,x, n,nbit=32,pretrained_weights=pretrained_weights),exclusive=False)\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3074, in case\n    raise TypeError(\"fn for pred %s must be callable.\" % pred.name)\nTypeError: fn for pred tower0/conv0/Greater:0 must be callable.\n```\n. @ppwwyyxx \nand if I use two tf.cond function, the error is like:\nFile \"/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 503, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 443, in __call__\n    self.build(input_shapes[0])\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/normalization.py\", line 189, in build\n    trainable=True)\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 383, in add_variable\n    trainable=trainable and self.trainable)\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1065, in get_variable\n    use_resource=use_resource, custom_getter=custom_getter)\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 962, in get_variable\n    use_resource=use_resource, custom_getter=custom_getter)\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 360, in get_variable\n    validate_shape=validate_shape, use_resource=use_resource)\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorpack-0.8.6-py2.7.egg/tensorpack/models/tflayer.py\", line 86, in custom_getter\n    return getter(name, *args, **kwargs)\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 352, in _true_getter\n    use_resource=use_resource)\n  File \"/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 682, in _get_single_variable\n    \"VarScope?\" % name)\nValueError: Variable conv0/bn/beta does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\nI have already usetf.get_variable_scope().reuse_variables() in the code to avoid error on conv0/W:0, but this error on conv0/bn/beta seems to be in the source code of tensorpack, because it uses the BNReLU function in tensorpack . @ppwwyyxx \nissue solved. Due to my tensorflow version too low.. this is a imagenet training task, and using tf.case to split the graph into 3 splits, like this:\ndef f1(x): return tf.identity(x+1)\nr = case({tf.less(global_step, y): f1(3), tf.greater(global_step, z): f1(4)},\n         default=f1(5), exclusive=True)\nbut the code just got stuck in :  \n[1115 07:27:24 @sessinit.py:220] Restoring from dict ...\n[1115 07:33:05 @base.py:240] Graph Finalized.\n[1115 07:33:09 @concurrency.py:37] Starting EnqueueThread QueueInput/input_queue ...\n[1115 07:33:09 @graph.py:73] Running Op sync_variables/sync_variables_from_main_tower ...\n[1115 07:33:37 @concurrency.py:37] Starting EnqueueThread DataParallelInferenceRunner/QueueInput/input_queue ...\n[1115 07:33:37 @inference_runner.py:101] [InferenceRunner] Will eval 782 iterations\n[1115 07:33:39 @base.py:272] Start Epoch 1 ...\n  0%|                                                                                                                                                                                                                      |0/5000[00:00<?,?it/s][1115 07:33:39 @input_source.py:551] Pre-filling StagingArea ...\n[1115 07:33:44 @input_source.py:555] 1 element was put into StagingArea on each tower.\n@ppwwyyxx \ntensorpack version is 0.9.0.1. @ppwwyyxx \nwhat if I want to apply two different operations , according to global_step, but not just two returned values of two functions? Like this: \nif global_step>100:\n   saver = tf.train.Saver(vars_1)\nelse:\n   saver = tf.train.Saver(vars_2)\ntf.condfunction seems not to work here.. ",
    "taneslle": "Yes, I just tried the example and it runs fine, and cfg.crop_size is an int, I'll check my other codes.\nThanks a lot.. @ppwwyyxx Here's the layer implementation\ndef InstanceNorm(net, train=True):\n    batch, rows, cols, channels = [i.value for i in net.get_shape()]\n    var_shape = [channels]\n    mu, sigma_sq = tf.nn.moments(net, [1,2], keep_dims=True)\n    shift = tf.Variable(tf.zeros(var_shape))\n    scale = tf.Variable(tf.ones(var_shape))\n    epsilon = 1e-3\n    normalized = (net-mu)/(sigma_sq + epsilon)**(.5)\n    return scale * normalized + shift\n@layer_register()\ndef conv_layer(net, filters, kernel_size, strides, relu=True):\n    net = Conv2D('conv', net, filters = filters, kernel_size = kernel_size, strides = strides)\n    net = InstanceNorm(net)\n    if relu:\n        net = tf.nn.relu(net)\n    return net\nclass Transform(ModelDesc):\n    def __init__(self, data_format='NHWC'):\n        pass\n    def get_inputs(self):\n        pass\n    def get_logits(self, image):\n        logits = (LinearWrap(image)\n                     .conv_layer('transfer_conv1', filters=32, kernel_size=9, strides=1)\n                     .conv_layer('transfer_conv2', 64)\n                     .conv_layer('transfer_conv3', 128)\nI was writing a style transfer model, get_logits used for transform input image to another style, and I also defined a vgg model in class Transform to extract features of styled_image and transformed_image.\nI assume the problem is from the variables defined in InstanceNorm, but I can't see why.. @ppwwyyxx So sorry I didn't notice that when readling the link, thanks a lot. ",
    "ivy94419": "@ppwwyyxx Have you solve this problems? I encounter a similar issues..... Waiting for your answers. Thank you very much!. ",
    "sammhho": "Thx for the quick response, things work as expected now.. Hi @ppwwyyxx, one question.\nWhen doing run_image,  the input image goes thru \nResizeShortestEdge and CenterCrop provided by fbresnet_augmentor, \nand then the returned img is passed to the OfflinePredictor, \nand since the model is an ImageNetModel,\neventually inside the graph, due to build_graph of ImageNetModel the img goes thru image_preprocess (and NCHW/NHWC handling) before being passed into get_logits, \nand what image_preprocess does is divide values by 255, minus per channel mean and then divides std,\nis that all !?\nSo what I'm doing is similar to replicating the inference result in say pure python+numpy environment,\nthe image after the ResizeShortestEdge and CenterCrop does match in both env, \nbut then if I just do \"div 255, minus mean, div std\" \n(I copied the exact mean and std used in ImageNetModel ), \nthe resulting image has different mean and std values than that given by TensorBoard.\nOn the pure env I used numpy's np.mean and np.std, on TensorPack/TensorFlow I added variable_summaries(image, '-image') to the model, where\nPython\ndef variable_summaries(var, name):\n  \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n  with tf.name_scope('summaries' + name):\n    mean = tf.reduce_mean(var)\n    tf.summary.scalar('mean', mean)\n    with tf.name_scope('stddev'):\n      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n    tf.summary.scalar('stddev', stddev)\n    tf.summary.scalar('max', tf.reduce_max(var))\n    tf.summary.scalar('min', tf.reduce_min(var))\n    tf.summary.histogram('histogram', var)\nAm I missing anything ? \nOn both envs the image is read using cv2.imread, so both are in BGR, and they both still match after resize + crop so no problem there.... @ppwwyyxx Thx for the reply.\nI used the above variable_summaries function only for TensorBoard visualization purpose, sorry for the confusion.\nSo within alexnet-dorefa.py's get_logits I added these (since TensorPack callbacks/monitors are for training but not inference...) to visualize the input image tensor's values:\nPython\nvariable_summaries(image, '-image')\nself.merged = tf.summary.merge_all(tf.GraphKeys.SUMMARIES)\nSome findings:\nIf I comment out the \"minus mean divide std\" part within image_preprocess like this:\n```Python\n    @staticmethod\n    def image_preprocess(image, bgr=True):\n        with tf.name_scope('image_preprocess'):\n            if image.dtype.base_dtype != tf.float32:\n                image = tf.cast(image, tf.float32)\n            image = image * (1.0 / 255)\n        mean = [0.485, 0.456, 0.406]    # rgb\n        std = [0.229, 0.224, 0.225]\n        if bgr:\n            mean = mean[::-1]\n            std = std[::-1]\n        image_mean = tf.constant(mean, dtype=tf.float32)\n        image_std = tf.constant(std, dtype=tf.float32)\n        #image = (image - image_mean) / image_std\n        #image = (image - image_mean)\n        return image\n\nWith the same picture as input, I get the mean and std values of the picture after resize+centerCrop from both TensorPack and pure Python env almost matched to 2 or 3 decimal place;\nHowever, in TensorPack the min and max value of the `image` tensor was shown as 0. and 1. on TensorBoard;\nBut in `run_image` I also printed the image's stats after `augement`, before launching `OfflinePredictor`,Python\ndef run_image(model, sess_init, inputs):\n    pred_config = PredictConfigMod(\n        model=model,\n        session_init=sess_init,\n        input_names=['input'],\n        output_names=['output']\n    )\n    predictor = OfflinePredictorMod(pred_config)\n    meta = dataset.ILSVRCMeta()\n    words = meta.get_synset_words_1000()\n    print('type(predictor)={}'.format(type(predictor)))\nwriter = tf.summary.FileWriter(logger.get_logger_dir())\n\ntransformers = imgaug.AugmentorList(fbresnet_augmentor(isTrain=False))\nfor f in inputs:\n    assert os.path.isfile(f), f\n    img = cv2.imread(f).astype('float32')\n    assert img is not None\n\n    print('img.shape ={}'.format(img.shape))\n    print('img mean={}, var={}, stddev={}'.format(np.mean(img), np.var(img), np.std(img)))\n    img = transformers.augment(img)[np.newaxis, :, :, :]\n    print('img.shape ={}'.format(img.shape))\n    print('img mean={}, var={}, stddev={}'.format(np.mean(img), np.var(img), np.std(img)))\n    print('img min={} max={}'.format(np.min(img), np.max(img)))\n    predicted = predictor(img)\n    outputs = predicted[0]\n    prob = outputs[0]\n    ret = prob.argsort()[-10:][::-1]\n\n    names = [words[i] for i in ret]\n    print(f + \":\")\n    print(list(zip(names, prob[ret])))\n    print(ret)\n\n    writer.add_summary(predicted[1])\n\n``\nand got the image min and max as:img min=-10.28548526763916 max=275.43719482421875,\nwhich is the same value as I got in pure Python env too, \nbut then things doesn't add up coz if all that's done withinimage_preprocessnow isimage = image * (1.0 / 255), \nwe really shouldn't have got min=0.0, max=1.0 on theimage` tensor, no !?\nSorry for the long story and thx for any opinions in advance.. @ppwwyyxx ah right that's what's missing! \nDid a .astype(np.uint8) and now both envs matches, thx a lot!. ",
    "h44rd": "Thank you for the quick reply! What should I use instead? I could make the change in the repo too. \nEdit: Nevermind, thanks!. ",
    "XinDongol": "Maybe you should try to check whether you have installed cv2. \nIf not, image augmentation is not available.. Thanks for the quick reply. \nclose this question.. Thanks a lot.. Thanks a lot.\nIf I use logic like class A, do I still have to claim a placeholder in Model.inputs like this ?\npython\nclass Model(ModelDesc):\n    def inputs(self):\n        return [tf.placeholder(tf.float32, [None, 32, 32, 3], 'input'),\n                tf.placeholder(tf.int32, [None], 'label'),\n                tf.placeholder(tf.float32, (), 'extra_para')]\nI plan to implement it by \n```python\nclass Get_Extra_Para(object):\n    def init(self):\n          ....\n    def step(self):\n          some rule....\n          return some value\nextra_source = Get_Extra_Para()\nclass A(Callback):\n    def before_run(self, ):\n        return tf.train.SessionRunArgs(fetches=[], \n                                     feed_dict={'extra_para:0': extra_source.step()})\nIs above codes enough if I want to change `extra_para` every iteration ?. I followed your suggestion.python\nclass Model(ModelDesc):\n    def inputs(self):\n        return [tf.placeholder(tf.float32, [None, 32, 32, 3], 'input'),\n                tf.placeholder(tf.int32, [None], 'label')]\ndef build_graph(self, image, label):\n    relax = tf.get_variable('relax', initializer=1.0, trainable=False)\n    logits = (LinearWrap(image)\n              .layerone(image, relax)\n              .layertwo( relax))\n\ndef get_config():\n   class Schdule_Relax():\n       def ident(self, x):\n           return x\nrelax_schduler = Schdule_Relax()\nclass RelaxSetter(Callback):\n    def _setup_graph(self):\n        self._relax = [k for k in tf.global_variables() if k.name == 'relax:0'][0]\n    def _trigger_step(self):\n        self._relax.load(relax_schduler.ident(1.))\n\nmodel = Model()\nreturn TrainConfig(\n    dataflow=dataset_train,\n    callbacks=[\n        ModelSaver(),\n        InferenceRunner(dataset_test,\n                        [ScalarStats('cost'), ClassificationError('wrong_tensor')]),\n        ScheduledHyperParamSetter('learning_rate',\n                                  [(1, 0.01), (82, 0.001), (123, 0.0002), (200, 0.0001)]),\n        RelaxSetter(),\n    ],\n    model=model,\n    max_epoch=args.epoches,\n), model\n\nif name == 'main':\n    config, model = get_config()\n    launch_train_with_config(config, SimpleTrainer())\n```\nThen I got this error,\n```\n[1005 22:45:54 @logger.py:109] WRN Log directory ../../../runs/trash/ exists! Use 'd' to delete it. \n[1005 22:45:54 @logger.py:112] WRN If you're resuming from a previous run, you can choose to keep it.\nPress any other key to exit. \nSelect Action: k (keep) / d (delete) / q (quit):d\n[1005 22:45:55 @logger.py:74] Argv: cifar_simon.py\n[1005 22:45:55 @cifar.py:32] Found cifar10 data in ../../../cifar10_data/.\n[1005 22:45:56 @parallel.py:185] [MultiProcessPrefetchData] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.\n[1005 22:45:56 @argtools.py:152] WRN Install python-prctl so that processes can be cleaned with guarantee.\n[1005 22:45:56 @cifar.py:32] Found cifar10 data in ../../../cifar10_data/.\n[1005 22:45:56 @argtools.py:152] WRN Install python-prctl so that processes can be cleaned with guarantee.\ncheck...................\n\nRuntimeError                              Traceback (most recent call last)\n/codes/tensorpack/examples/DoReFa-Net/cifar_simon.py in ()\n    274     config, model = get_config()\n    275     print('check...................')\n--> 276     launch_train_with_config(config, SimpleTrainer())\n    277 \n/usr/local/lib/python3.6/dist-packages/tensorpack/train/interface.py in launch_train_with_config(config, trainer)\n     83     trainer.setup_graph(\n     84         inputs_desc, input,\n---> 85         model._build_graph_get_cost, model.get_optimizer)\n     86     _check_unused_regularization()\n     87     trainer.train_with_defaults(\n/usr/local/lib/python3.6/dist-packages/tensorpack/utils/argtools.py in wrapper(*args, kwargs)\n    179         _FUNC_CALLED.add(key)\n    180 \n--> 181         return func(*args, kwargs)\n    182 \n    183     return wrapper\n/usr/local/lib/python3.6/dist-packages/tensorpack/train/tower.py in setup_graph(self, inputs_desc, input, get_cost_fn, get_opt_fn)\n    200 \n    201         # TODO setup may want to register monitor as well??\n--> 202         input_callbacks = self._setup_input(inputs_desc, input)\n    203         train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)\n    204         self.register_callback(input_callbacks + train_callbacks)\n/usr/local/lib/python3.6/dist-packages/tensorpack/train/tower.py in _setup_input(self, inputs_desc, input)\n    216     def _setup_input(self, inputs_desc, input):\n    217         assert not input.setup_done()\n--> 218         return input.setup(inputs_desc)\n    219 \n    220     def _make_get_grad_fn(self, input, get_cost_fn, get_opt_fn):\n/usr/local/lib/python3.6/dist-packages/tensorpack/utils/argtools.py in wrapper(*args, kwargs)\n    179         _FUNC_CALLED.add(key)\n    180 \n--> 181         return func(*args, kwargs)\n    182 \n    183     return wrapper\n/usr/local/lib/python3.6/dist-packages/tensorpack/input_source/input_source_base.py in setup(self, inputs_desc)\n     95             callbacks of InputSource cannot use any trigger*() method.\n     96         \"\"\"\n---> 97         self._setup(inputs_desc)\n     98         self._setup_done = True\n     99         return self.get_callbacks()\n/usr/local/lib/python3.6/dist-packages/tensorpack/input_source/input_source.py in _setup(self, inputs)\n    110     def _setup(self, inputs):\n    111         # placeholders as input are always safe to reuse.\n--> 112         self._all_placehdrs = [v.build_placeholder_reuse() for v in inputs]\n    113         self._cb = self._FeedCallback(self._iter_ds, self._all_placehdrs)\n    114 \n/usr/local/lib/python3.6/dist-packages/tensorpack/input_source/input_source.py in (.0)\n    110     def _setup(self, inputs):\n    111         # placeholders as input are always safe to reuse.\n--> 112         self._all_placehdrs = [v.build_placeholder_reuse() for v in inputs]\n    113         self._cb = self._FeedCallback(self._iter_ds, self._all_placehdrs)\n    114 \n/usr/local/lib/python3.6/dist-packages/tensorpack/graph_builder/model_desc.py in build_placeholder_reuse(self)\n     64             return self._cached_placeholder[g]\n     65         else:\n---> 66             return self.build_placeholder()\n     67 \n     68     def _register_cached_placeholder(self, placeholder):\n/usr/local/lib/python3.6/dist-packages/tensorpack/graph_builder/model_desc.py in build_placeholder(self)\n     48         with tf.name_scope(None):   # clear any name scope it might get called in\n     49             ret = tf.placeholder(\n---> 50                 self.type, shape=self.shape, name=self.name)\n     51         self._register_cached_placeholder(ret)\n     52         return ret\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py in placeholder(dtype, shape, name)\n   1733                        \"eager execution.\")\n   1734 \n-> 1735   return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n   1736 \n   1737 \n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py in placeholder(dtype, shape, name)\n   4923     shape = execute.make_shape(shape, \"shape\")\n   4924     , _, _op = _op_def_lib._apply_op_helper(\n-> 4925         \"Placeholder\", dtype=dtype, shape=shape, name=name)\n   4926     _result = _op.outputs[:]\n   4927     _inputs_flat = _op.inputs\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\n    785         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n    786                          input_types=input_types, attrs=attr_protos,\n--> 787                          op_def=op_def)\n    788       return output_structure, op_def.is_stateful, op\n    789 \n/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py in new_func(*args, kwargs)\n    452                 'in a future version' if date is None else ('after %s' % date),\n    453                 instructions)\n--> 454       return func(*args, kwargs)\n    455     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n    456                                        _add_deprecated_arg_notice_to_docstring(\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in create_op(failed resolving arguments)\n   3125     del compute_shapes\n   3126 \n-> 3127     self._check_not_finalized()\n   3128     for idx, a in enumerate(inputs):\n   3129       if not isinstance(a, Tensor):\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _check_not_finalized(self)\n   2798     \"\"\"\n   2799     if self._finalized:\n-> 2800       raise RuntimeError(\"Graph is finalized and cannot be modified.\")\n   2801 \n   2802   def _add_op(self, op):\nRuntimeError: Graph is finalized and cannot be modified.\n```. Many thanks! I fixed this bug.\nAnother question.\nFor my schduler, \npython\nclass Schdule_Relax():\n       def ident(self, x):\n           return x\nNow, the schduler is just a test version. I want to make it behave differently for training and inference. \nFor example, the hyperparameter is called relax.  I want to increase relax every batch by 1. After each epoch, we will do inference. During inference, I want to keep its value fixed.  Then, when we move into next epoch, I want to increase its value every batch by 1 again.\nI try to use get_current_tower_context().is_training, but I got AttributeError: 'NoneType' object has no attribute 'is_training'. It seem that this function cannot find the tower context. \nCould you please give some suggestions?. Thanks.. The thing is that I only want to monitor one tensor every batch. \nMergeAllSummaries.period=1 will make all the other monitors called every batch too? \nI want to keep monitors like train_cost, val_cot called every epoch.. ",
    "Michael3444": "@ppwwyyxx  Sorry  ,maybe I did't explain it clearly,I trained my models with keras and wrote my data generator with tensorpack ,I wrote my own Dataflow inherited from the RNGDataFlow, overrode the get_data() method:\ndef get_data(self):\n    idxs = np.arange(self.size())\n    self.rng.shuffle(idxs)\n    for idx in idxs:\n        yield [self.all_meta[idx]]\nbut at the end of the first eopch,it stopped iteration,the error logs: \n\nTraceback (most recent call last):\n  File \"train_pose.py\", line 225, in \n    initial_epoch=last_epoch)\n  File \"/home/lipengkun/anaconda2/envs/tf-pose-paf/lib/python3.5/site-packages/keras/legacy/interfaces.py\", line 87, in wrapper\n    return func(args, *kwargs)\n  File \"/home/lipengkun/anaconda2/envs/tf-pose-paf/lib/python3.5/site-packages/keras/engine/training.py\", line 2011, in fit_generator\n    generator_output = next(output_generator)\nStopIteration\n\nso ,I think the iterator is empty ,how do i fix my code so that at the end of the first epoch,the generator would  re-generate an iterator so that it could continued training\n. OK ,Thanks a lot ,I will try that !. ",
    "Rov67777": "after investigation, the code seems to crash when calling \"run_step\" of base.py of the trainer\nwhen executing the line  self.hooked_sess.run(self.train_op)\n```\ndef run_step(self):\n    \"\"\"\n    Defines what to do in one iteration. The default is:\n    ``self.hooked_sess.run(self.train_op)``.\n\n    The behavior of each iteration can be changed by either setting ``trainer.train_op``,\n    or overriding this method.\n    \"\"\"\n    if not hasattr(self, 'train_op'):\n        raise NotImplementedError(\n            \"Please either set `Trainer.train_op` or provide an implementation \"\n            \"of Trainer.run_step()!\")\n    self.hooked_sess.run(self.train_op)\n\n```. Hey,\nThanks a lot for your answer. I pasted below the code with the changes.\nTo answer : \"what makes you think it crashed\"? Well code stops running at this point + output error message in windows : \"Python has stopped working, A problem caused the program to stop working correctly\"\nI ran the MNIST examples in the basics and it worked correctly\n```\nimport numpy as np\nimport tensorflow as tf\nimport argparse\nimport os\nfrom tensorpack import \nfrom tensorpack.tfutils.symbolic_functions import \nfrom tensorpack.tfutils.summary import *\nBATCH_SIZE = 64\nclass Model(ModelDesc):\n    def init(self, depth):\n        super(Model, self).init()\n        self.N = int((depth - 4)  / 3)\n        self.growthRate =12\ndef _get_inputs(self):\n    return [InputDesc(tf.float32, [None, 32, 32, 3], 'input'),\n            InputDesc(tf.int32, [None], 'label')\n           ]\n\ndef _build_graph(self, input_vars):\n    image, label = input_vars\n    image = image / 128.0 - 1\n\n    def conv(name, l, channel, stride):\n        return Conv2D(name, l, channel, 3, stride=stride,\n                      nl=tf.identity, use_bias=False,\n                      W_init=tf.random_normal_initializer(stddev=np.sqrt(2.0/9/channel)))\n    def add_layer(name, l):\n        shape = l.get_shape().as_list()\n        in_channel = shape[3]\n        with tf.variable_scope(name) as scope:\n            c = BatchNorm('bn1', l)\n            c = tf.nn.relu(c)\n            c = conv('conv1', c, self.growthRate, 1)\n            l = tf.concat([c, l], 3)\n        return l\n\n    def add_transition(name, l):\n        shape = l.get_shape().as_list()\n        in_channel = shape[3]\n        with tf.variable_scope(name) as scope:\n            l = BatchNorm('bn1', l)\n            l = tf.nn.relu(l)\n            l = Conv2D('conv1', l, in_channel, 1, stride=1, use_bias=False, nl=tf.nn.relu)\n            l = AvgPooling('pool', l, 2)\n        return l\n\n\n    def dense_net(name):\n        l = conv('conv0', image, 16, 1)\n        with tf.variable_scope('block1') as scope:\n\n            for i in range(self.N):\n                l = add_layer('dense_layer.{}'.format(i), l)\n            l = add_transition('transition1', l)\n\n        with tf.variable_scope('block2') as scope:\n\n            for i in range(self.N):\n                l = add_layer('dense_layer.{}'.format(i), l)\n            l = add_transition('transition2', l)\n\n        with tf.variable_scope('block3') as scope:\n\n            for i in range(self.N):\n                l = add_layer('dense_layer.{}'.format(i), l)\n        l = BatchNorm('bnlast', l)\n        l = tf.nn.relu(l)\n        l = GlobalAvgPooling('gap', l)\n        logits = FullyConnected('linear', l, out_dim=10, nl=tf.identity)\n\n        return logits\n\n    logits = dense_net(\"dense_net\")\n\n    prob = tf.nn.softmax(logits, name='output')\n\n    cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n    cost = tf.reduce_mean(cost, name='cross_entropy_loss')\n\n    wrong = prediction_incorrect(logits, label)\n    # monitor training error\n    add_moving_summary(tf.reduce_mean(wrong, name='train_error'))\n\n    # weight decay on all W\n    wd_cost = tf.multiply(1e-4, regularize_cost('.*/W', tf.nn.l2_loss), name='wd_cost')\n    add_moving_summary(cost, wd_cost)\n\n    add_param_summary(('.*/W', ['histogram']))   # monitor W\n    self.cost = tf.add_n([cost, wd_cost], name='cost')\n\ndef _get_optimizer(self):\n    lr = tf.get_variable('learning_rate', initializer=0.1, trainable=False)\n    tf.summary.scalar('learning_rate', lr)\n    return tf.train.MomentumOptimizer(lr, 0.9, use_nesterov=True)\n\ndef get_data(train_or_test):\n    isTrain = train_or_test == 'train'\n    ds = dataset.Cifar10(train_or_test)\n    pp_mean = ds.get_per_pixel_mean()\n    if isTrain:\n        augmentors = [\n            imgaug.CenterPaste((40, 40)),\n            imgaug.RandomCrop((32, 32)),\n            imgaug.Flip(horiz=True),\n            #imgaug.Brightness(20),\n            #imgaug.Contrast((0.6,1.4)),\n            #imgaug.MapImage(lambda x: x - pp_mean),\n        ]\n    else:\n        augmentors = [\n            #imgaug.MapImage(lambda x: x - pp_mean)\n        ]\n    ds = AugmentImageComponent(ds, augmentors)\n    ds = BatchData(ds, BATCH_SIZE, remainder=not isTrain)\n    #if isTrain:\n        #ds = PrefetchData(ds, 3, 2)\n    return ds\ndef get_config():\n    log_dir = 'train_log/cifar10-single-fisrt%s-second%s-max%s' % (str(args.drop_1), str(args.drop_2), str(args.max_epoch))\n    logger.set_logger_dir(log_dir, action='n')\n# prepare dataset\ndataset_train = get_data('train')\nsteps_per_epoch = dataset_train.size()\ndataset_test = get_data('test')\n\nreturn TrainConfig(\n    dataflow=dataset_train,\n    callbacks=[\n        ModelSaver(),\n        InferenceRunner(dataset_test,\n            [ScalarStats('cost'), ClassificationError()]),\n        ScheduledHyperParamSetter('learning_rate',\n                                  [(1, 0.1), (args.drop_1, 0.01), (args.drop_2, 0.001)])\n    ],\n    model=Model(depth=args.depth),\n    steps_per_epoch=steps_per_epoch,\n    max_epoch=args.max_epoch,\n)\n\nif name == 'main':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--gpu', help='comma separated list of GPU(s) to use.') # nargs='' in multi mode\n    parser.add_argument('--load', help='load model')\n    parser.add_argument('--drop_1',default=150, help='Epoch to drop learning rate to 0.01.') # nargs='' in multi mode\n    parser.add_argument('--drop_2',default=225,help='Epoch to drop learning rate to 0.001')\n    parser.add_argument('--depth',default=40, help='The depth of densenet')\n    parser.add_argument('--max_epoch',default=300,help='max epoch')\n    args = parser.parse_args()\nif args.gpu:\n    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n\nconfig = get_config()\nif args.load:\n    config.session_init = SaverRestore(args.load)\n\nnr_tower = 0\nif args.gpu:\n    nr_tower = len(args.gpu.split(','))\n\n# SyncMultiGPUTrainer(config).train()\nlaunch_train_with_config(config, SimpleTrainer())\n\n```. Hey, thanks for your answers,\n\nAbout it being an OOM : I tried reducing the depth / number of parameters to very low amounts and it still not run so I don't think this is the problem\nAbout testing other official code: actually the cifar-convnet (from basic) does not run either with the same crash (crash at the first iteration). So it might come from a method added by cifar that is not used by mnist code.\n\nUPDATE: after investigation, the code does not crash anymore when removing the augmentation part and commenting\n#ds = AugmentImageComponent(ds, augmentors)\nso it seems to be a problem with the augmentation component. It is indeed a problem with opencv. More precisely it crashes when calling the following function:\nret = cv2.flip(img, self.code) \nin line 46 of the image augmentation package: https://github.com/tensorpack/tensorpack/blob/master/tensorpack/dataflow/imgaug/misc.py\nI have changed the code to the numpy version of flip which works correctly. \nret = np.flip(img, self.code)\nAltough, I guess I might encounter some reduction of speed and performance using numpy instead of opencv to compute the augmentation ?\nWell, if not perfect, at least it works. If somebody has any idea how i could try to debug the problem with the opencv call to cv2.flip it would still be great. Otherwise the issue can be closed.\nThanks all for the help. ",
    "muradalqurishee10": "Yes. Keras Tensorflow backend. Yes, its related to tensorpack. I am using tensorflow.\nRegards,\nMURAD AL QURISHEE\nTeaching Assistant  University of Tennessee at Chattanooga\nDept. of Civil and Chemical Engineering\nm: (423)-316-8547 a: 862 Oak St, Chattanooga, TN 37403\nw: http://muradalqurishee.wixsite.com/muradalqurishee\ne: mgp216@ muradalqurishee@gmail.commocs.utc.edu s: Murad Al Qurishee\nFollow me\nhttps://newoldstamp.com/editor/?utm_source=Free_Editor&utm_medium=social_icon&utm_campaign=nos\n. https://web.facebook.com/muradal.qurishee.7.\nhttps://www.linkedin.com/in/muradal-qurishee.\nGet your own signature\nhttps://newoldstamp.com/editor/?utm_source=Free_Editor&utm_medium=email_banner&utm_campaign=new\nOn 10 July 2018 at 20:55, Yuxin Wu notifications@github.com wrote:\n\nHow is \"keras tensorflow backend\" related to tensorpack?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/tensorpack/tensorpack/issues/818#issuecomment-404037088,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AkIKmyZKW35kRtxqgmXMfNjSpd1MU0G3ks5uFXcugaJpZM4VKbyj\n.\n. Yes. I am using tensorpack.\nSorry for my lack of knowledge.\n\nRegards,\nMURAD AL QURISHEE\nTeaching Assistant  University of Tennessee at Chattanooga\nDept. of Civil and Chemical Engineering\nm: (423)-316-8547 a: 862 Oak St, Chattanooga, TN 37403\nw: http://muradalqurishee.wixsite.com/muradalqurishee\ne: mgp216@ muradalqurishee@gmail.commocs.utc.edu s: Murad Al Qurishee\nFollow me\nhttps://newoldstamp.com/editor/?utm_source=Free_Editor&utm_medium=social_icon&utm_campaign=nos\n. https://web.facebook.com/muradal.qurishee.7.\nhttps://www.linkedin.com/in/muradal-qurishee.\nGet your own signature\nhttps://newoldstamp.com/editor/?utm_source=Free_Editor&utm_medium=email_banner&utm_campaign=new\nOn 10 July 2018 at 20:59, Yuxin Wu notifications@github.com wrote:\n\n\"Using tensorflow\" does not imply you're using tensorpack. Tensorflow is a\ndependency of tensorpack.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/tensorpack/tensorpack/issues/818#issuecomment-404037616,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AkIKm7pWKQ9FK-ibEH1G6r9BWfU2tXT6ks5uFXgagaJpZM4VKbyj\n.\n. Function: Counter.\n\nFrom my below code, How can I add a counter for count multiple class of multiple categories?\nresults = tfnet.return_predict(frame)\nfor color, result in zip(colors, results):\ntl = (result['topleft']['x'], result['topleft']['y'])\nbr = (result['bottomright']['x'], result['bottomright']['y'])\nlabel = result['label']. ",
    "engineer1109": "@ppwwyyxx , Thanks it works.\nDo you replace all BatchNormalization with GroupNormalization?. @ppwwyyxx How could I train a Res101-GroupNorm32-Alignpadding model\uff1f. @ppwwyyxx  Alignpadding means what?. @ppwwyyxx Have you tested the mAP on groupnorm?. @PacteraOliver The model provided is very awful\uff0c purchase Titan or Tesla to train yourself. ",
    "zxqcreations": "@ppwwyyxx  Thanks for your review. Here my model is generated by gennet, its a tool to generate a CNN.\nyes, differences for some weights,  'your model' is vgg16.ckpt from google drive while mine is generated by gennet.. @ppwwyyxx oh my god... It's all my fault...... I just clicked a wrong link.....the vgg16.ckpt is from google drive uploaded by totally an another author(endernewton). I'm so stupid and close this issue......\nI'm so sorry and really thank you.. ",
    "shineEtern": "I have added the tf.cond() in the ResNetFPNModel\nfunction as follows, but  still  has this problem, how can I fix it? Thank you.\n\n. Thanks for your quick response!. ",
    "lyyiangang": "@PatWie Sorry, I forget to fill the issue template. I will follow your guide if I meet issue again. \nAfter root my machine, mnist-tfslim.py runs successfully. it's my system's issue indeed. \nThanks very much. . @ppwwyyxx \nThanks your answer. Take following cases for example.\ncase 1. use  images=tf.transpose(images, [0, 3, 1, 2]) \ncase 2. don't use the tf.transpose code snippet.\nfor both cases, tf.transpose only changes the order of axis, images will occupy same size memory. If you are right, case 1 should fail too. In fact the train code works right and gives me right result for case 1. \nAnother question, In cifar-convnet,  we have following code snippet\nif tf.test.is_gpu_available():\n            image = tf.transpose(image, [0, 3, 1, 2])\n            data_format = 'channels_first'\n        else:\n            data_format = 'channels_last'\nwhy we have to make channels first when using GPU in training? some reasons? \nThanks again.. It's my fault rather than tensorpack's. Close this issue. . ",
    "deep-diver": "Thank you for your quick reply :)\nI am relatively new to deep learning, so can I ask what \"C\" stands for in R50-C4 ? I could guess R stands for ResNet only. Thanks for your kindness.\n. ",
    "PacteraOliver": "@ppwwyyxx \nHi, thank you for your update.\nI could successfully run the tf-faster-rcnn before or other models which need cuda/cudnn. \nI will try with docker to build the environment. Hope it could fix the environment problem.. @ppwwyyxx \nI fixed this problem by run as root.. @ppwwyyxx Have you ever considered to write a tool to transfer other pre-trained models such as here to npz model that tensorpack could use?. I read the #578 , but its problem it version is before 1.4.0.. Solved by upgrading.. ",
    "hanfeng0409": "@PacteraOliver hello, i have the same problem now, and solve it thanks to your advice. But i can't understand, do u know why it need root? Thanks~. ",
    "vicdxxx": "This problem is just caused by  Windows10 dos window cannot realize 'xxx' string, need convert to \"xxx\".\nSolved.. @ppwwyyxx \nSure, that's a good idea. Thanks!. ",
    "asjmasjm": "Thank you for your reply. However, two things are still not very clear for me:\n\n\nSuppose I have realized all the necessary quantized operations in the forwarded propagation, does it mean the downloaded pre-trained 1-2-6 or 8-8-8 model is testable after I manually quantized every weight elements using fw() as you have suggested in #573?\n\n\nBy claiming \"In this implementation, quantized operations are all performed through tf.float32\", my understanding is that all the data format is still float32 in the python scripts you provide. For example: \n\"\n......\n.Conv2D('conv0', 96, 12, strides=4, padding='VALID', use_bias=True)\n.apply(activate)\n.Conv2D('conv1', 256, 5, padding='SAME', split=2)\n......\n\"\nAlthough the quantized activate function are applied, the conv1 still perform float32-based convolution calculations in your open implementation. And the npz model is also trained under this environment which explains the float32 weights it contains. \nI do not know if it is correct.. Yes I kind of misunderstood the quantization mechanism until I walked through the discussions in #27. I now know the weights should still be float32 even after quantization whatsoever while most of the multiplications and additions in Cov and FC layers are bitwisely operated according to the equation (3) of your paper. \n\n\nAnother thought: I think it is no need of weights quantization if I use the provided dorefa-alexnet.py with the pre-trained npz model to test as the quantization layers are already provided between Conv layers in dorefa-alexnet.py. And the accuracy should be almost the same but the speed is slower because the Conv2d in it are float32-based. I do not know if my thought is correct as I am to test the float32 version net anyway.. I am not sure whether I understand what you mean. Suppose we use the w2a2 to perform the convolution 1/3 * 1/3 * [1, 2, 1, 3] * [2, 1, 0, 3]' and get the activated feature value 1/9 * [2, 7, 2, 10] and then clip to 1/9 * [2, 7, 2, 9] if using the clip(0, 1) after the convolution layer. Then it is convolved with the next weights 1/3 * [2, 2, 2, 2]' and we get 1/27 * [18, 18, 22, 22]. \nNo floating values are involved in the example above but it sounds like \"float->quantize + activation + quantize ...... ->quantize->float (final output)\" rather than \"quantize->float + activataion + float->quantize\". And through LUT, do you directly match the [1, 2, 1, 3] [2, 1, 0, 3] combination to the result [2, 7, 2, 9]?. Sorry for omitting that I fuse the batch normal parameters into the weights. In the training phase, the float weights are like [0.3333 * bn_gamma, 0.6667 * bn_gamma, 0.3333 * bn_gamma, 0.3333 * bn_gamma]. The errors are generated after the fused weights fixed-point to int8 and multiplied with fixed-point feature values. Without the bn layer, I think everything fits the algorithm in the paper and goes without error. \nIf my guess above and in my last post are correct about the fixed-point convolution, here comes two problems that puzzle me about the usage:    \n1) Is there any way to hide the floating point bn calculation? LUT maybe, but the size of the table could be very large if the bitwidth is long (>6)?\n2) Even for the fixed point operation, how to deal with the potential accumulation overflow? It seems impossible to make the intermediate additon result in int16 if there are more channels with large kernel size.. For some reasons no int32 variables is allowed even as the temporary ones to pick the intermediate accumulated result... And I can not train the network using weights less than w4a4 bits. Actually sharp decreases usually appears once 3 or less bits are applied. Maybe it is limited to the structure of the network. I think I will try LUT in w4a4 anyway.   . >The table size is 2^activation bitwidth;\nI am not clear about this conclusion.\nSuppose the weigth and activation bitwidths are both 2 and suppose we convolve weights 1/3 * [-3 to 3, ... -3 to 3] with activations 1/3 * [0 to 3, ... ,0 to 3]' and get 1/9 * [-9 to 9 + ... + -9 to 9] = 1/K * a0, where a0 is an integer in a rather large range. \nAfter the following bn and clip process we get a1 = clip(gamma * 1/K * a0 + beta, 0, 1). Therefore the interger a0 should be retained within these areas:\n[int(-betaK/gamma), int( (1-beta)K/gamma )] if gamma >0\n[int( (1-beta) * K/gamma), int( -beta * K/gamma )] if gamma <0 \nWithin these areas we perform Fa(a1), so the length of the LUT is calculated as (1-beta)K/gamma - (-betaK/gamma) = K/gamma = (2^(activation bitwidth) - 1) * (2^(weight bitwidth) - 1) / gamma\nWhat is the problem about my understanding of the LUT?\n. I kinda catch up with you now. So you mean that multiple indices in the LUT map to one value and the indexing should be like LUT[i>>weight_bitwidth] instead of LUT[i]. . Another question brought by this...\nI notice the tensorpack put the bn layer before activation (clip) layer while in tensorlayer after activation layer:\nhttps://tensorlayercn.readthedocs.io/zh/latest/_modules/tensorlayer/layers/convolution/dorefa_conv.html#DorefaConv2d \nThe order does matter as the negative convolved result can be converted to 0.0 to 1.0 if it go through bn layer before clip and thus kept after clip. It will not be kept if go through clip layer first.\nI do not know what is more reasonable?. Okay I see... and appreciate your patience greatly.. ",
    "silentobservers": "Oh, I see\nThanks for your patience.. ",
    "pkubik": "Thanks @ppwwyyxx . It's quite odd that they didn't simply consider integrating the tensorpack with tensorflow as they did with keras (although the keras integration is kind of broken IMHO). They didn't even mention the tensorpack in their whitepaper.\nThe biggest problem with estimators seems to be their lifecycle. It seems like they add few features, make it big on Google I/O and then forget about it for the rest of the year. E.g. I was recently thinking about implementing a run hook for printing the progress bar but it seems that there already is such thing in tensorpack.\nBtw. I just found out that keras also use a similar concept of callbacks https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/keras/callbacks/Callback.\nI guess that I'll just give the tensorpack a try.. @yg320 It seems like the support for the estimators is going to be limited in TensorFlow 2.0. The custom estimators are discouraged in favor of the Keras models. Actually, they never appeared to have huge adoption in the community. It was always difficult to find any hints on how to use them. Note that since, TF is very popular there are a lot of low-effort tutorials which mostly paraphrase the original docs, but do not touch any details that might interest you in practice.\nSource:\nhttps://medium.com/tensorflow/whats-coming-in-tensorflow-2-0-d3663832e9b8. ",
    "yuyijie1995": "\n. But I still can not train right now ...The same error still in there .What shoud I do @ppwwyyxx . Thankyou !. Did you solve this error ? @ywpkwon @horizonheart . Thanks for your reply, this error is fixed automatically,but another error happened .\n ERR [MultiProcessMapDataZMQ] buffer_size cannot be larger than the size of the DataFlow!\n\nCan you give me some suggestions ? . ",
    "minhson": "Thanks you very much! \nBut as i know, the trained model that is saved in checkpoint doesn\u2019t have quantized weight.\nSo if i want to run Inference on quantized weight model, i have to change it manually?\nIf i have to do that, how can i do that?\nThanks you!. How can i change meta = dataset.ILSVRCMeta() words = meta.get_synset_words_1000() to Cifar10 dataset. Because there is no that kind of function in dataflow.dataset.Cifar10.\nThanks you!\n. Thanks you!\nI have just figured it out!. Hi!\nCan i have 2 remap_variables() in code. Because i have 2 quantization function (one is normal TTQ, another is to change the weight again).\nThanks you!\n. Hi!\nHow can i print value of weight variable during Inference. \nThanks you!. Hi!\nI am sorry if i asked some questions which isn't related to Tensorpack.\nCould you show me how to change the value of variable individually?\nFor example: i only want to change some weights in a variable with shape = (3, 3, 16, 16)\nThanks you very much!. Hi!\nWhen i try to run print((sess.run(w))), i got the error as below:\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value res1.0/conv1/Wn\n     [[Node: res1.0/conv1/Wn/read = Identity[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](res1.0/conv1/Wn)]]\n     [[Node: mul_4/_1 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_49_mul_4\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nI tried to initialize the variables: sess.run(tf.initialize_variables([v for v in tf.all_variables()])) but i still get that error.\nThanks you!. Hi!\nHow can i see the graph of Inference only in Tensorboard?\nThanks you!. Hi!\nHow can i resume training after interrupted it?\nThanks you!. hello, what is the difference between OfflinePredictor and SimpleDatasetPredictor, because when i run inference on Imagenet with these functions, it give me different result.. Even if i run SimpleDatasetPredictor with batch_size=1 from dataset, it give different result compared with sess.run (the same result as OfflinePredictor) in Tensorflow. . ",
    "auroua": "The training is continuing, but quite slow 40%|###9      |199/500[03:10<04:49, 1.04it/s].\nI have used this tf-faster-rcnn implementation before. It doesn't report any error even with ResNet-101 backbone. I have set MODEL_MASK=False but I still got this error.\nCan you tell me what caused this error? Thanks.. ",
    "Dataphz": "Thank you for your apply, here is the issue:\n\n\nWhat you did:\n[train command, in order to transfer/finetune] \npython train.py --gpu 0,1,2,3 --datadir /coco --load /pretrained_models/COCO-R50C4-MaskRCNN-Standard.npz \n\n\nWhat you observed, including but not limited to the entire logs.\n  The AP performance on COCO val2014 dataset is almost 0. And the training precision/recall almost improves, it seems that the pretrained model on COCO is same as the ImageNet-R50.npz.\n\n\nYour environment:\n\nPython python3.6\nTF version: 1.4.0.\nTensorpack version: 0.8.8 (But using 0.8.5 version of FastRCNN code)\n. My TF version is limited to 1.4.0, so that choosing 0.8.5 version fasterrcnn code...  And i also evaluated the model, the ap is also almost 0. And i verified the model is loaded.\nDo u  have the pretrained model of 0.8.5 version model?  if convenient, can u share it with me.\nThanks for your apply!. \n",
    "lizaigaoge550": "Thank you. When i call TrainConfig,  I can write it as TrainConfig(data=StagingInput(QueueInput(...))). Is it right?  . ",
    "Myeongjoon": "I found the same error.\nIf install tensorpack by pip install tensorpack this will return this error.\nSo to solve this error, must use github code.. ",
    "liningxiao": "\nI found the same error.\nIf install tensorpack by pip install tensorpack this will return this error.\nSo to solve this error, must use github code.\n\nthanks! it works!. @ppwwyyxx based on this code, i trained my own data.the framework used cascade-rcnn,but the model i trained could not test my data,the returned result was empty. the hint was missing variables. i did not know if it was a problem with the model or a problem with the training code.there was a hint on it.\nthis is my config:./train.py --config \\\n    MODE_MASK=True MODE_FPN=True FPN.CASCADE=True BACKBONE.RESNET_NUM_BLOCK=[3,4,23,3] \\\n    DATA.BASEDIR=./data/coco/ \\\n    BACKBONE.WEIGHTS=./data/models/COCO-R101FPN-MaskRCNN-BetterParams.npz \\\n    TEST.RESULT_SCORE_THRESH=1e-4 \\\n    PREPROC.TRAIN_SHORT_EDGE_SIZE=[640,800] \\\n    TRAIN.LR_SCHEDULE=[420000,500000,540000]. > The warning said \"The following variables are in the checkpoint, but not found in the graph\". It means some variables are used in training but not found (i.e. not used) in inference, which is OK.\n\nIt would not be OK if it's the other way around.\n\n1 \u3001I didn't use BACKBONE. WEIGHTS. I trained 189 classes of models, - config and training parameters are consistent, but - pridict can't detect the target, but using the model you provided can detect the target. I'm not sure what the problem is? Do you have any suggestions? Thank you\n2\u3001Another question, how do I save the npz format model? It is consistent with the model you provide. I am not sure whether this question is related to format.. > 1. \"some variables are used in training but not found (i.e. not used) in inference\", and in the models I provided, I have manually removed such variables, so you won't see any warnings.\n\nAs for why your model does not perform well, it is a machine learning/computer vision question that we'll not be able to answer.\n2. You do not need to use that format because tensorpack recognize both. To make the conversion, load and save the variables with https://tensorpack.readthedocs.io/modules/tfutils.html#tensorpack.tfutils.varmanip.save_chkpt_vars\n\nOK\uff0cThanks.. > The config file currently has NUM_CATEGORY=80\nI have changed the config.py file.Where else should I modify it?\n_C.DATA.NUM_CATEGORY = 46 . > That's the only place you need to modify.\nThe problem still exists. Do you have any other suggestions?. > To modify the pre-trained model you can load it with np.load and save it with np.savez. Refer to numpy documentation for details.\nThank you. I'll try your method.. ",
    "fanq15": "I use my own dataset and use a generator to generate dataflow:\n```\nds = input_generator.get(**args)\nds = DataFromGenerator(ds)\nds = BatchData(ds, 8)\nds = PrefetchDataZMQ(ds, 1)\n...\nTrainConfig(\ndataflow = ds\n...\n)\nlaunch_train_with_config(config, SyncMultiGPUTrainerReplicated(4, average=False, mode='nccl')\n```\nAnd it stucks...\n[0829 14:08:18 @base.py:211] Initializing the session ...\n[0829 14:08:18 @sessinit.py:207] Variables to restore from dict: conv3_1/W:0, conv1_2/W:0, conv5_1/W:0, conv5_2/W:0, conv4_2/W:0, conv2_2/W:0, conv2_1/W:0, conv1_1/W:0, conv3_2/W:0, conv5_3/W:0, conv3_3/W:0, conv4_1/W:0, conv4_3/W:0\n[0829 14:08:18 @sessinit.py:90] WRN The following variables are in the graph, but not found in the dict: branch1/convfc/W:0, branch1/convfc/b:0, branch2/convfc/W:0, branch2/convfc/b:0, branch3/convfc/W:0, branch3/convfc/b:0, branch4/convfc/W:0, branch4/convfc/b:0, branch5/convfc/W:0, branch5/convfc/b:0, conv1_1/bn/beta:0, conv1_1/bn/gamma:0, conv1_1/bn/mean/EMA:0, conv1_1/bn/variance/EMA:0, conv1_2/bn/beta:0, conv1_2/bn/gamma:0, conv1_2/bn/mean/EMA:0, conv1_2/bn/variance/EMA:0, conv2_1/bn/beta:0, conv2_1/bn/gamma:0, conv2_1/bn/mean/EMA:0, conv2_1/bn/variance/EMA:0, conv2_2/bn/beta:0, conv2_2/bn/gamma:0, conv2_2/bn/mean/EMA:0, conv2_2/bn/variance/EMA:0, conv3_1/bn/beta:0, conv3_1/bn/gamma:0, conv3_1/bn/mean/EMA:0, conv3_1/bn/variance/EMA:0, conv3_2/bn/beta:0, conv3_2/bn/gamma:0, conv3_2/bn/mean/EMA:0, conv3_2/bn/variance/EMA:0, conv3_3/bn/beta:0, conv3_3/bn/gamma:0, conv3_3/bn/mean/EMA:0, conv3_3/bn/variance/EMA:0, conv4_1/bn/beta:0, conv4_1/bn/gamma:0, conv4_1/bn/mean/EMA:0, conv4_1/bn/variance/EMA:0, conv4_2/bn/beta:0, conv4_2/bn/gamma:0, conv4_2/bn/mean/EMA:0, conv4_2/bn/variance/EMA:0, conv4_3/bn/beta:0, conv4_3/bn/gamma:0, conv4_3/bn/mean/EMA:0, conv4_3/bn/variance/EMA:0, conv5_1/bn/beta:0, conv5_1/bn/gamma:0, conv5_1/bn/mean/EMA:0, conv5_1/bn/variance/EMA:0, conv5_2/bn/beta:0, conv5_2/bn/gamma:0, conv5_2/bn/mean/EMA:0, conv5_2/bn/variance/EMA:0, conv5_3/bn/beta:0, conv5_3/bn/gamma:0, conv5_3/bn/mean/EMA:0, conv5_3/bn/variance/EMA:0, convfcweight/W:0, global_step:0, learning_rate:0\n[0829 14:08:18 @sessinit.py:90] WRN The following variables are in the dict, but not found in the graph: conv1_1/b:0, conv1_2/b:0, conv2_1/b:0, conv2_2/b:0, conv3_1/b:0, conv3_2/b:0, conv3_3/b:0, conv4_1/b:0, conv4_2/b:0, conv4_3/b:0, conv5_1/b:0, conv5_2/b:0, conv5_3/b:0, fc6/W:0, fc6/b:0, fc7/W:0, fc7/b:0, fc8/W:0, fc8/b:0\n[0829 14:08:18 @sessinit.py:220] Restoring from dict ...\n[0829 14:08:20 @base.py:218] Graph Finalized.\n[0829 14:08:21 @concurrency.py:37] Starting EnqueueThread QueueInput/input_queue ...\n[0829 14:08:21 @graph.py:73] Running Op sync_variables/sync_variables_from_main_tower ...\n[0829 14:08:23 @base.py:250] Start Epoch 1 ...\n  0%|                                                                                                                                                                                                                    |0/100[00:00<?,?it/s][0829 14:08:23 @input_source.py:520] Pre-filling StagingArea .... What I really want to do is just using the dataflow to handle my data.\nThe data format is \n{'width': <tf.Tensor 'batch:4' shape=(16,) dtype=int64>, 'image': <tf.Tensor 'batch:1' shape=(16, 513, 513, 3) dtype=float32>, 'label': <tf.Tensor 'batch:3' shape=(16, 513, 513, 1) dtype=int32>, 'image_name': <tf.Tensor 'batch:2' shape=(16,) dtype=string>, 'height': <tf.Tensor 'batch:0' shape=(16,) dtype=int64>} which is processed by tf.train.batch\nor\n{'width': <tf.Tensor 'Reshape_6:0' shape=() dtype=int64>, 'image': <tf.Tensor 'cond/Merge:0' shape=(513, 513, 3) dtype=float32>, 'label': <tf.Tensor 'cond/Merge_1:0' shape=(513, 513, 1) dtype=int32>, 'image_name': <tf.Tensor 'Reshape_1:0' shape=() dtype=string>, 'height': <tf.Tensor 'Reshape_3:0' shape=() dtype=int64>} which is not processed by tf.train.batch\nAnd I don't know how to convert it to dataflow, because in your tutorials, the data are usually loaded totally first and are then processed.\nCould you help me fix it? \nThanks!\n. Thank you for your kind reply!\nWhen run \nds.reset_state()\nprint(next(ds.get_data()))\nTestDataSpeed(ds).start()\nThe print is :\n```\n[, ]\n  0%|                                                                                                                                                                                                                   |0/5000[00:00<?,?it/s]\n[0830 01:19:38 @param.py:194] Use train_log/hed/hyper.txt to set hyperparam: 'learning_rate'.\n[0830 01:19:42 @training.py:51] [DataParallel] Training a model of 4 towers.\n```\nMy generator is:\n```\nslim = tf.contrib.slim\ndataset_data_provider = slim.dataset_data_provider\ndef _get_data(data_provider, dataset_split):\n  if common.LABELS_CLASS not in data_provider.list_items():\n    raise ValueError('Failed to find labels.')\nimage, height, width = data_provider.get(\n      [common.IMAGE, common.HEIGHT, common.WIDTH])\n# Some datasets do not contain image_name.\n  if common.IMAGE_NAME in data_provider.list_items():\n    image_name, = data_provider.get([common.IMAGE_NAME])\n  else:\n    image_name = tf.constant('')\nlabel = None\n  if dataset_split != common.TEST_SET:\n    label, = data_provider.get([common.LABELS_CLASS])\nreturn image, label, image_name, height, width\ndef get(dataset,\n        crop_size,\n        batch_size,\n        min_resize_value=None,\n        max_resize_value=None,\n        resize_factor=None,\n        min_scale_factor=1.,\n        max_scale_factor=1.,\n        scale_factor_step_size=0,\n        num_readers=1,\n        num_threads=1,\n        dataset_split=None,\n        is_training=True,\n        model_variant=None):\nif dataset_split is None:\n    raise ValueError('Unknown dataset split.')\n  if model_variant is None:\n    tf.logging.warning('Please specify a model_variant. See '\n                       'feature_extractor.network_map for supported model '\n                       'variants.')\ndata_provider = dataset_data_provider.DatasetDataProvider(\n      dataset,\n      num_readers=num_readers,\n      num_epochs=None if is_training else 1,\n      shuffle=is_training)\n  image, label, image_name, height, width = _get_data(data_provider,\n                                                      dataset_split)\n  if label is not None:\n                                                                                                                                                                                                                                if label.shape.ndims == 2:\n      label = tf.expand_dims(label, 2)\n    elif label.shape.ndims == 3 and label.shape.dims[2] == 1:\n      pass\n    else:\n      raise ValueError('Input label shape must be [height, width], or '\n                       '[height, width, 1].')\nlabel.set_shape([None, None, 1])\n\noriginal_image, image, label = input_preprocess.preprocess_image_and_label(\n      image,\n      label,\n      crop_height=crop_size[0],\n      crop_width=crop_size[1],\n      min_resize_value=min_resize_value,\n      max_resize_value=max_resize_value,\n      resize_factor=resize_factor,\n      min_scale_factor=min_scale_factor,\n      max_scale_factor=max_scale_factor,\n      scale_factor_step_size=scale_factor_step_size,\n      ignore_label=dataset.ignore_label,\n      is_training=is_training,\n      model_variant=model_variant)\n  sample = {\n      common.IMAGE: image,\n      common.IMAGE_NAME: image_name,\n      common.HEIGHT: height,\n      common.WIDTH: width\n  }\n  if label is not None:\n    sample[common.LABEL] = tf.squeeze(label, -1)\nif not is_training:\n    # Original image is only used during visualization.\n    sample[common.ORIGINAL_IMAGE] = original_image,\n    num_threads = 1\n  yield [sample[common.IMAGE], sample[common.LABEL]]\nAnd I generate my dataflow through:\ndef get_data():\n    dataset = segmentation_dataset.get_dataset(\n      FLAGS.dataset, FLAGS.train_split, dataset_dir=FLAGS.dataset_dir)\n#tf.gfile.MakeDirs(FLAGS.train_logdir)\n#tf.logging.info('Training on %s set', FLAGS.train_split)\n\n#samples = MyDataFlow(\nsamples = input_generator_sync.get(\n      dataset,\n      FLAGS.train_crop_size,\n      FLAGS.train_batch_size,\n      min_resize_value=FLAGS.min_resize_value,\n      max_resize_value=FLAGS.max_resize_value,\n      resize_factor=FLAGS.resize_factor,\n      min_scale_factor=FLAGS.min_scale_factor,\n      max_scale_factor=FLAGS.max_scale_factor,\n      scale_factor_step_size=FLAGS.scale_factor_step_size,\n      dataset_split=FLAGS.train_split,\n      is_training=True,\n      model_variant=FLAGS.model_variant)\n\nds = DataFromGenerator(samples)\n#ds = BatchDataByShape(ds, 8, idx=0) # when remove it and the PrefecthDataZMQ, the test works.\n\n#ds = PrefetchDataZMQ(ds, 1) #\nreturn ds\n\nand\n    dataset_train = get_data() #get_data('train')\n    #dataset_train = input_generator_sync()\n    steps_per_epoch = 100 # dataset_train.size() * 40\n    #dataset_val = get_data('val')\n    print '================='\n    dataset_train.reset_state()\n    print(next(dataset_train.get_data()))\n    TestDataSpeed(dataset_train).start()\nAnd the dataset is generated by:\ndef get_dataset(dataset_name, split_name, dataset_dir):\n  \"\"\"Gets an instance of slim Dataset.\nArgs:\n    dataset_name: Dataset name.\n    split_name: A train/val Split name.\n    dataset_dir: The directory of the dataset sources.\nReturns:\n    An instance of slim Dataset.\nRaises:\n    ValueError: if the dataset_name or split_name is not recognized.\n  \"\"\"\n  if dataset_name not in _DATASETS_INFORMATION:\n    raise ValueError('The specified dataset is not supported yet.')\nsplits_to_sizes = _DATASETS_INFORMATION[dataset_name].splits_to_sizes\nif split_name not in splits_to_sizes:\n    raise ValueError('data split name %s not recognized' % split_name)\n# Prepare the variables for different datasets.\n  num_classes = _DATASETS_INFORMATION[dataset_name].num_classes\n  ignore_label = _DATASETS_INFORMATION[dataset_name].ignore_label\nfile_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n# Specify how the TF-Examples are decoded.\n  keys_to_features = {\n      'image/encoded': tf.FixedLenFeature(\n          (), tf.string, default_value=''),\n      'image/filename': tf.FixedLenFeature(\n          (), tf.string, default_value=''),\n      'image/format': tf.FixedLenFeature(\n          (), tf.string, default_value='jpeg'),\n      'image/height': tf.FixedLenFeature(\n          (), tf.int64, default_value=0),\n      'image/width': tf.FixedLenFeature(\n          (), tf.int64, default_value=0),\n      'image/segmentation/class/encoded': tf.FixedLenFeature(\n          (), tf.string, default_value=''),\n      'image/segmentation/class/format': tf.FixedLenFeature(\n          (), tf.string, default_value='png'),\n  }\n  items_to_handlers = {\n      'image': tfexample_decoder.Image(\n          image_key='image/encoded',\n          format_key='image/format',\n          channels=3),\n      'image_name': tfexample_decoder.Tensor('image/filename'),\n      'height': tfexample_decoder.Tensor('image/height'),\n      'width': tfexample_decoder.Tensor('image/width'),\n      'labels_class': tfexample_decoder.Image(\n          image_key='image/segmentation/class/encoded',\n          format_key='image/segmentation/class/format',\n          channels=1),\n  }\ndecoder = tfexample_decoder.TFExampleDecoder(\n      keys_to_features, items_to_handlers)\nreturn dataset.Dataset(\n      data_sources=file_pattern,\n      reader=tf.TFRecordReader,\n      decoder=decoder,\n      num_samples=splits_to_sizes[split_name],\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      ignore_label=ignore_label,\n      num_classes=num_classes,\n      name=dataset_name,\n      multi_label=True)\n```\n. You mean that I can use the tensorflow tensors as inputs in the tensorpack trainer?. But it seems that other data formats can't be used in tensorpack trainer:\n```\nAssertionError: \nAssertionError: \nAssertionError: \n. Sorry, it is my fault... \nBut when I use the `data=inputs_queue`, where the `inputs_queue=prefetch_queue.prefetch_queue(\n          samples, capacity=128 * config.num_clones)`\nI got the error:\nTraceback (most recent call last):\n  File \"deeplab/hed.py\", line 495, in \n    tf.app.run()\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\n    _sys.exit(main(argv))\n  File \"deeplab/hed.py\", line 484, in main\n    config = get_config()\n  File \"deeplab/hed.py\", line 440, in get_config\n    max_epoch=100,\n  File \"/usr/local/lib/python2.7/site-packages/tensorpack/train/config.py\", line 101, in init\n    assert_type(data, InputSource)\n  File \"/usr/local/lib/python2.7/site-packages/tensorpack/train/config.py\", line 94, in assert_type\n    assert isinstance(v, tp), v.class\nAssertionError: \n`` \nCan thedata=accept theFIFOQueue` as inputs?\nThe code is \n```\n    dataset = segmentation_dataset.get_dataset(\n      FLAGS.dataset, FLAGS.train_split, dataset_dir=FLAGS.dataset_dir)\nsamples = input_generator.get(\n      dataset,\n      FLAGS.train_crop_size,\n      FLAGS.train_batch_size,\n      min_resize_value=FLAGS.min_resize_value,\n      max_resize_value=FLAGS.max_resize_value,\n      resize_factor=FLAGS.resize_factor,\n      min_scale_factor=FLAGS.min_scale_factor,\n      max_scale_factor=FLAGS.max_scale_factor,\n      scale_factor_step_size=FLAGS.scale_factor_step_size,\n      dataset_split=FLAGS.train_split,\n      is_training=True,\n      model_variant=FLAGS.model_variant)\ndataset_train = prefetch_queue.prefetch_queue(\n      samples, capacity=128)\n\nsteps_per_epoch = 100 #dataset_train.size() * 40\nprint '===========dataset_train:', dataset_train\nprint '===========samples:', samples\n#dataset_val = get_data('val')\n\nreturn TrainConfig(\n    data=dataset_train,\n    callbacks=[\n        ModelSaver(),\n        ScheduledHyperParamSetter('learning_rate', [(30, 6e-6), (45, 1e-6), (60, 8e-7)]),\n        HumanHyperParamSetter('learning_rate') #,\n        #InferenceRunner(dataset_val,\n        #                BinaryClassificationStats('prediction', 'edgemap4d'))\n    ],\n    model=Model(),\n    steps_per_epoch=steps_per_epoch,\n    max_epoch=100,\n)\n\n``\nWhere thesamplesis atf.train.batchwith the format of{'image': , 'label': }`.\n. Alright, is there any InputSource suitable for the FIFOQueue?\nI want to handle the samples which has the data format: {'image': <tf.Tensor 'batch:0' shape=(8, 513, 513, 3) dtype=float32>, 'label': <tf.Tensor 'batch:1' shape=(8, 513, 513, 1) dtype=int32>}. It is a dict, and cannot be processed by the TensorInput.\nOr I can handle the dataset_train which has the data format: <class 'tensorflow.python.ops.data_flow_ops.FIFOQueue'>.\nIt seems that there is not suitable InputSource for my data.. It doesn't work...\nI change the samples to list with the format:\n===========samples: [<tf.Tensor 'cond/Merge:0' shape=(513, 513, 3) dtype=float32>, <tf.Tensor 'cond/Merge_1:0' shape=(513, 513, 1) dtype=int32>]\nBut I get the error:\nTraceback (most recent call last):\n  File \"deeplab/hed.py\", line 495, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\n    _sys.exit(main(argv))\n  File \"deeplab/hed.py\", line 489, in main\n    SyncMultiGPUTrainer(max(get_num_gpu(), 1)))\n  File \"/usr/local/lib/python2.7/site-packages/tensorpack/train/interface.py\", line 85, in launch_train_with_config\n    model._build_graph_get_cost, model.get_optimizer)\n  File \"/usr/local/lib/python2.7/site-packages/tensorpack/utils/argtools.py\", line 181, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/site-packages/tensorpack/train/tower.py\", line 194, in setup_graph\n    train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)\n  File \"/usr/local/lib/python2.7/site-packages/tensorpack/train/trainers.py\", line 93, in _setup_graph\n    self._make_get_grad_fn(input, get_cost_fn, get_opt_fn), get_opt_fn)\n  File \"/usr/local/lib/python2.7/site-packages/tensorpack/graph_builder/training.py\", line 160, in build\n    grad_list = DataParallelBuilder.build_on_towers(self.towers, get_grad_fn, devices)\n  File \"/usr/local/lib/python2.7/site-packages/tensorpack/graph_builder/training.py\", line 118, in build_on_towers\n    ret.append(func())\n  File \"/usr/local/lib/python2.7/site-packages/tensorpack/train/tower.py\", line 222, in get_grad_fn\n    cost = get_cost_fn(*input.get_input_tensors())\n  File \"/usr/local/lib/python2.7/site-packages/tensorpack/input_source/input_source_base.py\", line 81, in get_input_tensors\n    return self._get_input_tensors()\n  File \"/usr/local/lib/python2.7/site-packages/tensorpack/input_source/input_source.py\", line 352, in _get_input_tensors\n    ret = self.get_tensor_fn()\nTypeError: 'list' object is not callable. Alright. I find an example in the PTB training example. I should read the document more carefully. Thank you for your kind help! And I will reply later after I solve it.. It works!\nI use the \nsamples = input_generator.get(\n          dataset,\n          FLAGS.train_crop_size,\n          FLAGS.train_batch_size,\n          min_resize_value=FLAGS.min_resize_value,\n          max_resize_value=FLAGS.max_resize_value,\n          resize_factor=FLAGS.resize_factor,\n          min_scale_factor=FLAGS.min_scale_factor,\n          max_scale_factor=FLAGS.max_scale_factor,\n          scale_factor_step_size=FLAGS.scale_factor_step_size,\n          dataset_split=FLAGS.train_split,\n          is_training=True,\n          model_variant=FLAGS.model_variant)\nto generate input tensor list.\nAnd it is wrong to just put the samples into the TensorInput, which is data=TensorInput(samples).\nIt should be a lambda function to make the function callable:\ndata=TensorInput(lambda: samples, iter_steps)\nWe can see more details in the tensorpack/examples/PennTreebank.\nBTW, tensorpack is really a powerful tool! Thank you for your contribution! And I think the document about the use of tensorflow tensor input should be easier to access. . Sorry about my unsuitable issue...\nI define my graph using tensorpack layer and load a tensorflow model. I change some tensor names to load it correctly, such as: change W to weights and b to biases. Actually, I don't know why the tensor names in tensorpack's Conv2D is not corresponding to the tensorflow's. Even I go through the entire tensorpack code, I can not find anything about ExponentialMovingAverage, and there are nothing in tensorpack corresponding to it, so I cannot adapt it through change the code. . ",
    "Coderx7": "@ppwwyyxx you are right I also agree with that. Starting off with the imagenet models would be a good starting port and hopefully as the whole project gets momentum, other sections will be added/ported by contributors. . You are correct that this is not a high priority subject or even one to consider at all. But what I'm trying to convey here is not copy-pasting the code from an old repository to a newer one, but rather providing a base with more efficient implementation matters.\nTo me it makes no sense I spend twice or 5x the time on training something while I can easily have a massive speedup using tensorpack! this is currently the issue. \nIn order to address that, one way would be to port everything to tensorpack and training them to show that everything works as expected. Another way that's much suitable I guess would be to specify the bottlenecks and explain in the documentation that how one can overcome them. to be more exact, how to make an existing TF implementation run faster. (what would you do to make them run faster? what would you change and why?) \nI saw an example of this concerning datasets in the documentation. an explanation of how to go about such things would be enough and would have the same outcome of porting everything to tensorboard except that the contributors will be doing it. \n. ",
    "AlgoHunt": "sorry about the confusing,\nI clone the FasterRcnn example from \nhttps://github.com/tensorpack/tensorpack/tree/01245d68c30c223bb598d91b390d41abc7f94640\nrun it with\n python train.py \nthe only change in config is I load a resnet50 backbone weights. and FPN = True\nI also made some change in data.py from:\nreturn ret\nto\nnames = ['image', 'anchor_labels_lvl2', 'anchor_boxes_lvl2', 'anchor_labels_lvl3', 'anchor_boxes_lvl3', 'anchor_labels_lvl4', 'anchor_boxes_lvl4', 'anchor_labels_lvl5', 'anchor_boxes_lvl5', 'anchor_labels_lvl6', 'anchor_boxes_lvl6', 'gt_boxes', 'gt_labels', 'gt_masks']\nreturn [ret[name] for name in names]\n. Forgot to mention that I used a custom dataset, which only have a little  fg , but I don't think thats where the bug comes from\nwhen trained with tf1.10.0 for 20 epoch\n\nfastrcnn_losses/box_loss: 0.0022282\n [32m[0902 01:57:11 @monitor.py:435] [0m fastrcnn_losses/label_loss: 0.028814\n [32m[0902 01:57:11 @monitor.py:435] [0m fastrcnn_losses/label_metrics/accuracy: 0.9955\n [32m[0902 01:57:11 @monitor.py:435] [0m fastrcnn_losses/label_metrics/false_negative: 1\n [32m[0902 01:57:11 @monitor.py:435] [0m fastrcnn_losses/label_metrics/fg_accuracy: 2.2959e-37\n [32m[0902 01:57:11 @monitor.py:435] [0m learning_rate: 0.01\n [32m[0902 01:57:11 @monitor.py:435] [0m maskrcnn_loss/accuracy: 0.49791\n [32m[0902 01:57:11 @monitor.py:435] [0m maskrcnn_loss/fg_pixel_ratio: 0.50209\n [32m[0902 01:57:11 @monitor.py:435] [0m maskrcnn_loss/maskrcnn_loss: nan\n [32m[0902 01:57:11 @monitor.py:435] [0m maskrcnn_loss/pos_accuracy: 2.3443e-37\n [32m[0902 01:57:11 @monitor.py:435] [0m multilevel_roi_align/fpn_map_rois_to_levels/num_roi_level2: 511.92\n [32m[0902 01:57:11 @monitor.py:435] [0m multilevel_roi_align/fpn_map_rois_to_levels/num_roi_level3: 0.080237\n [32m[0902 01:57:11 @monitor.py:435] [0m multilevel_roi_align/fpn_map_rois_to_levels/num_roi_level4: 0.0023035\n [32m[0902 01:57:11 @monitor.py:435] [0m multilevel_roi_align/fpn_map_rois_to_levels/num_roi_level5: 2.3035e-37\n [32m[0902 01:57:11 @monitor.py:435] [0m multilevel_roi_align_mask/fpn_map_rois_to_levels/num_roi_level2: 2.222\n [32m[0902 01:57:11 @monitor.py:435] [0m multilevel_roi_align_mask/fpn_map_rois_to_levels/num_roi_level3: 0.080237\n [32m[0902 01:57:11 @monitor.py:435] [0m multilevel_roi_align_mask/fpn_map_rois_to_levels/num_roi_level4: 0.0023035\n [32m[0902 01:57:11 @monitor.py:435] [0m multilevel_roi_align_mask/fpn_map_rois_to_levels/num_roi_level5: 0\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/box_loss: 0.015912\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/label_loss: 0.12846\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level2/box_loss: 0.0099081\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level2/label_loss: 0.085681\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level2/label_metrics/precision_th0.1: 0.5\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level2/label_metrics/precision_th0.2: 0.5\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level2/label_metrics/precision_th0.5: 0.5\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level2/label_metrics/recall_th0.1: 0.26406\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level2/label_metrics/recall_th0.2: 0.26406\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level2/label_metrics/recall_th0.5: 0.26406\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level2/num_pos_anchor: 4.6261\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level2/num_valid_anchor: 200.4\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level3/box_loss: 0.0043817\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level3/label_loss: 0.034303\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level3/label_metrics/precision_th0.1: 0.5\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level3/label_metrics/precision_th0.2: 0.5\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level3/label_metrics/precision_th0.5: 0.5\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level3/label_metrics/recall_th0.1: 0.31281\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level3/label_metrics/recall_th0.2: 0.31281\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level3/label_metrics/recall_th0.5: 0.31281\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level3/num_pos_anchor: 2.068\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level3/num_valid_anchor: 45.1\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level4/box_loss: 0.001617\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level4/label_loss: 0.0082804\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level4/label_metrics/precision_th0.1: 0.5\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level4/label_metrics/precision_th0.2: 0.5\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level4/label_metrics/precision_th0.5: 0.5\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level4/label_metrics/recall_th0.1: 0.41653\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level4/label_metrics/recall_th0.2: 0.41653\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level4/label_metrics/recall_th0.5: 0.41653\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level4/num_pos_anchor: 0.55376\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level4/num_valid_anchor: 9.2134\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level5/box_loss: 5.1986e-06\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level5/label_loss: 0.00018782\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level5/label_metrics/precision_th0.1: 0.5\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level5/label_metrics/precision_th0.2: 0.5\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level5/label_metrics/precision_th0.5: 0.5\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level5/label_metrics/recall_th0.1: 0.49879\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level5/label_metrics/recall_th0.2: 0.49879\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level5/label_metrics/recall_th0.5: 0.49879\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level5/num_pos_anchor: 0.0049376\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level5/num_valid_anchor: 1.2441\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level6/box_loss: 0\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level6/label_loss: 6.2402e-06\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level6/label_metrics/precision_th0.1: 0.5\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level6/label_metrics/precision_th0.2: 0.5\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level6/label_metrics/precision_th0.5: 0.5\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level6/label_metrics/recall_th0.1: 0.5\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level6/label_metrics/recall_th0.2: 0.5\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level6/label_metrics/recall_th0.5: 0.5\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level6/num_pos_anchor: 0\n [32m[0902 01:57:11 @monitor.py:435] [0m rpn_losses/level6/num_valid_anchor: 0.04577\n [32m[0902 01:57:11 @monitor.py:435] [0m sample_fast_rcnn_targets/num_bg: 509.7\n [32m[0902 01:57:11 @monitor.py:435] [0m sample_fast_rcnn_targets/num_fg: 2.3046\n [32m[0902 01:57:11 @monitor.py:435] [0m sample_fast_rcnn_targets/proposal_metrics/best_iou_per_gt: nan\n [32m[0902 01:57:11 @monitor.py:435] [0m sample_fast_rcnn_targets/proposal_metrics/recall_iou0.3: 0.16727\n [32m[0902 01:57:11 @monitor.py:435] [0m sample_fast_rcnn_targets/proposal_metrics/recall_iou0.5: 0.051163\n [32m[0902 01:57:11 @monitor.py:435] [0m total_cost: nan\n [32m[0902 01:57:11 @monitor.py:435] [0m wd_cost: nan\n\nwhen trained with tf1.8.0 for 20 epoch\n\n[32m[0902 04:44:37 @monitor.py:435] [0m fastrcnn_losses/box_loss: 0.076984\n [32m[0902 04:44:37 @monitor.py:435] [0m fastrcnn_losses/label_loss: 0.03472\n [32m[0902 04:44:37 @monitor.py:435] [0m fastrcnn_losses/label_metrics/accuracy: 0.98665\n [32m[0902 04:44:37 @monitor.py:435] [0m fastrcnn_losses/label_metrics/false_negative: 0.16149\n [32m[0902 04:44:37 @monitor.py:435] [0m fastrcnn_losses/label_metrics/fg_accuracy: 0.83851\n [32m[0902 04:44:37 @monitor.py:435] [0m learning_rate: 0.01\n [32m[0902 04:44:37 @monitor.py:435] [0m maskrcnn_loss/accuracy: 0.88644\n [32m[0902 04:44:37 @monitor.py:435] [0m maskrcnn_loss/fg_pixel_ratio: 0.44969\n [32m[0902 04:44:37 @monitor.py:435] [0m maskrcnn_loss/maskrcnn_loss: 0.25286\n [32m[0902 04:44:37 @monitor.py:435] [0m maskrcnn_loss/pos_accuracy: 0.39374\n [32m[0902 04:44:37 @monitor.py:435] [0m multilevel_roi_align/fpn_map_rois_to_levels/num_roi_level2: 440.01\n [32m[0902 04:44:37 @monitor.py:435] [0m multilevel_roi_align/fpn_map_rois_to_levels/num_roi_level3: 53.552\n [32m[0902 04:44:37 @monitor.py:435] [0m multilevel_roi_align/fpn_map_rois_to_levels/num_roi_level4: 17.493\n [32m[0902 04:44:37 @monitor.py:435] [0m multilevel_roi_align/fpn_map_rois_to_levels/num_roi_level5: 0.94497\n [32m[0902 04:44:37 @monitor.py:435] [0m multilevel_roi_align_mask/fpn_map_rois_to_levels/num_roi_level2: 23.713\n [32m[0902 04:44:37 @monitor.py:435] [0m multilevel_roi_align_mask/fpn_map_rois_to_levels/num_roi_level3: 3.6506\n [32m[0902 04:44:37 @monitor.py:435] [0m multilevel_roi_align_mask/fpn_map_rois_to_levels/num_roi_level4: 0.13955\n [32m[0902 04:44:37 @monitor.py:435] [0m multilevel_roi_align_mask/fpn_map_rois_to_levels/num_roi_level5: 0\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/box_loss: 0.0092716\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/label_loss: 0.0047935\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level2/box_loss: 0.0081492\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level2/label_loss: 0.0037865\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level2/label_metrics/precision_th0.1: 0.58309\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level2/label_metrics/precision_th0.2: 0.59838\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level2/label_metrics/precision_th0.5: 0.61808\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level2/label_metrics/recall_th0.1: 0.70027\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level2/label_metrics/recall_th0.2: 0.70025\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level2/label_metrics/recall_th0.5: 0.69964\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level2/num_pos_anchor: 5.3862\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level2/num_valid_anchor: 199.5\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level3/box_loss: 0.00070578\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level3/label_loss: 0.00034172\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level3/label_metrics/precision_th0.1: 0.59589\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level3/label_metrics/precision_th0.2: 0.61341\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level3/label_metrics/precision_th0.5: 0.61765\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level3/label_metrics/recall_th0.1: 0.63041\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level3/label_metrics/recall_th0.2: 0.63041\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level3/label_metrics/recall_th0.5: 0.63041\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level3/num_pos_anchor: 1.5428\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level3/num_valid_anchor: 44.827\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level4/box_loss: 0.0003572\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level4/label_loss: 0.00065178\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level4/label_metrics/precision_th0.1: 0.60173\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level4/label_metrics/precision_th0.2: 0.60216\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level4/label_metrics/precision_th0.5: 0.60358\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level4/label_metrics/recall_th0.1: 0.59635\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level4/label_metrics/recall_th0.2: 0.59635\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level4/label_metrics/recall_th0.5: 0.59635\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level4/num_pos_anchor: 0.92755\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level4/num_valid_anchor: 10.267\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level5/box_loss: 5.9436e-05\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level5/label_loss: 1.3451e-05\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level5/label_metrics/precision_th0.1: 0.51215\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level5/label_metrics/precision_th0.2: 0.51215\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level5/label_metrics/precision_th0.5: 0.51215\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level5/label_metrics/recall_th0.1: 0.51215\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level5/label_metrics/recall_th0.2: 0.51215\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level5/label_metrics/recall_th0.5: 0.51214\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level5/num_pos_anchor: 0.10742\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level5/num_valid_anchor: 1.4042\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level6/box_loss: 0\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level6/label_loss: 2.7921e-08\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level6/label_metrics/precision_th0.1: 0.5\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level6/label_metrics/precision_th0.2: 0.5\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level6/label_metrics/precision_th0.5: 0.5\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level6/label_metrics/recall_th0.1: 0.5\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level6/label_metrics/recall_th0.2: 0.5\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level6/label_metrics/recall_th0.5: 0.5\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level6/num_pos_anchor: 0\n [32m[0902 04:44:37 @monitor.py:435] [0m rpn_losses/level6/num_valid_anchor: 0.0015343\n [32m[0902 04:44:37 @monitor.py:435] [0m sample_fast_rcnn_targets/num_bg: 484.5\n [32m[0902 04:44:37 @monitor.py:435] [0m sample_fast_rcnn_targets/num_fg: 27.504\n [32m[0902 04:44:37 @monitor.py:435] [0m sample_fast_rcnn_targets/proposal_metrics/best_iou_per_gt: 0.67628\n [32m[0902 04:44:37 @monitor.py:435] [0m sample_fast_rcnn_targets/proposal_metrics/recall_iou0.3: 0.86692\n [32m[0902 04:44:37 @monitor.py:435] [0m sample_fast_rcnn_targets/proposal_metrics/recall_iou0.5: 0.76383\n [32m[0902 04:44:37 @monitor.py:435] [0m total_cost: 0.84116\n [32m[0902 04:44:37 @monitor.py:435] [0m wd_cost: 0.46254. \n",
    "xiezheng-cs": "OK, thank you very much!. ",
    "Em-Lopez": "(tensorpack) C:\\Users\\cynth\\Documents\\Mini3\\Scripts>python -m pip install --upgrade pip\nCollecting pip\n  Using cached https://files.pythonhosted.org/packages/5f/25/e52d3f31441505a5f3af41213346e5b6c221c9e086a166f3703d2ddaf940/pip-18.0-py2.py3-none-any.whl\nInstalling collected packages: pip\n  Found existing installation: pip 10.0.1\n    Uninstalling pip-10.0.1:\n      Successfully uninstalled pip-10.0.1\nSuccessfully installed pip-18.0\n(tensorpack) C:\\Users\\cynth\\Documents\\Mini3\\Scripts>pip install --upgrade git+https://github.com/tensorpack/tensorpack.git\nCollecting git+https://github.com/tensorpack/tensorpack.git\n  Cloning https://github.com/tensorpack/tensorpack.git to c:\\users\\cynth\\appdata\\local\\temp\\pip-req-build-4qxs6zaz\n    Complete output from command python setup.py egg_info:\n    running egg_info\n    creating pip-egg-info\\tensorpack.egg-info\n    writing pip-egg-info\\tensorpack.egg-info\\PKG-INFO\n    writing dependency_links to pip-egg-info\\tensorpack.egg-info\\dependency_links.txt\n    writing requirements to pip-egg-info\\tensorpack.egg-info\\requires.txt\n    writing top-level names to pip-egg-info\\tensorpack.egg-info\\top_level.txt\n    writing manifest file 'pip-egg-info\\tensorpack.egg-info\\SOURCES.txt'\n    error: package directory 'find:                             # will call find_packages()' does not exist\n----------------------------------------\n\nCommand \"python setup.py egg_info\" failed with error code 1 in C:\\Users\\cynth\\AppData\\Local\\Temp\\pip-req-build-4qxs6zaz\\\nYou are using pip version 10.0.1, however version 18.0 is available.\nYou should consider upgrading via the 'python -m pip install --upgrade pip' command.\n(tensorpack) C:\\Users\\cynth\\Documents\\Mini3\\Scripts>python --version\nPython 3.6.6 :: Anaconda, Inc.\n. (tensorpack) C:\\Users\\cynth\\Documents\\Mini3\\Scripts>pip --verbose install --upgrade git+https://github.com/tensorpack/tensorpack.git\nConfig variable 'Py_DEBUG' is unset, Python ABI tag may be incorrect\nConfig variable 'WITH_PYMALLOC' is unset, Python ABI tag may be incorrect\nCreated temporary directory: C:\\Users\\cynth\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-67gp1d2g\nCreated temporary directory: C:\\Users\\cynth\\AppData\\Local\\Temp\\pip-install-1pqi0g_r\nCollecting git+https://github.com/tensorpack/tensorpack.git\n  Created temporary directory: C:\\Users\\cynth\\AppData\\Local\\Temp\\pip-req-build-2c09im7c\n  Cloning https://github.com/tensorpack/tensorpack.git to c:\\users\\cynth\\appdata\\local\\temp\\pip-req-build-2c09im7c\n  Running command git clone -q https://github.com/tensorpack/tensorpack.git C:\\Users\\cynth\\AppData\\Local\\Temp\\pip-req-build-2c09im7c\n  Running setup.py (path:C:\\Users\\cynth\\AppData\\Local\\Temp\\pip-req-build-2c09im7c\\setup.py) egg_info for package from git+https://github.com/tensorpack/tensorpack.git\n    Running command python setup.py egg_info\n    running egg_info\n    creating pip-egg-info\\tensorpack.egg-info\n    writing pip-egg-info\\tensorpack.egg-info\\PKG-INFO\n    writing dependency_links to pip-egg-info\\tensorpack.egg-info\\dependency_links.txt\n    writing requirements to pip-egg-info\\tensorpack.egg-info\\requires.txt\n    writing top-level names to pip-egg-info\\tensorpack.egg-info\\top_level.txt\n    writing manifest file 'pip-egg-info\\tensorpack.egg-info\\SOURCES.txt'\n    error: package directory 'find:                             # will call find_packages()' does not exist\nCleaning up...\n  Removing source in C:\\Users\\cynth\\AppData\\Local\\Temp\\pip-req-build-2c09im7c\nCommand \"python setup.py egg_info\" failed with error code 1 in C:\\Users\\cynth\\AppData\\Local\\Temp\\pip-req-build-2c09im7c\\\nException information:\nTraceback (most recent call last):\n  File \"C:\\Users\\cynth\\Documents\\Mini3\\lib\\site-packages\\pip_internal\\basecommand.py\", line 228, in main\n    status = self.run(options, args)\n  File \"C:\\Users\\cynth\\Documents\\Mini3\\lib\\site-packages\\pip_internal\\commands\\install.py\", line 291, in run\n    resolver.resolve(requirement_set)\n  File \"C:\\Users\\cynth\\Documents\\Mini3\\lib\\site-packages\\pip_internal\\resolve.py\", line 103, in resolve\n    self._resolve_one(requirement_set, req)\n  File \"C:\\Users\\cynth\\Documents\\Mini3\\lib\\site-packages\\pip_internal\\resolve.py\", line 257, in _resolve_one\n    abstract_dist = self._get_abstract_dist_for(req_to_install)\n  File \"C:\\Users\\cynth\\Documents\\Mini3\\lib\\site-packages\\pip_internal\\resolve.py\", line 210, in _get_abstract_dist_for\n    self.require_hashes\n  File \"C:\\Users\\cynth\\Documents\\Mini3\\lib\\site-packages\\pip_internal\\operations\\prepare.py\", line 324, in prepare_linked_requirement\n    abstract_dist.prep_for_dist(finder, self.build_isolation)\n  File \"C:\\Users\\cynth\\Documents\\Mini3\\lib\\site-packages\\pip_internal\\operations\\prepare.py\", line 154, in prep_for_dist\n    self.req.run_egg_info()\n  File \"C:\\Users\\cynth\\Documents\\Mini3\\lib\\site-packages\\pip_internal\\req\\req_install.py\", line 486, in run_egg_info\n    command_desc='python setup.py egg_info')\n  File \"C:\\Users\\cynth\\Documents\\Mini3\\lib\\site-packages\\pip_internal\\utils\\misc.py\", line 698, in call_subprocess\n    % (command_desc, proc.returncode, cwd))\npip._internal.exceptions.InstallationError: Command \"python setup.py egg_info\" failed with error code 1 in C:\\Users\\cynth\\AppData\\Local\\Temp\\pip-req-build-2c09im7c\\\nYou are using pip version 10.0.1, however version 18.0 is available.\nYou should consider upgrading via the 'python -m pip install --upgrade pip' command.\n(tensorpack) C:\\Users\\cynth\\Documents\\Mini3\\Scripts>. (tensorpack) C:\\Users\\cynth\\Documents\\Mini3\\Scripts>pip3 --version\npip 18.0 from c:\\users\\cynth\\documents\\mini3\\envs\\tensorpack\\lib\\site-packages\\pip (python 3.6)\n(tensorpack) C:\\Users\\cynth\\Documents\\Mini3\\Scripts>pip3 --verbose install --upgrade git+https://github.com/tensorpack/tensorpack.git\nConfig variable 'Py_DEBUG' is unset, Python ABI tag may be incorrect\nConfig variable 'WITH_PYMALLOC' is unset, Python ABI tag may be incorrect\nCreated temporary directory: C:\\Users\\cynth\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-_cvvlhug\nCreated temporary directory: C:\\Users\\cynth\\AppData\\Local\\Temp\\pip-req-tracker-i80u1q2z\nCreated requirements tracker 'C:\\Users\\cynth\\AppData\\Local\\Temp\\pip-req-tracker-i80u1q2z'\nCreated temporary directory: C:\\Users\\cynth\\AppData\\Local\\Temp\\pip-install-b5w7vwrp\nCollecting git+https://github.com/tensorpack/tensorpack.git\n  Created temporary directory: C:\\Users\\cynth\\AppData\\Local\\Temp\\pip-req-build-_x5nasqf\n  Cloning https://github.com/tensorpack/tensorpack.git to c:\\users\\cynth\\appdata\\local\\temp\\pip-req-build-_x5nasqf\n  Running command git clone -q https://github.com/tensorpack/tensorpack.git C:\\Users\\cynth\\AppData\\Local\\Temp\\pip-req-build-_x5nasqf\n  Added git+https://github.com/tensorpack/tensorpack.git to build tracker 'C:\\Users\\cynth\\AppData\\Local\\Temp\\pip-req-tracker-i80u1q2z'\n  Running setup.py (path:C:\\Users\\cynth\\AppData\\Local\\Temp\\pip-req-build-_x5nasqf\\setup.py) egg_info for package from git+https://github.com/tensorpack/tensorpack.git\n    Running command python setup.py egg_info\n    running egg_info\n    creating pip-egg-info\\tensorpack.egg-info\n    writing pip-egg-info\\tensorpack.egg-info\\PKG-INFO\n    writing dependency_links to pip-egg-info\\tensorpack.egg-info\\dependency_links.txt\n    writing requirements to pip-egg-info\\tensorpack.egg-info\\requires.txt\n    writing top-level names to pip-egg-info\\tensorpack.egg-info\\top_level.txt\n    writing manifest file 'pip-egg-info\\tensorpack.egg-info\\SOURCES.txt'\n    error: package directory 'find:                             # will call find_packages()' does not exist\nCleaning up...\n  Removing source in C:\\Users\\cynth\\AppData\\Local\\Temp\\pip-req-build-_x5nasqf\nRemoved git+https://github.com/tensorpack/tensorpack.git from build tracker 'C:\\Users\\cynth\\AppData\\Local\\Temp\\pip-req-tracker-i80u1q2z'\nRemoved build tracker 'C:\\Users\\cynth\\AppData\\Local\\Temp\\pip-req-tracker-i80u1q2z'\nCommand \"python setup.py egg_info\" failed with error code 1 in C:\\Users\\cynth\\AppData\\Local\\Temp\\pip-req-build-_x5nasqf\\\nException information:\nTraceback (most recent call last):\n  File \"c:\\users\\cynth\\documents\\mini3\\envs\\tensorpack\\lib\\site-packages\\pip_internal\\basecommand.py\", line 141, in main\n    status = self.run(options, args)\n  File \"c:\\users\\cynth\\documents\\mini3\\envs\\tensorpack\\lib\\site-packages\\pip_internal\\commands\\install.py\", line 299, in run\n    resolver.resolve(requirement_set)\n  File \"c:\\users\\cynth\\documents\\mini3\\envs\\tensorpack\\lib\\site-packages\\pip_internal\\resolve.py\", line 102, in resolve\n    self._resolve_one(requirement_set, req)\n  File \"c:\\users\\cynth\\documents\\mini3\\envs\\tensorpack\\lib\\site-packages\\pip_internal\\resolve.py\", line 256, in _resolve_one\n    abstract_dist = self._get_abstract_dist_for(req_to_install)\n  File \"c:\\users\\cynth\\documents\\mini3\\envs\\tensorpack\\lib\\site-packages\\pip_internal\\resolve.py\", line 209, in _get_abstract_dist_for\n    self.require_hashes\n  File \"c:\\users\\cynth\\documents\\mini3\\envs\\tensorpack\\lib\\site-packages\\pip_internal\\operations\\prepare.py\", line 298, in prepare_linked_requirement\n    abstract_dist.prep_for_dist(finder, self.build_isolation)\n  File \"c:\\users\\cynth\\documents\\mini3\\envs\\tensorpack\\lib\\site-packages\\pip_internal\\operations\\prepare.py\", line 126, in prep_for_dist\n    self.req.run_egg_info()\n  File \"c:\\users\\cynth\\documents\\mini3\\envs\\tensorpack\\lib\\site-packages\\pip_internal\\req\\req_install.py\", line 636, in run_egg_info\n    command_desc='python setup.py egg_info')\n  File \"c:\\users\\cynth\\documents\\mini3\\envs\\tensorpack\\lib\\site-packages\\pip_internal\\utils\\misc.py\", line 701, in call_subprocess\n    % (command_desc, proc.returncode, cwd))\npip._internal.exceptions.InstallationError: Command \"python setup.py egg_info\" failed with error code 1 in C:\\Users\\cynth\\AppData\\Local\\Temp\\pip-req-build-_x5nasqf\\\n(tensorpack) C:\\Users\\cynth\\Documents\\Mini3\\Scripts>. ",
    "kecsap": "Yeah, now I realized this logic. It would be nice to write this down somewhere in the documentation. I did not expect that the it/s changes the meaning from images/sec into batches/sec.. ",
    "hanxiao": "You can convert all DT_FLOAT from a float32 model to DT_HALF, resulting a float16 model. See my https://github.com/hanxiao/bert-as-service/blob/master/server/bert_serving/server/graph.py#L147 for example.. ",
    "thuzhf": "Welcome \u6615\u54e5\uff01. Yes, it seems more concise.. OK, I understand... Do you want me to keep this change or revert this change in order for you future change on this part?. ",
    "zachluo": "@ppwwyyxx It is the same as what I thought. Thanks for your sharing.. ",
    "wangg12": "@ppwwyyxx I mean the function segmentation_to_mask will merge the polygon of the hole, so in the resulted mask, the hole part is filled by 1 but 0 is expected. That\u2019s my problem.. @ppwwyyxx Take a donut as an example, it has two polygons(inner and outer contours found by opencv or skimage). So segmentation_to_mask would fail. Do you mean I need to change the polygons to contain ones only(for example by adding a virtual line between inner and outer contour)? That would be really hacky for complicated objects. Do you have any idea about it?. @ppwwyyxx Thanks. I think it might be easier by directly loading binary masks and changing the augmentation code.. ",
    "s7ev3n": "@ppwwyyxx Thank you so much for the quick reply !! I will check my code, and come back bringing more details. \nA little question: What does print(next(ds.get_data())) printing out numpy array mean ? My understanding is that the dataflow is running correctly.\nI just use FakeData to test, the issue still exists.. 1.RealData, RealLogits --> Stuck\n2.FakeData, RealLogits --> Stuck\n3.RealData, FakeLogits --> Work !\nThe model logits part should have bugs ! I will check it ! \nThanks so much !. Hi, @ppwwyyxx , I debug my model and find that the dataflow stuck in BatchNorm(), in which I use sync_statistics='nccl' with SyncMultiGPUTrainerParameterServer. When I set sync_statistics=None, the training works. \nSyncBN is the most attracting feature for me, what can I do to make it work with SyncBN? Thanks !!. In train.SyncMultiGPUTrainerReplicated(gpus, average=True, mode=None, use_nccl=None) , should I set mode='nccl' or use_nccl=True ? I am a little confused.. I tested SyncMultiGPUTrainerReplicated with a naive model and sync_statistics, it works ! And my model also works ! \nThanks so much, I may close this issue now !. Hi, @ppwwyyxx \nI tried SyncMultiGPUTrainerParameterServer with a naive model and constant layername like you said, the code is showed below. It just stuck (I waited for like 5 minutes) and when changed to SyncMultiGPUTrainerReplicated, the training process is smoothly running. This seems mysterious.\n```\n    def get_fake_logits(self, images):\n        '''\n        Fake logits for testing.\n        '''\n        net = Conv2D('fake_conv', images, filters=19, kernel_size=1, strides=1)\n        net = BatchNorm('fake_bn', net,  sync_statistics=\"nccl\")\n        logits = PReLU('fake_prelu', net)\n    return logits\n\n``\nWhile all variables in my model is under variable scope 'xxnetTensorpack', training now could run without any warning about syncBN usingSyncMultiGPUTrainerReplicated, should I delete this variable scope ? \nOr should I try deleting variable scope andParameterServer? Just tried, did not work.. @ppwwyyxx Sorry for the delay, the cifar example code could run successfully run on my machine. Does it mean that my tensorpack code still has some issues ?\nSince I useSyncMultiGPUTrainerReplicatedtrained my model for a few days, the result is much worse than the model I wrote using slim. I first thought the hyper parameter needs modification, but I have tried a few, still no improvement.. Thanks! I will check deeper.. Hi, @ppwwyyxx I manage to run sync bn withSyncMultiGPUTrainerParameterServer.\nThe issue is probably caused by computing loss part. I used slim's weight decay loss, an auxiliary loss and obtain all losses usingtf.losses.get_total_loss(), while using tensorpack I added regularize loss to loss collection andtf.losses.get_total_loss(). I now usetf.add_n()to add all the losses. I have no idea why this makes sync bn hang.\nBTW, I have two little question about the two SyncMulti Trainers: \n1.DoesSyncMultiGPUTrainerParameterServersynchronize variables per step andSyncMultiGPUTrainerReplicated` per epoch ? \n2.Does the two trainers sum losses from each tower or average losses across each tower ?\nI am new to this cross gpu training. Thanks very much for your answer.. I did not notice that until now...\nI still do not quite understand what happens to losses after reading the docs.\nData-parallel training in \u2018ParameterServer\u2019 mode. It builds one tower on each GPU with shared variable scope. It synchronizes the gradients computed from each tower, averages them and applies to the shared variables. \nIf loss from each tower have shared variable scope, does it mean that it will collect and sum losses from each tower, and then average them ? \nThanks.\n. OK, I get it now. Thanks !\nI was considering to modify learning rate compared to on single GPU.. Hi, @ppwwyyxx \nMay I ask how I save the best validation result model during training?\nI tried infs = [ScalarStats(names='mean_iou', prefix='val')] and InferenceRunner(dataset_val, infs) in callbacks, and also MaxSaver(monitor_stat='val_mean_iou').  While the summary of val_mean_iou does not seem right, its value much larger than mean_iou and has periodic oscillation.\nI guess my method is not right. Thanks !. Hi, @ppwwyyxx \nI would like to do inference with multiple scales inputs and add those model outputs, i.e. logits, to get final prediction. I tried to build multiple models using model.get_logits(image) under TowerContext, and feed the scaled input image which is a tf queue tensor, then restore the model variables using SaverRestore(), and use sess.run(final_pred) to get the final prediction. However, the prediction image is messy like random variables restore. I check the variable names in the saved model, it is the same as building the model, variables are auto-reuse.\nThe saved model has no problem, as I have tried to do inference using OfflinePredictor which is in the tutorial, and it is ok.\nMany thanks!. My apology that did not make the issue clear. I will rephrase it, and perhaps open a new issue. Thanks.. What I did:\nI simply tried to run export-model.py, and the error is ImportError: cannot import name ModelExporter. The ModelExporter just can not be imported. While importing other tensorpack modules has no problem.\nThe link in the tutorial is missing, because 'basic' in tutorial link:\n 'https://github.com/tensorpack/tensorpack/blob/master/examples/basic/export-model.py' should be 'basics'.. The importing issue still exists after pip install --upgrade git+https://github.com/tensorpack/tensorpack.git. \ntensorpack version is 0.8.9. I copied export.py to my model export script folder, and modified importing ,such as from ..utils import logger to from tensorpack.utils import logger. It did not report importing issue. \nHowever, new issue is below:\nTypeError: optimize_for_inference() takes exactly 4 arguments (5 given)\nThe script is below:\nfrom export import ModelExporter\nsess_init = SaverRestore(model_path=restore_ckpt, ignore=ignore_list)\npred_config = PredictConfig(model=model, \n                           session_init=sess_init,\n                           input_names=['images'], \n                           output_names=['fullconv/output'])\nModelExporter(pred_config).export_compact('/compact_graph.pb')\nIt is ok now, I deleted False in:\npruned_graph_def = optimize_for_inference_lib.optimize_for_inference(\n                frozen_graph_def,\n                [n.name[:-2] for n in input_tensors],\n                [n.name[:-2] for n in output_tensors],\n                [dtype.as_datatype_enum for dtype in dtypes],\n                False). ",
    "adoudou2008": "PS:\nmy nccl version:\napt list --installed | grep nccl\nWARNING: apt does not have a stable CLI interface. Use with caution in scripts.\nlibnccl-dev/unknown,now 2.2.13-1+cuda9.0 amd64 [installed,upgradable to: 2.2.13-1+cuda9.2]\nlibnccl2/unknown,now 2.2.13-1+cuda9.0 amd64 [installed,upgradable to: 2.2.13-1+cuda9.2]\n. PS:\nmy nccl version:\napt list --installed | grep nccl\nWARNING: apt does not have a stable CLI interface. Use with caution in scripts.\nlibnccl-dev/unknown,now 2.2.13-1+cuda9.0 amd64 [installed,upgradable to: 2.2.13-1+cuda9.2]\nlibnccl2/unknown,now 2.2.13-1+cuda9.0 amd64 [installed,upgradable to: 2.2.13-1+cuda9.2]\n. I am referring to the source code of tensorpack, wrote some code, may have some errors about NCCL, I hope someone can answer.. sorry\u3002my fault\u3002. thanks you so much!. ",
    "horizonheart": "\n\n\n\"some variables are used in training but not found (i.e. not used) in inference\", and in the models I provided, I have manually removed such variables, so you won't see any warnings.\n   As for why your model does not perform well, it is a machine learning/computer vision question that we'll not be able to answer.\nYou do not need to use that format because tensorpack recognize both. To make the conversion, load and save the variables with https://tensorpack.readthedocs.io/modules/tfutils.html#tensorpack.tfutils.varmanip.save_chkpt_vars\n\n\nOK\uff0cThanks.\nHas your problem been solved? I have encountered the same problem.. > For anyone to better diagnose your issue, please post relevant details following the issue template (click \"New Issue\" -> \"Unexpected Problems / Bugs\", or visit this link to post issues about unexpected problems)\n\ni used four gpu to train and used one gpu to inference!. > Please post relevant details following the issue template in the link above. In particular, I do not understand what you did, what you observed and what you expected.\ni used the tensorpack to train coco dataset and used the default configuration. i can train the model success. But when i predict the model , but the predict image is the same as original image,in other wors, the model don't find any box and segmentation.. > Please post the details following the issue template in the link above, i.e.:\n\n\n\nWhat you did:\n\n\nIf you're using examples:\n\n\nWhat's the command you run:\n\nHave you made any changes to the examples? Paste them if any:\nIf not, tell us what you did that may be relevant.\n  But we may not investigate it if there is no reproducible code.\n\nBetter to paste what you did instead of describing them.\n\n\nWhat you observed, including but not limited to the entire logs.\n\n\nBetter to paste what you observed instead of describing them.\n\n\nWhat you expected, if not obvious.\n\n\nSince you mentioned both training and prediction, I assume that's two separate commands. Therefore please post two group of relevant information (what you did and what you observed) for the two commands, respectively.\n\nok,thanks! i have pasted the question https://github.com/tensorpack/tensorpack/issues/955. > python train.py is not the correct command to train. You need to use a pre-trained weights. Read the README for details.\nbut if  the dataset don't have 80 NUM_CATEGORY,the pre-trained weights is wrong in the last .what should i do.. Do you mean that there  must have a backbone on the imagenet when training?. ./train.py --config \\\n    MODE_MASK=True MODE_FPN=True \\\n    DATA.BASEDIR=/path/to/COCO/DIR \\\n    BACKBONE.WEIGHTS=/path/to/ImageNet-R50-Pad.npz \\\nthis is your example .But i have change the config in the config.py other than the backnoe.weights. Why can't I run python train.py directly?. so, your mean is that i should set up the backbone anyway? thanks very much for your patient!. i am going to use ImageNet-R101-AlignPadding.npz this pre-trained model and submit task to try again. thank you very much.. > For anyone to better diagnose your issue, please post relevant details following the issue template (click \"New Issue\" -> \"Unexpected Problems / Bugs\", or visit this link to post issues about unexpected problems)\n\nI assume you're running one of the standard settings in the README. When evaluated with only 1 GPU, the evaluation time you saw is normal.\n\ni used four gpu. ",
    "ywpkwon": "Wow, thanks for the quick answer! I got it.\nSo, for those who may be wondering, the two options below worked very well.\noption 1\ndef _trigger_epoch(self):\n        tensor = tf.get_default_graph().get_tensor_by_name(\"tower0/in_image:0\")\n        val = self.trainer.hooked_sess.run(tensor)\n        print(val)\noption 2\n```\n    def _need_run(self):\n        if self.local_step == self.trainer.steps_per_epoch - 1:\n            return True\n        return False\ndef _before_run(self, run_context):\n    if self._need_run():\n        tensor = tf.get_default_graph().get_tensor_by_name(\"tower0/in_image:0\")\n        return tf.train.SessionRunArgs(tensor)\n    return None\n\ndef _after_run(self, run_context, run_values):\n    values = run_values.results\n    if values is None: return\n    print(values)\n\n``\nAlthough I was not that critical in increasing oneglobal_step`, I chose the second option. \n. I see. Thanks for the advice!\nI'll leave a few random related links:\nhttps://stackoverflow.com/questions/42394585/how-to-inspect-a-tensorflow-tfrecord-file\nhttps://www.kaggle.com/mpekalski/reading-tfrecord. I experienced a similar issue. I guess https://github.com/tensorpack/tensorpack/issues/945 might be related..?. Oh, I figured out my mistake. \nThe __iter__ function of DataFlow should be self-refreshable. I.e, initialization part should also be in the __iter__ function when using python generator.\nInstead of \nclass MyDataFlow(RNGDataFlow): \n    def __init__(self):\n        ... initialize self.generator here ...\n    def __iter__(self):\n        for _ in range(len(self.num_samples))\n            example = next(self.generator)\n            ....\n            yield example\nit should be\nclass MyDataFlow(RNGDataFlow): \n    def __init__(self):\n        ...\n    def __iter__(self):\n        ... initialize self.generator here ...\n        for _ in range(len(self.num_samples))\n            example = next(self.generator)\n            ....\n            yield example\nThis is because the queueing code uses something like ds = self.df.__iter__() in order for refreshing.\n\nBtw, sorry for duplicated same issue postings. Last night, any of the postings didn't appear, so I tried several times thinking as network issue.. One more!  Please see tensorpack/scripts/checkpoint-manipulate.py. There could be a bug?\n\nIn line 7 & 19--24,\nfrom tensorpack.tfutils.varmanip import dump_chkpt_vars\n...\n    if args.model.endswith('.npy'):\n        params = np.load(args.model, encoding='latin1').item()\n    elif args.model.endswith('.npz'):\n        params = dict(np.load(args.model))\n    else:\n        params = dump_chkpt_vars(args.model)    # <---- weird?\n1. There is no dump_chkpt_vars in tensorpack.tfutils.varmanip.\n2. This parts are for loading, so dump_chkpt_vars should be load_chkpt_vars, maybe?. Thanks!! \nPlus, np.save(args.dump, **params) --> np.savez(args.dump, **params).. Hi,  I had two almost-the-same machines, but only one has the BufferError: memoryview: underlying buffer is not C-contiguous bug. Interestingly, both machines has \nnumpy==1.14.3\nmsgpack==0.5.6\nmsgpack-numpy==0.4.4.1\ntensorflow-gpu==1.8.0\npyzmq==17.0.0\nCould you give me some idea why this may happen? I tried to minimize the code:\n```\nfrom tensorpack.utils import logger\nfrom tensorpack.utils.timer import timed_operation\nfrom tensorpack.utils.argtools import log_once\nfrom tensorpack import *\nfrom tensorpack.dataflow import dataset\nimport cv2\nds = FakeData([[320, 1024, 3], [320, 1024]], random=False,\n              dtype=['uint8', 'uint8'], domain=[(0, 255), (0, 81)])\naug_geo = [imgaug.ResizeShortestEdge(350, interp=cv2.INTER_CUBIC),\n           imgaug.RandomCrop(320),                          # <-- This line has the issue.\n           imgaug.Flip(horiz=True)]\nds = AugmentImageComponents(ds, aug_geo, (0, 1), copy=False)\nds = PrefetchDataZMQ(ds, nr_proc=5)  \nds = BatchData(ds, 1)\nTestDataSpeed(ds).start()\n**Very interestingly, if I remove `RandomCrop`, everything works fine. `RandomCrop` + `PrefetchDataZMQ` brings the issue.** Errors look like:\nProcess _Worker-1:5:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n    self.run()\n  File \"/home/paul/projects/tensorpack/tensorpack/dataflow/parallel.py\", line 290, in run\n    socket.send(dumps(dp), copy=False)\n  File \"/home/paul/projects/tensorpack/tensorpack/utils/serialize.py\", line 19, in dumps_msgpack\n    return msgpack.dumps(obj, use_bin_type=True)\n  File \"/home/paul/.virtualenvs/tf/lib/python3.5/site-packages/msgpack_numpy.py\", line 196, in packb\n    return Packer(**kwargs).pack(o)\n  File \"msgpack/_packer.pyx\", line 284, in msgpack._packer.Packer.pack\n  File \"msgpack/_packer.pyx\", line 290, in msgpack._packer.Packer.pack\n  File \"msgpack/_packer.pyx\", line 287, in msgpack._packer.Packer.pack\n  File \"msgpack/_packer.pyx\", line 263, in msgpack._packer.Packer._pack\n  File \"msgpack/_packer.pyx\", line 234, in msgpack._packer.Packer._pack\n  File \"msgpack/_packer.pyx\", line 266, in msgpack._packer.Packer._pack\nBufferError: memoryview: underlying buffer is not C-contiguous\n  0%|                                                                                                                                               |0/5000[00:00<?,?it/s]  0%|1                                                                                                                                      |5/5000[00:02<36:13, 2.30it/s]\nTraceback (most recent call last):\n  File \"why.py\", line 27, in \n    TestDataSpeed(ds).start()\n  File \"/home/paul/projects/tensorpack/tensorpack/dataflow/common.py\", line 61, in start\n    for idx, dp in enumerate(itr):\n  File \"/home/paul/projects/tensorpack/tensorpack/dataflow/parallel.py\", line 331, in iter\n    yield self._recv()\n  File \"/home/paul/projects/tensorpack/tensorpack/dataflow/parallel.py\", line 321, in _recv\n    return loads(self.socket.recv(copy=False))\n  File \"zmq/backend/cython/socket.pyx\", line 790, in zmq.backend.cython.socket.Socket.recv\n  File \"zmq/backend/cython/socket.pyx\", line 828, in zmq.backend.cython.socket.Socket.recv\n  File \"zmq/backend/cython/socket.pyx\", line 172, in zmq.backend.cython.socket._recv_frame\n  File \"zmq/backend/cython/checkrc.pxd\", line 12, in zmq.backend.cython.checkrc._check_rc\nKeyboardInterrupt\nPrefetchDataZMQ successfully cleaned-up.\n```\nPlus, how to downgrade msgpack-numpy if I have to downgrade for one machine? pip install msgpack-numpy==0.4.3.1? What about msgpack? Would you recommend downgrade together?\n. Thanks a lot.  \nActually, I editted my question (sorry, I didn't know if your response would be this quick..). Now the problem doesn't have the nested PrefetchDataZMQs (btw, good to know, thanks.). It seemed to relate to RandomCrop.\nHowever, I tried to make a new clean virtualenv and pip-installed packages from the working machine's pip status, and now it works! As you said, although pip looks the same, some packages seemed to have problems.  Thank you!. For those who have the similar issue, the simplest and final solution for me was\npip uninstall msgpack-numpy\npip uninstall msgpack-numpy   ( to make sure.. )\npip install msgpack-numpy\nIt gave me msgpack-numpy-0.4.4.2 and it works.. Thanks.  I tried to add ProgressBar\nreturn AutoResumeTrainConfig(\n        dataflow=dataset_train,\n        callbacks=[\n            ModelSaver(keep_checkpoint_every_n_hours=5),\n            GPUUtilizationTracker(),\n            EstimatedTimeLeft(),\n            ProgressBar(names=['cost']),\n        ],\n        model=Model(num_classes=NUM_CLASSES),\n        steps_per_epoch=steps_per_epoch,\n        max_epoch=500,\n    )\nNow, it shows two progress bars, maybe one is the default one.\n88%|###########################6           |88/100[00:32<00:04, 2.44it/s], cost=0.156\n 88%|#######################################         |88/100[00:32<00:04, 2.44it/s]\nHow can I make the default one to print out cost?\n. Wow, everything is already there. Sorry, I thought I went through quite much, but still, there are more! Thank you so much!. Thanks for your advice!\nPlease let me make sure three things in your advice:\n(1) the placeholders and all tensors will have mypredict/ prefix in their names (e.g., mypredict/image:0), right?\n(2) You mentioned tf.get_tensor_by_name, but more exactly, shouldn't I use self.get_tensors_maybe_in_tower? (I got an error that no get_tensor_by_name in tf.)\n(3) Does \"use session.run to evaluate them in your callback.\" mean I can use self.trainer.hooked_sess.run in my callback? Or is there another session?\n. Related to (3), if it is sess not hooked_sess, does it mean self.trainer.sess, or a new sess like sess = tf.Session() in the callback class? Or is there another way to access an existing usable sess?. ",
    "armandmcqueen": "Yup, pyarrow was the issue and pip uninstall pyarrow resolved it. Thank you for the quick response!. Okay, that's helpful, thank you. And am I correct that: \n- The rpn_head outputs box_logits which are in the coordinate system of the given FPN layer feature map. \n- Then anchor.decode_logits(box_logits) uses the anchors for that layer to convert from box_logits to the coordinate system of the input image. \n- This works because each FPN layer is associated with a given anchor size so as you move up in the FPN, the feature space is smaller, but the anchor boxes are larger. \n- The rpn_head isn't aware of which FPN layer the input comes from, but because the anchor boxes are larger in higher layers, the same (x1, y1, h1, w1) output of rpn_head will lead to a larger proposal when output on FPN_LVL_4 than when output on FPN_LVL_3.. Ah, I was misunderstanding how box_logits were parameterized. That's much clearer now - thank you for the help!. ",
    "hesz94": "I don't currently have access to the workstation, I'll update the issue with logs on monday. \nMeanwhile, could you tell me how to disable stagingarea? When I didn't specify staginginput tensorpack used a default one instead.. Forgot to post the update, sorry about that!\nTurns out there was a bug in the code for building the model for regular tf trainer (missed a line) so that's why it could fit much larger training batches. Turns out that in the end it's just the fact that the model is massive, and nothing was wrong on the side of tensorpack.\nThanks for the tips and sorry for the trouble!. ",
    "wangzilu": "from tensorpack.predict.feedfree import FeedfreePredictor\nI use this line instead.. @ppwwyyxx so, which version should I use?\nand how about if I use from tensorpack.predict.feedfree import FeedfreePredictor instead?. @I tried install using pip install -U git+https://github.com/ppwwyyxx/tensorpack.git. However, it is still the version 0.8.9 and sitll can not import name 'FeedfreePredict', please check it.. ",
    "brunoalano": "If I remove the None from the input_a input, it passes, but then I got an issue from my model, because it was expecting ndim=3 and received ndim=2.\nValueError: Input 0 is incompatible with layer lstm_1: expected ndim=3, found ndim=2. My mistake: On my DataFlow, I was generating data as [[input_a, input_b], y], but changing that to [input_a, input_b, y] solved the case.. ",
    "snownus": "@ppwwyyxx , I mean the results for DoReFa-Net. . ",
    "ideaRunner": "dataset_test.__len__() returns the number of batches.\n[1027 13:13:51 @format.py:92] Found 1213 entries in D:\\infor\\dataset\\lmdb\\val.lmdb\n38\nmy batch size is 32,  1213 entries in my lmdb file,  so 1213/32=37.90625\nI use reminder=True, so I will got 38 batches. Is that correct?\nBut when I run the  for loop you wrote above, both my codes and your mnist example codes got error. Those errors are different.\nFor my codes, it returns an error:\n```\nTraceback (most recent call last):\n  File \"D:\\PyCharm 2018.2.2\\helpers\\pydev\\pydevd.py\", line 1664, in \n    main()\n  File \"D:\\PyCharm 2018.2.2\\helpers\\pydev\\pydevd.py\", line 1658, in main\n    globals = debugger.run(setup['file'], None, None, is_module)\n  File \"D:\\PyCharm 2018.2.2\\helpers\\pydev\\pydevd.py\", line 1068, in run\n    pydev_imports.execfile(file, globals, locals)  # execute the script\n  File \"D:\\PyCharm 2018.2.2\\helpers\\pydev_pydev_imps_pydev_execfile.py\", line 18, in execfile\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\n  File \"D:/infor/train_with_tensorpack.py\", line 118, in \n    for cnt, data in enumerate(dataset_test):\n  File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorpack\\dataflow\\common.py\", line 116, in iter\n    for data in self.ds:\n  File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorpack\\dataflow\\common.py\", line 274, in iter\n    for dp in self.ds:\n  File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorpack\\dataflow\\common.py\", line 274, in iter\n    for dp in self.ds:\n  File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorpack\\dataflow\\common.py\", line 274, in iter\n    for dp in self.ds:\n  File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorpack\\dataflow\\common.py\", line 589, in iter\n    self._add_data()\n  File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorpack\\dataflow\\common.py\", line 581, in _add_data\n    dp = next(self.ds_itr)\nAttributeError: 'LocallyShuffleData' object has no attribute 'ds_itr'\n```\nFor your example, I test it, return a different error:\nAttributeError: 'Mnist' object has no attribute 'rng'. This it the code that how I generate lmdb file.\nI read raw jpeg data so that the lmdb file will not be too much larger than origin images' size.\nI designed a class, designed __len__ and __iter__, it looks no problem.\n```python\ndef generate_multi_label_lmdb(save_lmdb_path, image_to_labels):\n    \"\"\"\n    Generate lmdb file contain raw jpg format images, and a numpy array of multi-labels.\n    :param save_lmdb_path: path to save lmdb file\n    :param image_to_labels: a dict, key is image path, value is string labels, eg. \"1 0 1 0 0 1\"\n    :return:\n    \"\"\"\nfrom tensorpack.dataflow import PrefetchDataZMQ, LMDBSerializer, DataFlow\n\nclass ImageLabel(DataFlow):\n    def __init__(self, dict_image_labels):\n        self.dict = dict_image_labels\n\n    def __len__(self):\n        return len(self.dict)\n\n    def __iter__(self):\n        for img_path in self.dict:\n            # img = np.array(Image.open(img_path))\n            image_raw = open(img_path).read()\n            labels = str(image_to_labels[img_path])\n            labels_list = labels.split(\" \")\n            labels_list = [float(i) for i in labels_list]\n            labels_array = np.array(labels_list)\n            yield [image_raw, labels_array]\n\nds0 = ImageLabel(image_to_labels)\nds1 = PrefetchDataZMQ(ds0, nr_proc=1)\nLMDBSerializer.save(ds1, save_lmdb_path)\n\n```\nThe codes below is for decoding lmdb file.\npython\ndef decode_multi_label_lmdb(load_lmdb_path, batchsize=48,  remainder=False):\n    \"\"\"\n    Decode lmdb file and returan a BatchData\n    :param load_lmdb_path: lmdb file path\n    :param batchsize: batchsize of BatchData\n    :param remainder: according BatchData reminder\n    :return: BatchData [images_array, labels_array]\n    \"\"\"\n    import cv2\n    from tensorpack.dataflow import BatchData, LMDBSerializer, MapDataComponent, LocallyShuffleData, AugmentImageComponent\n    ds = LMDBSerializer.load(load_lmdb_path, shuffle=False)\n    ds = LocallyShuffleData(ds, 50000)\n    ds = MapDataComponent(ds, lambda x: np.asarray(bytearray(x), dtype=\"uint8\"), 0)\n    ds = MapDataComponent(ds, lambda x: cv2.imdecode(x, cv2.IMREAD_COLOR), 0)\n    ds = MapDataComponent(ds, lambda x: x/float(255), 0)\n    # ds = AugmentImageComponent(ds, lots_of_augmentors)\n    ds = BatchData(ds, batchsize, remainder=remainder)\n    return ds. Hi, \nIt is indeed the problem of LocallyShuffleData. Now it works. Thank you.\nI followed this document Sequential Read. It would be better if the Error mention about it or there has a warning.\nemmm, there is another GPU utilization question refer to Sequential Read. I am not sure if I need to raise another issue. Could I write it here?\nWhen I training with the lmdb decode codes I written before(commented out LocallyShuffleData, fixed problem), The GPU utilization is almost 30% all training epoch time and about 10% validation epoch.\nIt seems that ds = MapDataComponent(ds, lambda x: cv2.imdecode(x, cv2.IMREAD_COLOR), 0) cost too much CPU time.\nAt before, I use tf.image.decode_jpeg() function to decode raw jpeg data in tfrecord file, it looks well and my tensorflow codes can achieve almost 90% utilization.\nBut I tried here, seems that lambda works not well with it.\nAs I am not well understood tensorpack, is there any example that could use gpu to decode jpeg raw data? Otherwise, how can I achieve better GPU utilization mentioned in this document?. Hi\uff0c \nThe bottleneck is the data.\nQueueInput/queue_size: 2.0649e-38\nI know that Efficitent DataFlow/Sequential Read should solve that problem, as the codes I wrote above, but it seems didn't do what I thought.\nI tried to write the whole numpy array in lmdb generating part to reduce the time of lmdb decoding cost. Besides I got a large lmdb file, however the queus_size is similar (near to zero), and GPU utilization is still low. training 30% val 10%. \nDo you have any suggestion?. ",
    "tongwu92623": "Is there any way I can print the gradient in tensorpack? My purpose for\nthis question is to see how gradient change in gradient quzntization.\nYuxin Wu notifications@github.com\u4e8e2018\u5e7410\u670827\u65e5 \u5468\u516d21:28\u5199\u9053\uff1a\n\nIf you want to use session.run, don't use tensorpack trainer.\nYou can copy the graph building code to somewhere else and use sessions\nyourself.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/tensorpack/tensorpack/issues/951#issuecomment-433668732,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/Ap7TUvbkTRCBrJHXf_pSHsytaW2h-xv4ks5upQg9gaJpZM4X9rCH\n.\n. Thank you for your reply. I think I can get the gradient value form tf.Print() method. I got another question, where should I use tf.Print() method? In the dorefa.py, under the gradient quantization method? Otherwise, I need to use this inside the tensorpack trainer? Sorry for some trivial questions, I am just new for tensorflow and tensorpack.. Thanks man! The tf.Print method works! Thanks for your help.. Got it, thx. . \n",
    "zvikapeter": "Thanks. Downgrading TensorFlow to 1.10 solved the issue.. ",
    "yasserkhalil93": "I just downloaded the master-file from github. But still getting the same error. \nAfter downloading the package, I run the setup file through \"./setup.py build\" and then I go to \"examples/faster-rcnn\" directory and execute  the following:\n./train.py --config MODE_MASK=False MODE_FPN=False DATA.BASEDIR=/media/ext-drive/datasets/coco/coco BACKBONE.WEIGHTS=/media/ext-drive/faster_rcnn/faster_rcnn_new/tensorpack-master/examples/FasterRCNN/models/ImageNet-R50-AlignPadding.npz\nAny advice?. Thank you very much. Using pip install -U git+https://github.com/ppwwyyxx/tensorpack.git solved the issue.\nHowever, now I am facing another error.\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[512,1024,14,14]\nEventhough I pulled down the _C.RPN.TEST_POST_NMS_TOPK parameter from 1000 to 100 but still I am getting the same error.\nThe GPU I am using is Geforce GTX1080 with 8GB.\n. I reduced the parameter FRCNN.BATCH_PER_IM to 128 but still is giving me the same error.\nI want to train COCO on FasterRCNN. Hence, I need a model pretrained on Imagenet. \nSo, you are suggesting it is impossible to train such case on my GPU.\nRegards.. What do you mean by FPN? Setting the parameter to true in the following command:\n./train.py --config MODE_MASK=False MODE_FPN=False DATA.BASEDIR=/media/ext-drive/datasets/coco/coco BACKBONE.WEIGHTS=/media/ext-drive/faster_rcnn/faster_rcnn_new/tensorpack-master/examples/FasterRCNN/models/ImageNet-R50-AlignPadding.npz\nThanks. ",
    "Sunasity": "I am sorry for the wrong editing of the code above. The code for reading data is shown as follows:\nlmdb_data = os.path.join(datadir, 'ILSVRC-%s.lmdb'%name)\n    ds = LMDBData(lmdb_data, shuffle=False)\n    if isTrain:\n        ds = LocallyShuffleData(ds, 50000)\n    ds = PrefetchData(ds, 5000, 1)\n    ds = LMDBDataPoint(ds)\n    ds = MapDataComponent(ds, lambda x: cv2.imdecode(x, cv2.IMREAD_COLOR), 0)\n    ds = AugmentImageComponent(ds, augmentors, copy=False)\n    if parallel < 16:\n        logger.warn(\"DataFlow may become the bottleneck when too few processes are used.\")\n    ds = BatchData(ds, batch_size, remainder=False)\n    ds = PrefetchDataZMQ(ds, parallel)\n    ds.reset_state()\nThe val accuracy:\n\n. > The original models were trained without modifications to the code and on 8 GPUs. Since you've made changes to the code, getting different results is not surprising.\nHi, I conduct the experiments with the original code on 8 GPUs. The top-1 accuracy is still 67.5% (3% lower than reported). . > Could you provide your environment info (especially TF version and cudnn version)?\nMy TF version is 1.10.0 and the cudnn version is 9.0.. ",
    "jperl": "Thank you for the quick response and for pointing me to the dist-strategy rfc.\nSince the dataflow is already independent, it seems callbacks are the place to focus. Perhaps they could be framework agnostic as well -- with implementations on top of Keras callback and Pytorch ignite handlers.\nAfter I finish the project I am working on using Tensorpack, I can try to adapt the training loop / hooks I use to Keras callback and see if there are any gaps.. ",
    "sytham": "Thanks, I do indeed see the \"successfully cleaned-up\" message in your example.\nHowever, expanding your example to the minimal one that represents my use case, I don't see it:\n```python\nimport gc\nfrom tensorpack import *\nfrom tensorpack.contrib.keras import KerasModel\nimport tensorflow as tf\nfrom tensorflow import keras\nKL = keras.layers\ndef create_dataflow():\n    x = FakeData([[3], [1]], 1000)\n    x = BatchData(x, 3)\n    x = PrefetchDataZMQ(x, 3)\n    return x\ndef model_func(x):\n    M = keras.models.Sequential()\n    M.add(KL.InputLayer(input_tensor=x))\n    M.add(KL.Dense(1))\n    return M\nx = create_dataflow()\nQ = QueueInput(x)\nM = KerasModel(\n        model_func,\n        inputs_desc=[InputDesc(tf.float32, [None, 3], 'inputs')],\n        targets_desc=[InputDesc(tf.float32, [None, 1], 'labels')],\n        input=Q)\nM.compile(optimizer=tf.train.AdamOptimizer(), loss='binary_crossentropy')\nM.fit(steps_per_epoch=len(x), max_epoch=2)\ndel M\ndel Q\ndel x\ngc.collect()\nimport time; time.sleep(10)\n```\nI'm not sure what else I should delete here?. I think I've found the problem. I'm running everything from a Jupyter notebook. When I run the above minimal example from script, I do see the \"successfully cleaned-up\" message.\nI understand this has now basically ceased to be your problem :) but do you have any suggestions for alternative solutions that don't involve not using a notebook? Could there be another way of shutting down the processes without using del? I can move large parts of code to library/script, but our users mostly use notebooks to set up training runs, especially for collecting the right dataset and configuring the model.. Ah, I see. You're saying if it appears after sleep it's just because the whole kernel is shutting down.\nIt appears after sleep.. Thanks for the quick fix! Works for me now. I wrote a PR, but get remote: Permission to tensorpack/tensorpack.git denied to sytham.\nThe fix is (tested, works for me)\nif validation_data is not None:\n            callbacks.insert(0, InferenceRunner(\n                validation_data, ScalarStats(self._stats_to_inference)))\n. ",
    "TimonLiu": "@ppwwyyxx \nThanks for your response~\nMy preprocessing and postprocessing codes between eval_callback and offlinepredictor are almost the same. The only difference is the batchnorm param is_training: \nIn the training scripts:\npython\n    model_config = model_factory.get_default_config(args.model_name)\n    model_config.update({\n        'lr': args.lr,\n        'weight_decay': args.weight_decay,\n        'backbone_name': 'mobilefacenet',\n        'backbone_config':{'weight_decay': args.weight_decay, 'is_training': True},\n        'image_shape': args.img_size,\n        'train_stage': args.train_stage,\n        'num_classes': args.num_classes,\n        })\n    model = model_factory.get_model(args.model_name, model_config)\nWhile in the offlinepredictor scripts:\n```python\n    model_config = model_factory.get_default_config('arcface_model')\n    model_config.update({\n            'lr': 0.001,\n            'weight_decay': 0.00004,\n            'backbone_name': args.net_name,\n            'backbone_config':{'weight_decay': 0.00004, 'is_training': False},\n            'image_shape': img_size,\n            'train_stage': 'softmax',\n            'num_classes': 50841,\n        })\n    model = model_factory.get_model('arcface_model', model_config)\ntest_dataset, num_samples = FaceRecognDataflow.get_dataflow(args.data_dir, img_size, batch_size, is_training=False)\n\ntest_predictor = SimpleDatasetPredictor(config=PredictConfig(\n    model=model,\n    session_init=get_model_loader(os.path.join(args.model_dir, args.model_prefix)),\n    input_names=['input'],\n    output_names=['embedding']\n    ),\n    dataset=test_dataset)\n\nAnd I am using `tf.contrib.layers.*` functions to build the network like this way:python\n    self.batch_norm_param = {\n                'is_training': is_training,\n                'scale':True,\n                'center':True,\n                'epsilon':0.001,\n                'decay':0.995,\n                'updates_collections':None,\n                'variables_collections': [tf.GraphKeys.TRAINABLE_VARIABLES],\n                }\n    net = tf.contrib.layers.conv2d(\n                inp,\n                num_outputs=64,\n                kernel_size=3,\n                stride=2,\n                biases_initializer=None,\n                activation_fn=functools.partial(self.activation_fn, name='{}_{}'.format(self.net_name, layer_count)),\n                normalizer_fn=tf.contrib.layers.batch_norm,\n                normalizer_params=self.batch_norm_param,\n                weights_regularizer=self.regularizer, \n                weights_initializer=self.initializer,\n                scope=layer_name \n                )\n    net = ...\n``\nAnd I strill get different results betweeneval_callbackandofflinepredictor. @ppwwyyxx \nIt worked! Thank you very much ~\nI have made a mistake on thisis_trainingparam. After modified it toget_current_tower_context().is_traininginbuild_graph`, everything's back to normal.. ",
    "notecola": "Thanks for your fast response!\nI am on a master branch actually:\nroot@tower:~/a/tensorpack# git checkout master\nAlready on 'master'\nYour branch is up-to-date with 'origin/master'.\nwill try older examples now. Exactly this, my fault!. Works fine with msgpack-numpy 0.4.3.1. Thanks!. Hi @ppwwyyxx thanks for your response. But what about the Model(...) constructed every time for each image with the image original resolution? Is there any way to have it initialized once?. This would work if all input images have the same size. In other words, LR_SIZE_H and LR_SIZE_W are different for each input image.. ",
    "mohendra": "Thanks! Wu, that means you are using standard convolution method instead of\nbitwise operation?\n. Again if i want to change the dataset in svhn-digit.py code to MNIST and i change the code as following \n`#!/usr/bin/env python\n-- coding: utf-8 --\nFile: svhn-digit-dorefa.py\nAuthor: Yuxin Wu\nimport os\nimport argparse\nimport tensorflow as tf\nfrom tensorpack import *\nfrom tensorpack.tfutils.summary import add_moving_summary, add_param_summary\nfrom tensorpack.dataflow import dataset\nfrom tensorpack.tfutils.varreplace import remap_variables\nfrom dorefa import get_dorefa\n\"\"\"\nThis is a tensorpack script for the SVHN results in paper:\nDoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients\nhttp://arxiv.org/abs/1606.06160\nThe original experiements are performed on a proprietary framework.\nThis is our attempt to reproduce it on tensorpack.\nAccuracy:\n    With (W,A,G)=(1,1,4), can reach 3.1~3.2% error after 150 epochs.\n    With (W,A,G)=(1,2,4), error is 3.0~3.1%.\n    With (W,A,G)=(32,32,32), error is about 2.3%.\nSpeed:\n    With quantization, 60 batch/s on 1 1080Ti. (4721 batch / epoch)\nTo Run:\n    ./svhn-digit-dorefa.py --dorefa 1,2,4\n\"\"\"\nBITW = 1\nBITA = 2\nBITG = 4\nclass Model(ModelDesc):\n    def inputs(self):\n        return [tf.placeholder(tf.float32, [None, 40, 40], 'input'),\n                tf.placeholder(tf.int32, [None], 'label')]\ndef build_graph(self, image, label):\n    is_training = get_current_tower_context().is_training\n\n    fw, fa, fg = get_dorefa(BITW, BITA, BITG)\n\n    # monkey-patch tf.get_variable to apply fw\n    def binarize_weight(v):\n        name = v.op.name\n        # don't binarize first and last layer\n        if not name.endswith('W') or 'conv0' in name or 'fc' in name:\n            return v\n        else:\n            logger.info(\"Binarizing weight {}\".format(v.op.name))\n            return fw(v)\n\n    def nonlin(x):\n        if BITA == 32:\n            return tf.nn.relu(x)\n        return tf.clip_by_value(x, 0.0, 1.0)\n\n    def activate(x):\n        return fa(nonlin(x))\n\n    image = image / 256.0\n\n    with remap_variables(binarize_weight), \\\n            argscope(BatchNorm, momentum=0.9, epsilon=1e-4), \\\n            argscope(Conv2D, use_bias=False):\n        logits = (LinearWrap(image)\n                  .Conv2D('conv0', 48, 5, padding='VALID', use_bias=True)\n                  .MaxPooling('pool0', 2, padding='SAME')\n                  .apply(activate)\n                  # 18\n                  .Conv2D('conv1', 64, 3, padding='SAME')\n                  .apply(fg)\n                  .BatchNorm('bn1').apply(activate)\n\n                  .Conv2D('conv2', 64, 3, padding='SAME')\n                  .apply(fg)\n                  .BatchNorm('bn2')\n                  .MaxPooling('pool1', 2, padding='SAME')\n                  .apply(activate)\n                  # 9\n                  .Conv2D('conv3', 128, 3, padding='VALID')\n                  .apply(fg)\n                  .BatchNorm('bn3').apply(activate)\n                  # 7\n\n                  .Conv2D('conv4', 128, 3, padding='SAME')\n                  .apply(fg)\n                  .BatchNorm('bn4').apply(activate)\n\n                  .Conv2D('conv5', 128, 3, padding='VALID')\n                  .apply(fg)\n                  .BatchNorm('bn5').apply(activate)\n                  # 5\n                  .tf.nn.dropout(0.5 if is_training else 1.0)\n                  .Conv2D('conv6', 512, 5, padding='VALID')\n                  .apply(fg).BatchNorm('bn6')\n                  .apply(nonlin)\n                  .FullyConnected('fc1', 10)())\n    tf.nn.softmax(logits, name='output')\n\n    # compute the number of failed samples\n    wrong = tf.cast(tf.logical_not(tf.nn.in_top_k(logits, label, 1)), tf.float32, name='wrong_tensor')\n    # monitor training error\n    add_moving_summary(tf.reduce_mean(wrong, name='train_error'))\n\n    cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n    cost = tf.reduce_mean(cost, name='cross_entropy_loss')\n    # weight decay on all W of fc layers\n    wd_cost = regularize_cost('fc.*/W', l2_regularizer(1e-7))\n\n    add_param_summary(('.*/W', ['histogram', 'rms']))\n    total_cost = tf.add_n([cost, wd_cost], name='cost')\n    add_moving_summary(cost, wd_cost, total_cost)\n    return total_cost\n\ndef optimizer(self):\n    lr = tf.train.exponential_decay(\n        learning_rate=1e-3,\n        global_step=get_global_step_var(),\n        decay_steps=4721 * 100,\n        decay_rate=0.5, staircase=True, name='learning_rate')\n    tf.summary.scalar('lr', lr)\n    return tf.train.AdamOptimizer(lr, epsilon=1e-5)\n\ndef get_config():\nlogger.set_logger_dir(os.path.join('train_log', 'svhn-dorefa-{}'.format(args.dorefa)))\ndef get_config():\n    logger.set_logger_dir(os.path.join('train_log', 'svhn-MNIST-{}'.format(args.mnist)))\n# prepare dataset\n# d1 = dataset.SVHNDigit('train')\n# d2 = dataset.SVHNDigit('extra')\n# data_train = RandomMixData([d1, d2])\n# data_test = dataset.SVHNDigit('test')\nd1 = dataset.Mnist('train')\n#d2 = dataset.SVHNDigit('extra')\ndata_train = d1 #RandomMixData([d1, d2])\ndata_test = dataset.Mnist('test')\n\naugmentors = [\n    imgaug.Resize((40, 40)),\n    imgaug.Brightness(30),\n    imgaug.Contrast((0.5, 1.5)),\n]\ndata_train = AugmentImageComponent(data_train, augmentors)\ndata_train = BatchData(data_train, 128)\ndata_train = PrefetchDataZMQ(data_train, 5)\n\naugmentors = [imgaug.Resize((40, 40))]\ndata_test = AugmentImageComponent(data_test, augmentors)\ndata_test = BatchData(data_test, 128, remainder=True)\n\nreturn TrainConfig(\n    data=QueueInput(data_train),\n    callbacks=[\n        ModelSaver(),\n        InferenceRunner(data_test,\n                        [ScalarStats('cost'), ClassificationError('wrong_tensor')])\n    ],\n    model=Model(),\n    max_epoch=20,\n)\n\nif name == 'main':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--dorefa',\n                        help='number of bits for W,A,G, separated by comma. Defaults to \\'1,2,4\\'',\n                        default='1,2,4')\n    args = parser.parse_args()\nBITW, BITA, BITG = map(int, args.dorefa.split(','))\nconfig = get_config()\nlaunch_train_with_config(config, SimpleTrainer())\n\n`\nthe Error is like this\n[1115 02:14:47 @logger.py:108] WRN Log directory train_log/svhn-dorefa-1,2,4 exists! Use 'd' to delete it. \n[1115 02:14:47 @logger.py:111] WRN If you're resuming from a previous run, you can choose to keep it.\nPress any other key to exit. \nSelect Action: k (keep) / d (delete) / q (quit):d\n[1115 02:14:58 @logger.py:73] Argv: gdrive/My Drive/DoReFa-Net/svhn-digit-dorefa.py --dorefa 1,2,4\n[1115 02:14:58 @fs.py:100] WRN Env var $TENSORPACK_DATASET not set, using /root/tensorpack_data for datasets.\n[1115 02:14:58 @parallel.py:293] [PrefetchDataZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.\n[1115 02:14:58 @input_source.py:220] Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...\n[1115 02:14:59 @trainers.py:52] Building graph for a single training tower ...\n[1115 02:14:59 @registry.py:121] conv0 input: [None, 40, 40]\nTraceback (most recent call last):\n  File \"gdrive/My Drive/DoReFa-Net/svhn-digit-dorefa.py\", line 182, in <module>\n    launch_train_with_config(config, SimpleTrainer())\n  File \"/usr/local/lib/python3.6/dist-packages/tensorpack/train/interface.py\", line 87, in launch_train_with_config\n    model._build_graph_get_cost, model.get_optimizer)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorpack/utils/argtools.py\", line 176, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorpack/train/tower.py\", line 204, in setup_graph\n    train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorpack/train/trainers.py\", line 54, in _setup_graph\n    grads = self._make_get_grad_fn(input, get_cost_fn, get_opt_fn)()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorpack/train/tower.py\", line 232, in get_grad_fn\n    cost = get_cost_fn(*input.get_input_tensors())\n  File \"/usr/local/lib/python3.6/dist-packages/tensorpack/tfutils/tower.py\", line 284, in __call__\n    output = self._tower_fn(*args)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorpack/graph_builder/model_desc.py\", line 246, in _build_graph_get_cost\n    ret = self.build_graph(*inputs)\n  File \"gdrive/My Drive/DoReFa-Net/svhn-digit-dorefa.py\", line 76, in build_graph\n    .Conv2D('conv0', 48, 5, padding='VALID', use_bias=True)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorpack/models/linearwrap.py\", line 47, in layer_func\n    ret = layer(name, self._t, *args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorpack/models/registry.py\", line 124, in wrapped_func\n    outputs = func(*args, **actual_args)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorpack/models/tflayer.py\", line 66, in decorated_func\n    return func(inputs, **ret)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorpack/models/conv2d.py\", line 68, in Conv2D\n    ret = layer.apply(inputs, scope=tf.get_variable_scope())\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 817, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py\", line 374, in __call__\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 730, in __call__\n    self._assert_input_compatibility(inputs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 1477, in _assert_input_compatibility\n    str(x.shape.as_list()))\nValueError: Input 0 of layer conv2d_1 is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: [None, 40, 40]\nI dont know where to change so that i can feed [None, 40 40]. Thanks! Wu,\nI have fixed it, However, I don't find any method to tap the trained\nweights. I want the values of train weights and biases. How to do that?\n. ",
    "stoensin": "yeah, i removed some 'reuse_variables' in my model, it's working, thanks a lot.\n. Thank you for your patience, I have rewritten the format of the issue.\nThe model I am working on now refactors the model I implemented with tensorflow. The original model is already working properly. For the part of image feature extraction, I will now use the faster r-cnn example, the other part from the original Some models are migrated;\nThe error that now appears looks like a dimension mismatch in GroupNorm, but the only part of the network that uses GroupNorm is the code for faster r-cnn, but I have not made any changes to it.. thanks, after many times of debugging , now I can be sure that the problem is not caused by faster-rcnn, the error log confuses me\uff0cthe problem is the shape of our dataset\uff0cnot somewhere else.\nthanks a lot ,now it's not a faster-rcnn exalples issue. ",
    "bluerythem": "I think it is wrong not to handle crowd label even Detectron did not. The network would be penalized for doing true detection inside the crowd label region.\nI have not tried to train on Coco but for my private dataset which includes about 3% crowd label, it improve about 2 points on overall mAP.\nIf it did not make a difference for your former training, is it possible that former code based on iou did not properly ignore anchors in crowd box region?\nAnother possible theory is the crowd region would be marked as bgs if not taken cared of. But later the code samples fgs and bgs and there are many bg labels so some mistakes in crowd label region is may be statistically not significant. \nI am curious why ignoring small anchors inside large crowd box is arguable? Isn't true positives inside crowd box are usually small? Like a person in a crowd of people?. Works! Thanks for the quick reply!. ",
    "xtanitfy": "It can run now.Thank you very much!. I reinstall the environment by using conda.\npip install tensorflow-gpu==1.9.0\n. I found that your version of training is very fast compared to the version which link is \n https://github.com/matterport/Mask_RCNN and can be flexibly configured like MODE_MASK=False.\nI guess the reason for the slow convergence is because the number of graphics card is too small and the batch is very small.\nPlease also give me some advice Thanks\uff01.. ",
    "csuquan": "@xtanitfy \nI face the same error ,could you tell me how to solve it??thanks!. @xtanitfy \n\u60a8\u53ef\u4ee5\u628a\u8054\u7cfb\u65b9\u5f0f\u7ed9\u6211\u4e00\u4e0b\u5417~\u8c22\u8c22. ",
    "StackL0nG": "\u60a8\u597d\uff0c\u6211\u4e5f\u9047\u5230\u4e86\u540c\u6837\u7684\u95ee\u9898\uff0c\u53ef\u4ee5\u5206\u4eab\u4e0b\u60a8\u6700\u7ec8\u7684\u89e3\u51b3\u529e\u6cd5\u5417\uff1f\nHello, I have encountered the same problem, can you share your final solution?. ",
    "CYang0515": "I'm sorry, Next time I will pay attention.\nI use pip to install tensorpack, pip list includes msgpack (0.5.6) moudle.. \nso\uff0cplease, how can I solve it?\n                        454232091\n\n\n\n                                \u90ae\u7bb1\uff1a454232091@qq.com\n\nSignature is customized by Netease Mail Master\nOn 11/28/2018 01:28, Yuxin Wu wrote: msgpack 0.5.6 has the strict_types option: https://github.com/msgpack/msgpack-python/blob/0.5.6/msgpack/_packer.pyx#L117-L119\nTherefore you're not using msgpack 0.5.6. You may have different versions installed at different places.\n\u2014You are receiving this because you authored the thread.Reply to this email directly, view it on GitHub, or mute the thread.\n{\"api_version\":\"1.0\",\"publisher\":{\"api_key\":\"05dde50f1d1a384dd78767c55493e4bb\",\"name\":\"GitHub\"},\"entity\":{\"external_key\":\"github/tensorpack/tensorpack\",\"title\":\"tensorpack/tensorpack\",\"subtitle\":\"GitHub repository\",\"main_image_url\":\"https://assets-cdn.github.com/images/email/message_cards/header.png\",\"avatar_image_url\":\"https://assets-cdn.github.com/images/email/message_cards/avatar.png\",\"action\":{\"name\":\"Open in GitHub\",\"url\":\"https://github.com/tensorpack/tensorpack\"}},\"updates\":{\"snippets\":[{\"icon\":\"PERSON\",\"message\":\"@ppwwyyxx in #991: msgpack 0.5.6 has the strict_types option: https://github.com/msgpack/msgpack-python/blob/0.5.6/msgpack/_packer.pyx#L117-L119\\r\\n\\r\\nTherefore you're not using msgpack 0.5.6. You may have different versions installed at different places.\"}],\"action\":{\"name\":\"View Issue\",\"url\":\"https://github.com/tensorpack/tensorpack/issues/991#issuecomment-442146143\"}}}\n[\n{\n\"@context\": \"http://schema.org\",\n\"@type\": \"EmailMessage\",\n\"potentialAction\": {\n\"@type\": \"ViewAction\",\n\"target\": \"https://github.com/tensorpack/tensorpack/issues/991#issuecomment-442146143\",\n\"url\": \"https://github.com/tensorpack/tensorpack/issues/991#issuecomment-442146143\",\n\"name\": \"View Issue\"\n},\n\"description\": \"View this Issue on GitHub\",\n\"publisher\": {\n\"@type\": \"Organization\",\n\"name\": \"GitHub\",\n\"url\": \"https://github.com\"\n}\n},\n{\n\"@type\": \"MessageCard\",\n\"@context\": \"http://schema.org/extensions\",\n\"hideOriginalBody\": \"false\",\n\"originator\": \"AF6C5A86-E920-430C-9C59-A73278B5EFEB\",\n\"title\": \"Re: [tensorpack/tensorpack] Train faster rcnn  (#991)\",\n\"sections\": [\n{\n\"text\": \"\",\n\"activityTitle\": \"Yuxin Wu\",\n\"activityImage\": \"https://assets-cdn.github.com/images/email/message_cards/avatar.png\",\n\"activitySubtitle\": \"@ppwwyyxx\",\n\"facts\": [\n]\n}\n],\n\"potentialAction\": [\n{\n\"name\": \"Add a comment\",\n\"@type\": \"ActionCard\",\n\"inputs\": [\n{\n\"isMultiLine\": true,\n\"@type\": \"TextInput\",\n\"id\": \"IssueComment\",\n\"isRequired\": false\n}\n],\n\"actions\": [\n{\n\"name\": \"Comment\",\n\"@type\": \"HttpPOST\",\n\"target\": \"https://api.github.com\",\n\"body\": \"{\\n\\\"commandName\\\": \\\"IssueComment\\\",\\n\\\"repositoryFullName\\\": \\\"tensorpack/tensorpack\\\",\\n\\\"issueId\\\": 991,\\n\\\"IssueComment\\\": \\\"{{IssueComment.value}}\\\"\\n}\"\n}\n]\n},\n{\n\"name\": \"Close issue\",\n\"@type\": \"HttpPOST\",\n\"target\": \"https://api.github.com\",\n\"body\": \"{\\n\\\"commandName\\\": \\\"IssueClose\\\",\\n\\\"repositoryFullName\\\": \\\"tensorpack/tensorpack\\\",\\n\\\"issueId\\\": 991\\n}\"\n},\n{\n\"targets\": [\n{\n\"os\": \"default\",\n\"uri\": \"https://github.com/tensorpack/tensorpack/issues/991#issuecomment-442146143\"\n}\n],\n\"@type\": \"OpenUri\",\n\"name\": \"View on GitHub\"\n},\n{\n\"name\": \"Unsubscribe\",\n\"@type\": \"HttpPOST\",\n\"target\": \"https://api.github.com\",\n\"body\": \"{\\n\\\"commandName\\\": \\\"MuteNotification\\\",\\n\\\"threadId\\\": 416716672\\n}\"\n}\n],\n\"themeColor\": \"26292E\"\n}\n]. ",
    "haowu3": "I think the reason is that there is a default ProgressBar callback. You can change it instead of adding another one. The default callbacks are in extra_callbacks so you can override it.\nI dig into the source code to find out that. Maybe it's better to show it in the document.. ",
    "huangmozhilv": "Thanks a lot. It could be this reason. I am tuning the code and set the epoch as 5 steps which takes only 1 second. When the step_per_epoch set to 250, the issues disappear.. ",
    "Juicechen95": "\nYou can downgrade msgpack to 0.5.6\n\nhow to do that?. > You can downgrade msgpack to 0.5.6\nGreat! It works. Thank you!. ",
    "ralph1898": "Thanks for reply, Yuxin. Can we get some advice for achieving the validation loss during the training, such as adding some coding lines somewhere? It seems challenging to borrow some ideas from the codes from other example repositories. The available prediction or evaluation features seem to specifically generate the COCO metrics. Instead, we would like to monitor the validation loss during the training if possible. Thanks again! . So InferenceRunner(QueueInput(dataset_val), [ScalarStats('total_cost')]) this line will go to the condition of \"is.training=False\"? Therefore there was no total_lost calculation, am I right?. I see, thanks for your answers.. ",
    "DreamChaserMXF": "\nYou can use either style and it will not cause error.\n\nI'm quite confused about it. If  data_format is 'channels_last', then channel_axis will be 1 instead of 3 (which I think it should be) according to the if statement . Do you mean the following codes will behave normally? Or it doesn't matter whether channel_axis is right?. > The translation happens at this line:\n\ntensorpack/tensorpack/models/conv2d.py\nLine 77 in 82aa4b2\ndata_format = get_data_format(data_format, tfmode=False)\n\nOh! I got it. Thanks very much!. ",
    "SunskyF": "Thx a lot!. ",
    "FrancisFok": "Thus, we need to add a subclass of DetectionModel.\nThen, it may need to get the input/output tensor name.\nIs there a way to check from the npz file?. ",
    "KunpengLi1994": "This issue is to test the effectiveness of your implemented Squeeze-and-Excitation operation in 'resnet_model.py'\nIt seems has no improvement on your 'cifar10-resnet.py'. . Sure, thanks. ",
    "andreanicastro": "I know. That is exactly what I had. But then I wrote the wrapper for the nice logging and argscope. I thought might have been useful. Probably the split implementation is a bit useless though.\nOn 18 Dec 2018 18:08, Yuxin Wu notifications@github.com wrote:\nPlease see the following from https://tensorpack.readthedocs.io/tutorial/symbolic.html:\nThese layers were written only because there were no alternatives when tensorpack was first developed. Nowadays, these implementation actually call tf.layers directly. Tensorpack will not add any more layers into its core library because this is not the focus of tensorpack, and there are many other alternative symbolic libraries today.\nYou can use tf.layers.Conv3D directly in tensorpack. You do not need to add a tensorpack wrapper for them.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHubhttps://github.com/tensorpack/tensorpack/pull/1017#issuecomment-448315050, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AOeHoJxysJcFMMAkRUypXwhrn7Ux4fLnks5u6S8FgaJpZM4ZYzDm.\n. Perfect. Feel free to close the PR, then! :)\nOn 18 Dec 2018 21:24, Yuxin Wu notifications@github.com wrote:\nFor logging and argscope, see #778https://github.com/tensorpack/tensorpack/pull/778 by @PatWiehttps://github.com/PatWie\nYou can use:\nenable_argscope_for_module(tf.layers)\nto get the \"tensorpack-style\" logging and argscope for tf.layers.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHubhttps://github.com/tensorpack/tensorpack/pull/1017#issuecomment-448375779, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AOeHoAGV6ta0vqWbDUgd0X4C1jojaEYjks5u6V0AgaJpZM4ZYzDm.\n. ",
    "StarsMyDestination": "thanks!. ",
    "AngelinaRanhh": "I  have check this:\n./tensorpack-master/examples/FasterRCNN/train.py --predict person.jpg --load ./tensorpack-master/examples/FasterRCNN/model_cascade.py --config MODE_MASK=True MODE_FPN=True FPN.CASCADE=True BACKBONE.RESNET_NUM_BLOCKS=[3,4,23,3] TEST.RESULT_SCORE_THRESH=1e-4 PREPROC.TRAIN_SHORT_EDGE_SIZE=[640,800] TRAIN.LR_SCHEDULE=[420000,500000,540000] BACKBONE.WEIGHTS=./tensorpack-master/COCO-R101FPN-MaskRCNN-BetterParams.npz\n./tensorpack-master/examples/FasterRCNN/train.py --predict person.jpg --load ./tensorpack-master/examples/FasterRCNN/model_mrcnn.py --config MODE_MASK=True MODE_FPN=True FPN.CASCADE=True BACKBONE.RESNET_NUM_BLOCKS=[3,4,23,3] TEST.RESULT_SCORE_THRESH=1e-4 PREPROC.TRAIN_SHORT_EDGE_SIZE=[640,800] TRAIN.LR_SCHEDULE=[420000,500000,540000] BACKBONE.WEIGHTS=./tensorpack-master/COCO-R101FPN-MaskRCNN-BetterParams.npz\nthe error info is same.. Is this \"COCO-R101FPN-MaskRCNN-BetterParams.npz\" ?. I  got it , Thanks very much! ^_^. ",
    "Hukongtao": "@ppwwyyxx \u2018tensorpack/examples/FasterRCNN/\u2019. @ppwwyyxx That's OK. I just want to figure out the detailed principle of the code. But does this mean that my batch is 8 or 1?Because my machine has become very stuck\u3002. @\n\nYour batch size will be 1.\n\nOK. Thank you very much. ",
    "Remember2018": "Hi @ppwwyyxx , thanks a lot for your great tensorpack! I have  a question: do you mean the Faster-RCNN model trained with tensorpack cannot be depolyed in CPU environment? Thanks.. @ppwwyyxx thanks a lot for your reply! Could you please give some advice on how to do this? Do we need to replace all the maxpool(NCHW) with maxpool(NHWC)? Thanks. . Yeah, thanks a lot. I checked the FasterRCNN/model_fpn.py, it seems you have shown the solution from https://github.com/tensorpack/tensorpack/blob/b097e7d5acefa4eec13873f257035ac0c3db595b/examples/FasterRCNN/model_fpn.py#L40-L45\nAnd I think we may modify all the backbone graph and the FasterRCNN graph to replace the MaxPooling(channel_first) to MaxPooling(channel_last).\nSo I think maybe the easist way is to implement the Maxpooling(NCHW) in CPU mode. I'm not sure why this is not implemented in TensorFlow.\n. Hi @ppwwyyxx , it seems the tf.nn.max_pool support the NCHW format. Please check this issue https://github.com/tensorflow/tensorflow/issues/15364#issuecomment-394766712 \nDoes this mean the tf.layers APIs used in tensorpack have some inconsistency with the tf.nn APIs?. @ppwwyyxx I tested the code, it seems the code can be run in CPU mode. Please check the following screenshots.\n\n\nIn addition, please check the following line, https://github.com/tensorflow/tensorflow/blob/8855b0c12433b1bdebfa1ea72b966a35fb0925bf/tensorflow/core/kernels/pooling_ops_common.h#L299\nit seems the TensorFlow has more data formats (e.g., the NCHW_VECT_C), not only the NHWC and NCHW. I don't know if the tf.layers can support the new formats.\n. Thank you @ppwwyyxx . I used the tf-1.10.1. I'm still figuring out why the saved model trained with tensorpack cannot be deployed in tensorflow-serving CPU environment. \nI checked the tf.layers.MaxPooling2D API in \nhttps://github.com/tensorflow/tensorflow/blob/4dcfddc5d12018a5a0fdca652b9221ed95e9eb23/tensorflow/python/keras/layers/pooling.py#L304-L308\nit seems the function used the tf.nn.max_pool, if the code snippts above can be run in CPU mode, why not the tf.layers.MaxPooling2D? I probably missed something.. Thanks a lot! @ppwwyyxx , I just used the pip install from official website.. It seems the tensorflow also used the tranpose to fit the maxpool op with NCHW format. Please see the following lines:\nhttps://github.com/tensorflow/tensorflow/blob/a6d8ffae097d0132989ae4688d224121ec6d8f35/tensorflow/python/ops/nn_ops.py#L904-L913. Although this might be unrelated to TensorPack, please allow me to post what I have tried here. I tried the following methods, but both failed:\n(1) compile the tensorflow and the tensorflow_model_server with Intel-MKL support, then using the tensorflow-serving-api to serve the FasterRCNN model trained with TensorPack. But the following error occurs:\nraise _Rendezvous(state, None, None, deadline)\ngrpc._channel._Rendezvous: <_Rendezvous of RPC that terminated with (StatusCode.ABORTED, Operation received an exception:Status: 3, message: could not create a dilated convolution forward descriptor, in file external/org_tensorflow/tensorflow/core/kernels/mkl_conv_ops.cc:1125\n     [[{{node conv0/Conv2D}}]])>\n\n(2) replace all the MaxPooling2D and tf.nn.avg_pool with NCHW in the FasterRCNN model with the corresponding NHWC versions (plus the extra two tf.transpose operations).  But the following error occurs:\nraise _Rendezvous(state, None, None, deadline)\ngrpc._channel._Rendezvous: <_Rendezvous of RPC that terminated with (StatusCode.UNIMPLEMENTED, Generic conv implementation only supports NHWC tensor format for now.\n     [[{{node conv0/Conv2D}} = Conv2D[T=DT_FLOAT, _output_shapes=[[1,64,?,?]], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 2, 2], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Pad, conv0/W/read)]])>\n\nSo the final way is, retrain the ImageNet-Pretrained ResNet models with NHWC data format, and then modify all the NCHW operations to the NHWC version? \nIs it possible to re-create the graph with all operations in NHWC format, and then change all the variables in NCHW format to the NHWC format?. @ppwwyyxx thanks to your kind help, I have modified the FasterRCNN with NHWC format, and now the CPU inference of FasterRCNN is OK. \nBut the speed seems slow. For testing one resized image with short edge size of 800 and max size of 1333\uff0c the time of ResNet-50-FPN is about 2.5 seconds.  For the tf-faster-rcnn with VGG16 network, the time is about 1 second. . @roadcode In fact, @ppwwyyxx has posted one solution in https://github.com/tensorpack/tensorpack/tree/fpn_cpu_inference/examples/FasterRCNN branch.. You can also use the imgaug library in the dataflow of Tensorpack.. Yeah, understood. Thanks a lot! @ppwwyyxx .. ",
    "roadcode": "@Remember2018 how do yo modify the FasterRCNN with NHWC format?. ",
    "john81923": "Thanks for your reply. \nYes, It can be formulated as a single-cost optimization problem as you mentioned.\n To implement function f(inputs, weights) -> cost is what i want to achieve here.\nBut, My Technical difficulty is that how to implement the function practically.\nsuch as  accessed all the weights through tensorpack interface,\nand pass it down to my model so that f(inputs, weights) -> cost could work.. I'm trying to adapt model-agnostic meta learning to tensorpack example FasterRCNN.\nSo, you mean that I can not use the \"layer\"  in example, I should rewrite the model on my own ? . Hi, I've got further questions.\nNow, I'm writing my own functions. \nusing tf.nn.conv2D instead of tensorpackConv2D, so I can forwardly pass weights into layers.\nhere's what I do in tensorpack/example/FasterRCNN/train.py ,\nclass ResNetC4Model(DetectionModel):\ndef construct_weights( self, backbone_dir='/project/project-mira6/mcs/fasterRCNNmodels/ImageNet-ResNet50.npz'):\n    '''must call after '''\n    # resnet variable \n    backbone_weights = np.load( backbone_dir ) \n    weights = {}\n    weights['conv0/W'] = tf.Variable( backbone_weights['conv0/W:0'], name='conv0/W')\n    weights['conv0/beta'] = tf.Variable( backbone_weights['conv0/bn/beta:0'], name='conv0/beta')\n    weights['conv0/gamma'] = tf.Variable( backbone_weights['conv0/bn/gamma:0'], name='conv0/gamma')\n    RESNET_NUM_BLOCK = [3, 4, 6, 3] \n    for g in [0,1,2,3]:\n        for blc in range( RESNET_NUM_BLOCK[g]):\n            for cv in np.arange(1,4):\n                weights[f'group{g}/block{blc}/conv{cv}/W'] = tf.Variable( backbone_weights[f'group{g}/block{blc}/conv{cv}/W:0'],name=f'group{g}/block{blc}/conv{cv}/W' )\n                weights[f'group{g}/block{blc}/conv{cv}/gamma'] = tf.Variable( backbone_weights[f'group{g}/block{blc}/conv{cv}/bn/gamma:0'],name=f'group{g}/block{blc}/conv{cv}/gamma' )\n                weights[f'group{g}/block{blc}/conv{cv}/beta'] = tf.Variable( backbone_weights[f'group{g}/block{blc}/conv{cv}/bn/beta:0'], name=f'group{g}/block{blc}/conv{cv}/beta' )\n\n            if blc == 0:\n                weights[f'group{g}/block{blc}/convshortcut/W'] = tf.Variable( backbone_weights[f'group{g}/block{blc}/convshortcut/W:0'],name=f'group{g}/block{blc}/convshortcut/W' )\n                weights[f'group{g}/block{blc}/convshortcut/gamma'] = tf.Variable( backbone_weights[f'group{g}/block{blc}/convshortcut/bn/gamma:0'],name=f'group{g}/block{blc}/convshortcut/gamma' )\n                weights[f'group{g}/block{blc}/convshortcut/beta'] = tf.Variable( backbone_weights[f'group{g}/block{blc}/convshortcut/bn/beta:0'],name=f'group{g}/block{blc}/convshortcut/beta' )\n    # rpn\n    weights['rpn/conv0/W']=tf.get_variable('rpn/conv0/W', shape=[3,3,1024,1024], initializer=tf.random_normal_initializer(stddev=0.01))\n    weights['rpn/conv0/b']=tf.get_variable('rpn/conv0/b', shape=[1024], initializer=tf.zeros_initializer() )\n    weights['rpn/class/W']=tf.get_variable('rpn/class/W', shape=[1,1,1024,15], initializer=tf.random_normal_initializer(stddev=0.01))\n    weights['rpn/class/b']=tf.get_variable('rpn/class/b', shape=[15], initializer=tf.zeros_initializer() )\n    weights['rpn/box/W']=tf.get_variable('rpn/box/W', shape=[1,1,1024,60], initializer=tf.random_normal_initializer(stddev=0.01))\n    weights['rpn/box/b']=tf.get_variable('rpn/box/b', shape=[60], initializer=tf.zeros_initializer() )\n\n    # frcnn FC layer\n    weights['fastrcnn/class/W']=tf.get_variable('fastrcnn/class/W', shape=[2048, 81], initializer=tf.random_normal_initializer(stddev=0.01))\n    weights['fastrcnn/class/b']=tf.get_variable('fastrcnn/class/b', shape=[81], initializer=tf.zeros_initializer() )\n    weights['fastrcnn/box/W']=tf.get_variable('fastrcnn/box/W', shape=[2048, 324], initializer=tf.random_normal_initializer(stddev=0.01))\n    weights['fastrcnn/box/b']=tf.get_variable('fastrcnn/box/b', shape=[324], initializer=tf.zeros_initializer() )\n\n    return weights\n\ndef build_graph(self, *inputs):\n    inputs = dict(zip(self.input_names, inputs))\n    is_training = get_current_tower_context().is_training\n    image = self.preprocess(inputs['image'])     # 1CHW\n\n    with tf.variable_scope('Meta_FasterRCNN', reuse=None) as training_scope:\n        if 'weights' in dir(self):\n            training_scope.reuse_variables()\n            weights = self.weights\n        else:\n            # Define the weights\n            self.weights = weights = self.construct_weights()\n\n\n        featuremap = resnet_c4_backbone(image, cfg.BACKBONE.RESNET_NUM_BLOCK[:3], weights)`\n\nI load the ResNet50.npz into tf.Variables and save it in a dictionary name weights , and pass it through my model in order to accomplish the function f(inputs, weights) -> cost\nbut, I'm have fatal errors such as : \n\n[0107 03:39:27 @training.py:296] WRN [ReplicatedTrainer] Do not know how to sync variable 'tower0/Meta_FasterRCNN/conv0/W:0' across GPUs. Reason: Name should not have prefix 'tower0' in this trainer! This variable is trainable, so this is probably a fatal error.\n[0107 03:39:27 @training.py:296] WRN [ReplicatedTrainer] Do not know how to sync variable 'tower0/Meta_FasterRCNN/conv0/beta:0' across GPUs. Reason: Name should not have prefix 'tower0' in this trainer! This variable is trainable, so this is probably a fatal error.\n\nat the end I've got error:\n\nTraceback (most recent call last):\n  File \"train.py\", line 592, in \n    launch_train_with_config(traincfg, trainer)\n  File \"/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/train/interface.py\", line 95, in launch_train_with_config\n    extra_callbacks=config.extra_callbacks)\n  File \"/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/train/base.py\", line 319, in train_with_defaults\n    steps_per_epoch, starting_epoch, max_epoch)\n  File \"/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/train/base.py\", line 289, in train\n    self.setup_callbacks(callbacks, monitors)\n  File \"/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/utils/argtools.py\", line 181, in wrapper\n    return func(args, *kwargs)\n  File \"/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/train/base.py\", line 189, in setup_callbacks\n    self._callbacks.setup_graph(weakref.proxy(self))\n  File \"/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/callbacks/base.py\", line 52, in setup_graph\n    self._setup_graph()\n  File \"/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/callbacks/group.py\", line 70, in _setup_graph\n    cb.setup_graph(self.trainer)\n  File \"/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/callbacks/base.py\", line 52, in setup_graph\n    self._setup_graph()\n  File \"train.py\", line 422, in _setup_graph\n    self.predictors = [self._build_coco_predictor(k % num_gpu) for k in range(self.num_predictor)]\n  File \"train.py\", line 422, in \n    self.predictors = [self._build_coco_predictor(k % num_gpu) for k in range(self.num_predictor)]\n  File \"train.py\", line 436, in _build_coco_predictor\n    graph_func = self.trainer.get_predictor(self._in_names, self._out_names, device=idx)\n  File \"/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/train/tower.py\", line 135, in get_predictor\n    output_tensors = tower.get_tensors(output_names)\n  File \"/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/tfutils/tower.py\", line 405, in get_tensors\n    return [self.get_tensor(name) for name in names]\n  File \"/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/tfutils/tower.py\", line 405, in \n    return [self.get_tensor(name) for name in names]\n  File \"/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/tfutils/tower.py\", line 388, in get_tensor\n    ret = get_op_or_tensor_by_name(name_with_ns)\n  File \"/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/tfutils/common.py\", line 134, in get_op_or_tensor_by_name\n    return f(name)\n  File \"/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/tfutils/common.py\", line 129, in f\n    return G.get_tensor_by_name(n)\n  File \"/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3207, in get_tensor_by_name\n    return self.as_graph_element(name, allow_tensor=True, allow_operation=False)\n  File \"/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3035, in as_graph_element\n    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\n  File \"/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3077, in _as_graph_element_locked\n    \"graph.\" % (repr(name), repr(op_name)))\nKeyError: \"The name 'tower-pred-0/output/boxes:0' refers to a Tensor which does not exist. The operation, 'tower-pred-0/output/boxes', does not exist in the graph.\"\nMultiProcessMapDataZMQ successfully cleaned-up.\n\nplease help. As the Rules of Tower Function has mentioned:\n\nThe creation of any trainable variables must respect reuse variable scope. To respect variable reuse (i.e. sharing), use tf.get_variable instead of tf.Variable in the function.\n\nSo, I am not able to use tf.Variable in the case. \nhowever, how can I initial my training variable  weights[f'group{g}/block{blc}/conv{cv}/W'] with ResNet pre-trained weights using tf.get_variable  ?. Thank you , it works!\nI  have another question,\nIn meta-learning, there'sinput_aand inputs_b in each training iterations.\ncomputing losses such as loss( inputs_a, label_a) and  loss( inputs_b, label_b) \nbut  get_train_dataflow in FasterRCNN pass one image in each training iterations.\nThus, how can I get two or more data and labels from dataflow in each iterations\n. Can you tell me how to modify the dataflow please. I have no idea.. Thank you helps a lot\n I'd like to know according to code below:\n`       class DataFromList(RNGDataFlow):\ndef __init__(self, lst, shuffle=True):\n    \"\"\"\n    Args:\n        lst (list): input list. Each element is a datapoint.\n        shuffle (bool): shuffle data.\n    \"\"\"\n    super(DataFromList, self).__init__()\n    self.lst = lst\n    self.shuffle = shuffle\n\ndef __len__(self):\n    return len(self.lst)\n\ndef __iter__(self):\n    if not self.shuffle:\n        for k in self.lst:\n            yield k\n    else:\n        idxs = np.arange(len(self.lst))\n        self.rng.shuffle(idxs)\n        for k in idxs:\n            yield self.lst[k]`\n\nWill the data list shuffle every training iteration?\n. So, will it shuffle again after it produce everything in the list?\nsuch as, after training an epoch, will it shuffle the list again. Hi, I'd like to know how to implement an Update_Weights function : \nweights = Update_Weights( loss, weights) \nbase on the optimizer of FasterRCNN below\n`# optimizer function\n def optimizer(self):\n\n    lr = tf.get_variable('learning_rate', initializer=0.003, trainable=False)\n    tf.summary.scalar('learning_rate-summary', lr)\n\n    # The learning rate is set for 8 GPUs, and we use trainers with average=False.\n    lr = lr / 8.\n    opt = tf.train.MomentumOptimizer(lr, 0.9)\n    if cfg.TRAIN.NUM_GPUS < 8:\n        opt = optimizer.AccumGradOptimizer(opt, 8 // cfg.TRAIN.NUM_GPUS)\n\n    return opt`\n\nthank you. sorry, I did not make my question  clear.\nlet me define it again.\nMy target is to update weight two times in an iteration. So here's what I do\nI'm using a function that feeds inputs and weights and  return cost  f(inputs, weights) -> cost \ncomputes the gradients as \u2207\u03b8 , learning rate as lr\nTherefore, my final cost would be f(inputs,  \u03b8 - lr*\u2207\u03b8 ) \nSo, I have to access the accumulation  and the learning rate for my final cost.\nSince the optimizer is defined in class DetectionModel(ModelDesc):   def optimizer(self): ,\nis there any way I can access the slots and learning rate of optimizer through the tensorpack interface?. I tried computing gradients before final cost, but having error:\n\nTraceback (most recent call last):\n  File \"/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 551, in gradients\n    grad_fn = ops.get_gradient_function(op)\n  File \"/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2134, in get_gradient_function\n    return _gradient_registry.lookup(op_type)\n  File \"/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorflow/python/framework/registry.py\", line 93, in lookup\n    \"%s registry has no entry for: %s\" % (self._name, name))\nLookupError: gradient registry has no entry for: CropAndResizeGradImage\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"train.py\", line 558, in \n    launch_train_with_config(traincfg, trainer)\n  File \"/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/train/interface.py\", line 85, in launch_train_with_config\n    model._build_graph_get_cost, model.get_optimizer)\n  File \"/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/utils/argtools.py\", line 181, in wrapper\n    return func(args, *kwargs)\n  File \"/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/train/tower.py\", line 203, in setup_graph\n    train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)\n  File \"/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/train/trainers.py\", line 172, in _setup_graph\n    self._make_get_grad_fn(input, get_cost_fn, get_opt_fn), get_opt_fn)\n  File \"/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/graph_builder/training.py\", line 222, in build\n    use_vs=[False] + [True] * (len(self.towers) - 1))\n  File \"/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/graph_builder/training.py\", line 118, in build_on_towers\n    ret.append(func())\n  File \"/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorpack/train/tower.py\", line 246, in get_grad_fn\n    aggregation_method=self.AGGREGATION_METHOD)\n  File \"/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 414, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/auto/master05/john81923/ENV/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 555, in gradients\n    (op.name, op.type))\nLookupError: No gradient defined for operation 'tower0/gradients/tower0/roi_align/crop_and_resize/CropAndResize_grad/CropAndResizeGradImage' (op type: CropAndResizeGradImage)\n\nwhy is the name 'tower' after 'tower' such as  'tower0/gradients/tower0/***' . Thanks for your kindness reply, \nyes, tf.gradients is what I used here\ngrads = tf.gradients(total_cost, list(weights.values()))\nbut it ends up with error :\n\nLookupError: No gradient defined for operation 'tower0/gradients/tower0/roi_align/crop_and_resize/CropAndResize_grad/CropAndResizeGradImage' (op type: CropAndResizeGradImage)\n\nplease help. thank you for your reply,\nDo you have any ideas why does this error pop-up after I add tf.gradients in my method?\ncause it doesn't happen before I did this\n. Thank you,  I've been searching for a while still not found any useful solutions.\nIs there a way that I can fix this error ?\ncause I am not quite sure why does it contain theCropAndResizeGradImage operation when I use tf.gradients and why doesn't it contained when not using tf.gradients\n . ",
    "canhnp": "Dear,\nHow I can select a suitable parameters for theses:\ndataflow.imgaug.Brightness(delta??, clip=True)\n(i.e. delta = ?)\ndataflow.imgaug.Contrast(factor_range??, rgb=True, clip=True)\n(i.e. factor_range??)\ndataflow.imgaug.GaussianBlur(max_size=3)\ndataflow.imgaug.Gamma(range=(-0.5, 0.5))\nBest regards,\nCanh Nguyen. ",
    "xinli94": "Hi,\nThanks for the quick reply! So in my annotations json file, segmentation field is always set to be empty, which caused the error. I am struggling to convert the dataset to coco format properly right now.\nThanks,\nXin\n. I trained res101_cascade_fpn_gn from scratch and replicated trainer works perfectly for me! Closing the issue now. Will post here if I find anything.. Cool! Thanks!. ",
    "zhangyu2ustc": "Thanks for your help. Problem solved.\nI have another errors when trying compile the CNN model use the KerasModel class.\n    model_test_GPU_new.compile(optimizer=tf.train.AdamOptimizer(), loss='categorical_crossentropy', metrics='categorical_accuracy')\n  File \"/home/yu/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/contrib/keras.py\", line 267, in compile\n    metrics=metrics)\n  File \"/home/yu/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/contrib/keras.py\", line 212, in setup_keras_trainer\n    lambda: optimizer)\n  File \"/home/yu/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/utils/argtools.py\", line 176, in wrapper\n    return func(args, kwargs)\n  File \"/home/yu/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/train/tower.py\", line 214, in setup_graph\n    train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)\n  File \"/home/yu/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/train/trainers.py\", line 193, in _setup_graph\n    grad_list = self._builder.call_for_each_tower(tower_fn)\n  File \"/home/yu/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/graph_builder/training.py\", line 225, in call_for_each_tower\n    use_vs=[False] + [True] * (len(self.towers) - 1))\n  File \"/home/yu/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/graph_builder/training.py\", line 121, in build_on_towers\n    return DataParallelBuilder.call_for_each_tower(*args, kwargs)\n  File \"/home/yu/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/graph_builder/training.py\", line 116, in call_for_each_tower\n    ret.append(func())\n  File \"/home/yu/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/train/tower.py\", line 266, in get_grad_fn\n    return compute_grad_from_inputs(inputs)\n  File \"/home/yu/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/train/tower.py\", line 245, in compute_grad_from_inputs\n    cost = get_cost_fn(inputs)\n  File \"/home/yu/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/tfutils/tower.py\", line 286, in call\n    output = self._tower_fn(args)\n  File \"/home/yu/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/contrib/keras.py\", line 165, in get_cost\n    outputs = model_caller(input_tensors)\n  File \"/home/yu/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/contrib/keras.py\", line 106, in call\n    outputs = model.outputs\nAttributeError: 'Tensor' object has no attribute 'outputs'\nHere is my code:\n    train_gen = data_pipe_3dcnn_block(fmri_data_train, confounds_train, label_train,\n                                      target_name=target_name, flag_cnn=Flag_CNN_Model,block_dura=block_dura,\n                                      batch_size=batch_size, data_type='train', nr_thread=nr_thread, buffer_size=buffer_size)\n\n    #########################################\n    def one_hot(label):\n        return np.eye(len(target_name))[label]\n\n    train_gen = dataflow.MapDataComponent(train_gen, one_hot, 1)\n\n    model_test_GPU_new = KerasModel(model_test_GPU,\n                                    inputs_desc=[InputDesc(tf.float32, (None,) + img_shape, 'images')],\n                                    targets_desc=[InputDesc(tf.uint8, (None, len(target_name)), 'labels')],\n                                    input=train_gen, trainer=SyncMultiGPUTrainerReplicated(num_GPU))\n\n    model_test_GPU_new.compile(optimizer=tf.train.AdamOptimizer(), loss='categorical_crossentropy', metrics='categorical_accuracy')\n    callbacks = [ModelSaver(), GPUUtilizationTracker() ]\n\n    ######start training the model\n    model_test_GPU_new.fit(steps_per_epoch=100, max_epoch=100, callbacks=callbacks). Thanks for your quick reply.\n\nI actually used a keras model, with the function listed below:\ndef build_cnn_model_test(input_shape, Nlabels, filters=16, convsize=3, convsize2=5, poolsize=2, hidden_size=256, conv_layers=4):\n    input0 = Input(shape=input_shape)\n    drop1 = input0\n    ####quickly reducing image dimension first\n    for li in range(1):\n        conv1 = Conv2D(filters, (convsize, convsize), strides=2, padding='same',activation='relu',\n                    kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(l2_reg))(drop1)\n        conv1 = BatchNormalization()(conv1)\n        drop1 = Dropout(0.25)(conv1)\n        filters = 2\n    for li in range(conv_layers-1):\n        conv1 = Conv2D(filters, convsize, padding='same',activation='relu',\n                    kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(l2_reg))(drop1)\n        conv1 = BatchNormalization()(conv1)\n        conv1 = Conv2D(filters, convsize2, padding='same',activation='relu',\n                    kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(l2_reg))(conv1)\n        conv1 = BatchNormalization()(conv1)\n        conv1 = Conv2D(filters, (convsize, convsize), strides=2, padding='same',activation='relu',\n                    kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(l2_reg))(conv1)\n        conv1 = BatchNormalization()(conv1)\n        drop1 = Dropout(0.25)(conv1)\n        if (li+1) % 2 == 0:\n            filters = 2\n    drop2 = drop1\n    avg1 = AveragePooling2D(pool_size=(5, 5))(drop2)\n    flat = Flatten()(avg1)\n    hidden = Dense(hidden_size, kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(l2_reg), activation='relu')(flat)\n    drop3 = Dropout(0.4)(hidden)\n    out = Dense(Nlabels, activation='softmax')(drop3)\n    model = Model(inputs=input0, outputs=out)\n    model.summary()\n    return model\nmodel_test_GPU = build_cnn_model_test(img_shape, nb_class). Have solved the problem by replacing \"input0=Input(shape=input_shape)\" with \"input0=Input(tensor=image)\"\nThe script works now when I only use one-gpu following the mnist example.\nBut I have another issue when trying to use the multi-gpu for training and validation.\nI was following this example: https://github.com/tensorpack/tensorpack/blob/master/examples/keras/imagenet-resnet-keras.py\nHere is the new error message:\n[0109 12:01:46 @collection.py:151] Size of these collections were changed in tower1: (tf.GraphKeys.UPDATE_OPS: 68->136)\n/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/graph_builder/training.py:320: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n  \"Reason: {} \".format(name, reason))\n[0109 12:01:51 @training.py:320] WRN [ReplicatedTrainer] Do not know how to sync variable 'tower1/conv2d_34/kernel:0' across GPUs. Reason: Cannot find conv2d_34/kernel:0 in the graph! \nTraceback (most recent call last):\n  File \"./HCP_task_fmri_cnn_tensorpack_changesize_bk4_wm_test.py\", line 822, in \n    model_test_GPU_new.compile(optimizer=tf.train.AdamOptimizer(learning_rate), loss='categorical_crossentropy', metrics='categorical_accuracy')\n  File \"/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/contrib/keras.py\", line 267, in compile\n    metrics=metrics)\n  File \"/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/contrib/keras.py\", line 212, in setup_keras_trainer\n    lambda: optimizer)\n  File \"/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/utils/argtools.py\", line 176, in wrapper\n    return func(args, *kwargs)\n  File \"/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/train/tower.py\", line 214, in setup_graph\n    train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)\n  File \"/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/train/trainers.py\", line 194, in _setup_graph\n    self.train_op, post_init_op = self._builder.build(grad_list, get_opt_fn)\n  File \"/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/graph_builder/training.py\", line 303, in build\n    post_init_op = SyncMultiGPUReplicatedBuilder.get_post_init_ops()\n  File \"/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/graph_builder/training.py\", line 345, in get_post_init_ops\n    log_failure(v.name, \"Cannot find {} in the graph!\".format(realname))\n  File \"/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/graph_builder/training.py\", line 322, in log_failure\n    \"The aforementioned variable is trainable, so this is probably a fatal error.\"\nAssertionError: The aforementioned variable is trainable, so this is probably a fatal error.\n. upgrading does not fix the issue.\nIf you're asking about an unexpected problem you met, use this template.\nPLEASE DO NOT DELETE THIS TEMPLATE, FILL IT:\n1. What you did:\nI was trying to use multi-gpu for model training\n(1) If you're using examples, what's the command you run:\nfollowing example: https://github.com/tensorpack/tensorpack/blob/master/examples/keras/imagenet-resnet-keras.py\n(2) If you're using examples, have you made any changes to the examples? Paste them here:\nchange the optimizer to Adam:\n  model = KerasModel(build_model_resnet50,\n                                    inputs_desc=[InputDesc(tf.float32, (None,) + img_shape, 'images')],\n                                    targets_desc=[InputDesc(tf.float32, (None, len(target_name)), 'labels')],\n                                    input=train_gen, trainer=SyncMultiGPUTrainerReplicated(num_GPU)\n                                    )\n model.compile(optimizer=tf.train.AdamOptimizer(learning_rate), loss='categorical_crossentropy', metrics='categorical_accuracy')\n(3) If not using examples, tell us what you did here:\nNote that we may not be able to investigate it if there is no reproducible code.\nIt's always better to paste what you did instead of describing them.\n2. What you observed:\n(1) Include the ENTIRE logs here:\n[0109 12:20:48 @collection.py:151] Size of these collections were changed in tower1: (tf.GraphKeys.UPDATE_OPS: 68->136)\n/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/graph_builder/training.py:320: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n  \"Reason: {} \".format(name, reason))\n[0109 12:20:52 @training.py:320] WRN [ReplicatedTrainer] Do not know how to sync variable 'tower1/conv2d_34/kernel:0' across GPUs. Reason: Cannot find conv2d_34/kernel:0 in the graph! \nTraceback (most recent call last):\n  File \"./HCP_task_fmri_cnn_tensorpack_changesize_bk4_wm_test.py\", line 822, in \n    model_test_GPU_new.compile(optimizer=tf.train.AdamOptimizer(learning_rate), loss='categorical_crossentropy', metrics='categorical_accuracy')\n  File \"/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/contrib/keras.py\", line 267, in compile\n    metrics=metrics)\n  File \"/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/contrib/keras.py\", line 212, in setup_keras_trainer\n    lambda: optimizer)\n  File \"/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/utils/argtools.py\", line 176, in wrapper\n    return func(args, *kwargs)\n  File \"/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/train/tower.py\", line 214, in setup_graph\n    train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)\n  File \"/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/train/trainers.py\", line 194, in _setup_graph\n    self.train_op, post_init_op = self._builder.build(grad_list, get_opt_fn)\n  File \"/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/graph_builder/training.py\", line 303, in build\n    post_init_op = SyncMultiGPUReplicatedBuilder.get_post_init_ops()\n  File \"/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/graph_builder/training.py\", line 345, in get_post_init_ops\n    log_failure(v.name, \"Cannot find {} in the graph!\".format(realname))\n  File \"/home/yuzhang/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/graph_builder/training.py\", line 322, in log_failure\n    \"The aforementioned variable is trainable, so this is probably a fatal error.\"\nAssertionError: The aforementioned variable is trainable, so this is probably a fatal error.\nIt's always better to paste what you observed instead of describing them.\nA part of logs is sometimes enough, but it's always better to paste as much as possible.\nYou can run a command with CMD 2>&1 | tee logs.txt to save all stdout & stderr logs to one file.\n(2) Other observations, if any:\nFor example, CPU/GPU utilization, output images, tensorboard curves, if relevant to your issue.\n[0109 12:20:38 @training.py:49] [DataParallel] Training a model of 2 towers.\n[0109 12:20:38 @interface.py:31] Automatically applying QueueInput on the DataFlow.\n[0109 12:20:38 @interface.py:43] Automatically applying StagingInput on the DataFlow.\n[0109 12:20:38 @input_source.py:220] Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...\n[0109 12:20:38 @training.py:109] Building graph for training tower 0 on device /gpu:0 ...\n[0109 12:20:43 @training.py:109] Building graph for training tower 1 on device /gpu:1 ...\n3. What you expected, if not obvious.\nIf you expect higher accuracy, only in one of the two conditions can we help with it:\n(1) You're unable to match the accuracy documented in tensorpack examples.\n(2) It appears to be a tensorpack bug.\nOtherwise, how to get high accuracy is a machine learning question and is\nnot our responsibility to figure out.\n4. Your environment:\n\nPython version: python3.6\nTF version: python -c 'import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)'. 1.12.0\nTensorpack version: python -c 'import tensorpack; print(tensorpack.__version__);'.\n      You can install Tensorpack master by pip install -U git+https://github.com/ppwwyyxx/tensorpack.git\n      and see if your issue is already solved. 0.9\nIf you're not using tensorpack under a normal command line shell (e.g.,\n    using an IDE or jupyter notebook), please retry under a normal command line shell.  in shell\nHardware information, e.g. number of GPUs used. 2\n\nAbout efficiency issues, PLEASE first read http://tensorpack.readthedocs.io/en/latest/tutorial/performance-tuning.html\n. Upgrading again. Still the same version.\npip install -U git+https://github.com/ppwwyyxx/tensorpack.git\n...\nSuccessfully installed tensorpack-0.9.0.1\nYes. I have tested the example with fake_data using the two gpu setting. It worked.\nHere, I only need training on multi-gpus without hyperparamter tuning of learning rate.. Plus, how was the number of iterations determined? I have tried to change my training set but still result in the same iterations. In my case, it is simply too large, taking 60526112 hours to finish using single gpu. How can I modify this value?\n[0109 14:00:14 @inference_runner.py:97] [InferenceRunner] Will eval 62500000000 iterations. Is it possible to change the default value for validation_steps if I want to keep the validation_data option? Because my dataflow.size() is just a large value and meaningless.. ",
    "caicaibins": "  Ok, I will check it more detail, and I am still confused about data load mechanism\uff0cIt's will load whole datasets just one time  when  launch_train_with_config() ?\n  In training phase,   deploying batch_size = 32, thread_process number = 4, prefetch = 1000,  use QueueInput(), and dataset contain  5000 feature files ,  get  OOM,   but  down to 100 feature files, it's  work well, all the feature file shape is: [601 ,880] ,dtype np.float32.\n\ndefine:\nclass DataFlow(RNGDataFlow):\ndef __init__(self, data_path, batch_size):\n    self.batch_size = batch_size\n    self.wav_files = glob.glob(data_path)\n\ndef __call__(self, n_prefetch=1000, n_thread=1):\n    df = self\n    df = BatchData(df, self.batch_size)\n    df = PrefetchData(df, n_prefetch, n_thread)\n    return df\n\nclass Net2DataFlow(DataFlow):\n    def get_data(self):\n        for xx in xxx:\n             yiled\uff08read *npy file\uff09\ndf = Net2DataFlow(xx, bastch_size = 32 )\ndata=QueueInput(df(n_prefetch=1000, n_thread=4))\n            . ",
    "elmirador": "\nThanks a lot for finding this!\nA great example of what I wrote here: https://tensorpack.readthedocs.io/tutorial/symbolic.html#use-other-symbolic-libraries\n\nIt\u2019s best to not trust others\u2019 layers!\nFor non-standard layers that\u2019s not included in TensorFlow or Tensorpack, it\u2019s best to implement them yourself. Non-standard layers often do not have a mathematical definition that people all agree on, and different people can implement it differently. Also, deep learning models on github often have bugs, especially when there is no reproduced experiments with the code.\nFor your own good, it\u2019s best to implement the layers yourself. This is also why Tensorpack does not contain non-standard layers.\n\nBilinearUpSample was made un-public and removed from documentation (https://tensorpack.readthedocs.io/modules/models.html) a while ago because it's a non-standard layer and we should not provide an implementation for it. But it's good to know the bug!\n\nI see, but some of the non-standard layers like upsampling are pretty common. A separate non-standard layer codebase might be helpful though.\nThe reason I hesitate to use TF's implementation is that the image.resize_* functions are messy (at least in resize_area) and might cause some weird things to happen. tf.image.resize_bilinear works fine though.. > Might be helpful as a reference. But it will have the same issue: personally I would not trust any non-standard layers written by others.\nI agree. Someone should at least read the code before using it.\n\ntf.image.resize_bilinear has its own issues as well and it certainly is not equivalent to this layer in tensorpack during upsampling. That's why they're called non-standard.\n\nGot it. I'll dig harder on TF's code.\nAll in all, pretty happy to contribute. I love tensorpack much more than keras personally lol. I get where you come from. You're considering all parameters inside the cv2.warpAffine function as the transformation. I only consider the mat parameter as the transformation since it's the core of the rotation.\n\nCalling augment_with_params with params produced by a different augmentor instance is not the \"official\" usage and is also hacky\n\nSeems like my goal is just not achievable by using imgaug normally lol. I'll close this issue.. Actually, numpy has the same issue. The child processes for whatever reason don't re-initialize themselves with a new seed. Python's random module, on the other hand, is completely fine though.. Thanks! Really appreciate the clearer doc!. ",
    "tutu1234": "Please print the the keys and their shape of var_to_dump, Maybe you don't import the meta and load ckpt  correctly.. ",
    "Mykheievskyi": "I solved the problem. You are rights, problem was  with  not correct matching  COCO_id_to_category_id and  class_names.\nThank you for the help.. ",
    "ColonelKassad": "Thanks. ",
    "vxgu86": "and when I exec the code python mnist-visualizations.py \nthe following errors dump:\n/opt/anaconda2/lib/python2.7/site-packages/h5py/init.py:36: FutureWarning: Conversion of the second argument of issubdtype from float to np.floating is deprecated. In future, it will be treated as np.float64 == np.dtype(float).type.\n  from ._conv import register_converters as _register_converters\n[0118 14:52:56 @logger.py:94] WRN Log directory train_log/mnist-visualizations exists! Please either backup/delete it, or use a new directory.\n[0118 14:52:56 @logger.py:96] WRN If you're resuming from a previous run you can choose to keep it.\n[0118 14:52:56 @logger.py:97] Select Action: k (keep) / b (backup) / d (delete) / n (new) / q (quit):\nd\n[0118 14:52:58 @logger.py:74] Argv: mnist-visualizations.py\n[0118 14:52:58 @fs.py:89] WRN Env var $TENSORPACK_DATASET not set, using /home/lthpc/tensorpack_data for datasets.\nTraceback (most recent call last):\n  File \"mnist-visualizations.py\", line 128, in \n    model=Model(),\nTypeError: Can't instantiate abstract class Model with abstract methods _get_inputs\n. I reinstalled the tensorpack and reboot, then the process aborted only before import tensorpack\n>>> import tensorpack\n/opt/anaconda2/lib/python2.7/site-packages/h5py/init.py:36: FutureWarning: Conversion of the second argument of issubdtype from float to np.floating is deprecated. In future, it will be treated as np.float64 == np.dtype(float).type.\n  from ._conv import register_converters as _register_converters\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/opt/anaconda2/lib/python2.7/site-packages/tensorpack/init.py\", line 17, in \n    from tensorpack.models import \n  File \"/opt/anaconda2/lib/python2.7/site-packages/tensorpack/models/init.py\", line 48, in \n    _global_import(module_name)\n  File \"/opt/anaconda2/lib/python2.7/site-packages/tensorpack/models/init.py\", line 29, in _global_import\n    p = import(name, globals(), locals(), level=1)\n  File \"/opt/anaconda2/lib/python2.7/site-packages/tensorpack/models/regularize.py\", line 24, in \n    l2_regularizer = tf.contrib.layers.l2_regularizer\n  File \"/opt/anaconda2/lib/python2.7/site-packages/tensorflow/python/util/lazy_loader.py\", line 53, in getattr\n    module = self._load()\n  File \"/opt/anaconda2/lib/python2.7/site-packages/tensorflow/python/util/lazy_loader.py\", line 42, in _load\n    module = importlib.import_module(self.name)\n  File \"/opt/anaconda2/lib/python2.7/importlib/init.py\", line 37, in import_module\n    import(name)\n  File \"/opt/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/init.py\", line 40, in \n    from tensorflow.contrib import distributions\n  File \"/opt/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/distributions/init.py\", line 38, in \n    from tensorflow.contrib.distributions.python.ops.estimator import \n  File \"/opt/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/distributions/python/ops/estimator.py\", line 21, in \n    from tensorflow.contrib.learn.python.learn.estimators.head import _compute_weighted_loss\n  File \"/opt/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/init.py\", line 95, in \n    from tensorflow.contrib.learn.python.learn import \n  File \"/opt/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/init.py\", line 28, in \n    from tensorflow.contrib.learn.python.learn import \n  File \"/opt/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/init.py\", line 30, in \n    from tensorflow.contrib.learn.python.learn import estimators\n  File \"/opt/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/init.py\", line 302, in \n    from tensorflow.contrib.learn.python.learn.estimators.dnn import DNNClassifier\n  File \"/opt/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py\", line 35, in \n    from tensorflow.contrib.learn.python.learn.estimators import dnn_linear_combined\n  File \"/opt/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py\", line 36, in \n    from tensorflow.contrib.learn.python.learn.estimators import estimator\n  File \"/opt/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 52, in \n    from tensorflow.contrib.learn.python.learn.learn_io import data_feeder\n  File \"/opt/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_io/init.py\", line 26, in \n    from tensorflow.contrib.learn.python.learn.learn_io.dask_io import extract_dask_data\n  File \"/opt/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_io/dask_io.py\", line 33, in \n    import dask.dataframe as dd\n  File \"/opt/anaconda2/lib/python2.7/site-packages/dask/dataframe/init.py\", line 3, in \n    from .core import (DataFrame, Series, Index, _Frame, map_partitions,\n  File \"/opt/anaconda2/lib/python2.7/site-packages/dask/dataframe/core.py\", line 19, in \n    from .. import array as da\n  File \"/opt/anaconda2/lib/python2.7/site-packages/dask/array/init.py\", line 7, in \n    from .routines import (take, choose, argwhere, where, coarsen, insert,\n  File \"/opt/anaconda2/lib/python2.7/site-packages/dask/array/routines.py\", line 253, in \n    @wraps(np.matmul)\n  File \"/opt/anaconda2/lib/python2.7/functools.py\", line 33, in update_wrapper\n    setattr(wrapper, attr, getattr(wrapped, attr))\nAttributeError: 'numpy.ufunc' object has no attribute 'module'\n\n\n\nimport numpy\nnumpy.version\n'1.16.0'\n. the problem solved with \npip install numpy==1.15.4. \n\n\n",
    "kkihara": "Ah I see. I'd be happy to contribute, but that does seem complicated while supporting Python 2.. ",
    "WeiyiLi": "Thanks. But how can I get the input names and output names? I am trying shufflenet V2. . I tried the code in https://github.com/tensorpack/tensorpack/blob/master/examples/basics/export-model.py, but I didn't find where I should put \"print(tensor)\".... ",
    "chenxiaodanhit": "Which line of code?\nLet me explain a little bit more, I've spent couple of days read your code and I have two questions:\n\nDoes RPN only use resnet's conv4 when calculating label and box in this function? I didn't find any anchors in this line 'rpn_label_logits, rpn_box_logits = rpn_head('rpn', featuremap, cfg.RPN.HEAD_DIM, cfg.RPN.NUM_ANCHOR) '. And I thought this line constructed the basic model of RPN head.\n\ndef rpn_head(featuremap, channel, num_anchors): #featuremap 1024 15\n    \"\"\"\n    Returns:\n        label_logits: fHxfWxNA\n        box_logits: fHxfWxNAx4\n    \"\"\"\n    with argscope(Conv2D, data_format='channels_first',\n                  kernel_initializer=tf.random_normal_initializer(stddev=0.01)):\n        hidden = Conv2D('conv0', featuremap, channel, 3, activation=tf.nn.relu) #1024\u4e2a3*3\u7684\u5377\u79ef\u6838\u5bf9resnet\u7684\u8f93\u51faconv4\u8fdb\u884c\u5377\u79ef\n    label_logits = Conv2D('class', hidden, num_anchors, 1) #\u521b\u5efa\u5206\u7c7b\u7684\u5377\u79ef\u7f51\u7edc\n    box_logits = Conv2D('box', hidden, 4 * num_anchors, 1) #\u521b\u5efa\u56de\u5f52\u7684\u5377\u79ef\u7f51\u7edc\n    # 1, NA(*4), im/16, im/16 (NCHW)\n\n    label_logits = tf.transpose(label_logits, [0, 2, 3, 1])  # 1xfHxfWxNA\n    label_logits = tf.squeeze(label_logits, 0)  # fHxfWxNA\n\n    shp = tf.shape(box_logits)  # 1x(NAx4)xfHxfW\n    box_logits = tf.transpose(box_logits, [0, 2, 3, 1])  # 1xfHxfWx(NAx4)\n    box_logits = tf.reshape(box_logits, tf.stack([shp[2], shp[3], num_anchors, 4]))  # fHxfWxNAx4\nreturn label_logits, box_logits\n\n\nI'm also wondering the function of the code below.\nIs this code only make a little adjustment to the RPN bounding box?\n\npred_boxes_decoded = anchors.decode_logits(rpn_box_logits)\nI'm a \u83dc\u9e1f and thanks for your time!. Hi ppwwyyxx,\n  1.In the training pharse, I only modified dilation_rate in model_rpn.py(rpn_head function): hidden = Conv2D('conv0', featuremap, channel, 3, dilation_rate=2, activation=tf.nn.relu, padding='SAME').But it went wrong in  ### valid_label_logits = tf.boolean_mask(label_logits, valid_mask)### in model_rpn.py(rpn_losses function),and the error message is InvalidArgumentError (see above for traceback): Input to reshape is a tensor with 11400 values, but the requested shape has 196608.\n  2.I try to use session to print the value of multilevel_anchors and multilevel_label_logits in model_fpn.py(multilevel_rpn_losses function),which are input into the rpn_losses.Here is my print process, but there is nothing printed:\nwith tf.device('/gpu:4'):\n            #a = tf.Variable([1,2,3,4,5])\n            print('\u5728multilevel_rpn_losses\u4e2d\u6253\u5370tensor.....................................')\n            config = tf.ConfigProto(allow_soft_placement=True)\n            config.gpu_options.allow_growth = True\n            sess = tf.Session(config=config)\n            print(sess.run(multilevel_anchors[0]))\n            sess.close()\nThank you for your patience.. \n1. What I did:\n(1) This is the command I run: nohup ./train.py --config MODE_MASK=True MODE_FPN=True DATA.BASEDIR=/COCO/DIR BACKBONE.WEIGHTS=ImageNet-R50-AlignPadding.npz &\n(2) If you're using examples, have you made any changes to the examples? Paste git diff here:\nI'm using examples/FasterRcnn, and in the code I only changed one line as below: \n(mode_rpn.py in rpn_head() function)\nchange \nhidden = Conv2D('conv0', featuremap, channel, 3, activation=tf.nn.relu)\nto\nhidden = Conv2D('conv0', featuremap, channel, 3, dilation_rate=2, activation=tf.nn.relu, padding='SAME')\n2. What you observed:\n(1) Include the ENTIRE logs here:\nHere is the logs that I observed that is different from before:\n\n\n\n\n(2) Other observations, if any:\nI used two GPUs(GTX 1080)\n3. What you expected, if not obvious.\nWhat we want is to try to use atrous convolution to see anything change, we konw that atrous convlution has larger recption feild without needing redundant variables.\n4. Your environment:\n\nPython version: 3.6\nTF version: 1.9\nTensorpack version: latest version.\n      You can install Tensorpack master by pip install -U git+https://github.com/ppwwyyxx/tensorpack.git\n      and see if your issue is already solved.\nIf you're not using tensorpack under a normal command line shell (e.g.,\n    using an IDE or jupyter notebook), please retry under a normal command line shell. \nHardware information: two GPUS\nYou may often want to provide extra information related to your issue, but\nat the minimum please try to provide the above information accurately to save effort in the investigation.\n. Thank you very much for your instruction!. \n",
    "technicaldebt7": "Thanks for the reply!\nDo you have any recommendations for other ways of optimizing the frozen graph?. ",
    "scott89": "Oh, I see. It's a historical commit. Thanks.. ",
    "vsuthichai": "I see, thank you.. ",
    "chowkamlee81": "I didnt got the answer which is desired. KIndly let s know how can i finetune flownet algo with new dataset? Kindly suggest. ",
    "jheffez": "uhmm... ",
    "JonasAmrich": "Great, thanks for a quick reply!. ",
    "JamesBrightman": "\nThe argument --load should take a path to model, not graph.\nIf your directory does not contain a model, it may be just the training job haven't finished one epoch and saved one.\n\nAh I see, I didn't realise that a model was only saved after each epoch. It takes me an estimated 9 hours to train 1 epoch. If say I can only train maximum 4 hours continuously, would you recommend drastically lowering the num of steps per epoch? And would this effect training in any significant way?\nAppreciate the very fast response! . Then I must be doing something extremely wrong - I have it running on google colab in GPU accelerated mode and the time to complete 1 6000 step epoch is ~8 hours still. It sounds like it's still training on CPU somehow.. Fixes that issue, thanks. Doesn't fix the fact that it's forecast to train 1 epoch in over 10 hours with an it/s speed of ~10. Any thoughts on why this might be?. Yeah. that's exactly why i'm confused!\nI just lowered it to 1000 STEPS_PER_EPOCH (to see if anything would change) and as you can see for them attached pic, I have 10 it/s but apparently, it will still take nearly 3 hours. \n\n. welp, my bad. As for the GPU training it seems you're right if it's as long as 10s/it. Looking though the messages I do get this;\n[0211 18:01:46 @gpu.py:43] WRN Found non-empty CUDA_VISIBLE_DEVICES. But TensorFlow was not built with CUDA support!\nwhen I run it with;\n!python train-atari.py --env Breakout-v0 --gpu 1 or \n!python train-atari.py --env Breakout-v0 --gpu 0\nIs there a different set up that is needed for training in tensorflow to allow GPU training? . Ok thanks, I'll look to change tensorflow version. If anyone runs into this problem my current Colab library versions are;\ntensor2tensor            1.11.0               \ntensorboard              1.12.2               \ntensorboardcolab         0.0.22               \ntensorflow               1.13.0rc0            \ntensorflow-estimator     1.10.12              \ntensorflow-hub           0.2.0                \ntensorflow-metadata      0.9.0                \ntensorflow-probability   0.5.0                \ntensorflow-tensorboard   1.5.1                \ntensorpack               0.9.1   \ntf-estimator-nightly     1.14.0.dev2019021101 \ntf-nightly               1.13.0.dev20190208   \ntfds-nightly             0.0.2.dev201902070013. Ah ok, and I assume this means removing traces of non-gpu tensorflow libraries? eg/ removing \ntensorflow-estimator     1.10.12              \ntensorflow-hub           0.2.0                \ntensorflow-metadata      0.9.0                \ntensorflow-probability   0.5.0                \ntensorflow-tensorboard   1.5.1\ntf-estimator-nightly     1.14.0.dev2019021101 \ntf-nightly               1.13.0.dev20190208\n?. This was a LOT harder than I expected, I feel like i've been install and uninstalling packages for hours. I'll paste a list of all colab library versions so that if anyone else has this problem hopefully this will solve it.\nCurrently running at ~3s/it (not 3it/s! haha). \nAppreciate all the help, this stuff is hard to learn as a beginner! \n```\nPackage                  Version              \n\nabsl-py                  0.7.0              \nalabaster                0.7.12             \nalbumentations           0.1.12             \naltair                   2.3.0              \nastor                    0.7.1              \nastropy                  3.0.5              \natari-py                 0.1.7              \natomicwrites             1.3.0              \nattrs                    18.2.0             \naudioread                2.1.6              \nautograd                 1.2                \nBabel                    2.6.0              \nbackports.tempfile       1.0                \nbackports.weakref        1.0.post1          \nbatchglm                 0.4.1              \nbeautifulsoup4           4.6.3              \nbleach                   1.5.0              \nbokeh                    1.0.4              \nboto                     2.49.0             \nboto3                    1.9.89             \nbotocore                 1.12.89            \nBottleneck               1.2.1              \nbs4                      0.0.1              \nbz2file                  0.98               \ncachetools               3.1.0              \ncertifi                  2018.11.29         \ncffi                     1.11.5             \ncftime                   1.0.3.4            \nchainer                  5.0.0              \nchardet                  3.0.4              \nClick                    7.0                \ncloudpickle              0.6.1              \ncmake                    3.12.0             \ncolorlover               0.3.0              \ncommunity                1.0.0b1            \ncontextlib2              0.5.5              \nconvertdate              2.1.3              \ncoverage                 3.7.1              \ncoveralls                0.5                \ncrcmod                   1.7                \ncufflinks                0.14.6             \ncupy-cuda100             5.2.0              \ncvxopt                   1.2.3              \ncvxpy                    1.0.14             \ncycler                   0.10.0             \ncymem                    2.0.2              \nCython                   0.29.4             \ncytoolz                  0.9.0.1            \ndaft                     0.0.4              \ndask                     0.20.2             \ndataclasses              0.6                \ndatascience              0.10.6             \ndecorator                4.3.2              \ndefusedxml               0.5.0              \ndiffxpy                  0.4.2              \ndill                     0.2.9              \ndistributed              1.25.3             \nDjango                   2.1.5              \ndlib                     19.16.0            \ndm-sonnet                1.23               \ndocopt                   0.6.2              \ndocutils                 0.14               \ndopamine-rl              1.0.5              \neasydict                 1.9                \necos                     2.0.7.post1        \neditdistance             0.5.2              \nen-core-web-sm           2.0.0              \nentrypoints              0.3                \nenum34                   1.1.6              \nephem                    3.7.6.0            \net-xmlfile               1.0.1              \nfa2                      0.3.5              \nfancyimpute              0.4.2              \nfastai                   1.0.42             \nfastcache                1.0.2              \nfastdtw                  0.3.2              \nfastprogress             0.1.18             \nfastrlock                0.4                \nfbprophet                0.4.post2          \nfeaturetools             0.4.1              \nfilelock                 3.0.10             \nfix-yahoo-finance        0.0.22             \nFlask                    1.0.2              \nfolium                   0.2.1              \nfuture                   0.16.0             \ngast                     0.2.2              \nGDAL                     2.2.2              \ngdown                    3.6.4              \ngensim                   3.6.0              \ngeographiclib            1.49               \ngeopy                    1.17.0             \ngevent                   1.4.0              \ngin-config               0.1.2              \nglob2                    0.6                \ngoogle                   2.0.1              \ngoogle-api-core          1.7.0              \ngoogle-api-python-client 1.6.7              \ngoogle-auth              1.4.2              \ngoogle-auth-httplib2     0.0.3              \ngoogle-auth-oauthlib     0.2.0              \ngoogle-cloud-bigquery    1.8.1              \ngoogle-cloud-core        0.29.1             \ngoogle-cloud-language    1.0.2              \ngoogle-cloud-storage     1.8.0              \ngoogle-cloud-translate   1.3.3              \ngoogle-colab             0.0.1a1            \ngoogle-pasta             0.1.1              \ngoogle-resumable-media   0.3.2              \ngoogleapis-common-protos 1.5.6              \ngoogledrivedownloader    0.3                \ngraph-nets               1.0.2              \ngraphviz                 0.10.1             \ngreenlet                 0.4.15             \ngrpcio                   1.15.0             \ngspread                  3.0.1              \ngspread-dataframe        3.0.2              \ngunicorn                 19.9.0             \ngym                      0.10.11            \nh5netcdf                 0.6.2              \nh5py                     2.8.0              \nHeapDict                 1.0.0              \nholidays                 0.9.9              \nhtml5lib                 0.9999999          \nhttpimport               0.5.16             \nhttplib2                 0.11.3             \nhumanize                 0.5.1              \nhyperopt                 0.1.1              \nideep4py                 2.0.0.post3        \nidna                     2.6                \nimage                    1.5.27             \nimageio                  2.4.1              \nimagesize                1.1.0              \nimbalanced-learn         0.4.3              \nimblearn                 0.0                \nimgaug                   0.2.6              \nimutils                  0.5.2              \ninflect                  2.1.0              \nintel-openmp             2019.0             \nintervaltree             2.1.0              \nipykernel                4.6.1              \nipython                  5.5.0              \nipython-genutils         0.2.0              \nipython-sql              0.3.9              \nipywidgets               7.4.2              \nitsdangerous             1.1.0              \njdcal                    1.4                \njieba                    0.39               \nJinja2                   2.10               \njmespath                 0.9.3              \njoblib                   0.13.1             \njpeg4py                  0.1.4              \njsonschema               2.6.0              \njupyter                  1.0.0              \njupyter-client           5.2.4              \njupyter-console          6.0.0              \njupyter-core             4.4.0              \nkaggle                   1.5.2              \nkapre                    0.1.3.1            \nKeras                    2.2.4              \nKeras-Applications       1.0.7              \nKeras-Preprocessing      1.0.9              \nkeras-vis                0.4.1              \nkiwisolver               1.0.1              \nknnimpute                0.1.0              \nlibrosa                  0.6.2              \nlightgbm                 2.2.3              \nllvmlite                 0.27.0             \nlmdb                     0.94               \nlucid                    0.3.8              \nlunardate                0.2.0              \nlxml                     4.2.6              \nmagenta                  0.3.19             \nMarkdown                 3.0.1              \nMarkupSafe               1.1.0              \nmatplotlib               3.0.2              \nmatplotlib-venn          0.11.5             \nmesh-tensorflow          0.0.5              \nmido                     1.2.6              \nmir-eval                 0.5                \nmissingno                0.4.1              \nmistune                  0.8.4              \nmkl                      2019.0             \nmlxtend                  0.14.0             \nmock                     2.0.0              \nmore-itertools           5.0.0              \nmoviepy                  0.2.3.5            \nmpi4py                   3.0.0              \nmpmath                   1.1.0              \nmsgpack                  0.5.6              \nmsgpack-numpy            0.4.4.2            \nmultiprocess             0.70.7             \nmultitasking             0.0.7              \nmurmurhash               1.0.1              \nmusic21                  5.5.0              \nnatsort                  5.5.0              \nnbconvert                5.4.0              \nnbformat                 4.4.0              \nnetCDF4                  1.4.2              \nnetworkx                 2.2                \nnibabel                  2.3.3              \nnltk                     3.2.5              \nnose                     1.3.7              \nnotebook                 5.2.2              \nnp-utils                 0.5.9.0            \nnumba                    0.40.1             \nnumexpr                  2.6.9              \nnumpy                    1.14.6             \nnvidia-ml-py3            7.352.0            \noauth2client             4.1.3              \noauthlib                 3.0.1              \nokgrade                  0.4.3              \nolefile                  0.46               \nopencv-contrib-python    3.4.3.18           \nopencv-python            3.4.5.20           \nopenpyxl                 2.5.9              \nosqp                     0.5.0              \npackaging                19.0               \npandas                   0.22.0             \npandas-datareader        0.7.0              \npandas-gbq               0.4.1              \npandas-profiling         1.4.1              \npandocfilters            1.4.2              \npathlib                  1.0.1              \npatsy                    0.5.1              \npbr                      5.1.2              \npexpect                  4.6.0              \npickleshare              0.7.5              \nPillow                   4.0.0              \npip                      19.0.1             \nplac                     0.9.6              \nplotly                   1.12.12            \npluggy                   0.8.1              \nportpicker               1.2.0              \nprefetch-generator       1.0.1              \npreshed                  2.0.1              \npretty-midi              0.2.8              \nprettytable              0.7.2              \nprogressbar2             3.38.0             \npromise                  2.2.1              \nprompt-toolkit           1.0.15             \nprotobuf                 3.6.1              \npsutil                   5.4.8              \npsycopg2                 2.7.6.1            \nptyprocess               0.6.0              \npy                       1.7.0              \npyasn1                   0.4.5              \npyasn1-modules           0.2.4              \npycocotools              2.0.0              \npycparser                2.19               \npydot                    1.3.0              \npydot-ng                 2.0.0              \npydotplus                2.0.2              \npyemd                    0.5.1              \npyglet                   1.3.2              \nPygments                 2.1.3              \npygobject                3.26.1             \npymc3                    3.6                \npymongo                  3.7.2              \npymystem3                0.2.0              \nPyOpenGL                 3.1.0              \npyparsing                2.3.1              \npysndfile                1.3.2              \nPySocks                  1.6.8              \npystache                 0.5.4              \npystan                   2.18.1.0           \npytest                   3.10.1             \npython-apt               1.6.3+ubuntu1      \npython-chess             0.23.11            \npython-dateutil          2.5.3              \npython-louvain           0.13               \npython-rtmidi            1.2.1              \npython-slugify           2.0.1              \npython-utils             2.3.0              \npytz                     2018.9             \nPyWavelets               1.0.1              \nPyYAML                   3.13               \npyzmq                    17.0.0             \nqtconsole                4.4.3              \nregex                    2018.1.10          \nrequests                 2.18.4             \nrequests-oauthlib        1.2.0              \nresampy                  0.2.1              \nrpy2                     2.9.5              \nrsa                      4.0                \ns3fs                     0.2.0              \ns3transfer               0.2.0              \nscikit-image             0.13.1             \nscikit-learn             0.20.2             \nscipy                    1.1.0              \nscreen-resolution-extra  0.0.0              \nscs                      2.0.2              \nseaborn                  0.7.1              \nsetuptools               40.8.0             \nsetuptools-git           1.2                \nsimplegeneric            0.8.1              \nsix                      1.11.0             \nsklearn                  0.0                \nsmart-open               1.8.0              \nsnowballstemmer          1.2.1              \nsortedcontainers         2.1.0              \nspacy                    2.0.18             \nSphinx                   1.8.4              \nsphinxcontrib-websupport 1.1.0              \nSQLAlchemy               1.2.17             \nsqlparse                 0.2.4              \nstable-baselines         2.2.1              \nstatsmodels              0.8.0              \nsympy                    1.1.1              \ntables                   3.4.4              \ntabulate                 0.8.3              \ntb-nightly               1.13.0a20190211    \ntblib                    1.3.2              \ntensor2tensor            1.11.0             \ntensorboard              1.12.2             \ntensorboardcolab         0.0.22             \ntensorflow               1.12.0             \ntensorflow-estimator     1.10.12            \ntensorflow-hub           0.2.0              \ntensorflow-metadata      0.9.0              \ntensorflow-probability   0.5.0              \ntensorpack               0.9.1              \ntermcolor                1.1.0              \nterminado                0.8.1              \ntestpath                 0.4.2              \ntextblob                 0.15.2             \ntextgenrnn               1.4.1              \ntf-estimator-nightly     1.14.0.dev2019021101 \ntf-nightly-gpu           1.13.0.dev20190208 \ntfds-nightly             0.0.2.dev201902070013\ntflearn                  0.3.2              \nTheano                   1.0.4              \nthinc                    6.12.1             \ntoolz                    0.9.0              \ntorch                    1.0.0              \ntorchsummary             1.5.1              \ntorchtext                0.3.1              \ntorchvision              0.2.1              \ntornado                  4.5.3              \ntqdm                     4.28.1             \ntraitlets                4.3.2              \ntweepy                   3.6.0              \ntyping                   3.6.6              \ntzlocal                  1.5.1              \nujson                    1.35               \numap-learn               0.3.7              \nUnidecode                1.0.23             \nuritemplate              3.0.0              \nurllib3                  1.22               \nvega-datasets            0.7.0              \nwcwidth                  0.1.7              \nwebencodings             0.5.1              \nWerkzeug                 0.14.1             \nwheel                    0.32.3             \nwidgetsnbextension       3.4.2              \nwordcloud                1.5.0              \nwrapt                    1.11.1             \nxarray                   0.11.3             \nxgboost                  0.7.post4          \nxkit                     0.0.0              \nxlrd                     1.1.0              \nxlwt                     1.3.0              \nyellowbrick              0.9.1              \nzict                     0.1.3              \nzmq                      0.0.0 \n```. Yeah, I'm pleased with how it's all looking, holding 3it/s and targeting a ~20min epoch. . Thanks for the quick change - pulled the master again and all works well.\nRegards,\nJames. ",
    "liuzili97": "\nThis is not true. As you can see in the code, when the current step is after the end of all scheduled point (1000), it will return None, and the parameter will not be changed.\n\nOh you are right. I didn't notice that or laste == e is behind the if laste is None. After I ignore the learning_rate when loading the checkpoint, the problem solved. Thanks!. ",
    "catai": "In the newest version, it actually asks for GPU resources. Could you please double check about this or provide a link of the package that only uses CPU for dataflow? Thanks!. when I open the file: ~/tensorflow-py3.6/lib/python3.6/site-packages/tensorpack/dataflow/parallel.py\nit has this in the beginning: \nfrom ..utils.gpu import change_gpu\nand then:\nall = ['PrefetchData', 'MultiProcessPrefetchData',\n           'PrefetchDataZMQ', 'PrefetchOnGPUs', 'MultiThreadPrefetchData']\nI think this means in the newest version of dataflow, it actually asks for gpu resources. ",
    "chengyw": "\nhttps://github.com/tensorpack/tensorpack/tree/master/examples/ResNet\n\nthank you!\nI have used this one also, but i found the code is different with examples/FasterRCNN backbone.\none use activation but another not. i use my trained resnet model to train FasterRCNN, after some epochs, the cost is NAN. but i use the pre-trained model:http://models.tensorpack.com/FasterRCNN/ImageNet-R50-AlignPadding.npz is ok.\n\n. > The one in FasterRCNN also uses activation:\n\ntensorpack/examples/FasterRCNN/basemodel.py\nLine 76 in d8d35fb\nargscope(Conv2D, use_bias=False, activation=nonlin, \nAs to why it goes to NaN, it is a machine learning question and do not appear to be related to tensorpack.\n\n. ",
    "Yizhouw21": "Thanks for the reply! \nYes, I just realized the shape of elements in ret  returned by preprocess are different among images.\nFor the model, because we are using another model implementation that can handle images > 1, I think that would not be a problem.  And besides those two problems, would the usage of TensorPack dataflow and input source API be correct?\nI will try to find some solutions to solve the shape issue of the input data. Since it is not a TensorPack problem, I will close the issue. But it would be perfect if you can also share some thoughts about it.\nThanks!. Hi, I managed to encode the inputs to the same shape by add some tensor operations in preproc(). And I batched them successfully using `BatchData().\ndef get_train_dataflow(batch_size):\n   ....\n    ds = MapData(ds, preprocess)\n    ds = BatchData(ds, batch_size, use_list=True)\n    return ds\nAs the result the dataflow will produce list of lists of tensors. \n````\nbatch_size = 32\nds.reset_state(batch_size)\nfor k in ds:\n    print(k)\n[0228 19:34:05 @common.py:816] DataFlow Info:\ndatapoint 0<100 with 4 components consists of\n  0: list of len 32\n    0: Tensor\n    1: Tensor\n    2: Tensor\n    ...\n  1: list of len 32\n    0: Tensor\n    1: Tensor\n    2: Tensor\n    ...\n  2: list of len 32\n    0: Tensor\n    1: Tensor\n    2: Tensor\n    ...\n  3: list of len 32\n    0: Tensor\n    1: Tensor\n    2: Tensor\n    ...\n`\nAnd maintained the \u2018Inputs\u2019 method returning a list of `tf.placeholder`. And feed the data with  `QueueInputs()`. But I am getting error like this\n[0228 19:56:10 @base.py:242] Graph Finalized.\n[0228 19:56:10 @concurrency.py:38] Starting EnqueueThread QueueInput/input_queue ...\n[0228 19:56:10 @graph.py:73] Running Op sync_variables/sync_variables_from_main_tower ...\n[0228 19:56:11 @param.py:158] [HyperParamSetter] At global_step=0, learning_rate is set to 0.003300\n[0228 19:56:11 @eval.py:219] [EvalCallback] Will evaluate every 25 epochs\n[0228 19:56:12 @base.py:274] Start Epoch 1 ...\n  0%|                                                                                                                                                                   |0/500[00:00<?,?it/s][0228 19:56:48 @input_source.py:170] ERR Exception in EnqueueThread QueueInput/input_queue:\nTraceback (most recent call last):\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/tensorpack/input_source/input_source.py\", line 162, in run\n    self.op.run(feed_dict=feed)\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2377, in run\n    _run_using_default_session(self, feed_dict, self.graph, session)\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 5215, in _run_using_default_session\n    session.run(operation, feed_dict)\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 900, in run\n    run_metadata_ptr)\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1104, in _run\n    np_val = np.asarray(subfeed_val, dtype=subfeed_dtype)\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py\", line 492, in asarray\n    return array(a, dtype, copy=False, order=order)\nValueError: setting an array element with a sequence.\n[0228 19:56:48 @input_source.py:176] EnqueueThread QueueInput/input_queue Exited.\n[0228 19:56:48 @base.py:290] Training was stopped by exception FIFOQueue '_0_QueueInput/input_queue' is closed and has insufficient elements (requested 1, current size 0)\n         [[Node: QueueInput/input_deque = QueueDequeueV2component_types=[DT_FLOAT, DT_INT64, DT_FLOAT, DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]]\n         [[Node: apply_gradients/AccumGradOptimizer/apply_grad_0/Momentum/update_group1/block0/conv1/W/ApplyMomentum/_624 = _Recvclient_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_7355_...lyMomentum\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]]\nCaused by op 'QueueInput/input_deque', defined at:\n  File \"train_ssd.py\", line 451, in \n    launch_train_with_config(traincfg, trainer)\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/tensorpack/train/interface.py\", line 84, in launch_train_with_config\n    model._build_graph_get_cost, model.get_optimizer)\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/tensorpack/utils/argtools.py\", line 176, in wrapper\n    return func(*args, kwargs)\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/tensorpack/train/tower.py\", line 214, in setup_graph\n    train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/tensorpack/train/trainers.py\", line 193, in _setup_graph\n    grad_list = self._builder.call_for_each_tower(tower_fn)\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/tensorpack/graph_builder/training.py\", line 225, in call_for_each_tower\n    use_vs=[False] + [True] * (len(self.towers) - 1))\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/tensorpack/graph_builder/training.py\", line 121, in build_on_towers\n    return DataParallelBuilder.call_for_each_tower(*args, kwargs)\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/tensorpack/graph_builder/training.py\", line 116, in call_for_each_tower\n    ret.append(func())\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/tensorpack/train/tower.py\", line 242, in get_grad_fn\n    inputs = input.get_input_tensors()\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/tensorpack/input_source/input_source_base.py\", line 82, in get_input_tensors\n    return self._get_input_tensors()\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/tensorpack/input_source/input_source.py\", line 267, in _get_input_tensors\n    ret = self.queue.dequeue(name='input_deque')\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/data_flow_ops.py\", line 440, in dequeue\n    self._queue_ref, self._dtypes, name=name)\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 3734, in queue_dequeue_v2\n    timeout_ms=timeout_ms, name=name)\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in init\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\nOutOfRangeError (see above for traceback): FIFOQueue '_0_QueueInput/input_queue' is closed and has insufficient elements (requested 1, current size 0)\n         [[Node: QueueInput/input_deque = QueueDequeueV2component_types=[DT_FLOAT, DT_INT64, DT_FLOAT, DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]]\n         [[Node: apply_gradients/AccumGradOptimizer/apply_grad_0/Momentum/update_group1/block0/conv1/W/ApplyMomentum/_624 = _Recvclient_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_7355_...lyMomentum\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]]\n.\n``\nI am not sure if this is caused by wrong usages of TensorPack functions. But it would be perfect if you can share some thoughts to figure out the bug.\nThanks very much! . Thanks for the quick reply! I trieduse_list = False. I think the problem is that ourPreproc()function returns a list ofTensor, butBatchDataandnumpycannot undertandTensor` elements, rather than the inconstant shapes. \n````\ndef get_train_dataflow(batch_size):\n   ....\n    ds = MapData(ds, preprocess)\n    ds = BatchData(ds, batch_size, use_list=True)\n    return ds\nbatch_size = 32\nds.reset_state(batch_size)\nfor k in ds:\n    print(k)\n`\n[0301 12:06:49 @common.py:142] ERR Cannot batch data. Perhaps they are of inconsistent shape?\nTraceback (most recent call last):\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/tensorpack/dataflow/common.py\", line 140, in _batch_numpy\n    return np.asarray(data_list, dtype=dtype)\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py\", line 492, in asarray\n    return array(a, dtype, copy=False, order=order)\nTypeError: data type not understood\nPython 3.6.4 |Anaconda custom (64-bit)| (default, Jan 16 2018, 18:10:19)\nType 'copyright', 'credits' or 'license' for more information\nIPython 6.2.1 -- An enhanced Interactive Python. Type '?' for help.\n````. ",
    "dcyoung": "Thanks for the confirmation. Placing changes in the mapped preprocess method worked just fine.. Thanks for the quick response. \nBy \"trivial\" values i meant very small (to test that this wasn't an issue related to epoch transitions or epoch-based callbacks). More specifically I tried with TRAIN.STEPS_PER_EPOCH=5 and TRAIN.EVAL_PERIOD=3, and observed the training session run for >40 epochs without issue before manually killing it.\nMy first thought would be that the issue is related to a rare data sample which wasn't encountered in that time. However I encountered no issues looping through the entirety of both training and eval dataflows. \nI tried removing callbacks to identify the culprit but no such luck.\nChecking a resource monitor, it appears that main memory is filling up slowly during training, and hitting 100% (64GB) at the time of crash.... likely a slow down, which then causes a timeout via SessionRunTimeout. I'll investigate the source of the leak.\n. Training from a fresh tensorpack repo using COCO data results in stable memory consumption around ~40 GBs or so. With my own data, I could not avoid memory saturation when using MultiProcessMapDataZMQ. I switched to MapData instead which sits stable at ~17Gb. I'm closing for now as this is likely something specific to my own data preprocessing. Thanks again.. @ppwwyyxx  thanks for the suggestion.\nI experimented with different params in MultiProcessMapDataZMQ. However, even with small values (nr_proc=2 + buffer_size=5), I still saw rising memory usage until overflow during training.\nI know referring to exact numbers is difficult with custom data... but do the following relative values make sense? That is to say, is it expected that a call like:\npython\nds = MultiProcessMapDataZMQ(ds, nr_proc=10, map_func=preprocess, buffer_size=5)\ncould yield > 64GB of memory usage during training, while the following call:\npython\nds = MapData(ds, preprocess)\nyields a stable ~17Gb.\nAdditionally, why is it that memory usage would rise during training, but not when testing the dataflow in isolation, like so:\n```python\nds = get_train_dataflow()\nds = PrintData(ds, 100)\nTestDataSpeed(ds, 50000).start()\nds.reset_state()\nfor k in ds:\n    pass\n```. Given that in all cases, testing the dataflow directly does not show rising memory usage, I would have assumed the same. However I'm witnessing the following, when testing on my own local machine:\n1) Default Example\nTraining from a fresh tensorpack repo using COCO data.\nResults in stable memory consumption around ~40 GB or so during training. \n2) Training with Custom dataflow, MultiProcessMapDataZMQ\nUnmodified code, apart from a custom dataflow.\nResults in memory overflow > 64GB, regardless of params supplied to MultiProcessMapDataZMQ\n3) Training with Custom dataflow, MapData\nUnmodified code, apart from a custom dataflow. MultiProcessMapDataZMQ is replaced with MapData.\nResults in staple memory consumption around ~17GB  or so during training.\n4) Training with Custom dataflow, MultiProcessMapDataZMQ, Docker\nRepeat of 2 but running in a docker container.\nResults in stable memory consumption < 20GB\nGiven the following:\n- large difference in memory consumption between your experiment and my own using the default example (case 1 above) \n- only difference between the above cases 2 and 3, is the map function, and yet they each yield dramatically different memory usage during training\n- only difference between 2 and 4 is my local os vs fresh ubuntu in a docker container\nI would think there could be something unexpected about how my system is running MultiProcessMapDataZMQ. Any recommendations for further debugging?. Edit: added 4th case above, which I think verifies this is related to my system, so I'll close the issue again... . I see. It seems I haven't done my homework on Group Normalization. I had a fundamental misunderstanding that it could be ignored without implications to the rest of the model, similar to BN. Thank you for the clarification! I appreciate it.\nI was a little surprised to see the difference in inference performance between GN and non-GN models. Moving from a standard FPN model to a GN-FPN model, I'm seeing an average increase of ~70% in inference time.  Further reading suggests this isn't outlandish.\nFor now I will predict with a GN config, until i have retrained without GN. \nThanks again!. ",
    "johnjamin11": "Hey, \nYeah, I tested again with the muliple of 16 as you mentioned but still having the same problem. \nAlways has the mismatch in the end. \nIn the original code, you've divided height and width with a value of 16 x 16 (on line 282). \nI don't think this is the problem of // or /. \nThis line is really not changing the shape of an image, rather it just adds one dimension to fit it onto input tensor. But when I modify the code in a way that will actually change the values of width and height, then the error is there. Could you please check on this?\n. You are totally right. I cannot believe I missed the simple thing..\nNow the problem is completely solved. Thank you very much.\n. ",
    "zxpeter": "Or could you please just simply tell me in tensorpack/example/fasterrcnn, do we use loss control instead of only use TRAIN.LR_SCHEDULE and TRAIN.STEPS_PER_EPOCH to control training step?. Sorry for the late reply, the updated question as above.. ",
    "bairdzhang": "It seems swap x and y does not change the behaviour of NMS?. ",
    "liyinglr": "@ppwwyyxx . I updated more information, please take a look. ",
    "lianyingteng": "This URL is not available. tks. I found the problem of batch size. Previously, each gpu batch_size was 64, now it is 32. It can run very well.. ",
    "YJHMITWEB": "I did not make any other changes to any files under the example path.\nIt's weird.... Just that I have used tensorflow to develop for like more than 3 years, but haven't meet this kind of thing. \nMaybe there are some implicit problems in my machine, really have no idea...\nBut right now, I could just restart it every 220 epochs, it won't make any trouble.. Sure, I just upgrade tensorflow to 1.12, and now testing whether the problem still exists or gone.. Unfortunately, the problem still exists..\nEvery time after it runs abut 220 epochs, the whole process stops.\nI start from scratch, 0 epoch, and it stops at 220 epochs,\nthen restart it, load the last checkpoint, start from 220, then it stops again at 439 epochs,\nthen 648 epochs...\nAlso, I have found that as the training processes, the memory usage gradually increases (about 0.3~0.4% per epoch of a total 256 GB memory). I'm not sure if it's because of some dynamic nodes defined in the graph, and too many of them are generated after a long-time graph running, which may consume too much memory, leading to the crash.... Ok, I will modify the data.py and test again.\nBut it's still weird, unlike #1097, in my case, there is no error raising.. ",
    "memeda2232": "\n\ni changed configs like this  ,but you can see it doesnt changed. \nsorry for bother you again.. it stopped  there for more than ten minutes ,what is the problem?. [0318 10:37:00 @logger.py:87] Argv: train.py\n[0318 10:37:00 @config.py:279] Config: ------------------------------------------\n{'BACKBONE': {'FREEZE_AFFINE': False,\n              'FREEZE_AT': 2,\n              'NORM': 'None',\n              'RESNET_NUM_BLOCKS': [3, 4, 6, 3],\n              'STRIDE_1X1': False,\n              'TF_PAD_MODE': False,\n              'WEIGHTS': '/home/zysz1/FasterRCNN/weights/ImageNet-R50-AlignPadding.npz'},\n 'CASCADE': {'BBOX_REG_WEIGHTS': [[10.0, 10.0, 5.0, 5.0], [20.0, 20.0, 10.0, 10.0],\n                                  [30.0, 30.0, 15.0, 15.0]],\n             'IOUS': [0.5, 0.6, 0.7]},\n 'DATA': {'ABSOLUTE_COORD': False,\n          'BASEDIR': '/home/zysz1/FasterRCNN/coco',\n          'CLASS_NAMES': ['BG', 'tiekedahuoji', 'heidingdahuoji', 'daoju', 'dianyuanhedianchi',\n                          'jiandao'],\n          'NUM_CATEGORY': 5,\n          'NUM_CLASS': 6,\n          'TRAIN': 'train',\n          'VAL': ('val',)},\n 'FPN': {'ANCHOR_STRIDES': (4, 8, 16, 32, 64),\n         'CASCADE': False,\n         'FRCNN_CONV_HEAD_DIM': 256,\n         'FRCNN_FC_HEAD_DIM': 1024,\n         'FRCNN_HEAD_FUNC': 'fastrcnn_2fc_head',\n         'MRCNN_HEAD_FUNC': 'maskrcnn_up4conv_head',\n         'NORM': 'None',\n         'NUM_CHANNEL': 256,\n         'PROPOSAL_MODE': 'Level',\n         'RESOLUTION_REQUIREMENT': 32},\n 'FRCNN': {'BATCH_PER_IM': 512,\n           'BBOX_REG_WEIGHTS': [10.0, 10.0, 5.0, 5.0],\n           'FG_RATIO': 0.25,\n           'FG_THRESH': 0.5},\n 'MODE_FPN': True,\n 'MODE_MASK': False,\n 'MRCNN': {'HEAD_DIM': 256},\n 'PREPROC': {'MAX_SIZE': 1344.0,\n             'PIXEL_MEAN': [123.675, 116.28, 103.53],\n             'PIXEL_STD': [58.395, 57.12, 57.375],\n             'TEST_SHORT_EDGE_SIZE': 800,\n             'TRAIN_SHORT_EDGE_SIZE': [800, 800]},\n 'RPN': {'ANCHOR_RATIOS': (0.5, 1.0, 2.0),\n         'ANCHOR_SIZES': (32, 64, 128, 256, 512),\n         'ANCHOR_STRIDE': 16,\n         'BATCH_PER_IM': 256,\n         'CROWD_OVERLAP_THRESH': 9.99,\n         'FG_RATIO': 0.5,\n         'HEAD_DIM': 1024,\n         'MIN_SIZE': 0,\n         'NEGATIVE_ANCHOR_THRESH': 0.3,\n         'NUM_ANCHOR': 15,\n         'POSITIVE_ANCHOR_THRESH': 0.7,\n         'PROPOSAL_NMS_THRESH': 0.7,\n         'TEST_PER_LEVEL_NMS_TOPK': 1000,\n         'TEST_POST_NMS_TOPK': 1000,\n         'TEST_PRE_NMS_TOPK': 6000,\n         'TRAIN_PER_LEVEL_NMS_TOPK': 2000,\n         'TRAIN_POST_NMS_TOPK': 2000,\n         'TRAIN_PRE_NMS_TOPK': 12000},\n 'TEST': {'FRCNN_NMS_THRESH': 0.5,\n          'RESULTS_PER_IM': 100,\n          'RESULT_SCORE_THRESH': 0.05,\n          'RESULT_SCORE_THRESH_VIS': 0.5},\n 'TRAIN': {'BASE_LR': 0.01,\n           'EVAL_PERIOD': 0,\n           'LR_SCHEDULE': [240000, 320000, 360000],\n           'NUM_GPUS': 1,\n           'STARTING_EPOCH': 0,\n           'STEPS_PER_EPOCH': 500,\n           'WARMUP': 1000,\n           'WARMUP_INIT_LR': 0.0033000000000000004,\n           'WEIGHT_DECAY': 0.0001},\n 'TRAINER': 'replicated'}\n[0318 10:37:00 @train.py:472] Warm Up Schedule (steps, value): [(0, 0.0033000000000000004), (1000, 0.01)]\n[0318 10:37:00 @train.py:473] LR Schedule (epochs, value): [(2, 0.01), (3840.0, 0.001), (5120.0, 0.00010000000000000002)]\nloading annotations into memory...\nDone (t=0.18s)\ncreating index...\nindex created!\n[0318 10:37:00 @dataset.py:45] Instances loaded from /home/zysz1/FasterRCNN/coco/annotations/instances_train.json.\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 981/981 [00:00<00:00, 2638.87it/s]\n[0318 10:37:00 @timer.py:48] Load Groundtruth Boxes for train finished, time:0.3739sec.\n[0318 10:37:00 @data.py:49] Ground-Truth Boxes:\n| class             |   #box |\n|:------------------|-------:|\n| BG                |      0 |\n| tiekedahuoji      |    317 |\n| heidingdahuoji    |   1729 |\n| daoju             |    789 |\n| dianyuanhedianchi |   1783 |\n| jiandao           |    697 |\n| total             |   5315 |\n[0318 10:37:00 @data.py:294] Filtered 4 images which contain no non-crowd groudtruth boxes. Total #images for training: 977\n[0318 10:37:00 @train.py:477] Total passes of the training set is: 2947.8\n[0318 10:37:02 @input_source.py:220] Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...\n[0318 10:37:02 @training.py:109] Building graph for training tower 0 on device /gpu:0 ...\n[0318 10:37:02 @registry.py:125] conv0 input: [1, 3, None, None]\n[0318 10:37:02 @registry.py:133] conv0 output: [1, 64, None, None]\n[0318 10:37:02 @registry.py:125] pool0 input: [1, 64, None, None]\n[0318 10:37:02 @registry.py:133] pool0 output: [1, 64, None, None]\n[0318 10:37:02 @registry.py:125] group0/block0/conv1 input: [1, 64, None, None]\n[0318 10:37:02 @registry.py:133] group0/block0/conv1 output: [1, 64, None, None]\n[0318 10:37:02 @registry.py:125] group0/block0/conv2 input: [1, 64, None, None]\n[0318 10:37:02 @registry.py:133] group0/block0/conv2 output: [1, 64, None, None]\n[0318 10:37:02 @registry.py:125] group0/block0/conv3 input: [1, 64, None, None]\n[0318 10:37:02 @registry.py:133] group0/block0/conv3 output: [1, 256, None, None]\n[0318 10:37:02 @registry.py:125] group0/block0/convshortcut input: [1, 64, None, None]\n[0318 10:37:02 @registry.py:133] group0/block0/convshortcut output: [1, 256, None, None]\n[0318 10:37:02 @registry.py:125] group0/block1/conv1 input: [1, 256, None, None]\n[0318 10:37:02 @registry.py:133] group0/block1/conv1 output: [1, 64, None, None]\n[0318 10:37:02 @registry.py:125] group0/block1/conv2 input: [1, 64, None, None]\n[0318 10:37:02 @registry.py:133] group0/block1/conv2 output: [1, 64, None, None]\n[0318 10:37:02 @registry.py:125] group0/block1/conv3 input: [1, 64, None, None]\n[0318 10:37:02 @registry.py:133] group0/block1/conv3 output: [1, 256, None, None]\n[0318 10:37:02 @registry.py:125] group0/block2/conv1 input: [1, 256, None, None]\n[0318 10:37:02 @registry.py:133] group0/block2/conv1 output: [1, 64, None, None]\n[0318 10:37:02 @registry.py:125] group0/block2/conv2 input: [1, 64, None, None]\n[0318 10:37:02 @registry.py:133] group0/block2/conv2 output: [1, 64, None, None]\n[0318 10:37:02 @registry.py:125] group0/block2/conv3 input: [1, 64, None, None]\n[0318 10:37:02 @registry.py:133] group0/block2/conv3 output: [1, 256, None, None]\n[0318 10:37:02 @registry.py:125] group1/block0/conv1 input: [1, 256, None, None]\n[0318 10:37:02 @registry.py:133] group1/block0/conv1 output: [1, 128, None, None]\n[0318 10:37:02 @registry.py:125] group1/block0/conv2 input: [1, 128, None, None]\n[0318 10:37:02 @registry.py:133] group1/block0/conv2 output: [1, 128, None, None]\n[0318 10:37:02 @registry.py:125] group1/block0/conv3 input: [1, 128, None, None]\n[0318 10:37:02 @registry.py:133] group1/block0/conv3 output: [1, 512, None, None]\n[0318 10:37:02 @registry.py:125] group1/block0/convshortcut input: [1, 256, None, None]\n[0318 10:37:02 @registry.py:133] group1/block0/convshortcut output: [1, 512, None, None]\n[0318 10:37:02 @registry.py:125] group1/block1/conv1 input: [1, 512, None, None]\n[0318 10:37:02 @registry.py:133] group1/block1/conv1 output: [1, 128, None, None]\n[0318 10:37:02 @registry.py:125] group1/block1/conv2 input: [1, 128, None, None]\n[0318 10:37:02 @registry.py:133] group1/block1/conv2 output: [1, 128, None, None]\n[0318 10:37:02 @registry.py:125] group1/block1/conv3 input: [1, 128, None, None]\n[0318 10:37:02 @registry.py:133] group1/block1/conv3 output: [1, 512, None, None]\n[0318 10:37:02 @registry.py:125] group1/block2/conv1 input: [1, 512, None, None]\n[0318 10:37:02 @registry.py:133] group1/block2/conv1 output: [1, 128, None, None]\n[0318 10:37:02 @registry.py:125] group1/block2/conv2 input: [1, 128, None, None]\n[0318 10:37:02 @registry.py:133] group1/block2/conv2 output: [1, 128, None, None]\n[0318 10:37:02 @registry.py:125] group1/block2/conv3 input: [1, 128, None, None]\n[0318 10:37:02 @registry.py:133] group1/block2/conv3 output: [1, 512, None, None]\n[0318 10:37:02 @registry.py:125] group1/block3/conv1 input: [1, 512, None, None]\n[0318 10:37:02 @registry.py:133] group1/block3/conv1 output: [1, 128, None, None]\n[0318 10:37:02 @registry.py:125] group1/block3/conv2 input: [1, 128, None, None]\n[0318 10:37:02 @registry.py:133] group1/block3/conv2 output: [1, 128, None, None]\n[0318 10:37:02 @registry.py:125] group1/block3/conv3 input: [1, 128, None, None]\n[0318 10:37:02 @registry.py:133] group1/block3/conv3 output: [1, 512, None, None]\n[0318 10:37:02 @registry.py:125] group2/block0/conv1 input: [1, 512, None, None]\n[0318 10:37:02 @registry.py:133] group2/block0/conv1 output: [1, 256, None, None]\n[0318 10:37:02 @registry.py:125] group2/block0/conv2 input: [1, 256, None, None]\n[0318 10:37:02 @registry.py:133] group2/block0/conv2 output: [1, 256, None, None]\n[0318 10:37:02 @registry.py:125] group2/block0/conv3 input: [1, 256, None, None]\n[0318 10:37:02 @registry.py:133] group2/block0/conv3 output: [1, 1024, None, None]\n[0318 10:37:02 @registry.py:125] group2/block0/convshortcut input: [1, 512, None, None]\n[0318 10:37:02 @registry.py:133] group2/block0/convshortcut output: [1, 1024, None, None]\n[0318 10:37:02 @registry.py:125] group2/block1/conv1 input: [1, 1024, None, None]\n[0318 10:37:02 @registry.py:133] group2/block1/conv1 output: [1, 256, None, None]\n[0318 10:37:02 @registry.py:125] group2/block1/conv2 input: [1, 256, None, None]\n[0318 10:37:02 @registry.py:133] group2/block1/conv2 output: [1, 256, None, None]\n[0318 10:37:02 @registry.py:125] group2/block1/conv3 input: [1, 256, None, None]\n[0318 10:37:02 @registry.py:133] group2/block1/conv3 output: [1, 1024, None, None]\n[0318 10:37:02 @registry.py:125] group2/block2/conv1 input: [1, 1024, None, None]\n[0318 10:37:02 @registry.py:133] group2/block2/conv1 output: [1, 256, None, None]\n[0318 10:37:02 @registry.py:125] group2/block2/conv2 input: [1, 256, None, None]\n[0318 10:37:02 @registry.py:133] group2/block2/conv2 output: [1, 256, None, None]\n[0318 10:37:02 @registry.py:125] group2/block2/conv3 input: [1, 256, None, None]\n[0318 10:37:02 @registry.py:133] group2/block2/conv3 output: [1, 1024, None, None]\n[0318 10:37:02 @registry.py:125] group2/block3/conv1 input: [1, 1024, None, None]\n[0318 10:37:02 @registry.py:133] group2/block3/conv1 output: [1, 256, None, None]\n[0318 10:37:02 @registry.py:125] group2/block3/conv2 input: [1, 256, None, None]\n[0318 10:37:02 @registry.py:133] group2/block3/conv2 output: [1, 256, None, None]\n[0318 10:37:02 @registry.py:125] group2/block3/conv3 input: [1, 256, None, None]\n[0318 10:37:02 @registry.py:133] group2/block3/conv3 output: [1, 1024, None, None]\n[0318 10:37:02 @registry.py:125] group2/block4/conv1 input: [1, 1024, None, None]\n[0318 10:37:02 @registry.py:133] group2/block4/conv1 output: [1, 256, None, None]\n[0318 10:37:02 @registry.py:125] group2/block4/conv2 input: [1, 256, None, None]\n[0318 10:37:02 @registry.py:133] group2/block4/conv2 output: [1, 256, None, None]\n[0318 10:37:02 @registry.py:125] group2/block4/conv3 input: [1, 256, None, None]\n[0318 10:37:02 @registry.py:133] group2/block4/conv3 output: [1, 1024, None, None]\n[0318 10:37:02 @registry.py:125] group2/block5/conv1 input: [1, 1024, None, None]\n[0318 10:37:02 @registry.py:133] group2/block5/conv1 output: [1, 256, None, None]\n[0318 10:37:02 @registry.py:125] group2/block5/conv2 input: [1, 256, None, None]\n[0318 10:37:02 @registry.py:133] group2/block5/conv2 output: [1, 256, None, None]\n[0318 10:37:02 @registry.py:125] group2/block5/conv3 input: [1, 256, None, None]\n[0318 10:37:02 @registry.py:133] group2/block5/conv3 output: [1, 1024, None, None]\n[0318 10:37:02 @registry.py:125] group3/block0/conv1 input: [1, 1024, None, None]\n[0318 10:37:02 @registry.py:133] group3/block0/conv1 output: [1, 512, None, None]\n[0318 10:37:02 @registry.py:125] group3/block0/conv2 input: [1, 512, None, None]\n[0318 10:37:03 @registry.py:133] group3/block0/conv2 output: [1, 512, None, None]\n[0318 10:37:03 @registry.py:125] group3/block0/conv3 input: [1, 512, None, None]\n[0318 10:37:03 @registry.py:133] group3/block0/conv3 output: [1, 2048, None, None]\n[0318 10:37:03 @registry.py:125] group3/block0/convshortcut input: [1, 1024, None, None]\n[0318 10:37:03 @registry.py:133] group3/block0/convshortcut output: [1, 2048, None, None]\n[0318 10:37:03 @registry.py:125] group3/block1/conv1 input: [1, 2048, None, None]\n[0318 10:37:03 @registry.py:133] group3/block1/conv1 output: [1, 512, None, None]\n[0318 10:37:03 @registry.py:125] group3/block1/conv2 input: [1, 512, None, None]\n[0318 10:37:03 @registry.py:133] group3/block1/conv2 output: [1, 512, None, None]\n[0318 10:37:03 @registry.py:125] group3/block1/conv3 input: [1, 512, None, None]\n[0318 10:37:03 @registry.py:133] group3/block1/conv3 output: [1, 2048, None, None]\n[0318 10:37:03 @registry.py:125] group3/block2/conv1 input: [1, 2048, None, None]\n[0318 10:37:03 @registry.py:133] group3/block2/conv1 output: [1, 512, None, None]\n[0318 10:37:03 @registry.py:125] group3/block2/conv2 input: [1, 512, None, None]\n[0318 10:37:03 @registry.py:133] group3/block2/conv2 output: [1, 512, None, None]\n[0318 10:37:03 @registry.py:125] group3/block2/conv3 input: [1, 512, None, None]\n[0318 10:37:03 @registry.py:133] group3/block2/conv3 output: [1, 2048, None, None]\n[0318 10:37:03 @registry.py:125] fpn input: [1, 256, None, None],[1, 512, None, None],[1, 1024, None, None],[1, 2048, None, None]\n[0318 10:37:03 @registry.py:125] fpn/lateral_1x1_c2 input: [1, 256, None, None]\n[0318 10:37:03 @registry.py:133] fpn/lateral_1x1_c2 output: [1, 256, None, None]\n[0318 10:37:03 @registry.py:125] fpn/lateral_1x1_c3 input: [1, 512, None, None]\n[0318 10:37:03 @registry.py:133] fpn/lateral_1x1_c3 output: [1, 256, None, None]\n[0318 10:37:03 @registry.py:125] fpn/lateral_1x1_c4 input: [1, 1024, None, None]\n[0318 10:37:03 @registry.py:133] fpn/lateral_1x1_c4 output: [1, 256, None, None]\n[0318 10:37:03 @registry.py:125] fpn/lateral_1x1_c5 input: [1, 2048, None, None]\n[0318 10:37:03 @registry.py:133] fpn/lateral_1x1_c5 output: [1, 256, None, None]\n[0318 10:37:03 @registry.py:125] fpn/upsample_lat5 input: [1, 256, None, None]\n[0318 10:37:03 @registry.py:133] fpn/upsample_lat5 output: [1, 256, None, None]\n[0318 10:37:03 @registry.py:125] fpn/upsample_lat4 input: [1, 256, None, None]\n[0318 10:37:03 @registry.py:133] fpn/upsample_lat4 output: [1, 256, None, None]\n[0318 10:37:03 @registry.py:125] fpn/upsample_lat3 input: [1, 256, None, None]\n[0318 10:37:03 @registry.py:133] fpn/upsample_lat3 output: [1, 256, None, None]\n[0318 10:37:03 @registry.py:125] fpn/posthoc_3x3_p2 input: [1, 256, None, None]\n[0318 10:37:03 @registry.py:133] fpn/posthoc_3x3_p2 output: [1, 256, None, None]\n[0318 10:37:03 @registry.py:125] fpn/posthoc_3x3_p3 input: [1, 256, None, None]\n[0318 10:37:03 @registry.py:133] fpn/posthoc_3x3_p3 output: [1, 256, None, None]\n[0318 10:37:03 @registry.py:125] fpn/posthoc_3x3_p4 input: [1, 256, None, None]\n[0318 10:37:03 @registry.py:133] fpn/posthoc_3x3_p4 output: [1, 256, None, None]\n[0318 10:37:03 @registry.py:125] fpn/posthoc_3x3_p5 input: [1, 256, None, None]\n[0318 10:37:03 @registry.py:133] fpn/posthoc_3x3_p5 output: [1, 256, None, None]\n[0318 10:37:03 @registry.py:125] fpn/maxpool_p6 input: [1, 256, None, None]\n[0318 10:37:03 @registry.py:133] fpn/maxpool_p6 output: [1, 256, None, None]\n[0318 10:37:03 @registry.py:133] fpn output: [1, 256, None, None],[1, 256, None, None],[1, 256, None, None],[1, 256, None, None],[1, 256, None, None]\n[0318 10:37:03 @registry.py:125] rpn input: [1, 256, None, None]\n[0318 10:37:03 @registry.py:125] rpn/conv0 input: [1, 256, None, None]\n[0318 10:37:03 @registry.py:133] rpn/conv0 output: [1, 256, None, None]\n[0318 10:37:03 @registry.py:125] rpn/class input: [1, 256, None, None]\n[0318 10:37:03 @registry.py:133] rpn/class output: [1, 3, None, None]\n[0318 10:37:03 @registry.py:125] rpn/box input: [1, 256, None, None]\n[0318 10:37:03 @registry.py:133] rpn/box output: [1, 12, None, None]\n[0318 10:37:03 @registry.py:133] rpn output: [None, None, 3],[None, None, 3, 4]\n[0318 10:37:08 @registry.py:125] fastrcnn input: [None, 256, 7, 7]\n[0318 10:37:08 @registry.py:125] fastrcnn/fc6 input: [None, 256, 7, 7]\n[0318 10:37:09 @registry.py:133] fastrcnn/fc6 output: [None, 1024]\n[0318 10:37:09 @registry.py:125] fastrcnn/fc7 input: [None, 1024]\n[0318 10:37:09 @registry.py:133] fastrcnn/fc7 output: [None, 1024]\n[0318 10:37:09 @registry.py:133] fastrcnn output: [None, 1024]\n[0318 10:37:09 @registry.py:125] fastrcnn/outputs input: [None, 1024]\n[0318 10:37:09 @registry.py:125] fastrcnn/outputs/class input: [None, 1024]\n[0318 10:37:09 @registry.py:133] fastrcnn/outputs/class output: [None, 6]\n[0318 10:37:09 @registry.py:125] fastrcnn/outputs/box input: [None, 1024]\n[0318 10:37:09 @registry.py:133] fastrcnn/outputs/box output: [None, 24]\n[0318 10:37:09 @registry.py:133] fastrcnn/outputs output: [None, 6],[None, 6, 4]\n[0318 10:37:09 @regularize.py:95] regularize_cost() found 57 variables to regularize.\n[0318 10:37:09 @regularize.py:20] The following tensors will be regularized: group1/block0/conv1/W:0, group1/block0/conv2/W:0, group1/block0/conv3/W:0, group1/block0/convshortcut/W:0, group1/block1/conv1/W:0, group1/block1/conv2/W:0, group1/block1/conv3/W:0, group1/block2/conv1/W:0, group1/block2/conv2/W:0, group1/block2/conv3/W:0, group1/block3/conv1/W:0, group1/block3/conv2/W:0, group1/block3/conv3/W:0, group2/block0/conv1/W:0, group2/block0/conv2/W:0, group2/block0/conv3/W:0, group2/block0/convshortcut/W:0, group2/block1/conv1/W:0, group2/block1/conv2/W:0, group2/block1/conv3/W:0, group2/block2/conv1/W:0, group2/block2/conv2/W:0, group2/block2/conv3/W:0, group2/block3/conv1/W:0, group2/block3/conv2/W:0, group2/block3/conv3/W:0, group2/block4/conv1/W:0, group2/block4/conv2/W:0, group2/block4/conv3/W:0, group2/block5/conv1/W:0, group2/block5/conv2/W:0, group2/block5/conv3/W:0, group3/block0/conv1/W:0, group3/block0/conv2/W:0, group3/block0/conv3/W:0, group3/block0/convshortcut/W:0, group3/block1/conv1/W:0, group3/block1/conv2/W:0, group3/block1/conv3/W:0, group3/block2/conv1/W:0, group3/block2/conv2/W:0, group3/block2/conv3/W:0, fpn/lateral_1x1_c2/W:0, fpn/lateral_1x1_c3/W:0, fpn/lateral_1x1_c4/W:0, fpn/lateral_1x1_c5/W:0, fpn/posthoc_3x3_p2/W:0, fpn/posthoc_3x3_p3/W:0, fpn/posthoc_3x3_p4/W:0, fpn/posthoc_3x3_p5/W:0, rpn/conv0/W:0, rpn/class/W:0, rpn/box/W:0, fastrcnn/fc6/W:0, fastrcnn/fc7/W:0, fastrcnn/outputs/class/W:0, fastrcnn/outputs/box/W:0\n/home/zysz1/python3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n[0318 10:37:16 @training.py:347] 'sync_variables_from_main_tower' includes 0 operations.\n[0318 10:37:16 @model_utils.py:64] Trainable Variables: \nname                            shape                    dim\n\ngroup1/block0/conv1/W:0         [1, 1, 256, 128]       32768\ngroup1/block0/conv2/W:0         [3, 3, 128, 128]      147456\ngroup1/block0/conv3/W:0         [1, 1, 128, 512]       65536\ngroup1/block0/convshortcut/W:0  [1, 1, 256, 512]      131072\ngroup1/block1/conv1/W:0         [1, 1, 512, 128]       65536\ngroup1/block1/conv2/W:0         [3, 3, 128, 128]      147456\ngroup1/block1/conv3/W:0         [1, 1, 128, 512]       65536\ngroup1/block2/conv1/W:0         [1, 1, 512, 128]       65536\ngroup1/block2/conv2/W:0         [3, 3, 128, 128]      147456\ngroup1/block2/conv3/W:0         [1, 1, 128, 512]       65536\ngroup1/block3/conv1/W:0         [1, 1, 512, 128]       65536\ngroup1/block3/conv2/W:0         [3, 3, 128, 128]      147456\ngroup1/block3/conv3/W:0         [1, 1, 128, 512]       65536\ngroup2/block0/conv1/W:0         [1, 1, 512, 256]      131072\ngroup2/block0/conv2/W:0         [3, 3, 256, 256]      589824\ngroup2/block0/conv3/W:0         [1, 1, 256, 1024]     262144\ngroup2/block0/convshortcut/W:0  [1, 1, 512, 1024]     524288\ngroup2/block1/conv1/W:0         [1, 1, 1024, 256]     262144\ngroup2/block1/conv2/W:0         [3, 3, 256, 256]      589824\ngroup2/block1/conv3/W:0         [1, 1, 256, 1024]     262144\ngroup2/block2/conv1/W:0         [1, 1, 1024, 256]     262144\ngroup2/block2/conv2/W:0         [3, 3, 256, 256]      589824\ngroup2/block2/conv3/W:0         [1, 1, 256, 1024]     262144\ngroup2/block3/conv1/W:0         [1, 1, 1024, 256]     262144\ngroup2/block3/conv2/W:0         [3, 3, 256, 256]      589824\ngroup2/block3/conv3/W:0         [1, 1, 256, 1024]     262144\ngroup2/block4/conv1/W:0         [1, 1, 1024, 256]     262144\ngroup2/block4/conv2/W:0         [3, 3, 256, 256]      589824\ngroup2/block4/conv3/W:0         [1, 1, 256, 1024]     262144\ngroup2/block5/conv1/W:0         [1, 1, 1024, 256]     262144\ngroup2/block5/conv2/W:0         [3, 3, 256, 256]      589824\ngroup2/block5/conv3/W:0         [1, 1, 256, 1024]     262144\ngroup3/block0/conv1/W:0         [1, 1, 1024, 512]     524288\ngroup3/block0/conv2/W:0         [3, 3, 512, 512]     2359296\ngroup3/block0/conv3/W:0         [1, 1, 512, 2048]    1048576\ngroup3/block0/convshortcut/W:0  [1, 1, 1024, 2048]   2097152\ngroup3/block1/conv1/W:0         [1, 1, 2048, 512]    1048576\ngroup3/block1/conv2/W:0         [3, 3, 512, 512]     2359296\ngroup3/block1/conv3/W:0         [1, 1, 512, 2048]    1048576\ngroup3/block2/conv1/W:0         [1, 1, 2048, 512]    1048576\ngroup3/block2/conv2/W:0         [3, 3, 512, 512]     2359296\ngroup3/block2/conv3/W:0         [1, 1, 512, 2048]    1048576\nfpn/lateral_1x1_c2/W:0          [1, 1, 256, 256]       65536\nfpn/lateral_1x1_c2/b:0          [256]                    256\nfpn/lateral_1x1_c3/W:0          [1, 1, 512, 256]      131072\nfpn/lateral_1x1_c3/b:0          [256]                    256\nfpn/lateral_1x1_c4/W:0          [1, 1, 1024, 256]     262144\nfpn/lateral_1x1_c4/b:0          [256]                    256\nfpn/lateral_1x1_c5/W:0          [1, 1, 2048, 256]     524288\nfpn/lateral_1x1_c5/b:0          [256]                    256\nfpn/posthoc_3x3_p2/W:0          [3, 3, 256, 256]      589824\nfpn/posthoc_3x3_p2/b:0          [256]                    256\nfpn/posthoc_3x3_p3/W:0          [3, 3, 256, 256]      589824\nfpn/posthoc_3x3_p3/b:0          [256]                    256\nfpn/posthoc_3x3_p4/W:0          [3, 3, 256, 256]      589824\nfpn/posthoc_3x3_p4/b:0          [256]                    256\nfpn/posthoc_3x3_p5/W:0          [3, 3, 256, 256]      589824\nfpn/posthoc_3x3_p5/b:0          [256]                    256\nrpn/conv0/W:0                   [3, 3, 256, 256]      589824\nrpn/conv0/b:0                   [256]                    256\nrpn/class/W:0                   [1, 1, 256, 3]           768\nrpn/class/b:0                   [3]                        3\nrpn/box/W:0                     [1, 1, 256, 12]         3072\nrpn/box/b:0                     [12]                      12\nfastrcnn/fc6/W:0                [12544, 1024]       12845056\nfastrcnn/fc6/b:0                [1024]                  1024\nfastrcnn/fc7/W:0                [1024, 1024]         1048576\nfastrcnn/fc7/b:0                [1024]                  1024\nfastrcnn/outputs/class/W:0      [1024, 6]               6144\nfastrcnn/outputs/class/b:0      [6]                        6\nfastrcnn/outputs/box/W:0        [1024, 24]             24576\nfastrcnn/outputs/box/b:0        [24]                      24\nTotal #vars=72, #params=41097261, size=156.77MB\n[0318 10:37:16 @base.py:208] Setup callbacks graph ...\n[0318 10:37:16 @argtools.py:146] WRN \"import prctl\" failed! Install python-prctl so that processes can be cleaned with guarantee.\n[0318 10:37:17 @tower.py:130] Building graph for predict tower 'tower-pred-0' on device /gpu:0 ...\n[0318 10:37:20 @collection.py:151] Size of these collections were changed in tower-pred-0: (tf.GraphKeys.MODEL_VARIABLES: 11->22)\nloading annotations into memory...\nDone (t=0.07s)\ncreating index...\nindex created!\n[0318 10:37:20 @dataset.py:45] Instances loaded from /home/zysz1/FasterRCNN/coco/annotations/instances_val.json.\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 981/981 [00:00<00:00, 57799.24it/s]\n[0318 10:37:20 @timer.py:48] Load Groundtruth Boxes for val finished, time:0.0186sec.\nloading annotations into memory...\nDone (t=0.26s)\ncreating index...\nindex created!\n[0318 10:37:20 @dataset.py:45] Instances loaded from /home/zysz1/FasterRCNN/coco/annotations/instances_val.json.\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 981/981 [00:00<00:00, 70886.59it/s]\n[0318 10:37:20 @timer.py:48] Load Groundtruth Boxes for val finished, time:0.0147sec.\n[0318 10:37:20 @summary.py:46] [MovingAverageSummary] 69 operations in collection 'MOVING_SUMMARY_OPS' will be run with session hooks.\n[0318 10:37:20 @summary.py:93] Summarizing collection 'summaries' of size 71.\n[0318 10:37:24 @base.py:229] Creating the session ...\n2019-03-18 10:37:24.298313: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n2019-03-18 10:37:24.559692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: \nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\npciBusID: 0000:84:00.0\ntotalMemory: 10.92GiB freeMemory: 10.76GiB\n2019-03-18 10:37:24.559834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0\n2019-03-18 10:37:25.024130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\n2019-03-18 10:37:25.024187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 \n2019-03-18 10:37:25.024198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N \n2019-03-18 10:37:25.024542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11066 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:84:00.0, compute capability: 6.1)\n[0318 10:37:31 @base.py:235] Initializing the session ...\n[0318 10:37:31 @sessinit.py:204] Variables to restore from dict: group1/block2/conv3/W:0, group0/block2/conv1/W:0, group2/block2/conv1/W:0, group0/block0/conv2/W:0, group3/block1/conv2/W:0, group1/block0/convshortcut/W:0, group0/block0/conv3/W:0, group2/block1/conv3/W:0, group0/block1/conv2/W:0, group2/block4/conv3/W:0, group0/block1/conv3/W:0, group2/block0/conv2/W:0, group1/block0/conv2/W:0, group3/block2/conv3/W:0, group2/block1/conv1/W:0, group2/block2/conv2/W:0, group3/block0/conv1/W:0, group2/block4/conv2/W:0, group1/block1/conv2/W:0, group3/block0/conv3/W:0, group0/block2/conv2/W:0, group1/block1/conv1/W:0, group0/block0/convshortcut/W:0, group0/block2/conv3/W:0, group2/block5/conv1/W:0, group2/block5/conv2/W:0, group3/block0/conv2/W:0, group1/block3/conv2/W:0, group3/block1/conv1/W:0, group3/block2/conv2/W:0, group2/block3/conv2/W:0, group2/block0/convshortcut/W:0, group2/block3/conv1/W:0, group1/block0/conv3/W:0, group2/block0/conv1/W:0, group2/block4/conv1/W:0, group3/block0/convshortcut/W:0, group2/block5/conv3/W:0, group1/block2/conv1/W:0, conv0/W:0, group1/block3/conv3/W:0, group1/block3/conv1/W:0, group0/block0/conv1/W:0, group3/block1/conv3/W:0, group2/block3/conv3/W:0, group1/block2/conv2/W:0, group1/block0/conv1/W:0, group2/block0/conv3/W:0, group1/block1/conv3/W:0, group0/block1/conv1/W:0, group2/block1/conv2/W:0, group3/block2/conv1/W:0, group2/block2/conv3/W:0\n[0318 10:37:31 @sessinit.py:87] WRN The following variables are in the graph, but not found in the dict: fastrcnn/fc6/W, fastrcnn/fc6/b, fastrcnn/fc7/W, fastrcnn/fc7/b, fastrcnn/outputs/box/W, fastrcnn/outputs/box/b, fastrcnn/outputs/class/W, fastrcnn/outputs/class/b, fpn/lateral_1x1_c2/W, fpn/lateral_1x1_c2/b, fpn/lateral_1x1_c3/W, fpn/lateral_1x1_c3/b, fpn/lateral_1x1_c4/W, fpn/lateral_1x1_c4/b, fpn/lateral_1x1_c5/W, fpn/lateral_1x1_c5/b, fpn/posthoc_3x3_p2/W, fpn/posthoc_3x3_p2/b, fpn/posthoc_3x3_p3/W, fpn/posthoc_3x3_p3/b, fpn/posthoc_3x3_p4/W, fpn/posthoc_3x3_p4/b, fpn/posthoc_3x3_p5/W, fpn/posthoc_3x3_p5/b, global_step, learning_rate, rpn/box/W, rpn/box/b, rpn/class/W, rpn/class/b, rpn/conv0/W, rpn/conv0/b\n[0318 10:37:31 @sessinit.py:87] WRN The following variables are in the dict, but not found in the graph: conv0/bn/beta, conv0/bn/gamma, conv0/bn/mean/EMA, conv0/bn/variance/EMA, group0/block0/conv1/bn/beta, group0/block0/conv1/bn/gamma, group0/block0/conv1/bn/mean/EMA, group0/block0/conv1/bn/variance/EMA, group0/block0/conv2/bn/beta, group0/block0/conv2/bn/gamma, group0/block0/conv2/bn/mean/EMA, group0/block0/conv2/bn/variance/EMA, group0/block0/conv3/bn/beta, group0/block0/conv3/bn/gamma, group0/block0/conv3/bn/mean/EMA, group0/block0/conv3/bn/variance/EMA, group0/block0/convshortcut/bn/beta, group0/block0/convshortcut/bn/gamma, group0/block0/convshortcut/bn/mean/EMA, group0/block0/convshortcut/bn/variance/EMA, group0/block1/conv1/bn/beta, group0/block1/conv1/bn/gamma, group0/block1/conv1/bn/mean/EMA, group0/block1/conv1/bn/variance/EMA, group0/block1/conv2/bn/beta, group0/block1/conv2/bn/gamma, group0/block1/conv2/bn/mean/EMA, group0/block1/conv2/bn/variance/EMA, group0/block1/conv3/bn/beta, group0/block1/conv3/bn/gamma, group0/block1/conv3/bn/mean/EMA, group0/block1/conv3/bn/variance/EMA, group0/block2/conv1/bn/beta, group0/block2/conv1/bn/gamma, group0/block2/conv1/bn/mean/EMA, group0/block2/conv1/bn/variance/EMA, group0/block2/conv2/bn/beta, group0/block2/conv2/bn/gamma, group0/block2/conv2/bn/mean/EMA, group0/block2/conv2/bn/variance/EMA, group0/block2/conv3/bn/beta, group0/block2/conv3/bn/gamma, group0/block2/conv3/bn/mean/EMA, group0/block2/conv3/bn/variance/EMA, group1/block0/conv1/bn/beta, group1/block0/conv1/bn/gamma, group1/block0/conv1/bn/mean/EMA, group1/block0/conv1/bn/variance/EMA, group1/block0/conv2/bn/beta, group1/block0/conv2/bn/gamma, group1/block0/conv2/bn/mean/EMA, group1/block0/conv2/bn/variance/EMA, group1/block0/conv3/bn/beta, group1/block0/conv3/bn/gamma, group1/block0/conv3/bn/mean/EMA, group1/block0/conv3/bn/variance/EMA, group1/block0/convshortcut/bn/beta, group1/block0/convshortcut/bn/gamma, group1/block0/convshortcut/bn/mean/EMA, group1/block0/convshortcut/bn/variance/EMA, group1/block1/conv1/bn/beta, group1/block1/conv1/bn/gamma, group1/block1/conv1/bn/mean/EMA, group1/block1/conv1/bn/variance/EMA, group1/block1/conv2/bn/beta, group1/block1/conv2/bn/gamma, group1/block1/conv2/bn/mean/EMA, group1/block1/conv2/bn/variance/EMA, group1/block1/conv3/bn/beta, group1/block1/conv3/bn/gamma, group1/block1/conv3/bn/mean/EMA, group1/block1/conv3/bn/variance/EMA, group1/block2/conv1/bn/beta, group1/block2/conv1/bn/gamma, group1/block2/conv1/bn/mean/EMA, group1/block2/conv1/bn/variance/EMA, group1/block2/conv2/bn/beta, group1/block2/conv2/bn/gamma, group1/block2/conv2/bn/mean/EMA, group1/block2/conv2/bn/variance/EMA, group1/block2/conv3/bn/beta, group1/block2/conv3/bn/gamma, group1/block2/conv3/bn/mean/EMA, group1/block2/conv3/bn/variance/EMA, group1/block3/conv1/bn/beta, group1/block3/conv1/bn/gamma, group1/block3/conv1/bn/mean/EMA, group1/block3/conv1/bn/variance/EMA, group1/block3/conv2/bn/beta, group1/block3/conv2/bn/gamma, group1/block3/conv2/bn/mean/EMA, group1/block3/conv2/bn/variance/EMA, group1/block3/conv3/bn/beta, group1/block3/conv3/bn/gamma, group1/block3/conv3/bn/mean/EMA, group1/block3/conv3/bn/variance/EMA, group2/block0/conv1/bn/beta, group2/block0/conv1/bn/gamma, group2/block0/conv1/bn/mean/EMA, group2/block0/conv1/bn/variance/EMA, group2/block0/conv2/bn/beta, group2/block0/conv2/bn/gamma, group2/block0/conv2/bn/mean/EMA, group2/block0/conv2/bn/variance/EMA, group2/block0/conv3/bn/beta, group2/block0/conv3/bn/gamma, group2/block0/conv3/bn/mean/EMA, group2/block0/conv3/bn/variance/EMA, group2/block0/convshortcut/bn/beta, group2/block0/convshortcut/bn/gamma, group2/block0/convshortcut/bn/mean/EMA, group2/block0/convshortcut/bn/variance/EMA, group2/block1/conv1/bn/beta, group2/block1/conv1/bn/gamma, group2/block1/conv1/bn/mean/EMA, group2/block1/conv1/bn/variance/EMA, group2/block1/conv2/bn/beta, group2/block1/conv2/bn/gamma, group2/block1/conv2/bn/mean/EMA, group2/block1/conv2/bn/variance/EMA, group2/block1/conv3/bn/beta, group2/block1/conv3/bn/gamma, group2/block1/conv3/bn/mean/EMA, group2/block1/conv3/bn/variance/EMA, group2/block2/conv1/bn/beta, group2/block2/conv1/bn/gamma, group2/block2/conv1/bn/mean/EMA, group2/block2/conv1/bn/variance/EMA, group2/block2/conv2/bn/beta, group2/block2/conv2/bn/gamma, group2/block2/conv2/bn/mean/EMA, group2/block2/conv2/bn/variance/EMA, group2/block2/conv3/bn/beta, group2/block2/conv3/bn/gamma, group2/block2/conv3/bn/mean/EMA, group2/block2/conv3/bn/variance/EMA, group2/block3/conv1/bn/beta, group2/block3/conv1/bn/gamma, group2/block3/conv1/bn/mean/EMA, group2/block3/conv1/bn/variance/EMA, group2/block3/conv2/bn/beta, group2/block3/conv2/bn/gamma, group2/block3/conv2/bn/mean/EMA, group2/block3/conv2/bn/variance/EMA, group2/block3/conv3/bn/beta, group2/block3/conv3/bn/gamma, group2/block3/conv3/bn/mean/EMA, group2/block3/conv3/bn/variance/EMA, group2/block4/conv1/bn/beta, group2/block4/conv1/bn/gamma, group2/block4/conv1/bn/mean/EMA, group2/block4/conv1/bn/variance/EMA, group2/block4/conv2/bn/beta, group2/block4/conv2/bn/gamma, group2/block4/conv2/bn/mean/EMA, group2/block4/conv2/bn/variance/EMA, group2/block4/conv3/bn/beta, group2/block4/conv3/bn/gamma, group2/block4/conv3/bn/mean/EMA, group2/block4/conv3/bn/variance/EMA, group2/block5/conv1/bn/beta, group2/block5/conv1/bn/gamma, group2/block5/conv1/bn/mean/EMA, group2/block5/conv1/bn/variance/EMA, group2/block5/conv2/bn/beta, group2/block5/conv2/bn/gamma, group2/block5/conv2/bn/mean/EMA, group2/block5/conv2/bn/variance/EMA, group2/block5/conv3/bn/beta, group2/block5/conv3/bn/gamma, group2/block5/conv3/bn/mean/EMA, group2/block5/conv3/bn/variance/EMA, group3/block0/conv1/bn/beta, group3/block0/conv1/bn/gamma, group3/block0/conv1/bn/mean/EMA, group3/block0/conv1/bn/variance/EMA, group3/block0/conv2/bn/beta, group3/block0/conv2/bn/gamma, group3/block0/conv2/bn/mean/EMA, group3/block0/conv2/bn/variance/EMA, group3/block0/conv3/bn/beta, group3/block0/conv3/bn/gamma, group3/block0/conv3/bn/mean/EMA, group3/block0/conv3/bn/variance/EMA, group3/block0/convshortcut/bn/beta, group3/block0/convshortcut/bn/gamma, group3/block0/convshortcut/bn/mean/EMA, group3/block0/convshortcut/bn/variance/EMA, group3/block1/conv1/bn/beta, group3/block1/conv1/bn/gamma, group3/block1/conv1/bn/mean/EMA, group3/block1/conv1/bn/variance/EMA, group3/block1/conv2/bn/beta, group3/block1/conv2/bn/gamma, group3/block1/conv2/bn/mean/EMA, group3/block1/conv2/bn/variance/EMA, group3/block1/conv3/bn/beta, group3/block1/conv3/bn/gamma, group3/block1/conv3/bn/mean/EMA, group3/block1/conv3/bn/variance/EMA, group3/block2/conv1/bn/beta, group3/block2/conv1/bn/gamma, group3/block2/conv1/bn/mean/EMA, group3/block2/conv1/bn/variance/EMA, group3/block2/conv2/bn/beta, group3/block2/conv2/bn/gamma, group3/block2/conv2/bn/mean/EMA, group3/block2/conv2/bn/variance/EMA, group3/block2/conv3/bn/beta, group3/block2/conv3/bn/gamma, group3/block2/conv3/bn/mean/EMA, group3/block2/conv3/bn/variance/EMA, linear/W, linear/b\n[0318 10:37:31 @sessinit.py:217] Restoring 53 variables from dict ...\n[0318 10:37:39 @base.py:242] Graph Finalized.\n[0318 10:37:39 @concurrency.py:38] Starting EnqueueThread QueueInput/input_queue ...\n[0318 10:37:39 @graph.py:73] Running Op sync_variables/sync_variables_from_main_tower ...\nit doesnt make any error  it just stoped  like this  ..\nthank you \n. i try to train my own data \nthis is my configs:\nconfig = AttrDict()\n_C = config     # short alias to avoid coding\nmode flags ---------------------\n_C.TRAINER = 'replicated'  # options: 'horovod', 'replicated'\n_C.MODE_MASK = False        # FasterRCNN or MaskRCNN\n_C.MODE_FPN = True\ndataset -----------------------\n_C.DATA.BASEDIR = '/home/zysz1/FasterRCNN/coco'\nAll TRAIN dataset will be concatenated for training.\n_C.DATA.TRAIN = ('train')   # i.e. trainval35k, AKA train2017\nEach VAL dataset will be evaluated separately (instead of concatenated)\n_C.DATA.VAL = ('val', )  # AKA val2017\nThis two config will be populated later by the dataset loader:\n_C.DATA.NUM_CATEGORY = 5  # without the background class (e.g., 80 for COCO)\n_C.DATA.CLASS_NAMES = [\"BG\",\"tiekedahuoji\",\"heidingdahuoji\",\"daoju\",\"dianyuanhedianchi\",\"jiandao\"]  # NUM_CLASS (NUM_CATEGORY+1) strings, the first is \"BG\".\nwhether the coordinates in the annotations are absolute pixel values, or a relative value in [0, 1]\n_C.DATA.ABSOLUTE_COORD = False\nbasemodel ----------------------\n_C.BACKBONE.WEIGHTS = '/home/zysz1/FasterRCNN/weights/ImageNet-R50-AlignPadding.npz'   # /path/to/weights.npz\n_C.BACKBONE.RESNET_NUM_BLOCKS = [3, 4, 6, 3]     # for resnet50\nRESNET_NUM_BLOCKS = [3, 4, 23, 3]    # for resnet101\n_C.BACKBONE.FREEZE_AFFINE = False   # do not train affine parameters inside norm layers\n_C.BACKBONE.NORM = 'None'  # options: FreezeBN, SyncBN, GN, None\n_C.BACKBONE.FREEZE_AT = 2  # options: 0, 1, 2\nUse a base model with TF-preferred padding mode,\nwhich may pad more pixels on right/bottom than top/left.\nSee https://github.com/tensorflow/tensorflow/issues/18213\nIn tensorpack model zoo, ResNet models with TF_PAD_MODE=False are marked with \"-AlignPadding\".\nAll other models under ResNet/ in the model zoo are using TF_PAD_MODE=True.\nUsing either one should probably give the same performance.\nWe use the \"AlignPadding\" one just to be consistent with caffe2.\n_C.BACKBONE.TF_PAD_MODE = False\n_C.BACKBONE.STRIDE_1X1 = False  # True for MSRA models\nschedule -----------------------\n_C.TRAIN.NUM_GPUS = 1         # by default, will be set from code\n_C.TRAIN.WEIGHT_DECAY = 1e-4\n_C.TRAIN.BASE_LR = 1e-2  # defined for total batch size=8. Otherwise it will be adjusted automatically\n_C.TRAIN.WARMUP = 1000   # in terms of iterations. This is not affected by #GPUs\n_C.TRAIN.WARMUP_INIT_LR = 1e-2 * 0.33  # defined for total batch size=8. Otherwise it will be adjusted automatically\n_C.TRAIN.STEPS_PER_EPOCH = 500\n_C.TRAIN.STARTING_EPOCH = 0  # the first epoch to start with, useful to continue a training\nLR_SCHEDULE means equivalent steps when the total batch size is 8.\nWhen the total bs!=8, the actual iterations to decrease learning rate, and\nthe base learning rate are computed from BASE_LR and LR_SCHEDULE.\nTherefore, there is no need to modify the config if you only change the number of GPUs.\n_C.TRAIN.LR_SCHEDULE = [120000, 160000, 180000]      # \"1x\" schedule in detectron\n_C.TRAIN.LR_SCHEDULE = [240000, 320000, 360000]      # \"2x\" schedule in detectron\nLonger schedules for from-scratch training (https://arxiv.org/abs/1811.08883):\n_C.TRAIN.LR_SCHEDULE = [960000, 1040000, 1080000]    # \"6x\" schedule in detectron\n_C.TRAIN.LR_SCHEDULE = [1500000, 1580000, 1620000]   # \"9x\" schedule in detectron\n_C.TRAIN.EVAL_PERIOD = 0  # period (epochs) to run evaluation\npreprocessing --------------------\nAlternative old (worse & faster) setting: 600\n_C.PREPROC.TRAIN_SHORT_EDGE_SIZE = [800, 800]  # [min, max] to sample from\n_C.PREPROC.TEST_SHORT_EDGE_SIZE = 800\n_C.PREPROC.MAX_SIZE = 1333\nmean and std in RGB order.\nUn-scaled version: [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n_C.PREPROC.PIXEL_MEAN = [123.675, 116.28, 103.53]\n_C.PREPROC.PIXEL_STD = [58.395, 57.12, 57.375]\nanchors -------------------------\n_C.RPN.ANCHOR_STRIDE = 16\n_C.RPN.ANCHOR_SIZES = (32, 64, 128, 256, 512)   # sqrtarea of the anchor box\n_C.RPN.ANCHOR_RATIOS = (0.5, 1., 2.)\n_C.RPN.POSITIVE_ANCHOR_THRESH = 0.7\n_C.RPN.NEGATIVE_ANCHOR_THRESH = 0.3\nrpn training -------------------------\n_C.RPN.FG_RATIO = 0.5  # fg ratio among selected RPN anchors\n_C.RPN.BATCH_PER_IM = 256  # total (across FPN levels) number of anchors that are marked valid\n_C.RPN.MIN_SIZE = 0\n_C.RPN.PROPOSAL_NMS_THRESH = 0.7\nAnchors which overlap with a crowd box (IOA larger than threshold) will be ignored.\nSetting this to a value larger than 1.0 will disable the feature.\nIt is disabled by default because Detectron does not do this.\n_C.RPN.CROWD_OVERLAP_THRESH = 9.99\n_C.RPN.HEAD_DIM = 1024      # used in C4 only\nRPN proposal selection -------------------------------\nfor C4\n_C.RPN.TRAIN_PRE_NMS_TOPK = 12000\n_C.RPN.TRAIN_POST_NMS_TOPK = 2000\n_C.RPN.TEST_PRE_NMS_TOPK = 6000\n_C.RPN.TEST_POST_NMS_TOPK = 1000   # if you encounter OOM in inference, set this to a smaller number\nfor FPN, #proposals per-level and #proposals after merging are (for now) the same\nif FPN.PROPOSAL_MODE = 'Joint', these options have no effect\n_C.RPN.TRAIN_PER_LEVEL_NMS_TOPK = 2000\n_C.RPN.TEST_PER_LEVEL_NMS_TOPK = 1000\nfastrcnn training ---------------------\n_C.FRCNN.BATCH_PER_IM = 512\n_C.FRCNN.BBOX_REG_WEIGHTS = [10., 10., 5., 5.]  # Better but non-standard setting: [20, 20, 10, 10]\n_C.FRCNN.FG_THRESH = 0.5\n_C.FRCNN.FG_RATIO = 0.25  # fg ratio in a ROI batch\nFPN -------------------------\n_C.FPN.ANCHOR_STRIDES = (4, 8, 16, 32, 64)  # strides for each FPN level. Must be the same length as ANCHOR_SIZES\n_C.FPN.PROPOSAL_MODE = 'Level'  # 'Level', 'Joint'\n_C.FPN.NUM_CHANNEL = 256\n_C.FPN.NORM = 'None'  # 'None', 'GN'\nThe head option is only used in FPN. For C4 models, the head is C5\n_C.FPN.FRCNN_HEAD_FUNC = 'fastrcnn_2fc_head'\nchoices: fastrcnn_2fc_head, fastrcnn_4conv1fc_{,gn_}head\nC.FPN.FRCNN_CONV_HEAD_DIM = 256\n_C.FPN.FRCNN_FC_HEAD_DIM = 1024\n_C.FPN.MRCNN_HEAD_FUNC = 'maskrcnn_up4conv_head'   # choices: maskrcnn_up4conv{,gn_}head\nMask-RCNN\n_C.MRCNN.HEAD_DIM = 256\nCascade-RCNN, only available in FPN mode\n_C.FPN.CASCADE = False\n_C.CASCADE.IOUS = [0.5, 0.6, 0.7]\n_C.CASCADE.BBOX_REG_WEIGHTS = [[10., 10., 5., 5.], [20., 20., 10., 10.], [30., 30., 15., 15.]]\ntesting -----------------------\n_C.TEST.FRCNN_NMS_THRESH = 0.5\nSmaller threshold value gives significantly better mAP. But we use 0.05 for consistency with Detectron.\nmAP with 1e-4 threshold can be found at https://github.com/tensorpack/tensorpack/commit/26321ae58120af2568bdbf2269f32aa708d425a8#diff-61085c48abee915b584027e1085e1043  # noqa\n_C.TEST.RESULT_SCORE_THRESH = 0.05\n_C.TEST.RESULT_SCORE_THRESH_VIS = 0.5   # only visualize confident results\n_C.TEST.RESULTS_PER_IM = 100\n_C.freeze()  # avoid typo / wrong config keys\nthen  i run train.py\nthat is all\nthank you \n. ok  thanks a lot . ok thanks a lot. ",
    "yenchen-liu": "Sure, I found it only works after horovod 0.15. Will put a try catch block.. Move into the HorovodTrainer. "
}
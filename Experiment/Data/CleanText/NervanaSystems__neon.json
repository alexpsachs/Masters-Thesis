{
    "khosra": "I have one! \n\nOn Jan 27, 2015, at 1:57 PM, Nervana Admin notifications@github.com wrote:\nI want a banana!\n\u2014\nReply to this email directly or view it on GitHub https://github.com/NervanaSystems/neon/issues/1.\n. Hi Andrew, yes it is. Amir.\n. \n",
    "nervanasys": "Thanks! For now, please clone the latest version from github in to your home dir. We will update the system wide version soon.\n. Following neon v1.0, multi-GPU & multi-node support is on the Nervana Cloud only. These have been optimized for our Cloud HW setup.\n. We can host the auxiliary stuff in a separate DQN repo.\n. Would be nice to have a working MNIST example using Adam.\nBtw, please be aware that there will be upcoming changes due to https://github.com/NervanaSystems/neon/issues/62\nNevertheless, we can use your implementation as a template to get Adam working in the refactored neon.\n. Can you post your Yaml file and the full error message?\n. That is just an example for the NDSB Kaggle competition dataset. You'll have to modify the paths in the file to point to the directories for your dataset, and changes sizes of input data layer appropriately. \nImageset will expect a directory with two or more subdirectories. Each subdirectory will correspond to a different class of images. \n. How big is the image size? \nYou'll want to update:\noutput_image_size: &ois 128,\ncropped_image_size: &cis 64,\nAnd ofmshape: [64, 64],\nHow many output categories (which will determine size of softmax layer)?\n. For the output layer try setting nout to 2\n. Tracking this under #80 going forward.\n. We don't have plans to currently add an example, but take a look at the latest code (v1.0.0rc1). You might find it easier.\n. We are targeting a 1-2 month time frame.\n. Sorry, it is still pending. this is currently a priority item under active development. We hope to have it in this month. \n. We'll be sure to update our progress here. Would love to hear more about your use case. Please email info@nervanasys.com\n. This should be much easier in the latest version with the DataIterator example. Check out the mnist example. \n. What cost function are you using ? In general you will have to play with hyperparameters for this. \n. You could try the renamed get_outputs function that is clearer: https://github.com/NervanaSystems/neon/blob/master/neon/models/model.py#L212\nIt might be better for you to implement a function similar to get_outputs on your own for this use case as DataIterator currently assumes missing targets refers to building an autoencoder.\n. Yes. See #80\n. Kepler support is WIP\n. We were getting good performance using strided deconvolution instead of unpooling. Have you tried using that?\n. Yes, this is coming soon\n. Thanks for your responses. We have a list of speakers here: http://www.meetup.com/SF-Data-Science/events/228809230/\n. Yes, Kepler support is back in neon as of v1.2, 85fd5c1. See #80 and http://www.nervanasys.com/neon-v1-2-release-kepler-aws-support-are-back-deep-resnets-and-more/\n. Can you add a print before line 88 for check_gpu.get_compute_capability(device_id) ?\nWere the CUDA drivers installed at the time of neon installation, and does your PATH include /usr/local/cuda/bin and LD_LIBRARY_PATH include /usr/local/cuda/lib ?\n. You would need the CUDA SDK and drivers. \nGeneral docs on installation are here.\n. We are looking into what's up with the Anaconda installation. In the mean time can you use the regular apt-get python ?\n. Thanks for your question and the pointer to quora.\nI just responded to this question on quora.. This could be computed outside this function and doesn't need to be an array (just a scalar)\n. ",
    "scttl": "The system-wide installed version of neon has been bumped up to the latest master (v0.7.1+8a6fe7d).  Thanks for reporting this!\n. Hi Gabriel,\nThanks for catching this.  That link is actually outdated now as we've switched everything over to sphinx.  Instead please use: http://framework.nervanasys.com/docs/latest/models.html#adding-a-new-type-of-model\nI've got a fix to update this link in the top-level README.md that should be making its way to the master branch shortly.  I'll close this issue once it's visible.\n. Most recent master push now contains the updated link in README.md\n. Unfortunately we're not supporting Windows as a target platform at this time (just Linux and OSX).  That may change in the future, but is low priority at the moment.\nIf you'd like, we can set you up with an account on our Linux demo box, just shoot an email to: framework@nervanasys.com\n. Hi Gabriel,\nThanks for being our honorary first external committer! I've had a chance to review your pull request, and made some modifications (in general we're trying to be really stingy with allocating any temporary buffers, so most of the changes are around that).\n. As batch_size is no longer stored as part of model parameters, we are successfully able to train a model with one batch size, save it, load it, then use it to generate predictions of a different batch_size.\n. Added as part of the 1.1.2 release via the HistCallback class and nvis.  More info available here: http://neon.nervanasys.com/docs/latest/tools.html#layer-histogram-visualization\n. This occurs at the activation, not overall model (or layer) level.  As a result, it impacts any models that utilize these activations.\nYou can see the behavior in neon/transforms/logistic.py in fprop_func for instance.  To speed up computation during the training phase we update the input buffer with the derivative in that call instead of having to make a separate bprop_func call with additional calculations.\nIn situations where we just want to generate predictions without training, this additional computation is unnecessary so we should add a skip_derivative parameter and set it accordingly in these cases.  We did exactly this for neon/transforms/softmax.py but need to apply it consistently to some of the other activations.\n. ah, thanks for catching this!\n. Example is available in examples/mlp/mnist-small-noyaml.py and described in the docs.\n. Hi Andy,\nA couple of points that may help get you unstuck:\n- with the virtualenv based install, assuming user ubgpu created the .venv directory, you should only need to run make install not sudo make install as the python packages should be installed under that directory.\n- The real reported issue looks to me like the CUDA compiler nvcc is not being found.  Locating this should already be addressed in our FAQ: http://neon.nervanasys.com/docs/latest/faq.html#installation (and note in particular that you need to ensure certain environment variables are being passed through when run as sudo)\nGive those suggestions a try, and let me know if that helps\n. Yes this seems like a reasonable alternative check.  Thanks for the commit and producing Docker images!\n. Great to hear!\nAs a suggestion you may want to rename the docker image to make it clear that it is Maxwell only since you're using the nervanagpu backend. \nIn fact it would probably good to have another docker image based off our cudanet GPU backend since this will work on Kepler cards too. \n. Because it looks like you were using virtualenv in the other issue you opened (#18) I suspect you maybe forgot to activate it for the runs above.  Try the following (from your neon directory):\n. .venv/bin/activate\nneon --gpu nervanagpu examples/convnet/i1k-alexnet-fp16.yaml\nIf that still fails then you need to first install nervanagpu as @Kaixhin mentions.\n. @lglhuada I just pushed a commit that should address this.\n. Hi Kyle,\nPer #19 @Kaixhin has already created some docker images for neon which you can find from:\n- base cpu neon\n- nervanagpu neon\n- cudanetgpu neon\n. Thanks for the commits @kylef-lab41 and @Kaixhin!  I've accepted a slightly modified version of #27 so that users can more easily find these images going forward (documented from our main installation page, and linked to in the README and Quickstart pages).\n. Thanks for the commits.  In addition to my one-liner 10 -> 100 fix, can you also update the documentation  in a couple of places (just mimic what is done for cifar10 basically):\n- doc/source/datasets.rst\n- doc/source/api.rst\n- examples/ANNOTATED_EXAMPLE.yaml (just add CIFAR100 to the datasets list)\n. Shoot, picked up your old commit by accident.  Once you have the remaining changes ready to go, I'll merge those in too.\n. Looks good, thanks for contributing this dataset!\n. Hi @shuokay \nLack of GPU support for RNNs / LSTMs is already known and being tracked in issue #16 \n. @shuokay Not as such, but we're starting to look into the changes required.  Supporting this on the cudanet GPU backend requires a different set of changes than what supporting on the nervanagpu GPU backend requires.  As a loose estimate for seeing these changes in master we're probably talking on the order of a couple of weeks if not sooner.\n. We just pushed some fixes for GPU based RNN and LSTM networks to master (see #16).  cudanet/nervanagpu runs of the two example recurrent networks works ok.\n. @hjchen2 it should get downloaded automatically but it's possible project gutenburg is preventing direct linking.  The file in question is available from: http://www.gutenberg.org/cache/epub/2701/pg2701.txt  Simply copy this file to a new directory named MOBYDICK inside of wherever your repo_path points (which is ~/data by default).\n. The incremental save and epoch extend approach that is currently implemented is described in a bit more detail here: http://neon.nervanasys.com/docs/latest/experiments.html?highlight=saving#generating-predictions\n. Hi, thanks for catching this.  Confirmed it's a python3 compatibility bug.  We'll get cracking on fixing it, and will update once we've closed this.\n. Hi Thomas,\nWe're only supporting python 2.7 or later at this time.  Your error comes about because dict comprehension was a feature that was added in 2.7, and we're making use of that on line 55.\n. Hi Thomas,\nYou may also need to add at least one of blas-devel or openblas-devel\nIf that still doesn't work you'll need to provide more information for us like the CMakeOutput.log and CMakeError.log from wherever cmake is building from in your environment\n. Hi Thomas,\nAs to your current error, my hunch is that the version of libjpeg-devel that you have installed is too old.  I'd recommend upgrading this to something more recent.\n. You can pass a specific set of library paths into cmake via the CMAKE_LIBRARY_PATH environment variable (or include file paths via CMAKE_INCLUDE_PATH).  Perhaps running:\nexport CMAKE_LIBRARY_PATH=\"${CMAKE_LIBRARY_PATH}:${ATLAS_LIB_PATH}\"\nfirst will get you unstuck on the BLAS lib error.\n. With the 0.9.0 release we now support multi GPU data and model parallelization provided you are using the nervanagpu backend (Maxwell based cards).\nNote that this release drops support of MPI based parallelization, and will not work across all models/datasets when using the cudanet backend (Kepler based cards).\n. Hi @rongou this is actually already documented in our FAQ: http://neon.nervanasys.com/docs/latest/faq.html#installation  (see the last bullet point under installation)\n. Per #37 we won't be including this change due to the use of hard-coded paths, and because a work-around is already documented in our FAQ\n. Thanks for raising this.  Commit 541b6f9 addresses this by implementing Leaky ReLU's for the nervanagpu backend\n. This is outlined in the docs: http://neon.nervanasys.com/docs/latest/datasets.html#adding-a-new-type-of-dataset\n. Hi we've just merged an implementation of RMSProp, supported across all backends.  There are a couple of examples that you can try out by setting lrule_init = *rms for one or both of the layers in either examples/mlp/mnist-small.yaml or examples/mlp/mnist-tuned.yaml.\n. Hi,\nCould you post your yaml file as well as the contents of your new NB class?\nAre you importing NB directly into the dataset namespace?  See: neon/datasets/__init__.py\n. Another alternative would be to install neon in developer mode which you can accomplish via:\nmake develop\nAs the link indicates this will build symlinks allowing you to edit files in your current working directory without having to do a full make install each time (you only need to call make develop once).\nThis is briefly mentioned at the top of the install docs\n. Hi Ja,\nThough we don't yet have a slide deck or screencast to share, I think the easiest way to get to grips with using neon is to start by skimming through the: quick start then following that up by looking at:\n- neon/examples/mlp/mnist-small-noyaml.py to get a feel for the main objects and how they fit together\n- neon/examples/ANNOTATED_EXAMPLE.yaml to understand how these pieces can be configured\nAs for the Moby Dick example, I'd suggest reading through the RNN docs as there's some detail in there describing how the data is laid out and so forth.\n. thanks for catching this!\n. Confirmed that this is a CPU backend specific bug (eslice not being transposed correctly during convolutional update step when it is a 1D vector).  \nWe'll set about fixing this, however one should note that using such small batch sizes may result in suboptimal runtimes, and require additional parameter tuning.\nThanks for catching this!\n. Hi, \nThis error will come about if nvcc (CUDA C compiler) can't be located by the user running the command.  Does nvcc --version work ok for you?\nSince you're doing this as the super user, you may also need to adjust your PATH and or LD_LIBRARY_PATH variables as part of the command call.  This is described in the FAQ:\nsudo env \u201cPATH=$PATH\u201d env \u201cLD_LIBRARY_PATH=$LD_LIBRARY_PATH\u201d make install\nTry that and let us know if that gets you unstuck.\n. Does sudo nvcc --version produce the same output?  Which Linux distribution/version are you currently running?\n. Hi,\nBy default there is no such shuffling before each epoch in neon.  Most of the models are setup to run mini-batched gradient descent, where the batch_size model parameter defines how many examples are in each batch.\nIf you wanted to shuffle mini-batch order within an epoch that should be pretty straight forward and I'd recommend having a look in the DataLayer class code.  I'd suggest creating a subclass and overriding the has_more_data and fprop functions.  If however you wanted to shuffle individual examples this will be more complex and result in a lot of overhead (as data is stored on device in mini-batches and accessed as such).\n. Since you appear to be running as root on Ubuntu, can you first make sure that nvidia-smi is in that users PATH, and produces sensible output when run from the command line?  It doesn't look like this command is being found.\nI'd also suggest having a look at the items in our installation FAQ: http://neon.nervanasys.com/docs/latest/faq.html\n. nvidia-smi is not required to run any of the examples, we just use it as a proxy to validate that the user has the CUDA SDK installed.  Provided you were able to install the cudanet python library ok, for the moment you can work around your issue by editing neon/backends/__init__.py to replace the line:\ngpuflag = (os.system(\"nvidia-smi > /dev/null 2>&1\") == 0)\nwith\ngpuflag = (os.system(\"nvcc --version > /dev/null 2>&1\") == 0)\nWe made a similar change in the Makefile a while back, but needed to update the check here as well.  I've created a fix, and will get this merged into master for the next release of neon.\n. Hi,\nHow did you specify the location for helper_cuda.h?  If it was by passing CUDA_COMMON_INCLUDE_DIRS and so forth on the command line to your cmake call, what I'd suggest doing is in your cudanet source tree, first edit setup.py to augment the cmake command the same way on line 19.  Then from that directory a simple pip install . should successfully install cudanet and register it with pip, which will prevent neon from continually checking and re-downloading cudanet.\n. HI,\nYes with some python edits it should be possible to report additional metrics or other dataset partition performance at the end of each epoch.  We only report training set error by default to keep things running quickly but if you're ok with the additional overhead you should be able to create new metrics at the model level (either edit the yaml, or pass these into your model constructor call), then adjust your model's fit() function to:\n1. generate predictions (see your model's predict_fullset() or predict_generator() functions)\n2. use these predictions with your dataset partition's expected outputs to update your metric's statistics (see your metric(s) add() function)\n3. report your metric's value (see your metric(s) report() function)\nYou can see a lot of this actually carried out in fit_predict_err.py's run() method.\nAs for your second question regarding plotting, this is something we're actively developing but don't yet have the capability to visualize metric stats per epoch.  Most of the plotting currently in neon is based around timing/flops, and numeric representation.\n. Closing as we now support per epoch metric printing via #61 \n. Hi S\u00e9b,\nneon already supports the notion of validation dataset partitions, and typically the user would divide their dataset into train/test/validation prior to using neon.\nThat being said I agree that its occasionally useful to take an existing train/test divided dataset and have the ability to carve some of the training set cases out for validation purposes.  This functionality already exists in neon for datasets that utilize the BatchWriter class like imageset.py.  We could do a better job documenting it, but there is a dataset parameter called validation_pct that can be passed to this dataset and during the first load() call will be used to randomly select a portion of the training cases to use for validation purposes.\n. Closing this as validation_pct functionality has been merged in as part of #60 \n. Hi S\u00e9b,\nThanks for taking the effort to implement this pull request, but I don't think we can accept it as-is unfortunately.  \nSee my comments in #57 for additional detail, but we really don't want to be shuttling data around between train and validation sets after they have already been loaded and are sitting on device.  We currently have the ability to do this as part of the load() process for directories of images using the BatchWriter class and the validation_pct parameter.  What would be really useful for us is enabling this functionality at load time for non BatchWriter based datasets.  If that's something you could implement we'd be more than happy to take a look!\nAs for the validation scores while training, we're probably going to end up supporting functionality like this through asynchronous callbacks that users can specify to run at the end of each epoch, but we're still trying to determine exactly how that will look.  If you want to implement the print_validation (default to False) approach I think that would still have some use in the interim, so again we'd be happy to take a look at a pull request for that.\n. Hi S\u00e9b,\nThe dataset.format() function that most datasets call at the end of the load() will actually copy the first batch of values from host and onto the device (ex. GPU memory for a GPU backend), so that's why we need to implement these changes prior to that.  \nI think you can adapt your split_set code to 1) have it be called before format(), 2) not use backend calls for your random sampling -- numpy should be ok since the data should be on host at that stage, 3) not take any parameters except for maybe to_set and from_set (everything else should be part of the Dataset class).  I think doing something fairly similar to sample_training_data could work (for smaller datasets at least).  Another minor gripe would be to utilize one of the synthetic datasets like UniformRandom or ToyImages for your testing (neither has validation out of the box so should be useful).  That way you wouldn't need to download CIFAR100 to run the tests -- I'd also rename the file to something like test_val_split.py instead of test_cifar100.py.\nI'd put the print_validation stuff in a separate pull request if possible (easier to track independently).\n. Hi David,\nUnfortunately, all the GPU's Amazon currently offers through EC2 are older Kepler based models so again you wouldn't be able to use the nervanagpu backend there.\n. Thanks Seb, just merged this in (squashed your commits into one)\n. I've taken a look through this change and think the following updates are in order:\n1.  Revert the changes to mnist-small.yaml example.  Probably better to make a new mnist-epoch_val_print-small.yaml or something like that.\n2.  The current validation_metric name is misleading and should be changed.  We already have a metrics parameter for things to compute after training, maybe this parameter should instead be called per_epoch_metrics and have a similar format to metrics.\n3.  From the above we shouldn't be restricted to just specifying a single metric, and only be allowed to compute them on a validation set.  We should support per epoch computation of potentially multiple metrics on each of the train, test, and validation sets.  If this variable is not specified we default to our current training error reporting.\n4.  We should add this functionality to RNN's since doing so should be straightforward.\n. HI S\u00e9b,\nI've had a look through the updates and they look good from my perspective. :+1:   \nAs for table based printing, I think the best way to go may be to support dumping these metrics to a user specified flat text file (1 column per metric, one row per epoch).  We have this ability for regular, end of model training metrics via the -o parameter and the dump_metrics() function.  This computes other info so probably not what we want to use in this case, but may give you a couple of ideas to get started.\nOnly other suggestion would be to flatten all of your commits down to a single one with relevant description to keep the changelog clean, but I can do that on my side when I merge it in.  Let me know if I should go ahead and merge now, or wait for some table file dumping changes (maybe making these a separate pull request would be better anyway).\nThanks for the contribution!\n. Yeah this seems like something that should be easy to integrate into the main neon branch.  @seba-1511 did you want to put together a pull request for this?\n. closing this as a separate project/framework seems like a better approach than bolting everything into stock neon.\n. Hi Seb,\nWas able to reproduce your problem, but I'm not sure where we'd need to pass a transposed argument to max()?  If you're taking this over all axes you can pass in b as is and get the same result.  Or if it was over one axis for a 2D input, you can get the same result by choosing the other axis of an expected transposed input.\nUpdating the cc2 code to support transposed arguments would be a fair bit of effort (lots of other operations are similarly impacted), and we're likely going to deprecate the cc2 backend anyway.  We may end up backporting support for older cards into nervanagpu.\nInterestingly the code doesn't work with nervanagpu as written either, but this is due to b being a vector and c a 2D matrix.  With b a 1x3 2D matrix it works.\n. Hi Xiuxia,\nThat's right there's no explicit apply_derivative during the bprop call as the derivatives for these activations are pre-computed and stored during the forward pass because we have all the info we need to compute them at that time (this allows us to speed things up and use fewer buffers).  \nAs an example the tanh(x) derivative is 1 - tanh^2(x), so if you look in fprop_func() in tanh.py you'll see that after computing the value for tanh on the inputs and storing it in outputs, we're also squaring this output value, subtracting 1 from it, and storing this back in inputs which we'll utilize in the bprop phase.  If you look in say layers/fully_connected.py you'll see that during fprop() and bprop() calls we're passing in self.pre_act for the inputs parameter I mentioned earlier.\nHope that clears things up for you.\n. Hi Xiuxia,\nPlease see my answer to your previous \"issue\" in #65.  That explains why things setup the way they are.\n. Hi Xiuxia,\nCan you let us know what OS and version you are running and also what version of libjpeg is installed?  You can determine the latter via ldconfig -p | grep libjpeg.  I'm wondering if you have an older version, and that is what is leading to the error your report.\n. Hi Xiuxia,\nI do see you also have a 6.2 version, can you report what version of the lib is being used by PIL.  Instructions for doing this can be found here: http://stackoverflow.com/questions/24396727/find-which-libjpeg-is-being-used-by-pil-pillow\n. Great to hear!\n. Hi S\u00e9b,\nThanks for the comment.  Agreed, neon should either directly support (or make it easy to support) all the little things required to train nets well at scale, and skimming through the links I feel like we're a pretty reasonable portion of the way there.  Keep in mind a lot of this is ultimately up to the end user to ensure they have enabled these tricks and set up parameters appropriately.\nAs part of the refactor we'll definitely be revisiting these types of items.\n. Hi Xiuxia,\nThis isn't a bug -- in general you can't expect to just swap one cost function for another without also adjusting or rescaling the other network parameters.\nThere are any number of ways you can get into the numerical issues you're seeing, so I can't really provide specifics for your situation.  Your best bet for getting to the bottom of them is to use a debugger and monitor some of the tensors in each layer to see if their values are blowing up, or getting arbitrarily close to zero (which could yield INF values when acting as the denominator in a division operation like what happens during backprop).\n. Hi,\nUnfortunately neon's BatchWriter class (which is what Imageset based datasets use for a lot of the initial input file prep) doesn't currently operate in the manner you expect.  Today the base BatchWriter class only supports a single top-level input directory, with subdirectories for target label classes.  This directory of images gets read and randomly slotted into 80% train and 20% validation sets by default (though you can change this mix via the validation_pct parameter for Imageset).\nSo if your test set happens to be an arbitrary selected holdout set, the fastest way for you to get going would be to lump all your original train/test split data together under a single root directory and let BatchWriter do the splitting for you -- you'd also need to replace the test based metrics with validation metrics in your YAML file.\nIf you specifically want to keep your already defined test set images as a test set, you're going to have to write your own subclass of BatchWriter and make a change or two to Imageset to ensure it loads your derived BatchWriter class.\n. Hi,\nYes double precision float networks should certainly be possible for some of the backends, were you thinking of GPU or CPU backend?  Were you looking to use float64 uniformly throughout all Tensors in all layers, or just for specific ones?\nFirst you'll need to modify the YAML of each layer to set backend_type: 'float64' (this sets all tensors in that layer uniformly).  This should be all you need for fully connected layers, but for convolutional layers (on CPU backends at least) there's a small default dtype related issue that I've just pushed a fix for.\n. Sorry for closing earlier (was triggered from the commit I pushed).\nSince you're using imageset, there's one other change you'll need to make to imageset.py to add float64 support to backend_type right before where the ValueError was thrown.  You should add the following:\nelif self.backend_type in ['float64', 'np.float64', 'numpy.float64']:\n            self.backend_type = np.float64\nWith that change and the cudanet GPU backend running v0.8.2 of neon, I can successfully run your example.  Nervana GPU doesn't support float64 so it won't work, and the CPU backend doesn't support non-zero padding in convolution layers so it too won't work with this example.\n. Hi,\nAWS doesn't support newer Maxwell based GPU's, so the nervanagpu backend will not work, but cudanet should.\nWhat was the exact syntax of the command you were trying to run?  Did you try make -e GPU=cudanet install?  Were you trying to run as sudo?\n. If you modified the GPU line of setup.cfg then that should perform the same functionality as make -e GPU=cudanet so I don't think the result will differ.\nEven though nvcc --version appears to work when you run from the command line, it looks like the error you are reporting comes from the return value from that exact test.  In the top-level Makefile, the line is:\nifneq ($(shell nvcc --version > /dev/null 2>&1; echo $$?), 0) \nOut of curiosity what does running: $(nvcc --version > /dev/null 2>&1; echo $?) and sudo $(nvcc --version > /dev/null 2>&1; echo $?) from the command line report?\n. Yes, we haven't had too much use for RBM's or DBN's internally as of late and thats the reason why we haven't kept the code in rbm.py and dbn.py as up to date as other model types.  After the refactor we may look to reintroduce support for these model types, but it's pretty low on the priority list.\n. Well I may be biased, but I'd recommend uninstalling torch and installing neon instead! :)\nClosing as I assume you meant to post this on cutorch instead.\n. Kepler kernel support is now available publicly in v1.2.0 as of 85fd5c1\n. Yes, merge requests of this sort will continue to be gleefully accepted!  With apologies to Alex Graves.\n. Hi, \nJust realized we forgot to close this.  Support for 3D convolution exposed at the layer level was introduced way back in da492cd4 from last October.\n. Hi,\nThanks for updating these.  \nWe've just pushed a fix for #83, can you try building the gpu backend based image off of the latest master?  Now that we're trying to infer if an appropriate GPU is present as part of the basic build (falling back to CPU), I'm wondering if separate cpu/gpu docker images are still needed?\n. We've uncovered an issue with the GPU build procedure on some machines that we're currently looking into.  There's a good chance your kernels didn't get built (check neon/backends/kernels/cubin) which I think could lead to the error you are seeing.\n. Does Docker hub build on AWS?  Suspecting the reason you don't end up with kernels is because your build machine doesn't have a Maxwell capable GPU?\nUntil we build in this support (see #80) we'll need to do a better job of detecting and warning the user that this is the case.\n. We just pushed some changes that should address the kernel build issue that was introduced with the fix for #83.  \nWe've also modified things to build the kernels even on machines that don't have a Maxwell GPU, which should help with building the GPU based docker images.  You still won't be able to run the GPU backend without a Maxwell GPU though.  To remedy this we'll likely end up backporting nervanagpu kernels, and don't plan on resurrecting the cudanet backend.\n. wow 2 hours is pretty crazy, any idea what part of the build dominates?  I tried to login and view the build details page for a recent run but the logs panel was empty for me.  \nI just ran a clean GPU based build of neon and that took about 7 minutes start to finish (make so virtualenv based install).  One slowdown on a sysinstall based install currently is that building the kernels depends on maxas, which in turn depends on the virtualenv, so python packages end up getting installed twice -- once in the virtualenv and once in the system.  Fixing that should shave something like 5-10 minutes off of build time.\n. We've just pushed an update to neon which should remove the unnecessary virtualenv python dependency install as part of a system-wide install.  There's also a new make sysinstall_nodeps which could be even faster if you've already handled installing python dependencies elsewhere.\nAs to the memory consumption issue, what we think may be going on here is that the neon/backends/make_kernels.py utility used to compile cubin's and so forth is currently launching each of the nvcc subprocess calls simultaneously for all kernels.  We'll probably need to add some sort of throttling to the launch of these on machines with limited resources like the dockerhub build box.\n. Ok latest push throttles default number of concurrent kernel build processes to 10 (there were upwards of ~50 launching at the same time without this limit).  Hopefully this should be sufficient for the docker hub environment but if not you can further refine from the top level via:\nmake sysinstall -e KERNEL_BUILDER_BUILD_OPTS=\"--kernels --max_concurrent X\" where X is replaced with how many processes you can run simultaneously.\nTry playing around with that and let us know if you're still seeing issues.\n. thanks @chetan51 and @jcoreyes!\n. I believe this issue has been addressed in commit @1f4c906f which was pushed with v1.1.0 a couple of weeks ago.  Can you try again and let us know if you're still seeing the same problem?\n. Closing this as we now have embedding matrix support via our LookupTable layer, pushed as part of v1.1.0, and Scott pointed you at some RNN related benchmarks from the folks at Baidu.  As of v1.1.1 we've also got benchmark functionality baked into the Model class, so you can play around with that to get additional timings.\n. Hi Seb,\nI think this was actually fixed some time ago.  The behavior now is that we write layer info to the log but only if they've enabled info level logging by passing in -v.  By default this doesn't get printed anymore.\n. Hi,  Just to give you a long overdue update on this.  We're in the process of revamping data loading at the moment (including new macro batch support), and expect to push these changes publicly with the next neon release (v1.2.0).  This should happen around the beginning of December.\n. We pushed an updated data loader library (see the stuff in neon/data/loader along with our ImageLoader implementation in neon/data/imageloader.py) as part of the v1.1.2 release.  The data loader code allows you to ingest macro batches of data from disk in an efficient (multi-threaded and pipelined) manner and avoids the in-memory only limitations of DataIterator.  You can use neon.util.batch_writer.BatchWriter to prepare the macro batches, and examples/alexnet.py shows it in action.\nWe're still enhancing the macro batch format to make it more easily amenable to other data formats, but that should provide an initial starting point if you wanted to play around.  Unfortunately we're still somewhat tied into having to pre-specify the total number of batches in the dataset at this time but are thinking about ways to remove that.\n. Our updated macrobatch format is now available in v1.2.0 as of 6bf4891\nYou can find a tutorial on using it in the documentation\n. I think the condition was originally put in place as a guard to prevent warnings when none of the package_data paths could be found, something that could happen on systems without graphics cards for instance.  Since we distribute the .sass files now, these files will always be found, so you're right, it makes sense to remove this conditional.\nI'll provide a patch shortly, but note that the user you install as will still need to have nvcc in their PATH to ensure the .cubin's get built, prior to being copied into the install dir.  Suppose you could build as non-root with a user with nvcc, and then separately run sudo make sysinstall after.\n. Installation instructions are spelled out in the documentation ex. this page: http://neon.nervanasys.com/docs/latest/user_guide.html#installation\nBecause neon is a python package calling setuptools setup.py install will work but by using make based commands you'll get dependencies handled and get neon registered with pip's package manager in the case of a system-wide install.\nWe actually recommend the virtualenv based install over system-wide since you can do it all as an unprivileged user (which avoids the nvcc/sudo PATH issue you're facing), makes it easier to develop/extend neon (no reinstall needed, just edit code directly), and provides isolated sandboxes on multi-user systems or if you want to run multiple versions of neon.\n. Closing as #120 is now in master\n. Hi,\nThanks for your pull request.  Unfortunately we won't be able to accept it as it stands as we really don't want to introduce an extra reshape into the existing Linear layer across the board.  Until we actually add and document a proper Reshape layer your best bet is just to define your own custom layer and use that instead.\n. Now that we've pushed v1.1.0, both of your fixes should be included.  Thanks for spotting these!\n. Hi,\nI tried comparing a couple of equivalent convolutional nets on our CPU backend (running on a Late 2013 Macbook Pro w/ 2.3 GHz i7 processor) and I also see a slowdown with the newer release but nothing quite so dramatic as what you report:\nv1.1.1 (examples/cifar10_conv.py adjusted to turn off batch norm):\n(.venv)scottl@scottnrvlap:~/repo/neon> ./examples/cifar10_conv.py -b cpu\nEpoch 0   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  391/391  batches, 1.56 cost, 246.31s]\nEpoch 1   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  391/391  batches, 1.33 cost, 243.92s]\nEpoch 2   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  390/390  batches, 1.25 cost, 237.78s]\nv0.9.0 (cifar10-small.yaml adjusted to use uniform init, and full dataset instead of 10% sample that comes by default):\n```\n(v0.9.0)scottl@scottnrvlap:~/repo/neon> neon cifar10-small.yaml \nWARNING:neon.util.persist:deserializing object from:  cifar10-small.yaml\n2015-11-11 15:43:52,371 WARNING:neon - setting log level to: 20\n2015-11-11 15:43:52,371 INFO:cpu - Seeding random number generator with: None\n2015-11-11 15:43:52,375 INFO:init - CPU backend, RNG seed: None, numerr: None\n2015-11-11 15:43:52,378 INFO:mlp - Layers:\n    DataLayer d0: 3 x (32 x 32) nodes\n    ConvLayer layer1: 3 x (32 x 32) inputs, 16 x (28 x 28) nodes, Linear act_fn\n    PoolingLayer layer2: 16 x (28 x 28) inputs, 16 x (14 x 14) nodes, Linear act_fn\n    ConvLayer layer4: 16 x (14 x 14) inputs, 32 x (10 x 10) nodes, Linear act_fn\n    PoolingLayer layer5: 32 x (10 x 10) inputs, 32 x (5 x 5) nodes, Linear act_fn\n    FCLayer layer6: 800 inputs, 500 nodes, RectLin act_fn\n    FCLayer output: 500 inputs, 10 nodes, Logistic act_fn\n    CostLayer cost: 10 nodes, CrossEntropy cost_fn\n2015-11-11 15:43:52,416 INFO:val_init - Generating UniformValGen values of shape (75, 16)\n2015-11-11 15:43:52,435 INFO:val_init - Generating UniformValGen values of shape (400, 32)\n2015-11-11 15:43:52,436 INFO:val_init - Generating UniformValGen values of shape (500, 800)\n2015-11-11 15:43:52,445 INFO:val_init - Generating UniformValGen values of shape (10, 500)\n2015-11-11 15:43:52,447 INFO:cifar10 - loading: /Users/scottl/data/CIFAR10/cifar-10-batches-py/data_batch_1\n2015-11-11 15:43:52,447 WARNING:persist - deserializing object from:  /Users/scottl/data/CIFAR10/cifar-10-batches-py/data_batch_1\n2015-11-11 15:43:52,691 INFO:cifar10 - loading: /Users/scottl/data/CIFAR10/cifar-10-batches-py/data_batch_2\n2015-11-11 15:43:52,691 WARNING:persist - deserializing object from:  /Users/scottl/data/CIFAR10/cifar-10-batches-py/data_batch_2\n2015-11-11 15:43:52,931 INFO:cifar10 - loading: /Users/scottl/data/CIFAR10/cifar-10-batches-py/data_batch_3\n2015-11-11 15:43:52,931 WARNING:persist - deserializing object from:  /Users/scottl/data/CIFAR10/cifar-10-batches-py/data_batch_3\n2015-11-11 15:43:53,131 INFO:cifar10 - loading: /Users/scottl/data/CIFAR10/cifar-10-batches-py/data_batch_4\n2015-11-11 15:43:53,132 WARNING:persist - deserializing object from:  /Users/scottl/data/CIFAR10/cifar-10-batches-py/data_batch_4\n2015-11-11 15:43:53,326 INFO:cifar10 - loading: /Users/scottl/data/CIFAR10/cifar-10-batches-py/data_batch_5\n2015-11-11 15:43:53,326 WARNING:persist - deserializing object from:  /Users/scottl/data/CIFAR10/cifar-10-batches-py/data_batch_5\n2015-11-11 15:43:53,528 INFO:cifar10 - loading: /Users/scottl/data/CIFAR10/cifar-10-batches-py/test_batch\n2015-11-11 15:43:53,528 WARNING:persist - deserializing object from:  /Users/scottl/data/CIFAR10/cifar-10-batches-py/test_batch\n2015-11-11 15:43:53,698 WARNING:dataset - Incompatible batch size. Discarding 16 samples...\n2015-11-11 15:43:53,842 WARNING:dataset - Incompatible batch size. Discarding 80 samples...\n2015-11-11 15:43:54,521 WARNING:dataset - Incompatible batch size. Discarding 16 samples...\n2015-11-11 15:43:54,521 WARNING:dataset - Incompatible batch size. Discarding 80 samples...\n2015-11-11 15:43:54,535 INFO:mlp - commencing model fitting\n2015-11-11 15:45:22,090 INFO:mlp - epoch: 1, training error: 2.67356\n2015-11-11 15:46:47,932 INFO:mlp - epoch: 2, training error: 2.17991\n2015-11-11 15:48:14,780 INFO:mlp - epoch: 3, training error: 1.95347\n```\nSo on this architecture we're looking at a slowdown of about 2.8x going from v0.9.0 to v1.1.1.  Not insignificant but certainly not the ~1,000x you were seeing.\nThe differences that I see between this network and yours is that you are using Adadelta instead of GradientDescentMomentum, and you have non-square filters in your conv and pooling layers.\nSwapping in Adadelta for v1.1.1 cifar_conv showed no real difference in run-time:\n(.venv)scottl@scottnrvlap:~/repo/neon> ./examples/cifar10_conv.py -b cpu -e 3\nEpoch 0   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  391/391  batches, 2.03 cost, 239.54s]\nEpoch 1   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  391/391  batches, 1.94 cost, 240.31s]\nSo then I also tried changing the first layer conv filter to (1, 5, 16) and pooling to (1, 2):\n(.venv)scottl@scottnrvlap:~/repo/neon> ./examples/cifar10_conv.py -b cpu -e 3\nEpoch 0   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  391/391  batches, 2.03 cost, 486.60s]\nEpoch 1   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  391/391  batches, 1.95 cost, 491.00s]\nDefinite performance hit with non-square filters, but we're still only talking about a 5x difference or so from v0.9.0 (with square filters).\nOne final thing I noticed is that you are using CrossEntropyMulti cost with Logistic activation on the output.  If you have more than 2 output classes, you should switch the Logistic for Softmax so that you can take advantage of shortcut derivatives.  If you just have two outputs instead switch CrossEntropyMulti for CrossEntropyBinary.  I'd recommend trying that and also attempting to run the cifar10 conv examples I described across the two versions to ensure you see similar timing differences.\nTry that and let us know how it goes.\n. Fixed via #127 \n. Yes confirmed, we renamed this parameter in v1.1.0.  Thanks!\n. Hi guys,\nWe now have an internal patch that should restore the individual named arguments for Callbacks. We should be pushing that publicly in the next few days. \n-Scott\n. Closing as this behavior has been restored as of commit 41428445\n. Hi,\nThere are a few example networks using batch normalization.  See for instance: cifar10_conv.py and vgg_bn.py.\nYour last intuition is correct.  In newer versions of neon, setting batch_norm=True in a particular layer is all that is needed and you no longer need to separately specify an explicit BatchNorm() layer.\n. Thanks for spotting this!\n. Thanks for this.  Per Nvidia's own installation instructions (for Linux at least), they indicate that the user must set both PATH and LD_LIBRARY_PATH: http://docs.nvidia.com/cuda/cuda-getting-started-guide-for-linux/#post-installation-actions but hey who reads those anyway ;).\nThat being said I think its worth bringing this in, but there's one small issue in that attempts by the user to set HAS_GPU=false to force turning off the GPU will be ignored if CUDA is installed in one of the typical places.  Pretty rare case, but something we should still handle.\n. This is now available publicly (along with FastRCNN) via commits 811cd1a0 and 1c2250fa\n. We've got a backlog of changes internally that we'll push publicly tomorrow, and at that point we can bring this PR into master.\n. We've got a backlog of changes internally that we'll push publicly tomorrow, and at that point we can bring this into master.\n. Hi,\nThough we'd love to see a pull request for this, we don't have any plans internally to implement unpooling layers at this time.\nIf you want to see a successful application of the strided deconvolution approach, I'd recommend checking out @anlthms ' right whale localizer code\n. Closing this as it looks like Urs' 1x1 convolution suggestion should meet your needs.  Feel free to re-open if this isn't the case.\n. Thanks for providing this!  Some of the precision recall stuff was multi-label specific, overwrote buffers and so forth so I ended up modifying it to be more in-line with our other metrics.  I also squashed your history a bit to clean that up.\n. Hi,\nLooks like your environment doesn't support C++11 via the -std=c++11 flag.  CUDA's nvcc added support for this in v7 (not sure if this would have been before or after 7.0.27).  So you may need to upgrade that.  You'll also need to ensure you're using a version of gcc >= 4.7\n. Hi David,\nThanks for pointing this out.  We'll be pushing some updated documentation shortly which should help address this.  Note that the problem only occurs if you attempt to fit a model after first trying to do an eval (just fitting, just eval, fit then eval are all ok).\n. Hi,\nAre you referring to the tutorials described here: http://neon.nervanasys.com/docs/latest/introductory_resources.html, or the example networks in the examples directory, and described here: http://neon.nervanasys.com/docs/latest/examples.html?  If you click on the python files in the latter you'll see that most of them already do include paper references in the docstrings at the top.  We'll add those same references to the docs so its a bit more prominent.\n. Good point.  We'll roll this fix out with the next release, but longer term we do plan to support python3 as well\n. Hi William,\nLooking at your traceback it looks like that part of the code is attempting to compute the updated cost (i.e. error) by comparing a single mini-batch worth of model outputs against its expected target values.  \nTo make neon performant, one of the (many) things we do is collect together the sequence of required arithmetic operations into an OpTree so that they can be compounded and lazily evaluated at a later point, when they are actually needed.  In your case, this evaluation uncovered a shape mismatch between the operands while trying to perform an elementwise operation.\nAdmittedly, this lazy evaluation can make things more confusing and difficult to debug, and the error message itself is pretty uninformative.  As you've already discovered, the bottom line is that you need to make sure your number of output classes is a multiple of 4 when using the GPU backend on convolutional networks.  This is mentioned briefly here: http://neon.nervanasys.com/docs/latest/design.html\n. Closing as there hasn't been a response in 3+ weeks.  If you are still seeing this issue please let us know.  Also this may be related or a dupe of #176 \n. Hi,\nWe just posted the video and slides for a different Kaggle competition: Right Whale detection, given by @anlthms.  You can find links to those here: http://neon.nervanasys.com/docs/latest/introductory_resources.html#sv-deep-learning-meetup-2015-11-17\nAt this point I don't believe there are any plans to post something similar for the Winton challenge dataset, but we may revisit if we get some time.\n. Since you installed system wide, can you first check that /home/roshan/anaconda3/envs/python2/lib/python2.7/site-packages/neon/data/loader/loader.so exists and whatever user you are running as has the appropriate permissions to read/execute it?\n. That looks to me like a listing of your /home/roshan/neon/neon/data/loader dir.  From the first snippet, it appears neon was installed in a conda env under /home/roshan/anaconda3/envs/python2/lib/python2.7/site-packages/neon.  Can you list the data/loader directory under there instead?\n. Yes that's the issue.  It should get created as part of your make install.  If you are using conda, I'd recommend creating a conda env and using the environment file as described here: http://neon.nervanasys.com/docs/latest/user_guide.html#anaconda\n. thanks.  We've merged this in manually.\n. Going to close this for the time being. @yelite please re-open once you have a script that we can take a further look at.\n. Thanks for spotting this!\n. Brought in internally as part of v1.2.0 release\n. Brought in internally as part of v1.2.0 release.\n. Hi, thanks for pointing this out.  We've already changed the data loader internally so that aspect ratio can be maintained or adjusted during the scaling process, and we plan on pushing this publicly in our next release.\n. Just a heads up that the new data loader code has been pushed with the v1.2.0 release.\n. Hi,\nAre you on OSX?  It seems like reinstalling the requests python package may help:\npip install --upgrade requests per: \n- https://github.com/conda/conda/issues/1656\n- http://stackoverflow.com/questions/32986626/python-requests-importerror-cannot-import-name-headerparsingerror\n. Hi, thanks for pointing this out.\nWe actually have a fix already prepped internally that we'll be pushing out later this week.\nWe attempt to probe the GPU(s) to determine compute capability.  With this fix, if no GPU is found we will default to compute capability 5.0 instead of just failing outright.\n. We pushed the fix last night, and it looks like your cuda-neon builds are again completing successfully (let us know if there are any lingering issues, and we can re-open this).\n. Thanks!\n. thanks for spotting this.  We'll look to bring this in after our next public release.\n. Hi,\nYes restoring python 3.x support is definitely something on our radar, but hasn't yet made its way to the top of our TODO pile.  We'd absolutely welcome compatibility related contributions, but would probably want to start with expanding our testing infrastructure to ensure things correctly build, all unit tests pass, and examples run across different python versions.  We were using tox for this in the past, so that would probably be a good starting point.\n. Hi,\nSorry we shuffled priorities around and I never got around to removing this from the v1.3.0 milestone on github (so this shouldn't have been closed).  Since this task is currently on our internal backlog, I can't really provide an updated timeframe until the work is scoped and scheduled.\n. Yes, please see our response in #191 (closing this as a duplicate)\n. This looks like a dupe of #180 \n. This should be fixed as of a4b76c8 which we pushed during the v1.2.2 release yesterday.  Please let us know if you continue to spot problems here.\n. Just to piggyback off of what Hanlin already mentioned, nervanagpu is no longer under development as that code has been integrated directly into neon.\n. Keep us posted, and feel free to re-open if you have further questions.\n. thanks for spotting these!\n. You can get most of the way to your goal starting from examples/mnist_mlp.py so I suggest starting there.  Since this sounds an awful lot like a homework question, I'll leave the modifications as an exercise for the poster.\n. Hi Tambet,\nYou'll be happy to know that we recently corrected this inconsistent behavior as part of: 1eda2b0\n. can you change this to read: ...images of 100 classes.\n. Can you describe the coarse parameter in the docstring Attributes section (perhaps renaming to something more descriptive like coarse_targets?)  Also, instead of having it be part of the named constructor arguments, just let it be picked up and updated like the other kwargs.  The following should work:\ndef __init__(self, **kwargs):\n    self.macro_batched = False\n    self.coarse = False\n    self.__dict__.update(kwargs)\n. Sorry, one final thing here.  The data.backend.par = NoPar(data.backend) line currently won't work (and is in fact broken in datasets/tests/test_mnist.py too!).  You'll need to replace that line with:\npar = NoPar()\npar.associate(data.backend)\nThat should get the tests passing\n. Needs to be changed to:\npar = NoPar()\npar.associate(data.backend)\nPlease feel free to fix datasets/tests/test_mnist.py as well\n. ",
    "borromeotlhs": "Does this occur for all models?  Which model specifically were you attempting to run inference on (not that it matters, just would like to start from a known path to scope out breadth of the issue)?\n. ",
    "seba-1511": "Would this be continuous augmentation (every batch is augmented differently) or fixed augmentation (the images are all transformed before fitting) ?\n. Alright, working on this right now.\n. Cool, I just changed that in both files. (Hopefully it is correct) I did not know that GitHub would detect automatically an incremental change on top of a pull request.\nThanks for the comments and for the library ! \n. Cool ! (I meant the second one)\nAnd when using the hyperopt command with -n were you able to spread each optimization procedure across the GPUs ? Would neon be responsible for that ?\n. After hours of of looking, I realized I was loading the wrong experiment... Sorry about that, it has been one of these days.\n. Hello,\nAlright, I'll move the validation_split in the init() of the non BatchWriter datasets (and will rename it to validation_pct for consistency). I thought about the shuttling problem, but I assumed that it would only be the reference to memory that would be transfered. Is my assumption wrong, or is that kind of behavior outside of load that is problematic ? In the latter case, (and if having the batch approximation is not a problem) could we simply call the current split_set after format ? Otherwise, I'll work with the data before it is formatted.\nI can also do the print_validation approach, directly in the models. Is that fine if they both come in the same pull request ?\nHave a nice weekend,\nS\u00e9b\n. Thanks, just implemented that in #60 \n. Hello,\nThanks for the comments, I completely forgot to revert back to the train set. I just tested it with mnist small, and with the same random seed I get the same results when both printing and not printing the validation metrics, which should be news.\nOne thing I believe could still be improved is that after printing the metrics, I revert to the train set. But maybe the user used another set before printing the metric score. From looking at the DataLayer class, I can find a good way to get which set was being used before printing the metric score. Any idea ?\nHave a nice day,\nS\u00e9b\n. Alright, for some reason I thought that set_train_mode(False) was already in predict_generator and removed it in commit 2a6ddd1. I just re-added the line back, and testing with batch norm and dropout gives both times the same results, whether the validation metric is printed or not. \n. Good afternoon,\nDidn't have much time to work on this. I included the suggested modifications 1, 2, 3 and printing metrics for RNNs is coming soon. (the name of the parameter is now epoch_metrics)\nAs there might be a list of such metrics for each epoch (eg, in mnist-epoch_metrics-small.yaml), I added a \"header\" stating the epoch for which the metrics were printed.\n. Alright, I believe it is also working for RNNs. In addition I modified print_metric_score so that it takes a list of metrics for one set to avoid recomputing the predictions. One thing that could be nifty enhancement would be to print the metrics in a table so that it is easier to see their evolution. I am trying to see if there is a way to do it through python's logging.\n. Hello,\nI did not know about -o, that can be useful. If I do the table printout, it will be quite further in the future and so I believe it can wait for its own pull request.\nThank you for the comments and your help,\nS\u00e9b\n. One thing I believe works fairly well is the gen_backend() method, because it allows to easily switch from one backend to another. Adding a way to specify how the backend should be distributed (how many processes, etc...) programatically - as opposed to doing that when executing MPI - would be cool, but might not fit as a refactoring issue.\nIn addition, I like the idea of specifying models with JSON. It would be really nice to specify a model in JSON and then do something like load_model('path_to_json') and get back the model. I recently found out about util.persist.deserialize() but haven't used it yet. load_model could take parameters that would overwrite parameters in the JSON.\nFinally, as you pointed out, working with datasets is a bit of a mess. Especially, it is strange that doing stuff such as pre-processing the data involves writing your own Dataset class. And having the scikit-learn interface is a must have.\n. One additional thing that I find difficult to do and that might be useful, is to easily convert the backend and some parameters of the models. Example, I train a MLP on 4 GPUs with batch size 128 on Imagenet, and then I want to run it on a Raspberry Pi (on CPU) with batch size 1. Again, not sure if it fits in the refactoring.\n. Good evening,\nActually, I have an implementation for DQN and Gorila (with MPI) that works with neon. I am still polishing the code, and it does not really \"fit\" inside neon. (More of a side project built on top of it.) I was planning to make it public this weekend, but there are a few bugs remaining. I added @DoctorTeeth as a contributor to the repo, as he is also working on this.\nAlso, this DQN cost is probably wrong, an an updated version will be released. I'll post it here as soon as it works.\nS\u00e9b\n. Indeed, this is interesting. \nI ran into this problem when working with the predictions coming out of a net, but it seems that maxing on the other axis and then transposing would probably do the job. Anyway, thought that I'd point that out.\n. I've had the same issue when updating to 1.0. Could it be that your cuda_path/lib64 is not in your LD_LIBRARY_PATH ?\n. So my specific use case is for DQN. At every step of the game I get a new image vector on which I need to make a single prediction. Then in a later stage, all those images are used to create the training data for the function approximator. (this requires two more transfers) As anyway the data is on the device the first time, I should keep it there.\nWhat I think would be nice is to be able to create a DataIterator from a GPU/CPU array of Tensors. Maybe the easiest is to write a custom iterator, based on the DataIterator implementation. \n. @tambetm currently my implementation takes 0.12 seconds (on a 980) to make a a prediction on a single image, which is obviously unpractical. About the replay memory size, this is indeed an issue but my focus is primarily on having a working implementation rather than replicating DeepMind's results. \n@apark263 That's what I think I will end up doing, in a custom DataIterator class. Thank you.\n. ",
    "apark263": "This would be continuous augmentation, the same way they are currently\nflipped and shifted within the larger image, but with more exotic\ntransforms.\nOn Tuesday, June 9, 2015, S\u00e9bastien Arnold notifications@github.com wrote:\n\nWould this be continuous augmentation (every batch is augmented\ndifferently) or fixed augmentation (the images are all transformed before\nfitting) ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/12#issuecomment-110442892.\n. Color noise along with additional brightness/contrast adjustment will be added to the augmentation pipeline. \n. If you put your cuda bin directory in your path and run as non su, nvcc\nshould get picked up\n\nOn Friday, May 8, 2015, Scott Leishman notifications@github.com wrote:\n\nHi Andy,\nA couple of points that may help get you unstuck:\n- with the virtualenv based install, assuming user ubgpu created the\n  .venv directory, you should only need to run make install not sudo\n  make install as the python packages should be installed under that\n  directory.\n- The real reported issue looks to me like the CUDA compiler nvcc is\n  not being found. Locating this should already be addressed in our FAQ:\n  http://neon.nervanasys.com/docs/latest/faq.html#installation (and note\n  in particular that you need to ensure certain environment variables are\n  being passed through when run as sudo)\nGive those suggestions a try, and let me know if that helps\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/18#issuecomment-100323819.\n. Do you have the imagenet data files (the tar files containing the images)?\nThey are not distributed as part of neon, but you need to get them from\nilsvrc in order to run the imagenet example.\n\nAlex\nOn Saturday, May 16, 2015, Andy Yuan notifications@github.com wrote:\n\nubgpu@ubgpu:~/github/neon/neon$ neon --gpu nervanagpu\nexamples/convnet/i1k-alexnet-fp32.yaml\nWARNING:neon.util.persist:deserializing object from:\nexamples/convnet/i1k-alexnet-fp32.yaml\nWARNING:neon.datasets.imageset:Imageset initialized with dtype\n2015-05-15 22:00:54,319 WARNING:neon - setting log level to: 20\n2015-05-15 22:00:54,447 INFO:gpu - Initialized NervanaGPU with\nstochastic_round=None\n2015-05-15 22:00:54,447 INFO:gpu - Seeding random number generator with:\nNone\n2015-05-15 22:00:54,448 INFO:init - NervanaGPU backend, RNG seed: None,\nnumerr: None\n2015-05-15 22:00:54,449 INFO:mlp - Layers:\nImageDataLayer d0: 3 x (224 x 224) nodes\nConvLayer conv1: 3 x (224 x 224) inputs, 64 x (55 x 55) nodes, RectLin\nact_fn\nPoolingLayer pool1: 64 x (55 x 55) inputs, 64 x (27 x 27) nodes, Linear\nact_fn\nConvLayer conv2: 64 x (27 x 27) inputs, 192 x (27 x 27) nodes, RectLin\nact_fn\nPoolingLayer pool2: 192 x (27 x 27) inputs, 192 x (13 x 13) nodes, Linear\nact_fn\nConvLayer conv3: 192 x (13 x 13) inputs, 384 x (13 x 13) nodes, RectLin\nact_fn\nConvLayer conv4: 384 x (13 x 13) inputs, 256 x (13 x 13) nodes, RectLin\nact_fn\nConvLayer conv5: 256 x (13 x 13) inputs, 256 x (13 x 13) nodes, RectLin\nact_fn\nPoolingLayer pool3: 256 x (13 x 13) inputs, 256 x (6 x 6) nodes, Linear\nact_fn\nFCLayer fc4096a: 9216 inputs, 4096 nodes, RectLin act_fn\nDropOutLayer dropout1: 4096 inputs, 4096 nodes, Linear act_fn\nFCLayer fc4096b: 4096 inputs, 4096 nodes, RectLin act_fn\nDropOutLayer dropout2: 4096 inputs, 4096 nodes, Linear act_fn\nFCLayer fc1000: 4096 inputs, 1000 nodes, Softmax act_fn\nCostLayer cost: 1000 nodes, CrossEntropy cost_fn\n2015-05-15 22:00:54,449 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-05-15 22:00:54,450 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (363, 64)\n2015-05-15 22:00:54,452 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-05-15 22:00:54,453 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (1600, 192)\n2015-05-15 22:00:54,458 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-05-15 22:00:54,459 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (1728, 384)\n2015-05-15 22:00:54,469 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-05-15 22:00:54,470 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (3456, 256)\n2015-05-15 22:00:54,483 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-05-15 22:00:54,484 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (2304, 256)\n2015-05-15 22:00:54,492 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-05-15 22:00:54,493 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (4096, 9216)\n2015-05-15 22:00:54,964 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-05-15 22:00:54,965 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (4096, 4096)\n2015-05-15 22:00:55,175 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (1000, 4096)\n2015-05-15 22:00:55,229 WARNING:imageset - Batch dir cache not found in\n/home/ubgpu/data/I1K/imageset_batches/dataset_cache.pkl:\nPress Y to create, otherwise exit: Y\n/usr/local/lib/python2.7/dist-packages/neon/util/batch_writer.py:137:\nRuntimeWarning: divide by zero encountered in log10\nself.val_start = 10 * int(np.log10(self.ntrain * 10))\nTraceback (most recent call last):\nFile \"/usr/local/bin/neon\", line 199, in\nexperiment, result, status = main()\nFile \"/usr/local/bin/neon\", line 168, in main\nresult = experiment.run()\nFile\n\"/usr/local/lib/python2.7/dist-packages/neon/experiments/fit_predict_err.py\",\nline 97, in run\nsuper(FitPredictErrorExperiment, self).run()\nFile \"/usr/local/lib/python2.7/dist-packages/neon/experiments/fit.py\",\nline 70, in run\nself.dataset.load()\nFile \"/usr/local/lib/python2.7/dist-packages/neon/datasets/imageset.py\",\nline 176, in load\nself.bw.run()\nFile \"/usr/local/lib/python2.7/dist-packages/neon/util/batch_writer.py\",\nline 215, in run\nself.write_csv_files()\nFile \"/usr/local/lib/python2.7/dist-packages/neon/util/batch_writer.py\",\nline 137, in write_csv_files\nself.val_start = 10 * int(np.log10(self.ntrain * 10))\nOverflowError: cannot convert float infinity to integer\nubgpu@ubgpu:~/github/neon/neon$\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/26.\n. can you confirm that the following files are in  $repo_path/I1K  (where\nrepo_path is set as specified in the yam file):\n\nILSVRC2012_img_train.tar\nILSVRC2012_img_val.tar\nILSVRC2012_devkit_t12.tar.gz\nfrom the error it seems like the batch_writer is not finding the train tar\nfile.\nOn Sat, May 16, 2015 at 6:35 PM, Andy Yuan notifications@github.com wrote:\n\nyes, I have it and change the path of -fp32.yaml .\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/26#issuecomment-102720326.\n. What are your gpu specs?\n\nIf second phase is training, then what do you mean by first phase?\nOn Sunday, May 17, 2015, Andy Yuan notifications@github.com wrote:\n\nwhile I am running the second phase(training) of\nneon --gpu nervanagpu examples/convnet/i1k-alexnet-fp32.yaml\nI start the first phase of\nneon --gpu nervanagpu examples/convnet/i1k-alexnet-fp16.yaml\nit is OK!\nhowever,\nwhile the second phase of\nneon --gpu nervanagpu examples/convnet/i1k-alexnet-fp32.yaml\nstill ongoing, I launch the second phase of\nneon --gpu nervanagpu examples/convnet/i1k-alexnet-fp16.yaml\nit reports the error:\nubgpu@ubgpu:~/github/neon/neon$ neon --gpu nervanagpu\nexamples/convnet/i1k-alexnet-fp16.yaml\nWARNING:neon.util.persist:deserializing object from:\nexamples/convnet/i1k-alexnet-fp16.yaml\nWARNING:neon.datasets.imageset:Imageset initialized with dtype\n2015-05-17 14:50:39,937 WARNING:neon - setting log level to: 20\n2015-05-17 14:50:40,856 INFO:gpu - Initialized NervanaGPU with\nstochastic_round=None\n2015-05-17 14:50:40,856 INFO:gpu - Seeding random number generator with:\nNone\n2015-05-17 14:50:40,857 INFO:init - NervanaGPU backend, RNG seed: None,\nnumerr: None\n2015-05-17 14:50:40,858 INFO:mlp - Layers:\nImageDataLayer d0: 3 x (224 x 224) nodes\nConvLayer conv1: 3 x (224 x 224) inputs, 64 x (55 x 55) nodes, RectLin\nact_fn\nPoolingLayer pool1: 64 x (55 x 55) inputs, 64 x (27 x 27) nodes, Linear\nact_fn\nConvLayer conv2: 64 x (27 x 27) inputs, 192 x (27 x 27) nodes, RectLin\nact_fn\nPoolingLayer pool2: 192 x (27 x 27) inputs, 192 x (13 x 13) nodes, Linear\nact_fn\nConvLayer conv3: 192 x (13 x 13) inputs, 384 x (13 x 13) nodes, RectLin\nact_fn\nConvLayer conv4: 384 x (13 x 13) inputs, 256 x (13 x 13) nodes, RectLin\nact_fn\nConvLayer conv5: 256 x (13 x 13) inputs, 256 x (13 x 13) nodes, RectLin\nact_fn\nPoolingLayer pool3: 256 x (13 x 13) inputs, 256 x (6 x 6) nodes, Linear\nact_fn\nFCLayer fc4096a: 9216 inputs, 4096 nodes, RectLin act_fn\nDropOutLayer dropout1: 4096 inputs, 4096 nodes, Linear act_fn\nFCLayer fc4096b: 4096 inputs, 4096 nodes, RectLin act_fn\nDropOutLayer dropout2: 4096 inputs, 4096 nodes, Linear act_fn\nFCLayer fc1000: 4096 inputs, 1000 nodes, Softmax act_fn\nCostLayer cost: 1000 nodes, CrossEntropy cost_fn\n2015-05-17 14:50:40,858 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-05-17 14:50:40,860 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (363, 64)\n2015-05-17 14:50:40,862 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-05-17 14:50:40,863 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (1600, 192)\n2015-05-17 14:50:40,870 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-05-17 14:50:40,871 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (1728, 384)\n2015-05-17 14:50:40,888 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-05-17 14:50:40,889 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (3456, 256)\n2015-05-17 14:50:40,914 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-05-17 14:50:40,915 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (2304, 256)\n2015-05-17 14:50:40,931 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-05-17 14:50:40,932 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (4096, 9216)\n2015-05-17 14:50:42,483 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-05-17 14:50:42,484 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (4096, 4096)\n2015-05-17 14:50:43,188 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (1000, 4096)\n2015-05-17 14:50:43,391 INFO:fit - Unable to find saved model\n/home/ubgpu/data/I1K/I1K_alexnet_fp16_model.prm, starting over\n2015-05-17 14:50:43,393 INFO:mlp - commencing model fitting\nTraceback (most recent call last):\nFile \"/usr/local/bin/neon\", line 199, in\nexperiment, result, status = main()\nFile \"/usr/local/bin/neon\", line 168, in main\nresult = experiment.run()\nFile\n\"/usr/local/lib/python2.7/dist-packages/neon/experiments/fit_predict_err.py\",\nline 97, in run\nsuper(FitPredictErrorExperiment, self).run()\nFile \"/usr/local/lib/python2.7/dist-packages/neon/experiments/fit.py\",\nline 99, in run\nself.model.fit(self.dataset)\nFile \"/usr/local/lib/python2.7/dist-packages/neon/models/mlp.py\", line\n141, in fit\nself.fprop()\nFile \"/usr/local/lib/python2.7/dist-packages/neon/models/mlp.py\", line 81,\nin fprop\nll.fprop(y)\nFile \"/usr/local/lib/python2.7/dist-packages/neon/layers/layer.py\", line\n373, in fprop\nself.batch_idx)\nFile \"/usr/local/lib/python2.7/dist-packages/neon/datasets/imageset.py\",\nline 314, in get_mini_batch\nself.backend.subtract(self.inp_be, self.mean_be, self.inp_be)\nFile \"/usr/local/lib/python2.7/dist-packages/neon/backends/gpu.py\", line\n643, in subtract\nself.ng.subtract(left, right, out=out)\nFile \"/usr/local/lib/python2.7/dist-packages/nervanagpu/nervanagpu.py\",\nline 801, in subtract\ndef subtract (self, a, b, out=None): return OpTreeNode.build(\"sub\", a, b,\nout=out)\nFile \"/usr/local/lib/python2.7/dist-packages/nervanagpu/nervanagpu.py\",\nline 915, in build\nreturn OpTreeNode({ \"op\" : \"assign\" }, out, node).execute()\nFile \"/usr/local/lib/python2.7/dist-packages/nervanagpu/nervanagpu.py\",\nline 924, in execute\nreturn call_compound_kernel(\nget_rand_state(), stack) File\n\"/usr/local/lib/python2.7/dist-packages/nervanagpu/float_ew.py\", line 835,\nin call_compound_kernel kernel = _get_compound_kernel(tuple(type_args))\nFile \"\", line 2, in _get_compound_kernel File\n\"/usr/local/lib/python2.7/dist-packages/pycuda/tools.py\", line 423, in\ncontext_dependent_memoize result = func(args) File\n\"/usr/local/lib/python2.7/dist-packages/nervanagpu/float_ew.py\", line 670,\nin get_compound_kernel module = _get_module(template, template_vals) File\n\"/usr/local/lib/python2.7/dist-packages/nervanagpu/float_ew.py\", line 313,\nin _get_module return SourceModule(code, options=[\"--use_fast_math\" ],\nkeep=False) #,\"-G\" File\n\"/usr/local/lib/python2.7/dist-packages/pycuda/compiler.py\", line 251, in\n__init\narch, code, cache_dir, include_dirs)\nFile \"/usr/local/lib/python2.7/dist-packages/pycuda/compiler.py\", line\n241, in compile\nreturn compile_plain(source, options, keep, nvcc, cache_dir)\nFile \"/usr/local/lib/python2.7/dist-packages/pycuda/compiler.py\", line 73,\nin compile_plain\nchecksum.update(preprocess_source(source, options, nvcc).encode(\"utf-8\"))\nFile \"/usr/local/lib/python2.7/dist-packages/pycuda/compiler.py\", line 47,\nin preprocess_source\nresult, stdout, stderr = call_capture_output(cmdline,\nerror_on_nonzero=False)\nFile \"/usr/local/lib/python2.7/dist-packages/pytools/prefork.py\", line\n197, in call_capture_output\nreturn forker[0].call_capture_output(cmdline, cwd, error_on_nonzero)\nFile \"/usr/local/lib/python2.7/dist-packages/pytools/prefork.py\", line 54,\nin call_capture_output\n% ( \" \".join(cmdline), e))\npytools.prefork.ExecError: error invoking 'nvcc --preprocess\n--use_fast_math -arch sm_52\n-I/usr/local/lib/python2.7/dist-packages/pycuda/cuda /tmp/tmpIRAwNd.cu\n--compiler-options -P': [Errno 2] No such file or directory\nubgpu@ubgpu:~/github/neon/neon$\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/30.\n. You shouldn't need to write batches again for fp16. Both formats are\nintended to use the same underlying data.\n\nCan you run nvidia-smi when just the fp32 version is in training mode to\nsee how much memory is being used?\nAlso, sgray says you will probably get better overall performance by\nrunning the two training runs serially rather than in parallel.  Just in\ncase you are holding off on training while waiting to be able to get\nparallel working.\nOn Sunday, May 17, 2015, Andy Yuan notifications@github.com wrote:\n\nmy gpu is gtx970\nthe first phase I means like, maybe we should call it 'writing batches'\nubgpu@ubgpu:~/github/neon/neon$ neon --gpu nervanagpu\nexamples/convnet/i1k-alexnet-fp16.yaml\nWARNING:neon.util.persist:deserializing object from:\nexamples/convnet/i1k-alexnet-fp16.yaml\nWARNING:neon.datasets.imageset:Imageset initialized with dtype\n2015-05-16 20:21:56,367 WARNING:neon - setting log level to: 20\n2015-05-16 20:21:56,791 INFO:gpu - Initialized NervanaGPU with\nstochastic_round=None\n2015-05-16 20:21:56,791 INFO:gpu - Seeding random number generator with:\nNone\n2015-05-16 20:21:56,792 INFO:init - NervanaGPU backend, RNG seed: None,\nnumerr: None\n2015-05-16 20:21:56,793 INFO:mlp - Layers:\nImageDataLayer d0: 3 x (224 x 224) nodes\nConvLayer conv1: 3 x (224 x 224) inputs, 64 x (55 x 55) nodes, RectLin\nact_fn\nPoolingLayer pool1: 64 x (55 x 55) inputs, 64 x (27 x 27) nodes, Linear\nact_fn\nConvLayer conv2: 64 x (27 x 27) inputs, 192 x (27 x 27) nodes, RectLin\nact_fn\nPoolingLayer pool2: 192 x (27 x 27) inputs, 192 x (13 x 13) nodes, Linear\nact_fn\nConvLayer conv3: 192 x (13 x 13) inputs, 384 x (13 x 13) nodes, RectLin\nact_fn\nConvLayer conv4: 384 x (13 x 13) inputs, 256 x (13 x 13) nodes, RectLin\nact_fn\nConvLayer conv5: 256 x (13 x 13) inputs, 256 x (13 x 13) nodes, RectLin\nact_fn\nPoolingLayer pool3: 256 x (13 x 13) inputs, 256 x (6 x 6) nodes, Linear\nact_fn\nFCLayer fc4096a: 9216 inputs, 4096 nodes, RectLin act_fn\nDropOutLayer dropout1: 4096 inputs, 4096 nodes, Linear act_fn\nFCLayer fc4096b: 4096 inputs, 4096 nodes, RectLin act_fn\nDropOutLayer dropout2: 4096 inputs, 4096 nodes, Linear act_fn\nFCLayer fc1000: 4096 inputs, 1000 nodes, Softmax act_fn\nCostLayer cost: 1000 nodes, CrossEntropy cost_fn\n2015-05-16 20:21:56,793 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-05-16 20:21:56,794 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (363, 64)\n2015-05-16 20:21:56,796 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-05-16 20:21:56,797 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (1600, 192)\n2015-05-16 20:21:56,803 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-05-16 20:21:56,804 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (1728, 384)\n2015-05-16 20:21:56,815 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-05-16 20:21:56,816 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (3456, 256)\n2015-05-16 20:21:56,830 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-05-16 20:21:56,831 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (2304, 256)\n2015-05-16 20:21:56,841 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-05-16 20:21:56,842 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (4096, 9216)\n2015-05-16 20:21:57,386 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-05-16 20:21:57,387 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (4096, 4096)\n2015-05-16 20:21:57,628 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (1000, 4096)\n2015-05-16 20:21:57,688 WARNING:imageset - Batch dir cache not found in\n/home/ubgpu/data/I1K/imageset_batches_16/dataset_cache.pkl:\nPress Y to create, otherwise exit: Y\n2015-05-16 20:22:04,634 INFO:batch_writer - Loaded synset tars.\n2015-05-16 20:22:04,634 INFO:batch_writer - Building trainset list ( can\ntake a while)...\n2015-05-16 20:22:04,634 INFO:batch_writer - 0% ...\n2015-05-16 20:24:16,828 INFO:batch_writer - 10% ...\n2015-05-16 20:25:50,283 INFO:batch_writer - 20% ...\n2015-05-16 20:27:43,577 INFO:batch_writer - 30% ...\n2015-05-16 20:29:44,691 INFO:batch_writer - 40% ...\n2015-05-16 20:31:35,079 INFO:batch_writer - 50% ...\n...............\n2015-05-17 03:27:14,005 WARNING:persist - serializing object to:\n/home/ubgpu/data/I1K/imageset_batches_16/dataset_cache.pkl\n2015-05-17 03:27:21,257 WARNING:imageset - Done writing batches - please\nrerun to train.\nubgpu@ubgpu:~/github/neon/neon$\nhere are the resource while runing the second phase of\nneon --gpu nervanagpu examples/convnet/i1k-alexnet-fp32.yaml\nubgpu@ubgpu:~$ nvidia-smi\nSun May 17 15:13:37 2015\n+------------------------------------------------------+\n| NVIDIA-SMI 346.46 Driver Version: 346.46 |\n|-------------------------------+----------------------+----------------------+\n| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |\n| Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |\n|===============================+======================+======================|\n| 0 GeForce GTX 970 Off | 0000:01:00.0 N/A | N/A |\n| 62% 70C P2 N/A / N/A | 1817MiB / 4095MiB | N/A Default |\n+-------------------------------+----------------------+----------------------+\n+-----------------------------------------------------------------------------+\n| Processes: GPU Memory |\n| GPU PID Type Process name Usage |\n|=============================================================================|\n| 0 C+G Not Supported |\n+-----------------------------------------------------------------------------+\nubgpu@ubgpu:~$\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/30#issuecomment-102894538.\n. You can use the sum_squared cost (see neon/transforms/sum_squared.py)\n\nWe'll try to provide an example at some point, but it should be as simple\nas swapping the CrossEntropy item in the CostLayer to be SumSquaredDiffs\ninstead\nOn Mon, May 18, 2015 at 3:41 PM, Alex Rothberg notifications@github.com\nwrote:\n\nWhere the output is real values (R^n) and the loss function is something\nlike MSE.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/33.\n. seems like nvcc is not in your path.\n\nTry adding the cuda bin dir that has nvcc to your user path and not running\nas sudo, or export the path variable when you run as sudo.\nOn Tuesday, May 19, 2015, Andy Yuan notifications@github.com wrote:\n\nwhile I am runung 'neon --gpu nervanagpu\nexamples/convnet/i1k-alexnet-fp32.yaml\n' on training about 50 hours , I had to restart my computer, when I resume\nthe same command, the error happens, here the log:\nubgpu@ubgpu:~/github/neon/neon$ neon --gpu nervanagpu\nexamples/convnet/i1k-alexnet-fp32.yaml\n[sudo] password for ubgpu:\nWARNING:neon.util.persist:deserializing object from:\nexamples/convnet/i1k-alexnet-fp32.yaml\nWARNING:neon.datasets.imageset:Imageset initialized with dtype\n2015-05-18 20:44:17,102 WARNING:neon - setting log level to: 20\n2015-05-18 20:44:17,233 INFO:gpu - Initialized NervanaGPU with\nstochastic_round=None\n2015-05-18 20:44:17,233 INFO:gpu - Seeding random number generator with:\nNone\n2015-05-18 20:44:17,234 INFO:init - NervanaGPU backend, RNG seed: None,\nnumerr: None\n2015-05-18 20:44:17,234 INFO:mlp - Layers:\nImageDataLayer d0: 3 x (224 x 224) nodes\nConvLayer conv1: 3 x (224 x 224) inputs, 64 x (55 x 55) nodes, RectLin\nact_fn\nPoolingLayer pool1: 64 x (55 x 55) inputs, 64 x (27 x 27) nodes, Linear\nact_fn\nConvLayer conv2: 64 x (27 x 27) inputs, 192 x (27 x 27) nodes, RectLin\nact_fn\nPoolingLayer pool2: 192 x (27 x 27) inputs, 192 x (13 x 13) nodes, Linear\nact_fn\nConvLayer conv3: 192 x (13 x 13) inputs, 384 x (13 x 13) nodes, RectLin\nact_fn\nConvLayer conv4: 384 x (13 x 13) inputs, 256 x (13 x 13) nodes, RectLin\nact_fn\nConvLayer conv5: 256 x (13 x 13) inputs, 256 x (13 x 13) nodes, RectLin\nact_fn\nPoolingLayer pool3: 256 x (13 x 13) inputs, 256 x (6 x 6) nodes, Linear\nact_fn\nFCLayer fc4096a: 9216 inputs, 4096 nodes, RectLin act_fn\nDropOutLayer dropout1: 4096 inputs, 4096 nodes, Linear act_fn\nFCLayer fc4096b: 4096 inputs, 4096 nodes, RectLin act_fn\nDropOutLayer dropout2: 4096 inputs, 4096 nodes, Linear act_fn\nFCLayer fc1000: 4096 inputs, 1000 nodes, Softmax act_fn\nCostLayer cost: 1000 nodes, CrossEntropy cost_fn\n2015-05-18 20:44:17,234 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-05-18 20:44:17,236 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (363, 64)\n2015-05-18 20:44:17,237 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-05-18 20:44:17,238 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (1600, 192)\n2015-05-18 20:44:17,244 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-05-18 20:44:17,245 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (1728, 384)\n2015-05-18 20:44:17,255 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-05-18 20:44:17,256 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (3456, 256)\n2015-05-18 20:44:17,269 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-05-18 20:44:17,270 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (2304, 256)\n2015-05-18 20:44:17,279 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-05-18 20:44:17,280 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (4096, 9216)\n2015-05-18 20:44:17,748 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-05-18 20:44:17,749 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (4096, 4096)\n2015-05-18 20:44:17,959 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (1000, 4096)\n2015-05-18 20:44:18,016 INFO:fit - Unable to find saved model\n/home/ubgpu/data/I1K/I1K_alexnet_fp32_model.prm, starting over\n2015-05-18 20:44:18,017 INFO:mlp - commencing model fitting\nTraceback (most recent call last):\nFile \"/usr/local/bin/neon\", line 199, in\nexperiment, result, status = main()\nFile \"/usr/local/bin/neon\", line 168, in main\nresult = experiment.run()\nFile\n\"/usr/local/lib/python2.7/dist-packages/neon/experiments/fit_predict_err.py\",\nline 97, in run\nsuper(FitPredictErrorExperiment, self).run()\nFile \"/usr/local/lib/python2.7/dist-packages/neon/experiments/fit.py\",\nline 99, in run\nself.model.fit(self.dataset)\nFile \"/usr/local/lib/python2.7/dist-packages/neon/models/mlp.py\", line\n141, in fit\nself.fprop()\nFile \"/usr/local/lib/python2.7/dist-packages/neon/models/mlp.py\", line 81,\nin fprop\nll.fprop(y)\nFile \"/usr/local/lib/python2.7/dist-packages/neon/layers/layer.py\", line\n373, in fprop\nself.batch_idx)\nFile \"/usr/local/lib/python2.7/dist-packages/neon/datasets/imageset.py\",\nline 314, in get_mini_batch\nself.backend.subtract(self.inp_be, self.mean_be, self.inp_be)\nFile \"/usr/local/lib/python2.7/dist-packages/neon/backends/gpu.py\", line\n643, in subtract\nself.ng.subtract(left, right, out=out)\nFile \"/usr/local/lib/python2.7/dist-packages/nervanagpu/nervanagpu.py\",\nline 801, in subtract\ndef subtract (self, a, b, out=None): return OpTreeNode.build(\"sub\", a, b,\nout=out)\nFile \"/usr/local/lib/python2.7/dist-packages/nervanagpu/nervanagpu.py\",\nline 915, in build\nreturn OpTreeNode({ \"op\" : \"assign\" }, out, node).execute()\nFile \"/usr/local/lib/python2.7/dist-packages/nervanagpu/nervanagpu.py\",\nline 924, in execute\nreturn call_compound_kernel(\nget_rand_state(), stack) File\n\"/usr/local/lib/python2.7/dist-packages/nervanagpu/float_ew.py\", line 835,\nin call_compound_kernel kernel = _get_compound_kernel(tuple(type_args))\nFile \"\", line 2, in _get_compound_kernel File\n\"/usr/local/lib/python2.7/dist-packages/pycuda/tools.py\", line 423, in\ncontext_dependent_memoize result = func(args) File\n\"/usr/local/lib/python2.7/dist-packages/nervanagpu/float_ew.py\", line 670,\nin get_compound_kernel module = _get_module(template, template_vals) File\n\"/usr/local/lib/python2.7/dist-packages/nervanagpu/float_ew.py\", line 313,\nin _get_module return SourceModule(code, options=[\"--use_fast_math\" ],\nkeep=False) #,\"-G\" File\n\"/usr/local/lib/python2.7/dist-packages/pycuda/compiler.py\", line 251, in\n__init\narch, code, cache_dir, include_dirs)\nFile \"/usr/local/lib/python2.7/dist-packages/pycuda/compiler.py\", line\n241, in compile\nreturn compile_plain(source, options, keep, nvcc, cache_dir)\nFile \"/usr/local/lib/python2.7/dist-packages/pycuda/compiler.py\", line 73,\nin compile_plain\nchecksum.update(preprocess_source(source, options, nvcc).encode(\"utf-8\"))\nFile \"/usr/local/lib/python2.7/dist-packages/pycuda/compiler.py\", line 47,\nin preprocess_source\nresult, stdout, stderr = call_capture_output(cmdline,\nerror_on_nonzero=False)\nFile \"/usr/local/lib/python2.7/dist-packages/pytools/prefork.py\", line\n197, in call_capture_output\nreturn forker[0].call_capture_output(cmdline, cwd, error_on_nonzero)\nFile \"/usr/local/lib/python2.7/dist-packages/pytools/prefork.py\", line 54,\nin call_capture_output\n% ( \" \".join(cmdline), e))\npytools.prefork.ExecError: error invoking 'nvcc --preprocess\n--use_fast_math -arch sm_52\n-I/usr/local/lib/python2.7/dist-packages/pycuda/cuda /tmp/tmpltSwQ9.cu\n--compiler-options -P': [Errno 2] No such file or directory\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/34.\n. Snapshotting as a feature is a work in progress.  It's easy enough to do in\nthe fit function of mlp.py by calling serialize(self.get_params(),\nself.serialized_path), but for yaml exposure we're trying to decide on a\nway that isn't confusing to the user.\n\nFor now you can just save out incrementally and extend the number of epochs\neach time.  (i.e. use num_epochs: 10, save out, then extend to 20, etc._\nOn Tue, May 19, 2015 at 11:02 AM, CAWEHbKA notifications@github.com wrote:\n\nThere is also another problem. It is in line:\n2015-05-18 20:44:18,016 INFO:fit - Unable to find saved model\n/home/ubgpu/data/I1K/I1K_alexnet_fp32_model.prm, starting over\nAll the previous execution was not saved. If you fix you cuda problem, you\nstill will not be able to continue the previous execution...\nSomebody knows how to configure neon to save snapshots?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/34#issuecomment-103538586.\n. Serialization on a schedule for snapshotting has now been added\n\nOn Tue, May 19, 2015 at 12:09 PM, Scott Leishman notifications@github.com\nwrote:\n\nThe incremental save and epoch extend approach that is currently\nimplemented is described in a bit more detail here:\nhttp://neon.nervanasys.com/docs/latest/experiments.html?highlight=saving#generating-predictions\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/34#issuecomment-103569410.\n. Hi Thomas,\n\nYes, the Nervana cuda-convnet2 needs to be installed in order to run gpu backend examples on kepler cards.\n-Alex\n. Hi Thomas,\nPlease don't use the google code repository for compiling cuda-convnet2 if you plan to use the cudanet backend .  It is not compatible with neon and does not have the cudanet python interface.\nJust grab the Nervana Systems cuda-convnet2 fork and use the pip install instructions.\n(In addition to providing the cudanet python interface, we have made some modifications to the core cuda-convnet2)\n. Hi,\nWe do not support data parallelization yet but it is in progress.\n-Alex\nOn Thu, May 21, 2015 at 5:16 AM, CAWEHbKA notifications@github.com wrote:\n\nI would like to understand if NEON supports data parallelization on\nmultiple GPUs?\nI want to use multiple GPUs in order to increase mini-batch size...\nThe second question if MPI parallelization can be used with -gpu option?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/36.\n. We are following the convention here of displaying progress as EPOCH_IDX.BATCH_IDX\nwhere BATCH_IDX can be adjusted using the step_print variable in the yaml file.  We are not showing percentage complete.\n. You can set your PYTHONPATH to point to your home directory neon which will\navoid the need to keep installing while making frequent changes.\n\nOn Monday, June 1, 2015, Keren Zhou notifications@github.com wrote:\n\nHi,\nI find that the issue might be related with the search path. I go into the\ndirectory neon and then try to make install, then the ~/neon/bin/neon\nprogram will invoke the python files under\n.local/lib/python2.7/site-packages/neon/datasets. If I modify the file\nunder that directory, it works.\nI do not know whether my operation is right: Modify the file, and then make\ninstall each time?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/44#issuecomment-107577369.\n. We have capability for shuffling within a macrobatch for imageset based\ndatasets. It hasn't been pushed to external master yet, but it could be a\ngood compromise between full set shuffling and just presenting the same\nminibatches on each epoch.\n\nOn Friday, June 19, 2015, Scott Leishman notifications@github.com wrote:\n\nHi,\nBy default there is no such shuffling before each epoch in neon. Most of\nthe models are setup to run mini-batched gradient descent, where the\nbatch_size model parameter defines how many examples are in each batch.\nIf you wanted to shuffle mini-batch order within an epoch that should be\npretty straight forward and I'd recommend having a look in the DataLayer\nclass code. I'd suggest creating a subclass and overriding the\nhas_more_data and fprop functions. If however you wanted to shuffle\nindividual examples this will be more complex and result in a lot of\noverhead (as data is stored on device in mini-batches and accessed as such).\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/49#issuecomment-113699995.\n. Imageset was originally meant to support training and testing of ImageNet\n1k models, however we have tried to make it general enough to handle the\ncase that you are describing.\n\nMost of the modifications you are looking for can be handled by supplying\nthe appropriate options to batchwriter, which preprocesses the images into\nmacrobatches (it also converts non-jpg images to jpegs for decoding by\nImageset). The types of formats that batchwriter can support are whatever\nimage formats that can be read by PIL.\nRegarding not wanting to use random cropping or flipping: If you just\npreprocess the images to the final size that you want to test, then set\ncropped_image_size and output_image_size to that fixed image size, then no\ncropping will occur. If you also specify dotransforms=False, no flips will\noccur.\nOn Saturday, June 20, 2015, Keren Zhou notifications@github.com wrote:\n\nThank you!\n@apark263 https://github.com/apark263, after reading codes these days,\nI notice what you mentioned about the Imagest.\nI am currently working on modifying Imageset for my own tasks. Imageset\nis a useful tool to read large set of images from the disk, whereas there\nare some additional features we do not need such as cropping images.\nFurthermore, it can not be used for testing data and pictures other than\njpeg.\nI guess the imageset is originally designed for contest dataset such as\nimagenet?\nMaybe there could be a another class, which is the subclass of the Dataset\nand the super class of ImageSet, only provides the feature of reading\nhuge amount of images. Or we should modify the batch_writer?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/49#issuecomment-113724690.\n. Please install the latest pycuda from github or from the pycuda website.\nThe pypi distribution is out of date.\n\nhttps://github.com/inducer/pycuda\nOn Monday, June 15, 2015, zhengdong914 notifications@github.com wrote:\n\ni've installed nervanagpu sucessfully,when i run \"neon --gpu nervanagpu\nexamples/convnet/i1k-alexnet-fp32.yaml\" mistakes happens as below:\ndsp@dsp:~/neon$ neon --gpu nervanagpu\nexamples/convnet/i1k-alexnet-fp32.yaml\nWARNING:neon.util.persist:deserializing object from:\nexamples/convnet/i1k-alexnet-fp32.yaml\nWARNING:neon.datasets.imageset:Imageset initialized with dtype\n2015-06-15 22:35:55,300 WARNING:neon - setting log level to: 20\n2015-06-15 22:35:55,385 INFO:gpu - Initialized NervanaGPU with\nstochastic_round=None\n2015-06-15 22:35:55,385 INFO:gpu - Seeding random number generator with:\nNone\n2015-06-15 22:35:55,386 INFO:init - NervanaGPU backend, RNG seed: None,\nnumerr: None\n2015-06-15 22:35:55,386 INFO:mlp - Layers:\nImageDataLayer d0: 3 x (224 x 224) nodes\nConvLayer conv1: 3 x (224 x 224) inputs, 64 x (55 x 55) nodes, RectLin\nact_fn\nPoolingLayer pool1: 64 x (55 x 55) inputs, 64 x (27 x 27) nodes, Linear\nact_fn\nConvLayer conv2: 64 x (27 x 27) inputs, 192 x (27 x 27) nodes, RectLin\nact_fn\nPoolingLayer pool2: 192 x (27 x 27) inputs, 192 x (13 x 13) nodes, Linear\nact_fn\nConvLayer conv3: 192 x (13 x 13) inputs, 384 x (13 x 13) nodes, RectLin\nact_fn\nConvLayer conv4: 384 x (13 x 13) inputs, 256 x (13 x 13) nodes, RectLin\nact_fn\nConvLayer conv5: 256 x (13 x 13) inputs, 256 x (13 x 13) nodes, RectLin\nact_fn\nPoolingLayer pool3: 256 x (13 x 13) inputs, 256 x (6 x 6) nodes, Linear\nact_fn\nFCLayer fc4096a: 9216 inputs, 4096 nodes, RectLin act_fn\nDropOutLayer dropout1: 4096 inputs, 4096 nodes, Linear act_fn\nFCLayer fc4096b: 4096 inputs, 4096 nodes, RectLin act_fn\nDropOutLayer dropout2: 4096 inputs, 4096 nodes, Linear act_fn\nFCLayer fc1000: 4096 inputs, 1000 nodes, Softmax act_fn\nCostLayer cost: 1000 nodes, CrossEntropy cost_fn\n2015-06-15 22:35:55,386 INFO:batch_norm - BatchNormalization set to train\nmode\nTraceback (most recent call last):\nFile \"/home/dsp/anaconda/bin/neon\", line 6, in\nexec(compile(open(file).read(), file, 'exec'))\nFile \"/home/dsp/neon/bin/neon\", line 240, in\nexperiment, result, status = main()\nFile \"/home/dsp/neon/bin/neon\", line 207, in main\nexperiment.initialize(backend)\nFile \"/home/dsp/neon/neon/experiments/fit_predict_err.py\", line 62, in\ninitialize\nsuper(FitPredictErrorExperiment, self).initialize(backend)\nFile \"/home/dsp/neon/neon/experiments/fit.py\", line 62, in initialize\nself.model.initialize(backend)\nFile \"/home/dsp/neon/neon/models/mlp.py\", line 61, in initialize\nll.initialize(kwargs)\nFile \"/home/dsp/neon/neon/layers/convolutional.py\", line 39, in initialize\nsuper(ConvLayer, self).initialize(kwargs)\nFile \"/home/dsp/neon/neon/layers/layer.py\", line 479, in initialize\nself.bn.initialize(kwargs)\nFile \"/home/dsp/neon/neon/transforms/batch_norm.py\", line 90, in initialize\nself._xhat = self.backend.zeros(self.in_shape, dtype=self.dtype)\nFile \"/home/dsp/neon/neon/backends/gpu.py\", line 582, in zeros\nreturn self.ng.zeros(shape, dtype=dtype)\nFile\n\"/home/dsp/anaconda/lib/python2.7/site-packages/nervanagpu/nervanagpu.py\",\nline 483, in zeros\nname=name, rounding=self.round_mode)._assign(0)\nFile\n\"/home/dsp/anaconda/lib/python2.7/site-packages/nervanagpu/nervanagpu.py\",\nline 298, in _assign\ndrv.memset_d32_async(self.gpudata,\nAttributeError: 'module' object has no attribute 'memset_d32_async'\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/50.\n. Try reducing your batch size down to 32 and see if the problem still\nexists. If it runs then you probably don't have enough memory to train at\nmb=128\n\nIs there any particular reason you are using this system to train?  You\nwould get much better performance by using a more standard graphics card.\nOn Monday, June 29, 2015, yuehusile notifications@github.com wrote:\n\nthanks scttl! editing neon/backends/init.py works, but I still can't\nrun this sample because another erro appears:\nroot@tegra-ubuntu:/home/hsl/neon# neon --gpu cudanet\nexamples/convnet/i1k-alexnet-fp32.yaml\nWARNING:neon.util.persist:deserializing object from:\nexamples/convnet/i1k-alexnet-fp32.yaml\nWARNING:neon.datasets.imageset:Imageset initialized with dtype\n2015-07-01 04:52:29,170 WARNING:neon - setting log level to: 20\n2015-07-01 04:52:31,733 INFO:init - Cudanet backend, RNG seed: None,\nnumerr: None\n2015-07-01 04:52:31,735 INFO:mlp - Layers:\nImageDataLayer d0: 3 x (224 x 224) nodes\nConvLayer conv1: 3 x (224 x 224) inputs, 64 x (55 x 55) nodes, RectLin\nact_fn\nPoolingLayer pool1: 64 x (55 x 55) inputs, 64 x (27 x 27) nodes, Linear\nact_fn\nConvLayer conv2: 64 x (27 x 27) inputs, 192 x (27 x 27) nodes, RectLin\nact_fn\nPoolingLayer pool2: 192 x (27 x 27) inputs, 192 x (13 x 13) nodes, Linear\nact_fn\nConvLayer conv3: 192 x (13 x 13) inputs, 384 x (13 x 13) nodes, RectLin\nact_fn\nConvLayer conv4: 384 x (13 x 13) inputs, 256 x (13 x 13) nodes, RectLin\nact_fn\nConvLayer conv5: 256 x (13 x 13) inputs, 256 x (13 x 13) nodes, RectLin\nact_fn\nPoolingLayer pool3: 256 x (13 x 13) inputs, 256 x (6 x 6) nodes, Linear\nact_fn\nFCLayer fc4096a: 9216 inputs, 4096 nodes, RectLin act_fn\nDropOutLayer dropout1: 4096 inputs, 4096 nodes, Linear act_fn\nFCLayer fc4096b: 4096 inputs, 4096 nodes, RectLin act_fn\nDropOutLayer dropout2: 4096 inputs, 4096 nodes, Linear act_fn\nFCLayer fc1000: 4096 inputs, 1000 nodes, Softmax act_fn\nCostLayer cost: 1000 nodes, CrossEntropy cost_fn\n2015-07-01 04:52:31,738 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-07-01 04:52:32,228 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (363, 64)\n2015-07-01 04:52:32,254 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-07-01 04:52:32,340 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (1600, 192)\n2015-07-01 04:52:32,370 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-07-01 04:52:32,432 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (1728, 384)\n2015-07-01 04:52:32,506 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-07-01 04:52:32,552 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (3456, 256)\n2015-07-01 04:52:32,602 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-07-01 04:52:32,639 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (2304, 256)\n2015-07-01 04:52:32,691 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-07-01 04:52:32,702 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (4096, 9216)\n2015-07-01 04:52:34,805 INFO:batch_norm - BatchNormalization set to train\nmode\n2015-07-01 04:52:34,813 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (4096, 4096)\n2015-07-01 04:52:35,728 INFO:val_init - Generating AutoUniformValGen\nvalues of shape (1000, 4096)\nTraceback (most recent call last):\nFile \"/usr/local/bin/neon\", line 240, in\nexperiment, result, status = main()\nFile \"/usr/local/bin/neon\", line 207, in main\nexperiment.initialize(backend)\nFile\n\"/usr/local/lib/python2.7/dist-packages/neon/experiments/fit_predict_err.py\",\nline 62, in initialize\nsuper(FitPredictErrorExperiment, self).initialize(backend)\nFile \"/usr/local/lib/python2.7/dist-packages/neon/experiments/fit.py\",\nline 62, in initialize\nself.model.initialize(backend)\nFile \"/usr/local/lib/python2.7/dist-packages/neon/models/mlp.py\", line 68,\nin initialize\ndtype=self.layers[1].deltas_dtype)\nFile \"/usr/local/lib/python2.7/dist-packages/neon/backends/cc2.py\", line\n536, in zeros\ndtype=dtype)),\nMemoryError\nIs memory size a problem? tegra k1 has 2GB memory. or just something else\nlead to this problem? any advice for me to check out what happened?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/51#issuecomment-116924730.\n. Do you mean parallelism as in launching multiple runs each one assigned to\na different gpu?  If so there is no way to specify it via yaml, you have to\nlaunch separate runs from command line using the -i option to specify which\ndevice you want to run on.  If you mean parallelism as in splitting a\nsingle run to run on several gpus at once, it is something that we are\nworking on adding to neon and is close to being added to the main\nrepository.\n\nOn Fri, Jun 19, 2015 at 3:40 PM, S\u00e9bastien Arnold notifications@github.com\nwrote:\n\nIs there a way to define the parallelism in the YAML file ? It could be\nuseful for example when optimizing the hyperparameters and using all the\nGPUs in the machine. (I am not sure if doing hyperopt -n THREADS will\nspread the tasks across the GPUs)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/52.\n. This is an experimental branch whose github version doesn't work yet. The\nworking one is Maxwell only and will be merged in to the next release of\nneon.\n\nOn Thursday, June 25, 2015, Keren Zhou notifications@github.com wrote:\n\nI tried to checkout this branch, and run the\nexample/convnet/mnist-small.yaml. But there are even indentation errors in\nthe file:\nWARNING:neon.util.persist:deserializing object from: mnist-small.yaml\n2015-06-26 11:26:21,774 WARNING:neon - setting log level to: 20\nTraceback (most recent call last):\nFile \"/home/zxx/Install/python2.7.9/bin/neon\", line 6, in\nexec(compile(open(file).read(), file, 'exec'))\nFile \"/home/zxx/neon_dev/bin/neon\", line 236, in\nexperiment, result, status = main()\nFile \"/home/zxx/neon_dev/bin/neon\", line 198, in main\ndevice_id=args.device_id)\nFile \"/home/zxx/neon_dev/neon/backends/init.py\", line 98, in gen_backend\nfrom neon.backends.cc2 import GPU\nFile \"/home/zxx/neon_dev/neon/backends/cc2.py\", line 1367\ndef update_fc_bias(self, err, out):\n^\nIndentationError: unindent does not match any outer indentation level\nDoes that indicate the master is the only stable branch we could use?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/53.\n. Try printing the shapes of the associated tensors at the spot where the\nerror occurs and let us know what it says.\n(I.e. start, stop, reference.shape and batch_refs.shape)\n\nOn Monday, June 29, 2015, S\u00e9bastien Arnold notifications@github.com wrote:\n\nI trained a mlp on CIFAR10, and deserialized it in a later script. (That\nstep works fine, I have the correct weights and everything I need.) When I\ncall mlp.predict_fullset, cudanet raises the error:\nTraceback (most recent call last):\nFile \"11_merge_predictions.py\", line 202, in\nmlp.predict_fullset(data, 'test')\nFile \"/home/ubuntu/neon/neon/models/mlp.py\", line 245, in predict_fullset\nreference[:, start:end] = batch_refs\nFile \"/home/ubuntu/neon/neon/backends/cc2.py\", line 265, in setitem\nself._tensor.set_col_slice(start, stop, value)\nFile \"/home/ubuntu/cuda-convnet2/cudanet/cudanet.py\", line 767, in\nset_col_slice\nraise generate_exception(err_code)\ncudanet.cudanet.CUDANetException: Incompatible matrix dimensions.\nI am again predicting on the test set of CIFAR10, and the GPU I use is the\none on AWS. (~4gb) It is also the same as the one I used for training.\nFurther more, mlp.predict_generator works fine.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/55.\n. try installing maxas directly first:\n\nhttps://github.com/NervanaSystems/maxas\nThe nervanagpu makefile should be trying to install it for you, but it\nseems to have failed.  or perhaps it's installed but the path to it is not\navailable to your sudo.\nOn Fri, Jul 10, 2015 at 2:06 PM, dbl001 notifications@github.com wrote:\n\nMakefile:\ndefine list_includes\n$(shell sed -rn 's/^/\\1/p' $(call strip_codes,$(1)))\nendef\nDavid-Laxers-MacBook-Pro:nervanagpu davidlaxer$ sudo !!\nsudo make all\nThe directory '/Users/davidlaxer/Library/Caches/pip/http' or its parent\ndirectory is not owned by the current user and the cache has been disabled.\nPlease check the permissions and owner of that directory. If executing pip\nwith sudo, you may want sudo's -H flag.\ninstalling maxas...\n/tmp/nervanagpu.XXXXXXXX.waaER91X\nCloning into 'maxas'...\nremote: Counting objects: 170, done.\nremote: Total 170 (delta 0), reused 0 (delta 0), pack-reused 170\nReceiving objects: 100% (170/170), 163.15 KiB | 0 bytes/s, done.\nResolving deltas: 100% (67/67), done.\nChecking connectivity... done.\nChecking if your kit is complete...\nLooks good\nWarning: prerequisite Carp 1.29 not found. We have 1.26.\nWarning: prerequisite Data::Dumper 2.145 not found. We have 2.13506.\nWriting Makefile for MaxAs::MaxAs\nWriting MYMETA.yml and MYMETA.json\ncp lib/MaxAs/MaxAs.pm blib/lib/MaxAs/MaxAs.pm\ncp lib/MaxAs/Cubin.pm blib/lib/MaxAs/Cubin.pm\ncp lib/MaxAs/MaxAsGrammar.pm blib/lib/MaxAs/MaxAsGrammar.pm\ncp bin/maxas.pl blib/script/maxas.pl\n/opt/local/bin/perl5.16 -MExtUtils::MY -e 'MY->fixin(shift)' --\nblib/script/maxas.pl\nManifying blib/man3/MaxAs::MaxAs.3pm\nAppending installation info to\n/opt/local/lib/perl5/5.16.3/darwin-thread-multi-2level/perllocal.pod\nsed: illegal option -- r\nusage: sed script [-Ealn] [-i extension] [file ...]\nsed [-Ealn] [-i extension] [-e script] ... [-f script_file] ... [file ...]\nbuilding kernel: hgemm_nn_128x128...\nmake: maxas.pl: No such file or directory\nmake: *** [nervanagpu/kernels/cubin/hgemm_nn_128x128.cubin] Error 1\nDavid-Laxers-MacBook-Pro:nervanagpu davidlaxer$ sed -r\nsed: illegal option -- r\nusage: sed script [-Ealn] [-i extension] [file ...]\nsed [-Ealn] [-i extension] [-e script] ... [-f script_file] ... [file ...]\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/59.\n. Hi David,\n\nI noticed that you are trying to install nervanagpu on a macbook pro.  I wanted to point out that nervanagpu currently only supports maxwell class gpus, so even if it gets installed, you probably will not be able to use it.\n. Top K metrics added\n. They will be added later -- in the meantime, you can follow the template of how the current Conv layers are implemented and input the 3 dimensional parameters in there.\n. Try getting the latest commit and doing a make clean and make.  Build issues should be fixed.\n. > pycuda._driver.RuntimeError: cuModuleLoad failed: file not found\nThis message indicates that the kernel wasn't built.\n. The right way to handle this is to allocate a batch GPU tensor that is going to be used for the function approximator.  When you get the image vector from the single prediction, store it in one of the slots of the batch tensor.  You can predict on that single slice, and then perform the training when the batch tensor is full.\n. the shape of your inputs are probably incompatible with your weights\n. make sure that the batch size of your input is the same as the batch size when you trained the network.  you can zero-pad if you need to.\n. it's impossible to tell without seeing your code\n. so before when you said it still didn't work you were just using approximately batch_size examples?\nif you have more input vectors than batch_size, use multiple batches.  If you have fewer (say K), then zero pad the input buffer and only look at the first K columns of the output\n. This is a clean up error that can be disregarded -- a fix is on the way.\n. Fastest way to handle this is to transfer the batchsize major host (numpy)\narray directly onto gpu with the same memory layout and then perform the\ntranspose on gpu, which will be much faster using the element wise\ntranspose kernel.\nh_a = np.empty((N, M))\nd_a = be.empty((N, M))\nd_aT = be.empty((M, N))\nd_a.set(h_a)  # host to device transfer\nd_aT[:] = d_a.T  # device to device transpose\nIf you are using the cpu backend you won't see any benefit, in fact it will\nprobably be slower than another cpu implementation you may be using.\nThe reason we use batchsize minor ordering in neon is because it has\nsuperior memory locality for convolution and for concatenation along\nfeature map dimensions.  From our perspective, the cpu backend is purely\nfor prototyping and testing for correctness.\nOn Wednesday, September 23, 2015, tambetm notifications@github.com wrote:\n\nI'm using Model.fprop() to implement prediction in my application. The\nproblem is, that fprop() expects the batch size dimension (number of\nsamples) to be the last. I can certainly create a Numpy array that matches\nthese expectations, but as I fill out my samples in batch one-by-one, when\nusing C-ordered array, filling out the Numpy matrix is quite slow (because\nof non-contiguous memory).\nfrom neon.util.argparser import NeonArgparser\nfrom neon.backends import gen_backend\nimport numpy as np\nparser = NeonArgparser()\nargs = parser.parse_args()\nbe = gen_backend(backend=args.backend,\n                 batch_size=32,\n                 rng_seed=args.rng_seed,\n                 device_id=args.device_id,\n                 default_dtype=args.datatype,\n                 stochastic_round=False)\ntensor = be.empty((4,84,84,32))\nworks, but filling minibatch is slow\nmatrix = np.empty((4,84,84,32))\nfor i in xrange(32):\n  matrix[...,i] = np.zeros((4,84,84)) # more complicated logic here\ntensor.set(matrix)\nI can use batch size dimension as first in Numpy matrix and permute\ndimensions when assigning to Neon tensor, but this gives an error:\ndoes not work\nmatrix = np.zeros((32,4,84,84))\ntensor.set(np.transpose(matrix, axes = (1, 2, 3, 0)))\nTraceback (most recent call last):\n  File \"test.py\", line 32, in \n    tensor.set(np.transpose(matrix, axes = (1, 2, 3, 0)))\n  File \"/home/tambet/neon_new/neon/backends/nervanagpu.py\", line 378, in set\n    self.dtype.itemsize * s for s in self.strides)\nAssertionError\nIf I make copy before assigning the permuted matrix, it works:\nworks, but unnecessary copy\nmatrix = np.zeros((32,4,84,84))\ntensor.set(np.transpose(matrix, axes = (1, 2, 3, 0)).copy())\nI also tried to match the dimensions of tensor and use F-ordering in my\nmatrix, so that filling it is still efficient. Unfortunately that didn't\nhelp:\nfast filling of minibatch, but does not work either\nmatrix = np.zeros((4,84,84,32), order='F')\ntensor.set(matrix)\nTraceback (most recent call last):\n  File \"test.py\", line 36, in \n    tensor.set(matrix)\n  File \"/home/tambet/neon_new/neon/backends/nervanagpu.py\", line 378, in set\n    self.dtype.itemsize * s for s in self.strides)\nAssertionError\nIs there a way to use efficient Numpy array layout in my case and avoid\ncopying of the array? Why fprop() has the batch_size as last dimension\nanyway, when Neon is using C-ordered arrays?\nBTW, I'm using Neon to implement DeepMind's deep Q-learning algorithm. It\nalready learns simple games like Pong, but not on par with DeepMind's\nresults yet. I take a pride in the very compact code, and also hope to make\nit fast thanks to Neon. See https://github.com/tambetm/simple_dqn.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/103.\n. Yes\n. You can pass  models.layers_to_optimize to the optimizers instead of using the original layers list. \n. > The documentation for gen_backend states that if backend='cpu' then it will create a single CPU core, float 32 backend.\n- Is this a feature that is on the roadmap?\n\nyes, we support single cpu backend\n\n\nIs this already implemented and this is just a documentation issue?\n\n\nit's implemented and should be documented\n\nI see that Multi GPU support is coming soon (if you set backend='mgpu'). Does the implementation of this lend to an easier implementation of MultiCPU support?\n\nMultiCPU and MultiGPU would be very different implementation-wise and one would not make the other any easier.  We currently have no plans to implement a MultiCPU backend, and even if we did, it would still lag single GPU by a very wide margin.   If you are interested in faster performance, my recommendation would be to try using the GPU backend.\n. that's a good idea -- would you be able to check your assumption by doing a quick modification of your nervanagpu?  you just need to change lines 384-394 in nervanagpu.py to take an ary as an arg, assert the size matches, and then remove line 392.\nwould be interesting to see if the time is materially improved.\n. if you're using softmax as an output activation, your cost function has to be multiclass cross entropy, not binomial cross entropy.  we don't support softmax -> binomial cross entropy or logistic -> multiclass cross entropy.\n. Hey Seb -- we'll be changing this to being conditioned off of verbosity.\n. Will be merged via rebase.\n. Affine layers implicitly de-sequencify sequential input.\nIn order to pass activations to a layer that demands sequential input\n(recurrent/lstm/gru), you have to re-introduce the sequential attribute of\nthe activations coming out of the affine layer.\nHowever, since affine layer is fully connected, there is no longer a causal\nrelationship between the inputs.\nIf you're ok with that, it would be easy enough to define a pass through\nlayer that just changes the in_shape to a recurrent tuple.\n(just make a layer that passes the inputs straight through, but the config\nstep has flat in_shape and sequential out_shape)\nOn Wed, Oct 28, 2015 at 9:48 AM, Yuri Baburov notifications@github.com\nwrote:\n\nfrom neon.backends import gen_backend\nfrom neon.initializers import Uniform\nfrom neon.layers import GeneralizedCost, Affine, Recurrent\nfrom neon.models import Model\nfrom neon.optimizers import RMSProp\nfrom neon.transforms import Tanh, CrossEntropyMulti\nfrom neon.callbacks.callbacks import Callbacks\nfrom neon.util.argparser import NeonArgparser\nfrom neon.transforms.activation import Rectlin, Softmax\nfrom neon.data.dataiterator import DataIterator\nfrom neon.layers.recurrent import RecurrentLast\nimport numpy\nparser = NeonArgparser(doc)\nargs = parser.parse_args()\nCLIP_GRADIENTS = True\nNUM_EPOCHS = args.epochs\nBATCH_SIZE = 4\nLEN = 20\nWIDTH = 170\nSTEPS = 20\nX = numpy.zeros((LEN, WIDTH*STEPS))\nY = numpy.zeros((LEN, 1))\nbe = gen_backend(backend=args.backend,\n                 batch_size=BATCH_SIZE,\n                 rng_seed=args.rng_seed,\n                 device_id=args.device_id,\n                 default_dtype=args.datatype)\ntrain_set = DataIterator(X, Y, 10, (WIDTH, STEPS), True)\nvalid_set = DataIterator(X, Y, 10, (WIDTH, STEPS), True)\ninit = Uniform(low=-0.08, high=0.08)\nlayers = [\n    Affine(230, init, bias=init, activation=Rectlin()), # comment out this layer and it will work again\n    Recurrent(140, init, activation=Tanh()), # attempts to run C(230, 80) += A(230, 170) * B(3400, 4)\n    RecurrentLast(),\n    Affine(10, init, bias=init, activation=Softmax()),\n]\ncost = GeneralizedCost(costfunc=CrossEntropyMulti(usebits=True))\nmodel = Model(layers=layers)\noptimizer = RMSProp(clip_gradients=CLIP_GRADIENTS, stochastic_round=args.rounding)\ncallbacks = Callbacks(model, train_set, output_file=args.output_file,\n                      valid_set=valid_set, valid_freq=args.validation_freq,\n                      progress_bar=args.progress_bar)\nmodel.fit(train_set,\n          optimizer=optimizer,\n          num_epochs=NUM_EPOCHS,\n          cost=cost,\n          callbacks=callbacks)\nNetwork Layers:\n    Linear Layer 'LinearLayer': 170 inputs, 230 outputs\n    Bias Layer 'BiasLayer': size 230\n    Activation Layer 'ActivationLayer': Rectlin\n    Recurrent RecurrentLayer\n    RecurrentOutput choice RecurrentLastLayer : (140, 20) inputs, 140 outputs\n    Linear Layer 'LinearLayer': 140 inputs, 10 outputs\n    Bias Layer 'BiasLayer': size 10\n    Activation Layer 'ActivationLayer': Softmax\nTraceback (most recent call last):\n  File \"mytest.py\", line 59, in \n    callbacks=callbacks)\n  File \"/usr/local/lib/python2.7/dist-packages/neon-1.0.0rc1-py2.7.egg/neon/models/model.py\", line 149, in fit\n    self._epoch_fit(dataset, callbacks)\n  File \"/usr/local/lib/python2.7/dist-packages/neon-1.0.0rc1-py2.7.egg/neon/models/model.py\", line 171, in _epoch_fit\n    x = self.fprop(x)\n  File \"/usr/local/lib/python2.7/dist-packages/neon-1.0.0rc1-py2.7.egg/neon/models/model.py\", line 202, in fprop\n    x = l.fprop(x, inference)\n  File \"/usr/local/lib/python2.7/dist-packages/neon-1.0.0rc1-py2.7.egg/neon/layers/layer.py\", line 503, in fprop\n    self.be.compound_dot(A=self.W, B=self.inputs, C=self.outputs, bsum=self.batch_sum)\n  File \"/usr/local/lib/python2.7/dist-packages/neon-1.0.0rc1-py2.7.egg/neon/backends/nervanagpu.py\", line 1091, in compound_dot\n    assert n == C.shape[1]\nAssertionError\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/122.\n. So conservatively speaking you are noticing 3 orders of magnitude slowdown?\n (200+ batches in under 1sec , vs .2 batches per second).\n\nCould you post your model architecture?  It is possible that we slowed cpu\nperformance in the course of updating backends (0.9 to 1.0 was a complete\nrewrite), but I didn't think it was that bad\nOn Sunday, November 1, 2015, felipefariax notifications@github.com wrote:\n\nHello!\nFirst of all, congratulations to all the team that is developing neon!\nI was using the version 0.9 and this was really fast on CPU.\nI updated the neon version to 1.1 and, with the same model architecture,\neach batch is taking ~5 seconds. On 0.9, hundreds of batches was done in\nless than 1s.\nI am using Convolutional Layers in my model.\nThe CPU Backend became slow in order to have the autodiff?\nIs there anything that I could do, or only a downgrade would give me the\nprior speed?\nThank you!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/125.\n. Pending fix will be pushed soon\n. We're delaying the fix until we're sure that it won't affect anything else, but in the meantime, adding the following lines to your existing code should accomplish the same thing:\n\nEnsure same parallelism mode all the way through the network as follows\nPrior to model initialization for self.model here\nfor l in self.model.layers.layers:\n    l.parallelism = 'Disabled'\nAnd prior to model initialization for self.target_model here\nfor l in self.target_model.layers.layers:\n    l.parallelism = 'Disabled'\nI should also note that you can now get_description from the model to accomplish your deep copy without serializing to disk (will still transfer weights from device to host and back) by doing:\npdict = self.model.get_description(get_weights=True)\nself.target_model.deserialize(pdict, load_states=False)\n. Could you give me some additional details of what you are trying to do?\nBy input layers, do you mean the input to the network?  Or do you just mean layers that are not the final cost.  And by values, do you mean the activations?  Or the parameters (weights) themselves?\nI imagine you are trying to do some kind of regularization by penalizing the magnitude of the weights -- is that correct?\n. What version of gcc do you have?\n. Hi,\nWe will push a fix for this soon, but in the meantime could you just do a\ncast to signed int for that offending line?\nThanks\nOn Wednesday, February 3, 2016, etienne87 notifications@github.com wrote:\n\nHello! I just cloned neon & make My config is Ubuntu 1504, so I'm guessing\ngcc5 could be responsible?\nBuilding loaderso\nIn file included from readerhpp:31:0,\nfrom loaderhpp:28,\nfrom loadercpp:19:\nbufferhpp: In instantiation of \u2018T* Buffer::getItem(int, int&) [with T =\nchar]\u2019:\nloaderhpp:190:63: required from here\nbufferhpp:109:19: error: comparison between signed and unsigned\ninteger expressions [-Werror=sign-compare]\nif (index >= _itemssize()) {\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/197.\n. We have a fix that wil be pushed within the next few days\n\nOn Monday, February 22, 2016, Alex Shevchenko notifications@github.com\nwrote:\n\nSeems like recurrent layers are using different pattern and hack in\ncontainer.py:69-70 doesn't really work.\nIntroduced in c10a2a5\nhttps://github.com/NervanaSystems/neon/commit/c10a2a5774e744efb91e3a62d095dfcef9e8c579\nand broken as of 8066f3f\nhttps://github.com/NervanaSystems/neon/commit/8066f3f945ede9db8b2e96db46ad785c088bb4ce\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/202.\n. We just tried it and couldn't reproduce that behavior (both on kepler and maxwell cards).  Could you give us some more info on your system configuration (card, distro, etc)\n\nHere's the diff:\n```\n--- a/examples/mnist_mlp.yaml\n+++ b/examples/mnist_mlp.yaml\n@@ -18,10 +18,7 @@ epochs: 10\n batchsize: 128\nwt_init: &wt_init\n-    type: Gaussian\n-    config:\n-        loc: 0.0\n-        scale: 0.01\n+    type: GlorotUniform\nlayers:\n -\n@@ -31,6 +28,14 @@ layers:\n         init: wt_init\n         activation:\n             type: Rectlin\n+\n+-\n+    type: Affine\n+    config:\n+        nout: 100\n+        init: wt_init\n+        activation:\n+            type: Rectlin\n -\n     type: Affine\n```\nand here's the output:\n:~/Code/neon$ bin/neon -d f32 -b gpu examples/mnist_mlp.yaml\nEpoch 0   [Train |-|  469/469  batches, 0.24 cost, 0.86s]\nEpoch 1   [Train |-|  469/469  batches, 0.20 cost, 0.91s]\nEpoch 2   [Train |-|  469/469  batches, 0.18 cost, 0.90s]\nEpoch 3   [Train |-|  468/468  batches, 0.16 cost, 0.89s]\nEpoch 4   [Train |-|  468/468  batches, 0.13 cost, 0.89s]\nEpoch 5   [Train |-|  468/468  batches, 0.10 cost, 0.88s]\nEpoch 6   [Train |-|  468/468  batches, 0.08 cost, 0.88s]\nEpoch 7   [Train |-|  468/468  batches, 0.08 cost, 0.88s]\nEpoch 8   [Train |-|  468/468  batches, 0.07 cost, 0.88s]\nEpoch 9   [Train |-|  468/468  batches, 0.06 cost, 0.89s]\n:~/Code/neon$ bin/neon -d f16 -b gpu examples/mnist_mlp.yaml\nEpoch 0   [Train |-|  469/469  batches, 0.25 cost, 0.89s]\nEpoch 1   [Train |-|  469/469  batches, 0.18 cost, 0.87s]\nEpoch 2   [Train |-|  469/469  batches, 0.14 cost, 0.87s]\nEpoch 3   [Train |-|  468/468  batches, 0.10 cost, 0.87s]\nEpoch 4   [Train |-|  468/468  batches, 0.09 cost, 0.87s]\nEpoch 5   [Train |-|  468/468  batches, 0.07 cost, 0.87s]\nEpoch 6   [Train |-|  468/468  batches, 0.09 cost, 0.87s]\nEpoch 7   [Train |-|  468/468  batches, 0.08 cost, 0.87s]\nEpoch 8   [Train |-|  468/468  batches, 0.06 cost, 0.87s]\nEpoch 9   [Train |-|  468/468  batches, 0.06 cost, 0.87s]\n. The number of outputs for affine should be a number rather than a tuple. In\nthis case you actually want 72x72x3, since it seems like your input is a 3\nchannel image, and you need to match your input size for a SSE cost\nThat should get rid of the error, but im not sure itll get the\nreconstruction performance you want\nOn Tuesday, March 1, 2016, Markus Woodson notifications@github.com wrote:\n\nI am trying to do something similar to what #145\nhttps://github.com/NervanaSystems/neon/issues/145 mentioned with my\noutput y being a 72x72 image. I followed the instructions in that issue to\ncreate my ArrayIterator and set my output layer to\nAffine((72,72), init=Gaussian(scale=0.03), bias=None, activation=Rectlin())\nThough when I try to train the network I get the following error:\nTraceback (most recent call last):\nFile \"correlation_encoder.py\", line 64, in\nmodel.fit(train_set, optimizer=opt, num_epochs=num_epochs, cost=cost,\ncallbacks=callbacks)\nFile \"/home/mwoodson/installs/neon/neon/models/model.py\", line 141, in fit\nself.initialize(dataset, cost)\nFile \"/home/mwoodson/installs/neon/neon/models/model.py\", line 103, in\ninitialize\ncost.initialize(prev_input)\nFile \"/home/mwoodson/installs/neon/neon/layers/layer.py\", line 1336, in\ninitialize\nparallelism=self.prev_layer.parallelism)\nFile \"/home/mwoodson/installs/neon/neon/backends/backend.py\", line 536, in\niobuf\npersist_values=persist_values)\nFile \"/home/mwoodson/installs/neon/neon/backends/nervanagpu.py\", line\n1192, in zeros\nrounding=self.round_mode).\n_assign(0) File\n\"/home/mwoodson/installs/neon/neon/backends/nervanagpu.py\", line 135, in\ninit\nself.gpudata = allocator(self.nbytes)\nBoost.Python.ArgumentError: Python argument types in\npycuda._driver.mem_alloc(tuple)\ndid not match C++ signature:\nmem_alloc(unsigned long)\nHelp here would be much appreciated. For reference my input dim is\n(128,15552) and output dim is (128,72,72).\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/208.\n. Yes, easiest thing to do would actually be to do some operation on the data\nitself (X_train) to make it single channel. There is specific weighting of\nthe color channels that needs to be done to convert from RGB into\ngrayscale. Once that is done change the shape tuple in ArrayIterayor to be\n(1,72,72)\n\nNote that the shape is only important if you are feeding the data into a\nlayer that needs some notion of shape, e.g. Pooling or convolution. If the\ndata is going straight into another affine layer, the shape is ignored\nOn Tuesday, March 1, 2016, Markus Woodson notifications@github.com wrote:\n\nThanks for the quick response. I am not exactly creating an autoencoder so\nreconstruction is not the objective. Since would like the output to simply\nbe a 72x72 grayscale image, changing my code to\nAffine(72*72, init=Gaussian(scale=0.03), bias=None, activation=Rectlin())\nwould mean I would have to change my ArrayIterator initialization correct?\nFor reference here is what it is now:\ntrain_set = ArrayIterator(X_train, y_train, nclass=y_train.shape[1],\nmake_onehot=False, lshape=(3,72,72))\nwhich was taken from #145\nhttps://github.com/NervanaSystems/neon/issues/145 .\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/208#issuecomment-190787876\n.\n. Yes that should let you proceed without error, but I'm still not sure that\nwill give you the results you want.  I suppose it depends on what your y\nand cost function are.\n\nOn Tuesday, March 1, 2016, Markus Woodson notifications@github.com wrote:\n\nAh sorry I think I should clarify. I would still like the input to be RGB\nbut my output would be grayscale(my output is not neccesarily an image but\na 2D array that can be considered an image). The idea is I would like to\nlearn a single filter but from what I have seen neon does not allow a Conv\nlayer with only 1 filter channel. Thus I changed it to an Affine layer with\nsize (72,72) as that was the filter dimension I desired.\nI have now changed the Affine layer to have nout=72_72 and my nclass=72_72\nin my ArrayIterator. Reshaping the output of the Affine layer to 72x72 will\nbe exactly what I desire correct?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/208#issuecomment-190803878\n.\n. mnist_mlp runs at .68s per epoch.  you need it to run faster?\n\nin all seriousness, we haven't released multi-gpu support for neon.  for now it is a cloud only feature.  things may change in the future.\n. good find -- some layers are indeed missing support beta for accumulation\nat the moment...  could you try switching the order of sidepath and\nmainpath in your MergeSum layer?  that may be a quick, albeit deeply\nunsatisfying fix for now.\nOn Fri, Apr 15, 2016 at 12:50 PM, Thouis (Ray) Jones \nnotifications@github.com wrote:\n\nThe code pasted below dies with this traceback:\n(I have other bugs related to this network structure, which I'll file\nseparately).\nI'm at 1.3.0+344372b.\ntrace\nTraceback (most recent call last):\n  File \"bsum_bug.py\", line 65, in \n    model.fit(train, optimizer=opt_gdm, num_epochs=num_epochs, cost=cost, callbacks=Callbacks(model))\n  File \"/net/seasfs02/srv/export/pfister_lab/share_root/thouis/neon/neon/models/model.py\", line 149, in fit\n    self._epoch_fit(dataset, callbacks)\n  File \"/net/seasfs02/srv/export/pfister_lab/share_root/thouis/neon/neon/models/model.py\", line 171, in _epoch_fit\n    x = self.fprop(x)\n  File \"/net/seasfs02/srv/export/pfister_lab/share_root/thouis/neon/neon/models/model.py\", line 202, in fprop\n    return self.layers.fprop(x, inference)\n  File \"/net/seasfs02/srv/export/pfister_lab/share_root/thouis/neon/neon/layers/container.py\", line 192, in fprop\n    x = l.fprop(x, inference)\n  File \"/net/seasfs02/srv/export/pfister_lab/share_root/thouis/neon/neon/layers/container.py\", line 396, in fprop\n    l.fprop(inputs, inference, beta=beta)\n  File \"/net/seasfs02/srv/export/pfister_lab/share_root/thouis/neon/neon/layers/container.py\", line 190, in fprop\n    x = l.fprop(x, inference, beta=beta)\nTypeError: fprop() got an unexpected keyword argument 'beta'\n\n\n\nimport pdb\np>>> pdb.pm()\n/net/seasfs02/srv/export/pfister_lab/share_root/thouis/neon/neon/layers/container.py(190)fprop()\n-> x = l.fprop(x, inference, beta=beta)\n(Pdb) p l\n\n(Pdb)\n\n\n\ncode\nimport numpy as np\nfrom neon.initializers import GlorotUniform, Constant, Uniform\nfrom neon.layers import Conv, GeneralizedCost, Dropout, SkipNode, Activation, Bias, Affine, MergeSum\nfrom neon.models import Model\nfrom neon.optimizers import GradientDescentMomentum\nfrom neon.transforms import Rectlin, CrossEntropyBinary, Softmax\nfrom neon.callbacks.callbacks import Callbacks\nfrom neon.util.argparser import NeonArgparser\nfrom neon.data import ArrayIterator\nparse the command line arguments\nparser = NeonArgparser(doc)\nargs = parser.parse_args()\nhyperparameters\nnum_epochs = args.epochs\nnetwork_depth = 16\nnum_features = 128\ndef conv_params(fsize, relu=True, batch_norm=True):\n    padding = {'pad_h': fsize[0] // 2, 'pad_w': fsize[1] // 2}  # always pad to preserve width and height\n    return dict(fshape=fsize,\n                activation=(Rectlin() if relu else None),\n                padding=padding,\n                batch_norm=batch_norm,\n                init=GlorotUniform())\ndef resnet_module(nfm):\n    sidepath = SkipNode()\n    mainpath = [Conv(conv_params((3, 3, nfm))),\n                Bias(Constant()),\n                Dropout(0.9),\n                Conv(conv_params((3, 3, nfm), relu=False)),\n                Bias(Constant())]\n    return [MergeSum([sidepath, mainpath]),\n            Activation(Rectlin())]\ndef build_model(depth, nfm):\n    # input - expand to #nfm feature maps\n    layers = [Conv(**conv_params((5, 5, nfm))),\n              Dropout(0.8)]\n```\nfor d in range(depth):\n    layers += resnet_module(nfm)\nfinal output: 1 channel\nlayers += [Dropout(0.5),\n           Affine(362, init=Uniform(-1.0 / (362 * nfm), 1.0 / (362 * nfm)), activation=Softmax())]\nreturn Model(layers=layers)\n```\nX = X = np.random.rand(10000, 48 * 19 * 19)  # X.shape = (10000, 48_19_19)\ny = np.random.randint(0, 362, 10000)  # y.shape = (10000, )\ntrain = ArrayIterator(X=X, y=y, nclass=362, lshape=(48, 19, 19))\ncost = GeneralizedCost(costfunc=CrossEntropyBinary())\nopt_gdm = GradientDescentMomentum(learning_rate=0.01,\n                                  momentum_coef=0.9,\n                                  stochastic_round=args.rounding)\nmodel = build_model(network_depth, num_features)\nmodel.fit(train, optimizer=opt_gdm, num_epochs=num_epochs, cost=cost, callbacks=Callbacks(model))\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/232\n. dropout, bias, and activation layers are a little bit more complicated because they don't own their own output buffers, so just accumulating as self.outputs[:] = self.outputs * beta + ... would yield the incorrect result.\n\nWe could make versions of these layers that do own their output buffers, but there would be a corresponding increase in memory usage, which could be significant for these deep networks.  In the meantime, the more recent resnet architectures do exhibit swap out-like behavior by allowing more direct paths through the network, and we have implemented that in our later cifar msra example.\n. oh, one thing i should mention is that bias is unnecessary if you are including batch norm since there is a bias-like term built in to the batch norm z-transform (the batch norm beta, not the accumulation beta)\nso #232 would probably be ameliorated a bit by removing the biases altogether.\nregarding the bsum issue for kepler arch, the problem is because we don't include the capability to calculate the sum across feature maps used for batch mean in those kernels.  To get around this issue there is a mode where batch sum is explicitly calculated outside of the kernel.  you can activate that by just providing a random seed on the command line (i.e. doing -r 0 when running script). \ni think that should work...\n. I'm not quite sure what you meant by position-dependent bias terms.  If you mean some conv layers having batch norm and no bias, and some having bias and no batch norm depending on their position in the module, then the answer would be to define another conv_params dict that has batch_norm=False and a bias.\nIn an upcoming release we should also have support for non-deterministic batch summation for kepler.\n. Yes you are correct that the comment indicates batch norm and bias but only has bias in the actual network. We will edit that comment. \n. I'm not sure why this would be the case, but I can no longer replicate this behavior after upgrading to driver version 375.. Try installing the system requirements for aeon listed here:  \nhttps://github.com/NervanaSystems/aeon/blob/master/README.md\nOn ubuntu, that is: libcurl4-openssl-dev clang libopencv-dev libsox-dev\n. ",
    "sihyun0826": "I found out that color jittering is not implemented yet, so the performance of original Alexnet couldn't be reproduced. It would be nice if we can use color augmentation too! \n. ",
    "Creative-Punch": "This is not a typo. Neon is short for \"NErvana's pythON based Deep Learning Framework\". Hence the \"ON\" in pythON (and the \"NE\" in \"NErvana\").\n. ",
    "ross-t": "A link to the example (for maximum laziness)\n. ",
    "ursk": "RNN models can now be run with both the NervanaGPU backend (fp32 and fp16 support), Cudanet backend (fp32) and CPU backend (fp32 and fp64). \n. Text, and more generally, temporal sequence datasets are handled a bit differently in neon from image datasets (what the example in the documentation refers to). You can use the Moby Dick dataset as a template, and adapt it to use your own data. To get started, look for indat = self.read_txt_file(repo_file, 'float32') in mobydick.py and replace repo_file by your own text file. This dataset assumes that all your data is in one long string, and it will break it up into minibatches for you. \n. @lglhuada we added a bit more documentation to the RNN about how the data is expected to be laid out, have a look at http://neon.nervanasys.com/docs/latest/generated/neon.models.rnn.RNN.html#neon.models.rnn.RNN \n. Hi Thomas,\ncuda-convnet2 has a dependency on the ATLAS libraries which need to be installed separately. Please see the documentation at the original cuda-convnet2 project page for details: https://code.google.com/p/cuda-convnet2/wiki/Compiling \n. Hi Thomas,\nNote that the other libraries you listed, e.g.\npython-devel-2.6.6-52.el6.x86_64\nnumpy-1.4.1-9.el6.x86_64\nare also too old, the minimum version for python is 2.7 and the minimum numpy 1.8.1. We recommend Anaconda Python, or installing Python through Homebrew, to easily get the latest versions. \n. Hi,\ncan you provide more information, what kind of system and which python distribution you are using? You might be better off opening a new issue for that, since this one has been closed for a while.\n. Hi rongou, \nI don't think it's safe to assume that nvcc will always be in the location /usr/local/cuda/bin/nvcc. You can try to use sudo -E  to preserve the environment variables, or just explicitly include the path in your sudo call. \n. Hi,\nwe actually require numpy 1.8.1. The easiest way to ensure that is by installing through pip (i.e. pip install numpy) and using a Python distribution such as Homebrew or Anaconda, as the system Python tends to be quite out of date. \n. HI S\u00e9b,\nthere are two things we do going from training to prediction mode: In predict_generator, there is the self.data_layer.use_set(setname, predict=True), which tells the dataset provider to take batches from the test set. Then in fit_predict_err we have self.model.set_train_mode(False) which touches the drop-out layers, switching them to deterministic mode, and batch normalization, switching to inference mode. All of these need to be undone if you want to continue training after running inference. \nIt should be relatively easy to check if you still get the same final error if you have the callback in place, as you get without it. Also make sure the validation misclass percentage is the same as what you get from the final metrics call, to verify that the dropout layers and batch norm layers switch mode correctly. \n. Hi S\u00e9b,\nIt great that the feature works with the MNIST-small model; see above though for difficulties I expect with dropout and batch normalization because they rely on self.model.set_train_mode. In this case, your feature would lead to unexpected behavior, i.e. you would get wrong metrics reported. Note the MNIST example does not use those layer types.\nAll dataset have, or should have, a training set, so generally I think it's safe to attempt to revert to that for continued training. \n. Hi Mark,\nthanks for pointing out the issue. This appears to be a bug with the current neon 0.9.0 release. As a quick workaround I would suggest that you use v0.8.2, where the example works with both the CPU and NervanaGPU backend. (Note that changing the default_dtype = np.float16 to float32 in nervanagpu.py will still be required). \nFor a more permanent solution, we will soon release a new version of neon with extensive improvements to RNNs and LSTMs.\n. It's hard to take a guess as to what could be going on without the details of the network you are trying to run. Some general comments, assuming that this is indeed a hyper-parameter issue and not a bug you have stumbled across: Making the network deeper can easily require retuning the learning rates, random weight initialization, and might not work at all unless the right combination of convolutional, pooling and fully connected layers are used. RMSProp is a very forgiving optimizer and can hide problems to some degree, but of course it's no magic bullet. We can't provide support for tuning custom network configurations, but please have a look at the other examples and try to go from there. If you find similar issues with the example networks, please let us know. \n. Dimitri,\nwe cannot run your code without access to 'cinput_vyb48.csv', but it's pretty likely that a network with six tanh layers and a fixed Gaussian(loc=0.0, scale=0.01) initialization is going to suffer from vanishing gradients and won't be able to learn. You may want to try with ReLu nonlinearities (http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_NairH10.pdf), with a more elaborate initialization scheme (http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization) or you can insert batch normalization layers between the tanh layers, like this:\nlayers = []\nlayers.append(Affine(nout=80, init=init_norm, activation=Tanh()))\nlayers.append(BatchNorm())\nlayers.append(Affine(nout=64, init=init_norm, activation=Tanh()))\nlayers.append(BatchNorm())\nlayers.append(Affine(nout=64, init=init_norm, activation=Tanh()))\nlayers.append(BatchNorm())\nlayers.append(Affine(nout=64, init=init_norm, activation=Tanh()))\nlayers.append(BatchNorm())\nlayers.append(Affine(nout=20, init=init_norm, activation=Softmax()))\nany of those should get the network unstuck.\n. The be.array() function, per our documentation, \"converts a numpy array to a GPUTensor\". \nIf you are trying to create a reference to b without copying any data, why not just \nb = a. If you are trying to copy data from GPU tensor to GPU tensor without going via host, \na = be.zeros(b.shape)\na[:] = b\nwould do the job, and it's only slightly more verbose. \n. Thanks for finding this bug. We have confirmed that your suggested fix is correct, and will include it in the next update. \n. Hi,\nby your own words, you get 95% accuracy on CPU and 5% misclassification on GPU, so it doesn\u2019t seem like a bug.  We checked the code and did not find any issues. \n. It's a bit hard for us to follow what is required to reproduce the behavior that you are seeing. For example if you are training with a SumSquared cost, it seems like the model is doing regression, and it's not clear how you measure misclassification. \nGenerally, if you find a bug, the best way to go is to provide a minimal example that reproduces it. Preferably this would be modifying one of the examples slightly, but still using the original example data so that we can run it. We will then do our best to provide a speedy fix. \n. Looks like you may not have virtualenv installed on your machine. Have a look at http://neon.nervanasys.com/docs/latest/user_guide.html#installation for requirements that need to be satisfied before you can run make. \nAlso note that with your hardware (GeForce GTX 675MX), you will not be able to use the GPU and you will be limited to the CPU backend. \n. Just for context, Kepler generation cards are now a couple of years behind current technology and we recently dropped support in neon. While we are planning to add support for these GPUs again in the future, the performance of a high end Kepler card will only be about that of a $200 GTX960 Maxwell card, which has enough memory to support most current models. If you do not have access to a machine with a current generation Maxwell GPU (e.g. GTX960, GTX980, Titan X), we recommend running on the CPU backend. Older versions of neon (e.g. 0.8.2) did support Kepler GPUs, but there have been significant API changes in neon so we do not recommend using this older version. \n. Thanks for your contribution. We are going add your fix to the next point release of neon. \n. We currently have no plans to add OpenCL support to neon. Community contributions are always welcome though! A good starting point would be to look at Andreas Kl\u00f6ckner's PyOpenCL package, that provides a very similar interface to his PyCUDA that neon is using for CUDA.\n. Can you elaborate a bit what your use case it? Typically a deconv layer would take a feature map with some spatial dimension, and convolve that feature map with a filter. As you noticed, the affine layer has no concept of spatial dimensions in neon. \nIn cases where we want to treat an affine layer as a convolutional layer with a spatial feature map of 1x1 pixels and the number of channels equal to the number of units in the layer, we do just that, make it a convolutional layer with feature map size 1x1. Would that work for your application? \n. Thanks for finding this! I tried it on a couple of Titan X's and can confirm the issue, with about the same frequency as you get on the 750. We will look into what's causing it. \n. Oleg, we have a fix for the issue that will be included in the next point release of neon. Thanks again!\n. We are actively working on python 3 support right now, and it is going to come back soon. Thanks for your patience!\n. Glad that you got it to work. Shouldn't that be deterministic_update to True?\n. Can you run the following checks to make sure the GPUs, Cuda and PyCuda are being recognized correctly?\nnvidia-smi\nnvcc --version\npython -m pycuda.autoinit\nFor the last one make sure that the neon virtualenv has been activated. \n. Something went wrong during installation, and the pycuda package, which is a dependency for neon, did not get installed. It looks like you are using anaconda python, which does not support the standard virtualenv installation. There is a community contributed neon install script for anaconda you can find through the install docs at http://neon.nervanasys.com/docs/latest/user_guide.html#anaconda \nHope that helps!\n. If I execute the following code, on a clean checkout of e7ab2c2,\n```\nimport numpy as np\nfrom neon.backends import gen_backend\nfrom neon.data import ArrayIterator\nbe = gen_backend()\nX = np.random.rand(10000,3072)\ny = np.random.randint(1,11,10000)\ntrain = ArrayIterator(X=X, y=y, nclass=10, lshape=(3,32,32))\n```\nit runs without error. Can you confirm that your build environment is clean, in particular since you are running neon from /Library/Python/2.7/site-packages/neon and not the recommended virtualenv installation? \nPS:\nLooks like you don't have the error when creating the dataset, but when you are running the model. In that case, can you post the full script (or a minimal example that reproduces the error), so we can look into it? \n. Hi Marc,\nI still had to guess how you set up the model (i.e. backend and command line args), but with the following script I still can't reproduce the issue:\n```\n!/usr/bin/env python\nimport logging\nfrom neon.callbacks.callbacks import Callbacks\nfrom neon.data import ArrayIterator, load_mnist\nfrom neon.initializers import Gaussian\nfrom neon.layers import GeneralizedCost, Affine\nfrom neon.models import Model\nfrom neon.optimizers import GradientDescentMomentum\nfrom neon.transforms import Rectlin, Logistic, CrossEntropyBinary, Misclassification\nfrom neon.util.argparser import NeonArgparser\nparse the command line arguments\nparser = NeonArgparser(doc)\nimport numpy as np\nargs = parser.parse_args()\nlogger = logging.getLogger()\nlogger.setLevel(args.log_thresh)\nX = np.random.rand(10000,3072)\ny = np.random.randint(1,11,10000)\ntrain_set = ArrayIterator(X=X, y=y, nclass=10, lshape=(3,32,32))\nvalid_set = train_set #just to see if it generally runs\ninit_norm = Gaussian(loc=0.0, scale=0.01)\nsetup model layers\nlayers = [Affine(nout=100, init=init_norm, activation=Rectlin()),\n          Affine(nout=10, init=init_norm, activation=Logistic(shortcut=True))]\nsetup cost function as CrossEntropy\ncost = GeneralizedCost(costfunc=CrossEntropyBinary())\nsetup optimizer\noptimizer = GradientDescentMomentum(0.1, momentum_coef=0.9, stochastic_round=args.rounding)\ninitialize model object\nmlp = Model(layers=layers)\nconfigure callbacks\ncallbacks = Callbacks(mlp, eval_set=valid_set, **args.callback_args)\nrun fit\nmlp.fit(train_set, optimizer=optimizer, num_epochs=args.epochs, cost=cost, callbacks=callbacks)\nprint('Misclassification error = %.1f%%' % (mlp.eval(valid_set, metric=Misclassification())*100))\n```\nWhat my previous comment refers to was that if you have a local folder with the neon repository, e.g. ~/code/neon, and you git pulled master e7ab2c2 into that folder, your models will still run on whatever is in /Library/Python/2.7/site-packages/neon and not on ~/code/neon.\n. Digging a little bit deeper there is actually a typo in the documentation. Instead of \ny = np.random.randint(1,11,10000)\ntry setting up the labels as \ny = np.random.randint(0,10,10000) \nso the class labels can directly map to a 10 dimensional one-hot encoding. Currently, what happens is the CPU backend fails to assign to the 11th row in a 10-dimensional array with the error you saw, and the GPU backend fails silently (which is why I wasn't able to reproduce your error at first). \nThanks for pointing this out, we will update the docs accordingly and make sure there will be a more informative error message if converting the class-labels to a one-hot encoding fails. \n. Hi Marc,\nsounds like you already got a solution, but I still want to point out that ArrayIterator has an optional argument make_onehot that defaults to True. If you set it to False, you can pass in target vectors directly, instead of just passing the index of the target class. Watch out though, some of the costs / metrics only work with one-hot encoded data and might give unexpected results if more than one class is true. \n. Closing as duplicate. Answer on Google Groups refers to https://github.com/NervanaSystems/meetup/blob/master/cifar_example.ipynb for single image inference example. \n. You can still pad K and leave the extra categories empty. You just have to make the corresponding change to the dataset, e.g. if you are using ArrayIterator, pass in the nclass argument with the padded number of categories.\n. Nervana intends to continue all existing development efforts including the Nervana Neon deep learning framework, and it is going to stay open source. See our blog post https://www.nervanasys.com/intel-nervana for more details. \n. This is not something we have currently planned, but we welcome contributions from the open source community. \n. There are two sets of GPU kernels in neon. One has special assembly level optimizations for Maxwell and Pascal generation cards (e.g. GTX980, GTX1080, most cards from 2015 and newer) and supports 3D convolution. The other set is using less optimized CUDA C, supports older GPUs, and no 3D convolution. \nUnless you actually have a Maxwell GPU, and it is erroneously detected as an oder GPU, this example is not expected to work. Can you clarify which GPU you have?. The K series cards are Kepler architecture, so you won't be able to use 3D convolution. It should not be hard to add that functionality to our CUDA C kernels though, and we welcome community contributions if you would like to add it yourself. . Hi,\nI think you are the first to try neon on a POWER8 system, so you are going to be a bit of a guinea pig for installing neon on this architecture. That being said, there is nothing x86 specific in neon and it runs out of the box on ARM with Ubuntu. \nTo help you debugging, could you give more details of they system you are using (operating system etc.), what version of Python and Numpy are installed, attach a copy of the full error message, and let us know if python and numpy are working normally outside of neon. . Neon does not officially support Windows, and this error does not occur under Linux / Mac operating systems. . We cannot reproduce your error on a MacBook Pro with OS X Sierra and Python 2.7.12 so this appears to be specific to your system configuration. Try checking firewall rules etc. and make sure urllib2 can connect freely. Make sure you are using a supported Python distribution, e.g. from Brew. Also see http://stackoverflow.com/questions/27835619/ssl-certificate-verify-failed-error/27847883 for some more context and possible work-arounds. . Glad you found a fix. We'll use this as the recommended workaround if the problem comes up in the future! . Multi-GPU support is a premium feature only included with Nervana Cloud. It's not available in the open source release of neon. Please contact us directly for a demo of the Nervana Cloud. . The neon installer checks for the presence of the nvcc compiler to decided if the GPU backend should be installed or not. In this case it looks like you are doing the installation on a different machine from where the code will be running on, and this machine does not have a GPU. To force the GPU backend to be installed anyway, edit the Makefile in the neon root directory and set the variable HAS_GPU to be true. . There is probably no easy way, see if you can install CUDA locally (it should be possible to install without having a GPU) to get pycuda installed. If you know which version of cuda runs on the cluster, and where it is installed, you can try to mimic that setup locally. Pycuda will link against cuda libraries at install time, and if you run under a different environment, it may complain that it can't find cuda libraries in the expected path. . ",
    "andyyuan78": "it works well ,thanks apark263 and scttl\n. ubgpu@ubgpu:~/github/neon$ neon --gpu nervanagpu examples/convnet/i1k-alexnet-fp16.yaml\nWARNING:neon.util.persist:deserializing object from:  examples/convnet/i1k-alexnet-fp16.yaml\nWARNING:neon.datasets.imageset:Imageset initialized with dtype \n2015-05-11 01:19:23,544 WARNING:neon - setting log level to: 20\n2015-05-11 01:19:23,548 WARNING:init - nervanagpu not found, can't run via GPU\nTraceback (most recent call last):\n  File \"/usr/local/bin/neon\", line 199, in \n    experiment, result, status = main()\n  File \"/usr/local/bin/neon\", line 162, in main\n    device_id=args.device_id)\n  File \"/usr/local/lib/python2.7/dist-packages/neon/backends/init.py\", line 157, in gen_backend\n    raise RuntimeError(\"Can't find CUDA capable GPU\")\nRuntimeError: Can't find CUDA capable GPU\nubgpu@ubgpu:~/github/neon$ \n. it solved!\n. yes, I have it and change the path of -fp32.yaml .\n. cool. it works!\n. a little suggestion: maybe we should provide meanful debug/error message.  ;)\n. my gpu is gtx970\nthe first phase I means like,  maybe we should call it 'writing batches'\nubgpu@ubgpu:~/github/neon/neon$ neon --gpu nervanagpu examples/convnet/i1k-alexnet-fp16.yaml\nWARNING:neon.util.persist:deserializing object from:  examples/convnet/i1k-alexnet-fp16.yaml\nWARNING:neon.datasets.imageset:Imageset initialized with dtype \n2015-05-16 20:21:56,367 WARNING:neon - setting log level to: 20\n2015-05-16 20:21:56,791 INFO:gpu - Initialized NervanaGPU with stochastic_round=None\n2015-05-16 20:21:56,791 INFO:gpu - Seeding random number generator with: None\n2015-05-16 20:21:56,792 INFO:init - NervanaGPU backend, RNG seed: None, numerr: None\n2015-05-16 20:21:56,793 INFO:mlp - Layers:\n        ImageDataLayer d0: 3 x (224 x 224) nodes\n        ConvLayer conv1: 3 x (224 x 224) inputs, 64 x (55 x 55) nodes, RectLin act_fn\n        PoolingLayer pool1: 64 x (55 x 55) inputs, 64 x (27 x 27) nodes, Linear act_fn\n        ConvLayer conv2: 64 x (27 x 27) inputs, 192 x (27 x 27) nodes, RectLin act_fn\n        PoolingLayer pool2: 192 x (27 x 27) inputs, 192 x (13 x 13) nodes, Linear act_fn\n        ConvLayer conv3: 192 x (13 x 13) inputs, 384 x (13 x 13) nodes, RectLin act_fn\n        ConvLayer conv4: 384 x (13 x 13) inputs, 256 x (13 x 13) nodes, RectLin act_fn\n        ConvLayer conv5: 256 x (13 x 13) inputs, 256 x (13 x 13) nodes, RectLin act_fn\n        PoolingLayer pool3: 256 x (13 x 13) inputs, 256 x (6 x 6) nodes, Linear act_fn\n        FCLayer fc4096a: 9216 inputs, 4096 nodes, RectLin act_fn\n        DropOutLayer dropout1: 4096 inputs, 4096 nodes, Linear act_fn\n        FCLayer fc4096b: 4096 inputs, 4096 nodes, RectLin act_fn\n        DropOutLayer dropout2: 4096 inputs, 4096 nodes, Linear act_fn\n        FCLayer fc1000: 4096 inputs, 1000 nodes, Softmax act_fn\n        CostLayer cost: 1000 nodes, CrossEntropy cost_fn\n2015-05-16 20:21:56,793 INFO:batch_norm - BatchNormalization set to train mode\n2015-05-16 20:21:56,794 INFO:val_init - Generating AutoUniformValGen values of shape (363, 64)\n2015-05-16 20:21:56,796 INFO:batch_norm - BatchNormalization set to train mode\n2015-05-16 20:21:56,797 INFO:val_init - Generating AutoUniformValGen values of shape (1600, 192)\n2015-05-16 20:21:56,803 INFO:batch_norm - BatchNormalization set to train mode\n2015-05-16 20:21:56,804 INFO:val_init - Generating AutoUniformValGen values of shape (1728, 384)\n2015-05-16 20:21:56,815 INFO:batch_norm - BatchNormalization set to train mode\n2015-05-16 20:21:56,816 INFO:val_init - Generating AutoUniformValGen values of shape (3456, 256)\n2015-05-16 20:21:56,830 INFO:batch_norm - BatchNormalization set to train mode\n2015-05-16 20:21:56,831 INFO:val_init - Generating AutoUniformValGen values of shape (2304, 256)\n2015-05-16 20:21:56,841 INFO:batch_norm - BatchNormalization set to train mode\n2015-05-16 20:21:56,842 INFO:val_init - Generating AutoUniformValGen values of shape (4096, 9216)\n2015-05-16 20:21:57,386 INFO:batch_norm - BatchNormalization set to train mode\n2015-05-16 20:21:57,387 INFO:val_init - Generating AutoUniformValGen values of shape (4096, 4096)\n2015-05-16 20:21:57,628 INFO:val_init - Generating AutoUniformValGen values of shape (1000, 4096)\n2015-05-16 20:21:57,688 WARNING:imageset - Batch dir cache not found in /home/ubgpu/data/I1K/imageset_batches_16/dataset_cache.pkl:\nPress Y to create, otherwise exit: Y\n2015-05-16 20:22:04,634 INFO:batch_writer - Loaded synset tars.\n2015-05-16 20:22:04,634 INFO:batch_writer - Building trainset list ( can take a while)...\n2015-05-16 20:22:04,634 INFO:batch_writer - 0% ...\n2015-05-16 20:24:16,828 INFO:batch_writer - 10% ...\n2015-05-16 20:25:50,283 INFO:batch_writer - 20% ...\n2015-05-16 20:27:43,577 INFO:batch_writer - 30% ...\n2015-05-16 20:29:44,691 INFO:batch_writer - 40% ...\n2015-05-16 20:31:35,079 INFO:batch_writer - 50% ...\n...............\n2015-05-17 03:27:14,005 WARNING:persist - serializing object to: /home/ubgpu/data/I1K/imageset_batches_16/dataset_cache.pkl\n2015-05-17 03:27:21,257 WARNING:imageset - Done writing batches - please rerun to train.\nubgpu@ubgpu:~/github/neon/neon$\n\nhere are the resource while runing the second phase of\n    neon --gpu nervanagpu examples/convnet/i1k-alexnet-fp32.yaml\nubgpu@ubgpu:~$ nvidia-smi\nSun May 17 15:13:37 2015\n+------------------------------------------------------+\n| NVIDIA-SMI 346.46     Driver Version: 346.46         |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 970     Off  | 0000:01:00.0     N/A |                  N/A |\n| 62%   70C    P2    N/A /  N/A |   1817MiB /  4095MiB |     N/A      Default |\n+-------------------------------+----------------------+----------------------+\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0            C+G   Not Supported                                         |\n+-----------------------------------------------------------------------------+\nubgpu@ubgpu:~$ \n. | 0 GeForce GTX 970 Off | 0000:01:00.0 N/A | N/A |\n | 62% 70C P2 N/A / N/A | 1817MiB / 4095MiB | N/A Default |\nthe above is the result of nvidia-smi while  just the fp32 version is in training mode .\njust test it can be parallel!\n. it works well in 'paraller' with tow process like\nneon --gpu nervanagpu examples/convnet/i1k-alexnet-fp32.yaml\n neon --gpu nervanagpu examples/convnet/i1k-alexnet-fp16.yaml\nThe error happens due to someone delete the bin/nvcc\n. yes, not sure something delete my /usr/localcuda/bin\nnow it is OK after re-install CUDA\n. ",
    "Kaixhin": "Just following up since I've found time to run some models. The CUDA version fails on Kepler GPUs (as expected) but works beautifully on Maxwell GPUs!\n. Good point - I've now replaced the CUDA version with nervanagpu-neon and cudanet-neon.\nAs far as I can tell it isn't necessary to install cuda-convnet2 beforehand as the neon installer does it. One weird issue I've had on doing local builds is that when attempting to do a multithreaded make on the cudanet repo it works fine, but it consistently fails when run on the neon repo with the cudanet flag. I'm not going to raise an issue but if you wish to investigate further the most pertinent log I saw was\nread jobs pipe: Resource temporarily unavailable.  Stop\nwhich looks like somewhere along the line pip isn't handling memory well.\n. 2015-05-11 01:19:23,548 WARNING:init - nervanagpu not found, can't run via GPU\nHave you installed nervanagpu and made sure all the tests passed? Details are in the README of the repository.\nThe full list of dependencies for GPU usage are in the docs for neon.\n. If my Docker images suffice then I'd be happy to have them referenced somewhere in the readme. If there are any problems I would recommend raising an issue on the source repo as it might affect other images I provide for the deep learning community.\n. Looks good to me :+1:\n. The CPU image just builds neon on Ubuntu Core 14.04, but the CUDA images also include the CUDA SDK (with v6.5, v7.0 and v7.5 available with kaixhin/cuda-neon:6.5, kaixhin/cuda-neon:7.0 and kaixhin/cuda-neon respectively). I've had an email from someone asking if CUDA 7.0 would still be supported in the transition to neon v1.0, so the versioning seems useful.\nIncluded below is a stack trace from trying to run neon -b gpu examples/mnist_mlp.yaml, which occurs after the dataset downloads. It appears the CPU backend runs by default, so I'm guessing that it doesn't build the GPU backend properly. Looking at my nervanagpu Dockerfile, running pip install --upgrade six did solve one error I saw in an earlier stacktrace (after the https://github.com/NervanaSystems/neon/issues/83 fix), but installing PyCUDA with pip didn't make a difference.\nTraceback (most recent call last):\n  File \"/usr/local/bin/neon\", line 172, in <module>\n    callbacks=callbacks)\n  File \"/usr/local/lib/python2.7/dist-packages/neon/models/model.py\", line 120, in fit\n    self._epoch_fit(dataset, callbacks)\n  File \"/usr/local/lib/python2.7/dist-packages/neon/models/model.py\", line 142, in _epoch_fit\n    x = self.fprop(x)\n  File \"/usr/local/lib/python2.7/dist-packages/neon/models/model.py\", line 173, in fprop\n    x = l.fprop(x, inference)\n  File \"/usr/local/lib/python2.7/dist-packages/neon/layers/layer.py\", line 422, in fprop\n    self.be.compound_dot(A=self.W, B=inputs, C=self.outputs)\n  File \"/usr/local/lib/python2.7/dist-packages/neon/backends/nervanagpu.py\", line 1134, in compound_dot\n    kernel = _get_gemm_kernel(self.cubin_path, clss, op, size)\n  File \"<string>\", line 2, in _get_gemm_kernel\n  File \"/usr/local/lib/python2.7/dist-packages/pycuda/tools.py\", line 430, in context_dependent_memoize\n    result = func(*args)\n  File \"/usr/local/lib/python2.7/dist-packages/neon/backends/nervanagpu.py\", line 1711, in _get_gemm_kernel\n    module = _get_module(path, clss, op, size)\n  File \"<string>\", line 2, in _get_module\n  File \"/usr/local/lib/python2.7/dist-packages/pycuda/tools.py\", line 430, in context_dependent_memoize\n    result = func(*args)\n  File \"/usr/local/lib/python2.7/dist-packages/neon/backends/nervanagpu.py\", line 1703, in _get_module\n    return drv.module_from_file(os.path.join(path, cubin))\npycuda._driver.RuntimeError: cuModuleLoad failed: file not found\n. Both PATH and LD_LIBRARY_PATH have been set up. I also confirmed with a manual check right now.\n. It is missing - the kernels folder only contains C_interface, cu and sass. There's a lot to look through, but if you start from the end you might find something in the Docker build logs that get produced.\n. Probably, if not something similar - which is why I changed nvidia-smi to nvcc in https://github.com/NervanaSystems/neon/commit/e519b81b5c8bdb13fcba9eb17f1d56702870d948.\nSome kind of flag/manual make option to force the install, with a warning, would work. Although, if you have to use the cudanet backend to support older GPUs (https://github.com/NervanaSystems/neon/issues/75) I'll need to move back to the kaixhin/nervanagpu-neon and kaixhin/cudanet-neon images to support this properly.\n. The builds have been failing for a while now, so after some investigation it looks like this is the result of the Automated Build limits. Judging from the timings between creating builds and the exceptions being thrown, the builds are hitting the 2 hour limit.\nAny ideas? Perhaps something in my Dockerfile can be changed? That said, the builds were failing before I added pkg-config and libopencv-dev, so I don't think the visualisation functionality is adding much time.\n. Docker Support sent me an email about it: \"We are seeing your build is failing due to too much memory consumption and crashing again and again\". It appears that it could fail earlier, but the exception is only thrown after 2 hours. To go over the limits of the Automated Builds:\n- 2 hours\n- 2 GB RAM\n- 1 CPU\n- 30 GB Disk Space\nThe suggested solution is to break this into several Automated Builds, but I'm not sure that would even solve the problem. There are basically 2 steps - installing a few Ubuntu packages, and doing the actual make. The first step has never been an issue for software with similar requirements.\nIn the latest failed build I removed the -j tag from make to lower memory consumption, but it still failed (although there is a chance that this has become a timeout issue, I doubt it). The CUDA base image is 1GB, so I doubt disk space is a problem.\n. On the suggestion from Docker Support I created a virtual machine with the same resource limits as their machines, but my builds were succeeding. I contacted them and they tried to see what the issue is (response below), but yes it seems that you've identified what's causing the memory consumption issue. Let me know once the throttling is in place so that I can try again.\n\nWe ran the build twice, and when maxas is installed, it still uses a bunch of parallelism, and it looks like that was the part that was causing problems initially.\nWe can't find anything more definitive, but when it is installing maxas, it looks like it is running a bunch of cudafe++ and regular c/c++ compilers. \nRunning it directly with the logs going to our screen, we can see\nnvcc -arch sm_50 -cubin -o /root/neon/neon/backends/kernels/cubin/hconv_bprop_C32_N64.cubin /root/neon/neon/backends/kernels/cu/hconv_bprop_C32_N64.cu\nKilled\nA bunch of lines like these.\nIt does use up all the swap on the system, of which we only have 512MB. \nI know that most linux distros tend to create a larger amount of swap, so perhaps that is why you were able to build it locally.\n. Great - thanks to that commit the automated builds are now succeeding on the Docker Hub! I've set up weekly builds for both the CPU and CUDA versions, so you can add them to the docs if you want.\n\nFYI the following error gets thrown for both the mnist_mlp.yaml and mnist_mlp.py example with the GPU backend, but not the CPU backend. It doesn't seem to stop testing on the Python example though, so I'm not considering it a major issue.\nException pycuda._driver.LogicError: 'context::detach failed: invalid device context - cannot detach from invalid context' in <bound method NervanaGPU.__del__ of <neon.backends.nervanagpu.NervanaGPU object at 0x7f5b170abf50>> ignored\n. I've changed this line from make sysinstall to make sysinstall HAS_GPU=true, but still get the same error when running neon examples/mnist_mlp.yaml using nvidia-docker:\nsh\nFile \"/usr/local/bin/neon\", line 49, in <module>\n    from neon.backends import gen_backend\n...\nOSError: CUDA driver library not found\nAny ideas on how to debug? An issue that might be relevant is https://github.com/NVIDIA/nvidia-docker/issues/37.\n. I've added this location to the front of LD_LIBRARY_PATH before make is called, but this has not helped. For reference, line 20 and below of my Dockerfile has been replaced with the following:\n``` sh\nENV LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu:${LD_LIBRARY_PATH}                                                                                                                            \n                                                                                                                                                                                               # Clone neon repo and move into it                                                                                                                                                          \nRUN cd /root && git clone https://github.com/NervanaSystems/neon.git && cd neon && \\                                                                                                          \nMake (no multithreading to save resources/prevent concurrency errors)\nmake sysinstall HAS_GPU=true\nSet ~/neon as working directory\nWORKDIR /root/neon\n```\n. ",
    "lglhuada": "@ursk Thanks for your answers, it is clear now. So all vectors in my text data should have the same dimension.\n. @ursk WOW, That's great! Thanks for your help.\n. Has it been solved or not? I am going to train a LSTM-based model.\n. ",
    "kfoss": "@Kaixhin @scttl These are great. I searched Docker's registry last week for an existing image - the CUDA version didn't show up (not sure why...maybe I just missed it), so I think it would be very helpful to reference these in the Readme or even include/submodule in neon itself.\n. Addressed by @Kaixhin in #19, which included Docker images for cpu/cuda/nervanagpu\n. ",
    "shuokay": "@scttl thank you very much. \nIs there a time schedule of GPU support for RNNs/LSTMs ?\n. ",
    "hjchen2": "@shuokay I have tried to run the LSTM example, but I have no mobydick dataset.  Please help me, thanks very much.\n. @scttl that's great. Thanks a lot. \n. @ursk Hi ursk.  I have updated python version to 2.7.10, however it shows using python2.6 when I install neon once again. What can I do with this problem\n\n. OK, Thanks\n. I have solved the problem by reinstalling numpy and PyYAML. Now everything goes well.\n. ",
    "CAWEHbKA": "There is also another problem. It is in line:\n2015-05-18 20:44:18,016 INFO:fit - Unable to find saved model /home/ubgpu/data/I1K/I1K_alexnet_fp32_model.prm, starting over\nAll the previous execution was not saved. If you fix you cuda problem, you still will not be able to continue the previous execution...\nSomebody knows how to configure neon to save snapshots?\n. ",
    "Thomas-Yang": "I also encountered this problem after setup. I am working on CentOS 6.6 with python 2.6.6.\nWARNING:neon.util.persist:deserializing object from:  examples/mlp/mnist-small.yaml\nTraceback (most recent call last):\n  File \"/usr/bin/neon\", line 240, in \n    experiment, result, status = main()\n  File \"/usr/bin/neon\", line 126, in main\n    experiment = deserialize(args.yaml_file)\n  File \"/usr/lib/python2.6/site-packages/neon/util/persist.py\", line 192, in deserialize\n    return yaml.safe_load(load_path)\n  File \"/usr/lib64/python2.6/site-packages/yaml/init.py\", line 93, in safe_load\n    return load(stream, SafeLoader)\n  File \"/usr/lib64/python2.6/site-packages/yaml/init.py\", line 71, in load\n    return loader.get_single_data()\n  File \"/usr/lib64/python2.6/site-packages/yaml/constructor.py\", line 39, in get_single_data\n    return self.construct_document(node)\n  File \"/usr/lib64/python2.6/site-packages/yaml/constructor.py\", line 43, in construct_document\n    data = self.construct_object(node)\n  File \"/usr/lib64/python2.6/site-packages/yaml/constructor.py\", line 90, in construct_object\n    data = constructor(self, tag_suffix, node)\n  File \"/usr/lib/python2.6/site-packages/neon/util/persist.py\", line 134, in obj_multi_constructor\n    cls = import(module)\n  File \"/usr/lib/python2.6/site-packages/neon/experiments/init.py\", line 16, in \n    from neon.experiments.check_grad import GradientChecker  # noqa\n  File \"/usr/lib/python2.6/site-packages/neon/experiments/check_grad.py\", line 22, in \n    from neon.datasets.synthetic import UniformRandom\n  File \"/usr/lib/python2.6/site-packages/neon/datasets/init.py\", line 23, in \n    from neon.datasets.imageset import Imageset  # noqa\n  File \"/usr/lib/python2.6/site-packages/neon/datasets/imageset.py\", line 55\n    lbl_macro = {k: jdict['labels'][k] for k in self.ds.label_list}\n                                         ^\nSyntaxError: invalid syntax\nAny ideas? Thanks.\n. Hi Scott,\nThank you for your reply. I updated my python version to 2.7, now the CPU example works well. However, I got the following error when run the GPU example (neon --gpu cudanet examples/convnet/i1k-alexnet-fp32.yaml):\nWARNING:neon.util.persist:deserializing object from:  examples/convnet/i1k-alexnet-fp32.yaml\nWARNING:neon.datasets.imageset:Imageset initialized with dtype \n2015-05-26 20:30:53,764 WARNING:neon - setting log level to: 20\n2015-05-26 20:30:53,902 WARNING:init - cudanet not found, can't run via GPU\nTraceback (most recent call last):\n  File \"/usr/local/bin/neon\", line 240, in \n    experiment, result, status = main()\n  File \"/usr/local/bin/neon\", line 202, in main\n    device_id=args.device_id)\n  File \"/usr/local/lib/python2.7/site-packages/neon/backends/init.py\", line 157, in gen_backend\n    raise RuntimeError(\"Can't find CUDA capable GPU\")\nRuntimeError: Can't find CUDA capable GPU\nI have a K40 GPU installed on this machine. Do I have to install the cudanet, which is another project from nervana? (https://github.com/NervanaSystems/cuda-convnet2 https://github.com/NervanaSystems/cuda-convnet2)\nThanks!\n\nOn May 26, 2015, at 6:30 PM, Scott Leishman notifications@github.com wrote:\nHi Thomas,\nWe're only supporting python 2.7 or later at this time. Your error comes about because dict comprehension https://www.python.org/dev/peps/pep-0274/ was a feature that was added in 2.7, and we're making use of that on line 55.\n\u2014\nReply to this email directly or view it on GitHub https://github.com/NervanaSystems/neon/issues/35#issuecomment-105688263.\n. I got the following error installing cudanet:\n\nCMake Error at /usr/share/cmake/Modules/FindBLAS.cmake:594 (message):\n  A required library with BLAS API not found.  Please specify library\n  location.\nCall Stack (most recent call first):\n  CMakeLists.txt:36 (find_package)\n-- Configuring incomplete, errors occurred!\nSee also \"/home/thomas/cuda-convnet2/build/CMakeFiles/CMakeOutput.log\".\nSee also \"/home/thomas/cuda-convnet2/build/CMakeFiles/CMakeError.log\u201d.\nI have cublas and cudnn libraries installed under /usr/local/cuda/lib64/. The location is exposed to LD_LIBRARY_PATH env variable.\nAny ideas?\nBest\n\nOn May 26, 2015, at 9:44 PM, apark263 notifications@github.com wrote:\nHi Thomas,\nYes, the Nervana cuda-convnet2 needs to be installed in order to run gpu backend examples on kepler cards.\n-Alex\n\u2014\nReply to this email directly or view it on GitHub https://github.com/NervanaSystems/neon/issues/35#issuecomment-105716257.\n. Hi there,\n\nI installed all the required libraries on the web page. However, it still complains missing BLAS library:\nCMake Error at /usr/share/cmake/Modules/FindBLAS.cmake:594 (message):\n  A required library with BLAS API not found.  Please specify library\n  location.\nCall Stack (most recent call first):\n  CMakeLists.txt:36 (find_package)\nall the installed libraries are listed below:\npython-devel-2.6.6-52.el6.x86_64\nnumpy-1.4.1-9.el6.x86_64\nscipy-0.7.2-8.el6.x86_64\npython-magic-5.04-21.el6.x86_64\npython-matplotlib-0.99.1.2-1.el6.x86_64\natlas-devel-3.8.4-2.el6.x86_64\nlibjpeg-turbo-devel-1.2.1-3.el6_5.x86_64\nopencv-2.0.0-12.el6.x86_64\nopencv-devel-2.0.0-12.el6.x86_64\nAny thoughts?\n\nOn May 26, 2015, at 10:22 PM, Urs notifications@github.com wrote:\nHi Thomas,\ncuda-convnet2 has a dependency on the ATLAS libraries which need to be installed separately. Please see the documentation at the original cuda-convnet2 project page for details: https://code.google.com/p/cuda-convnet2/wiki/Compiling https://code.google.com/p/cuda-convnet2/wiki/Compiling\n\u2014\nReply to this email directly or view it on GitHub https://github.com/NervanaSystems/neon/issues/35#issuecomment-105726153.\n. Hi Scott,\n\nI passed the blas error and followed the instruction on https://code.google.com/p/cuda-convnet2/wiki/Compiling https://code.google.com/p/cuda-convnet2/wiki/Compiling to install cudanet with build.sh.\nHowever, I came across the following error complaining jpeg.\nsrc/jpeg.cpp: In member function \u2018void DecoderThread::decodeJpeg(int, int&, int&)\u2019:\nsrc/jpeg.cpp:74:37: error: \u2018jpeg_mem_src\u2019 was not declared in this scope\n     jpeg_mem_src(&cinf, src, src_len);\n                                     ^\nmake: *** [obj/release/./src/jpeg.cpp.o] Error 1\nAny thoughts? \nAlso what is the difference of cudanet on googlecode and github? Which one am I supposed to use?\n\nOn May 27, 2015, at 1:46 PM, Scott Leishman notifications@github.com wrote:\nHi Thomas,\nYou may also need to add at least one of blas-devel or openblas-devel\nIf that still doesn't work you'll need to provide more information for us like the CMakeOutput.log and CMakeError.log from wherever cmake is building from in your environment\n\u2014\nReply to this email directly or view it on GitHub https://github.com/NervanaSystems/neon/issues/35#issuecomment-106010263.\n. Hi all,\n\nThank you for your reply. I am working on CentOS 6.6. I have updated python version to 2.7.10 as well as the numpy to be 1.9.2. After I switched back to Nervana cuda-convnet2, I got the initial missing blas library error:\nCMake Error at /usr/share/cmake/Modules/FindBLAS.cmake:594 (message):\n  A required library with BLAS API not found.  Please specify library\n  location.\nCall Stack (most recent call first):\n  CMakeLists.txt:36 (find_package)\n-- Configuring incomplete, errors occurred!\nI guess the default path that makefile looks for does not contain the blas library. Is there a place I can specify the library path?, which should be the following:\nCUDA toolkit installation directory.\nexport CUDA_INSTALL_PATH=/usr/local/cuda\nPython include directory. This should contain the file Python.h, among others.\nexport PYTHON_INCLUDE_PATH=/usr/local/include/python2.7\nNumpy include directory. This should contain the file arrayobject.h, among others.\nexport NUMPY_INCLUDE_PATH=/usr/local/lib/python2.7/site-packages/numpy/core/include/numpy\nATLAS library directory. This should contain the file libcblas.so, among others.\nexport ATLAS_LIB_PATH=/usr/lib64/atlas\nThis is how I passed the blas error with the googlecode version.\nBest\n\nOn May 27, 2015, at 2:57 PM, Urs notifications@github.com wrote:\nHi Thomas,\nNote that the other libraries you listed, e.g.\npython-devel-2.6.6-52.el6.x86_64\nnumpy-1.4.1-9.el6.x86_64\nare also too old, the minimum version for python is 2.7 and the minimum numpy 1.8.1. We recommend Anaconda Python, or installing Python through Homebrew, to easily get the latest versions.\n\u2014\nReply to this email directly or view it on GitHub https://github.com/NervanaSystems/neon/issues/35#issuecomment-106036123.\n. \n",
    "MaratZakirov": "Following neon v1.0, multi-GPU & multi-node support is on the Nervana Cloud only.\nSo I can't use multiple GPUs locally?. ",
    "rongou": "At least on my system, sudo -E doesn't work. The only way to make it work is do sudo env PATH=$PATH make install. Maybe just need to document this.\n. ",
    "kuprel": "Awesome.  This might be specific to me but I get 1.9s with cudanet and 4.7s with nervanagpu.  I have a Titan X\n. ",
    "fchollet": "It is possible that you are using an older version of Numpy where the sum function did not yet support the argument keepdims... Reinstalling Numpy from pip should likely solve your problem.\n. Existing examples seem to be covering the main use cases for Neon (quite industry-flavored): RNNs, Convnets, MLPs. It could be there aren't much demand and applications for RBMs. \nWhich leads to another question: why go through the effort of implementing RBMs in Neon, a library that seems geared towards use in industry? Maybe it's just my lack of expertise, but I do not know of any practical (non-research) applications of RBMs. Nor of any research application where the RBM couldn't be replaced with something simpler, for that matter.\n. ",
    "Tgaaly": "my numpy is up to date. but i reinstalled it in anycase and this didnt fix anything. i have numpy version 1.6.2\n. my bad. reinstalled everything and it worked. thanks.\n. ",
    "llllaaaa": "Great! Thanks a lot.\n. I think automatic differentiation may be introduced with the refactoring, then the activation functions and cost functions can be written easily without providing the derivative function.\n. I have some suggestions on RNN\n1. Allow the output of t0 to be the input of t1\n2. Allow saving and setting the states of the network, then the trained network can move forward step by step. This will be very useful in real time application. \n3. Support variations of RNN, such as GRU, Gated Feedback LSTM, Gated Feedback GRU, Gird LSTM\n. I think a separate DQN framework will be very useful. I hope it will allow the user to define the input data format, network structure and reward, so we can use it in applications other than game playing. @seba-1511 @DoctorTeeth Thanks for your great work.\n. ",
    "Jokeren": "Hi,\nI find that the issue might be related with the search path. I go into the directory neon and then try to make install, then the ~/neon/bin/neon program will invoke the python files under .local/lib/python2.7/site-packages/neon/datasets. If I modify the file under that directory, it works.\nI do not know whether my operation is right: Modify the file, and then make install each time?\n. @apark263 , @scttl \nThank you for your replies! :+1: \nAdditionally, I suggest to add those instructions in your document:\nhttp://neon.nervanasys.com/docs/latest/datasets.html#adding-a-new-type-of-dataset\n. @scttl \nI got it. \nBy the way, I set the parameter to 1 because my dataset only contains 1 picture. Instead of training the whole picture, my aim is to train for each pixel by its peripheral pixels.\n. Thank you!\n@apark263, after reading codes these days, I notice what you mentioned about the Imagest.\nI am currently working on modifying Imageset for my own tasks. Imageset is a useful tool to read large set of images from the disk, whereas there are some additional features we do not need such as cropping images. Furthermore, it can not be used for testing data and pictures other than jpeg.\nI guess the imageset is originally designed for contest dataset such as imagenet?\nMaybe there could be a another class, which is the subclass of the Dataset and the super class of ImageSet, only provides the feature of reading huge amount of images. Or we should modify the batch_writer? \n. I have a similar problem.\nI used g++ v4.8.4, and I tried opencv V2.0.0 and V3.0.0.\nI had the message:\n/home/wangcw/src/neon/neon/data/loader/loader.so: undefined symbol:_ZN2cv4flipERKNS_11_InputArrayERKNS_12_OutputArrayEi\n. @jennifermyers, Actually I fixed this problem a few months ago. Finally I noticed that it was caused by inconsistency among versions of libraries needed by opencv. After configuring them to the same version, it worked as I expected. \nSo you are right, it is a very specific problem. \n. Hi, @scott-gray , thanks for the reply!\nI think the following codes might be correct:\n```\nISETP.LT.AND P4, PT, crst, param_CRST, PT; \nISETP.LT.AND P5, PT, crst1, param_CRST, PT; \nISETP.LT.AND P6, PT, crst2, param_CRST, PT; \nISETP.LT.AND P1, PT, crst3, param_CRST, PT; \n@P4 LD.E.CI load0F0, [track0F + 4x<0>];\n@P5 LD.E.CI load0F1, [track0F + 4x<1>];\n@P6 LD.E.CI load0F2, [track0F + 4x<2>];\n@P1 LD.E.CI load0F3, [track0F + 4x<3>];\n@!P4 LDS.32 load0F0, [RZ + addr_zero];\n@!P5 LDS.32 load0F1, [RZ + addr_zero];\n@!P6 LDS.32 load0F2, [RZ + addr_zero];\n@!P1 LDS.32 load0F3, [RZ + addr_zero];\n@P4 LD.E.CI load4F0, [track4F + 4x<0>];\n@P5 LD.E.CI load4F1, [track4F + 4x<1>];\n@P6 LD.E.CI load4F2, [track4F + 4x<2>];\n@P1 LD.E.CI load4F3, [track4F + 4x<3>];\n@!P4 LDS.32 load4F0, [RZ + addr_zero];\n@!P5 LDS.32 load4F1, [RZ + addr_zero];\n@!P6 LDS.32 load4F2, [RZ + addr_zero];\n@!P1 LDS.32 load4F3, [RZ + addr_zero];\n```\nCould you tell me a lit bit more about the (R2P, P2R) instructions, and how you use them to handle the boundary conditions?\n. ",
    "japerez20": "Hello guys:\nI am looking forward to learning more about neon. \nDo u have tutorial or slides where u show basic ideas of neon? Something like neon for dummies? \nCould you suggest me a link to understand better the  Moby  Dick example that you include with the first release of neon?\nThank, you,\nja\n. ",
    "yinyinl": "We have added the sentiment analysis example using movie review data. The example can be found at:\nimdb_lstm.py\nAlong with the example script, we have added several layer components for word embeddings, processing recurrent layer outputs. Also some text preprocessing methods. \n. We just released a fix to this problem, please pull the latest master. Thank you. \n. I didn't see any bad results using 300 units. The accuracy is a little better, and training time is only slightly longer. Can you give more details?\n. If you look at the RNN/LSTM implementation, you can see internal buffers to store activations for each time step of the sequence. You can follow the same design, but allocate separate buffers for forward passing the sequence and backward passing the sequence. \nWe are working on and planning to add that feature to neon in the near future. Stay tuned. \n. We are working on adding embedding layers and it does take advantage the numpy style fancy indexing supported in our backend. Some other optimizations will also be used for speedup. Comparing to one-hots, it also makes the word embedding scale better for larger dataset. Such layer will be added into neon soon. \n. Yes. If you look at our RNN examples, eg. char_lstm, word_lstm, in the example directory, you can see these examples use a text object to load the dataset, and during training or validation, the minibatch of text is converted to one-hot representations before going into the RNN models. Hope those will get you started quickly.\n. We don't have off the shelf support to do layer padding for variable length or EOS symbol detection to stop generating outputs yet. \nWe have a neon-users google group for questions or discussions like this:\nhttps://groups.google.com/forum/?hl=en#!forum/neon-users\n. We have implemented such ROI pooling layer and the Fast RCNN model internally. Currently looking into ways to optimize the efficiency for such special case of pooling, before releasing to the public. Stay tuned. \n. Did you revert commit c16b796 intentionally? Do you plan to do a PR for those in the future?\n. Thanks for the PR! Have a few questions inline...\n. Have you tried running neon right before  85a478e and after to isolate if it is the optimizer?\nAre you using gradient_clip_value or gradient_clip_norm in your model? Before that commit 85a478e, it was by clip_gradients set to True, and gradient_limit set to some value.\nIf the states all of sudden become almost 0 from epoch 7, are you able to see the \"grad\" value or the \"state\" from epoch 6? As states are updated by:\nstate[:] = decay * state + self.be.square(grad) * (1.0 - decay) \nThen we can trace further down where causes this issue. \n. In neon release v1.2.1, we've added a function get_google_word2vec_W to allow loading Word2Vec for  a vocabulary you build from a dataset. \nUsing the returned W, the LookupTable Layer or any neon layer can initialize the layer weights instead of using a initializer. The usage is demonstrated in the IMDB example. \nAlso, a LookupTable layer can be configured whether to update existing vectors further.\nHope these will be useful for your use case. Thank you!\n. We are looking into bringing this in. Thanks for adding the test! Have you thought about subclassing it from the data iterator or adding mask as an optional input, to make data iterator handle both case (and re-use what is being done in there)? \n. The optree currently supports numerical operations and transpose, but not reshape operation. And in your case, (y+0) returns an optree object. It is designed like this, so one can keep adding operations to an optree and hold off the execution. If you have a tensor z, e.g.\nz = ng.zeros(y.shape)\nz[:] = y+0\nIt is when you do assignment using [:], the optree gets executed and put results into Tensor structure, after which, you can do a reshape.\n. Agreed. For now, the way it is supposed to work is that a user uses [:] to execute the optree and assign to another tensor or \"y\" itself, before a reshape can be done. \n. The RNN layers in neon are stateful by default, by setting reset_cell to False when instantiating the layers. You can see them being used for many of our RNN examples. Setting reset_cell to True can make them stateless, which is used in our image_caption example. \n. Close this issue for now. Let us know if you have more questions!\n. After our v1.2.0 release, the Tree container outputs results from all branches. So the output will be a list. If you look at the Fast RCNN model in neon examples, you can see how it is being used. \n. You are right, and we are working on reducing the slowdown due to time unrolling. But we are not able to give you an exact date on when that will be available for the public yet. Thanks.\n. The CPU backend is implemented with python by wrapping numpy. And the interfaces are consistent with the GPU backend or any other backend in the future to form this MOP layer, which allows easy switch from neon.\n. This layer combination is not supported in neon currently. Conv layer is not able to handle the out_shape from lookuptable layer, since lookuptable layer out_shape is a tuple with a time dimension as the 2nd element. \nWe are working on adding a reshape layer, which allows more flexible combination of various layer types, and hopefully release that soon. And we will make sure your example works as a test case. Thanks.\n. Please take a look at the test for reshape layer for layer combination examples.\n. Seems great techniques for LSTM! If you look at neon LSTM implementation, you can see clear steps of getting the \"inputs to hidden\" pre-activations, and do the time unrolling for hidden-hidden activations. According the paper, 2 separate BN operations need to be applied on each of the sources. Then there are extra variables to keep track of the BN statistics, and parameters for BN (especially the careful initialization proposed in this paper) needed to be added into the layer. \nLet us know if you run into more questions. \n. We currently don't support LSTM that takes multi-dimensional tensors like Grid LSTM. The various RNN layers in neon use this data layout. \nOur backends supports multi-dimensional tensors that you can do numerical operations on. So if you decides to construct your own layer type, you can write out the forward and backward processes to handle those multi-dimensional tensors.\n. neon RNN layers uses this data layout. What you described looks similar to a character RNN model. Please look at this example to see how this type of input-output is being handled in neon.\n. You can run the model with one single image, but you do need to implement the image preprocessing steps before feeding it. You can look at steps in the PASCALInference for reference. Also fast RCNN needs precomputed bounding box proposals from selective search as inputs besides image, so you will need that precomputed from any open source selective search package. Also look at fast RCNN test.py for how to process the model output. \nWe are planning to add Faster RCNN to neon soon, which doesn't require selective search to provide bounding boxes. It will provide a more end-to-end solution to process an arbitrary image. \n. You are correct. We will include your fix right away. Thanks for looking into this! \n. The amazon G2 instances use kepler-based GPUs. While neon does provide support on pre-Maxwell GPUs for most of the kernels, but the ones needed by Faster RCNN model (minibatch size N = 1) currently are not available. \n. The K80 GPUs are kepler-based ones. So no, for the reason above. Thanks.. In the code, we added some description \nsplit_inputs (bool): to expect the input coming from the same source of separate sources\nModels like Deep Speech (mentioned in this issue ) stack up multiple BiRNN layers. In neon, we use DeepBiRNN to easily construct this. For first BiRNN layer, the inputs do NOT need to be split (split_input=False). But after processing it through 2 directions, the outputs are concatenated before going to the next layer. So for following BiRNN layers, their inputs need to be split (split_input=True). \nHope that helps!. Can you tell us more what did you mean by \"training process just can not start\". Any error message? It doesn't seem that you can also just do self.out_shape = self.in_shape. Also, are you running on GPU? if you do, this part may not work given the data are on different memories z_mean + np.exp(z_log_var)*epsilon. . Do you think this class can be derived from DataIterator or add into it?\n. ",
    "duyhtq": "hi scott, yes, i have cuda 7 installed and nvcc works fine with my other cuda code.  \nd@3:~/code/neon$ nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2015 NVIDIA Corporation\nBuilt on Mon_Feb_16_22:59:02_CST_2015\nCuda compilation tools, release 7.0, V7.0.27\nd@3:~/code/neon$ sudo env \u201cPATH=$PATH\u201d env \u201cLD_LIBRARY_PATH=$LD_LIBRARY_PATH\u201d make install\nNo CUDA capable GPU installed.  Forcing GPU=0\nRequirement already satisfied (use --upgrade to upgrade): numpy>=1.8.1 in /home/d/.local/lib/python2.7/site-packages\nRequirement already satisfied (use --upgrade to upgrade): PyYAML>=3.11 in /home/d/.local/lib/python2.7/site-packages\nCleaning up...\nRunning install(DEV=0 CPU=1 GPU=0 DIST=0)...\nUnpacking /home/d/code/neon\n  Running setup.py (path:/tmp/pip-ND4we5-build/setup.py) egg_info for package from file:///home/d/code/neon\nno previously-included directories found matching 'doc/build'\nRequirement already satisfied (use --upgrade to upgrade): neon==0.8.1 from file:///home/d/code/neon in /usr/local/lib/python2.7/dist-packages\nRequirement already satisfied (use --upgrade to upgrade): numpy>=1.8.1 in /home/d/.local/lib/python2.7/site-packages (from neon==0.8.1)\nRequirement already satisfied (use --upgrade to upgrade): PyYAML>=3.11 in /home/d/.local/lib/python2.7/site-packages (from neon==0.8.1)\nCleaning up...\n. scott... it's working now.  thanks for your help!  much appreciated for making neon open source!\n. ",
    "zhengdong914": "@apark263 thank you! it works!!\n. ",
    "yuehusile": "Hi scttl, thanks for your reply. you are right about that the nvidia-smi command is not found.\nI've checked my installation and configuration carefully, and it seems that it's a tegra k1 specific problem.\nWhat I am doing is trying to run neon on the nvidia jetson tk1 devkit, and NVML is not supported on Jetson TK1, so nvidia-smi command would not be found, even when CUDA installation is all right.\nIs NVML required to run neon demo ? or Is there anyway to solve my problem without NVML ?\nps: I can ran caffe with cudnn with no problem on jetson tk1, so I guess the CUDA installation is all right\n. thanks scttl! editing neon/backends/init.py works, but I still can't run this sample because another erro appears:\nroot@tegra-ubuntu:/home/hsl/neon# neon --gpu cudanet examples/convnet/i1k-alexnet-fp32.yaml\nWARNING:neon.util.persist:deserializing object from:  examples/convnet/i1k-alexnet-fp32.yaml\nWARNING:neon.datasets.imageset:Imageset initialized with dtype \n2015-07-01 04:52:29,170 WARNING:neon - setting log level to: 20\n2015-07-01 04:52:31,733 INFO:init - Cudanet backend, RNG seed: None, numerr: None\n2015-07-01 04:52:31,735 INFO:mlp - Layers:\n        ImageDataLayer d0: 3 x (224 x 224) nodes\n        ConvLayer conv1: 3 x (224 x 224) inputs, 64 x (55 x 55) nodes, RectLin act_fn\n        PoolingLayer pool1: 64 x (55 x 55) inputs, 64 x (27 x 27) nodes, Linear act_fn\n        ConvLayer conv2: 64 x (27 x 27) inputs, 192 x (27 x 27) nodes, RectLin act_fn\n        PoolingLayer pool2: 192 x (27 x 27) inputs, 192 x (13 x 13) nodes, Linear act_fn\n        ConvLayer conv3: 192 x (13 x 13) inputs, 384 x (13 x 13) nodes, RectLin act_fn\n        ConvLayer conv4: 384 x (13 x 13) inputs, 256 x (13 x 13) nodes, RectLin act_fn\n        ConvLayer conv5: 256 x (13 x 13) inputs, 256 x (13 x 13) nodes, RectLin act_fn\n        PoolingLayer pool3: 256 x (13 x 13) inputs, 256 x (6 x 6) nodes, Linear act_fn\n        FCLayer fc4096a: 9216 inputs, 4096 nodes, RectLin act_fn\n        DropOutLayer dropout1: 4096 inputs, 4096 nodes, Linear act_fn\n        FCLayer fc4096b: 4096 inputs, 4096 nodes, RectLin act_fn\n        DropOutLayer dropout2: 4096 inputs, 4096 nodes, Linear act_fn\n        FCLayer fc1000: 4096 inputs, 1000 nodes, Softmax act_fn\n        CostLayer cost: 1000 nodes, CrossEntropy cost_fn\n2015-07-01 04:52:31,738 INFO:batch_norm - BatchNormalization set to train mode\n2015-07-01 04:52:32,228 INFO:val_init - Generating AutoUniformValGen values of shape (363, 64)\n2015-07-01 04:52:32,254 INFO:batch_norm - BatchNormalization set to train mode\n2015-07-01 04:52:32,340 INFO:val_init - Generating AutoUniformValGen values of shape (1600, 192)\n2015-07-01 04:52:32,370 INFO:batch_norm - BatchNormalization set to train mode\n2015-07-01 04:52:32,432 INFO:val_init - Generating AutoUniformValGen values of shape (1728, 384)\n2015-07-01 04:52:32,506 INFO:batch_norm - BatchNormalization set to train mode\n2015-07-01 04:52:32,552 INFO:val_init - Generating AutoUniformValGen values of shape (3456, 256)\n2015-07-01 04:52:32,602 INFO:batch_norm - BatchNormalization set to train mode\n2015-07-01 04:52:32,639 INFO:val_init - Generating AutoUniformValGen values of shape (2304, 256)\n2015-07-01 04:52:32,691 INFO:batch_norm - BatchNormalization set to train mode\n2015-07-01 04:52:32,702 INFO:val_init - Generating AutoUniformValGen values of shape (4096, 9216)\n2015-07-01 04:52:34,805 INFO:batch_norm - BatchNormalization set to train mode\n2015-07-01 04:52:34,813 INFO:val_init - Generating AutoUniformValGen values of shape (4096, 4096)\n2015-07-01 04:52:35,728 INFO:val_init - Generating AutoUniformValGen values of shape (1000, 4096)\nTraceback (most recent call last):\n  File \"/usr/local/bin/neon\", line 240, in \n    experiment, result, status = main()\n  File \"/usr/local/bin/neon\", line 207, in main\n    experiment.initialize(backend)\n  File \"/usr/local/lib/python2.7/dist-packages/neon/experiments/fit_predict_err.py\", line 62, in initialize\n    super(FitPredictErrorExperiment, self).initialize(backend)\n  File \"/usr/local/lib/python2.7/dist-packages/neon/experiments/fit.py\", line 62, in initialize\n    self.model.initialize(backend)\n  File \"/usr/local/lib/python2.7/dist-packages/neon/models/mlp.py\", line 68, in initialize\n    dtype=self.layers[1].deltas_dtype)\n  File \"/usr/local/lib/python2.7/dist-packages/neon/backends/cc2.py\", line 536, in zeros\n    dtype=dtype)),\nMemoryError\nIs memory size a problem? tegra k1 has 2GB memory. or just something else lead to this problem? any advice for me to check out what happened?\n. ",
    "ypkang": "Hi,\nThis works perfectly ! Thank you for the help.\n. ",
    "dbl001": "David-Laxers-MacBook-Pro:maxas davidlaxer$ pip install Carp\nCollecting Carp\n  Downloading carp-0.0.3.tar.gz\nCollecting clepy (from Carp)\n  Downloading clepy-0.3.23.tar.gz\nRequirement already satisfied (use --upgrade to upgrade): jinja2 in /Users/davidlaxer/anaconda/lib/python2.7/site-packages (from Carp)\nRequirement already satisfied (use --upgrade to upgrade): decorator>=3.0 in /Users/davidlaxer/anaconda/lib/python2.7/site-packages (from clepy->Carp)\nRequirement already satisfied (use --upgrade to upgrade): nose in /Users/davidlaxer/anaconda/lib/python2.7/site-packages (from clepy->Carp)\nRequirement already satisfied (use --upgrade to upgrade): mock in /Users/davidlaxer/anaconda/lib/python2.7/site-packages (from clepy->Carp)\nInstalling collected packages: clepy, Carp\n  Running setup.py install for clepy\n  Running setup.py install for Carp\nSuccessfully installed Carp-0.0.3 clepy-0.3.23\nDavid-Laxers-MacBook-Pro:maxas davidlaxer$ make\ncp lib/MaxAs/MaxAs.pm blib/lib/MaxAs/MaxAs.pm\ncp lib/MaxAs/Cubin.pm blib/lib/MaxAs/Cubin.pm\ncp lib/MaxAs/MaxAsGrammar.pm blib/lib/MaxAs/MaxAsGrammar.pm\ncp bin/maxas.pl blib/script/maxas.pl\n/opt/local/bin/perl5.16 -MExtUtils::MY -e 'MY->fixin(shift)' -- blib/script/maxas.pl\nManifying blib/man3/MaxAs::MaxAs.3pm\nDavid-Laxers-MacBook-Pro:maxas davidlaxer$ sudo make install\nPassword:\nAppending installation info to /opt/local/lib/perl5/5.16.3/darwin-thread-multi-2level/perllocal.pod\nDavid-Laxers-MacBook-Pro:maxas davidlaxer$ \nDavid-Laxers-MacBook-Pro:nervanagpu davidlaxer$ pip install Carp --upgrade\nRequirement already up-to-date: Carp in /Users/davidlaxer/anaconda/lib/python2.7/site-packages\nRequirement already up-to-date: clepy in /Users/davidlaxer/anaconda/lib/python2.7/site-packages (from Carp)\nRequirement already up-to-date: jinja2 in /Users/davidlaxer/anaconda/lib/python2.7/site-packages (from Carp)\nRequirement already up-to-date: decorator>=3.0 in /Users/davidlaxer/anaconda/lib/python2.7/site-packages (from clepy->Carp)\nRequirement already up-to-date: nose in /Users/davidlaxer/anaconda/lib/python2.7/site-packages (from clepy->Carp)\nCollecting mock (from clepy->Carp)\n  Downloading mock-1.1.1.tar.gz (69kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 73kB 813kB/s \nCollecting pbr>=0.11 (from mock->clepy->Carp)\n  Downloading pbr-1.2.0-py2.py3-none-any.whl (83kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86kB 1.0MB/s \nRequirement already up-to-date: six>=1.7 in /Users/davidlaxer/anaconda/lib/python2.7/site-packages (from mock->clepy->Carp)\nRequirement already up-to-date: funcsigs in /Users/davidlaxer/anaconda/lib/python2.7/site-packages (from mock->clepy->Carp)\nInstalling collected packages: pbr, mock\n  Found existing installation: mock 1.0.1\n    DEPRECATION: Uninstalling a distutils installed project (mock) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\n    Uninstalling mock-1.0.1:\n      Successfully uninstalled mock-1.0.1\n  Running setup.py install for mock\nSuccessfully installed mock-1.1.1 pbr-1.2.0\nDavid-Laxers-MacBook-Pro:nervanagpu davidlaxer$ pip install Dumper --upgrade\nCollecting Dumper\n  Downloading Dumper-1.0.4-py2.py3-none-any.whl\nInstalling collected packages: Dumper\nSuccessfully installed Dumper-1.0.4\nDavid-Laxers-MacBook-Pro:nervanagpu davidlaxer$ make all\ninstalling maxas...\n/var/folders/nj/nphdkhyj6s1dttb0pd9zb2wc0000gn/T/nervanagpu.XXXXXXXX.A9H2VpAv\nCloning into 'maxas'...\nremote: Counting objects: 170, done.\nremote: Total 170 (delta 0), reused 0 (delta 0), pack-reused 170\nReceiving objects: 100% (170/170), 163.15 KiB | 0 bytes/s, done.\nResolving deltas: 100% (67/67), done.\nChecking connectivity... done.\nChecking if your kit is complete...\nLooks good\nWarning: prerequisite Carp 1.29 not found. We have 1.26.\nWarning: prerequisite Data::Dumper 2.145 not found. We have 2.13506.\nWriting Makefile for MaxAs::MaxAs\nWriting MYMETA.yml and MYMETA.json\ncp lib/MaxAs/MaxAs.pm blib/lib/MaxAs/MaxAs.pm\ncp lib/MaxAs/Cubin.pm blib/lib/MaxAs/Cubin.pm\ncp lib/MaxAs/MaxAsGrammar.pm blib/lib/MaxAs/MaxAsGrammar.pm\ncp bin/maxas.pl blib/script/maxas.pl\n/opt/local/bin/perl5.16 -MExtUtils::MY -e 'MY->fixin(shift)' -- blib/script/maxas.pl\nManifying blib/man3/MaxAs::MaxAs.3pm\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\nERROR: Can't create '/opt/local/lib/perl5/site_perl/5.16.3/MaxAs'\nDo not have write permissions on '/opt/local/lib/perl5/site_perl/5.16.3/MaxAs'\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n at -e line 1.\nmake[1]: * [pure_site_install] Error 13\nproblems installing maxas\nmake: * [maxas_check] Error 1\nDavid-Laxers-MacBook-Pro:nervanagpu davidlaxer$ sudo make all\nThe directory '/Users/davidlaxer/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\ninstalling maxas...\n/tmp/nervanagpu.XXXXXXXX.JNGuTdEo\nCloning into 'maxas'...\nremote: Counting objects: 170, done.\nremote: Total 170 (delta 0), reused 0 (delta 0), pack-reused 170\nReceiving objects: 100% (170/170), 163.15 KiB | 0 bytes/s, done.\nResolving deltas: 100% (67/67), done.\nChecking connectivity... done.\nChecking if your kit is complete...\nLooks good\nWarning: prerequisite Carp 1.29 not found. We have 1.26.\nWarning: prerequisite Data::Dumper 2.145 not found. We have 2.13506.\nWriting Makefile for MaxAs::MaxAs\nWriting MYMETA.yml and MYMETA.json\ncp lib/MaxAs/MaxAs.pm blib/lib/MaxAs/MaxAs.pm\ncp lib/MaxAs/Cubin.pm blib/lib/MaxAs/Cubin.pm\ncp lib/MaxAs/MaxAsGrammar.pm blib/lib/MaxAs/MaxAsGrammar.pm\ncp bin/maxas.pl blib/script/maxas.pl\n/opt/local/bin/perl5.16 -MExtUtils::MY -e 'MY->fixin(shift)' -- blib/script/maxas.pl\nManifying blib/man3/MaxAs::MaxAs.3pm\nAppending installation info to /opt/local/lib/perl5/5.16.3/darwin-thread-multi-2level/perllocal.pod\nsed: illegal option -- r\nusage: sed script [-Ealn] [-i extension] [file ...]\n       sed [-Ealn] [-i extension] [-e script] ... [-f script_file] ... [file ...]\nbuilding kernel: hgemm_nn_128x128...\nmake: maxas.pl: No such file or directory\nmake: *** [nervanagpu/kernels/cubin/hgemm_nn_128x128.cubin] Error 1\nDavid-Laxers-MacBook-Pro:nervanagpu davidlaxer$ \n\nOn Jul 10, 2015, at 2:11 PM, apark263 notifications@github.com wrote:\ntry installing maxas directly first:\nhttps://github.com/NervanaSystems/maxas\nThe nervanagpu makefile should be trying to install it for you, but it\nseems to have failed. or perhaps it's installed but the path to it is not\navailable to your sudo.\nOn Fri, Jul 10, 2015 at 2:06 PM, dbl001 notifications@github.com wrote:\n\nMakefile:\ndefine list_includes\n$(shell sed -rn 's/^/\\1/p' $(call strip_codes,$(1)))\nendef\nDavid-Laxers-MacBook-Pro:nervanagpu davidlaxer$ sudo !!\nsudo make all\nThe directory '/Users/davidlaxer/Library/Caches/pip/http' or its parent\ndirectory is not owned by the current user and the cache has been disabled.\nPlease check the permissions and owner of that directory. If executing pip\nwith sudo, you may want sudo's -H flag.\ninstalling maxas...\n/tmp/nervanagpu.XXXXXXXX.waaER91X\nCloning into 'maxas'...\nremote: Counting objects: 170, done.\nremote: Total 170 (delta 0), reused 0 (delta 0), pack-reused 170\nReceiving objects: 100% (170/170), 163.15 KiB | 0 bytes/s, done.\nResolving deltas: 100% (67/67), done.\nChecking connectivity... done.\nChecking if your kit is complete...\nLooks good\nWarning: prerequisite Carp 1.29 not found. We have 1.26.\nWarning: prerequisite Data::Dumper 2.145 not found. We have 2.13506.\nWriting Makefile for MaxAs::MaxAs\nWriting MYMETA.yml and MYMETA.json\ncp lib/MaxAs/MaxAs.pm blib/lib/MaxAs/MaxAs.pm\ncp lib/MaxAs/Cubin.pm blib/lib/MaxAs/Cubin.pm\ncp lib/MaxAs/MaxAsGrammar.pm blib/lib/MaxAs/MaxAsGrammar.pm\ncp bin/maxas.pl blib/script/maxas.pl\n/opt/local/bin/perl5.16 -MExtUtils::MY -e 'MY->fixin(shift)' --\nblib/script/maxas.pl\nManifying blib/man3/MaxAs::MaxAs.3pm\nAppending installation info to\n/opt/local/lib/perl5/5.16.3/darwin-thread-multi-2level/perllocal.pod\nsed: illegal option -- r\nusage: sed script [-Ealn] [-i extension] [file ...]\nsed [-Ealn] [-i extension] [-e script] ... [-f script_file] ... [file ...]\nbuilding kernel: hgemm_nn_128x128...\nmake: maxas.pl: No such file or directory\nmake: *** [nervanagpu/kernels/cubin/hgemm_nn_128x128.cubin] Error 1\nDavid-Laxers-MacBook-Pro:nervanagpu davidlaxer$ sed -r\nsed: illegal option -- r\nusage: sed script [-Ealn] [-i extension] [file ...]\nsed [-Ealn] [-i extension] [-e script] ... [-f script_file] ... [file ...]\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/59.\n\u2014\nReply to this email directly or view it on GitHub https://github.com/NervanaSystems/neon/issues/59#issuecomment-120529674.\n. Ok.\n\n\nChipset Model:  NVIDIA GeForce 9600M GT\nIt runs Cuda 6.5.\nHow about an EC2 GPU: \n\nOn Jul 10, 2015, at 2:25 PM, apark263 notifications@github.com wrote:\nHi David,\nI noticed that you are trying to install nervanagpu on a macbook pro. I wanted to point out that nervanagpu currently only supports maxwell class gpus, so even if it gets installed, you probably will not be able to use it.\n\u2014\nReply to this email directly or view it on GitHub https://github.com/NervanaSystems/neon/issues/59#issuecomment-120532871.\n. \n",
    "pranv": ":+1: for JSON\n. This may be a bit out order, but here it is:\nWhy can't neon have a Keras style API?\n. Thanks for the quick reply. We will share our progress in the coming weeks :)\n. ",
    "tambetm": "I just wanted to add, that layer methods mentioned in #65 were confusing to me as well. I expected layers to have two simple methods: forward() and backward(). Instead tanh had three methods, all of which did forward pass first and two of them did backward pass as well. That seems like a lot of redundancy, although as I understand, apply_derivatives() (and possibly also apply_function()?) is not called at all. At least they would deserve clarifying comment.\nBTW, I implemented tanh version optimized for normalized inputs, as described in \"Efficient BackProp\". This takes form of f(x) = 1.7159_tanh(x_2/3). At least in my case it improved results considerably. If anybody is interested, the code is here: https://github.com/tambetm/neon/commit/f31fc675a6f652bfcbeeda21ced0d581c4e14ff6.\n. @seba-1511, do you plan to keep entire replay memory in GPU? Default replay memory size is 1000000 in DeepMind's code. Assuming you can keep every screen frame in memory only once, you would need 1000000 * 84 * 84 * 2 (assuming 16-bit) = 13GB, which is above even Titan X.\nAlso you would need to randomly sample from replay memory for training, so the technique suggested by @apark263 wouldn't work. Can you create a tensor as non-contiguous slice of another tensor? And efficiently apply operators to it?\nI just started working on my own implementation of DQN using Neon, but currently I'm striving for simplicity, rather than performance. Probably it will take quite some time to debug it.\n. Thanks, that was easy. I was used to thinking, that layers_to_optimize is somehow \"private\" to Model.\n. Hmm, after git pull I'm not able to reproduce the issue. Will look into \nit again tomorrow...\nTambet\nOn 24.09.2015 21:08, apark263 wrote:\n\nthat's a good idea -- would you be able to check your assumption by \ndoing a quick modification of your nervanagpu? you just need to change \nlines 384-394 in |nervanagpu.py| to take an |ary| as an arg, assert \nthe size matches, and then remove line 392.\nwould be interesting to see if the time is materially improved.\n\u2014\nReply to this email directly or view it on GitHub \nhttps://github.com/NervanaSystems/neon/issues/108#issuecomment-143007352.\n. Just for the record - the results didn't improve. Here is the original result:\n\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\n    10000   14.318    0.001   15.226    0.002 nervanagpu.py:394(get)\n    10284    8.949    0.001    8.949    0.001 ale_python_interface.py:102(act)\n    10000    3.452    0.000    3.452    0.000 {method 'copy' of 'numpy.ndarray' objects}\n    10010    3.106    0.000    4.667    0.000 nervanagpu.py:370(set)\n    10011    1.537    0.000    1.537    0.000 {method 'astype' of 'numpy.ndarray' objects}\n    60000    1.354    0.000    2.642    0.000 float_ew.py:636(call_compound_kernel)\nAnd here is the result with modified get() method:\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\n    10000   15.384    0.002   15.391    0.002 nervanagpu.py:394(get)\n    10236    8.877    0.001    8.877    0.001 ale_python_interface.py:102(act)\n    10000    3.377    0.000    3.377    0.000 {method 'copy' of 'numpy.ndarray' objects}\n    10011    3.229    0.000    3.229    0.000 {method 'astype' of 'numpy.ndarray' objects}\n    10010    3.065    0.000    6.318    0.001 nervanagpu.py:370(set)\n    50000    1.165    0.000    2.246    0.000 float_ew.py:636(call_compound_kernel)\nHow the modified code looked like:\n```\n    def get(self, ary, stream=None):\n        \"\"\"\n        Copy device array to host.\n    Returns:\n        numpy.ndarray: A host numpy array\n    \"\"\"\n    assert ary.shape == self.shape\n    assert ary.dtype == self.dtype\n\n    if self.is_contiguous:\n        drv.memcpy_dtoh_async(ary, self.gpudata, stream)\n    else:\n        # if it is not contiguous, need to copy it over to new device mem\n        ary_d = self.backend.empty(self.shape, self.dtype)\n        ary_d.copy(self)\n        drv.memcpy_dtoh_async(ary, ary_d.gpudata, stream)\n    return ary\n\ndef asnumpyarray(self):\n    \"\"\"\n    asnumpyarray is an alias of get(), needed for MOP compatibility\n\n    Returns:\n        numpy.ndarray: A host numpy array\n    \"\"\"\n    ary = np.empty(self.shape, self.dtype)\n    return self.get(ary)\n\n```\n. In DataIterator constructor set X to your input, y to your output, nclass to number of columns in output and make_onehot=False. Like this:\ntrain_set = DataIterator(X_train, y_train, nclass=y_train.shape[1], make_onehot=False)\n. Thanks @nervetumer for suggestions! I checked the serialized models and weights are not zeros. But all the serialized models after epoch 7 are exactly the same (at least means of weights of all layers).\nCurrent hypothesis is, that it might be something related to RMSProp optimizer and state handling. While weight means seem ok, mean state values for all layers are 1.26117e-44 starting from epoch 7. Possibly related to commit https://github.com/NervanaSystems/neon/commit/85a478edb63552edc396cce17ab80ae8c730ab2d, because that's the only commit that touched optimizers in between working and not working version. I also tried Adam, and while it doesn't have the exact same problem of predicting zero, it still fails to learn the way it used to. \nYes, https://github.com/NervanaSystems/neon/commit/7a56fa9645a51e97c05f2e5afbbd1df7057ae832 is the working commit. Upper plot is done with latest GitHub HEAD, probably https://github.com/NervanaSystems/neon/commit/e479ce351ce7149e566cb4be1d8101c1c3e4d179. I'm serializing-deserializing several times during epoch. \nDebugging it takes time, I will keep you informed.\n. I played with serialization test in test_model.py and I couldn't replicate the exact error, but found another weird result:\n```\nprint mlp.eval(train_set, Misclassification())\nSerialize model\nsave_obj(mlp.serialize(keep_states=True), tmp_save)\nLoad model\nmlp = Model(layers=layers)\nmlp.load_weights(tmp_save)\nprint mlp.eval(train_set, Misclassification()) \n```\nThis outputs:\n[ 0.74373335]\n[ 0.88763332]\nShouldn't the misclassification rates be exactly the same, if evaluated on the same dataset and with the same model? The difference is even more evident, when you increase n_test variable and let it train longer - after loading the model the result is always the same - 0.8876. Amusingly outputs and weights match...\nFull code is here (just run python test_model.py):\nhttps://gist.github.com/tambetm/65542362cb24256350d8\n. Did you mean to post only model.py? There is only one minor change there and it didn't really affect the result - the second evaluation still produces 0.8876.\n[ 0.73153335]\n[ 0.88763332]\nI'm not using MergeMultistream in my code, so I guess the fix doesn't apply directly. But once we get this fixed, hopefully I can modify the test code to reproduce my error as well (so far no luck).\nI also noticed, that version 1.1.3 (e479ce351ce7149e566cb4be1d8101c1c3e4d179) tends to have worse performance than version 1.1.0 (7a56fa9645a51e97c05f2e5afbbd1df7057ae832) - average score in Breakout is 305 vs 319. I'm using the exact same model snapshot (breakout_77.pkl), not doing training at all, only prediction.\n. Give me a few days for this. All my Maxwell GPU-s are busy at the moment :).\n. As expected, this container fix didn't fix my problem:\n\nBack to testing...\n. I can assure, that the problem is not related to saving-loading the model. When I turned off saving-loading, then 7a56fa9645a51e97c05f2e5afbbd1df7057ae832 still learned, but 367651890e646c6861dbdc93c1bf4788c51a363d didn't. Currently testing if https://github.com/NervanaSystems/neon/issues/171#issuecomment-168426500 fixes the problem.\n. Nope. Tested with 64 threads as outlined here: https://github.com/NervanaSystems/neon/issues/171#issuecomment-168426500. Currently I'm out of ideas.\n. Thanks @nervetumer for tracking that down! I've been really busy last week\nand this week doesn't look any better... Will test it ASAP.\nOn Mon, Feb 22, 2016 at 8:03 AM, nervetumer notifications@github.com\nwrote:\n\nI think i tracked down the commit that is causing your problem. I think it\nis adab385\nhttps://github.com/NervanaSystems/neon/commit/adab385dace2ef75f58852157b55788cb56b8647.\nThat said I have no idea why this would affect you at all. Will need to\nlook at it closely.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/153#issuecomment-187025434\n.\n. I can also confirm, that there is no error with c8003191bb309b6df9208631dbef74a4b2388203 and the error occurs with 257b5d5aa8e154dce43dbe718d26f19aa1f3871a (which is one commit after adab385dace2ef75f58852157b55788cb56b8647,  adab385dace2ef75f58852157b55788cb56b8647 had a weird bug and I had to test with the next commit).\n\nThe lines that cause the error are beyond my depth with Neon, so I'm relying on you here. I wonder if it would be possible to create a simpler test case to reproduce the issue? I tried it once, based on test_model_serialize() in tests/test_model.py, but failed. Reproducing the issue currently takes couple of hours and that's the main bummer.\n. Thanks a lot @nervetumer, that did the trick! I would be happy to update my code, once #153 gets fixed. Any news with this?\n. Just a minor inconsistency between GPU and CPU backends - CPUTensor.asnumpyarray() returns shared instance (pointer), while GPUTensor.asnumpyarray() always returns a new instance (copy). I had a very hard-to-find bug regarding this.\n. Slightly nicer and more Ubuntuish would be to use:\n$ sudo update-alternatives --config gcc\n. ",
    "BenjaminBossan": "My biggest wish: Making it possible to skip the Dataset class and use numpy arrays as default data containers instead (such as in scikit learn). Despite the documentation, I found it painful to use neon with my own data.\nPersonally, I do not care for yaml or json model definitions. I find the model defintion in pure python very succinct and readable.\nI'm eager to use neon more, keep up the great work.\n. ",
    "avostryakov": "\nI want to see LSTM support with mini-batches and multi-GPUs support. \nFor NLP tasks, it's cool to have EmbeddingLayer with possibility to switch off training of embedding matrix like following in Lasagne: http://lasagne.readthedocs.org/en/latest/modules/layers/embedding.html#lasagne.layers.EmbeddingLayer\n. Actually, I don't understand how I can use a masked cost. After LSTM layer,\nI take a last hidden state and put it in a usual feedforward layer with\nsoftmax. Only after it I have a cost function.\nSo, because I have sequences of different length in mini-batch I have to\nmask them.\n\nAnatoly Vostryakov,\nEnergy,\nmailto: a.vostrjakov@gmail.com\n//////\n2015-09-11 22:12 GMT+03:00 John Co-Reyes notifications@github.com:\n\nDepending on what you're doing, using a masked cost might be enough\nhttp://neon.nervanasys.com/docs/1.0.0rc1/generated/neon.layers.layer.GeneralizedCostMask.html#neon.layers.layer.GeneralizedCostMask.\nThough I can see for some cases where you'll need a mask in the lstm layer.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/85#issuecomment-139635040.\n. \n",
    "DoctorTeeth": "Yeah this is something that I thought about as well: we had wanted to do \"DQNs in neon\", but basically the deep learning part of DQN is a pretty simple convnet with a least squares cost. You could probably put some DQN related stuff in neon, but without the rest of the code (dealing with the replay memory, dealing with ALE, etc) you're not going to have a fully functional DQN that you can train after just installing neon, and I suspect that we don't want to support that auxiliary stuff.\n. ",
    "kashif": "thanks @nervanasys I'll have a look at this... \n. closing this since it is in the new refactored version.\n. ",
    "xiuxiazhang": "I am using Redhat 6.3 OS and installed jpeg-8c.\n[zxx@ga85 ~]$ cat /etc/redhat-release \nRed Hat Enterprise Linux Server release 6.3 (Santiago)\n[zxx@ga85 ~]$ ldconfig -p | grep libjpeg\n    libjpeg.so.62 (libc6,x86-64) => /usr/lib64/libjpeg.so.62\nlibjpeg.so.8 (libc6,x86-64) => /usr/local/lib/libjpeg.so.8\n    libjpeg.so (libc6,x86-64) => /usr/local/lib/libjpeg.so\n    libjpeg.so (libc6,x86-64) => /usr/lib64/libjpeg.so\n. Hi Scott,\n      You're right. Although I installed libjpeg-8c, PIL is using libjpeg.so.62.  I adjusted the order of LD_LIBRARY_PATH. I uninstalled  pillow and reinstall pillow with --no-cache-dirs.  Finally PIL links to libjpeg.so.8.\nNow the problem is fixed. Thank you.\n. ",
    "cveres": "Hello nervanasys,\nMy YAML file is exactly the same as the example in the link I provided. I even used the same directory names. I will paste it anyway at the end, as you asked. \nHere is the full run of my training attempt including the error trace:\nCsabas-Mac-mini:NEON csabaveres$ neon ndsb.yaml \nWARNING:neon.util.persist:deserializing object from:  ndsb.yaml\nWARNING:neon.datasets.imageset:Imageset initialized with dtype \n2015-08-11 16:44:52,247 WARNING:neon - setting log level to: 20\n2015-08-11 16:44:52,248 INFO:cpu - Seeding random number generator with: None\n2015-08-11 16:44:52,253 INFO:init - CPU backend, RNG seed: None, numerr: None\n2015-08-11 16:44:52,285 INFO:mlp - Layers:\n    ImageDataLayer d0: 1 x (64 x 64) nodes\n    ConvLayer conv: 1 x (64 x 64) inputs, 128 x (30 x 30) nodes, RectLin act_fn\n    PoolingLayer mpool: 128 x (30 x 30) inputs, 128 x (15 x 15) nodes, Linear act_fn\n    ConvLayer conv: 128 x (15 x 15) inputs, 256 x (13 x 13) nodes, RectLin act_fn\n    PoolingLayer mpool: 256 x (13 x 13) inputs, 256 x (6 x 6) nodes, Linear act_fn\n    ConvLayer conv: 256 x (6 x 6) inputs, 512 x (5 x 5) nodes, RectLin act_fn\n    PoolingLayer mpool: 512 x (5 x 5) inputs, 512 x (2 x 2) nodes, Linear act_fn\n    FCLayer fc: 2048 inputs, 1024 nodes, RectLin act_fn\n    DropOutLayer dropout: 1024 inputs, 1024 nodes, Linear act_fn\n    FCLayer fc: 1024 inputs, 512 nodes, RectLin act_fn\n    DropOutLayer dropout: 512 inputs, 512 nodes, Linear act_fn\n    FCLayer output: 512 inputs, 128 nodes, Softmax act_fn\n    CostLayer cost: 128 nodes, CrossEntropy cost_fn\n2015-08-11 16:44:52,372 INFO:val_init - Generating UniformValGen values of shape (25, 128)\n2015-08-11 16:44:52,559 INFO:val_init - Generating UniformValGen values of shape (1152, 256)\n2015-08-11 16:44:52,608 INFO:val_init - Generating UniformValGen values of shape (1024, 512)\n2015-08-11 16:44:52,620 INFO:val_init - Generating UniformValGen values of shape (1024, 2048)\n2015-08-11 16:44:52,668 INFO:val_init - Generating UniformValGen values of shape (512, 1024)\n2015-08-11 16:44:52,679 INFO:val_init - Generating UniformValGen values of shape (128, 512)\n2015-08-11 16:44:52,681 ERROR:imageset - Batch dir cache not found in /Users/csabaveres/data/NDSB/batches/dataset_cache.pkl:\nPress Y to create, otherwise exit:    Y\n2015-08-11 16:45:44,868 INFO:batch_writer - train /Users/csabaveres/data/NDSB/batches/train_file.csv.gz 0\n2015-08-11 16:45:47,595 INFO:batch_writer - Writing train batches...\n2015-08-11 16:45:56,660 WARNING:persist - serializing object to: /Users/csabaveres/data/NDSB/batches/data_batch_0\n2015-08-11 16:45:56,707 INFO:batch_writer - Wrote to /Users/csabaveres/data/NDSB/batches (train batch 1 of 2) (9.11 sec)\n2015-08-11 16:46:02,962 WARNING:persist - serializing object to: /Users/csabaveres/data/NDSB/batches/data_batch_1\n2015-08-11 16:46:02,970 INFO:batch_writer - Wrote to /Users/csabaveres/data/NDSB/batches (train batch 2 of 2) (6.12 sec)\n2015-08-11 16:46:03,022 INFO:batch_writer - validation /Users/csabaveres/data/NDSB/batches/val_file.csv.gz 10\n2015-08-11 16:46:03,044 INFO:batch_writer - Writing validation batches...\n2015-08-11 16:46:06,450 WARNING:persist - serializing object to: /Users/csabaveres/data/NDSB/batches/data_batch_10\n2015-08-11 16:46:06,455 INFO:batch_writer - Wrote to /Users/csabaveres/data/NDSB/batches (validation batch 1 of 1) (3.41 sec)\n2015-08-11 16:46:06,470 WARNING:persist - serializing object to: /Users/csabaveres/data/NDSB/batches/dataset_cache.pkl\n2015-08-11 16:46:06,471 ERROR:imageset - Done writing batches - please rerun to train.\n(Then I re run as requested)\nCsabas-Mac-mini:NEON csabaveres$ neon ndsb.yaml \nWARNING:neon.util.persist:deserializing object from:  ndsb.yaml\nWARNING:neon.datasets.imageset:Imageset initialized with dtype \n2015-08-11 16:46:23,453 WARNING:neon - setting log level to: 20\n2015-08-11 16:46:23,453 INFO:cpu - Seeding random number generator with: None\n2015-08-11 16:46:23,458 INFO:init - CPU backend, RNG seed: None, numerr: None\n2015-08-11 16:46:23,459 INFO:mlp - Layers:\n    ImageDataLayer d0: 1 x (64 x 64) nodes\n    ConvLayer conv: 1 x (64 x 64) inputs, 128 x (30 x 30) nodes, RectLin act_fn\n    PoolingLayer mpool: 128 x (30 x 30) inputs, 128 x (15 x 15) nodes, Linear act_fn\n    ConvLayer conv: 128 x (15 x 15) inputs, 256 x (13 x 13) nodes, RectLin act_fn\n    PoolingLayer mpool: 256 x (13 x 13) inputs, 256 x (6 x 6) nodes, Linear act_fn\n    ConvLayer conv: 256 x (6 x 6) inputs, 512 x (5 x 5) nodes, RectLin act_fn\n    PoolingLayer mpool: 512 x (5 x 5) inputs, 512 x (2 x 2) nodes, Linear act_fn\n    FCLayer fc: 2048 inputs, 1024 nodes, RectLin act_fn\n    DropOutLayer dropout: 1024 inputs, 1024 nodes, Linear act_fn\n    FCLayer fc: 1024 inputs, 512 nodes, RectLin act_fn\n    DropOutLayer dropout: 512 inputs, 512 nodes, Linear act_fn\n    FCLayer output: 512 inputs, 128 nodes, Softmax act_fn\n    CostLayer cost: 128 nodes, CrossEntropy cost_fn\n2015-08-11 16:46:23,518 INFO:val_init - Generating UniformValGen values of shape (25, 128)\n2015-08-11 16:46:23,688 INFO:val_init - Generating UniformValGen values of shape (1152, 256)\n2015-08-11 16:46:23,734 INFO:val_init - Generating UniformValGen values of shape (1024, 512)\n2015-08-11 16:46:23,745 INFO:val_init - Generating UniformValGen values of shape (1024, 2048)\n2015-08-11 16:46:23,796 INFO:val_init - Generating UniformValGen values of shape (512, 1024)\n2015-08-11 16:46:23,806 INFO:val_init - Generating UniformValGen values of shape (128, 512)\n2015-08-11 16:46:23,808 INFO:fit - Unable to find saved model /Users/csabaveres/data/NDSB/ndsb-model.prm, starting over\n2015-08-11 16:46:23,809 INFO:mlp - commencing model fitting\nTraceback (most recent call last):\n  File \"/usr/local/bin/neon\", line 240, in \n    experiment, result, status = main()\n  File \"/usr/local/bin/neon\", line 208, in main\n    result = experiment.run()\n  File \"/usr/local/lib/python2.7/site-packages/neon/experiments/fit_predict_err.py\", line 99, in run\n    super(FitPredictErrorExperiment, self).run()\n  File \"/usr/local/lib/python2.7/site-packages/neon/experiments/fit.py\", line 108, in run\n    self.model.fit(self.dataset)\n  File \"/usr/local/lib/python2.7/site-packages/neon/models/mlp.py\", line 150, in fit\n    self.bprop()\n  File \"/usr/local/lib/python2.7/site-packages/neon/models/mlp.py\", line 91, in bprop\n    ll.bprop(error)\n  File \"/usr/local/lib/python2.7/site-packages/neon/layers/layer.py\", line 283, in bprop\n    self.cost.apply_derivative(self.reference)\n  File \"/usr/local/lib/python2.7/site-packages/neon/transforms/cross_entropy.py\", line 254, in apply_derivative\n    targets, self.temp, self.scale)\n  File \"/usr/local/lib/python2.7/site-packages/neon/transforms/cross_entropy.py\", line 148, in shortcut_derivative\n    backend.subtract(outputs, targets, out=temp[0])\n  File \"/usr/local/lib/python2.7/site-packages/neon/backends/cpu.py\", line 508, in subtract\n    np.subtract(self._unwrap(left), self._unwrap(right), out._tensor)\nValueError: operands could not be broadcast together with shapes (128,128) (2,128) (128,128) \nHere is the YAML. Thanks\n----------------------------------------------------------------------------\nCopyright 2014 Nervana Systems Inc.\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n----------------------------------------------------------------------------\n!obj:experiments.FitPredictErrorExperiment {\n  inference_sets: ['validation', 'train'],\ndataset: &ds !obj:datasets.Imageset {\n    repo_path: '~/data',\n    imageset: 'NDSB',\n    square_crop: True,\n    image_dir: '~/data/NDSB/train',\n    save_dir: '~/data/NDSB/batches',\n    output_image_size: &ois 128,\n    cropped_image_size: &cis 64,\n    macro_size: 1024,\n    num_channels: 1,\n  },\nmetrics: {\n    train: [\n      !obj:metrics.MisclassPercentage {},\n    ],\n    test: [\n      !obj:metrics.MisclassPercentage {},\n    ],\n  },\nweight_init: &wt_init !obj:params.UniformValGen {\n    low: -0.1,\n    high: 0.1,\n  },\nlrules: [\n      &ada {\n        type: adadelta,\n        lr_params: {\n          rho: &ro 0.99,\n          epsilon: &eps 0.0000001,\n        },\n      },\n  ],\n# CNN model specification\n  # essentially a multi-layer perceptron with convolutional and pooling layers\n  model: !obj:models.MLP {\n    num_epochs: 20,\n    batch_size: &bs 128,\n    serialized_path: '~/data/NDSB/ndsb-model.prm',\n    layers: [\n      &datalayer !obj:layers.ImageDataLayer {\n        name: d0,\n        is_local: True,\n        nofm: 1,\n        ofmshape: [64, 64],\n      },\n      !obj:layers.ConvLayer {\n        name: conv,\n        activation: !obj:transforms.RectLin {},\n        nofm: 128,\n        fshape: [5, 5],\n        stride: 2,\n        # pad: 2,\n        lrule_init: ada,\n        weight_init: wt_init,\n      },\n      !obj:layers.PoolingLayer {\n        name: mpool,\n        op: 'max',\n        fshape: [2, 2],\n        stride: 2,\n      },\n      !obj:layers.ConvLayer {\n        name: conv,\n        activation: !obj:transforms.RectLin {},\n        nofm: 256,\n        fshape: [3, 3],\n        stride: 1,\n        # pad: 1,\n        lrule_init: ada,\n        weight_init: wt_init,\n      },\n      !obj:layers.PoolingLayer {\n        name: mpool,\n        op: 'max',\n        fshape: [2, 2],\n        stride: 2,\n      },\n      !obj:layers.ConvLayer {\n        name: conv,\n        activation: !obj:transforms.RectLin {},\n        nofm: 512,\n        fshape: [2, 2],\n        stride: 1,\n        # pad: 1,\n        lrule_init: ada,\n        weight_init: wt_init,\n      },\n      !obj:layers.PoolingLayer {\n        name: mpool,\n        op: 'max',\n        fshape: [2, 2],\n        stride: 2,\n      },\n      !obj:layers.FCLayer {\n        name: fc,\n        nout: 1024,\n        activation: !obj:transforms.RectLin {},\n        lrule_init: ada,\n        weight_init: wt_init,\n      },\n      !obj:layers.DropOutLayer {\n        name: dropout,\n        keep: 0.4,\n      },\n      !obj:layers.FCLayer {\n        name: fc,\n        nout: 512,\n        activation: !obj:transforms.RectLin {},\n        lrule_init: ada,\n        weight_init: wt_init,\n      },\n      !obj:layers.DropOutLayer {\n        name: dropout,\n        keep: 0.5,\n      },\n      &lastlayer !obj:layers.FCLayer {\n        name: output,\n        nout: 128,\n        activation: !obj:transforms.Softmax {},\n        lrule_init: ada,\n        weight_init: wt_init,\n      },\n      &costlayer !obj:layers.CostLayer {\n        name: cost,\n        ref_layer: *datalayer,\n        ref_label: 'labels',\n        cost: !obj:transforms.CrossEntropy {},\n      },\n    ],\n  },\n# logging options that are passed to logging.basicConfig\n  # level value thresholds (set level lower to display them):\n  #   CRITICAL 50\n  #   ERROR    40\n  #   WARNING  30\n  #   INFO     20\n  #   DEBUG    10\n  #   NOTSET    0\n  logging: {\n    level: 20,\n    format: '%(asctime)-15s %(levelname)s:%(module)s - %(message)s'\n  },\n}\n. Thanks. The paths are correct. I created directories to match the names.\nIt is the sizes of the input data layer that is confusing me. Are there any hints about how to do that, and avoid the broadcast error?\n. The images are of various sizes. I was under the impression your back end resizes them. Perhaps not?\nCurrently I have image sizes like\n228x240\n500x375\n425x446\nIf I resize them all to 128x128 it should work?\nthanks,  Csaba\n. Yes of course, cat or dog :)\nI did the image resize and changed out and the model is now training.\nThanks so much for your help!\n. ",
    "ipanchenko": "Hello! I tried to make a float64 AlexNet modifying your examples for float16 and float32.\nI'm interested in both GPU backends.\nHere is my YAML: https://gist.github.com/ipanchenko/4e934e6c3f25fa319ec2\nI get the same error on CPU and GPU.\np.s. I also tried to modify layers a way you told me in previous comment - but failed.\nWARNING:neon.util.persist:deserializing object from:  examples/convnet/i1k-alexnet-fp64.yaml\nTraceback (most recent call last):\n  File \"/opt/anaconda/bin/neon\", line 235, in \n    experiment, result, status = main()\n  File \"/opt/anaconda/bin/neon\", line 122, in main\n    experiment = deserialize(args.yaml_file)\n  File \"/opt/anaconda/lib/python2.7/site-packages/neon/util/persist.py\", line 192, in deserialize\n    return yaml.safe_load(load_path)\n  File \"/opt/anaconda/lib/python2.7/site-packages/yaml/init.py\", line 93, in safe_load\n    return load(stream, SafeLoader)\n  File \"/opt/anaconda/lib/python2.7/site-packages/yaml/init.py\", line 71, in load\n    return loader.get_single_data()\n  File \"/opt/anaconda/lib/python2.7/site-packages/yaml/constructor.py\", line 39, in get_single_data\n    return self.construct_document(node)\n  File \"/opt/anaconda/lib/python2.7/site-packages/yaml/constructor.py\", line 43, in construct_document\n    data = self.construct_object(node)\n  File \"/opt/anaconda/lib/python2.7/site-packages/yaml/constructor.py\", line 90, in construct_object\n    data = constructor(self, tag_suffix, node)\n  File \"/opt/anaconda/lib/python2.7/site-packages/neon/util/persist.py\", line 148, in obj_multi_constructor\n    res = cls(__loader.construct_mapping(node, deep=True))\n  File \"/opt/anaconda/lib/python2.7/site-packages/yaml/constructor.py\", line 208, in construct_mapping\n    return BaseConstructor.construct_mapping(self, node, deep=deep)\n  File \"/opt/anaconda/lib/python2.7/site-packages/yaml/constructor.py\", line 133, in construct_mapping\n    value = self.construct_object(value_node, deep=deep)\n  File \"/opt/anaconda/lib/python2.7/site-packages/yaml/constructor.py\", line 90, in construct_object\n    data = constructor(self, tag_suffix, node)\n  File \"/opt/anaconda/lib/python2.7/site-packages/neon/util/persist.py\", line 148, in obj_multi_constructor\n    res = cls(__loader.construct_mapping(node, deep=True))\n  File \"/opt/anaconda/lib/python2.7/site-packages/neon/datasets/imageset.py\", line 131, in init\n    raise ValueError('Datatype not understood')\nValueError: Datatype not understood\n. Thank you! But it seems like another problem with cudanet running should be closed to let me check it. \n. ",
    "alexander-rakhlin": "Hi,\nI set in setup.cfg:\nGPU=cudanet\nAnd tried both:\nsudo env \u201cPATH=$PATH\u201d env \u201cLD_LIBRARY_PATH=$LD_LIBRARY_PATH\u201d make install\nand\nsudo make install\nI didn't try:\nsudo make -e GPU=cudanet install \nneither\nmake -e GPU=cudanet install \nShould it work?\n. I have to wait for reasonable price to check it (something unusual goes on with AWS GPU spot instances recently, up to $6.5/hour for g2.2xlarge)\n. I guess, this is due to parallel operations with floats. They vary precision depending on order of consecutive accumulations. Anyway, I faced such an issue with Numpy matrix dot (that uses BLAS). Only after I turned multi-threading off I was able to reproduce result on Windows. Interestingly, Linux was not affected, have no idea why.\n. ",
    "bibbly": "For various reasons that I won't bore you with, I have had occasion to need DBNs for large-scale image classification tasks (in industry!). And Neon suits my needs pretty well at the moment. I would be most grateful if you guys could put up even a single example of the DBN in use - I can't for the life of me get it working!\nAll the best, Bibbly.\n. ",
    "lorefrn": "nvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2013 NVIDIA Corporation\nBuilt on Wed_Jul_17_18:36:13_PDT_2013\nCuda compilation tools, release 5.5, V5.5.0\n. ",
    "mfassler": "Hello,\nThank you for the confirmation.  \nI can confirm that commit 5d28ddfc512afbfb3... works\nand commit 19829290a7...  does not.\n(in both cases I am forcing the dtype to float32)\n. ",
    "SimoV8": "Hi, do you approximately know when older GPUs will be supported? \n. Ok, thank you :)\n. ",
    "jjgraham": "@nervanasys has this issue been solved ?\nI have a new K80 Kepler and i am having problems\n[root@jupyter neon]# python /Public/neon/neon/backends/util/check_gpu.py\nFound GPU(s) with compute capability: [3.7, 3.7]\n3.7\nWhen i try to run examples i get\nRuntimeError: Device 0 does not have CUDA compute capability 5.0 or greater\n. @nervanasys Thanks for the quick reply !\nWe have a lot of K80s at UCSD and want to use neon \n. ",
    "great-thoughts": "Does neon support older GPU?\n. ",
    "rene4jazz": "Is there support for Fermi cards?\n. ",
    "sensus-sextus": "Thank you very much, it works!\nBut now when I'm trying to modify the network from this mnist example making it \"deeper\" (with more than 1 hidden layer) it stops learning-- the cost function does not decrease upon epochs.\nCan you advice any remedy for this?\n. For some reason, it stucks for any cost function if the optimizer is not RMSProp. Can you explain this somehow?\n. Thank you for reply.\nI 'm now just copy-pasting this code below . For some reason something weird happens for any network with more than  one hidden layer.\nI will be graeatful for any advice regarding this network.\nIf you wish,  you can write me directly sensus.sextud@gmail.com\nThanks again, \nDimitri\n```\nfrom numpy import *\nfrom numpy import genfromtxt\nimport logging\nimport os\nfrom neon.backends import gen_backend\nfrom neon.callbacks.callbacks import Callbacks\nfrom neon.data import DataIterator, load_mnist\nfrom neon.initializers import Gaussian\nfrom neon.layers import GeneralizedCost, Affine, BatchNorm\nfrom neon.models import Model\nfrom neon.optimizers import GradientDescentMomentum, RMSProp\nfrom neon.transforms import Rectlin, Logistic, Tanh, Softmax, CrossEntropyBinary, SumSquared, Misclassification\nfrom neon.util.argparser import NeonArgparser\nlogger = logging.getLogger()\nparse the command line arguments\nparser = NeonArgparser(doc)\nparser.add_argument('--serialize', nargs='?', type=int,\n                    default=0, const=1, metavar='N',\n                    help='serialize model every N epochs')\nparser.add_argument('--model_file', help='load model from pkl file')\nargs = parser.parse_args()\nhyperparameters\nbatch_size = 1000\nnumclass= 20\nnum_epochs =200\nsetup backend\nbe = gen_backend(backend=args.backend,\n                 batch_size=batch_size,\n                 rng_seed=args.rng_seed,\n                 device_id=args.device_id,\n                 default_dtype=args.datatype,\n                 stochastic_round=False)\nload up the mnist data set\nsplit into train and tests sets\n(X_train, y_train), (X_test, y_test), nclass = load_mnist(path=args.data_dir)\ncomboinput = genfromtxt('cinput_vyb48.csv', delimiter=';')# input vector\nretbin = genfromtxt('retbin_vyb48.csv', delimiter=';')#\nX_train =comboinput;\nX_test = comboinput;\nY_train =retbin; # np_utils.to_categorical(y_train, nb_classes)\nY_test = retbin; #np_utils.to_categorical(y_test, nb_classes)\nX_train = X_train.astype(\"float32\")\nX_test = X_test.astype(\"float32\")\nY_train = Y_train.astype(\"float32\")\nY_test = Y_test.astype(\"float32\")\nprint(X_train.shape[0], 'train samples')\nprint(X_test.shape[0], 'test samples')\nsetup a training set iterator\ntrain_set = DataIterator(X_train, Y_train, nclass=numclass)\nsetup a validation data set iterator\nvalid_set = DataIterator(X_test, Y_test, nclass=numclass)\nsetup weight initialization function\ninit_norm = Gaussian(loc=0.0, scale=0.01)\nsetiup model layers\nlayers = []\nlayers.append(Affine(nout=80, init=init_norm, activation=Tanh()))\nlayers.append(Affine(nout=64, init=init_norm, activation=Tanh()))\nlayers.append(Affine(nout=64, init=init_norm, activation=Tanh()))\nlayers.append(Affine(nout=64, init=init_norm, activation=Tanh()))\nlayers.append(Affine(nout=64, init=init_norm, activation=Tanh()))\nlayers.append(Affine(nout=64, init=init_norm, activation=Tanh()))\nlayers.append(Affine(nout=20, init=init_norm, activation=Softmax()))\nshortcut=True\nsetup cost function as CrossEntropy\ncost = GeneralizedCost(costfunc=SumSquared())\nsetup optimizer\nllrate= 2e-4\noptimizer = GradientDescentMomentum(llrate, momentum_coef=0.7, stochastic_round=args.rounding)\noptimizer =RMSProp(stochastic_round=False, decay_rate=0.95, learning_rate=llrate, epsilon=1e-06, clip_gradients=False, gradient_limit=5, name='rmsprop')\ninitialize model object\nmlp = Model(layers=layers)\nif args.model_file:\n    assert os.path.exists(args.model_file), '%s not found' % args.model_file\n    logger.info('loading initial model state from %s' % args.model_file)\n    mlp.load_weights(args.model_file)\nsetup standard fit callbacks\ncallbacks = Callbacks(mlp, train_set, output_file=args.output_file,\n                      progress_bar=args.progress_bar)\nadd a callback ot calculate\nif args.validation_freq:\n    # setup validation trial callbacks\n    callbacks.add_validation_callback(valid_set, args.validation_freq)\nif args.serialize > 0:\n    # add callback for saving checkpoint file\n    # every args.serialize epchs\n    checkpoint_schedule = args.serialize\n    checkpoint_model_path = args.save_path\n    callbacks.add_serialize_callback(checkpoint_schedule, checkpoint_model_path)\nrun fit\nmlp.fit(train_set, optimizer=optimizer, num_epochs=num_epochs, cost=cost, callbacks=callbacks)\nprint('Misclassification error = %.6f%%' % (mlp.eval(valid_set, metric=Misclassification())*100))\n```\n. Thanks again, it works much better with Xavier now!\nSorry, but now I'm stuck running fprop to get network predictions.\nThis code:\nX0= X_test #[0:999,:]\nxx= be.array(X0)\nprdc= mlp.fprop(xx)\nproduces the following error:\nFile \"D:/Dima/Python/neonka/bloo_neon5.py\", line 157, in \n    prdc= mlp.fprop(xx)\nFile \"D:\\Python27\\lib\\site-packages\\neon-1.0.0rc1-py2.7.egg\\neon\\models\\model.py\", line 173, in fprop\n    x = l.fprop(x, inference)\nFile \"D:\\Python27\\lib\\site-packages\\neon-1.0.0rc1-py2.7.egg\\neon\\layers\\layer.py\", line 422, in fprop\n    self.be.compound_dot(A=self.W, B=inputs, C=self.outputs)\nFile \"D:\\Python27\\lib\\site-packages\\neon-1.0.0rc1-py2.7.egg\\neon\\backends\\nervanacpu.py\", line 763, in compound_dot\n    assert B.shape[1] == C.shape[1]\nI have no ideas how to fix it?\n. This is not the case. I've tryed to use inputs from the training or testing set (that were succesfully used to train/test the net) but it does not help!\n. How can I control batch size of the inputs? \nI trieed making second dimension of the input array equal to \"batch_size \"  parameter of the network but it does not remedy the situation?!\n. now it works if the input have exactly batch_size examples, thank you!\nis there any way to get prediction if I have different number of input vectors?\n. there was an indexing error while creating sub-arrays.\nthank you very much for explanation\n. Sorry for insisting, but  the main isssue is that such a network cannot predict the class when trained on GPU!!!!\nThe actual accuracy (what I measured using fprop()) was 95% and 0.1% on CPU and GPU respectively, so misclassification rate was 5% and 99.9%\nOn the contrary, if checked by mlp.eval(valid_set, metric=Misclassification()), the reported rate was close to 5% in both cases.\n. In all this experiments I'm using SumSquared() as a cost.\nIt works perfectly on CPU but fails on GPU.\nI think this error is something technical not algorithmic.\n. Actually, this situation does not depend on the last-layer activation function (softmax or logistic) \nI was able to fix it by using the following code to get predictions:\nX0= X_test [0:1000,:]\nX1= X0.transpose()\nxx= gp.array(X0, dtype=float32)\nxx1=xx.T\nprdc= mlp.fprop(xx1)\nFor some reasons that are unknown to me, if I transpose the array before putting it to backend, the prediction fails, as I described above\n. ",
    "jcoreyes": "Depending on what you're doing, using a masked cost might be enough http://neon.nervanasys.com/docs/1.0.0rc1/generated/neon.layers.layer.GeneralizedCostMask.html#neon.layers.layer.GeneralizedCostMask. \nThough I can see for some cases where you'll need a mask in the lstm layer.\n. There's an example here https://github.com/NervanaSystems/neon/blob/master/neon/data/imagecaption.py. The data layout for lstm input is (feature_size, steps * batch_size) where moving form left to right of the input tensor, you have the first step of a batch, then the second step of a batch and so on. So the mask tensor will have 1's where the sequence continues and 0 elsewhere. To use the cost mask, the dataset iterator has to return 2 elements where the 1st element is your normal input x and the 2nd element is a tuple (targets, mask).\n. That's correct. Be careful with the data layout because a batch is contiguous not a sentence.\n. It's going to take some work to get it working on new images but there are multiple ways to do it. You can use the VGG model from NeuralTalk to get the features, then create a new dataset iterator in Neon based on imagecaption.py which just loads in the images. We're working on creating a script to convert caffee models to neon models so that you'd be able to run a pretrained VGG from caffee in Neon. We're also planning on adding a VGG example ourselves.\n. This will have most of what you need https://github.com/NervanaSystems/neon/blob/master/neon/data/imagecaption.py. In read_images(), load in the VGG features from NeuralTalk instead of the presaved ones. In ImageCaptionTest, you can remove the sentence loading code in the constructor and the ref sentence loading in the iter. \n. On line 128 in that file try wrapping pred as an int so use x[int(pred), 0] = 1. This was only tested on a GPU where it was working. If you have access to a Maxwell GPU btw, one training epoch takes about 58 s.\n. How big are these array of tensors? You might still get a bottleneck using a gpu array of gpu tensors vs a cpu array of gpu tensors depending on how you access them. Is using b = np.array([tens1, tens2]) too slow?\n. ",
    "sxjzwq": "I have the same question of using the masked cost. Do you have any examples of using the masked cost layer?\nfor example, given an image and a question with an answer sentence, both the words of question and answer will be input the LSTM step by step, however, I only want to measure the cost and errors between the generated answer words and the target (ground truth), which means I only want to calculate the cost and error only based on parts steps of the LSTMs outputs. I guess the masked cost layer will be helpful in this case. But how should I use it and where should I give the mask?\nIt will be really helpful if you have any examples. And please also let me know if I made any mistakes of understanding of the usage of this layer in the above example.\nThanks.\n. Thanks. I think that's what I need.\nAs my understanding, in the captioning case, the mask is like [1,1,1,1,1,0,0,0...] the last '1' means the end of target sentence. In the question answering case, the mask should be like [0,0,0,0,0,1,1,1,1,0,0,0,0...]. The first sequence of '0's corresponds to the given questions. All '1's corresponds answers and the last '1' is the end of answer. Then the cost function will be only applied on those steps marked as '1'.\nAm I right?\nThanks.\n. ",
    "soheilb": "I also think having a mask implementation for lstm/rnn to support varying length sequences within a batch is quite helpful. After all, one of the main advantages of using recurrent architectures is that they can handle varying length sequences...Here is the corresponding Theano example:\nhttp://deeplearning.net/tutorial/lstm.html\n. ",
    "hyln9": "I see. Thanks!\n. I found it's very difficult to debug when nervanagpu kernel crashed.\nSo I'm looking forward to the official implementation.\n. Wow, thanks!\n. ",
    "scott-gray": "Seems I introduced a bug in 3d conv with some recent convolution changes.  A fix is already in and will be released soon.  We'll have better unit tests for 3d conv as well.\n. As far as the basic pooling algorithm works, I'd have a look at the CPU backend numpy code.  My kernels are implemented in the same way (though I have a much faster kernel now for bprop avg pooling that will be released soon).\nThe GPU kernels are currently written in assembly.  The primary reason for this is that I wanted to leverage the fp16x2 atomic adds that maxwell has but are inexplicably unexposed in the cuda or ptx APIs.\nBut if you're only concerned about fp32 than there's no reason they can't be written in cuda c.  Because our framework sits on top of pycuda it's rather trivial to integrate some custom cuda code.  Take a look at the bottom of float_ew for some examples of this.  Most of what what float_ew does is dynamically building cuda-c and compiling (and caching) it on the fly.\nI'm curious, what pooling operation are you interested in?  Maybe I can give you more explicit guidance if I knew more of what you were trying to do. \n. Well I could outline the best approach to writing those kernels, I just need to better understand how the computation works.  For example, do you truncate the Gaussian tails of the kernel?  Or does the filter size just match the image size (and then maybe move the mean around as the sliding operation)?\nThe stochastic pooling seems pretty doable.  Should be pretty similar to regular max pooling.\n. You say it can be similar to an average.  Does this mean you also divide by the pooling window size?  Or is each kernel position just the summed pointwise multiplication of the underlying image pixels?\nIf you have multiple pooling kernels, how does that effect the output.  Does that expand the feature map dimension or do you sum each kernel into a single output?\nFrom what you describe I think I can make this only slightly more expensive than average pooling.  And with my new code this is quite fast (bprop is no longer bottle-necked on atomic adds in the overlapping filter case).\n. Ok, so I wrote the kernels tonight.  You can find them here: https://gist.github.com/scott-gray/5a3cd70465dcd2fe1df1\nNot sure if you wanted padding or not so I implemented fprop both ways.  The way bprop works it needs padding logic so there's just one version.  Both of these kernels should run at the same speed as the new  avg pooling kernels.\nI'll write the pycuda wrappers and the numpy to test them tomorrow night or over the weekend.  I'll add some more comments to explain how they work too.\nI need to ponder some more the best way to implement multiple kernels.  I guess the question depends on how many you want to have.\n. Ok, I just updated the gist with a basic cpu/gpu implementation and a single test.  You should be able to take things from there.  I opted not to implement padding as in this case it makes the derivative really complicated when using the efficient method used here to compute bprop.  The other way you can compute bprop is pretty much a copy of the forward code and then a bunch of atomic adds to the output.  For large filers this way can be really slow.  But padding should be straight forward with that method.\nYou'll also note I added normalization to the Gaussian.  I had left that out previously.  This is actually what makes bprop so tricky when padding is used (each filter position can potentially be normalized differently).\nAs far as multiple filters,  you have two choices.  You can keep the code as is and have multiple parallel layers.  In fprop you would output to a slice of the full output and in bprop you would accumulate the deltas with the beta param.  Take a look at the Inception class in the layer_gpu.py code for an example of this.\nOr you could pack an additional filter dimension into one of the block indexes and use integer division to extract it.  With careful consideration of block index ordering you could maximize cache usage so you can get more filters with little extra cost.  Bprop would use atomic adds on the output.\nIf you want to make the var and mean params learnable, then I'd store them in device memory instead of passing them as params.  If you have multiple filters you could have an array of such params.\nThis implementation is pretty tantalizingly close to how convolution works.  The main difference is that you don't reduce the input feature maps.  If you're going to be using big filters I suppose modifying the fprop and bprop conv kernels might be a really efficient way to implement this. \n. In raw benchmarks our gemm kernels are definitely faster than cublas.  This is particularly true for small minibatches (32,64,96) or in fp16 where the speedup can be 2-3x over cublas.  I have not yet had a chance to profile our RNN code to make sure we're compounding all the operations we can be, but I do plan to do that soon.  Having your own gemm kernels offers interesting additional compounding opportunities not available when using a closed source library.  For example I was recently able to compound the mean component of batch norm inside the convolution kernels at no additional cost.  I'll be adding that code to the gemm kernels soon.  Or I could add a bias and/or perform various activations (relu is already supported).\nI'll let the RNN experts here speak to the point on embeddings.  I will say that our backend supports numpy style fancy indexing and take operations which sounds similar to what you want.  But if there's some missing functionality that can speed these networks up it's a safe bet that we'll make the time to add it.\n. Here's a great post from Erich at Baidu reviewing performance optimizations for RNNs.  He also has lots of benchmarks comparing my gemm kernels to cublas:\nhttp://svail.github.io/\n. Here's an example of some much faster cpu conv code.  It eliminates the fancy indexing which requires a copy of the tensor to be made prior to the dot.  So this code does the dots in place.  We'll update all the conv/pooling operations with this code at somepoint soon, but you may want to take a crack at it sooner.  The new elementwise/autodiff code might also be making more tensor copies for various operations.. we'll investigate that as well.\n```\ndef fconv_slice(q, S, X, padding, strides):\n    qs = q * strides - padding\n    firstF = None\n    for s in range(S):\n        x = qs + s\n        if x >= 0 and x < X:\n            if firstF is None:\n                firstF = s\n                firstI = x\n            lastF = s\n            lastI = x\n    return (slice(firstF,lastF+1), slice(firstI,lastI+1))\ndef fprop_direct(I, F, O, padding, strides):\nC, Y, X, N = I.shape\nC, R, S, K = F.shape\nK, P, Q, N = O.shape\n\nqSlice = [ fconv_slice(q, S, X, padding, strides) for q in range(Q) ]\n\nfor p in range(P):\n    sliceR, sliceY = fconv_slice(p, R, Y, padding, strides)\n\n    for q in range(Q):\n        sliceS, sliceX = qSlice[q]\n\n        slicedF = F[:,sliceR,sliceS,:].reshape((-1, K))\n        slicedI = I[:,sliceY,sliceX,:].reshape((-1, N))\n\n        O[:,p,q,:] = np.dot( slicedF.T,  slicedI )\n\n```\n. I haven't been able to track it down yet.  I only reduced the number of\nreduction threads:\nFrom float_ew:\n# speed up deep reduction by using more than 32 threads\n                # if reduction and not argminmax:\n                #     if red_depth >= 4096:\n                #         threads = 1024\n                #     elif red_depth >= 2048:\n                #         threads = 512\n                #     elif red_depth >= 1024:\n                #         threads = 256\n                #     elif red_depth >= 512:\n                #         threads = 128\n                #     elif red_depth >= 256:\n                #         threads = 64\n                # TODO: The above code can lead to a race condition.\n                # Temporary workaround while investigating further:\n                if red_depth > 256:\n                        threads = 64\nBy doing this, the shared memory reduction is much simplified and the\nproblem seems to go away.  This does reduce reduction performance a bit\nthough.\nOn Sat, Jan 2, 2016 at 12:24 PM, Oleg Trott notifications@github.com\nwrote:\n\nThat's great to hear, thanks! I know these things can be hard to track\ndown.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/171#issuecomment-168426223\n.\n. There are no missing synchthreads.  I analyzed the code at the assembly\nlevel and didn't spot any potential race issues.  This may actually be a\nhardware bug (I've seen flakiness with predicated shared stores before).  I\nneed to spend some more time generating the simplest possible repro.  But\nfor now the above code change should let you work.\n\nOn Sat, Jan 2, 2016 at 2:05 PM, Oleg Trott notifications@github.com wrote:\n\nOne possibly silly suggestion I have is to insert _synchthreads(); after\nevery ; , \\n and } in the kernel, one at a time, and retest. This is\ntedious to do manually, but I think it can be automated and run overnight.\nMany of those insertions will break compilation, others might make the\nprogram incorrect, but if one of them apparently fixes the issue, it might\nprovide insights into what's going on. (Feel free to ignore, especially if\nyou are sure that it's not a missing _synchthreads() )\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/171#issuecomment-168433614\n.\n. Thanks,  don't know why I didn't think to try adding a __syncthreads there.  I still want to spend some time investigating why it's needed.  It's probably the best work around to a hardware shared memory store erratum.  Most of the time is spent in the first loop reducing memory into all the threads.  Adding a syncthreads here will not effect performance in any meaningful way.\n. I'm not sure.  I need to study it more.\n. Actually, I'm not sure why that works at all.  You shouldn't be putting a __syncthreads inside code that can be executed conditionally.  At least that's how it used to work.  But I see in the sass the branching is managed with SYNCs so maybe that's ok.  A SYNC branch doesn't actually branch, just executes the instructions as no-ops if the thread is inactive.\n\nLooking very closely at the sass I still can not spot any problem.  Sometimes adding __syncthreads just modifies the timing of things making the problem go away.  Shuffle instructions require no syncing as far as I know.  It's more likely the first __syncthreads is the effective one so maybe try moving it out and just above the loop:\n```\n    if ( tid < 32 )\n    {{\n        %(share2_red{0})s\n    __syncthreads(); // Seems to be required to prevent a race condition.\n\n    #pragma unroll\n    for (int i = 16; i > 0; i >>= 1)\n        %(shfl_red{0})s\n\n    sPartials[tid] = %(var_red{0})s;\n}}\n\n```\nIndeed I just checked this and it works too.  There's probably something very subtle going on with interactions between SYNC branches and shared memory operations.  If I were hand coding the sass I'd see that all the branches are warp uniform and hence use real branches.  This would run faster.  But the compiler doesn't seem smart enough to tie conditional logic back to the thread id and assumes all conditionals can be non-warp-uniform.  I guess I could use inline ptx here and use real branches.  But, if that syncthreads mysteriously solves the problem, I guess I'll hold off on that.\n. Neon now can be run on the X1.  Nervanagpu development has stopped and has been been moved into neon so it wont get that update.  Neon now builds the sass kernel stubs with ptxas instead of nvcc. This allows you to specify a 64 bit address size in the 32bit binaries.   This is what was previously broken.\nNew kernels will be available soon to allow training and inference with very small batch sizes.  These should significantly outperform cudnn on the X1.\n. Seems like a nice to have feature, but we'll probably wait till we have a full computational graph for that one.\n. I already have a custom axis=0 reduction kernel that I use in convolution for deterministic mode.  I was planning on integrating that method into the ew code at some point.\nThe one advantage of the current method is that it blends well with the other code keeping complexity low.  Not sure how much the new way will add to that complexity.\n. Adding arbitrary slicing support for input/output tensors would be pretty easy.  Just a few more kernel params and some additional pointer arithmetic.   I think the common use case there is merging the feature map output of two branches.  But K for us is an outer dimension and so merges are just simple contiguous concatenations.  I haven't had any requests for this and was unaware it was a desired feature.   I'll check out cudnn and see if I can at least match their support.\n. Originally I didn't add this because I wasn't aware of a use, plus I was using atomics to accumulate the gradient.  But the new deterministic mode reduces the gradient in a separate cuda-c kernel.  It would be trivial to add a beta to that code.  For non-determ mode I can just pre-apply the beta and have the atomics accumulate on top of that.\n. You're right, it should be KRST in kernel_specs.  But as it's coded now,\nthose params are never actually used by name.  They're just there to\ngenerate the signature.  Then the kernels are called with the driver api\nand a packed struct.\nIn bprop, K and C are swapped so it is correct there.  Anyway, I'll change\nit for better consistency.\nI'll have a brand new set of more flexible direct conv kernels out soon.\nOn Fri, Mar 25, 2016 at 1:07 AM, Ming Wei notifications@github.com wrote:\n\nFrom neon/backends/kernels/ptx/sconv_fprop_K128_N128.ptx, we can see\nthere is a parameter called param_CRST, which I think is supposed to\nrepresent C_R_S_T. However, in the actual calling code in\nneon/backends/convolution.py line 130, the argument is K_R_S_T.\nIs it a typo or something? I believe the ptx declaration (which is\ngenerated from neon/backends/kernel_specs.py) should be param_KRST\ninstead.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/220\n. This seems like it could be relatively easy to add to the direct convolution kernels.  I just need to work out the striding math for bprop.  I'm working on a brand new set of direct conv kernels now and will look into this.\n. @gpapan maybe you can help me a bit.  I think the below convolution code is fairly easy to understand.  Perhaps you could modify the slicing function to do what you want.  The sass kernels work pretty closely to this logic.\n\nfprop conv code\n. @gpapan That looks super easy to implement.  I'm just finishing up some major code cleanup so new features like this should be trivial to drop in going forward.\n. This one is done is pending merge for release.\n. That's fine.  Or a link to neon if you're talking very specifically about our implementation.\n. cuDDN v5 has a F(2x2,3x3) implementation now, though it's quite a bit slower than our F(2x2,3x3).  And of course we also have F(4x4,3x3) which cuDNN will also eventually have.  I'm also working on kernels for 5x5 and 7x1 filters.\nAnyway, just site both then.\n. In that reddit post I said this:\n\nShader assembly (sass) level programming is essential for this task to get the most performance, and even required for some of the more complicated kernels that are extremely resource tight.\n\nThe fastest winograd implementations will be partially or fully fused (transforms + batched gemm).  But you could still get decent speed doing a non-fused version in cuda-c or opencl and just use a basic batch gemm BLAS call.\n. That restriction has been lifted in newer kernels.  These will be released in a week or so.  In the meantime you should be able to pad out your K dim to a multiple of 4.\n. The magic number code comes from here:\nhttp://www.hackersdelight.org/hdcodetxt/magicgu.py.txt\nI wasn't too creative about renaming it.\nOh, and you know the kepler cuda code is just direct conv right?  It's not clear why you named your repo winogradcl.. but I guess you could be intending to add implementations of that?\n. Actually, Stewart wrote those kernels.   And again. .they're meant to be run on sm_30 devices (Amazon Cloud mainly).  So they're not nearly as fast as they could be if designed for >=sm_35 (lots more registers and shared memory).\nBut for winograd, one approach is just write external transforms and use a BLAS lib for the batched gemm.  Some of the external transforms are already done for you in the winograd_conv.py file.  You would just need the output transforms.  The 4x4 transform will work much better externally because it only expands the input/output/delta data by 2.25x.\n. When bprop is cast as fprop upside down, you also need to mirror RST.  You could do that inside of the conv kernel but that would add extra logic, where adding it here is just tacking it onto an already bandwidth bound operation at no cost.  Plus, when your stride is 1 you can use the fprop kernel with no changes.  This comes in handy for winograd where the stride has to be 1 and cuts out a whole kernel you need to implement.\n. You are right in that the code is technically incorrect in that it could try to fetch data out of bounds.  In practice this rarely generates an error.   Given how out of bounds detection works on the gpu it would take an extreme set of circumstances for this to be a problem.  The output predicates are accurate so any invalid data that may have been computed is thrown away.\nBut assumptions can sometimes be wrong, and it's generally better to be on the safe side and properly predicate every memory access.  I think I wrote this kernel prior to discovering the utility of the predicate spilling instructions (R2P, P2R).  I no longer take this shortcut in more recent kernels.\n. Here's some code that demonstrates their use:\n// Calc and store some predicates in a register:\n--:-:-:-:1      IADD   txb1, txb, 1;\n--:-:-:-:1      IADD   txb2, txb, 2;\n--:-:-:-:1      IADD   txb3, txb, 3;\n--:-:-:-:1      ISETP.LT.AND P2, PT, txb,  param_n, PT;\n--:-:-:-:1      ISETP.LT.AND P3, PT, txb1, param_n, PT;\n--:-:-:-:1      ISETP.LT.AND P4, PT, txb2, param_n, PT;\n--:-:-:-:1      ISETP.LT.AND P6, PT, txb3, param_n, PT;\n// 0x5c is the bitmask for preds 2,3,4,6\n--:-:-:-:1      P2R predsB, PR, RZ, 0x5c;\n// Load and use the stored predicates:\n--:-:-:-:1      ISETP.GE.AND P2, PT, k, 8, PT;\n--:-:-:-:1 @!P2 R2P PR, RZ,     0x5c;\n--:-:-:-:1  @P2 R2P PR, predsB, 0x5c;\n// let 13 clocks pass\n08:-:-:-:1  @P2 LDG.E.CI.$dtype loadB0, [trackB + ${dsize}x<0>];\n--:-:-:-:1  @P3 LDG.E.CI.$dtype loadB1, [trackB + ${dsize}x<1>];\n--:-:-:-:1  @P4 LDG.E.CI.$dtype loadB2, [trackB + ${dsize}x<2>];\n--:-:4:-:1  @P6 LDG.E.CI.$dtype loadB3, [trackB + ${dsize}x<3>];\nBut what you have is fine.  Just look out for all the places P1 is updated after K is modifed.  4,5,6 would also have to be updated.  \n--:-:-:-:0      ISETP.GT.AND P1, PT, K, RZ, P1;\nWith predicate un-spilling it only takes 3 instructions instead of 4.\n. Stewart, it should be pretty trivial to integrate these new gemm kernels into neon:\nhttps://github.com/openai/openai-gemm\nThe 32x32 tiles really help utilization and are at least as fast as the 128x32 kernels.  The python code also memoizes most of the launch overhead.  Those kernels also allow arbitrary mixed precision (though that python code doesn't expose it to limit kernel permutations for libs that statically load them)\n. X is a tensor.  It can be used for beta or bprop-relu but not both.  And keep in mind the bprop-relu is the grad of the relu from the previous layer, so it's a little tricky to leverage this with layer based code.  This option is better suited for enabling though a graph fusion engine like XLA or ngraph.. ",
    "pcallier": "FWIW I have the same issues and error message with missing kernels on our Maxwell GPU with CUDA 7.0\n. In lieu of opening a new issue, fixed another typo in the same file\n. Behavior is correct when using np.int32. Comments make reference to \"numpy integer\" type which should be more specific.\n. I'll second this. Relatedly, using the NeonArgparser is unnecessarily cumbersome for many applications, especially to salvage existing code that didn't use cmd line arguments. Would one way to address this be to open up the NeonArgparser interface to inputs from within the program in addition to sys.argv or config files?\n. ",
    "EricZeiberg": "Can you elaborate more on the NeuralTalk VGG model option? I understand how to extract the features from the image, but then how do I pipe it back into the imagecaption file?\n. ",
    "chetan51": "@jcoreyes Thanks, I sent a pull request to that effect.\n. ",
    "NeuroCampos": "I just cloned the repo today and have the following error (instead of generating text) on the text_generation_lstm.py example.  I'm running this on a TitanX in a DIGITS DevBox.\n(.venv)michael@BlackBox:~/code/neon4/neon/examples$ ./text_generation_lstm.py\nNetwork Layers:\n    LSTM LstmLayer\n    Linear Layer 'LinearLayer': 512 inputs, 67 outputs\n    Bias Layer 'BiasLayer': size 67\n    Activation Layer 'ActivationLayer': Softmax\nEpoch 0   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1004/1004 batches, 2.78 cost, 67.14s][Validation 2.36 cost, 3.05s]\nEpoch 1   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1004/1004 batches, 2.10 cost, 67.41s][Validation 2.20 cost, 3.07s]\nEpoch 2   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1004/1004 batches, 1.96 cost, 67.31s][Validation 2.13 cost, 3.05s]\nEpoch 3   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1004/1004 batches, 1.89 cost, 67.51s][Validation 2.10 cost, 3.07s]\nEpoch 4   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1004/1004 batches, 1.85 cost, 67.37s][Validation 2.09 cost, 3.07s]\nEpoch 5   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1004/1004 batches, 1.82 cost, 67.07s][Validation 2.08 cost, 3.06s]\nEpoch 6   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1004/1004 batches, 1.80 cost, 67.40s][Validation 2.08 cost, 3.06s]\nEpoch 7   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1004/1004 batches, 1.78 cost, 67.47s][Validation 2.08 cost, 3.07s]\nEpoch 8   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1004/1004 batches, 1.77 cost, 67.28s][Validation 2.08 cost, 3.05s]\nEpoch 9   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1004/1004 batches, 1.76 cost, 67.56s][Validation 2.07 cost, 3.07s]\nTraceback (most recent call last):\n  File \"./text_generation_lstm.py\", line 120, in \n    y = model.fprop(x)\n  File \"/home/michael/code/neon4/neon/neon/models/model.py\", line 202, in fprop\n    x = l.fprop(x, inference)\n  File \"/home/michael/code/neon4/neon/neon/layers/recurrent.py\", line 287, in fprop\n    self.init_buffers(inputs)\n  File \"/home/michael/code/neon4/neon/neon/layers/recurrent.py\", line 95, in init_buffers\n    self.xs = get_steps(inputs, self.in_shape)\nAttributeError: 'LSTM' object has no attribute 'in_shape'\nI figure that it either has something to do with skipped/different configuration steps relative to what happens with model.fit, or maybe it has to do with the reduced number of timesteps in the text generation portion of the code.\n. Tried it and it works!  Thanks @yinyinl \n. ",
    "sherjilozair": "It seems maxwell stuff wasn't installed since it was not visible at compile time. I'll close this issue.\n. ",
    "vsov": "Unfortunately, that wasn't a build issue. I have pulled latest master, it's being recompiling everything right now, but it hardly will fix the issue.\n. Nope, we've got the same error... Everything was rebuilt from the scratch, make test says that everything is ok...\n. I have resolved the issue by removing all components (including maxas and nervanagpu) and installing everything from the scratch. Without that some files in neon were never replaced by newer versions, I don't know why.\n. ",
    "Sowasa": "Particularly I am interested in these two:\nhttp://www.matthewzeiler.com/pubs/arxiv2012/arxiv2012.pdf\nhttp://www.matthewzeiler.com/pubs/iclr2013/iclr2013.pdf\nAnd in parametric ReLu's, but that should be doable.\nFor now I want to start with the first, which is differentiable pooling. My plan is to first implement it without learnable parameters, so I will set them by hand in the layer definition. Then I can already do some checks and implement the gradients only if everything looks good and some toy experiments succeed.\nSo basically what I need first is a pooling layer that convolves with a Gaussian kernel and has a parameter to set an aspect (for now only variance) of the weight filler. The layer has to be fairly standard, just like any other pooling layer with one (or later more) additional parameter. However, the Gaussian pooling with different variances makes it necessary to convolve with each pooling kernel and I assume for now the tricky part is how to do this efficiently.\nCan you give me some guidance on how to achieve this within your framework without slowing everything down too much? A few steps would already be highly appreciated.\n. Basically the kernel works like any other pooling kernel, just that here a Gaussian can move around in it. It is of an a priori defined size, maybe a bit bigger than in usual max-pooling (e.g. 11x11) to not truncate the tails too early for a reasonable averaging window in the extreme case of it just doing average pooling (this happens if mean is in center and variance is very big). The kernel is centered to every other pixel in the image depending on the stride and then convolved with each location. So I'd say it is again quite similar to max-pooling in practice for the forward pass, just that one needs a convolution every time, which might make it more costly. The experiments I want to make first is to try if one needs to define more than one pooling kernel per feature map. So the design should allow let's say 2 or 3 pooling kernels per feature map in parallel, first experiments will show if this is necessary.\n. In the Gaussian average case, this is taken care of by the fact that a Gaussian lowpass is normalized such that it always sums to one afaik, regardless of its size. Which means it becomes more flat if it is bigger. So it is just a convolution with the kernel and there is no normalization necessary afterwards if the kernel was scaled properly.\n. The output of the different kernels should be kept separate for now, hence expand the feature maps, as some experiments will be needed to see what works best.\n. This is amazing. I will wait for your tests and then start playing around. \nThe question on how many kernels per feature map are needed is hard to answer, as this is not clear to me from the paper, so best would be flexibility in that regard to leave room for some experiments.\n. Thanks a lot, this is all very helpful!\n. ",
    "petrux": "@yinyinl glad to ear that the embedding layer is coming soon. So, right now, the only way is to use a one-hot multiplied for the embedding matrix which is the weight matrix of a linear layer, right?\n. Thanks. Actually I am experiencing some issues. For instance: is there any way to generate variable length output using a recurrent model? It doesn't seems so...\nP.S. is there a neon user mailing list to ask such question? ok, too many questions here... :-)\n. Great. Thanks!!!\n. ",
    "tmbo": "@scottl Thats great! I am looking forward to that as well (just had a look to see if I can implement a DataIterator on my own that does that, but unfortunately one needs to provide dataset.nbatches which is not possible in my case). \nHow is the process on this one, are you still on track for beginning of December?\n. ",
    "buriy": "\nSuppose you could build as non-root with a user with nvcc, and then separately run sudo make sysinstall after.\n\nYes this is exactly my case, that didn't work.\nAlso please could you make INSTALL file with install instructions...\nCurrently had to read Makefile to understand that \"sudo make sysinstall\" is the right command that does installation. There was also \"sudo python setup.py install\" option -- which one is better?\n. Thanks a lot!\nReshapeLayer is what I thought about (similar to http://caffe.berkeleyvision.org/doxygen/classcaffe_1_1ReshapeLayer.html)\nI also realized that all RNN examples don't have this Affine+RNN combination.\nI'd like if you could add an example like this, and possibly include that kind of layer -- I believe a lot of people would be interested in learning how to do that transformation.\nFrom documentation it's not currently possible to get how to solve this task in the best way -- only by experimentation & learning library design.\nI'll try to make a pull request with my implementation.\n. Found the patch looks more simple than I thought initially:\n``` diff\ndiff --git a/neon/layers/layer.py b/neon/layers/layer.py\nindex df2976e..589ea95 100644\n--- a/neon/layers/layer.py\n+++ b/neon/layers/layer.py\n@@ -618,13 +618,14 @@ class Linear(ParameterLayer):\n def fprop(self, inputs, inference=False):\n     self.inputs = inputs\n\n\nself.be.compound_dot(A=self.W, B=self.inputs, C=self.outputs, bsum=self.batch_sum)\nself.dev_inputs = inputs.reshape((self.nin, -1))\n\nself.be.compound_dot(A=self.W, B=self.dev_inputs, C=self.outputs, bsum=self.batch_sum)\n         return self.outputs\ndef bprop(self, error, alpha=1.0, beta=0.0):\n     if self.deltas:\n         self.be.compound_dot(A=self.W.T, B=error, C=self.deltas, alpha=alpha, beta=beta)\n-        self.be.compound_dot(A=error, B=self.inputs.T, C=self.dW)\n+        self.be.compound_dot(A=error, B=self.dev_inputs.T, C=self.dW)\n     return self.deltas\n```\n\n\nAll tests pass. And I can continue to pass 2-dimensional data through DataIterator.\n. @apark263 could you take a look at the proposed patch? Should I make a pull request?\n. Hmm. Actually can't confirm whether it's learning correctly after this patch. Looks like it is not.\n. Btw my reshape layer looks like this:\n```\nclass Reshape(Layer):\n\"\"\"\nArguments:\n    init (Initializer, optional): Initializer object to use for\n        initializing layer weights\n\"\"\"\n\ndef __init__(self, name=\"Reshape\"):\n    super(Reshape, self).__init__(name, \"Disabled\")\n    self.has_params = False\n    self.inputs = None\n\ndef __str__(self):\n    return \"Reshape Layer '%s': %s inputs, %s outputs, %s steps\" % (\n           self.name, self.nin, self.nin, self.nsteps)\n\ndef configure(self, in_obj):\n    super(Reshape, self).configure(in_obj)\n    (self.nin, self.nsteps) = self.in_shape\n    self.out_shape = (self.nin, self.nsteps)\n    return self\n\ndef fprop(self, inputs, inference=False, beta=0.0):\n    self.inputs = inputs\n    tmp = inputs.reshape((self.nin, self.nsteps, self.be.bsz))\n    self.outputs = tmp.reshape((self.nin, self.nsteps * self.be.bsz))\n    return self.outputs\n\ndef bprop(self, error, alpha=1.0, beta=0.0):\n    return error\n\n```\nMight be also useful for #221 .\n. Hmm. Actually can't confirm whether it's learning correctly after this patch. Looks like it is not.\n. Yes, I'm trying to do L2 penalization.\nBy \"input\" i mean hidden / non-final layers.\nBy values I mean weights themselves.\nSomething like\nhttp://lasagne.readthedocs.org/en/latest/modules/regularization.html\nIdeally if I could sum up several different loss functions for same or different layers and they would work automagically.\nAlso I'd like to have weight decay and learning rate change schedules not tied to GradientDescentMomentumWeightDecay , I guess that's in progress for v 1.2 .\n. Cool! Thanks for details! Are there any docs on this? @jennifermyers @thestew42 \nShould we reopen the issue?\n. @tambetm You also need sudo update-alternatives --install then : http://askubuntu.com/a/26518/346907 , also having multiple versions of gcc with update-alternatives isn't considered a good option ( why: http://askubuntu.com/a/26500/346907 ).\nAlso, even after installing gcc-4.9, you will have 5.3 version of linux headers.\nSo anyway it's only a workaround for the scripts not aware of CC / CXX flags, and that's why manually linking is ok here.\n. Thanks,\nI would like though if you could correct your documentation because right now you have the following in the docs:\n\"OpenCV is also a required package. We recommend installing with a package manager (e.g. apt-get or homebrew).\" at http://neon.nervanasys.com/docs/latest/installation.html , \nand I'm describing you what will you have if you installed it with the package manager.\nThere is no way at the moment to install OpenCV binaries with Cuda support in Debian and Ubuntu .\n. If I remember correctly, h5py v 2.5.0 can't install from a binary package and attempts to compile (but doesn't do well with numpy 1.9), v 2.6.0 can.\nI just changed all \"==\" to \">=\" in neon requirements.txt file.\n. ",
    "shivshil": "Thanks, overlooked that section. Understood.\n. ",
    "felipefariax": "Yes, @apark263! I am also surprised...\nIt's a very simple model...\n```\ninit = GlorotUniform()\nlayers = [Conv(fshape=(1, 3, self.nb_channels * 2), init=init, activation=Rectlin()), \\\n           Pooling(op='max', fshape={'str_h' : 1, 'str_w' : 2}), \\\n           Affine(nout=self.nb_classes, init=init, activation=Logistic())]\nself.cost = GeneralizedCost(costfunc = CrossEntropyMulti())\nself.optimizer = Adadelta(decay=0.9, epsilon=1e-10)\nself.model = Model(layers=layers, optimizer=self.optimizer)\n```\nI've printed some infos during each batch run:\nCPUTensor(base 0x7f49ad4e9f80) name:None shape:(2048, 128) dtype: strides:(512, 4) is_c_contiguous:True\nbatch: (2048, 128)\nfprop in 2.05480408669\nbprop in 10.1289958954\noptimize in 0.0104749202728\nEpoch 0   [Train |                    |    1/5909 batches, 0.03 cost, 12.34s]CPUTensor(base 0x7f49ad4e9f80) name:None shape:(2048, 128) dtype: strides:(512, 4) is_c_contiguous:True\nbatch: (2048, 128)\nfprop in 2.08032488823\nbprop in 9.82120990753\noptimize in 0.0126368999481\nEpoch 0   [Train |                    |    2/5909 batches, 0.00 cost, 24.30s]CPUTensor(base 0x7f49ad4e9f80) name:None shape:(2048, 128) dtype: strides:(512, 4) is_c_contiguous:True\nbatch: (2048, 128)\nfprop in 2.13239097595\nbprop in 9.75532984734\noptimize in 0.0100581645966\n. Thank you for the code, @scott-gray.\nI have started with neon two days ago, I am not aware about the right places to modify, so I will wait... Do you know how much 'soon' the change would occur? days, weeks or months?\nIf I may contributte a little, I think that the bprop phase must also be reviewed, since it took about 10s. I hope the problems are related.\nThanks!\n. @linuxthink, I think that's ok...\nHere's the output:\nlapack_opt_info:\nlibraries = ['mkl_lapack95_lp64', 'mkl_intel_lp64', 'mkl_intel_thread', 'mkl_core', 'iomp5', 'pthread']\nlibrary_dirs = ['/home/fcf/anaconda/lib']\ndefine_macros = [('SCIPY_MKL_H', None)]\ninclude_dirs = ['/home/fcf/anaconda/include']\n    blas_opt_info:\nlibraries = ['mkl_intel_lp64', 'mkl_intel_thread', 'mkl_core', 'iomp5', 'pthread']\nlibrary_dirs = ['/home/fcf/anaconda/lib']\ndefine_macros = [('SCIPY_MKL_H', None)]\ninclude_dirs = ['/home/fcf/anaconda/include']\n    openblas_lapack_info:\n  NOT AVAILABLE\n    lapack_mkl_info:\nlibraries = ['mkl_lapack95_lp64', 'mkl_intel_lp64', 'mkl_intel_thread', 'mkl_core', 'iomp5', 'pthread']\nlibrary_dirs = ['/home/fcf/anaconda/lib']\ndefine_macros = [('SCIPY_MKL_H', None)]\ninclude_dirs = ['/home/fcf/anaconda/include']\n    blas_mkl_info:\nlibraries = ['mkl_intel_lp64', 'mkl_intel_thread', 'mkl_core', 'iomp5', 'pthread']\nlibrary_dirs = ['/home/fcf/anaconda/lib']\ndefine_macros = [('SCIPY_MKL_H', None)]\ninclude_dirs = ['/home/fcf/anaconda/include']\n    mkl_info:\nlibraries = ['mkl_intel_lp64', 'mkl_intel_thread', 'mkl_core', 'iomp5', 'pthread']\nlibrary_dirs = ['/home/fcf/anaconda/lib']\ndefine_macros = [('SCIPY_MKL_H', None)]\ninclude_dirs = ['/home/fcf/anaconda/include']\n. ",
    "yxlao": "Hi @felipefariax, it might also be helpful to double check that the numpy BLAS / ATLAS is properly configured, since sometimes virtualenv does not pick up these libraries. \nimport numpy as np\nnp.__config__.show()\n. Thanks for spotting this. Issue confirmed. It's due to the fact the gradient for x.T is not accumulated correctly to the gradient for x. Will push a fix soon.\n. This behavior is expected. The linearity of the gradient operator applies to element-wise operations. However, min, max and sum are reduction operations. In max(x) + x for example, the value of max(x) (scalar) gets broadcast to the subsequent operations.\nYou could checkout the following scripts to test against numerical gradients:\n```\nimport numpy as np\ndef get_numerical_gradient(f, tensors, delta=1e-5):\n    # buffer for gradients\n    gradients = []\n    for i in range(len(tensors)):\n        tensors[i] = tensors[i].astype(np.float64)\n        gradients.append(np.zeros(tensors[i].shape))\n# iterate through tensor\nfor tensor, gradient in zip(tensors, gradients):\n\n    tensor_flat = tensor.reshape((-1, ))\n    gradient_flat = gradient.reshape((-1, ))\n\n    # iterate through each element\n    for idx in range(len(tensor_flat)):\n        # backup\n        backup = tensor_flat[idx]\n        # increment\n        tensor_flat[idx] = tensor_flat[idx] + delta\n        f_inc = np.sum(f(*tensors))\n        # decrement\n        tensor_flat[idx] = backup - delta\n        f_dec = np.sum(f(*tensors))\n        # recover\n        tensor_flat[idx] = backup\n        # gradient\n        gradient_flat[idx] = (f_inc - f_dec) / (2.0 * delta)\n\nreturn gradients\n\nprint get_numerical_gradient(lambda x: np.sum(np.max(x) + x), \n                             [np.array([2., 3.])], \n                             delta=1e-5)\nThe result is [array([ 1.,  3.])]\n```\nOr, we could also use HIPS's autograd to verify the result:\n```\nimport autograd.numpy as np\nfrom autograd import grad\nfunc_grad = grad(lambda x: np.sum(np.max(x) + x))\nprint func_grad(np.array([2., 3.]))  # [ 1.  3.]\nfunc_grad = grad(lambda x: np.sum(np.max(x)))\nprint func_grad(np.array([2., 3.]))  # [ 0.  1.]\nfunc_grad = grad(lambda x: np.sum(x))\nprint func_grad(np.array([2., 3.]))  # [ 1.  1.]\n```\nIn the above examples, applying sum to the final results is equivalent to setting next_error to a matrix of ones in neon. Also notes that autodiff of slicing operations is not supported in neon yet and we plan to add that in the future.\n. Try changing next_error's shape to the shape of the op_tree or set it to None. The shape ofnext_error shall be consistent to the output rather than the shape of x. \n```\nIn [13]: x = ng.array(np.array([[2, 3]]))\nIn [14]: def grad(e): return Autodiff(op_tree=e, be=ng, next_error=ng.ones(e.shape)).get_grad_asnumpyarray([x])\nIn [15]: grad(ng.max(x))\nOut[15]: [array([[ 0.,  1.]], dtype=float32)]\nIn [16]: grad(ng.max(x)+0.)\nOut[16]: [array([[ 0.,  1.]], dtype=float32)]\n```\nA check will be added to ensure the shape consistence.\n. Yeah, but I guess in that case it might be more reasonable to do the broadcasting manually. next_error is typically the back-propagated gradients from the final cost to an intermediate output (such as a layer's activation), so the shape of next_error shall be consistent to the shape of that intermediate output. \nIf the output shape needs broadcasting to match the shape of the next_error, we could just define another \"broadcasted output\" that is the same shape as the next_error.\n. ",
    "suixudongi8": "Thanks for your hard work;)\n. ",
    "nervetumer": "You need to set up a backend before setting up the image loader.  In our examples this is done behind the scenes in the NeonArgparser based on the command line options given (or the defaults).  \nTry:\nfrom neon.backends import gen_backend\ngen_backend(backend='cpu', batch_size=128)\nNote there are many backend options please see the docs.  For examples, if you have a compatible GPU you can make a GPU backend by changing backend='cpu' to backend='gpu'.\n. Have you had a chance to try the backend generation?  If so, can i close this issue?\n. Are you setting the epochs correctly.  If you're provious sessions went for 10 epochs, and you want to train for 10 more epochs, you'll need to set the number of epochs to be 20.  So on the command line put -e 20.\n. Do you have access to the serialized model files for epoch 6 and 7?  If so, we can directly check to see if the weights are going to zero.  The weights are saved via pickle so I don't know how that would truncate on serialization.  Maybe there's an issue with another part of the model initialization from a serialized file.  \nIs 7a56fa9 a working commit (meaning linked to the lower plot)?  Which commit is the other plot run on?\nAre you serializing and deserializing every epoch?  Can we maybe add a logging line to print the average Q-value for each iteration (mini-batch) between the epochs so see if this is sudden of slowly developing over the course of an epoch?\nThanks\n. I'm looking into the serialization issue.\n. Thanks for noting this issue with the test.  It is not a serialization issue but actually has to to with a buffer mismatch that happens when the the second instance of the Model object is generated.  Since the layers passed to Model (i.e. Model(layers=layers)) have already been instantiated, they do not get new buffers but a new shared buffer is generated for the MergeMultistream layer container.  So when the model is run the second time the layers in MergeMultistream output to the buffer used the first time around but the MergeMultistream  layer container is expecting those outputs in a new location.  This causes the outputs from the MergeMultistream layer on the second run to remain 0.\nThat said, we will try to push a fix for this.  In the meantime, I'm wondering if you can try it out to see if this is the root of your problem as well - if your model touch on the affected code then maybe this is your issue as well.\nThe code to try is here:\nhttps://gist.github.com/nervetumer/7a220708967b88cbdcf4\nSo this should only affect container layers that inherit from the MergeBroadcast class.\n. Sorry that was the wrong file.  The one i meant to give you layers/containter.py :\nhttps://gist.github.com/nervetumer/7c538802bad3f7a60576\n. Hi @tambetm,  Have you had a chance to test #171 with this?  I'm hoping to get some time to work on this soon.\n. Hi @tambetm  can you speak to @yinyinl comment above with respect to isolating the commit that caused this.  Is it for sure that 85a478e caused the break?\n. I think i tracked down the commit that is causing your problem.  I think it is https://github.com/NervanaSystems/neon/commit/adab385dace2ef75f58852157b55788cb56b8647.  That said I have no idea why this would affect you at all.  Will need to look at it closely.\n. If you can confirm that that is the commit causing the issue, that would help.  I'm looking at the commit and working with people here to see why it may be causing you an issue.\n. @tambetm.  I think I have verification that this is the commit that led to your model breaking. I believe the problem may be coming from line 143-145 of neon/layers/layer.py. I'm testing it now and will report as soon as enough epochs run to have some confidence.   Still not sure why this would cause a problem.\n. Below is the Q plot for the commit SHA adab385\n\nIf I comment out lines 143 and 144 in neon/layers/container.py\nif self.next_layer is None or self.next_layer.parallelism != self.paral\n            self.owns_delta = True\nI get a good run:\n\nWhat is happening is that this code is switching the self.owns_delta from False to True for the Linear layers in your model.   Still not exactly sure why this is causing a strange, random error like this.  We will try to put a fit for this asap.  If you want comment those lines out for now.\n. We have a PR staged to fix this.\n. Also can you provide more information on ary.  For example, ary.flags and ary.strides\n. The output sizes of conv and pooling layers in neon and caffe are computed differently.  Setting the caffe flag will make neon use the caffe formula for setting the output size of a conv and pooling layer.  The code that shows this difference is located in the class \"Backend\" under the method named \"output_dim\".  https://github.com/NervanaSystems/neon/blob/master/neon/backends/backend.py\n. Some speed optimizations can lead to non-deterministic behavior.  When running on GPU, this model uses a convolution and batchnorm implementations that require atomic addition operations.  The non-determinisim here arises from the numerical differences caused by running these operations in a different order.  We are going to push a change soon that will make the convolution operations not use atomics by default.  We are looking into a similar change for the batchnorm operations.\n. For the convolution layer you can set the \"deterministic_update\" parameter in the nervanagpu.py file to True.  For batch norm it may be a little more complicated and the solution is not tested yet.  You can try to set the sum parameter sum to be False.  For macro layers like Conv and Affine, you'll need to keep batch_norm active but not have it pass bsum=True to the Convolution or Linear layer.\n. For this example the issue is due to partial mini batches.  You can see in the progress bar output above that the number of mini batches goes from 469 down to 468 every 4 epochs.  This is because the training set size is not an exact multiple of the batch size.  For more info on this checkout the neon FAQ   When you save at epoch 9 and restart from that saved file, you are restarting from the beginning of the training set and, therefore, the sequence of images will be different for that run compared to the full run.  \nWe have been thinking about moving away from epochs to iteration (mini-batch) based counting for the progression of training.  That may be an appropriate place to address this issue .  We may also be able to zero pad the last mini-batch of each epoch to avoid this source of non-determinisim.  If you limit the training data set to be an exact multiple of the batch size, this issue should go away.\n. @tambetm which SHA of neon are you getting this problem with?  Also, any chance you know which commit sha the file was created on?\n. @tambetm, it looks like your files are from a very old version of neon which was deprecated and then with the new serialization changes in January, was removed.  If you run the script below, it should update your files to the currently deprecated serialization, then you should be able to create current files by loading the weights and serializing the model.\n```\nimport pickle\nwith open('breakout_77.pkl', 'r') as fid:\n    model_param = pickle.load(fid)\nfor lay_param in model_param['layer_params_states']:\n    w = lay_param['params']\n    lay_param['params'] = {'W': w}\nwith open('breakout_77_new.pkl', 'w') as fid:\n    pickle.dump(model_param, fid)\n```\nI'm getting a core dump when I try to run  ./play.sh  snapshots/breakout_77.pkl \nso I could not test this.\n. We had some changes coming that would make the offending lines unnecessary.  I'll check to see if we can remove that lines so we can retest your model.\n. Can you post the model code that causes this failure.  I added an LRN layer after the first pooling layer to and it ran without the error above.\n. Also please sync to the current tip of the repo master branch and try to run again.\n. I am unable to replicate this error.  Some code must be missing because you need to generate a backend first.  But the error you printed is not the error that occurs if a backend is not there.  I ran this code below and got no error:\nfrom neon.backends import gen_backend\nbe=gen_backend(backend='gpu', batch_size=32)\nfrom neon.data import ArrayIterator\nimport numpy as np\nX = np.random.rand(1000, 1)\ny = 2*X + 1 + 0.01*np.random.randn(1000, 1) # y = 2X+1 with some gaussian noise\ntrain = ArrayIterator(X=X, y=y, make_onehot=False)\nas well as \nfrom neon.backends import gen_backend\nbe=gen_backend(backend='cpu')\nfrom neon.data import ArrayIterator\nimport numpy as np\nX = np.random.rand(1000, 1)\ny = 2*X + 1 + 0.01*np.random.randn(1000, 1) # y = 2X+1 with some gaussian noise\ntrain = ArrayIterator(X=X, y=y, make_onehot=False)\nThe code above was run with version 1.7.0. for GPU you need to set the batch_size as i did in the code i copied above.  please use the exact code i used:\n\"be=gen_backend(backend='gpu', batch_size=32)\"\nNOT\nbe=gen_backend(backend='cpu')\nThe line without batch_size is for the CPU backend. ",
    "leovetter": "Yes it's working. Thanks for the help.\n. ",
    "eracah": "Thanks for the quick response. Currently trying out something similar in architecture to the conv-ae example. I'll let you know how it goes.\n. Urs,\nI am making a convolutional autoencoder, where I have alternating conv and maxpool layers connected to a few FC layers and then those FC layers are upsampled with deconv layers. I wanted to have the FC layers, so I could have a learned feature vector that I could analyze.\nGreat point about the 1x1xK feature map being equivalent to the output of connecting the last maxpool feature map to a FC layer of size K. I will implement that. \nThank you for your help,\nEvan\n. ",
    "ankitp94": "@eracah I would also like to see how to use unpooling instead of strided deconvolution ? Please do share your results and implementation once you're done. Thanks! \n. Hi @hanlin-nervana \nThanks for the feedback. \nWhat you say makes a lot of sense. \nI was not implementing LeNet for its performance but for its historical significance.\nWithout partial connections, the implementation is very easy and straight-forward with neon (Great work by the team! Cheers).\nI want to try/propose new type of architectures  for academic research purposes and that would require flexibility in terms of  layers connections (example Semi-Supervised Learning with Ladder Networks ) , different cost functions (Bridge Correlational Neural Networks for Multilingual Multimodal Representation Learning uses correlation of minibatch in cost ) etc.\nWould you suggest using a different library for trying new flexible architectures ? \n. ",
    "etienne87": "@eracah +1, i would have thought unpooling to more effective? anyway, isn't there a way to do it using just the autodiff of the pooling layer (using last convolutional maps as inputs) ?\n. Hello! \nThanks a lot for your reactivity! 3 minutes whaa :+1: \nThis is just what i did! It builds fine now! can't wait to try your lib\n. ",
    "mwoodson1": "Ah yes I didn't take that into account. Works now, thanks!\n. Any update on this? I was thinking of re-writing my old code for sequence data input to be more general but if something is already in the works I can hold off.\n. Thanks for the quick response. I am not exactly creating an autoencoder so reconstruction is not the objective. Since would like the output to simply be a 72x72 grayscale image, changing my code to \nAffine(72*72, init=Gaussian(scale=0.03), bias=None, activation=Rectlin())\nwould mean I would have to change my ArrayIterator initialization correct?\nFor reference here is what it is now:\ntrain_set = ArrayIterator(X_train, y_train, nclass=y_train.shape[1], make_onehot=False, lshape=(3,72,72))\nwhich was taken from #145 .\n. Ah sorry I think I should clarify. I would still like the input to be RGB but my output would be grayscale(my output is not neccesarily an image but a 2D array that can be considered an image). The idea is I would like to learn a single filter but from what I have seen neon does not allow a Conv layer with only 1 filter channel. Thus I changed it to an Affine layer with size (72,72) as that was the filter dimension I desired.\nI have now changed the Affine layer to have nout=72*72  and my nclass=72*72 in my ArrayIterator. Reshaping the output of the Affine layer to 72x72 will be exactly what I desire correct?\n. Thank you very much for your support! \n. You can take a look at mnist_merge.py and mnist_branch.py in the examples provided. You can have 3 pathways that get merged and then split into multple branches with one for each output you desire. While this may work I can't guarantee how well it will work for what you want to do.\n. Take a look at the ArrayIterator method if your curious but at a high level it converts your train labels to one-hot vectors on the fly. So the (6000,) vector you see are the actual labels(0-9) which get converted by the following call:\ntrain_set = ArrayIterator(X_train, y_train, nclass=nclass, lshape=(1, 28, 28))\nThis will convert your training labels into vectors of size nclass with your input dimensions being (1,28,28). Hope that helps.\n. To your first question adding nclass will not change the shape of y_train. All you did was shift the labels from 0-9 to 10-19. This is still essentially the same problem.\nYou're one hot code looks correct. Without looking at the internal code I can only assume the small difference in performance is due to how batching is handled between DataIterator and ArrayIterator. If you are unsatisfied you could take a look at those two methods to see what might be the cause but such a small performance difference shouldn't be of concern.\n. ",
    "Bam4d": "Yes, the tree LSTM is on the backburner for me at the moment, so don't want to commit it just yet.\nI will attach a unit in due course\n. Closing due to history issues, will re-open\n. Need to deal with the history on this branch...\n. closing due to history issues\n. ",
    "DavidWiesner": "Thx. It was indeed the wrong version of CUDA. I had CUDA 6.5 installed. \n. ",
    "DavidYoungSoton": "Many thanks.\nDavid\nOn 14/01/2016 02:07, Scott Leishman wrote:\n\nHi David,\nThanks for pointing this out. We'll be pushing some updated\ndocumentation shortly which should help address this. Note that the\nproblem only occurs if you attempt to fit a model after first trying to\ndo an eval (just fitting, just eval, fit then eval are all ok).\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/151#issuecomment-171501443.Web\nBug from\nhttps://github.com/notifications/beacon/ALiY8WZVRQwgUTGRfjUQhsdbHgSep42yks5pZvpYgaJpZM4Gvh5i.gif\n. \n",
    "shyamalschandra": "Can you include something like a embedded code example with comments (https://trinket.io) with paper references (similiar to theanets)?\n. @anlthms : Could you tell the exact steps to apply this patch?  Thanks!\n. ",
    "guiambros": "I guess you meant neon/layers/layer.py, no?\nI'm trying to reproduce the error here with adab385d, but no luck yet (but likely my fault). Starting again from scratch; will leave it running overnight and report back tomorrow.\n. ",
    "jennifermyers": "This should be fixed now in 1.5.1.\n. Closing this as we have been unable to reproduce. Please reopen if it happens again.\n. Thank you @Jokeren for the followup!\n. A fix for this will be part of the imminent v1.5.0 neon release.\n. A fix for this will be part of the imminent v1.5.0 neon release.\n. Python3 support will be part of the imminent neon 1.5.0 release.\n. Hi @twangcat, I'm going to close this old issue as we haven't been able to reproduce it. Please reopen and provide your model code if you are able to reproduce it on current tip of master. Thanks!\n. @buriy, I'm going to close this for now. Please reopen if you still see issues with v1.5.1. Thanks!\n. Closing this issue as reshape layer is now available in neon 1.6.0.\n. Thank you David! This change was merged to neon as of version 1.4.\n. Closing this - let us know if you have more questions.\n. Hi @mingwugmail - I'm going to close this issue. Please let us know if you have any problems with the suggestions Hanlin outlined!\n. Hi @szeitlin, I'm going to close this old issue assuming that Hanlin's suggestion was able to get you past the issue with the MNIST tutorial. Please feel free to reopen if you are still having a problem.\n. The fix is lined up for v1.5.0 release.\n. Thanks for reporting this! This will be fixed in the documentation for our imminent v1.5.0 release.\n. Thanks for the report! This is addressed in the neon 1.5.1 release.\n. Hi @glebreutov,\nPlease try building with the CUDA_ROOT set, that is:\nmake CUDA_ROOT=/usr/local/cuda\nThen also make sure to include the CUDA directories in your PATH and DYLD_LIBRARY_PATH environment variables as follows:\nexport PATH=/usr/local/cuda/bin:$PATH\nexport DYLD_LIBRARY_PATH=/usr/local/cuda/lib:$DYLD_LIBRARY_PATH\n. A fix to the Makefile will also be made in the next release so the setting of CUDA_ROOT is not necessary.\n. Hi Hugh,\nDo you mean that there is both http://github.com/nervanasystems/neon and https://github.com/nervanasys/neon ? \nIf so, the latter is not the official one and I can talk to the owner (Arjun) about whether to rename or remove it.\nThanks!\n. OK, great! Arjun has removed his fork of neon, so now we just have http://github.com/nervanasystems/neon :-)\n. This may be an issue with OpenCV - see https://github.com/opencv/opencv/issues/5852\nCan you retry with a more recent version of OpenCV and neon (1.5.3 is latest).\n. Hi Hugh,\nThanks for submitting this pull request! \nOur next release (neon 1.5) will be releasing very soon with Python3 support, so I am going to close this pull request for now. \n. Hi Oleg,\nThanks for the note! We have added support for this internally and it is currently in test. We expect to release it in an upcoming version of neon - possibly not the imminent 1.5 release, but likely by 1.6.\n. Good news! This is now expected to be part of the imminent v1.5.0 release.\n. Sorry for the delay in response. It sounds interesting and we'd be very happy to get a PR from you.\n. Hi @utkarshsimha - absolutely! :-) . Thanks @BaxterEaves, we'll get this in the next release.\n. It's in 1.5.4\n. Hi ticoneva, \nThat is surprising. I am not aware of a make implementation that doesn't look for a file named Makefile by default.\nCan you tell me what OS you are on and what version of make (make -v)?\nThanks!\nJennifer\n. OK - glad to hear it is working now!\n. Hi Kaixhin,\nWe recently changed HAS_GPU to be automatically set to whether there was a functional GPU in the system rather than just looking for the presence of the CUDA libraries (which people install for debugging purposes on non-GPU systems).\nYou should be able to override this with a manual setting as follows:\nmake HAS_GPU=true\nCan you try that?\nThanks!\nJennifer\n. Thanks for the pointer. Following up on that, it seems likely that you are running into  the same issue as NVIDIA/nvidia-docker#172 which might be addressed by adding /usr/lib/x86_64-linux-gnu to the LD_LIBRARY_PATH. Can you confirm?\n. Hi,\nThanks for the detailed report! It looks like we're not generating the version file for \"make sysinstall\", and we'll work on a fix. In the meantime, you could try making this change to the Makefile and re-running \"make sysinstall\":\ndiff --git a/Makefile b/Makefile\nindex 71c10fa..1600863 100644\n--- a/Makefile\n+++ b/Makefile\n@@ -148,7 +148,7 @@ sysinstall_nodeps: $(DATA_LOADER) neon_install\n sysinstall: sysdeps $(DATA_LOADER) neon_install\n neon_install:\n        @echo \"Installing neon system wide...\"\n-       @pip install .\n+       @python setup.py install\n        @echo\nAlternatively, just run \"python setup.py install\" from the command line.\n. Hi shyamalschandra,\nSorry for the delay in getting back to you.\nDid you install opencv via homebrew (http://brew.sh/), i.e., \"brew install opencv\"? If not, can you try this?\nThanks!\nJennifer\n. @Willib - sorry to hear that you are also encountering this problem. Can you help us to reproduce by providing the install steps you followed for OpenCV?\n. Thanks for the fix! We'll add it as part of neon 1.6.\n. Thanks for the reports on this issue! In the next release, neon 1.6, we'll be bringing all requirements.txt up to date which should address this issue.\n. Closing this issue as fixed in neon 1.6.0.\n. Hi Marcj,\nAccording to https://github.com/Homebrew/legacy-homebrew/issues/43916 it looks like this issue was fixed in Numpy 1.9.3. \nYou should be able to install if you edit the version of numpy in neon/requirements.txt, changing:\nnumpy==1.9.2\nto\nnumpy==1.9.3\nThen rerun make.\nThis should be fixed also once we release neon 1.6, as we will be bringing numpy up-to-date in this release.\n. Closing this issue as fixed in neon 1.6.0.\n. thank you for this PR! We're going to incorporate it as part of neon 1.6.\n. Faster RCNN will be available in 1.6.0.\n. Marking this issue as closed - please take a look at Faster RCNN in 1.6.0.\n. Thanks for the PR! It has been incorporated in neon 1.6.0.\n. Hi @ticoneva - thanks for spotting that! We'll correct the docs in the next release.\n. Thanks for the pull request! This was merged in neon 1.7.0 (released today).. Similar to the suggestion in #328, you might want to try updating to neon 1.7, which no longer has a dependency on ffmpeg.. Thanks for the pull request! neon 1.7.0 (released today) has deprecated the batchwriter in favor of aeon, so this change is no longer needed.. Hi @robi56,\nIn neon 1.6. make sysinstall calls python setup.py install which generates neon/version.py.\nCan you check whether neon/version.py was generated and if not, run python setup.py install?\nThanks,\nJennifer\n. Closing this as cannot reproduce. @photoncanon, please reopen with the exact script you are using if you are still having trouble and we can take a look.. Closing this issue. @koglp let us know if you still have problems after setting batch_size.. closing - merged; thanks for the submission!. This is fixed in 1.8.1, thanks for the report!. This is fixed in 1.8.1 - thanks for the report!. Thanks for the contribution! It is merged in v1.9.0.. Thanks for the contribution! It is merged in v1.9.0.. Thanks for the contribution! It is merged in v1.9.0.. Thanks for the contribution! It is merged in v1.9.0.. thanks for the fix! will be part of neon-2.3. Thanks for the fix - will be part of neon-2.3.. ",
    "hanlin-nervana": "We've added in version 1.5.1 macrobatching support for images, video, and audio. See http://neon.nervanasys.com/docs/latest/loading_data.html#dataloader for the documentation, as well as our examples/video-c3d folder for an example usage.\n. To provision multiple labels to the model, we would recommend implementing a custom iterator that yields for each minibatch a tuple of (X, (Y1, Y2)). The two target labels will be matched against the output branches of your network using the associated costs.\nSee:\nhttps://github.com/NervanaSystems/neon/tree/master/examples/fast-rcnn\nhttps://github.com/NervanaSystems/neon_course/blob/master/03%20Writing%20your%20own%20dataset.ipynb\n. Closing this issue, please reopen if you have further questions, thanks!\n. I've been able to configure neon for anaconda on a Ubuntu 14.04 Kepler instance on AWS with the following:\nCreate cuda environment variables (substitute your own proper cuda path)\nexport PATH=\"/usr/local/cuda/bin:$PATH\"\nexport LD_LIBRARY_PATH=\"/usr/local/cuda/lib64:$LD_LIBRARY_PATH\"\nThen, create a conda environment\nconda create --name neon pip\nActivate the environment, then run a system-wide install\nsource activate neon\ncd ~/path/to/neon\nmake sysinstall\nBecause you are inside a conda environment, sysinstall will install all the files in /path-to-anaconda/envs/neon/bin/python. I was able to run GPU examples without any issues. Let me know if that works!\n. Thanks for your interest! \nIf you are creating custom layers for models in neon, then the approach is straightforward. All objects inherit from NervanaObject which has a static be variable for the backend. \nIf you are making custom implementations outside of our neon framework, we expose the backend exactly for this purpose. You can generate a backend via:\npython\nfrom neon.backends import gen_backend\nbe = gen_backend(backend='gpu')\nYou can specify either a CPU or a GPU backend. The backend API supports many of the standard tensor operations that you would expect. Let us know if you run into any problems or if you need more guidance.\nFor resources, see:\nBackend overview: http://neon.nervanasys.com/docs/latest/backends.html\nBackend API: http://neon.nervanasys.com/docs/latest/ml_operational_layer.html\nOp-Tree: http://neon.nervanasys.com/docs/latest/optree.html\nAuto-diff: http://neon.nervanasys.com/docs/latest/autodiff.html\n. Thanks, we'll fix this. Here is the link to the installation section: http://neon.nervanasys.com/docs/latest/installation.html\n. Hi Jinlong,\nSince the CPU backend wraps numpy, you can install OpenBLAS (see optional packages udner http://neon.nervanasys.com/docs/latest/installation.html) to enable multi-core CPU. You would have to do the following within the neon virtual environment:\n1. Install OpenBLAS (http://www.openblas.net/)\n2. Reinstall and configure numpy for OpenBLAS [edit numpy's site.cfg file and link to your openblas library] \nHere are sample instructions for ubuntu OS: https://hunseblog.wordpress.com/2014/09/15/installing-numpy-and-openblas/\n. Hi Jinlong,\nWe would welcome contributions to the CPU backend. Right now we have optimized C code, but primarily for ingesting datasets and keeping the GPU fed with data.\nThanks! \nHanlin\n. Layer buffers in neon are pre-allocated during model creation for a particular batch size. To run prediction on samples < be.bsz, you could either regenerate the model with a new batch size or just pad your M x samples matrix into a larger M x be.bsz matrix where you can ignore the padded entries. Where prediction speed is not critical, the latter approach is easier.\n. Happy to help. Can you edit the makefile to echo $CUDA_ROOT and rerun the make sysinstall command? Thanks!\n. We can certainly add some input checking to the ArrayIterator. Thanks for your suggestion. For classification, ArrayIterator expects integer labels from (0, 1, ... K-1), where K is the number of classes. \nMore information on what ArrayIterator expects and one-hot encoding can be found here: http://neon.nervanasys.com/docs/latest/loading_data.html.\n. Additional input/output shape checks for the ArrayIterator are now in neon v1.5.1. Thanks for your suggestion.\n. Hello, that repository is no longer maintained. Please see https://github.com/NervanaSystems/neon_course for notebooks that are updated to use neon v1.8.. If which nvcc returns empty, you may need to add cuda to your path. Depending on your cuda install location, something like this:\nexport PATH=\"/usr/local/cuda/bin:\"$PATH\nexport LD_LIBRARY_PATH=\"/usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib:\"$LD_LIBRARY_PATH\n. Hello,\nIn that case, the -v verbose flag should meet your needs, as it prints the network layout right before training. Alternatively, you can first initialize the model by calling mlp.initialize(train_set, cost), which will configure all the layer shapes. Then, you can inspect the network via print mlp.\nHanlin\n. Hello,\nThat error arises either because the virtual environment was not activated, or neon/version.py doesn't exist. That file is created as part of the makefile install. neon isn't a pure python package, and the makefile is required to compile our GPU kernels and our C++ dataloading library.\nCan you try a clean virtualenv install by cloning neon, and then issuing make from the neon directory? You can test the install by first activating the virtual environment and then running the MNIST example. Here is the sequence of commands:\ngit clone https://www.github.com/NervanaSystems/neon.git\ncd neon\nmake\n. .venv/bin/activate\n./examples/mnist_mlp.py\nThanks!\n. Good point, we should add some python version checking to the virtual env install. Thanks for your suggestion.\nTo install within an existing venv, activate the virtual env, then run make sysinstall. Neon will install the dependencies in your virtual environment\u2019s python folder. \n. OK to help diagnose the issue, can you modify the makefile to install a python2.7 venv (or use a sysinstall inside a py2.7 virtualenv), and then share the install logs and the stack trace when you run ./examples/mnist_mlp.py?\n. Great that you were able to get the install working, although if you have the stack trace and install log for the failed sysinstall to share, that would be helpful for us to diagnose.\nI was unable to reproduce your error with the MNIST tutorial. That error occurs if the backend has not been initialized yet. The backend is generated at this line: args = parser.parse_args() in the tutorial (preamble section). After executing that line, you can check that the backend was properly generated via:\nfrom neon import NervanaObject\nNervanaObject.be\nYou should see [for CPU]: <neon.backends.nervanacpu.NervanaCPU at 0x1061e3ed0>\n. Implementing arbitrary partial connections within neon can be quite tricky and not recommended. From the LeNet paper, they motivate their proposed approach via:\n\nWhy not connect every S3 feature map to every C2 feature map? The reason is twofold. First, a noncomplete connection scheme keeps the number of connections within reasonable bounds\nMore importantly, it forces a break of symmetry in the network. Different feature maps are forced to extract different hopefully complementary, features because they get different sets of inputs.\n\nModern hardware can run efficiently on the fully connected network, and the second rationale (symmetry-breaking) can be achieved with random weight initialization and/or dropout layers. I would recommend first trying a LeNet implementation without worrying about the partial connections.\n. neon exposes a backend API of numpy-like calls that can be used to define custom layers/costs relatively easily, so I would certainly recommend you give the library a try. We also have containers to build flexible tree/merge architectures. Please let us know if you run into any issues.\n. Hello,\nThe error you report may be an issue with pip itself:\nSee: \nhttps://github.com/pypa/pip/issues/3282\nhttps://github.com/pypa/pip/pull/3287\nBoth report the same stack trace error. Can you try the fix mentioned in the last post in https://github.com/pypa/pip/issues/3282? Unfortunately we don't have your exact system setup so cannot attempt to reproduce your error and diagnose. Thanks!\n. There is an out-of-memory error because new buffers are being allocated at every for loop. We would not recommend this approach. \nSupporting dynamically generated data through an API is not on our near-term roadmap. Writing your custom dataset iterator however, is fairly straightforward. For a tutorial, please see https://github.com/NervanaSystems/neon_course/blob/master/03%20Writing%20your%20own%20dataset.ipynb\n. Hello,\nSupplying a tuple instead of a tensor is used with multiple input/outputs. We will amend the API to reflect this. Thanks for bringing this to our attention.\nIf your data is being generated dynamically for each minibatch, then I'm afraid the only route is to write a custom dataset object that will load the generated data into the GPU and provision those buffers to the model. We've developed a notebook here to guide users through this process.\nI am closing this issue for now. Feel free to reopen if you have issues implementing the above.\n. Hello,\nWe are having some trouble reproducing your issue. Can you try building a fresh copy of neon and point it at a different data directory with the -w flag to force a fresh download of the dataset? Thanks!\n. From the output, looks like your opencv install was not configured with WITHCUDA=ON, leading the makefile not being able to find the cuda-related components of opencv.\nTo disable the loader makefile from compiling with -DHAS_GPU, comment out lines 46-59\nin https://github.com/NervanaSystems/neon/blob/master/loader/Makefile\nAlternatively, to skip the loader compilation completely (only necessary for macrobatching loading),  comment out lines 29-40 in the file above.\n. Revisiting this issue, opencv with Cuda support is actually not required for neon to operate. We try to link with all the libraries specified in opencv.pc, so perhaps your opencv.pc contained references to the cuda libs that aren't really installed. On Ubuntu 14.04, the normal package install route for opencv is compatible with neon. \nIn the upcoming release, opencv will now be an optional install that is only needed to enable neon's dataloader capabilities.\n. Can you provide more information on the command you used and the error message you received? Thanks!\n. There is likely something wrong with your GPU setup, as we are unable to reproduce your error. What GPU do you have and what operating system are you using?\n. Closing this as a duplicate of #269 \n. Thanks for the catch, the doc links are back up now. \n. Thanks, we fixed the link! Let us know if you run into any other issues.\n. To access the activation of any hidden layer in the model, call model.layers. In our mnist_mlp.py example:\nipdb> mlp.layers.layers\n[<neon.layers.layer.Linear object at 0x7f25e39f5110>, <neon.layers.layer.Activation object at 0x7f25e39f5150>, <neon.layers.layer.Linear object at 0x7f25e39f51d0>, <neon.layers.layer.Activation object at 0x7f25e39f5210>]\nThe above call returns a list of layers of the model. Each layer has the outputs attribute which contains the activations. For example, mlp.layers.layers[0].outputs returns a GPUTensor with the activation of the first hidden linear layer (pre-ReLu).\nipdb> mlp.layers.layers[0].outputs\nGPUTensor(base 0x902c89e00) name:None shape:(100, 128) dtype:float32 strides:(128, 1) is_trans:False is_contiguous:True\n. That should work. Please let us know if you run into any issues.\n. The Array initializer has the attribute val, and not vals. There is a typo in our API documentation that we will fix in the next release. Thanks for catching this.\n. When ImageParams uses the settings shape = dict(channel_count=3, height=112, width=112, scale_min=128, scale_max=128), the data loader will first scale the frame such that the short side is 128 pixels, and then take a center crop of 122x122. Try implementing the scaling operation in your generation of the frame_array.\n. Support for reservoir computing is not on our roadmap but we certainly welcome external contributions.\n. Windows support is not high on a priority list right now. We do have an install procedure, but it is currently unsupported:\nhttps://github.com/NervanaSystems/neon/wiki/Windows-install-instructions-(Unsupported)\n. If I understand correctly, you will want to use our MergeMultistream container to stack features along the non-batch dimension. See our mnist_merge.py for example usage. \nTo pass the image input directly, you will have to patch our SkipNode layer slightly:\ndiff --git a/neon/layers/layer.py b/neon/layers/layer.py\nindex cb131d1..9bd6561 100644\n--- a/neon/layers/layer.py\n+++ b/neon/layers/layer.py\n@@ -413,7 +413,9 @@ class SkipNode(Layer):\n         Returns:\n             Tensor: deltas to propagate to the adjacent lower layer\n         \"\"\"\n-        self.deltas[:] = self.deltas * beta + alpha * error\n+        if self.deltas is not None:\n+            self.deltas[:] = self.deltas * beta + alpha * error\n+\n         return self.deltas\nThen, you can use the SkipNode layer in this way:\n```\npath1 = Sequential(layers=[SkipNode()])\npath2 = Sequential(layers=[Conv(**params)])\nlayers = [MergeMultistream(layers=[path1, path2], merge=\"stack\"),\n               Affine(nout=500),\n               Affine(nout=10)]\n```\nPlease let us know if this solution works for you.\n. In general, we can install on OSX with GPU by:\n1. installing CUDA\n2. installing the required packages from this list using homebrew.\n3. for the dataloader, installing opencv and ffmpeg using homebrew.\nPlease let us know any issues you are encountering so we can help!\n. I am not familiar with spyder IDE, but likely there is a virtualenv setting needed somewhere (or sypder need to be installed in and run from your virtualenv). These may be relevant: \nhttp://stackoverflow.com/questions/28190500/virtualenv-ipython-in-spyder-not-working\nhttp://stackoverflow.com/questions/30170468/how-to-run-spyder-in-virtual-environment\n. I've just uploaded a model file from one of our training runs here: https://s3-us-west-1.amazonaws.com/nervana-modelzoo/Faster_RCNN/frcn_pascalvoc.prm\nThis model was obtained by training the Faster-RCNN model for 7 epochs on the PASCALVOC dataset. To test the model, use the inference.py script:\npython inference.py --model_file frcn_pascalvoc.prm -w <path/to/data/folder>\nThis particular model should obtain an MAP of 69.2% (similar to the pycaffe implementation)\n. Using fixed 1000x1000 images our model is ~25% faster compared to the pycaffe implementation (370ms/iteration vs. 465ms) on Titan X GPUs. \n. You can use the -H flag to store model history. For example the command -s mymodel.p -H 60 will retain the 60 most recent model files, saved as mymodel_0.p, mymodel_1.p, and so forth.\n. We have a caffe2neon utility (https://github.com/NervanaSystems/caffe2neon) for converting caffe models using the protobuf formatted Caffe model definition file and a binary protobuf model parameter file. For more information, see the instructions in that repository.\n. Hello,\nCan  you share the model file or script in question? I was unable to reproduce this error with neon v1.8.1 and using the mnist_mlp.py example. I appended the following code to the end of the mnist_mlp.py script:\n```\nimport numpy as np\nmlp.save_params('model1.p')\nmlp2 = Model('model1.p')\npreds1 = mlp.get_outputs(valid_set)\npreds2 = mlp2.get_outputs(valid_set)\nprint np.allclose(preds1, preds2)\n```\nI obtained a result True. . We don't have an automatic way of converting python train scripts to yaml. Also note that yaml support can be limited especially for more complex models with aeon-based dataloaders.\nIs there a particular script/model that you would like to convert?. int8 support is not currently in our roadmap.. To freeze a layer, we typically using Multiple Optimizers (e.g. see https://github.com/NervanaSystems/neon/blob/master/examples/faster-rcnn/train.py#L101) to set the learning rate of particular layers to 0.\n. No layers that are frozen still undergo the same bprop computations, just the weights are not updated. If this is absolutely necessary, you could consider having a frozen attribute and writing a new container that controls the allocation and bprop methods appropriately to achieve this optimization.\nIs your use case where you have a large front-end model that is frozen (e.g. VGG) and would like to reduce its memory consumption for the follow-on components?. Thanks. Unfortunately we do not have anything built in for that situation, but we certainly welcome contributions. When training on large datasets across many epochs where the feature-extractor (e.g. VGG) is fixed, you may also consider just generating a new dataset with the features pre-computed to eliminate the fprop compute time and the memory footprint of the front-end. For example, see the YouTube-8M dataset.. That's correct, we support multiple Metric Callbacks being added, but not multiple metric callbacks with the same metric_name. (we use the metric_name to create the hdf5 dataset where we store the results). Each TopKMisclassification metric adds duplicate metric names such as LogLoss, causing the error you report.\nA workaround for now is to modify the offending line to check if the group_name already exists before attempting to create a new dataset in callback_data.. batch_writer and ImageLoader will be removed in future releases. We recommend you switch to our dataloader module, aeon. aeon removes the need for a separate batch_writer script, and also includes more functionality and easier use.. Thanks, we've noticed this internally as well.. Hello,\nYou will first want to use the MergeBroadcast container (see examples/image_caption.py for an example where you have two inputs) to build your model. For the data side, you can either write a custom iterator that yields a tuple of inputs. In your case something like:\nyield (x1, x2, x3), (y1, y2, y3)\nAlternatively, if you pack your data into hdf5 datasets, you may also be able to use the HDF5 iterator, but we haven't really tested this.. Hello @armando-fandango , we have several options. If you are running in terminal, we have several command line options to both serialize the model parameters, but also generate the cost/loss/accuracy telemetry data.\npython mnist_mlp.py --output_file data.hdf5 --save_path mymodel.prm --history 10\nWhen you run the above, the accuracy/cost after every epoch of training will be logged in an HDF format dataset for your subsequent use. The model will also be saved as mymodel.prm, while keeping the last 10 epochs of trained models.\nMechanistically, those callbacks are triggered in this line:\ncallbacks = Callbacks(mlp, eval_set=valid_set, **args.callback_args)\nWhere the relevant command line arguments are passed in. If you running an ipython notebook, you can achieve the same effect by passing in the arguments directly. For example:\ncallbacks = Callbacks(mlp, eval_set=valid_set, output_file='data.hdf5', save_path='mymodel.prm', history=10, metric='Accuracy')\nFor a full list of accepted callback arguments, please see here.. Additionally, if you are running in ipython, and want to see the log output, you will have to increase the logger level via:\nimport logging\nmain_logger = logging.getLogger('neon')\nmain_logger.setLevel(10)\nFor an example of this, see this notebook:\nWe also have some experimental routines for directly plotting the loss during training. See this notebook. . ",
    "willtwr": "Hi,\nI tried to remove one class so that I could use nclass=24, which is a number supported by neon and it works.\nIt seems that ImgMaster only support number of classes that are factor of 8 (when using float16)?\n. ",
    "mingwugmail": "I want to do  regularization by penalizing the magnitude of the weights too.\nBut I didn't find anyway to that in Neon.\nHas this issue be fixed?\n. OK, I saw there is a make_onehot param, but why there is no  index out of bound error after my hack:\ny_train += nclass\nAnother experiment, I defined my own make_onehot func:\n(X_train, y_train), (X_test, y_test), nclass = load_mnist(path=args.data_dir)\ndef make_onehot(y):\n  one_hots = np.zeros((len(y), nclass))\n  one_hots[np.arange(len(y)), y] = 1\n  return one_hots\ny_train = make_onehot(y_train)\ny_test  = make_onehot(y_test )\ntrain_set = DataIterator(X_train, y_train, make_onehot=False)\nvalid_set = DataIterator(X_test, y_test, make_onehot=False)\nIs this equivalent to what Neon does internally?\nWhen I run the code, it gives slightly bigger \nMisclassification error = 2.8%\nMisclassification error = 2.9%\nwhile the original code typically gives:\nMisclassification error = 2.5%\nMisclassification error = 2.6%\ndid my code miss something?\n. I tried:\n(X_train, y_train), (X_test, y_test), nclass = load_mnist(path=args.data_dir)\ny_train[0] = nclass * 10\nprint y_train[0]\nnow it has 11 labels [0..9, 100], but it still run thru, didn't give me an index bound error.\n. @mwoodson1 \n\"\"\nYou're one hot code looks correct.\n\"\"\nI just checked the doc of ArrayIterator:\nhttp://neon.nervanasys.com/docs/latest/generated/neon.data.dataiterator.ArrayIterator.html?highlight=arrayiterator\nit says:\ny (ndarray, shape:[# examples, 1], optional) \u2013 Labels corresponding to the input features. If absent, the input features themselves will be returned as target values (AutoEncoder)\nMy question is: does the y.shape ([# examples, 1]) is enforced? or does it matter?\nBecause after my code change of using make_onehot=False, my y.shape is ([# examples, 10]), or [# examples, 11]\nSince Neon didn't check / complain anything, I have no idea if I did something wrong.\n. If I use args.batch_size\nbe = gen_backend(backend=args.backend,\n                 batch_size=args.batch_size,\n                 rng_seed=args.rng_seed,\n                 device_id=args.device_id,\n                 datatype=args.datatype,\n                 stochastic_round=False)\ntest = be.array(X_train[:args.batch_size].T.copy())\npreds = mlp.fprop(test, inference=True)\nprint preds.transpose().get()[0]\nand run with $mnist_mlp.py -b cpu, it worked.\nBut with $mnist_mlp.py -b gpu, it errors out:\n\"neon/neon/backends/nervanagpu.py\", line 904, in _set_rand_state_dev\n    drv.memcpy_htod(rand_state, state)\npycuda._driver.LogicError: cuMemcpyHtoD failed: an illegal memory access was encountered\nSo where are two issues:\n1) why it doesn't work on gpu?\n2) how to call fprop on single data?\n. model have a model.eval(dataset, metric) method, can it also provide a model.predict(data) method?\n. DataIterator is gone in Neon 1.8, how this code should be changed to?\nhttps://github.com/NervanaSystems/meetup/blob/master/cifar_example.ipynb\ninference_set = DataIterator(x_new, None, nclass=nclass, \n                             lshape=(3, 32, 32))\n. as in Torch7: print network layout, and each layer's shape\nnn.Sequential {\n  input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> (9) -> (10) -> (11) -> output: nn.SpatialConvolutionMM(1 -> 32, 5x5)\n  (2): nn.Tanh\n  (3): nn.SpatialMaxPooling(3,3,3,3)\n  (4): nn.SpatialConvolutionMM(32 -> 64, 5x5)\n  (5): nn.Tanh\n  (6): nn.SpatialMaxPooling(2,2,2,2)\n  (7): nn.Reshape(256)\n  (8): nn.Linear(256 -> 200)\n  (9): nn.Tanh\n  (10): nn.Linear(200 -> 10)\n  (11): nn.LogSoftMax\n}\n. Related discussion here:\nhttps://groups.google.com/forum/#!topic/neon-users/hMTRAT4Hf4M\n\ufffc==================================================================\njdh...@colby.edu    \ufffc\ufffcJun 24\n\ufffcHello,\nI am developing a network that is trained using reinforcement learning, and I need to input two streams of data into the Model.fprop, which then get merged after a sequence of layers before the output. At each time step, I have an MxM image and an N dimensional state vector. I need to load them separately, so that the image can be run through a sequence of convolutional layers before being merged with the state vector for the fully connected layers. I have checked the mnist_merge.py and mnist_branch.py examples, but they don't address the issue of loading two different data vectors simultaneously. Is there a way to load data like this?\nThank you,\nJosh\n\ufffc==================================================================\nOn Friday, June 24, 2016 at 5:04:06 PM UTC-7, han...@nervanasys.com wrote:\nHi Josh,\nThanks for your question. Take a look at our image_caption.py example that demonstrates how to use neon's MergeMultistream container for handling the merging of multiple streams of data.\nHanlin\n\ufffc==================================================================\nAre you referring to \nhttps://github.com/NervanaSystems/neon/blob/master/neon/data/imagecaption.py#L217\nprob = model.fprop((x[0], y), inference=True).get()[:, :-self.be.bsz].copy()\nIt feed a tuple to model.fprop. and y is init-ed to:\ny = self.be.zeros(self.dev_X.shape)\nBut in Neon doc: http://neon.nervanasys.com/docs/latest/generated/neon.models.model.Model.html?highlight=model.fprop#neon.models.model.Model.fprop\nit says:\nfprop(x, inference=False)[source]\nForward propagates a minibatch x through the model.\nParameters: \nx (Tensor) \u2013 Input minibatch data\nand if you click Tensor, it refers to neon.backends.backend.Tensor\nI think there are 2 issues here:\n1) the Neon doc need to be more accurate; it clearly not only accept Tensor, but also some tuple type, (maybe also other types?)\n2) all these self.be.zeros, self.be.iobuf backend device types are difficult for library user to master. I guess Neon choose to do it this way for performance reason. But as library user, I don't want to know such implementation detail. I just want to prepare my data as (MxM matrix, N vector) in this particular example, and pass it to model.fit() or model.fprop(). I think Neon should provide a better API in this case (maybe via s thin wrapper which I don't care as a library user).\n. If I do:\nself.model.initialize((self.screen_input_shape[:-1], self.extra_input_shape[:-1]),         self.cost)\n\".../neon/layers/container.py\", line 504, in configure\n    assert hasattr(in_obj, 'shape') and isinstance(in_obj.shape, list)\nAssertionError\n. looks like 375 is ok.. ",
    "jinyixin621": "Any help? Please!\n. ",
    "bhack": ":100: \n. I think that the strategy it is in the roadmap of open sourcing the DNN part of MKL-DNN soon. But I don't know if all the potentially interesting kernels will be developed proprietary behind MKL (no more Nvidia assembly of course) . Will part of the work a near the metal proprietary cudnn alternative for Xeon/Phi? \n. So What will be your Nvidia assembly strategy?\n. Ok but this still doesn't solve maintenance of Nvidia assembly kernels in Neon.\n. ",
    "roshankr": "scttl, thanks for the suggestion. loader.so is available with full permissions, but still having the same issue\nroshan@roshan-810-230qe:~/neon/neon/data/loader$ ls -l\ntotal 1016\n-rwxrwxrwx 1 roshan roshan   3227 Dec 16 09:56 buffer.hpp\n-rwxrwxrwx 1 roshan roshan   2976 Dec 16 09:56 decoder.hpp\n-rwxrwxrwx 1 roshan roshan   5506 Dec 16 09:56 device.hpp\n-rwxrwxrwx 1 roshan roshan 846765 Dec 16 10:19 loader\n-rwxrwxrwx 1 roshan roshan  23889 Dec 16 09:56 loader.cpp\n-rwxrwxrwx 1 roshan roshan 122376 Dec 16 10:19 loader.so\n-rwxrwxrwx 1 roshan roshan   1925 Dec 16 10:19 Makefile\n-rwxrwxrwx 1 roshan roshan   1924 Dec 16 10:12 Makefile~\n-rwxrwxrwx 1 roshan roshan   1053 Dec 16 09:56 matrix.hpp\n-rwxrwxrwx 1 roshan roshan   9398 Dec 16 09:56 reader.hpp\ndrwxrwxrwx 2 roshan roshan   4096 Dec 16 09:56 test\n. there were no directory \"loader\" under /home/roshan/anaconda3/envs/python2/lib/python2.7/site-packages/neon/data ...its missing...If that's the issue , shall i copy it from other directory and make ?\n. I will try this and let you know the outcome...Many thanks\n. Hi scttl, It worked, Many thanks for helping me out this.\n. ",
    "wconstab": "Yea, that's a bug. Thanks for the fix- we'll include it in our next release!\n. Which model were you running? It would be helpful to have the exact command line, and any other info needed to reproduce the issue locally.\n. Were you able to come up with a script to reproduce the issue? Are you still seeing the issue?\n. ",
    "yelite": "ary.flags\nC_CONTIGUOUS : True\n  F_CONTIGUOUS : False\n  OWNDATA : True\n  WRITEABLE : True\n  ALIGNED : True\n  UPDATEIFCOPY : False\nary.strides = (128, 4)\nI'm still working on a minimal script for reproducing this.\n. During the winter break I am away from my GPU... It will be a few weeks before I can touch it again.\n. ",
    "smly": ":+1:\n. ",
    "wei-v-wang": "Thank you @Bam4d It has been quite a while and sorry for the delay. \nCould you please resolve the conflicts? . Thank you for trying to improve the installation process for ubuntu 16.04.\ntools like clang, libsox, libcurl opencv etc are required to install aeon, which is part of neon. So dependencies are expected to be manually installed by the users.\n . We believe this issue is addressed in recent neon releases. Could you please try again? \nThank you! . Please re-open if \"latest\" still has the same issue.. Hi @pedropgusmao We have winograd implementation in https://github.com/NervanaSystems/neon/blob/41e746ad25d5cf51eb8dedc1a67be2bacd639aa9/neon/backends/winograd_conv.py \nhttps://github.com/NervanaSystems/neon/blob/485033cd5e703e70befd82ca8234c97cdf61ab39/neon/backends/winograd5.py \nCan you please check if that fits your needs? Thanks!. cherry-picked for merging this PR. . PR was cherry-picked into neon and will appear in next private/public sync cycle. . @mchrusci thanks for your commits. \n.venv is actually a symbolic link to .venv2 (meaning python2 virtual environment)\nDo you have .venv hidden directory? If yes, there is no need to rename .venv to .venv2 right (since .venv2 exists)? \nlrwxr-xr-x   1 wangwei   wangwei    6B Jul  5 15:34 .venv@ -> .venv2\ndrwxr-xr-x   7 wangwei  wangwei   238B Jul  5 15:34 .venv2/. Errors in cifar10_msra like above disappeared with neon 2.2. Please verify. Thanks, @chengchingwen !. Please re-open if errors occur in latest neon. https://github.com/NervanaSystems/neon/tree/latest . Thanks Peter for your PR. We will get your PR validated, reviewed, and merged. It will show up in our next push of commits to public neon. Thanks for your contribution!. Our new neon2.2 release seems to have fixed this issue while upgrading the cifar10_msra example to use new aeon1.0 version. \nThanks Peter @chengchingwen for reporting and creating the PR for fixing the issue. \nSimilar issues with other examples seem to still exist, will take a separate look. \nFor cifar10_msra, if the errors still are not gone. Please let us know. . Please re-create if latest neon still has this issue. Thanks!. Thank you Milind for trying neon! \nhttp://neon.nervanasys.com/docs/2.0.0/installation.html  indeed shows both virtualenv and pkg-config as pre-requisites. But you have done more to let users know this easily and avoid having to go to the page which the user might not be aware of. Your change is very helpful, thank you! We will make sure we integrate your PR in our next release. . Hi @guoxuesong Thanks for your report! \nThis is a known issue to us and we have been trying to fix it. \nCan you test with this commit id to see if you still have the issue with adding bias? \nhttps://github.com/NervanaSystems/neon/commit/199f215aeac1b699a63cb808ada88ece180c46f0 \n . Hi @guoxuesong We have done fusion of convolution + bias to improve the performance for convolution with bias. Can you please verify if you observe big perf. improvement? \nhttps://github.com/NervanaSystems/neon/tree/v2.3.0  (neon v2.3.0). The reason adding bias was slower was because Bias layer was a non-MKL op and data conversion needed to happen.\nWith neon v2.3.0. Convolution +Bias layers become Convolution_Bias layers. . Please re-open if issues still exist. Thanks for reporting!. @Quallyjiang  Thank you for trying neon! \nYou are right about MKL components not installed when \"make sysinstall\". It it a bug that we will make sure we address it in our next release. \nYou may already know, you can make the following changes to install MKL components as well during sysinstall: \n-sysinstall: sysdeps $(DATA_LOADER) neon_install\n+sysinstall: $(MKL_ENGINE) sysdeps $(DATA_LOADER) neon_install\nSo in conclusion, MKL component is definitely allowed to be installed system-wide. If neon is to be shared by multiple users, the suggestion is to put neon directory on a shared location, the purpose is for all users to be able to locate neon's MKL library.  Please let us know if you encounter further issues. . The way I did was:\n1) \"make sysuninstall\"\n2) Add the $(MKL_ENGINE) target to sysinstall \n3) \"make sysinstall\" \nAlternatively, I think you can also try the following if you do not want to do \"make sysuninstall\": \n./install_mkl.sh  . Thanks @anonymous-u for trying neon. \nMKL is not available for 32-bit systems, so it will not be possible to install neon with MKL support on a 32-bit system.. Theoretically you can remove/disable the MKL component and try installing neon (make sure GPU requirement/dependencies like cuda driver etc are met). \nThis configuration (32 bit + GPU) is less tested, so please let us know if you cannot successfully install neon for testing GPU. . Thanks for reporting this! We will be fixing it in one of the future releases. . N should be the minibatch size according to the API on the LRN document page: \nset_batch_size(N) | Set minibatch size.\n. Thanks @chonbas Do you mind resolving the conflict and update this PR? Thanks!. Hi @jamilbk Please see above comments from @baojun-nervana \nWere you using a very new Perl version? As of now, we cannot reproduce the error with Perl 5.26.  . Ah that makes sense @jamilbk I should have asked about \"mnist_mkl.py\", I thought it was \"mnist_mlp.py\". Closing this PR now, thank you for your efforts @jamilbk @baojun-nervana . @moderato Could you try another MKL version (as found in here: https://github.com/01org/mkl-dnn/releases) e.g. mklml_lnx_2018.0.20170425.tgz? \nThe way you switch to using an older MKL version is 1) delete the existing MKL folder under neon, 2) comment out the prepare_mkl.sh 's \"wget\" and \"tar(unzip)\"  line \n. @moderato Can you please use the MKL library downloaded automatically by neon (when you type \"make\")? You probably need to unset MKL library path so that neon uses the default one under its root directory rather than the one from Intel Parallel Studio XE student edition. It may or may not make a difference, please let me know what you find.  . @moderato From the above, may I suggest you starting from scratch, downloading neon, and by making sure the pre-requisites are satisfied \" python-pip, python-virtualenv, libhdf5-dev, libyaml-dev, pkg-config\" \nand then type \"make clean && make\" ? \nFrom our side, we do not see a need to manually build numpy or scipy, they would be automatically installed in a virtual environment. Our performance does not come from \"MKL-based numpy\" but pure MKL. So please do not worry about building NUMPY/SCIPY based on MKL. I believe \"site.cfg\" was something used by numpy? Please start from scratch, after downloading neon, just do \"make clean && make\", provided you have the aforementioned packages installed.\nFor example, on Ubuntu system, you can do \"apt-get install python-virtualenv\" if python-virtualenv is not already installed.\n . @moderato We have been using numpy 1.11.1 and numpy 1.13 without any problem. However, please note that the numpy is recommended to be in virtualenv. Please see below for the difference: the first \"pip list\" is without virtualenv, and you can see numpy is even not installed. However, by activating virtual environment, \". .venv/bin/activate\", and do another \"pip list\", it says numpy 1.11 is used. If it complains about numpy 1.11, can you please do \"pip uninstall numpy\", I think neon would be able to take care of installing numpy in neon's virtualenv. \nwangwei3@wangwei3-mac01: ~/git/private-neon$ pip list\nDEPRECATION: The default format will switch to columns in the future. You can use --format=(legacy|columns) (or define a format=(legacy|columns) in your pip.conf under the [list] section) to disable this warning.\npip (9.0.1)\nsetuptools (32.1.0)\nvirtualenv (15.1.0)\nwheel (0.29.0)\nwangwei3@wangwei3-mac01:  ~/ git/private-neon$ . .venv2/bin/activate \n(.venv2) wangwei3@wangwei3-mac01: ~/ git/private-neon$ pip list |grep numpy\nDEPRECATION: The default format will switch to columns in the future. You can use --format=(legacy|columns) (or define a format=(legacy|columns) in your pip.conf under the [list] section) to disable this warning.\nnumpy (1.11.1) \n. @moderato I might be wrong, but 0xb = numpy v1.11, and during runtime numpy v1.10 (0xa) is used? \nAnaconda might have added one more layer of confusion. Could you please just use the normal python distribution and not use the python in Anaconda? \nSo far, have you been able to use neon with the mkl backend? Did the \"make\" command eventually work? . Ok, got you. thanks for the feedback @moderato. In future we probably will make sure we test in Anaconda environment as well. \nPlease stay tuned, we are going to have a new release that will hopefully get rid of the errors you see. At that time, you can delete the existing \"0720 mkl\" first and do \"git pull\" and \"make\" again.  . If you can not wait, can you apply the following patch and retry? @moderato \nPlease change prepare_mkl.sh by applying the following changes\ni.e. only the following several lines of change in prepare_mkl.sh \nold: \n-VERSION_MATCH=20170720\n-ARCHIVE_BASE=mklml_lnx_2018.0.20170720     \nnew: \n+VERSION_MATCH=20170908\n+ARCHIVE_BASE=mklml_lnx_2018.0.$VERSION_MATCH\nold:\n-GITHUB_RELEASE_TAG=v0.9\nnew: \n+GITHUB_RELEASE_TAG=v0.10\n. @moderato Glad to know the NaN is gone and thank you for the update!\nI see what you mean, you used to at least have 246 batches done in 70 to 80 seconds with the MKL backend. Now, it is slower than the CPU backend. Do you mind sharing the model file that you use? We want to debug the perf. downgrade with that. Maybe we could include the fix to this perf. drop as well if you could provide the model file you use. \nYou can also send it via email to wei.v.wang@intel.com if that is better. Thanks!. Thanks @moderato We will take a look and let you know when we have improved the performance. . Hi @moderato, Thanks for your update!\nTo clarify, I know you already said the NaN error is gone above. Do you also mean the problem of MKL being slow for your topology is also gone? . Hi @moderato I am very glad to hear your NaN problem is gone with some good performance observations using MKL as well. \nYes, you are right, we prioritized things by accelerating common cases/popular cases that we are aware of.  Neon is not only optimized for Resnets, but also Alexnet, Googlenet, VGG, Deep Speech 2 etc.  But we do not guarantee universal performance advantage over the numpy CPU backend with any random model or random size. . Hi @moderato I am closing this issue. Feel free to open more issues.. @campos537 Can you please start with examples/binary ? It uses MNIST dataset. \nFor advanced data ingestion, please also refer to aeon (which is part of neon and is automatically installed). https://github.com/nervanasystems/aeon\nYou might also want to take a look at examples/imagenet/ and  examples/ssd examples/faster-rcnn to get an idea how the general ingestion process looks like. \n. Thank you for reporting the fix and providing a fix, this PR has been cherry-picked into internal for review and testing.. Hi @abatkins Can you please provide your command used to run ingest_pascalvoc? \nI tried our ingestion code and did not encounter the error: \n~/git/private-neon/examples/faster-rcnn$ python ingest_pascalvoc.py  --input_dir ~/VOC-Tar/ --output_dir ~/faster-rcnn-VOC\nWrote config file to: /home/weiwang/git/private-neon/examples/faster-rcnn/pascalvoc.cfg\nExtracting /home/weiwang/VOC-Tar/VOCtrainval_06-Nov-2007.tar to /home/weiwang/faster-rcnn-VOC\nExtracting /home/weiwang/VOC-Tar/VOCtest_06-Nov-2007.tar to /home/weiwang/faster-rcnn-VOC\nReading PASCAL XML files from /home/weiwang/faster-rcnn-VOC/VOCdevkit/VOC2007/Annotations\nConverting XML files to json format, writing to: /home/weiwang/faster-rcnn-VOC/VOCdevkit/VOC2007/Annotations-json\nConverting XML files to json format (including objects with difficult flag),\nwriting to: /home/weiwang/faster-rcnn-VOC/VOCdevkit/VOC2007/Annotations-json-difficult\n. Please re-open with detailed instructions -- we were unable to reproduce. Thank you anyway!. Thank you for reporting the fix and providing a fix, this PR has been cherry-picked into internal for review and testing. . Thanks @ami-GS! This PR has been cherry-picked into internal repo and is now being reviewed and verified. If there is no issues, the commit will appear in neon master. . Public neon now includes this PR. However, the original author info seems to get lost. Sorry about this and thank you for your contribution!. Hello @iNomaD \nThank you for trying neon, and especially neon with MKL! \nAs you may know, we have only tested neon with MKL Linux versions. \nHowever, from the information you provided, I see at least we can try one more thing: \nFirst apologies for the confusion regarding \"MKL\", in neon when we say \"MKL\" we mean the small library (Deep Neural Network part of MKL): https://github.com/01org/mkl-dnn/releases from this page you can see mklml_lnx_2018.0.20170908.tgz  (since you are using msys64). Otherwise I would have suggested trying mklml_win_2018.0.20170908.tgz (again, sorry we never tried this windows MKLML version). \nFrom the above  \"cannot find -lmklml_gnu\" and \"cannot find -liomp5\" I know it is trying to find the libary (.so format) in the \nmklml_lnx_2018.0.20170908/lib directory. \nSo instead of putting INTEL (BIG) MKL distribution path in MKLROOT, can you please check if directory (under neon) mklml_lnx_2018.0.20170908 is available (and futher whether mklml_lnx_2018.0.20170908/lib) contains the following .so files? \nlibiomp5.so  libmklml_gnu.so  libmklml_intel.so\nIf you do not have \"mklml_lnx_2018.0.20170908\" directory AFTER typing \"make\" under neon. Then I would suggest checking whether mklml_lnx_2018.0.20170908.tgz was downloaded successfully: \n66M Sep 13 13:41 mklml_lnx_2018.0.20170908.tgz \nIf not, I would suggest downloading directly from https://github.com/01org/mkl-dnn/releases where you can find the mklml_lnx_2018.0.20170908.tgz  \nPlease keep us updated whether the above is of any help. Thank you again for trying neon with MKL (MKLML) on IA!. Thanks @iNomaD ! We will work on merging your PR. . https://github.com/NervanaSystems/neon/commit/082da5dc72c6e5b1dd7c1851a9110df56ae4aa41 \nhas the merged PR.. Internal testing in progress. . Thank you @iNomaD  for your contribution! Your PR will appear in public neon in a few days. . Hi, \nYour issues might be different but from this thread https://github.com/NervanaSystems/neon/issues/398#issuecomment-340120280 \nthe following was tried to fix the numpy 0xb vs. 0xa error: \n\"\nI just rebuilt neon and tried\npip install --upgrade numpy --no-cache-dir\nto fix that error\n\". Thanks @riccitensor for reporting. Issue acknowledged, working on a fix. . Hi @riccitensor In the latest neon commits, we added fixes to deep_dream. \nCould you please try the tip of neon master branch and verify? \nThank you!. Ok, thanks @riccitensor for confirming. . Hello @griffonn Could you please follow the discussion in https://github.com/NervanaSystems/neon/issues/415 and see if you can convert the model weights? \nIf not, I can do the conversion for you. . This usually points to a MKL download problem. \nDo you see a file named mklml_lnx_2018.0.20170908.tgz under neon directory? \n. The mklml_lnx_2018.0.20170908.tgz is supposed to be automatically downloaded (when there is good internet configuration) from https://github.com/01org/mkl-dnn/releases/download/v0.10/mklml_lnx_2018.0.20170908.tgz  \nIf for some reason, the file has to be manually downloaded and put to the ROOT of neon directory (in your case /usr/bin/neon). Please also manually unzip it (tar xvf mklml_lnx_2018.0.20170908.tgz)\nThen, please comment out the following two lines from prepare_mkl.sh \n61       wget --no-check-certificate -P $DST $MKLURL -O $DST/$ARCHIVE_BASENAME > /dev/null 2>&1\n 62       tar -xzf $DST/$ARCHIVE_BASENAME -C $DST > /dev/null 2>&1\nSo that becomes \n61       #wget --no-check-certificate -P $DST $MKLURL -O $DST/$ARCHIVE_BASENAME > /dev/null 2>&1\n 62       #tar -xzf $DST/$ARCHIVE_BASENAME -C $DST > /dev/null 2>&1\ntype \"make clean && make\" after you verify that you have the following under neon ROOT directory\n/usr/bin/neon/mklml_lnx_2018.0.20170908$ ls  * \nlicense.txt\ninclude:\ni_malloc.h  mkl_cblas.h  mkl_dnn_types.h  mkl_lapack.h   mkl_trans.h  mkl_version.h      mkl_vml_functions.h  mkl_vml_types.h    mkl_vsl_functions.h  mkl_vsl_types.h\nmkl_blas.h  mkl_dnn.h    mkl_lapacke.h    mkl_service.h  mkl_types.h  mkl_vml_defines.h  mkl_vml.h            mkl_vsl_defines.h  mkl_vsl.h\nlib:\nlibiomp5.so  libmklml_gnu.so  libmklml_intel.so\n. I use the following \"pseudo\" code to convert old weights to new weights. \nlayers = [....] # Conversion routine would require the same layers used to create the old model\nmodel = Model(layers=layers) \nmodel.load_params(old_weight_file, load_state=False/True) load_state=False if no states information are in the old_weight_file, True otherwise\nmodel.save_params(new_weight_file). Take VGG for example: \n[omitting unrelated code ...] Full files can be found at https://github.com/NervanaSystems/ModelZoo/blob/VGG_neon2.3/ImageClassification/ILSVRC2012/VGG/vgg_neon_train.py \n54 # Set up the model layers\n 55 layers = []\n 56 \n 57 # set up 3x3 conv stacks with different feature map sizes\n 58 for nofm in [64, 128, 256, 512, 512]:\n 59     layers.append(Conv((3, 3, nofm), conv_params))\n 60     layers.append(Conv((3, 3, nofm), conv_params))\n 61     if nofm > 128:\n 62         layers.append(Conv((3, 3, nofm), conv_params))\n 63         if args.vgg_version == 'E':\n 64             layers.append(Conv((3, 3, nofm), conv_params))\n 65     layers.append(Pooling(2, strides=2))\n 66 \n 67 layers.append(Affine(nout=4096, init=initfc, bias=Constant(0), activation=relu))\n 68 layers.append(Dropout(keep=0.5))\n 69 layers.append(Affine(nout=4096, init=initfc, bias=Constant(0), activation=relu))\n 70 layers.append(Dropout(keep=0.5))\n 71 layers.append(Affine(nout=1000, init=initfc, bias=Constant(0), activation=Softmax()))\n 72 \n 73 cost = GeneralizedCost(costfunc=CrossEntropyMulti())\n 74 \n 75 model = Model(layers=layers)\nSuppose you have old weights named VGG_D.p, assuming the VGG_D.p model contains the same number of layers as layers variable above, you can print out the VGG_D.p to double check. If VGG_D.p does not contain all the above layers (e.g. it does not contain line 66-71 layers), then please delete line 66071 from the layers. \nstep 1: load it\nmodel.load_params(VGG_D.p) \nRun the program,  if VGG_D.p does not contain states information, it will error out and ask you to change the above to: \nmodel.load_params(VGG_D.p, load_state=False)\nstep 2: save it (in new weight format)\nmodel.save_params(VGG_D_fused_conv_bias.p) \nDone. \nPlease let me know if you are able to convert. \nIf you prefer, you can send me the weight file and the script used to generate the weight file, I would be able to convert it for you. \nThank you!\n. Oh, I see. It is DS2 pre-trained model https://s3-us-west-1.amazonaws.com/nervana-modelzoo/Deep_Speech/Librispeech/librispeech_16_epochs.prm \nWill work on converting that. . OK, thanks for the update and I am glad that you have got the new model weights. \nFor me I had to do the following extra in neon/layers/recurrent.py to convert ds2 model: \n            if getattr(self, key) is not None:\n                serial_dict['params'][key] = getattr(self, key).get()\n            else:\n                serial_dict['params'][key] = None. Closing this issue. Feel free to open new issues.. @shaofushih Can you please provide your changes so that issue#413 could benefit and be resolved?\n\nThanks!. https://github.com/xingjinglu/PerfAILibs/blob/master/README.md seems to already get the workaround by @xingjinglu. . Hi @pantherso48 \nIn neon v2.3, we had an internal layers layout change (fused convolution + bias layer) for performance reasons. Weight loading code that worked in neon v2.2 should be changed in order to work for neon v2.3. \nAre you loading a weight file generated by neon v2.2? \nIf yes, instead of loading the old weight file (e.g. old VGG) into your customized model (e.g. fasteeeeest_rcnn) that uses a subset of the layers in the old VGG model and weight file, our recommendation (sorry, working on this part of the documentation) is to convert the old VGG model file to new format, according to suggestions in https://github.com/NervanaSystems/neon/issues/415 \nIs the weight from your customized model, or is it from us? \nThanks for reporting the issue. . For VGG_D.p, we have already converted and is available \nurl = 'https://s3-us-west-1.amazonaws.com/nervana-modelzoo/VGG/'\nfilename = 'VGG_D_fused_conv_bias.p'\nCan you try replacing VGG_D.p with VGG_D_fused_conv_bias.p?. Yes, please help do so and create a PR. Thanks!. Is \"Bad config encountered during initialization\" causing the \"unrecognized flag\" issue? \nthe cifar10 example should work with \"python data.py --out_dir  data/cifar10\", right?  . Great! \nI am closing this issue, feel free to open new issues. . If it is cifar10 data, can you please check whether that worked for cifar10_msra.py under examples/ directory? \nIf \"python cifar10_msra.py\" does not work, it might be data ingestion issue. \nOtherwise, can you incrementally transform cifar10_msra.py to your model (leaving the data loading part intact)? . My bad. Under cifar10_msra folder, there is data.py, train.py, README.md \nPlease run \n\"python data.py  --out_dir .\"  (data ingestion, which produces csv files etc under \".\" current directory )\nYou will see output like: \nDISPLAY:neon:Downloading file: ./cifar-10-python.tar.gz\nDownload Progress |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| DISPLAY:neon:Download Complete\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50000/50000 [00:51<00:00, 974.63it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:09<00:00, 1103.53it/s]\nManifest files written to:\n./train-index.csv\n./val-index.csv\nThen you can run the following: under the same directory\n\" python train.py  -b mkl\" \nYou will see output like these: \nEpoch 0   [Train |\u2588                   |   61/782  batches, 1.78 cost..... . I think the data.py and several lines of train.py (see below)\ntrain = make_train_loader(args.manifest['train'], args.manifest_root, model.be, args.subset_pct,\n                          random_seed)\ntest = make_validation_loader(args.manifest['val'], args.manifest_root, model.be, args.subset_pct)\nwould achieve what you want \"train_set = AeonDataLoader(config, be)\"  to achieve. \nActually you can open data.py and check how was make_train_loader implemented to load the dataset. \n. There is imagenet folder under examples/imagenet \nThere is also LSUN dataset for GANS. Hi @mrysztow Could you please take a look?. Thanks @miketout for digging into the above issue and coming up a workaround. We will try to take a look and try to reproduce after you release the open source ML project.. Thanks for your PR, we will look into bringing your PR in. . @yangyang-zhang please tell a look at this closed issue: \nhttps://github.com/NervanaSystems/neon/issues/414\n. @ZE0TRON Currently neon with Intel Architecture optimizations (w/ Intel MKL Integration) do not support MacOS -- although Intel Math Kernel Library (MKL) already supports MacOS. \nThe reason you saw the above was because neon by default download Linux MKL, while MacOS MKL is available from a similar URL https://github.com/01org/mkl-dnn/releases/download/v0.11/mklml_mac_2018.0.1.20171007.tgz  (you can change prepare_mkl.sh or download_mkl.sh) but compiling on MacOS that supports Clang OpenMP is probably the hurdle\n. Turned out using gcc might work on Mac for enabling MKL. Please stay tuned. . Thanks for reporting. Looks like there is a URL change for the tar.gz. \nYour approach using latest mkl-dnn (open-source) was able to avoid calling buggy URL.. We'll fix our code on the URL. Reopen till URL is changed. Sorry for the confusion.... https://github.com/NervanaSystems/neon/commit/f43cfa2e26f9c84b0f42fcda50b6b83104623223 closes this issue. Thanks again for reporting!. What do you mean by \"v1 model\" ? Which model is that. \nIs that DeepSpeech model and was it used to work with an older neon version? \n. neon 2.3+ versions should be able to load the model trained in neon 1.8, and re-save it in neon 2.3+ format. \nPlease see this issue: https://github.com/NervanaSystems/neon/issues/415\nand please pay attention to the comment that \"i was able to load and save the new model by printing out the model architecture and verifying that the structure of the layers where equal\". \n. Can you paste your \"echo $LD_LIBRARY_PATH\" and see if that path contains libmklml_gnu.so?. As a workaround, can you download neon and do \"make clean; make\" , i.e. not using pip for now. . Thanks for reporting, we will address this gap. . https://github.com/NervanaSystems/ngraph is the continuation and evolved version of neon. Please have a try, ngraph is compatible with neon models. Please file issues at https://github.com/NervanaSystems/ngraph if you encounter neon issues while using ngraph. Thanks!. Hi @jamilbk Can you please explain why this \"\\\" helps? \nWould this be compatible with lower Perl versions? . Ok, thanks for your link @jamilbk We'll take your PR for internal testing.. ",
    "oleg-trott": "As I mentioned, it's the second result that's suspect, rather than the third.\nNote that ng.max(x) and ng.max(x) + 0 can have different gradients in Neon:\n```\nIn [498]: x = ng.array(np.array([[2, 3]]))\nIn [499]: def grad(e): return Autodiff(op_tree=e, be=ng, next_error=ng.ones(x.shape)).get_grad_asnumpyarray([x])\nIn [500]: grad(ng.max(x))\nOut[500]: [array([[ 0.,  1.]], dtype=float32)]\nIn [501]: grad(ng.max(x)+0)\nOut[501]: [array([[ 0.,  2.]], dtype=float32)]\n```\nI'm assuming that this is because the broadcasting isn't taken into account in the gradient calculation, even though the broadcasting itself takes place with or without +0:\n```\nIn [502]: y = ng.zeros(x.shape)\nIn [503]: y[:] = ng.max(x)\nIn [504]: y.get()\nOut[504]: array([[ 3.,  3.]], dtype=float32)\n``\n. I realize that it's the shape ofnext_errorthat triggers the bug -- that's why I mentioned broadcasting. Broadcasting is very useful, and it's obviously a differentiable operation. I think it's more helpful to fix its gradient than to forbid it. Perhaps add a0to all expressions? :-)\n. That's great to hear, thanks! I know these things can be hard to track down.\n. One possibly silly suggestion I have is to insert_synchthreads();after every;,\\nand}` in the kernel, one at a time, and retest. This is tedious to do manually, but I think it can be automated and run overnight.\nMany of those insertions will break compilation, others might make the program incorrect, but if one of them apparently fixes the issue, it might provide insights into what's going on. (Feel free to ignore, especially if you are sure that it's not a missing _synchthreads() )\n. I think I may have a fix for this issue (relative to v1.1.3):\nIf you add __syncthreads(); after https://github.com/NervanaSystems/neon/blob/v1.1.3/neon/backends/cuda_templates.py#L102\n(with extra {{ and }} around the body of the loop), \n#pragma unroll\n    for (int i = 16; i > 0; i >>= 1) {{\n        __syncthreads();\n        %(shfl_red{0})s\n    }}\nthe problem goes away.\nAs I understand it, the need to syncthreads within a warp is somewhat controversial:\nhttps://devtalk.nvidia.com/default/topic/632471/is-syncthreads-required-within-a-warp-/\nI don't want to get into that debate, because I'm not a hardware guy, but I just wanted to report that this change appears to fix this problem for me: the number of errors over 5 trials goes from about 1000 to 0.\n. > a hardware shared memory store erratum\nNvidia acknowledged this as a hardware bug, or is this something unrelated?\n. I think you could probably just dispatch certain simple float_ew calls to the custom kernel without increasing the complexity (especially if it's already in the code). \n. Thanks!\n. That was fast. I thought that paper just came out.\n. https://arxiv.org/abs/1605.06489 seems to be the reference for this.\nThe fully-connected version of such grouping would use gemmBatched http://docs.nvidia.com/cuda/cublas/#cublas-lt-t-gt-gemmbatched\nNervanaGPU also has batched_dot, but unlike the cuBLAS version, it requires one of its operands to be shared by all the dot products. So as far as can tell, batched_dot cannot be used here.\n(By the way, I'm a very surprised that Intel seems to be continuing to invest in open-source Nvidia kernels. What's in it for them?)\n. cuDNN includes \"im2col\" among its many algorithm choices, but I was just talking about fully-connected layers, which are an important special case of conv ones. \nChannel grouping such as https://arxiv.org/abs/1605.06489 can obviously be applied there too.\ngemmBatched has other uses too. I'll add a separate feature request.\n. Some kind of superhuman intelligence may have been involved.. Hi, Scott (I didn't think you were still involved in this project)\nI'm just reporting this as a behavior contradicting the documentation, which, without warnings or precondition checks, can be a surprise.\nAs to why combining brelu and beta can be useful, assuming the right semantics:\nConsider  a branching network structure (These are becoming increasingly common) \nLet X be the input and conv stand for convolutional layers (with possibly different strides and receptive fields)\nA = relu(conv(X))\nB = relu(conv(A))\nC = relu(conv(A))\nD = relu(conv(A))\nIf brelu and beta could be combined (assuming the right semantics), this network could be backpropagated without any unnecessary copying: B uses beta=0, and C and D use beta=1 to write to the same tensor.\nOtherwise, I don't see how this can be done without an additional buffer and copying.\n. ",
    "mtamburrano": "thank you, I missed that change\n. ",
    "diyessi": "Since ldd says loader.so is okay, this could be a problem with how symbols are resolved when the library is loaded.  Do either of you have a relatively simple python program you could share that exhibits the problem?\n. In deference to performance, the GPU backend does little if any bounds checking.  We will add something to the documentation about this.\n. It looks like a bug to me.\n. ",
    "lolz0r": "Thanks nice job!\n. ",
    "Shashidhar123": "Thanks scttl, I will try it. I will try to give out the details on how I resolved Neon compilation and finally installed it on my machine, hoping that someone may benefit it. Actually, it took me 2 days to finally install Neon. \nHow Neon was  finally installed\n1. Had to switch to ubuntu 14.04 as 15.x ubuntu gave compile errors unsigned int cannot be compared to int,    so switched to 14.04 ubuntu, did not fresh install g++\n2. first git cloned Neon\n3. Installed Anaconda by running bash anaconda\n4. Installed all external dependencies given in the http://neon.nervanasys.com/docs/latest/user_guide.html#installation\n5. wget https://raw.githubusercontent.com/wleepang/sd-deep-learning/master/2015-12-02/neon-conda- environment.yaml\n   conda env create -f neon-conda-environment.yaml\n   source activate neon\n   The above commands did not work initially on centos 6.x.But this time it worked on 14.04 ubuntu without any problems.\n6. Then cd to Neon and then did make sysinstall\n7. This compiled the loader and .so was created , this step was giving error on centos 6.x .\n8. Executed /home/ubuntu/anaconda2/bin/python setup.py develop which created the egg.\nThis is how Neon was installed on my ubuntu 14.04. But the question still remains why the above steps did not work for ubuntu 15.x and Centos 6.x. Maybe somebody can write a blog on it.\nThanks\n. ",
    "anlthms": "Thanks for reporting this. We have fixed the comparison in the code. The fix will be available in release 1.2.0.\n. Two things:\n1) The data returned by the DataLoader is in CHW format. OpenCV returns the video frames in HWC format. In order to have the color channels as the first dimension, you can do this in your code:\nframe = np.transpose(frame, axes=[2, 0, 1])\n2) The DataLoader performs mean subtraction by default. You can turn it off by setting subtract_mean to False within ImageParams.\nWith these changes, you should see the data from both sources match exactly.\n. Please try this after installing Homebrew.\nbrew install ffmpeg\ncd neon\nmake -C loader clean\nmake\nThis should rebuild the neon data loader library with support for audio decoding.\n. @pchankh, good to know that fixed the problem. We will add these steps to our documentation. Meanwhile, you might want to do a git pull on the sp-2016 repo. I committed a bugfix earlier today.\n. Can you try this command from the neon directory and tell us what it prints?\nnvcc neon/backends/util/check_gpu.c && ./a.out; echo $?\n. Does this command return a number >= 3.0? If not, you're out of luck :-(\n./neon/backends/util/check_gpu.py\nNote that you should activate the neon virtual env for this command to work.\n. It looks like you don't have a good pycuda installation.\n. Most likely you have an issue with pycuda. Trying this in the python interpreter might provide more clues:\nimport pycuda.driver as drv\n. Thanks for reporting this. This patch should fix the problem that you are seeing:\n--- a/loader/Makefile\n+++ b/loader/Makefile\n@@ -34,6 +34,7 @@ ifeq ($(shell pkg-config --exists opencv; echo $$?), 0)\n ifeq ($(shell pkg-config --exists libavutil libavformat libavcodec libswscale; echo $$?), 0)\n        VIDFLAG     := -DHAS_VIDLIB\n        AUDFLAG     := -DHAS_AUDLIB\n+       INC         := $(shell pkg-config --cflags libavutil libavformat libavcodec libswscale) $(INC)\n        VIDLDIR     := $(shell pkg-config --libs-only-L libavutil libavformat libavcodec libswscale)\n        VIDLIBS     := -lavutil -lavformat -lavcodec -lswscale\n endif\nPlease add the line shown in the above patch to neon/loader/Makefile and then try running make again.\n. @shyamalschandra, add the following line inside neon/loader/Makefile at the location shown by the diff above:\nINC         := $(shell pkg-config --cflags libavutil libavformat libavcodec libswscale) $(INC)\nThis should take care of the compiler error that you were seeing. Let us know if this works for you.\n. Please give this a try:\nReplace this line inside neon/loader/Makefile\n       IMGLIBS     := $(shell pkg-config --libs-only-l opencv)\nwith \n       IMGLIBS     := $(shell pkg-config --libs opencv)\nand then issue this command from the top level neon directory:\nmake -C loader clean && make\n. Can you run make again and look for error messages in the output? Most likely you are missing a prerequisite. You will need both opencv and ffmpeg. You can try installing them with brew.\n. You might want to try issuing the same command without the offending library. Alternatively, find the file opencv.pc on your system and delete the word -lippicv.\nThough you have ffmpeg installed, make is unable to find the associated libraries. The following command is supposed to display linker options for libavutil.\npkg-config --libs libavutil\nIn your case, running this command should give more clues about how to troubleshoot the ffmpeg installation.\n. Yes, AIFF is supported. However, you'll need to have ffmpeg installed. Reinstall neon after you have all the prerequisites.\n. Does this happen when you use the -s command line argument to save the model? We have identified a bug related to this and will include a fix in the next release.. ",
    "royalstream": "Got it! Thanks.\n. Thanks. That was it.\nI would suggest a change to the install instructions but since you're going to change the makefile there's no need to I guess.\n. ",
    "untom": "It looks like v1.3.0 has come and gone without Python3 support making it into that release. Is there any time frame for when we can expect a Py3 compatible version of neon?\n. ",
    "genghis88": "I was trying to use neon for a project. I ported my network from keras to neon and tried testing, and realized it wasn't working and that python3 is not supported. Supporting python3 is good for neon and for users like me. Can you give us an approximate time frame at least?\n. ",
    "raymondjplante": "After encountering Nervana at Boston Deep Learning Summit, I was interested in comparing to Keras + Theano, but lack of Python3 support is a non-starter.  Hope you get a chance to move this up in the queue!\n. ",
    "ilija139": "Is there a way to turn off the use of atomics with the current version? \n. Ok, ICML deadline is over, so I had time to test this. And it works, thanks a lot!\nModifications needed:\n1. In neon/backends/nervanagpu.py, line 665, change the default value of deterministic_update to True.\n2. In neon/layers/layer.py line 1130 (Conv constructor) set bsum=False when appending Convolution layer. \n3. In neon/layers/layer.py line 1092 (Affine constructor) set bsum=False when appending Linear layer. \nAfter this changes CNN with batch norm are deterministic. \nThanks\n. I can confirm that the issue is due to partial mini batches. \nI think this is ok, as long we are aware of it. Since we can always adjust the batch size or save the model on exact multiples. \nThanks for addressing the issue so promptly. \nfull log:\n(.venv)ilija@deep29:~/hyper/neon-1.1.5/examples$ ./mnist_mlp.py -z 100 -e 5 -r 139 --serialize 1\n2016-01-28 11:15:25,220 - neon.util.argparser - WARNING - No path given for model serialization, usin\ng default \"neon_model.pkl\"\nEpoch 0   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  600/600  batches, 0.27 cost, 2.81s]\nEpoch 1   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  600/600  batches, 0.20 cost, 2.00s]\nEpoch 2   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  600/600  batches, 0.17 cost, 1.73s]\nEpoch 3   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  600/600  batches, 0.16 cost, 1.97s]\nEpoch 4   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  600/600  batches, 0.13 cost, 1.84s]\nMisclassification error = 3.0%\n(.venv)ilija@deep29:~/hyper/neon-1.1.5/examples$ ./mnist_mlp.py -z 100 -e 10 -r 139 --model_file neon_model.pkl\nEpoch 5   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  600/600  batches, 0.12 cost, 4.52s]\nEpoch 6   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  600/600  batches, 0.10 cost, 1.77s]\nEpoch 7   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  600/600  batches, 0.10 cost, 1.96s]\nEpoch 8   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  600/600  batches, 0.08 cost, 2.02s]\nEpoch 9   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  600/600  batches, 0.09 cost, 1.71s]\nMisclassification error = 2.8%\n(.venv)ilija@deep29:~/hyper/neon-1.1.5/examples$ ./mnist_mlp.py -z 100 -e 10 -r 139                  \nEpoch 0   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  600/600  batches, 0.27 cost, 2.19s]\nEpoch 1   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  600/600  batches, 0.20 cost, 1.83s]\nEpoch 2   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  600/600  batches, 0.17 cost, 1.94s]\nEpoch 3   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  600/600  batches, 0.16 cost, 1.81s]\nEpoch 4   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  600/600  batches, 0.13 cost, 1.94s]\nEpoch 5   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  600/600  batches, 0.12 cost, 1.99s]\nEpoch 6   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  600/600  batches, 0.10 cost, 1.76s]\nEpoch 7   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  600/600  batches, 0.10 cost, 1.98s]\nEpoch 8   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  600/600  batches, 0.08 cost, 1.76s]\nEpoch 9   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  600/600  batches, 0.09 cost, 1.94s]\nMisclassification error = 2.8%\n. ",
    "ozancaglayan": "Thank you very much!\n. ",
    "fjanoos": "Great ! \nI'm trying out neon on a Linux 3.16 machine with 4x K80's and I'm getting the following error. Please advise.\npython\nfrom neon.backends import gen_backend\nbe = gen_backend(backend='gpu', batch_size=128, rng_seed=1, stochastic_round=False)\n```\nRuntimeError                              Traceback (most recent call last)\n in ()\n      1 # setup the backedn\n      2 from neon.backends import gen_backend\n----> 3 be = gen_backend(backend='gpu', batch_size=128, rng_seed=1, stochastic_round=False)\n/home/firdaus/.conda/envs/neon/lib/python2.7/site-packages/neon-1.2.1-py2.7.egg/neon/backends/init.pyc in gen_backend(backend, rng_seed, datatype, batch_size, stochastic_round, device_id, max_devices, compat_mode, deterministic_update)\n     89         if gpuflag is False:\n     90             raise RuntimeError(\"Device \" + str(device_id) + \" does not have CUDA compute \" +\n---> 91                                \"capability 3.0 or greater\")\n     92         if backend == 'gpu':\n     93             from neon.backends.nervanagpu import NervanaGPU\nRuntimeError: Device 0 does not have CUDA compute capability 3.0 or greater\n```\n. No - I'm using a remote cloud machine and I don't think cuda was installed (at least the above path's don't exist). Is there some documentation on what pre-reqs are needed to run neon with gpus ?\nThanks !\n. Hi,\nI added cuda/lib64 to LD_LIBRARY_PATH  and cuda/bin to PATH (for cuda sdk version 7.0). Also, I added the print statement on line 88 of init.py.\nthis is the result:\n``` python\nsetup the backedn\nfrom neon.backends import gen_backend\nbe = gen_backend(backend='gpu', batch_size=128, rng_seed=1, stochastic_round=False)\n```\n```\n0\n\nRuntimeError                              Traceback (most recent call last)\n in ()\n      1 # setup the backedn\n      2 from neon.backends import gen_backend\n----> 3 be = gen_backend(backend='gpu', batch_size=128, rng_seed=1, stochastic_round=False)\n/home/firdaus/.conda/envs/neon/lib/python2.7/site-packages/neon-1.2.1-py2.7.egg/neon/backends/init.py in gen_backend(backend, rng_seed, datatype, batch_size, stochastic_round, device_id, max_devices, compat_mode, deterministic_update)\n     90         if gpuflag is False:\n     91             raise RuntimeError(\"Device \" + str(device_id) + \" does not have CUDA compute \" +\n---> 92                                \"capability 3.0 or greater\")\n     93         if backend == 'gpu':\n     94             from neon.backends.nervanagpu import NervanaGPU\nRuntimeError: Device 0 does not have CUDA compute capability 3.0 or greater\n```\nPlease advise.\n. ```\n\nnvidia-smi                                                        [0/9941]\nThu Feb 11 01:02:48 2016\n+------------------------------------------------------+\n| NVIDIA-SMI 346.89     Driver Version: 346.89         |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           Off  | 0000:05:00.0     Off |                    0 |\n| N/A   38C    P0    61W / 149W |     55MiB / 11519MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K80           Off  | 0000:06:00.0     Off |                    0 |\n| N/A   36C    P0    73W / 149W |     55MiB / 11519MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   2  Tesla K80           Off  | 0000:84:00.0     Off |                    0 |\n| N/A   40C    P0    57W / 149W |     55MiB / 11519MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   3  Tesla K80           Off  | 0000:85:00.0     Off |                    0 |\n| N/A   35C    P0    72W / 149W |     55MiB / 11519MiB |     96%      Default |\n+-------------------------------+----------------------+----------------------+\n```\n\n```\n\nnvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2015 NVIDIA Corporation\nBuilt on Mon_Feb_16_22:59:02_CST_2015\nCuda compilation tools, release 7.0, V7.0.27\n```\n\n```\n\npython -m pycuda.autoinit\n/home/firdaus/.conda/envs/neon/bin/python: No module named pycuda\n```\n. Thanks.  I will try that out and get back!\nOn Feb 24, 2016 10:33 PM, \"hanlin-nervana\" notifications@github.com wrote:\nI've been able to configure neon for anaconda on a Ubuntu 14.04 Kepler\ninstance on AWS with the following:\nCreate cuda environment variables (substitute your own proper cuda path)\nexport PATH=\"/usr/local/cuda/bin:$PATH\"\nexport LD_LIBRARY_PATH=\"/usr/local/cuda/lib64:$LD_LIBRARY_PATH\"\nThen, create a conda environment\nconda create --name neon pip\nActivate the environment, then run a system-wide install\nsource activate neon\ncd ~/path/to/neon\nmake sysinstall\nBecause you are inside a conda environment, sysinstall will install all\nthe files in /path-to-anaconda/envs/neon/bin/python. I was able to run\nGPU examples without any issues. Let me know if that works!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/200#issuecomment-188588569\n.\n. \n",
    "magnord": "I have the same problem (but with a GTX980). I'm using Anaconda on Ubuntu 14.04. I've followed the Anaconda installation instructions, but they seem to be for Mac OS X, as the dependencies contain OS X-only packages (like appnope). Also, some of the dependencies in neon-conda-environment.yaml are referring to specific package versions that are no longer available. \nAfter some changes I got the installation running:\n```\n(neon)neon>python setup.py develop\nrunning develop\nrunning egg_info\ncreating neon.egg-info\nwriting neon.egg-info/PKG-INFO\nwriting top-level names to neon.egg-info/top_level.txt\nwriting dependency_links to neon.egg-info/dependency_links.txt\nwriting manifest file 'neon.egg-info/SOURCES.txt'\nreading manifest file 'neon.egg-info/SOURCES.txt'\nwriting manifest file 'neon.egg-info/SOURCES.txt'\nrunning build_ext\nCreating /home/manordin/anaconda2/envs/neon/lib/python2.7/site-packages/neon.egg-link (link to .)\nAdding neon 1.2.1 to easy-install.pth file\nInstalling neon script to /home/manordin/anaconda2/envs/neon/bin\nInstalling nvis script to /home/manordin/anaconda2/envs/neon/bin\nInstalled /home/manordin/neon\nProcessing dependencies for neon==1.2.1\nFinished processing dependencies for neon==1.2.1\n```\nI can run examples using CPU but not GPU.\nexamples>nvidia-smi \nThu Feb 11 11:54:18 2016       \n+------------------------------------------------------+                       \n| NVIDIA-SMI 352.79     Driver Version: 352.79         |                       \n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 980     Off  | 0000:02:00.0      On |                  N/A |\n| 32%   39C    P8    16W / 180W |    381MiB /  4086MiB |      1%      Default |\n+-------------------------------+----------------------+----------------------+\n(neon)examples>nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2015 NVIDIA Corporation\nBuilt on Tue_Aug_11_14:27:32_CDT_2015\nCuda compilation tools, release 7.5, V7.5.17\n(neon)examples>python -m pycuda.autoinit\n/home/manordin/anaconda2/envs/neon/bin/python: No module named pycuda\n. I'll do that. Thanks for looking into this. \nI think another problem with the Anaconda install instructions (in addition to the dependency issues) is that skipping the \"make\" step makes no sense. A lot of stuff will never get built if you use those instructions. \n. ",
    "rfarouni": "@hanlin-nervana your solution worked for me on Ubuntu  15.04 with  GeForce GTX 960 and Cuda 7. Thanks\n. ",
    "marcj": "Sorry, I forgot to copy the complete code. It crashes at the mlp.fit call. I updated the original post. This osx machine is a few days old, I believe it's clear although I don't know what clean exactly means and how I can see if its \"not clean\".\n. Thanks Urs for the really fast response! That was indeed the issue. I'm currently trying to build my own train set as already mentioned, but unfortunately I have not much luck yet. The example with train_y of the documentation is a bit strange to me. It generates a list of 10k numbers. I guess each number represents the idx of a output which should get as expected result a maximal activation. (3 => 0001000000, 0=>1000000000, etc), but if I want to have several outputs generate a max activation? like 1001000110. I can't find any documentation about such use case. Unfortunately all examples lie behind some data generators which do some magic in other libraries so I don't fully understand yet how x and y data should be exactly shaped and which scenarios are possible.\nI have currently a 2d array (65, 14400), 65 examples with each 120x120=14400 as input and output another 2d array (65, 55), 65 examples with each 55 possible categories.\npython\ntrain_set = ArrayIterator(imagesTrainXFixed, imagesTrainYFixed, nclass=len(labels), lshape=(1, 120, 120))\nvalid_set = train_set #just to demo if the network is at min possible to determine the train data itself\nHere just in case a screenshot of my debugger showing imagesTrainXFixed and imagesTrainYFixed.\n\nI'm not sure here if I need to adjust somewhere the model so Neon is capable of using my y-data. Currently with these data I get a network which isn't learning anything, constant cost of 38,12 and I really believe this has something to do with the way my y-data looks. Do you have some hint? Maybe some examples I can look at or did I miss some documentation?\n. Oh well, I believe I got it. I guess neon is detecting the array shape and transforms it. If you don't say now  I need to call something else additionally to get it working, this can be closed :) Thanks!\n. ",
    "thouis": "Here's OS/GPU/.venv information.  The one thing I did change was to install numpy 1.10 instead of 1.9.2 to get around a linker error during the build (complaints about numpy interface 9 vs 'a').\nOS: Ubuntu 14.04.3 LTS\nLinux molly 4.2.0-040200-generic #201508301530 SMP Sun Aug 30 19:31:40 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\nCPU: Intel(R) Xeon(R) CPU E5-2670 v3 @ 2.30GHz\n(x48 cores)\nGPU info:\n```\n nvidia-smi \nFri Feb 26 08:46:30 2016     \n+------------------------------------------------------+                     \n| NVIDIA-SMI 352.39     Driver Version: 352.39         |                     \n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 980 Ti  On   | 0000:04:00.0     Off |                  N/A |\n| 22%   38C    P8    15W / 250W |     20MiB /  6140MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  GeForce GTX 980 Ti  On   | 0000:05:00.0     Off |                  N/A |\n| 22%   36C    P8    15W / 250W |     20MiB /  6140MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   2  GeForce GTX 980 Ti  On   | 0000:08:00.0     Off |                  N/A |\n| 22%   38C    P8    15W / 250W |     20MiB /  6140MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   3  GeForce GTX 980 Ti  On   | 0000:09:00.0     Off |                  N/A |\n| 22%   37C    P8    16W / 250W |     20MiB /  6140MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n```\n.venv packages:\nalabaster==0.7.7\nappdirs==1.4.0\nastroid==1.4.4\nBabel==2.2.0\nConfigArgParse==0.9.3\ncoverage==4.0.3\nCython==0.23.1\ndecorator==4.0.9\ndocutils==0.12\nflake8==2.4.1\nh5py==2.5.0\nJinja2==2.8\nlazy-object-proxy==1.2.1\nlogilab-common==1.1.0\nMako==1.0.3\nMarkupSafe==0.23\nmccabe==0.3.1\n-e git+https://github.com/NervanaSystems/neon.git@5c8380c8ec0ea4b6e7f174c895ba6052a8e0f714#egg=neon\nnumpy==1.10.4\npep8==1.7.0\nPillow==2.9.0\nposix-ipc==1.0.0\npy==1.4.31\npycuda==2015.1.3\npyflakes==0.8.1\nPygments==2.1.1\npylint==1.4.4\npytest==2.8.7\npytest-cov==2.0.0\npytools==2016.1\npytz==2015.7\nPyYAML==3.11\nscikit-cuda==0.5.1\nsix==1.10.0\nsnowballstemmer==1.2.1\nSphinx==1.3.1\nsphinx-rtd-theme==0.1.9\nsubprocess32==3.2.6\nwrapt==1.10.6\n. A second install, from scratch, seems to have fixed this.  My first build had some issues (missing virtualenv, numpy versions), which may have put it in a bad state as I worked around these issues.   Also, the new build continues to work after upgrading numpy (so that wasn't the cause).\nI'll close this issue.  Sorry for the noise.\n. Hitting #233 when I do that.\n. I hit this again, today, trying to implement a Swapout network (https://arxiv.org/pdf/1605.06465v1.pdf), where both paths from the residual network terminate in a Dropout().\nIs there a simple fix for this?  It seems like just adding \"beta * self.outputs + ...\" to the output computation of Dropout is the right answer, but would like some verification.\n. Aha.  I was hoping for a position-dependent bias term.  Is there a way to get that?\nRemoving biases and adding \"-r 0\" does fix it.  Thanks.\n. ",
    "dongjoon-hyun": "Nothing. Thank NervanaSystems for today's meetup.\n. While fixing test module, I searched the same typo and fixed it together.\nThat is the reason why this PR includes backend.py.\n. Thank you for merging!\n. ",
    "thestew42": "This CPU overhead issue is resolved for limited cases in our recent v1.5 release. We have implemented persistent RNN and looping GEMM kernels for BiRNN layers when the activation function is Rectlinclip. These kernels should greatly improve performance in the cases where they can be used. If you have a specific case which is not covered by these kernels yet (different activation function or layer type) we can consider adding support to a future release.\nWe also did some code refactoring in these layers which should improve performance in the general case even when the new kernels can't be used.\n. Greg gave a talk at GTC in April on persistent RNN kernels which we have been working to implement.\n. As you said, maxas will use the NV tools to build a basic shell cubin where it can then insert the assembled sass. In the past few months we changed to a system that assembles kernels on the fly which is probably why you can't find them after installing neon. They are no longer built as part of the install process.\nIf you want to get the cubin files that neon compiles at runtime, you should be able to control the cache where these files are saved with the NEON_CACHE_DIR environment variable. If you want to build a cubin from the sass kernels yourself, you can use maxas directly which is documented here: https://github.com/NervanaSystems/maxas.\n. @jennifermyers can you close this?\n. Hi, \nIt is possible that neon could take advantage of this feature, but it would likely require a lot of work. We use pycuda to access the CUDA API. While pycuda does appear to have support for managed memory, this support is experimental (https://documen.tician.de/pycuda/driver.html#managed-memory) and we have not done any work to support this in neon.\nManaged memory has been supported since CUDA 6.0 and is supported by pycuda, but the new page faulting mechanism on Pascal is required to support over-subscribing device memory as you are referring to. This is explained here (https://devblogs.nvidia.com/parallelforall/cuda-8-features-revealed/). It is not clear to me whether this is only a P100 feature or if other Pascal GPUs support it (1080, 1070, etc). Pre-Pascal GPUs (980, Titan X, etc) would not support this feature, even with CUDA 8 installed.\nThe bottom line is that, although this is certainly possible, it would require several things:\n- pycuda support for managed/virtual memory\n- changes to neon GPUTensor/NervanaGPU backend for managed memory support\n- running on Pascal GPUs\nThe majority of our GPUs are still Maxwell, so I don't think this is a high priority for us at the moment, but we would welcome contributions from to community to support features such as this. One thing to note is that our upcoming graph backend may affect how this feature is implemented (and may even make it easier).\nThanks,\n-Stewart\n. Hi,\nNeon does not currently support operations on a single GPU in multiple streams. However this problem sounds like it could be solved with a batched GEMM operation. The neon GPU backend supports batched GEMM, which can compute many matrix multiplications assuming they are the same size and are stored in a strided memory pattern (in this case meaning a 3d tensor). There are some limitations on how this can be applied, but it could give you some speedup since this will map all of your GEMMs to different CUDA blocks, as opposed to different streams.\n-Stewart\n. Are you describing the caffe feature described here (http://caffe.berkeleyvision.org/doxygen/classcaffe_1_1ConvolutionLayer.html) under the 'group' parameter?\nMy understanding is that the input channels, filters, and output channels are divided into groups and not connected between groups. This would probably involve some change in the parameters to the kernel and memory indexing for inputs/filters/outputs. While these sound like minor changes, they could require more significant work since most of our convolution kernels are written in assembly, we may have to change register allocation and instruction ordering.\nI would expect this to improve performance in cases where we don't have enough CUDA blocks to fill all SMs. This would be likely in layers where the size of the feature map is very small. In cases with medium-large feature maps, we will almost certainly have enough CUDA blocks to fill the SMs at any given time. This kind of optimization (at least the simple route I explained) will not improve performance in cases with a small number of feature maps. If you could provide profiling data and layer parameters to show the potential performance improvements, it would make it easier for us to make a decision on this feature.\nThanks,\n-Stewart\n. Yes the changes are algorithmically simple and representing the logic in SASS is likely not a big deal. However our kernels very carefully use the available registers and in many cases we don't have extra registers to spare without changing lots of code. I would expect support for this with full testing to require about a week. To give you an idea, we would need to either allocate a register to store this group offset or compute it on the fly each time an address is needed (which will require a temporary register and preferably shift operation since mod is somewhat complicated in SASS). Additionally, we have numerous kernels for convolution which would each need to be updated, taking into consideration their individual properties.\nIn terms of profiling data, it would be good if you have a neon network that you can run with nvidia's profiler (nvprof) for several iterations and show that the SMs are indeed not full for certain layers. This would give us a good idea of what kind of workload is problematic and how these kernel changes will improve efficiency before we invest the effort in changing the kernels.\nThis sounds like an interesting feature that I would like to work on at some point, but my plate is full right now with our upcoming graph backend. I probably won't have time to work on this any time soon, but we welcome contributions from the community if you would like to work on these changes. I would be happy to address any questions you have along the way.\n-Stewart\n. > I may be wrong, because I'm not into SASS, but as far as I understand, all you have to do is just like a 64 bit add to the register that stands for a painter to output and likewise for input and you're done, nah? Probably something like a merged multiply-add with a constant and blockIdx.z that's held in some special register as far as my knowledge goes, being multiplied. Kind of like that. All done at the very beginning, so any registers can fully be reused afterwards.\nYes that would be the idea, however we don't store the pointers in registers. One trick we use is computing pointers on the fly from constants. However after a cursory glance it looks like our direct convolution kernels do store an offset for input/filters in a 32 bit register which could likely be used for this offset. Also we do use the blockIdx.z already, so this would require some kind of packing. Currently most of our packing is done with magic number division (where we have constants for magic and shift and the formula is something like blockIdx.z * magic >> shift) so we would need to add these parameters to the kernel arguments and generation code to the python side. As far as the winograd kernels, I'm not sure that something this simple is possible. I would need to spend some time looking at them.\n\nIf it's far from being that easy, I imagine, one could just launch Gnum of kernels consecutively to get Gnum of groups and only sync after they're all completed, though that will come at a price, as as far as I've read, launching each kernel would take around some 10 microseconds and probably more if the launch code is written in Python.\n\nYes you could do this as long as the kernels were launched in separate CUDA streams. The launch time will be a fixed overhead, so this could be nearly as efficient as the proposed kernel changes for longer-running cases. However, for small layers, this overhead may erase any performance gains of using multiple streams/kernels. This is one reason why it would be helpful to see profiling data. If 'x' microseconds is negligible for the cases where this optimization would help, it may be simpler to just change how the kernels are launched.\n. Yes many of our performance-critical GPU kernels are written in SASS rather than CUDA C. SASS is the assembly code which runs on NVIDIA GPUs. We use an assembler for the Maxwell/Pascal architecture in particular which was written by Scott Gray. There are some tutorials and explanations on the github page: https://github.com/NervanaSystems/maxas\n. I've never encountered this error myself, but it looks like the CUDA compiler does not support your version of GCC. You should be able to install a supported version of GCC (version 5) along with whatever version you have installed.\nTo get the CUDA compiler to use the older version, you can try using this solution: http://stackoverflow.com/questions/6622454/cuda-incompatible-with-my-gcc-version\n. This will really depend on the model that you are training. If the model has small layers (feature map size and/or filter size), the GPU kernels will execute very fast and the python code in neon becomes the limiting factor. More computationally intensive models should be able load the GPU at nearly 100%. For example, you should see higher utilization in general for models training on imagenet data than cifar or mnist data. You can also try using more feature maps for each layer to get higher utilization, although whether this is worthwhile will depend on if that improves your model.\n. Right so for that network, the hidden size is 512 and your batch size is probably set to 128. The LSTM layers are basically doing a for loop over time steps and evaluating several matrix multiplies at each step. In this case we are bound both by available parallelism and kernel launch overhead. In order to be compute bound in the GPU kernels and fully utilize available FLOPs, our matrix multiply kernels have a tile (CUDA block) size of at least 128x32. So at best you are getting 16 blocks (assuming 512x128 output of the matrix multiply), which may not fill your GPU depending on which one you have (for example Titan X has 24 SMs, each of which could concurrently execute 2 blocks of this kernel). The second problem here is kernel launch overhead. This matrix multiply kernel will probably complete in less than a millisecond due to the small size of the matrices involved, which may be shorter than the time it takes the python code to launch the next kernel, leaving your GPU under-utilized. We have some special RNN kernels available to help mitigate this launch overhead, but they currently only support specific cases of RNN and BiRNN layers.\n. Thanks for sharing Scott. I'll take a look at these.\n. I do have some changes worked out to integrate these GEMM kernels into neon, but need to figure out how to install the code from the openai-gemm repository with neon. Temporarily using a setup.py in my local.\nUnfortunately Tom this won't really give you much improvement on that network. I did some profiling and the launch overhead is significant as I expected. However, most of the overhead appears to be coming from the elementwise optree processing (see image below). In the future, our graph based framework should help with this. We have a preview of that available now if you are interested, but it doesn't support these types of recurrent networks yet: https://github.com/NervanaSystems/ngraph\n\nThe GEMM kernel, where the majority of compute is happening, is only running about 7% of the time. The other time appears to be spent mostly in the python code involved in calling the elementwise kernels.. The sass files are hand-written, but also contain blocks of perl code used to generate specific parts of the kernels (such as the main loop of GEMM). Some information on maxas and writing kernels for maxas can be found here: https://github.com/NervanaSystems/maxas. The instructions are in SASS which is a level below PTX, but many of the instructions have similar functionality to PTX so you can start there if you're interested in what the kernels are doing.. This should be fairly straightforward to add, since cuda has a roundf function which should work. I'm assuming that we just never hit this case and it was overlooked.. If the Sun grid has CUDA installed, it should be possible. If you are using one of the example scripts or a script with the argparser, you should select -b gpu to use the GPU backend. Otherwise you would need to specify the backend type in your script (probably with the gen_backend call).. ",
    "jinhou": "Hi Hanlin,\nThanks for your reply. That means the CPU performance should be very poor in neon compared with other DL frameworks, which have dedicated CPU code for conv layers and other nn layers implementation? Do you have any plan to add optimized CPU code in your framework?\nThanks\nJinlong\n. ",
    "gpapan": "Thanks for the quick response Scott! It would be a great feature to add. Let me know if I can be of any help to that.\n. For reference, we have recently added support for atrous convolution into Tensorflow:\nhttps://github.com/tensorflow/tensorflow/issues/1815\nTorch/nn has also done the same:\nhttps://github.com/torch/nn/pull/797\nIn our TF implementation, we have followed a different implementation than caffe + torch (which use a modified im2col, followed by gemm): Instead, we first shuffle the data from the spatial dimensions to the batch dimension, followed by regular convolution, followed by rearranging the data from the batch back to the spatial dimensions. This allows us to exploit existing fast conv implementations, but we do need extra memory to store the intermediate results. Incorporating the rate parameter (also called dilation, input_stride, or hole parameter) into the cuda kernels and natively supporting atrous convolution would probably be the best solution.\n. @scott-gray \nSorry for the delay, I just saw your request. Here is the slicing code modified for atrous convolution:\natrous fprop code\n. @soumith would be great to keep this thread up-to-date if you figure out how to make this work via the non-contiguous cudnn tensor approach!\n. @scott-gray\nGreat! Looking forward to it. \n. ",
    "soumith": "I haven't fully checked whether this particular tensor layout is supported in cudnn, but cudnn supports non-contiguous tensors of certain kinds (for input / output). If I understand this correctly, there is a way to simply play with the sizes and strides of the input and make cudnn do it's thing.\n. ",
    "andravin": "This transform appears to be the same or slightly more accurate:\n```\nAT =\n\u23a11   1    1     1      1    0\u23a4\n\u23a2                            \u23a5\n\u23a2   \u221a2   -\u221a2                 \u23a5\n\u23a20  \u2500\u2500   \u2500\u2500\u2500\u2500   \u221a2    -\u221a2   0\u23a5\n\u23a2   2     2                  \u23a5\n\u23a2                            \u23a5\n\u23a20  1/2  1/2    2      2    0\u23a5\n\u23a2                            \u23a5\n\u23a2   \u221a2   -\u221a2                 \u23a5\n\u23a20  \u2500\u2500   \u2500\u2500\u2500\u2500  2\u22c5\u221a2  -2\u22c5\u221a2  1\u23a5\n\u23a3   4     4                  \u23a6\nG =\n\u23a1 1     0     0  \u23a4\n\u23a2                \u23a5\n\u23a2      -\u221a2       \u23a5\n\u23a2-2/3  \u2500\u2500\u2500\u2500  -1/3\u23a5\n\u23a2       3        \u23a5\n\u23a2                \u23a5\n\u23a2       \u221a2       \u23a5\n\u23a2-2/3   \u2500\u2500   -1/3\u23a5\n\u23a2       3        \u23a5\n\u23a2                \u23a5\n\u23a2       \u221a2       \u23a5\n\u23a21/6    \u2500\u2500   1/3 \u23a5\n\u23a2       6        \u23a5\n\u23a2                \u23a5\n\u23a2      -\u221a2       \u23a5\n\u23a21/6   \u2500\u2500\u2500\u2500  1/3 \u23a5\n\u23a2       6        \u23a5\n\u23a2                \u23a5\n\u23a3 0     0     1  \u23a6\nBT =\n\u23a11   0    -5/2   0    1  0\u23a4\n\u23a2                         \u23a5\n\u23a2                \u221a2       \u23a5\n\u23a20  -\u221a2    -2    \u2500\u2500   1  0\u23a5\n\u23a2                2        \u23a5\n\u23a2                         \u23a5\n\u23a2               -\u221a2       \u23a5\n\u23a20   \u221a2    -2   \u2500\u2500\u2500\u2500  1  0\u23a5\n\u23a2                2        \u23a5\n\u23a2                         \u23a5\n\u23a2   -\u221a2                   \u23a5\n\u23a20  \u2500\u2500\u2500\u2500  -1/2   \u221a2   1  0\u23a5\n\u23a2    2                    \u23a5\n\u23a2                         \u23a5\n\u23a2    \u221a2                   \u23a5\n\u23a20   \u2500\u2500   -1/2  -\u221a2   1  0\u23a5\n\u23a2    2                    \u23a5\n\u23a2                         \u23a5\n\u23a30   1     0    -5/2  0  1\u23a6\n```\nI had posted a variation of these matrices earlier that scaled some of the columns of AT and rows of G, but that seemed to hurt accuracy slightly.\n. ",
    "madhurgoel": "Could you please show how to create a Model with different batch size, as I guess batch_size is fixed by backend, wrt \ndef set_batch_size(self, N):\n \"\"\"\n        Set the actual minibatch size, so even though the buffers are allocated considering\n        excessive padding, the processing for some layers may be shortened.\n        Currently most of the neon layers don't use that to control the processing. The\n        interface is here only for when someone wants to set that information and experiment.\n\"\"\"\n1) has it been implemented, to actually doing the whole exercise compute efficient ?\n2) How would I copy weights from the old Model(lager batch size) to the new Model(batch size ==1) e.g. do you think the combination of get_description(get_weights=True, keep_states=False) and deserialize(load_states=False) would work ?\n. ",
    "iaroslav-ai": "With the last question regarding implementation with neon backend I mostly want to get an idea of how complicated it would be.\n. Well, basically I want to know whether it is hard to deal in neon with complex inputs, the example is made up :) I did not found so far a definite answer in the docs\n. Thanks for the pointer! I will look into it.\n. Sorry I did not respond for so long. I actually reinstalled my whole system shortly after I posted the issue, and now I have no problems with building the loader.so. I think for now it makes sense to close the issue, and maybe reopen it if someone experienced similar issues as I did. \n. ",
    "gujunli": "\"which nvcc\" returns nothing\nI am installing pycuda now, if that is the issue\n. right. I added it in the .bashrc file. But i forgot to source it.\nAfter sourcing it, it seems to make progress now\n. map.py example ran successfully. you can close this thread now. Thank you!\n. ",
    "szeitlin": "No, I can't. I tried that. The default python in my path is python3, which your library doesn't support.\nIt should be possible to install the dependencies inside an existing venv. Otherwise, I would need to change the makefile so it adds python 2.7 when it creates the venv. \n. Thanks, I tried that. It didn't work. That's why I opened this issue. \n. aha, I think the sysinstall step was actually making things worse. \nJust did regular make and then I had to do source .venv/bin/activate in order to get the virtualenv to work, but now the examples look like they ran just fine, and after installing jupyter notebook, I'm able to import the library. Thanks!\n. fwiw, now I'm trying to walk through the MNist tutorial and I can't proceed. \n```\nIn [10]:\nsetup training set iterator\ntrain_set = ArrayIterator(X_train, y_train, nclass=nclass)\nAttributeError                            Traceback (most recent call last)\n in ()\n      1 # setup training set iterator\n----> 2 train_set = ArrayIterator(X_train, y_train, nclass=nclass)\n/Users/szeitlin/mystuff/neon/neon/neon/data/dataiterator.pyc in init(self, X, y, nclass, lshape, make_onehot, name)\n     76         X = X if isinstance(X, list) else [X]\n     77         self.ndata = len(X[0])\n---> 78         assert self.ndata >= self.be.bsz\n     79         self.start = 0\n     80         self.nclass = nclass\nAttributeError: 'NoneType' object has no attribute 'bsz'\n```\n. ",
    "colorless1": "for me it has nothing to do with this framework but some kind of network problem the error is gone anyway.thank you \n. ",
    "hughperkins": "Would it be http://arxiv.org/abs/1509.09308 ?\n. Ok.  Thanks!\n\nOr a link to neon if you're talking very specifically about our implementation.\n\nHmmm.  Well... I'm basically saying something like:\n\"CUDNN implementation is proprietary, so we cannot reason well on how it works, and what we can learn from it. However, Gray has provided descriptions of how the Winograd kernels work, and the Winograd kernels provide similar, or better, performance to CUDNN.\n\"Winograd is not written in CUDA, but in low-level assembler.  Gray asserts that the performance of Winograd could not be obtained without using low-level assembler.  It seems plausible that to reach\nthe same performance on AMD and Intel hardware, we would need a similarly low-level implementation.\"\n. Cool.  Thanks! :-)\n. (hmmmm, seems this comment https://www.reddit.com/r/MachineLearning/comments/4e5g6z/implementing_custom_convolution_kernels/d1xk3tn is a good source for my assertion about your assertion, though not sure I can actually cite that in a paper, but at least my assertion about your assertion is not entirely without evidence :-) )\nEdit: Ah, https://github.com/NervanaSystems/maxas/wiki/Introduction part 5 suggests a 33% speed boost of SASS over CUDA/PTX ?\nEdit2: I think I shall add the maxas introduction as an additional reference, for sass/maxas, ie something like\n@article{maxas,\n   title={maxas},\n   howpublished = {\\url{https://github.com/NervanaSystems/maxas/wiki/Introduction}},\n   author = {Gray, Scott},\n   note = {accessed: 2016-05-22}\n}\n)\n. Just to confirm, is it the case that:\n- Winograd kernels are using direct convolution? (cf im2col, fft, or some other blas-based implementation for example)\n- given that cuda-convnet demonstrated that direct convolutions provide excellent performance, on very specific geometries, but generalize poorly to other geometries, is it the case that Winograd provides multiple kernel implementations, each for one or more families of geometries?  Which families of geometries are implemented?  Is it the case that some families of geometries would obtain better performance using im2col+cublas and/or fft+cublas and/or are not currently implemented?\n. Hmmm, Scott, skimming through the Lavin and Gray paper, which I confess I havent yet read, and probably should do, it looks like a huge performance gain is algorithmic?  And that using standard CUDA kernels, based on your numbers on https://github.com/NervanaSystems/maxas/wiki/Introduction , the performance might approach around 70-75% of the performance of the current Winograd SASS kernels? And therefore, and I know this is getting a bit speculative, if we assume a performance drop for OpenCL vs CUDA for around 33-50%, we might be able to get convolutional performance on OpenCL approaching around ~30% that of the Winograd kernels, simply by porting the Lavin and Gray algorithms into OpenCL?\n. Ok.  Concerned that the performance is going to be limited by the BLAS implementation, I hacked around a bit on neon, to find out how/where gemm is being used, on a Maxwell device.  It looks like two kernels take up approximately 50% of the time each:\n- sconv_winograd_4x4_3x3_32x32\n- xprop_image_trans_4x4\n\nI instrumented nervanagpu.compound_dot and nervanagpu.batched_dot, which were the only functions I found using cublas?, to have a print statement at the start of each, but these print statements were never triggered.\nI entirely emptied the neon/backends/kernels/ptx directory, renamed sass to sass.disabled (I know this is only used during make, but still), and removed everything from cubin directory except for sconv_winograd_4x4_3x3_32x32_*.cubin, and everything continued to run ok.\nxprop_image_trans_4x4_kernel on the other hand seems to be embedded in winograd_conv.py\nSeems like somewhat plausible that I just need to write an appropriate generator to create appropriate OpenCL for these two kernels, and then OpenCL convolution will run ok???  Or I'm missing a whole bunch of stuff/concepts?  (I still didnt quite read the paper yet I confess...)\nEdit: oh, I guess that the sconv_winograd_4x4_3x3_32x32 is a fused kernel, that combines a whole bunch of things together, including gemm, as you are alluding to above?\nEdit2: hmmm, xconv_winograd_4x4_3x3_32x32.sass is 1461 lines :-O  of detailed assembler.    That seems ... challenging :-P ... to read/convert/have written originally.\n. Ah... on Kepler, commenting out cublas declaration causes it to fail, so it seems like:\n- gemm is still needed?\n- on Kepler, you are using cublas for gemm?\n- on Maxwell, you've written an entire gemm implementaiton in sass :-O\nSo, my options, for non-CUDA hardware are:\n- use an existing gemm implementation (clblas, viennacl, clblast)\n- write my own gemm implementation, dedicated for the needed geometries\n- port your sass gemm to opencl (via search and replace??? but seems like 64-bit operations are split across multiple operations... anyway...might take a peek)\nQuestion: thoughts on the extent to which your acceleration is due to:\n- algorithm (ie Winograd) ?\n- writing gemm in sass?\n- other bits being written in sass? (trans sounds like it's doing a fair amount of work?)\n- other stuff?\n(Edit: hmmm, on a maxwell, forcing kepler version causes perf to drop by 50%)\n. actually seems wheel is installed somehow\n. Following http://stackoverflow.com/questions/6622454/cuda-incompatible-with-my-gcc-version#comment56532695_8693381 I did \n$ sudo ln -s /usr/bin/gcc-4.9 /usr/local/cuda/bin/gcc\n$ sudo ln -s /usr/bin/g++-4.9 /usr/local/cuda/bin/g++\n... to solve this.\n. > Slightly nicer and more Ubuntuish would be to use:\nThis is a system level modification, and may come back to bite you later.  For example, what happens if you build a project that has c++ linkings to a system installed library?\nMy change:\n- is strictly limited to cuda-7.5 installation, which was broken anyway\n- will be purged automatically, as soon as I upgrade to cuda-8.0\n. Not sure I understand the link between 'a,b,c are required by neon', and 'so users are expected to install a,b,c themselves', but I'm not using neon at this time, so closing.. > Do you mean that there is both http://github.com/nervanasystems/neon and https://github.com/nervanasys/neon ? \nyes, exactly :-)\n. Cool.  That was quick! :-)\n. Cool.  Sounds good :-)\n. > Oh, and you know the kepler cuda code is just direct conv right? It's not clear why you named your repo winogradcl.. but I guess you could be intending to add implementations of that?\nI didnt know that.  Thats new infomration :D  Thats good actually, means that should be able to make it a ton faster, without needing to use anything other than opencl?\n. (also good because then I can do something more creative then simply search and replace threadIdx.x with get_local_id(0) :-D  Though obviously incrdibly embarrassing that I didnt actually spot this earlier :-D )\n. I've renamed it to neonCl for now :-D  I think I might as well get the direct ones working, since they're already super fast.... and getting them working will help me learn kind of how your mind works and stuff (eg will know what 'magic' does), which will help with dealing with winograd somehow, however that will be.\n. > And again. .they're meant to be run on sm_30 devices (Amazon Cloud mainly). So they're not nearly as fast as they could be if designed for >=sm_35 (lots more registers and shared memory).\nOk.  I'm in two minds about this.  On the one hand, my current semi-stated objective is to get opencl convolution to run as fast as possible on Titan X / NVIDIA devices.  So, at least no-one can say 'well opencl is slow, and nvidia nerfed it'.  And then it's down to AMD or whoever to do whatever they need to do to make them run fast on their own devices.\nOn the othre hand, realisticlaly no-one is going to use opencl on nvidia devices.  AMD and Intel device users are the only realistic clients, and I heard AMD devices are not overwhelmingly endowed with registers and shared memory (heresay, not having an AMD device mysefl...), so in this sense the sm_30 kernels sound not unreasonable.\n\nBut for winograd, one approach is just write external transforms and use a BLAS lib for the batched gemm. Some of the external transforms are already done for you in the winograd_conv.py file. You would just need the output transforms. The 4x4 transform will work much better externally because it only expands the input/output/delta data by 2.25x.\n\nOk, sounds good.  I will probably dabble in using Cedric's CLBlast, as the underlying BLAS implementation, and see how that goes.\n. Hmmm, your idea seems awesome.  Reading...\n. Dont think this should be 'open', so closing it.  Then will continue the conversation below.\n. Hi Scott,\nStarted looking at this approach, got as far as calcing U and V using the existing CUDA kernels, ported to OpenCL, https://github.com/hughperkins/neonCl-underconstruction/blob/play-winograd/winograd_cl.py , then pondering how to get M.  It looks like the output of the U and V transforms is:\n# U                           Co // 32,       Ci,    6,   6, Co % 32\n                         # bytes:           eg 150KB, 4.6K, 768,     128\n    # V            # tiles, tiles, N // 32,       Ci,    6,   6,  N % 32\n            # bytes                         eg 150KB, 4.6K, 768,     128\nSeems like these are already split into conveniently sized blocks of [32, 6, 6, 32], for each of [Ci % 32, 6, 6, Co % 32], and [Ci % 32, 6, 6, N % 32].  Seems like I could simply directly utilize this blocking, and write a kernel to pull down each of these blocks, loop over Ci; and then at the end add the results of each block together?  ie, instead of calling out to a third-party GEMM implementation?  The sizes of these blocks are too big to fit into __shared__ memory?, but seems like you are predominantly using L2 cache to handle this, which is I think 3MB on a Titan X?, and so should be largely sufficient for this?\n. (Ah, I guess I should have two levels of blocking?  Hence the name 'superblock'?  But seems like the effort involved in writing sub-blcoking within such regularly-sized superblocks should be relatively low compared to the overkill of somehow plugging in a third-party gemm??? )\n. But ... I cant quite figure out why Ci, ie your C, is not in the innermost dimension?  Since we are reducing over this, we'd want it to be contiguous?\n(Edit: oh, unless it's something like:\n- create a workgroup with 32 threads\n- each thread will handle 32 K values, and all associated xi,nu pairs for those values\n- the warp will pull down data for a few C values at a time, and each thread will keep track of its own sums over the 36 xi/nu combinations\n?\n)\n(Edit2: hmmm, maybe xi and nu should be the outermost dimensions?  Seems like all the values are conditiioanlly independent on xi and nu, no values are shared across different xi/nu pairs?)\n. Ah!  Makes sense.  Thanks! :-)\n. ",
    "tastyminerals": "In case @hughperkins solution does not work for some of you sitting on Archlinux or Manjaro. You should create links to /usr/bin/gcc and /usr/bin/g++ instead of /usr/local/cuda/bin. Before doing that don't forget to rename the original binaries! That will be needed if you decide to run ~/torch/install/bin/luarocks separately or if you're a Manjaro user.\nManjaro users should also add the following condition to install.sh in order to install everything smoothly:\n```bash\nIf we're on Manjaro linux, use gcc v49\nif [[ uname -a == \"MANJARO\" ]]; then\n    path_to_gcc49=$(which gcc-4.9)\n    if [ -x \"$path_to_gcc49\" ]; then\n      export CC=\"$path_to_gcc49\"\n    else\n      echo \"Warning: GCC v4.9 not found. CUDA v8 is incompatible with GCC v6, if installation fails, consider running \\$ pacman -S gcc49\"\n    fi\nfi\n```\n. ",
    "hookover": "@hughperkins \nevent:\nubuntu 16.04 default gcc version 5.3\ncuda7.5\ntensorflow1.0.1\nI do this:\n$ sudo ln -s /usr/bin/gcc-4.9 /usr/local/cuda/bin/gcc\n$ sudo ln -s /usr/bin/g++-4.9 /usr/local/cuda/bin/g++\n\u2601  build [master] \u26a1 gcc --version\ngcc (Ubuntu 4.9.3-13ubuntu2) 4.9.3\nCopyright (C) 2015 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\u2601  build [master] \u26a1 g++ --version\ng++ (Ubuntu 4.9.3-13ubuntu2) 4.9.3\nCopyright (C) 2015 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\u2601  build [master] \u26a1 make     \n```              \n[ 10%] Building NVCC (Device) object CMakeFiles/warpctc.dir/src/warpctc_generated_reduce.cu.o\nIn file included from /usr/local/cuda/include/cuda_runtime.h:76:0,\n                 from :0:\n/usr/local/cuda/include/host_config.h:115:2: error: #error -- unsupported GNU version! \ngcc versions later than 4.9 are not supported!\n #error -- unsupported GNU version! gcc versions later than 4.9 are not supported!\n  ^\nCMake Error at warpctc_generated_reduce.cu.o.cmake:207 (message):\n  Error generating\n  /srv/python/lstm_ctc_ocr_with_tf_1.0.1/warp-ctc/build/CMakeFiles/warpctc.dir/src/./warpctc_generated_reduce.cu.o\nCMakeFiles/warpctc.dir/build.make:336: recipe for target 'CMakeFiles/warpctc.dir/src/warpctc_generated_reduce.cu.o' failed\nmake[2]:  [CMakeFiles/warpctc.dir/src/warpctc_generated_reduce.cu.o] Error 1\nCMakeFiles/Makefile2:141: recipe for target 'CMakeFiles/warpctc.dir/all' failed\nmake[1]:  [CMakeFiles/warpctc.dir/all] Error 2\nMakefile:127: recipe for target 'all' failed\nmake: *** [all] Error 2\n```\nooooh no.... ",
    "Aurora11111": "I meet this problem ,and I sloved it just by uninstall old gcc,and reinstall a gcc with verison of 4.9:\n1.uninstall gcc\nsudo pacman -Rsc gcc          \n2.reinstall gcc\nmkdir -p $HOME/build/\ncd $HOME/build/\ngit clone https://aur.archlinux.org/gcc49.git\ncd gcc49\nmakepkg --syncdeps\nsudo pacman --upgrade gcc-4.9.3-1-x86_64.pkg.tar.xz\n. ",
    "abduallahnoorsayket": "Error: No results matching query were found.\nhow can i fix this issue ?? i am using ubantu 18+\n. ",
    "freedom9393": "This solved my problem:\nsudo rm /usr/local/cuda/bin/gcc\nsudo rm /usr/local/cuda/bin/g++\nsudo apt install gcc-4.9 g++-4.9\nsudo ln -s /usr/bin/gcc-4.9 /usr/local/cuda/bin/gcc\nsudo ln -s /usr/bin/g++-4.9 /usr/local/cuda/bin/g++. ",
    "marcinma": "I think some more tests should be made to validate this script (ideally with docker container). On my new dev machine still some dependencies were missing:\nFor ubuntu 14.04 i had to install clang\nsudo apt-get install clang\nFor ubuntu 16.04 i had to:\nsudo apt-get install libsox-fmt-all libsox-dev sox libcurl4-gnutls-dev. ",
    "Cpruce": "I solved this by downloading the ippicv lib\nhttps://sourceforge.net/projects/opencvlibrary/files/3rdparty/ippicv/\nand copying the appropriate files like so\n/usr/local/opencv/libippicv.a\n/usr/local/include/opencv/ippicv_types.h\n/usr/local/include/opencv/ippicv_redefs.h\n/usr/local/share/OpenCV/3rdparty/lib/libippicv.a\nI also added the line \n/usr/local/opencv/\nto /etc/ld.so.conf.d/opencv.conf (had to create this).\nAfter that, run sudo ldconfig. The creates a new location for that static library and tells the linker to look for that in the opencv dir.\nP.S. I'm not sure if I had the header files beforehand too. This very well could've been my problem.\nHope this helps! \n. Hi I had this same exact issue. However, I haven't been able to resolve this one and I have been able to resolve all my issues so far.\nGTX 1080\ndriver 367.48\nUbuntu 16.04\nCan switch between Cuda Toolkit 7.5 and 8.0 via symlinks to /usr/local/cuda\nI built pycuda from source. The tgz copy from his website seems to looks for libcurand.so.8.0 as I had that library not found after doing ldd _driver.so. I then cloned from github and this copy looked for 7.5 and saw that I had it (symlinked to 7.5 at that time). I even got import pycuda.driver as drv when symlinked to 7.5. That said, I then get \"OSError: CUDA runtime library not found\".\nIf I symlink to the cuda-8.0 directory, your (@anlthms) covnet runs, even though the backend defaults to cpu and the pycuda driver import complains about not having libcurand.so.7.5.\n@anlthms Do you (or anyone else reading) have any leads on this? Do you only run 7.5?\nThanks for all the help so far!\n. ",
    "Maratyszcza": "Thanks for prompt response! Padding K wouldn't work for my use-case: its a fully convolutional network with uneven number of categories. Looking forward to the next release!\n. Thanks for the pointer. Is Neon smart enough to avoid allocation memory for gradients and reuse deltas when frozen layers are in the beginning of the network?. Yes, thats exactly my use-case.. ",
    "vg123": "How do you fix the above error , asked in the first post. please explain in detail.. Ok, I got the solution I just removed all those dependencies from Makefile which were not found when I run make command. and Yes those opencv libraries  were depending on cuda which is not installed on my system.  . ",
    "ghost": "Originally dropout has been designed specifically for training right ?\n. \n@nervetumer, thank for reply. But, I got another error after I added the 'be = gen_backend('gpu')'.\nTraceback (most recent call last):\n  File \"test.py\", line 3, in \n    from neon.data import ArrayIterator\n  File \"/usr/local/lib/python2.7/dist-packages/neon-1.7.0-py2.7.egg/neon/data/init.py\", line 18, in \n    from neon.data.datasets import Dataset\n  File \"/usr/local/lib/python2.7/dist-packages/neon-1.7.0-py2.7.egg/neon/data/datasets.py\", line 20, in \n    standard_library.install_aliases()  # triggers E402, hence noqa below\n  File \"/usr/local/lib/python2.7/dist-packages/future/standard_library/init.py\", line 483, in install_aliases\n    import test\n  File \"/root/example/test.py\", line 7, in \n    train = ArrayIterator(X=X, y=y, make_onehot=False)\n  File \"/usr/local/lib/python2.7/dist-packages/neon-1.7.0-py2.7.egg/neon/data/dataiterator.py\", line 152, in init\n    self.Xdev, self.Xbuf, self.unpack_func = list(zip(*[transpose_gen(x) for x in X]))\n  File \"/usr/local/lib/python2.7/dist-packages/neon-1.7.0-py2.7.egg/neon/data/dataiterator.py\", line 145, in transpose_gen\n    return (self.be.array(z), self.be.iobuf(z.shape[1]),\n  File \"/usr/local/lib/python2.7/dist-packages/neon-1.7.0-py2.7.egg/neon/backends/backend.py\", line 588, in iobuf\n    out_tsr[:] = 0\n  File \"/usr/local/lib/python2.7/dist-packages/neon-1.7.0-py2.7.egg/neon/backends/nervanagpu.py\", line 190, in setitem\n    self.getitem(index)._assign(value)\n  File \"/usr/local/lib/python2.7/dist-packages/neon-1.7.0-py2.7.egg/neon/backends/nervanagpu.py\", line 357, in _assign\n    self.gpudata, unpack_from('I', value)[0], self.size, stream)\nBoost.Python.ArgumentError: Python argument types in\n    pycuda._driver.memset_d32_async(NoneType, int, int, NoneType)\ndid not match C++ signature:\n    memset_d32_async(unsigned long long dest, unsigned int data, unsigned int size, boost::python::api::object stream=None)\n. ",
    "BaxterEaves": "Yep. Using it during training is supposed to reduce overfitting. To use it to get an estimate of the variance in the output, you provide a single input and then generate a bunch of outputs while leaving dropout on.\n. @utkarshsimha Go for it. I won't be submitting a PR.. ",
    "utkarshsimha": "Is this still open? I'd like to work on this! :)\n@jennifermyers Could I make a PR if @BaxterEaves isn't working on it?. ",
    "zengjichen": "for example, when i run the file examples/mnist_mlp.py  choose 'gpu',the result on each epoch are error 90.2% and costs never decline,but run in  'cpu',it works well ,and get the good answer, could you give some guide on use 'gpu' ,the tutorials may not introduce completely.. thank you , best wish\n. thanks for your replay,I solved it by recompiled the neon,and it works well .\n. ",
    "ticoneva": "I am using Ubuntu 15.10 with Anaconda. It was not working the first time, but I just deleted everything and tried again and make does work as stated. Sorry for the false alarm---I have no idea why it did not work the first time.\n. ",
    "ottolu": "Hi @thestew42, thanks for your quick reply.\nFinally, I did some workaround stuff to get those kernels:\nThe function get_kernel in kernel_specs.py should be the entry of building kernels process.\nSo I wrote a tiny script to call this function with the names of kernels I want.\nAnd yep, that seems work, and I can get the built cubin and dump_sass in the cache dir.\nThanks again! Please help me to close this issues, @thestew42 \nAnd I hope this thread could help other guys who need it.\n. ",
    "saurav111": "Cool. Thanks! \n. I'll ask a simpler question. Is there a good way of accessing the activation layer of a hidden layer from any model?\n. Thanks. I want to access the activations for a particular batch at test time. Will it be correct to call outputs on the layer I want to look at, after calling fprop with that batch?\n. Great. Thanks a lot.\n. The size of frame in this case is (128,171). So, there is no need to scale.\n. @anlthms Thank you :)\n. ",
    "FuriouslyCurious": "@bhack Nervana is building FPGA accelerator chips which can do 55 TOPS, while NVidia's $10,000 Tesla chip does 44 TOPS.  NVidia is going to feel the heat in Q1 2017 as long as Nervana price it under $5K.\n@ursk thanks!\n. ",
    "Willib": "Hi,\nI have the same question, but I don't know how to solve it.\nDid you solve the question?\nI sincerely hope that you can help me, thanks!\n. ",
    "yhd4711499": "AudioConveterDispose is included in the AudioToolbox in OS X frameworks.\nadd this to your cmake file:\ntarget_link_libraries(<your_module_name> \"-framework AudioToolbox\"). ",
    "swapnilsj": "Thanks. It worked! Do you think these changes in requirements.txt be pushed since there is no information about the above changes for installation on RHEL??\n. I too encountered above issue & installed ffmpeg on my MacBook Pro and make sysinstall on Neon ...still getting the same issue. What could be wrong?. @binarybana Thanks. Let me give it a try...I will get back to you with the results/issues if any.. ",
    "tdeboissiere": "Fixed this by building pycuda from source and removing the corresponding line in gpu_requirements.txt\n. ",
    "MadcowD": "@thestew42 Interesting! I was aware of this functionality, the interesting bit comes, at least in my case when the matrices are of different sizes. Thank's for the prompt response.\n. ",
    "ibmua": "Yes, it's that.\nSomething like\n[input, output] += blockIdx.z * [input, output]_STRIDE (<---- \"define\" compiled into the kernel)\nif compiled with groups>1 is what comes to my mind. But that's naive C and it also makes an assumption that blockIdx.z's not already used, which I guess may be wrong. So it'll be something more of a blockIdx.z%max_of_already_used but not naively so, as %mod is generally not the fastest operation on CUDA as far as I know, if max_of_already_used != 2^n. But generally speaking it should be just as simple even in SASS, I can't imagine why not. Just stride the input and output pointer by some value from blockIdx, that's the whole trick.\nI don't know if I'm correct, but as far as I understand, neon's layers get compiled with several versions of parameters, as in how to partition threads, then tested and the fastest version is taken, right? While you can fill all, or a large part of SMs with threads and will be able to compute faster than without such smart partitioning of compute, most likely, this speed boost comes with efficiency sacrifices.\nIf you can elaborate on what kind of data I should provide, I'll be very happy to help you help me. =) If it's about some examples of what kind of convolutions one would use this to compute, I think a fast implementation of grouped convolutions where one could partition convs into like a 100-10000 blocks (this is extremely slow in all current implementations) could lead to discoveries of many more places where this can be used. But just stating some relatively obvious cases without going far into research space, some of the very obvious use cases would be an ensemble-style microstructure where one uses a stack of grouped convs and then merges their result, one very obviously working type of merge would be Max, or Avg against same feature index in each group. That's just a drop-in replacement for ordinary stacks of layers, like 3x3 conv going after 3x3 conv. You make the first one Nx wider and the next one N-grouped and then merge results of the groups and, presumably, get a better result and much faster training (yeah, I know, \"why the faster training?\", but it actually works like that). Also, one could change structure of the ResNet-type residuals to be more group-kind-of-wide like that and less serial if that would optimize calculation speed, without much loss in accuracy. I'm currently running some tests on that using WideResnet https://github.com/szagoruyko/wide-residual-networks Torch+CuDNN and will let you know how good of an idea the one with ResNets actually was.\nTorch's CuDNN has groups, but it's very poor at them, especially with larger numbers of groups. For example, without any fancy mathematical optimizations I was able to implement 20x faster fully-grouped convolutions in Cuda vs CuDNN v5.1's and for many cases my CPU implementation is as fast as CuDNNs. Caffe's groups, on the other hand, last time I checked were implemented to an extremely poor performance. So my hope lies with Neon.\nAlso, one other meaningful parameter for grouped convs I can imagine is whether one wants to output resulting feature maps as  111122223333444 (usual), or 1234123412341234. This would  provide for an ability to make less isolated stacks of grouped layers. How useful this would be is yet to be discovered, though, something one has to implement first to try. But another thing where it should definitely be useful is in giving ability for conventional volumetric pooling like in Torch's https://github.com/torch/nn/blob/master/doc/convolution.md#nn.VolumetricAveragePooling to pull results of the groups together. If someone would also make such a layer in neon, though, of course. =) \n. So I ran a test with original WideResnet 2-40 and a modified version that has 3 residuals instead of 6, but has them in 2x groups combined with a 1x1 layer, because, sadly, I couldn't find a way to easily combine groups in Torch via Avg, or Max. And the results were actually better for that one. But there was a pretty strong problem that I didn't use weight decay, so now I'm retesting to get better results out of both. The likely results for the original are reported in their publication, but I want to first squeeze such results out of the trainer and then train grouped variant with same hyper-parameters. Also, lack of weight decay may have been the main driver in the aforementioned \"speedup in training\" in terms of epochs until convergence.. =/\n. So I ran my experiment and got to ~25% error so far on that test (390 epoch, ~0.002 LR at the moment), which is better than the published 26% for the usual thing. Though, I'm using an [LR decay 0.98 / 2 epochs] approach and not steps like original authors.\nOkay, so I had no idea, you guys pushed optimizations THAT far. Okay, I'll try exploring how to profile stuff with nvprof, so far I've only profiled in nsight. =) Will write back when I'll get some data.\n. Regarding contributions, IMHO, Neon could become a framework of choice for people exploring new concepts that require writing custom CUDA code due to PyCUDA and more overall cleanliness vs Torch/TensorFlow/Caffe. But you guys need to work a bit on clearly explaining how to extend different parts of it. With some rather simple non-cryptic non-SASS examples.\n. I may be wrong, because I'm not into SASS, but as far as I understand, all you have to do is just like a 64 bit add to the register that stands for a painter to output and likewise for input and you're done, nah? Probably something like a merged multiply-add with a constant and blockIdx.z that's held in some special register as far as my knowledge goes, being multiplied. Kind of like that. All done at the very beginning, so any registers can fully be reused afterwards. So it shouldn't really touch any code further down the line. Unless there's some trouble with a register for blockIdx.z, like if it's used further in the code and it's read only, so you'll have to allocate another register to hold the value you currently hold inside blockIdx.z.\n. If it's far from being that easy, I imagine, one could just launch Gnum of kernels consecutively to get Gnum of groups and only sync after they're all completed, though that will come at a price, as as far as I've read, launching each kernel would take around some 10 microseconds and probably more if the launch code is written in Python.\n. @oleg-trott OMFG! That's exactly what I've been researching for the last month! I've looked for similar papers a month ago, but couldn't find any. I've used these things half a year before in my project with the shitty Caffe implementation of groups and they gave a pretty large accuracy increase, although, speed suffered (due to how shittily they're made in Caffe), so I recently decided to experiment with them on benchmark datasets. Because, obviously, they would also be useful to others. And to try them on engines that would compute them more efficiently, so I can try using larger groupings and wider structures. Thanks a lot! Now I know that my ideas were correct. =) \nOne of the reasons why I tried running with a very small weight decay and why I've said that they learn faster (most likely, because of lack of the said decay) is that highly-grouped networks I was previously running didn't seem to benefit much from weight decay.\n. So now that it's clear how important groups are, I hope Neon devs will prioritize this higher.\n. https://github.com/soumith/cudnn.torch/blob/master/SpatialConvolution.lua So the way Torch's cudnn module works is actually the same as I described, launching few kernels consecutively. So it turns out that that wasn't a very good idea. It works well only for small amounts of groups, and we need fast 64+ groups. If you'll be writing that in Python that will work even much worse than in Torch, because CPython is an extremely-poor-speed interpreter and Torch's LuaJIT is a fast JIT.\n. @oleg-trott To my understanding both gemm and dot is not any good for convolutions, especially on GPU (but I may be wrong.. in that it may be even worse on CPU compared to more optimal ways than the difference on GPU) and is only used by people who know too much linear algebra and are too lazy to learn CUDA. Luckily, Neon's implementations are made purely and optimally for the convs on GPU.\n. Regarding profiling, with these guys in the paper cutting convs into 64 groups without even making them wider, I guess, no more profiling is required, right? Seems fairly obvious that you need to change the kernel to do that right.\n. @oleg-trott If by other uses you mean for example grouped linear, I'm totally subscribing. =)\nRegarding im2col, yeah, that's what I'm talking about. When I was searching for a framework with clear code where I could easily implement a grouped kernel myself I bumped into plenty of these im2cols in all of the different frameworks. Still haven't a framework with clear code, btw. I decided to make my own at one point, so after thinking and researching a bit on what things to use and how to implement it I ended up considering making it with optimizing compiled kernels in PyCUDA on Python, & because CPU implementation for training would be an unnecessary waste of time, & because directly interfacing C++/CUDA code from any high-level language is pure hell & even more so if you also want to integrate it with Py's Numpy. So in the end it looked as if I've made all of the same decisions as Nervana guys and that's basically why I'm here. That's why I've said above that Nervana should seriously consider making a very clear guide on source tinkering. Because, they've already made all of the decisions needed to become the primary research engine of NN and they're the only serious ones who did that.\nSo as I was talking about im2col. I really struggled understanding what it means at first. I did know CUDA and it looked too awful to be true. I looked it up on the internet, but it didn't fit into my head that people could seriously consider implementing such a stupidity for conv computation in serious frameworks. But this paper https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/ got me convinced they actually do. Luckily, even the old CuDNN circa 2014 didn't use it http://arxiv.org/pdf/1410.0759.pdf , though they may have went some more fancy and less productive way than the plain obvious convolutional one would be and they acknowledge that, showing around 30% worse efficiency than Alex's plain conv, but theirs seemed to perform better for smaller batches, which is not actually something too popular now anyway, although smaller batches do give better results.\nBut yeah, if you're talking about linear layers, BLAS is probably a way to go. IMHO, it would be something you could easily code yourself instead of requesting if only there were good docs on contributing. All of these frameworks are open source, but when you actually open the source you instantly know you can't change anything. There's a serious lack of an easily editable framework. And that one would be the winner, because all of the time the NN's are constantly evolving and you either have to keep up with that, throwing 20h into what could be done by some 5 lines of code if your code was actually not a candidate for IOCCC (yes, I'm referring to the current issue), or you can be the one to help with the evolving, be the engine for NN research and get all implementations straight from the authors without any effort on your part and be the framework cited in publications.\nNo, I understand that Neons SASS kernels with groups will be faster than C++ kernels with groups. It's just that if the original authors of the paper could have access to C++ kernels when they wrote the paper, meddle with code a bit, change some 5-10 lines and get them working in groups, it would help them a lot and could lead them in different directions. Also, the numbers would be much different. If they didn't have like a 100 grand gear, that they luckily seem to have had, it could be a question of life and death of their research.\n. On the other hand, for a CPU, maybe, im2col would be relatively appropriate for non-, or little grouped convs at least for large feature map sizes. Needs testing and comparing to say for sure.\n. @oleg-trott regarding an Intel company making optimal kernels, one funny reason to do that would be to cut demand for more and higher-cost-margin GPUs by making fewer cheaper ones \"good enough\". Currently there's a certain wall on what performance boost you can get with a bigger model. And there's also a wall related to how large can a model put into production be.\n. Turns out grouped convs are available on TensorFlow as depthwise_conv2d https://www.tensorflow.org/versions/r0.10/api_docs/python/nn.html#depthwise_conv2d . Fully-grouped only, though.\n. https://arxiv.org/pdf/1611.05431.pdf\nSo grouped convs are now officially the next step in ResNet.. @revilokeb Sadly, I'm guessing that the answer lies between Nan, Undefined, {} and []. NVidia guys emailed me that \"This feature has been on our radar and we\u2019ll be introducing it in a future release of cuDNN.\" but I'm not 100% sure if it's actually going to be a part of CuDNN v7.. @revilokeb As for the CPU implementations, I don't know of any good CPU implementations in C/C++ either, though it should probably take no more than a few days to develop an implementation for a person who knows how to integrate a module like that into some existing framework. I tried doing this once and I did make a very efficient C++ implementation for fully-grouped convs, but the process of integrating it with any existing framework was too opaque for me. Also, going fully-grouped (max amount of groups) is probably not the best idea.. ",
    "revilokeb": "@ibmua do you happen to have a comparison what framework currently is supporting grouped convs efficiently?. @ibmua :-) ok interesting thanks!! Based on benchmarks of mobilenet in TF here it seems that their implementation of grouped convs probably is at least superior to current pytorch.... ",
    "futurely": "What\u2019s New in cuDNN 7?\nGrouped Convolutions for models such as ResNeXt and Xception and CTC (Connectionist Temporal Classification) loss layer for temporal classification\nhttps://developer.nvidia.com/cudnn. ",
    "LiyuCode": "Sorry for the trouble.\nArgs \"inner_size\" and \"scale_range\" has no default value defined in ImageLoader, so one needs to pass them to init a ImageLoad instance. One way to do it is add these two args into \"img_set_options\", after this the error gone.\n. ",
    "EnriqueSMarquez": "Thanks Hanlin for your reply. It worked indeed. I changed the Multistream with a Merge Broadcast, since the input is the same.\nCheers,\nEnrique Marquez\n. ",
    "pchankh": "Thanks for the help. That solves the problem.  Maybe we should put that installation instructions into the installation step too. Thanks.\nPatrick\n. ",
    "Pavel-Konarik": "Thanks for the reply. The output is 0.\n(.venv2) root@a:/home/ai/ai/neon# nvcc neon/backends/util/check_gpu.c && ./a.out; echo $?\n0\n. Thanks for the help\n(.venv2) root@a:/home/ai/ai/neon# ./neon/backends/util/check_gpu.py\nDISPLAY:neon:0\n(.venv2) root@a:/home/ai/ai/neon#\nI have no idea what went wrong. \nDoes the Cuda toolkit need to be installed before the neon make? I am quite sure I had it correctly set up beforehand, but I am out of ideas.\n. ",
    "bitstormGER": "I can confirm that building pycuda from source (https://github.com/inducer/pycuda) solved the problem for me. Configuration CUDA 8 / nvidia driver 370.28 / ArchLinux\n. ",
    "klezmen": "I'm in the same position, but when i do:\n./neon/backends/util/check_gpu.py\ni get 0\nWhat does this mean? My gpu is not suitable for Neon? \n. ",
    "Clayton-Davis": "@Cpruce try this\nhad the same issue, but fixed it with a much easier solution than recompiling pycuda.\ncheck_gpu.py was returning DISPLAY:neon:0.0\nwhen I ran the import suggestion that @anlthms posted, I got an ImportError for libcurand.so.8.0 \nI fixed it by setting the LD_LIBRARY_PATH environment variable with export on the command line like this:\nexport LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64/\nnow check_gpu.py returns DISPLAY:neon:5.0\nyou have to add the export command to the .bashrc file in your home directory if you want to make the export permanent. ",
    "sbass301": "I have the same issue:\nerror: argument -b/--backend: invalid choice: 'gpu' (choose from 'cpu')\nnvcc neon/backends/util/check_gpu.c && ./a.out; echo $?\nreturns: 0\n./neon/backends/util/check_gpu.py\nreturns: DISPLAY:neon:3.7\nAll this after iinstalling pycuda from source... \nCuda 8.0 toolset\nLD_LIBRARY_PATH=LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64\nI am running on a Google Compute Engine instance w/ Nvidia Tesla K80\nOS = Ubuntu 16.04\nAny other ideas on what might be wrong here?   Thanks\n. ",
    "iNomaD": "Device 0 does not have CUDA compute capability 3.0 or greater\nIs there any way to use neon with 2.1 capability GPU?. Thank you, @wei-v-wang \nI made it work with zip release and sent my solution as a pull request https://github.com/NervanaSystems/neon/pull/407. ",
    "weibogit": "I had tried @anlthms  solution, but it still has some problems. I can use neon in terminal, but in Spyder IDE, it can not import neon. And, I have tried to \"source activate\". So, please give me a help. Details as follow scripts.\n```\n#####edit Makefile as you(@anlthms) said above\nweibo@ubuntu:/opt/neon$ cd /opt/neon/loader\nweibo@ubuntu:/opt/neon/loader$ sudo vim Makefile       # add \"INC.....\"line\nweibo@ubuntu:/opt/neon/loader$ cd /opt/neon/\n###### install neon with python2\nweibo@ubuntu:/opt/neon$ sudo make python2\nUpdating virtualenv dependencies in: .venv2...\nRunning virtualenv with interpreter /usr/bin/python2.7\nNew python executable in /opt/neon/.venv2/bin/python2.7\nAlso creating executable in /opt/neon/.venv2/bin/python\nInstalling setuptools, pip, wheel...done.\nThe directory '/home/weibo/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\nThe directory '/home/weibo/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\nRequirement already up-to-date: pip in ./.venv2/lib/python2.7/site-packages\nThe directory '/home/weibo/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\nThe directory '/home/weibo/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\nCollecting cython==0.23.1\n  Downloading Cython-0.23.1-cp27-cp27mu-manylinux1_x86_64.whl (5.6MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.6MB 150kB/s \nInstalling collected packages: cython\nSuccessfully installed cython-0.23.1\nThe directory '/home/weibo/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\nThe directory '/home/weibo/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\nCollecting configargparse==0.10.0 (from -r requirements.txt (line 1))\n  Downloading ConfigArgParse-0.10.0.tar.gz\nCollecting numpy==1.11.1 (from -r requirements.txt (line 2))\n  Downloading numpy-1.11.1-cp27-cp27mu-manylinux1_x86_64.whl (15.3MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15.3MB 63kB/s \nCollecting pyyaml==3.12 (from -r requirements.txt (line 3))\n  Downloading PyYAML-3.12.tar.gz (253kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 256kB 949kB/s \nCollecting pep8==1.7.0 (from -r requirements.txt (line 4))\n  Downloading pep8-1.7.0-py2.py3-none-any.whl (41kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 2.0MB/s \nCollecting flake8==3.0.4 (from -r requirements.txt (line 5))\n  Downloading flake8-3.0.4-py2.py3-none-any.whl (64kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 1.5MB/s \nCollecting pytest==3.0.1 (from -r requirements.txt (line 6))\n  Downloading pytest-3.0.1-py2.py3-none-any.whl (169kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 174kB 2.2MB/s \nCollecting pytest-cov==2.3.1 (from -r requirements.txt (line 7))\n  Downloading pytest_cov-2.3.1-py2.py3-none-any.whl\nCollecting posix_ipc==1.0.0 (from -r requirements.txt (line 8))\n  Downloading posix_ipc-1.0.0.tar.gz (74kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 81kB 981kB/s \nCollecting pillow==3.3.1 (from -r requirements.txt (line 9))\n  Downloading Pillow-3.3.1-cp27-cp27mu-manylinux1_x86_64.whl (5.6MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.6MB 148kB/s \nCollecting pylint==1.6.4 (from -r requirements.txt (line 10))\n  Downloading pylint-1.6.4-py2.py3-none-any.whl (569kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 573kB 305kB/s \nCollecting sphinx==1.4.6 (from -r requirements.txt (line 11))\n  Downloading Sphinx-1.4.6-py2.py3-none-any.whl (1.6MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.6MB 334kB/s \nCollecting h5py==2.6.0 (from -r requirements.txt (line 12))\n  Downloading h5py-2.6.0-1-cp27-cp27mu-manylinux1_x86_64.whl (4.2MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.2MB 194kB/s \nCollecting appdirs==1.4.0 (from -r requirements.txt (line 13))\n  Downloading appdirs-1.4.0-py2.py3-none-any.whl\nCollecting future==0.15.2 (from -r requirements.txt (line 14))\n  Downloading future-0.15.2.tar.gz (1.6MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.6MB 124kB/s \nCollecting enum34; python_version < \"3.4\" (from flake8==3.0.4->-r requirements.txt (line 5))\n  Downloading enum34-1.1.6-py2-none-any.whl\nCollecting configparser; python_version < \"3.2\" (from flake8==3.0.4->-r requirements.txt (line 5))\n  Downloading configparser-3.5.0.tar.gz\nCollecting mccabe<0.6.0,>=0.5.0 (from flake8==3.0.4->-r requirements.txt (line 5))\n  Downloading mccabe-0.5.2-py2.py3-none-any.whl\nCollecting pycodestyle<2.1.0,>=2.0.0 (from flake8==3.0.4->-r requirements.txt (line 5))\n  Downloading pycodestyle-2.0.0-py2.py3-none-any.whl (42kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 816kB/s \nCollecting pyflakes!=1.2.0,!=1.2.1,!=1.2.2,<1.3.0,>=0.8.1 (from flake8==3.0.4->-r requirements.txt (line 5))\n  Downloading pyflakes-1.2.3-py2.py3-none-any.whl (209kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 215kB 379kB/s \nCollecting py>=1.4.29 (from pytest==3.0.1->-r requirements.txt (line 6))\n  Downloading py-1.4.31-py2.py3-none-any.whl (81kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 789kB/s \nCollecting coverage>=3.7.1 (from pytest-cov==2.3.1->-r requirements.txt (line 7))\n  Downloading coverage-4.2.tar.gz (359kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 368kB 449kB/s \nCollecting six (from pylint==1.6.4->-r requirements.txt (line 10))\n  Downloading six-1.10.0-py2.py3-none-any.whl\nCollecting astroid<1.5.0,>=1.4.5 (from pylint==1.6.4->-r requirements.txt (line 10))\n  Downloading astroid-1.4.8-py2.py3-none-any.whl (213kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 215kB 662kB/s \nCollecting isort>=4.2.5 (from pylint==1.6.4->-r requirements.txt (line 10))\n  Downloading isort-4.2.5-py2.py3-none-any.whl (40kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40kB 2.3MB/s \nCollecting backports.functools-lru-cache; python_version == \"2.7\" (from pylint==1.6.4->-r requirements.txt (line 10))\n  Downloading backports.functools_lru_cache-1.2.1-py2.py3-none-any.whl\nCollecting babel!=2.0,>=1.3 (from sphinx==1.4.6->-r requirements.txt (line 11))\n  Downloading Babel-2.3.4-py2.py3-none-any.whl (7.1MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7.1MB 129kB/s \nCollecting imagesize (from sphinx==1.4.6->-r requirements.txt (line 11))\n  Downloading imagesize-0.7.1-py2.py3-none-any.whl\nCollecting Pygments>=2.0 (from sphinx==1.4.6->-r requirements.txt (line 11))\n  Downloading Pygments-2.1.3-py2.py3-none-any.whl (755kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 757kB 383kB/s \nCollecting docutils>=0.11 (from sphinx==1.4.6->-r requirements.txt (line 11))\n  Downloading docutils-0.12.tar.gz (1.6MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.6MB 349kB/s \nCollecting alabaster<0.8,>=0.7 (from sphinx==1.4.6->-r requirements.txt (line 11))\n  Downloading alabaster-0.7.9-py2.py3-none-any.whl\nCollecting Jinja2>=2.3 (from sphinx==1.4.6->-r requirements.txt (line 11))\n  Downloading Jinja2-2.8-py2.py3-none-any.whl (263kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 266kB 619kB/s \nCollecting snowballstemmer>=1.1 (from sphinx==1.4.6->-r requirements.txt (line 11))\n  Downloading snowballstemmer-1.2.1-py2.py3-none-any.whl (64kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 1.5MB/s \nCollecting lazy-object-proxy (from astroid<1.5.0,>=1.4.5->pylint==1.6.4->-r requirements.txt (line 10))\n  Downloading lazy_object_proxy-1.2.2-cp27-cp27mu-manylinux1_x86_64.whl (56kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 1.2MB/s \nCollecting wrapt (from astroid<1.5.0,>=1.4.5->pylint==1.6.4->-r requirements.txt (line 10))\n  Downloading wrapt-1.10.8.tar.gz\nCollecting pytz>=0a (from babel!=2.0,>=1.3->sphinx==1.4.6->-r requirements.txt (line 11))\n  Downloading pytz-2016.6.1-py2.py3-none-any.whl (481kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 481kB 573kB/s \nCollecting MarkupSafe (from Jinja2>=2.3->sphinx==1.4.6->-r requirements.txt (line 11))\n  Downloading MarkupSafe-0.23.tar.gz\nInstalling collected packages: configargparse, numpy, pyyaml, pep8, enum34, configparser, mccabe, pycodestyle, pyflakes, flake8, py, pytest, coverage, pytest-cov, posix-ipc, pillow, six, lazy-object-proxy, wrapt, astroid, isort, backports.functools-lru-cache, pylint, pytz, babel, imagesize, Pygments, docutils, alabaster, MarkupSafe, Jinja2, snowballstemmer, sphinx, h5py, appdirs, future\n  Running setup.py install for configargparse ... done\n  Running setup.py install for pyyaml ... done\n  Running setup.py install for configparser ... done\n  Running setup.py install for coverage ... done\n  Running setup.py install for posix-ipc ... done\n  Running setup.py install for wrapt ... done\n  Running setup.py install for docutils ... done\n  Running setup.py install for MarkupSafe ... done\n  Running setup.py install for future ... done\nSuccessfully installed Jinja2-2.8 MarkupSafe-0.23 Pygments-2.1.3 alabaster-0.7.9 appdirs-1.4.0 astroid-1.4.8 babel-2.3.4 backports.functools-lru-cache-1.2.1 configargparse-0.10.0 configparser-3.5.0 coverage-4.2 docutils-0.12 enum34-1.1.6 flake8-3.0.4 future-0.15.2 h5py-2.6.0 imagesize-0.7.1 isort-4.2.5 lazy-object-proxy-1.2.2 mccabe-0.5.2 numpy-1.11.1 pep8-1.7.0 pillow-3.3.1 posix-ipc-1.0.0 py-1.4.31 pycodestyle-2.0.0 pyflakes-1.2.3 pylint-1.6.4 pytest-3.0.1 pytest-cov-2.3.1 pytz-2016.6.1 pyyaml-3.12 six-1.10.0 snowballstemmer-1.2.1 sphinx-1.4.6 wrapt-1.10.8\nInstalling neon in development mode...\nrunning develop\nrunning egg_info\ncreating neon.egg-info\nwriting neon.egg-info/PKG-INFO\nwriting top-level names to neon.egg-info/top_level.txt\nwriting dependency_links to neon.egg-info/dependency_links.txt\nwriting manifest file 'neon.egg-info/SOURCES.txt'\nreading manifest file 'neon.egg-info/SOURCES.txt'\nreading manifest template 'MANIFEST.in'\nwriting manifest file 'neon.egg-info/SOURCES.txt'\nrunning build_ext\nCreating /opt/neon/.venv2/lib/python2.7/site-packages/neon.egg-link (link to .)\nAdding neon 1.6.0 to easy-install.pth file\nInstalling neon script to /opt/neon/.venv2/bin\nInstalling nvis script to /opt/neon/.venv2/bin\nInstalled /opt/neon\nProcessing dependencies for neon==1.6.0\nFinished processing dependencies for neon==1.6.0\n\nSetup complete.  Type:\n    . '.venv2/bin/activate'\nto work interactively (.venv2 also symlinked to .venv)\n\nmake[1]: Entering directory '/opt/neon/loader'\nBuilding bin/loader.so...\ng++ -shared -o bin/loader.so -fPIC -Wall -Wno-deprecated-declarations -O3 -std=c++11  -DHAS_IMGLIB   src/loader.cpp -I/usr/local/include/opencv -I/usr/local/include -L/usr/local/lib  -lopencv_calib3d -lopencv_contrib -lopencv_core -lopencv_features2d -lopencv_flann -lopencv_gpu -lopencv_highgui -lopencv_imgproc -lopencv_legacy -lopencv_ml -lopencv_nonfree -lopencv_objdetect -lopencv_ocl -lopencv_photo -lopencv_stitching -lopencv_superres -lopencv_ts -lopencv_video -lopencv_videostab -lrt -lpthread -lm -ldl \nmake[1]: Leaving directory '/opt/neon/loader'\nweibo@ubuntu:/opt/neon$ . .venv/bin/activate\n(.venv2) weibo@ubuntu:/opt/neon$ sudo neon examples/mnist_mlp.yaml\nsudo\uff1aneon: command not found\n(.venv2) weibo@ubuntu:/opt/neon$ neon examples/mnist_mlp.yaml\n2016-09-30 11:16:37,271 - neon - DISPLAY - Downloading file: ./mnist.pkl.gz\nTraceback (most recent call last):\n  File \"/opt/neon/.venv2/bin/neon\", line 6, in \n    exec(compile(open(file).read(), file, 'exec'))\n  File \"/opt/neon/bin/neon\", line 147, in \n    train, test = load_data(data_dir=args.data_dir)\n  File \"/opt/neon/bin/neon\", line 126, in load_data\n    dataiters = dataset.gen_iterators()\n  File \"/opt/neon/neon/data/image.py\", line 78, in gen_iterators\n    (X_train, y_train), (X_test, y_test), nclass = self.load_data()\n  File \"/opt/neon/neon/data/image.py\", line 64, in load_data\n    self.fetch_dataset(self.url, self.filename, filepath, self.size)\n  File \"/opt/neon/neon/data/datasets.py\", line 137, in fetch_dataset\n    with open(destfile, 'wb') as f:\nIOError: [Errno 13] Permission denied: './mnist.pkl.gz'\n####### in terminal, there is no error report\n**(.venv2) weibo@ubuntu:/opt/neon$ python\nPython 2.7.9 (default, Apr  2 2015, 15:33:21) \n[GCC 4.9.2] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport neon\nhelp(neon)**\n\n\n\n######## in spyder IDE\nPython 2.7.9 (default, Apr  2 2015, 15:33:21) \n[GCC 4.9.2] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport neon\nTraceback (most recent call last):\n  File \"\", line 1, in \nImportError: No module named neon\n\n\n\n```\n. ",
    "karthikmswamy": "Thank you @hanlin-nervana for the quick update with the model file. I'm downloading it.\nCan you share some timing information about this model? Thanks in advance.\n. Thanks a lot for the quick updates, @hanlin-nervana. This should help with our benchmarks.\n. ",
    "ternaus": "Thank you. This solved the problem.\n. ",
    "loofahcus": "Now I think it's a bug in timeseries_lstm.py. \nIn timeseries_lstm.py#L200, it should be modified from:\nX_batch = self.X[:, self.batch_index].reshape(\n    self.X_dev.shape[::-1]).T.copy()\ny_batch = self.y[:, self.batch_index].reshape(\n    self.y_dev.shape[::-1]).T.copy()\nto:\nX_batch = self.X[:, self.batch_index].T.reshape(\n    self.X_dev.shape).copy()\ny_batch = self.y[:, self.batch_index].T.reshape(\n    self.y_dev.shape).copy()\nWhen the batch_size is 1, it does not matter, but otherwise it hurts.\n. ",
    "saeedaghabozorgi": "anlthms, thanks for fast reply.\n1) ffmpeg is already installed:\n\n2) opencv is already installed\n\n3) I am installing Neon using Conda, adn here is the error:\n\n. ",
    "Nr90": "Thank you for the quick response!\n. ",
    "ronilp": "Will Nvidia K80 GPUs work with faster rcnn?. ",
    "a62153232": "@anlthms ,thank you very much for your answer. The DataLoader works perfectly after i have installed ffmpeg.\n. I found that the error results from CSV file.\nThe CSV file looks same,but results are different. Why is it\uff1f\n. ",
    "TensorTom": "Great, that fixed it. Just had to install gcc-4.9 package, 'sudo ln -s /usr/bin/gcc-4.9 /usr/local/cuda/bin/gcc' then install g++4.9 package and now training super fast on my Titan! Thanks.\n. Well, for instance, I'm planning to do a lot of work with RNNs. The first example I ran was the Shakespeare text example. I ran python text_generation_lstm.py -b gpu . I was watching a movie at the same time so prior to running it my GPU was at around 5-10%. While training, it stayed around 25% and took about 1 minute 5 seconds for each epoch. I know I could blow through the training if I could utilize most my card.. ",
    "L-CY": "Great. That's what I want. Thanks.\n. I'll see how it works. Thanks.\n. ",
    "binarybana": "We have used one successfully in the past. Not sure about available documentation, if I find any I'll let you know.. @swapnilsj I pulled up a few pointers from another engineer, hope this helps:\nInstall JetPack\napt-get install git python-virtualenv python-dev libyaml-dev libhdf5-dev\npycuda\napt-get install libboost-all-dev\nfollow configure, make, and install instructions on website\ngit clone https://github.com/NervanaSystems/neon\nmake sysinstall\n2 ** 31 -2 in nervanagpu.py instead of 2**32-1. ",
    "atlury": "Thanks from my side too ...shall try it out. ",
    "rkimballn1": "Neon 1.7 was recently released with an all-new dataloader and you may want to give it a try. Go through all of the prerequisites for the new install.. ",
    "devareddy": "We are using Tesla K80. ",
    "tianqig": "Hi ursk, from your suggested url, I found an easy way to try  is to set: \nPYTHONHTTPSVERIFY=0 \nI added this statement in .bash_profile , somehow my problem remained. Then I found the following  website: Certificate verification in Python standard library HTTP clients, then I tried\n $ PYTHONHTTPSVERIFY=0  python examples/mnist_mlp.py\nNo more errors for the mnist demo. Not sure why the change of my .bash_profile did not fix my problem.\nthank ursk for quick reply and suggestions. BTW, I checked my urllib2 worked no problem.\n. ",
    "tyler-nervana": "Could you clarify what you mean? What does your dataset look like?. ",
    "sdvillal": "Perfect. Thanks Jennifer.. ",
    "HengjLi": "You can run ./tests/run_benchmarks.py to generate those cubin files. Those cubin files are stored under ~/.cache/neon/kernels/cubin. You should also change the dir in test.cu before you compile&run it.\nAlthough I have the cubin files needed in nervana_c_api.cu, I still meet a cabin file loading problem when invoking cuModuleLoad. The function returns CUDA_ERROR_FILE_NOT_FOUND.\nIt seems not the problem that the cuda arch is not matching. For those cubin files are generated by kernel_specs.py automatically. So I want to know why.. ",
    "Bonsen": "@apark263 \nIt works.\nTHANKS. ",
    "galactica147": "Thanks! but i got some errors passing the '-b gpu' option: \nUsage:\n    python examples/cifar10_conv.py\n: error: argument -b/--backend: invalid choice: 'gpu' (choose from 'cpu')\ncould you please demonstrate a bit more on how to specify the backend with gen_backend? . Thanks @ursk! that calls for installing packages in gpu_requirement.txt and i got lots of errors when installing pycuda, which depends on cudalibararies to begin with. Is there any easy way to get around this? . @ursk Thanks! I'll see if that's doable at my env. closing ticket for now.. ",
    "mas-dse-greina": "In env.sh, I changed line 60 from:\nexport CFLAGS=\"-Wno-deprecated-declarations -std=c++11\"\nto \nexport CFLAGS=\"-Wno-deprecated-declarations -std=c++11 -stdlib=libc++\"\nThen ran the install file:\nsudo python setup.py install\nThat seemed to solve the error for me.\nBest,\n-Tony\n. It looks like this works:\n\ngit clone https://github.com/01org/mkl-dnn.git\ncd mkl-dnn/scripts && ./prepare_mkl.sh && cd ..\nmkdir -p build && cd build && cmake .. && make\nsudo make install\necho 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib' >> ~/.bashrc\n\nMaybe the .tar.gz file that is being included with neon needs to be updated??\n. Thanks!\n-Tony\nOn Fri, Jan 26, 2018 at 2:54 PM, Wei Wang notifications@github.com wrote:\n\nWe'll fix our code on the URL\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/438#issuecomment-360927312,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AVReEkyDX2-endS1m3gqreZiMG2B1IO7ks5tOledgaJpZM4RqlHx\n.\n. \n",
    "pabutcher": "Thanks, this has been driving me nuts! \nPerhaps the env.sh can be updated so others don't run into the problem?\nCheers,\nPeter. ",
    "guoxuesong": "I just make my project public: deepstacks\ndeepstacks is: A build_network() for Lasagne and noen. Define your network model in a datacheet with stack machine mechanisms. Support reuse part of model as function, and share parameters.\nPlease have a look at  deepstacks/deepstacks/neon/implement.py\nTo complete the implement for neon, I need ( in the words of Lasagne): ElemwiseMergeLayer,SliceLayer,Upscale[123]DLayer,LocallyConnected[123]DLayer,DimshuffleLayer,GaussianNoiseLayer,ExpressionLayer. Leave them not implemented is ok, but I want to complete it if posible.\nI wish my project can help more peaple to take advantage of neon. Though, myself is new to neon, so if any part of may code is wrong, just let me known.. @chengchingwen would you please explain these lines for me ? in bprop:\n    if self.deltas:\n        self.be.compound_dot(A=self.W.T, B=error, C=self.deltas, alpha=alpha, beta=beta)\n    self.be.compound_dot(A=error, B=self.inputs.T, C=self.dW). I tried to implement a [GaussianNoiseLayer](https://github.com/Lasagne/Lasagne/blob/master/lasagne/layers/noise.py#L169), following is my code. I'm not sure whether this is correctly, I does not really understand the alpha, beta things, just copied them from SkipNode:\n\n```\nclass GaussianNoiseLayer(Layer):\n    def init(self, sigma=0.1, name=None):\n        super(GaussianNoiseLayer, self).init(name)\n        self.sigma = sigma\n        self.owns_delta = True\n        self.is_mklop = True\ndef fprop(self, inputs=None, inference=False, beta=0):\n    self.be.fill_normal(self.noisebuf, stdv=self.sigma)\n    self.be.fprop_skipnode(inputs, self.outputs, beta)\n    self.outputs[:] = self.outputs + self.noisebuf\n    return self.outputs\n\ndef configure(self, in_obj):\n    super(GaussianNoiseLayer, self).configure(in_obj)\n    self.out_shape = self.in_shape\n\n    self.noisebuf = self.be.iobuf(self.in_shape, dtype=np.float32)\n    # self.noisebuf = self.be.iobuf(self.in_shape)\n    return self\n\ndef bprop(self, error, alpha=1.0, beta=0.0):\n    # for better performance, mkl do nothing\n    # otherwise, convert back and deal with beta and alpha.\n    self.be.bprop_skipnode(error, self.deltas, alpha, beta)\n    return self.deltas\n\n```. ",
    "chengchingwen": "there is a neon tutorial\nmaybe you can take a look at this one. @guoxuesong \nin bprop, self.deltas is the error need to be back prop to the previous layer, alpha & beta is just parameters of self.be.compound_dot, you may want to take a look at the doc.\nI guess it just say take the dot product of self.W.T & error and assign to self.deltas if self.deltas is not set, and compute the dW every time the bprop is called. . ",
    "elena-orlova": "I've found the solution\nhttps://github.com/NervanaSystems/neon/issues/142\nThank you anyway!. ",
    "Quallyjiang": "thanks!. ",
    "anonymous-u": "Ok. Thanks for your response.\nI read that MKL is for CPU .\nHow about NVidia Tegra TK1, which has Kepler GPU ?\nDoes it make any difference?\n. Could you tell me how to disable MKL component?\nAnd Tegra TK1 has CUDA v6.5. Is there any cuda requirement? \n. ",
    "meysam-madadi": "Hi,\nFor anybody who has the same problem. I modified ArrayIterator init function as follows and it is able to receive a list of lshapes.\ndef __init__(self, X, y=None, nclass=None, lshape=None, make_onehot=True, name=None):\n    \"\"\"\n    During initialization, the input data will be converted to backend tensor objects\n    (e.g. CPUTensor or GPUTensor). If the backend uses the GPU, the data is copied over to the\n    device.\n\n    Args:\n        X (ndarray, shape: [# examples, feature size]): Input features of the\n            dataset.\n        y (ndarray, shape:[# examples, 1 or feature size], optional): Labels corresponding to\n            the input features. If absent, the input features themselves will be returned as\n            target values (e.g. autoencoder)\n        nclass (int, optional): The number of classes in labels. Not necessary if\n            labels are not provided or where the labels are non-categorical.\n        lshape (tuple, optional): Local shape for the input features\n            (e.g. # channels, height, width)\n        make_onehot (bool, optional): True if y is a categorical label that has to be converted\n            to a one hot representation.\n\n    \"\"\"\n    # Treat singletons like list so that iteration follows same syntax\n    super(ArrayIterator, self).__init__(name=name)\n    X = X if isinstance(X, list) else [X]\n    self.ndata = len(X[0])\n    assert self.ndata >= self.be.bsz\n    self.start = 0\n    self.nclass = nclass\n    self.ybuf = None\n\n    if make_onehot and nclass is None and y is not None:\n        raise AttributeError('Must provide number of classes when creating onehot labels')\n\n    # if labels provided, they must have same # examples as the features\n    if y is not None:\n\n        assert all([y.shape[0] == x.shape[0] for x in X]), \\\n            \"Input features and labels must have equal number of examples.\"\n\n        # for classifiction, the labels must be from 0 .. K-1, where K=nclass\n        if make_onehot:\n            assert y.max() <= nclass - 1 and y.min() >= 0, \\\n                \"Labels must range from 0 to {} (nclass-1).\".format(nclass - 1)\n\n            assert (np.floor(y) == y).all(), \\\n                \"Labels must only contain integers.\"\n\n    # if local shape is provided, then the product of lshape should match the\n    # number of features\n    if lshape is not None:\n        lshape = lshape if isinstance(lshape, list) else [lshape]\n        assert len(lshape)==len(X)\n        assert all([x.shape[1] == np.prod(s) for (x,s) in zip(X,lshape)]), \\\n            \"product of lshape {} does not match input feature size\".format(lshape)\n\n    # store shape of the input data\n    self.shape = [x.shape[1] if lshape is None else s for s in lshape]\n    if len(self.shape) == 1:\n        self.shape = self.shape[0]\n        self.lshape = lshape\n\n. ",
    "baojun-nervana": "It seems I cannot reproduce the error\n(.venv2) [baojunli@nervana-titanxp05 private-neon]$ perl -v\nThis is perl 5, version 26, subversion 1 (v5.26.1) built for x86_64-linux\n(with 1 registered patch, see perl -V for more detail)\nCopyright 1987-2017, Larry Wall\nPerl may be copied only under the terms of either the Artistic License or the\nGNU General Public License, which may be found in the Perl 5 source kit.\nComplete documentation for Perl, including FAQ lists, should be found on\nthis system using \"man perl\" or \"perldoc perl\".  If you have access to the\nInternet, point your browser at http://www.perl.org/, the Perl Home Page.\n(.venv2) [baojunli@nervana-titanxp05 private-neon]$ python examples/mnist_mlp.py -b gpu\nEpoch 0   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  469/469  batches, 0.28 cost, 1.04s]\nEpoch 1   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  469/469  batches, 0.22 cost, 1.06s]\nEpoch 2   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  469/469  batches, 0.19 cost, 1.06s]\nEpoch 3   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  468/468  batches, 0.16 cost, 1.05s]\nEpoch 4   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  468/468  batches, 0.15 cost, 1.05s]\nEpoch 5   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  468/468  batches, 0.13 cost, 1.05s]\nEpoch 6   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  468/468  batches, 0.12 cost, 1.05s]\nEpoch 7   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  468/468  batches, 0.11 cost, 1.05s]\nEpoch 8   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  468/468  batches, 0.10 cost, 1.06s]\nEpoch 9   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  468/468  batches, 0.09 cost, 1.05s]\n2017-11-17 14:50:09,167 - neon - DISPLAY - Misclassification error = 2.5%. Can you upgrade to Neon2.2 or 2.3? It sounds a numpy version conflict.. which platform are you running? Can you specify the backend as \"-b mkl\" or \"-b cpu\"?\n. I just tested the example and it supports three backends - gpu, mkl and cpu. I am using the latest version v2.3.0 (we just released the newest version last Friday). \nYou may try to download the new version and build again, and pay attention if there is any error during the build.. Thanks for the feedback. We will look into cleaning up the dependencies. . thanks for the feedback. Looking into it.. @paritoshsaxena   The examples are meant to be references to users. Would you mind downloading them separately?\n. @armando-fandango Some issue for python3 sysinstall is due to the system having default python2.\nHave you tried python3 system install under a python3 virtualenv?. @armando-fandango Sorry for the confusion. When I said \"python3 virtualenv\", I mostly meant to have a system / environment with python3 as default. In that case, paths and links will be ready for python3, e.g. pip will have a softlink to pip3.  Virtualenv is an easy way to have that environment ready, and \"make sysinstall\" will work smoothly.\nI agree it is confusing to request virtualenv for sys install.. There is a parameter PY = 2 or 3 (line 129) which is mostly used to diff venv for python 2 and 3. We may expand that to differ python and pip command for python 2 and 3. \nit will be something like the following:\nifeq ($(PY), 2)\n    .......\n    PYTHON_EXE := python\n    PIP_EXE := pip\nelse\n    ........\n    PYTHON_EXE := python3\n    PIP_EXE := pip3\nendif\nIn the recipes, it can be called as $(PYTHON_EXE) and $(PIP_EXE).  The neon_install recipe will be like:\nneon_install:\n    @echo \"Installing neon system wide...\"\n    @$(PYTHON_EXE) setup.py install\n    @echo\nThis way it will be an integrated recipe for python2 and 3.\nTo run python3 sysinstall, it will be \nmake PY=3 sysinstall\n@armando-fandango Are you interested in updating the PR to merge the python2 and 3 sys install recipes?\n. @yangyang-zhang Thanks for the feedback. We will look into this.. @yangyang-zhang   You may install the gpu requirements if you want to run on gpu backend.\nhttps://github.com/NervanaSystems/neon/blob/master/gpu_requirements.txt\npip install pycuda, scikit-cuda, pytools. @armando-fandango Can you try to install the gpu dependencies?\npip install pycuda, scikit-cuda, pytool. It seems you don't have nvcc compiler installed or it is not set in the PATH. \nmake sure \"which nvcc\" return right path first.. It sounds it should work. Is the PATH set up right too?\nexport PATH=\"/usr/local/cuda/bin:\"$PATH\nexport LD_LIBRARY_PATH=\"/usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib:\"$LD_LIBRARY_PATH\n. would you try \nmake clean\nrm -rf mklml_lnx_* build \nWe are working on to update the \"make clean\" to clean up mkl artifacts.. did you try to delete the build folder as well?\nrm -rf mklml_lnx_* build. We will update \"make clean\" to remove the mkl artifacts for next release. Thanks.. thanks. We will look into it.. @armando-fandango Would you reference to https://github.com/NervanaSystems/neon/issues/428?\nYou may try\nmake clean\nrm -rf build mklml_lnx_*\nmake\n. @sangameshnr It seems you encountered a scenario which is currently not supported. The batchnorm reduced 1 dimension and created a shape mismatch. Would you be able to work around in a 2d domain?. @pantherso48   Did you try \"--serialize N\"?. @armando-fandango @pantherso48 I believe the \"--serialize N\" argument will do the same for local and cloud system. \nThe following example has log on accuracy. Hope it can help.\nhttps://github.com/NervanaSystems/neon/blob/master/examples/babi/train.py. @armando-fandango  I don't think there is a convenient way to save accuracy and loss info  the model parameters. The model usually has output on lost and accuracy. Can you extract those info from the output?. @sangameshnr  Do you have the similar issue with cpu backend (-b cpu)? I am wondering if this is mkl specific?. @sangameshnr  Thank you for the info. We are looking into it.. @sangameshnr We will include the fix in next release.. @moderato Thank you for the information. We will evaluate the option on this.  . @scigeek72 \nWhich backend do you use?\nBelow is an example I used to run skip-thought model. It used a small subset of data to demo it can train. (--subset_pct 0.1 means 0.1% of the dataset) \npython examples/skip-thought/train.py -e 2 -b mkl --serialize 1 --no_progress_bar -v -s examples/skip-thought/train.prm -l tmpfile.log  -w /dataset --subset_pct 0.1\n. @scigeek72 The example can run on either cpu or gpu.\nWith gpu backend, it takes about 3 hrs per epoch. For cpu mkl it will takes about 30hrs per epoch.. @mas-dse-greina It is good you can fix the issue by setting the library path. \nSometimes it may need to clean up before built. You may run\nmake clean   => this will clean up all the mkl artifacts\nmake install  => it will download mkl and do a fresh install.\n. @gabrielbriones Thanks. We will validate and merge to repo.. @gabrielbriones \nIt seems it runs fine on my system.\n$ python examples/cifar10.py \nEpoch 0   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  391/391  batches, 2.64 cost, 1.39s]\n. @c54852533 It seems there is issue with cuda v9. We are using v8.\n$ nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2016 NVIDIA Corporation\nBuilt on Tue_Jan_10_13:22:03_CST_2017\nCuda compilation tools, release 8.0, V8.0.61. @yataoz We don't have such a layer I believe.. @lixiangchun Below is an example. Hope it can help you.\nmanifest = [train:/dataset/aeon/V3D/ucf-extracted/train-index.csv, test:/dataset/aeon/V3D/ucf-extracted/test-index.csv]\nmanifest_root = /dataset/aeon/V3D/ucf-extracted\nbackend = gpu\nepochs = 10\nbatch_size = 32\neval_freq = 1\nlog = video-c3d.log\noutput_file = video-c3d.hdf5\ndevice_id = 0\ndata_dir = /dataset. That might be an issue related to configargparse version. That occurs on the newest version of the configargparse. The requirements.txt file recommends to use the following version.\nconfigargparse==0.10.0. @lixiangchun The example can run with GPU backend. What error did you see with gpu backend?\nyou might need to install the gpu dependencies.\nhttps://github.com/NervanaSystems/neon/blob/master/gpu_requirements.txt. @lixiangchun Are you using cuda9?\nI am using cuda8 and there was issue reported on cuda9.\n$nvcc --version                                              \u2502\u00b7\nnvcc: NVIDIA (R) Cuda compiler driver                                                                             \u2502\u00b7\nCopyright (c) 2005-2016 NVIDIA Corporation                                                                        \u2502\u00b7\nBuilt on Tue_Jan_10_13:22:03_CST_2017                                                                             \u2502\u00b7\nCuda compilation tools, release 8.0, V8.0.61. @zhiltsov-max Agreed. A type check is needed here.. @zhiltsov-max Thanks for your findings and recommendations. Appreciated!. @zhiltsov-max Thanks for the finding and improving neon.. ",
    "jamilbk": "Hmm that's strange. The example I originally ran, examples/mnist_mkl.py, is missing from the source tree and not present in the git history either.\nI can verify that python examples/mnist_mlp.py (and many other examples) successfully trains on all my systems with perl 5.26.1 with and without the -b gpu switch or -b mkl switch, but I don't think neon/backends/kernels/maxas/MaxAs/MaxAsGrammar.pm is being hit anymore with the stuff in examples/.\nIs the maxas library still being used? I'm not sure this MR matters any more if not.\nAnd thanks for checking into this :). I haven't tested it, but this was added to Perl 5.22 as a deprecation warning, and in 5.26 and 5.30 and later it is an error. So the mnist_mkl.py example is broken with recent versions of Perl and will be for all versions going forward.\nUnfortunately I am not very proficient in Perl so the optimal fix here may be something else, but perhaps this will help:\nhttps://stackoverflow.com/questions/31753737/deprecated-left-curly-bracket-in-perl-regex-exactly-when. ",
    "indie": "Hmm... without knowing more about what kind of functions or code you are running in this model,  here's a possible cause:\nThe high-level interface for this backend (-b mkl) includes an optional, on-by-default NaN check \non all matrix inputs before they can call any LAPACKE functions. Basically what happens is that when an input matrix contains any NaNs, the input parameter corresponding to that matrix can be flagged with an INFO parameter error.\nSee also:\n- https://software.intel.com/en-us/mkl-developer-reference-c-c-interface-conventions-for-lapack-routines\n- https://software.intel.com/en-us/mkl-linux-developer-guide-managing-behavior-of-function-domains-with-environment-variables\nFor your reference (and this was recently added to the documentation for Intel MKL 2018 Gold), you can turn NaN check OFF for savings a couple different ways: \n\nThrough the environment variable:\nSet LAPACKE_NANCHECK to 0 to turn NaN checking OFF\nSet LAPACKE_NANCHECK to 1 (or any non-zero integer) to turn NaN checking back ON.\nThrough the API \nCall LAPACKE_set_nancheck(flag) where flag = 0 turns OFF NaN checking.\nCall LAPACKE_set_nancheck(flag) where  flag \u2260  0 turns NaN checking back ON.\n\nIt's also possibly something else, but based only on the output you are showing here, this might be a good place to start.  NaN checks are not always a bad idea; just depends on the data. \n. This is how the codeblocks render on a default install of Ubuntu 17.04 before my fix ... not very becoming. \n\n. Comparison view with the (tested) fix in place.\n\n. ",
    "moderato": "@wei-v-wang Sorry for the late reply! I rebuild numpy and scipy from source based on the mkl library that comes with Intel Parallel Studio XE student edition (https://software.intel.com/en-us/parallel-studio-xe/choose-download/student-linux-fortran), and the nan cost problem still exists. Usually it pops up these two warnings:\n/home/moderato/miniconda3/envs/neon/lib/python3.5/site-packages/neon-2.1.0-py3.5.egg/neon/backends/math_cpu.py:138: RuntimeWarning: invalid value encountered in maximum\n  return np.log(np.maximum(x, np.exp(-50.)))\n/home/moderato/miniconda3/envs/neon/lib/python3.5/site-packages/neon-2.1.0-py3.5.egg/neon/backends/math_cpu.py:242: RuntimeWarning: invalid value encountered in multiply\n  return np.multiply(x1, x2)\n\nAny hints?\n. @indie Ohhh I guess you were giving this answer in terms of using C? I'm actually using Python and sorry for not being clear enough. Btw is there a way to do nancheck using Python?. @wei-v-wang I tried your suggestion. I edited the site.cfg file and .bashrc accordingly as follows:\nsite.cfg\n[mkl]\nlibrary_dirs = /home/moderato/Documents/neon/mklml_lnx_2018.0.20170720/lib\ninclude_dirs = /home/moderato/Documents/neon/mklml_lnx_2018.0.20170720/include\nmkl_libs = mkl_rt\nlapack_libs =\n.bashrc (only the newly added part)\n```\nmkl\nexport PATH=$PATH:/opt/intel/bin\nmkl intel, comment for neon\nexport LD_LIBRARY_PATH=/opt/intel/compilers_and_libraries_2018/linux/mkl/lib/intel64/:/opt/intel/compilers_and_libraries_2018/linux/lib/intel64:$LD_LIBRARY_PATH\nmkl neon, comment for intel\nexport LD_LIBRARY_PATH=/home/moderato/Documents/neon/mklml_lnx_2018.0.20170720/lib:/opt/intel/compilers_and_libraries_2018/linux/lib/intel64:$LD_LIBRARY_PATH\n```\nActually I have no idea which compiler to use. I used the same settings as that when I build numpy on top of mkl coming with parallel studio. Here's the tutorial I followed: https://software.intel.com/en-us/articles/numpyscipy-with-intel-mkl\nHowever this time it gave me an error like this:\n```\n\n\n\nimport numpy\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/home/moderato/miniconda3/envs/neon/lib/python3.5/site-packages/numpy/init.py\", line 142, in \n    from . import add_newdocs\n  File \"/home/moderato/miniconda3/envs/neon/lib/python3.5/site-packages/numpy/add_newdocs.py\", line 13, in \n    from numpy.lib import add_newdoc\n  File \"/home/moderato/miniconda3/envs/neon/lib/python3.5/site-packages/numpy/lib/init.py\", line 19, in \n    from .polynomial import \n  File \"/home/moderato/miniconda3/envs/neon/lib/python3.5/site-packages/numpy/lib/polynomial.py\", line 20, in \n    from numpy.linalg import eigvals, lstsq, inv\n  File \"/home/moderato/miniconda3/envs/neon/lib/python3.5/site-packages/numpy/linalg/init.py\", line 51, in \n    from .linalg import \n  File \"/home/moderato/miniconda3/envs/neon/lib/python3.5/site-packages/numpy/linalg/linalg.py\", line 31, in \n    from numpy.linalg import lapack_lite, _umath_linalg\nImportError: /home/moderato/miniconda3/envs/neon/lib/python3.5/site-packages/numpy/linalg/_umath_linalg.cpython-35m-x86_64-linux-gnu.so: undefined symbol: __intel_avx_rep_memset\n```\n\n\n\nAny ideas?. @wei-v-wang The reason why I built numpy manually is that the version of numpy coming with neon is 1.11.1 which is not the latest version, so each time when I reinstall neon and get started, it gives me an error on numpy version in the first place, then I have to reinstall numpy by myself. Does the numpy version make any difference?. @wei-v-wang The numpy version error I mentioned is like:\nRuntimeError: module compiled against API version 0xb but this version of numpy is 0xa\nEvery time I reinstall neon and run a neon program it pops up, and usually I solve it by 'pip uninstall numpy' then 'pip install numpy'. By the way, I use Anaconda for virtual environment management.  Are they making any difference?. @wei-v-wang I think I'll stick to Anaconda as I also installed many other necessary packages in this environment. The 'make' command is always working, only the mkl backend is producing weird results.\nI just rebuilt neon and tried \npip install --upgrade numpy --no-cache-dir \nto fix that error, and different from mkl-based numpy, the \"RuntimeWarning: invalid value encountered\" problem is gone, but the cost is still getting exploded easily.. @wei-v-wang I see. Maybe the problem is related to anaconda. For your reference, I am using miniconda3 with Python 3.5. Thanks for your help and looking forward to the new release!. @wei-v-wang Just edited the prepare_mkl.sh file and took a quick try. Good news is that the nan cost problem is gone! But there's another problem comes up: the speed is much slower than I expect. The output looks like:\nEpoch 0   [Train |\u2588\u2588\u2588\u2588\u2588\u2588              |   78/246  batches, 3.62 cost, 117.85s]\nBased on the cnn model and the dataset I use, the ratio of time over trained batch num would be around 1 using neon cpu backend (works fine), and around 0.5-0.6 using mkl-based numpy with neon mkl backend (although it has nan cost). I think in my case this ratio is supposed be at least <1 right? After all the mkl backend should be faster than cpu. The cpu on my PC is an i7 core.\nHope it helps with your testing!. @wei-v-wang Program code sent. Thank you for your help!. @wei-v-wang I tried Neon 2.2 on my model and looks like the problem is gone! I also spotted MKL's training loss has big fluctuations compared to the CPU&GPU backends as well as other frameworks somehow, although the validation accuracy is barely affected. Just let you know about it in case it's helpful to your work. Thanks!. Hi @wei-v-wang , sorry for just seeing the message... Yes, the NaN problem is gone, and MKL is way better than CPU in this version. It got the best performance in some of our tests, i.e. on Resnet-50 with an image size of 32x32, while it was defeated in others tests using a random model with a random size, i.e. 47x47. I wonder if that's because Neon is only optimized for 2^n size and famous models like Resnets?. @baojun-nervana Thanks! That would be very helpful.. Looks like Neon is no longer alive. Sadly close this issue.. ",
    "campos537": "Thanks , i will take o look on aeon, i was able to train a model using a dataset on .h5 extension converting to numpy array\n--\nCrystal Silva Campos\nGraduando em Engenharia da Computa????o\nUniversidade Estadual de Feira de Santana/Bahia\nLattes: http://lattes.cnpq.br/2563260359267761https://wwws.cnpq.br/cvlattesweb/PKG_MENU.menu?f_cod=CC3509E4E54FEB21774DC166D4ACCDE3#\n\nDe: Wei Wang notifications@github.com\nEnviado: segunda-feira, 16 de outubro de 2017 22:44\nPara: NervanaSystems/neon\nCc: Crystal S Campos; Mention\nAssunto: Re: [NervanaSystems/neon] how can i add my own collection of images on neon to train a model ? (#400)\n@campos537https://github.com/campos537 Can you please start with examples/binary ? It uses MNIST dataset.\nFor advanced data ingestion, please also refer to aeon (which is part of neon and is automatically installed). https://github.com/nervanasystems/aeon\nYou might also want to take a look at examples/imagenet/ and examples/ssd examples/faster-rcnn to get an idea how the general ingestion process looks like.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHubhttps://github.com/NervanaSystems/neon/issues/400#issuecomment-337065381, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AadWAkl1YdhjHNstxVrgdNCeVpOwu5fLks5ss9xkgaJpZM4Pc4HZ.\n. ",
    "ldurka": "Hi,\nTo reproduce Your issue I've used clean docker image and Ubuntu:16.04\nI've installed anacoda and all required packages\napt-get update && apt-get install -y bzip2 git cmake clang pkg-config sox lib-sox libopencv-dev python-opencv libcurl4-openssl-dev\nwget https://repo.continuum.io/archive/Anaconda2-5.0.0.1-Linux-x86_64.sh\nbash Anaconda2-5.0.0.1-Linux-x86_64.sh\nsource ~/.bashrc\nconda list\ngit clone https://github.com/NervanaSystems/neon.git\ncd neon && make -j\n.venv/bin/python -c \"from aeon import DataLoader as AeonLoader\"\nI've got error like\nraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nImportError: /lib/x86_64-linux-gnu/libz.so.1: version `ZLIB_1.2.9' not found (required by /anaconda2/lib/./libpng16.so.16)\nCould you try to run and send us otput \nPATH_TO_YOUR_VIRTUALNEV_WITH_NEON/bin/python -c \"from aeon import DataLoader as AeonLoader\". ",
    "kbinias": "Could you also attach output log from build process ?\nDo you run Neon training process ?. @Drea1989 close this issue please.. ",
    "Drea1989": "i have deleted the anaconda environments, deleted and reinstalled CUDA-8.0 recloned neon in a new folder, run the make sysinstall in a new anaconda environment and this time it worked with neon 2.2 and aeon 1.0 thanks for the support. hello, thanks for the help, i was able to load and save the new model by printing out the model architecture and verifying that the structure of the layers where equal. all good now i was able to train and infer.\n. ",
    "riccitensor": "I installed the latest version of Neon. Also, I did upgrade numpy, but this time the error is \"backend must be one of ('cpu'). First thing I see that that no CUDA-capable device is detected (I have GeForce 1070gfx, which seems to be supported https://en.wikipedia.org/wiki/CUDA#GPUs_supported). Ubuntu 16.4. \"-b mkl\" -> invalid choice: 'mkl' (choose from 'cpu'),  \"-b cpu\" seems to work (10 epoch-training and the misclassification error 2.6%). Looks like, for some reason, GPU is really not supported, even though this is GFX 1070 (included in the CUDA-capable device list). You are right. Re-installed and now works smoothly with all backends.. Hey @wei-v-wang Works for me now. Thanks a lot!. ",
    "griffonn": "Hi, the same assertion error appears when using deepspeech with neon 2.3:\npython evaluate.py --manifest val:/home/user/librispeech/dev-clean.manifest.csv --model_file /home/user/deepspeech/speech/librispeech_16_epochs.prm\nFile \"/home/user/neon/neon/layers/container.py\", line 200, in load_weights\n    assert len(pdict['config']['layers']) == len(self.layers)\nAssertionError. ",
    "xingjinglu": "It seems the python-pycuda not work with cuda9.0.\nI test the pycuda with the test case:\npython test_driver.py\nIt causes segmentation fualt either.\nI fill check the reason further.\n. These errors happens when I install within docker. \nThen I tried to install pycuda and neon without docker, and it works.\nbut When I tried run the benchmark of openai-gemm, it introduces other errors, such as follow:\nAre there some suggestions about it?\n(.venv2) [@nmyjs_186_118 openai-gemm]$ ./benchmark.py \nTITAN Xp\n|     M|     N|     K| Op|OpenAI_32|cuBLAS_32|ratio_32|OpenAI_16|cuBLAS_16|ratio_16|\n|------|------|------|---|---------|---------|--------|---------|---------|--------|\nTraceback (most recent call last):\n  File \"./benchmark.py\", line 126, in \n    data = matmul(A, B, C, bench=True)\n  File \"/search/odin/luxingjing/project/openai-gemm/openai_gemm.py\", line 73, in matmul\n    kernel, params, dynamic_shared = _get_gemm_kernel(prefix, op, cda, cdb, cdc, m, n, k)\n  File \"\", line 2, in _get_gemm_kernel\n  File \"/search/odin/luxingjing/project/neon/.venv2/lib/python2.7/site-packages/pycuda/tools.py\", line 430, in context_dependent_memoize\n    result = func(args)\n  File \"/search/odin/luxingjing/project/openai-gemm/openai_gemm.py\", line 280, in _get_gemm_kernel\n    kernel = get_kernel(base, opts)\n  File \"\", line 2, in get_kernel\n  File \"/search/odin/luxingjing/project/neon/.venv2/lib/python2.7/site-packages/pycuda/tools.py\", line 430, in context_dependent_memoize\n    result = func(args)\n  File \"/search/odin/luxingjing/project/openai-gemm/openai_gemm.py\", line 693, in get_kernel\n    run_command([ \"ptxas -v -arch\", arch, \"-o\", cubin_file, ptx_file, \";\" ] + maxas_i + [sass_file, cubin_file])\n  File \"/search/odin/luxingjing/project/openai-gemm/openai_gemm.py\", line 622, in run_command\n    raise RuntimeError(\"Error(%d):\\n%s\\n%s\" % (proc.returncode, cmd, err))\nRuntimeError: Error(2):\nptxas -v -arch sm_61 -o /search/speech/luxingjing/.cache/openai-gemm/sm_61/cubin/sgemm_128x128x8_NN_vec.cubin /search/speech/luxingjing/.cache/openai-gemm/sm_61/ptx/sgemm_128x128x8_NN_vec.ptx ; PERL5LIB=/search/odin/luxingjing/project/openai-gemm/maxas /search/odin/luxingjing/project/openai-gemm/maxas/maxas.pl -i -w -k sgemm_128x128x8_NN_vec -Dtype s -DNN 1 -Dvec 1 /search/odin/luxingjing/project/openai-gemm/sass/xgemm_128x128x8.sass /search/speech/luxingjing/.cache/openai-gemm/sm_61/cubin/sgemm_128x128x8_NN_vec.cubin\n/bin/sh: line 1: 38826 Floating point exceptionptxas -v -arch sm_61 -o /search/speech/luxingjing/.cache/openai-gemm/sm_61/cubin/sgemm_128x128x8_NN_vec.cubin /search/speech/luxingjing/.cache/openai-gemm/sm_61/ptx/sgemm_128x128x8_NN_vec.ptx\n/search/speech/luxingjing/.cache/openai-gemm/sm_61/cubin/sgemm_128x128x8_NN_vec.cubin: No such file or directory at /search/odin/luxingjing/project/openai-gemm/maxas/MaxAs/Cubin.pm line 138.\n. When I dig further, the reason is ptxas failed to compile .ptx files into .cubin files. The error happend when I try to run openai-gemm on Titanxp with CUDA9 with neon as the backend.\nThe exact command is:\npython test.py\nMore detailed command is:\nptxas -v -arch sm_61 -o /search/speech/luxingjing/.cache/openai-gemm/sm_61/cubin/sgemm_128x128x8_NN.cubin /search/speech/luxingjing/.cache/openai-gemm/sm_61/ptx/sgemm_128x128x8_NN.ptx\nFloating point exception\nThe content of sgemm_128x128x8_NN.ptx is as follow\n.version 5.0\n.target sm_61\n.address_size 64\n// args: {'type': 's'}\n.visible .entry  sgemm_128x128x8_NN(\n    .param .u64 param_C,\n    .param .u64 param_A,\n    .param .u64 param_B,\n    .param .f32 param_alpha,\n    .param .f32 param_beta,\n    .param .u32 param_cda,\n    .param .u32 param_cdb,\n    .param .u32 param_cdc,\n    .param .u32 param_m,\n    .param .u32 param_n,\n    .param .u32 param_k,\n    .param .u32 param_blk_a,\n    .param .u32 param_blk_b\n)\n.reqntid 256\n{\n.shared .align 4 .b32 share[4228];\n\nret;\n\n}\n. https://github.com/xingjinglu/PerfAILibs/blob/master/README.md\nOn Tue, Nov 14, 2017 at 5:20 AM, Yangzihao Wang notifications@github.com\nwrote:\n\nI'm trying to compare openai-gemm with cublas v9 and I get the same FPE\nerror when using the following command line to build:\n'make lib/c_interface.o'\nIs there any plan to make openai-gemm work with CUDA 9?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/NervanaSystems/neon/issues/413#issuecomment-344062087,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AHg108UkoGxD0BELjAzYVAWAU8xF7WI_ks5s2LK7gaJpZM4QS2An\n.\n. \n",
    "yzhwang": "I'm trying to compare openai-gemm with cublas v9 and I get the same FPE error when using the following command line to build:\nmake lib/c_interface.o\nIs there any plan to make openai-gemm work with CUDA 9?\n. ",
    "pantherso48": "Not seeing it in the GUI or terminal, where can I get it to add to my project?\n\n. That worked well, thank you!!!. The weight file is from your s3 bucket: \nurl = 'https://s3-us-west-1.amazonaws.com/nervana-modelzoo/VGG/'\nfilename = 'VGG_D.p'\nI will work through #415 and try to convert the VGG weights file into the new format. So I understand this more clearly as I look at the source code in Neon:\n assert type(pdict) is dict\n--> 736         for key in pdict['params']:\n    737             if not hasattr(self, key):\nIf there is no attribute for a key then it throws an error, so the following param I send to this function does not have an attribute with the key config so that is causing the error?\n{'config': {'transform': {'config': {'name': 'Rectlin_0'}, 'type': 'neon.transforms.activation.Rectlin'}, 'name': 'Convolution_11_Rectlin'}, 'type': 'neon.layers.layer.Activation'}\nI will try the conversion and report back, thanks for the help!. That worked, just needed to update the load_states to false. Thanks!\nI could update the tutorial 2 documentation and send a push request if you want also while it is fresh in my head. Thanks again.. I'm running the following script in a Jupyter terminal which has neon installed and it keeps arguing the   --out_dir flag but the data.py file is looking for that argument in the source code. Any insight would be appreciated, thanks!\njupyter notebook data.py --out_dir data/cifar10\n\nSource code:\nif __name__ == '__main__':\n   from configargparse import ArgumentParser\n   parser = ArgumentParser()\n   parser.add_argument('--out_dir', required=True, help='Directory to write ingested files')\n   parser.add_argument('--padded_size', type=int, default=40, help='Size of image after padding')\n   parser.add_argument('--overwrite', action='store_true', default=False, help='Overwrite files')\n   args = parser.parse_args()\n\ningest_cifar10(args.out_dir, args.padded_size, overwrite=args.overwrite). Using jupyter notebook cli was the problem, now I am getting an import error on an import that is in my env:\n\n\n. This was a python version error ran it with python3 and worked great.. Which cifar example? I am not seeing a cifar10_msra.py file.\n\n. Ok that is running now, so since this is training the model does that train.py script replace this line of code?\ntrain_set = AeonDataLoader(config, be). Okay that makes sense, is there a list of the datasets Neon provides? I see MNIST and cifar10 but was curious if there are others like cats and dogs?. No it is like Dense, a fully connected layer is what the Affine layer is used for in Neon.\nhttp://neon.nervanasys.com/docs/latest/generated/neon.layers.layer.Affine.html#neon.layers.layer.Affine. Hi guys,\nNeon is expecting an image and label (X and Y) values in the iterators to train the model. So I've tweaked the code to look like this:\n`from neon.backends import gen_backend\nfrom neon.data import ArrayIterator\nimport numpy as np\nset mkl to use cpu not gpu\nbe = gen_backend(backend='mkl', batch_size=128)\nimport glob\nfrom PIL import Image\nX_training_filelist = glob.glob('/data/datasets/catsanddogs_1k/training_set/.jpg')\nX_test_filelist = glob.glob('/data/datasets/catsanddogs_1k/test_set/.jpg')\nnames = []\nwith open('/data/datasets/catsanddogs_1k/catsanddogsMetaData.txt', 'r') as f:\n    for line in f.readlines():\n        names.append(np.array(line.rstrip()))\nnames = np.array(names)\ny_train = names.astype(int)\nnames = []\nwith open('/data/datasets/catsanddogs_1k/catsanddogsMetaData_Test.txt', 'r') as f:\n     for line in f.readlines():\n        names.append(np.array(line.rstrip()))\nnames = np.array(names)\ny_test = names.astype(int)  \ntraining_Array = np.array([np.array(Image.open(fname)) for fname in training_filelist])\ntest_Array = np.array([np.array(Image.open(fname)) for fname in test_filelist])\ntrain_set = ArrayIterator(training_Array, y_train, nclass=2, lshape=(3, 32, 32))`\nNow I need to make all my image arrays the same size and be 3 for RGB. That is where I could use guidance now. Thanks for keeping this thread alive.. Update:\nI have tried to get the Accuracy of each epoch's run in two ways, one by adding the metric=Accuracy() argument to the Callbacks instantiation. In the source code this argument causes a callback to be added. But I am not seeing any output in my logs. I am using jupyter notebooks, would running in terminal help?\nThe other way I tried was using the add_callback() function in Callbacks class which did not show any output as well. The code examples are below:\n 1) callbacks = Callbacks(mlp, eval_set=train_set, metric=Accuracy())\n 2) callbacks.add_callback(MetricCallback(eval_set=train_set, metric=Accuracy(), epoch_freq=1))\nMy next option is to try to write a custom callback and print out the Accuracy that way, any direction would be greatly appreciated, thank you!\n. Created a custom callback function and retrieved the accuracy and cost by using the callback dictionary, thanks for the help!. from ncloud import config, commands\nfrom ncloud.commands import model\nfrom ncloud.commands.model import Train\nimport inspect\nc = config.Config()\nprint(model.deploy.call(c))\ninspect.getsourcelines(Train)\nThis is the solution I used to look into the source code while the documentation is being updated, hope this helps anyone out that needs it!. ",
    "shaofushih": "We changed the make file to support sm_61 and that worked. ",
    "miketout": "It looks like the issue is related to padding. I have removed some areas on our model where there was padding that we didn't need, and the leak is gone. Right now, I'd consider this a workaround. If you're interested, or if I can in the near future, I will try to put together a repro case.. Here's a link to the open source project, which was just released by Synacor:\nhttps://github.com/Zimbra/zimbra-ml\nIt requires some modifications I made to Neon in my fork, primarily for allowing broadcast or multistream containers to be fed by tree or sequential graphs from above. I also disable MKL, as I've found that it doesn't run. I'll probably open a separate issue on that.\nAs far as this memory leak, I could probably reproduce it, but I believe it was due to padding on the height when not intended, which I've since removed completely from the code.. ",
    "armando-fandango": "In the above PR, the number of tests failed was reduced to 1, not related to the PR:\n`====================================================== FAILURES ======================================================\n______ test_dilated_conv[fargs_tests16-cpu] ________\nbackend_default = , fargs_tests = (7, 2, 1)\ndef test_dilated_conv(backend_default, fargs_tests):\n\n    fsz = fargs_tests[0]\n    dil = fargs_tests[1]\n    stride = fargs_tests[2]\n    be = backend_default\n\n    o1, w1 = run(be, False, fsz, stride, 1, dil)\n    o2, w2 = run(be, True, fsz, stride, 1, dil)\n    # Verify that the results of faked dilation match those of actual dilation.\n\n\n  assert allclose_with_out(o1, o2, atol=0, rtol=3e-3)\n\nE       assert False\nE        +  where False = allclose_with_out(array([[-6622, 11619, 39178, -17867, 17616, -13393, -17023, -3494, 2915, -2020, -35793, 37675, -18931, 10426, -31960, ... 37881, -69578, -38721, 35254, 8138, 16923, -20529, -20013, -8539, -18421, -27050, -14500, 36606, 187]], dtype=float32), array([[-6622, 11619, 39178, -17867, 17616, -13393, -17023, -3494, 2915, -2020, -35793, 37675, -18931, 10426, -31960, ... 37881, -69578, -38721, 35254, 8138, 16923, -20529, -20013, -8539, -18421, -27050, -14500, 36606, 187]], dtype=float32), atol=0, rtol=0.003)\n\ntests/test_dilated_conv.py:174: AssertionError\n------------------------------------------------ Captured stdout call ------------------------------------------------\nNetwork Layers:\nSequential\n  Convolution Layer 'Convolution_40': 1 x (32x32) inputs, 8 x (28x28) outputs, 0,0 padding, 1,1 stride, 1,1 dilation\n  Convolution Layer 'Convolution_41': 8 x (28x28) inputs, 8 x (18x18) outputs, 1,1 padding, 1,1 stride, 2,2 dilation\n  Convolution Layer 'Convolution_42': 8 x (18x18) inputs, 8 x (16x16) outputs, 0,0 padding, 1,1 stride, 1,1 dilation\n  Linear Layer 'Linear_16': 2048 inputs, 1 outputs\nNetwork Layers:\nSequential\n  Convolution Layer 'Convolution_10': 1 x (32x32) inputs, 8 x (28x28) outputs, 0,0 padding, 1,1 stride, 1,1 dilation\n  Convolution Layer 'Convolution_13': 8 x (28x28) inputs, 8 x (18x18) outputs, 1,1 padding, 1,1 stride, 1,1 dilation\n  Convolution Layer 'Convolution_12': 8 x (18x18) inputs, 8 x (16x16) outputs, 0,0 padding, 1,1 stride, 1,1 dilation\n  Linear Layer 'Linear_6': 2048 inputs, 1 outputs\n------------------------------------------------ Captured stderr call ------------------------------------------------\nDISPLAY:neon:abs errors: 1.171875e-02 [0.000000e+00, 5.468750e-02] Abs Thresh = 0.000000e+00\nDISPLAY:neon:worst case: 1.533482e+04 1.533477e+04\nDISPLAY:neon:rel errors: 5.932112e-07 [0.000000e+00, 4.104107e-03] Rel Thresh = 3.000000e-03\nDISPLAY:neon:worst case: 9.538086e+00 9.577393e+00\n`. @baojun-nervana The sysinstall means I want to install as a shared library at the system level, not in the virtual environment. Hence I did not understand the meaning of your comment : \"Have you tried python3 system install under a python3 virtualenv?\"\nThe system install, whether for python 2 or python 3, should not ask for the virtual environment to be installed or to be present.. Yes virtualenv is a nice way to create virtual environments with different versions of python etc., but for production environment, sometimes we install python 3 at system level and python 2 is not available, hence the sysinstall previously did not work for such cases.. Actually from the makefile, it seems like that the intent of original makefile author was to install in virtual environment unless 'sys' prefix was added to install and clean.\nI think the makefile should be refactored heavily to allow for different parameters such as --env=virtual or non-virtual, --python=2 or 3, --system etc\n. Yes I can update  my PR to have the recipes as you mentioned. Sounds good. \n-- Armando \nSent from phone.\n\nOn Dec 22, 2017, at 5:18 PM, baojun notifications@github.com wrote:\nThere is a parameter PY = 2 or 3 (line 129) which is mostly used to diff venv for python 2 and 3. We may expand that to differ python and pip command for python 2 and 3.\nit will be something like the following:\nifeq ($(PY), 2)\n.......\nPYTHON_EXE := python\nPIP_EXE := pip\nelse\n........\nPYTHON_EXE := python3\nPIP_EXE := pip3\nendif\nIn the recipes, it can be called as $(PYTHON_EXE) and $(PIP_EXE). The neon_install recipe will be like:\nneon_install:\n@echo \"Installing neon system wide...\"\n@$(PYTHON_EXE) setup.py install\n@echo\nThis way it will be an integrated recipe for python2 and 3.\nTo run python3 sysinstall, it will be\nmake PY=3 sysinstall\n@armando-fandango Are you interested in updating the PR to merge the python2 and 3 sys install recipes?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @baojun-nervana I am getting the same error even after installing all pre-req:\n\n: error: argument -b/--backend: invalid choice: 'gpu' (choose from 'cpu', 'mkl'). @baojun-nervana yes all the gpu dependencies are installed and I think my message above said that \"even after installing all pre-req\". I see that in makefile you are doing this : nvcc neon/backends/util/check_gpu.c > /dev/null 2>&1 && ./a.out && rm a.out && echo true\nThis is always returning an empty HAS_GPU string no matter what. I compiled the check_gpu.c outside of the makefile and it compiles.\nIs something fishy in above code ?. this command works and produces the a.out : nvcc neon/backends/util/check_gpu.c\nI am reinstalling the whole nvidia driver and CUDA library to see if there is some problem with that.. After reinstalling CUDA and Neon, now I am getting this error:\npython3 examples/mnist_mlp.py -b gpu\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.5/dist-packages/pycuda/tools.py\", line 426, in context_dependent_memoize\n    return ctx_dict[cur_ctx][args]\nKeyError: \nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"examples/mnist_mlp.py\", line 88, in \n    num_epochs=args.epochs, cost=cost, callbacks=callbacks)\n  File \"/usr/local/lib/python3.5/dist-packages/nervananeon-2.4.0-py3.5.egg/neon/models/model.py\", line 183, in fit\n    self.epoch_fit(dataset, callbacks)\n  File \"/usr/local/lib/python3.5/dist-packages/nervananeon-2.4.0-py3.5.egg/neon/models/model.py\", line 205, in _epoch_fit\n    x = self.fprop(x)\n  File \"/usr/local/lib/python3.5/dist-packages/nervananeon-2.4.0-py3.5.egg/neon/models/model.py\", line 236, in fprop\n    res = self.layers.fprop(x, inference)\n  File \"/usr/local/lib/python3.5/dist-packages/nervananeon-2.4.0-py3.5.egg/neon/layers/container.py\", line 395, in fprop\n    x = l.fprop(x, inference=inference)\n  File \"/usr/local/lib/python3.5/dist-packages/nervananeon-2.4.0-py3.5.egg/neon/layers/layer.py\", line 1288, in fprop\n    bsum=self.batch_sum)\n  File \"/usr/local/lib/python3.5/dist-packages/nervananeon-2.4.0-py3.5.egg/neon/backends/nervanagpu.py\", line 1585, in compound_dot\n    kernel = kernel_specs.get_kernel(\"\".join((clss, op, size)), vec_opt)\n  File \"\", line 2, in get_kernel\n  File \"/usr/local/lib/python3.5/dist-packages/pycuda/tools.py\", line 430, in context_dependent_memoize\n    result = func(*args)\n  File \"/usr/local/lib/python3.5/dist-packages/nervananeon-2.4.0-py3.5.egg/neon/backends/kernel_specs.py\", line 849, in get_kernel\n    run_command(maxas_i + [sass_file, cubin_file])\n  File \"/usr/local/lib/python3.5/dist-packages/nervananeon-2.4.0-py3.5.egg/neon/backends/kernel_specs.py\", line 785, in run_command\n    raise RuntimeError(\"Error(%d):\\n%s\\n%s\" % (proc.returncode, cmd, err))\nRuntimeError: Error(13):\nPERL5LIB=/usr/local/lib/python3.5/dist-packages/nervananeon-2.4.0-py3.5.egg/neon/backends/kernels/maxas /usr/local/lib/python3.5/dist-packages/nervananeon-2.4.0-py3.5.egg/neon/backends/kernels/maxas/maxas.pl -i -w -k sgemm_nn_32x128_vec -Dvec 1 /usr/local/lib/python3.5/dist-packages/nervananeon-2.4.0-py3.5.egg/neon/backends/kernels/sass/sgemm_nn_32x128.sass /home/armando/.cache/neon/kernels/cubin/sgemm_nn_32x128_vec.cubin\nb'Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/ { <-- HERE (?5)?,?(?4)?,?(?3)?,?(?2)?,?(?1)?,?(?0)?}/ at /usr/local/lib/python3.5/dist-packages/nervananeon-2.4.0-py3.5.egg/neon/backends/kernels/maxas/MaxAs/MaxAsGrammar.pm line 239.\\nUnescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/^(?^:\\@(?\\!)?P(?[0-6]) )?DEPBAR(?^: { <-- HERE (?5)?,?(?4)?,?(?3)?,?(?2)?,?(?1)?,?(?0)?});/ at /usr/local/lib/python3.5/dist-packages/nervananeon-2.4.0-py3.5.egg/neon/backends/kernels/maxas/MaxAs/MaxAsGrammar.pm line 275.\\nError: could not open /home/armando/.cache/neon/kernels/cubin/sgemm_nn_32x128_vec.cubin for writing: Permission denied at /usr/local/lib/python3.5/dist-packages/nervananeon-2.4.0-py3.5.egg/neon/backends/kernels/maxas/MaxAs/Cubin.pm line 640.\\n'. after I run this command:\nsudo chown -R armando.armando ~/.cache\nNow it works:\npython3 examples/mnist_mlp.py -b gpu\nEpoch 0   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  469/469  batches, 0.24 cost, 6.15s]\nEpoch 1   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  469/469  batches, 0.20 cost, 1.03s]\nEpoch 2   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  469/469  batches, 0.17 cost, 1.06s]\nEpoch 3   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  468/468  batches, 0.14 cost, 1.22s]\nEpoch 4   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  468/468  batches, 0.13 cost, 1.14s]\nEpoch 5   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  468/468  batches, 0.11 cost, 1.07s]\nEpoch 6   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  468/468  batches, 0.10 cost, 1.57s]\nEpoch 7   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  468/468  batches, 0.09 cost, 1.14s]\nEpoch 8   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  468/468  batches, 0.08 cost, 1.18s]\nEpoch 9   [Train |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  468/468  batches, 0.07 cost, 1.06s]\n2017-12-20 14:15:58,969 - neon - DISPLAY - Misclassification error = 2.5%\n@yangyang-zhang let me know if I can help you :-)\n. @wei-v-wang in my case the file is getting downloaded automatically, and also has the mkl_dnn.h:\nls *\nlicense.txt\ninclude:\ni_malloc.h  mkl_cblas.h  mkl_dnn_types.h  mkl_lapack.h   mkl_trans.h  mkl_version.h      mkl_vml_functions.h  mkl_vml_types.h    mkl_vsl_functions.h  mkl_vsl_types.h\nmkl_blas.h  mkl_dnn.h    mkl_lapacke.h    mkl_service.h  mkl_types.h  mkl_vml_defines.h  mkl_vml.h            mkl_vsl_defines.h  mkl_vsl.h\nlib:\nlibiomp5.so  libmklml_gnu.so  libmklml_intel.so\n. @baojun-nervana The make clean doesnt work for me so I am doing make sysclean and removing the mkl folders manually, and then running make sysinstall_python3.\nThe mkl is downloaded and unarchived but then the build engine fails.. ok after deleting it couple of times it worked finally and no mkl engine build errors. thanks.. @baojun-nervana please also incorporate my patch if you can, specially for system install it keeps asking for venv and also for python3 install the current makefile doesnt work properly.\n. @baojun-nervana we are trying to use neon on the local machine. Does this option work without the ncloud? Also, does it save accuracy and loss after every epoch? If not what is the best way to get accuracy vs. epoch graph after running for let us say 500 iterations.. @baojun-nervana This example logs accuracy at the end. We want to log the accuracy and cost after every epoch of training, within the fit function.. ",
    "yangyang-zhang": "Thank you, I've already solved it. . Thank you, I've already solved it. . ",
    "sangameshnr": "Hi,\nThanks for the reply. I was able to do that in 2d after a couple of transposes and reshapes. \nFollowing up, I have another question. I need to tap outputs from multiple convnet layers of my VGG pretrained network. Is there a simple way to do that? model.get_ouputs() gives final layer outputs but I need outputs from some middle layers also in the VGG-16 network that has \"pretrained weights\" loaded into it. I am modifying the sequential container \"fprop\" to get that done. Is there a simple way to do that?. I figured out that I need to use model.layers.layers[i].outputs after running model.get_outputs() to get outputs from different layers.\nThanks.. Yes. I get this error for 'mkl' backend and not for 'cpu'. \n. I have one more comment. For cpu backend, though there is no error, the fprop output of the MergeSum layer is equal to the second path only. It does not return the sum of two paths. Am I missing something here  ??. @airofjune thanks for the comment on the arguments. I will note that.\n@baojun-nervana thanks for taking care of the issue. I will look forward to the release.. ",
    "airofjune": "@sangameshnr \nMergeSum on CPU backend applies an adding  function dependent on: 1 shared output  2 beta argument.\nThus custom layer should write correct allocate() and fprop().. ",
    "zhiltsov-max": "@moderato, diving into the source code, I found this undocumented parameter. Also, this can be found in the ResNet example. Seems like the Pooling(fshape='all', ...) does just the right thing. . ",
    "scigeek72": "Thanks @baojun-nervana for our quick response.\nI am using MacOSX and no GPUs. But I can fire up a compute node on AWS to use GPU. \nI don't have any experience with neon. Do I need to use any other ? I use keras with tensorflow as a backend for deep learning related experiments. \nDoes skip thought vector module ship with neon or do we have to separately download them from github?\nAlso, what's the average training time for the skip thought model? \nThanks a lot. . ",
    "jannepy": "It is based on the DeepSpeech model, and it has been trained with Neon 1.8. It appears that the models serialized with the old 1.* versions are no longer compatible with the latest Neon. Is there some way to convert those models such that they can be used with version 2.6?. I followed the instructions in #415, and by reconstructing the layers I was able to load the model into Neon 2.6 and save in the new format. Thanks!. ",
    "ashahba": "Thanks @gabrielbriones \nI had someone reported this on StackOverflow and I responded here:\nhttps://stackoverflow.com/questions/48837894/typeerror-init-got-multiple-values-for-keyword-argument-add-config-file/49016052#49016052\nHowever I think you are on the right track \ud83d\ude41 \n. ",
    "c54852533": "@baojun-nervana \nProblems solved well! Thanks!. ",
    "yataoz": "@baojun-nervana Alright, got it. Thanks!. ",
    "yiqiaoc11": "ctypes.cdll.LoadLibrary(mkl_engine_path), mkl_engine_path is correct, but can't be loaded.. :/usr/local/lib\nI just got this one resolved by reinstalling it with conda rather than pip. We can close this one. \nBTW, the installation tutorial indicates we need to create source activation for neon. Not sure if we can ignore it as we might need include some pytorch packages which could conflict with activated neon.\n. ",
    "Pratyusha1796": "@yiqiaoc11 @wei-v-wang   can you guys help me out with this same error i am facing\n2018-03-10 12:13:24,836 - neon.backends - WARNING - deterministic_update and deterministic args are deprecated in favor of specifying random seed\nTraceback (most recent call last):\n  File \"inference.py\", line 57, in \n    be = gen_backend(**extract_valid_args(args, gen_backend))\n  File \"/home/pratyusha/Envs/ronin/local/lib/python2.7/site-packages/neon/backends/init.py\", line 113, in gen_backend\n    deterministic=deterministic)\n  File \"/home/pratyusha/Envs/ronin/local/lib/python2.7/site-packages/neon/backends/backend.py\", line 468, in allocate_backend\n    return Backend.backendsname\n  File \"/home/pratyusha/Envs/ronin/local/lib/python2.7/site-packages/neon/backends/nervanamkl.py\", line 213, in init\n    self.mklEngine = ctypes.cdll.LoadLibrary(mkl_engine_path)\n  File \"/usr/lib/python2.7/ctypes/init.py\", line 443, in LoadLibrary\n    return self._dlltype(name)\n  File \"/usr/lib/python2.7/ctypes/init.py\", line 365, in init\n    self._handle = _dlopen(self._name, mode)\nOSError: libmklml_gnu.so: cannot open shared object file: No such file or directory\nthis my path \n echo $LD_LIBRARY_PATH\n/usr/local/lib\n. ",
    "lixiangchun": "@baojun-nervana Thanks for your help.\nError in running python3 examples/video-c3d/train.py:\nTraceback (most recent call last):\n  File \"/media/storage1/software/github/neon/examples/video-c3d/train.py\", line 31, in <module>\n    parser = NeonArgparser(__doc__, default_config_files=config_files)\n  File \"/usr/local/lib/python3.5/dist-packages/neon/util/argparser.py\", line 80, in __init__\n    super(NeonArgparser, self).__init__(*args, **kwargs)\nTypeError: __init__() got multiple values for argument 'add_config_file_help'\n. Thanks, it works now.\nHowever, I found that this repo only supports CPU or MLK as backend.The training process is very slow.\nHow to enable GPU as the backend for this repo?\n. @baojun-nervana After installing all packages in gpu_requirements.txt, the GPU backend can be used; however, the following error occurs:\n```\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.5/dist-packages/pycuda/tools.py\", line 426, in context_dependent_memoize\n    return ctx_dict[cur_ctx][args]\nKeyError: \nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/media/storage1/software/github/neon/examples/video-c3d/train.py\", line 57, in \n    model.fit(train, optimizer=opt, num_epochs=args.epochs, cost=cost, callbacks=callbacks)\n  File \"/usr/local/lib/python3.5/dist-packages/neon/models/model.py\", line 183, in fit\n    self._epoch_fit(dataset, callbacks)\n  File \"/usr/local/lib/python3.5/dist-packages/neon/models/model.py\", line 205, in _epoch_fit\n    x = self.fprop(x)\n  File \"/usr/local/lib/python3.5/dist-packages/neon/models/model.py\", line 236, in fprop\n    res = self.layers.fprop(x, inference)\n  File \"/usr/local/lib/python3.5/dist-packages/neon/layers/container.py\", line 395, in fprop\n    x = l.fprop(x, inference=inference)\n  File \"/usr/local/lib/python3.5/dist-packages/neon/layers/layer.py\", line 1061, in fprop\n    bias=self.weight_bias, bsum=self.batch_sum, layer_op=self)\n  File \"/usr/local/lib/python3.5/dist-packages/neon/backends/nervanagpu.py\", line 1990, in fprop_conv\n    return self._execute_conv(\"fprop\", layer, layer.fprop_kernels, repeat)\n  File \"/usr/local/lib/python3.5/dist-packages/neon/backends/nervanagpu.py\", line 2072, in _execute_conv\n    kernels.execute(repeat)\n  File \"/usr/local/lib/python3.5/dist-packages/neon/backends/convolution.py\", line 551, in execute\n    kernel = kernel_specs.get_kernel(self.kernel_name, self.kernel_options)\n  File \"\", line 2, in get_kernel\n  File \"/usr/local/lib/python3.5/dist-packages/pycuda/tools.py\", line 430, in context_dependent_memoize\n    result = func(*args)\n  File \"/usr/local/lib/python3.5/dist-packages/neon/backends/kernel_specs.py\", line 842, in get_kernel\n    run_command([ \"ptxas -v -arch\", arch, \"-o\", cubin_file, ptx_file ])\n  File \"/usr/local/lib/python3.5/dist-packages/neon/backends/kernel_specs.py\", line 785, in run_command\n    raise RuntimeError(\"Error(%d):\\n%s\\n%s\" % (proc.returncode, cmd, err))\nRuntimeError: Error(136):\nptxas -v -arch sm_61 -o /home/lixc/.cache/neon/kernels/cubin/sconv_direct_fprop_64x32_SN_bias.cubin /home/lixc/.cache/neon/kernels/ptx/sconv_direct_fprop_64x32_SN_bias.ptx\nb'Floating point exception (core dumped)\\n'\n```\nMy train.cfg is:\nmanifest = [train:/media/storage1/project/deep_learning/c3d_ucf/data/ucf-extracted/train-index.csv, test:/media/storage1/project/deep_learning/c3d_ucf/data/ucf-extracted/test-index.csv]\nmanifest_root = /media/storage1/project/deep_learning/c3d_ucf/data/ucf-extracted\nbackend = gpu\nepochs = 10\nbatch_size = 16\neval_freq = 1\nlog = video-c3d.log\noutput_file = video-c3d.hdf5\ndevice_id = 1\ndata_dir = train_output_dir\nserialize = 1\nTraining was done via:\nbash\nexport LD_LIBRARY_PATH=/media/storage1/software/github/neon/mklml_lnx_2018.0.1.20171227/lib:$LD_LIBRARY_PATH\npython3 /media/storage1/software/github/neon/examples/video-c3d/train.py -c train.cfg. @baojun-nervana Thanks. Yes, I use cuda9. Will go back to cuda8 and try again.. ",
    "divyanshj16": "@wei-v-wang Thankyou for the information. But it still doesn't answer my question.\nI asked whether neon will be continued or not? . ",
    "Ziyi-Zhao": "Nice question!. "
}
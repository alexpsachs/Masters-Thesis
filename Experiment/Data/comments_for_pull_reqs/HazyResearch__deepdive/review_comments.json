[
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/11186787",
        "pull_request_review_id": null,
        "id": 11186787,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExMTg2Nzg3",
        "diff_hunk": "@@ -327,19 +327,19 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n     }\n \n     log.info(\"Serializing weights...\")\n-    selectForeach(selectWeightsForDumpSQL) { rs => \n-      serializer.addWeight(rs.long(\"id\"), rs.boolean(\"is_fixed\"), \n-        rs.double(\"initial_value\"), rs.string(\"description\"))\n+    selectForeach2(selectWeightsForDumpSQL) { rs => ",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": null,
        "original_position": 25,
        "commit_id": "15f9b8d17752756316bdc8968a930dc63f138351",
        "original_commit_id": "ae543dc3a55d0bf76f2e9ad6f8c637c0a529d883",
        "user": {
            "login": "dennybritz",
            "id": 403133,
            "node_id": "MDQ6VXNlcjQwMzEzMw==",
            "avatar_url": "https://avatars0.githubusercontent.com/u/403133?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/dennybritz",
            "html_url": "https://github.com/dennybritz",
            "followers_url": "https://api.github.com/users/dennybritz/followers",
            "following_url": "https://api.github.com/users/dennybritz/following{/other_user}",
            "gists_url": "https://api.github.com/users/dennybritz/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/dennybritz/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/dennybritz/subscriptions",
            "organizations_url": "https://api.github.com/users/dennybritz/orgs",
            "repos_url": "https://api.github.com/users/dennybritz/repos",
            "events_url": "https://api.github.com/users/dennybritz/events{/privacy}",
            "received_events_url": "https://api.github.com/users/dennybritz/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I think we should rename this function to something more descriptive ;) \n",
        "created_at": "2014-04-01T23:16:11Z",
        "updated_at": "2014-04-03T19:24:37Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/15#discussion_r11186787",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/15",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/11186787"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/15#discussion_r11186787"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/15"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/11220195",
        "pull_request_review_id": null,
        "id": 11220195,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExMjIwMTk1",
        "diff_hunk": "@@ -0,0 +1,79 @@\n+package org.deepdive.inference\n+\n+import org.deepdive.Logging\n+import org.deepdive.settings._\n+import java.io._\n+\n+class BinarySerializer(weightsOuput: OutputStream, variablesOutput: OutputStream, \n+  factorsOutput: OutputStream, edgesOutput: OutputStream, metaDataOutput: OutputStream) extends Serializer with Logging {\n+\n+  val weightStream = new DataOutputStream(weightsOuput)\n+  val variableStream = new DataOutputStream(variablesOutput)\n+  val factorStream = new DataOutputStream(factorsOutput)\n+  val edgeStream = new DataOutputStream(edgesOutput)\n+  val metaStream = new PrintStream(metaDataOutput)\n+\n+\n+\n+  def addWeight(weightId: Long, isFixed: Boolean, initialValue: Double, desc: String) : Unit = {\n+    weightStream.writeLong(weightId)\n+    weightStream.writeBoolean(isFixed)\n+    weightStream.writeDouble(initialValue)\n+    // nodescription\n+  }\n+\n+  def addVariable(variableId: Long, isEvidence: Boolean, initialValue: Double, \n+    dataType: String, edgeCount: Long, cardinality: Long) : Unit = {\n+    val variableDataType = dataType match {\n+      case \"Boolean\" => 'B'\n+      case \"Multinomial\" => 'M'\n+    } \n+    variableStream.writeLong(variableId)\n+    variableStream.writeBoolean(isEvidence)\n+    variableStream.writeDouble(initialValue)\n+    variableStream.writeChar(variableDataType)\n+    variableStream.writeLong(edgeCount)  \n+    variableStream.writeLong(cardinality)  \n+  }\n+\n+  def addFactor(factorId: Long, weightId: Long, factorFunction: String, edgeCount: Long) : Unit = {\n+    val factorFunctionType = factorFunction match {\n+      case \"ImplyFactorFunction\" => 'I'\n+      case \"OrFactorFunction\" => 'O'\n+      case \"AndFactorFunction\" => 'A'\n+      case \"EqualFactorFunction\" => 'E'\n+      case \"IsTrueFactorFunction\" =>  'I'",
        "path": "src/main/scala/org/deepdive/inference/serialization/BinarySerializer.scala",
        "position": null,
        "original_position": 45,
        "commit_id": "15f9b8d17752756316bdc8968a930dc63f138351",
        "original_commit_id": "ae543dc3a55d0bf76f2e9ad6f8c637c0a529d883",
        "user": {
            "login": "dennybritz",
            "id": 403133,
            "node_id": "MDQ6VXNlcjQwMzEzMw==",
            "avatar_url": "https://avatars0.githubusercontent.com/u/403133?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/dennybritz",
            "html_url": "https://github.com/dennybritz",
            "followers_url": "https://api.github.com/users/dennybritz/followers",
            "following_url": "https://api.github.com/users/dennybritz/following{/other_user}",
            "gists_url": "https://api.github.com/users/dennybritz/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/dennybritz/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/dennybritz/subscriptions",
            "organizations_url": "https://api.github.com/users/dennybritz/orgs",
            "repos_url": "https://api.github.com/users/dennybritz/repos",
            "events_url": "https://api.github.com/users/dennybritz/events{/privacy}",
            "received_events_url": "https://api.github.com/users/dennybritz/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Is IsTrue the same as Imply? I feel like this should be an integer value with documentation of which value maps to which function.\n",
        "created_at": "2014-04-02T18:32:45Z",
        "updated_at": "2014-04-03T19:24:37Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/15#discussion_r11220195",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/15",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/11220195"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/15#discussion_r11220195"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/15"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/11220266",
        "pull_request_review_id": null,
        "id": 11220266,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExMjIwMjY2",
        "diff_hunk": "@@ -0,0 +1,78 @@\n+package org.deepdive.inference\n+\n+import org.deepdive.Logging\n+import org.deepdive.settings._\n+import java.io._\n+\n+class CSVSerializer(weightsOuput: OutputStream, variablesOutput: OutputStream, \n+  factorsOutput: OutputStream, edgesOutput: OutputStream, metaDataOutput: OutputStream) extends Serializer with Logging {",
        "path": "src/main/scala/org/deepdive/inference/serialization/CSVSerializer.scala",
        "position": null,
        "original_position": 8,
        "commit_id": "15f9b8d17752756316bdc8968a930dc63f138351",
        "original_commit_id": "ae543dc3a55d0bf76f2e9ad6f8c637c0a529d883",
        "user": {
            "login": "dennybritz",
            "id": 403133,
            "node_id": "MDQ6VXNlcjQwMzEzMw==",
            "avatar_url": "https://avatars0.githubusercontent.com/u/403133?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/dennybritz",
            "html_url": "https://github.com/dennybritz",
            "followers_url": "https://api.github.com/users/dennybritz/followers",
            "following_url": "https://api.github.com/users/dennybritz/following{/other_user}",
            "gists_url": "https://api.github.com/users/dennybritz/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/dennybritz/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/dennybritz/subscriptions",
            "organizations_url": "https://api.github.com/users/dennybritz/orgs",
            "repos_url": "https://api.github.com/users/dennybritz/repos",
            "events_url": "https://api.github.com/users/dennybritz/events{/privacy}",
            "received_events_url": "https://api.github.com/users/dennybritz/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Just a minor thing, but I usually try to keep all lines < 100 characters because longer lines are difficult to view in git. (That's also in the Scala style guide http://docs.scala-lang.org/style/)\n",
        "created_at": "2014-04-02T18:34:03Z",
        "updated_at": "2014-04-03T19:24:37Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/15#discussion_r11220266",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/15",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/11220266"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/15#discussion_r11220266"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/15"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/11220306",
        "pull_request_review_id": null,
        "id": 11220306,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExMjIwMzA2",
        "diff_hunk": "@@ -0,0 +1,78 @@\n+package org.deepdive.inference\n+\n+import org.deepdive.Logging\n+import org.deepdive.settings._\n+import java.io._\n+\n+class CSVSerializer(weightsOuput: OutputStream, variablesOutput: OutputStream, \n+  factorsOutput: OutputStream, edgesOutput: OutputStream, metaDataOutput: OutputStream) extends Serializer with Logging {\n+\n+  val printStream_weight = new PrintStream(weightsOuput)\n+  val printStream_variable = new PrintStream(variablesOutput)\n+  val printStream_factor = new PrintStream(factorsOutput)\n+  val printStream_edge = new PrintStream(edgesOutput)\n+  val printStream_meta = new PrintStream(metaDataOutput)\n+\n+\n+\n+  def addWeight(weightId: Long, isFixed: Boolean, initialValue: Double, desc: String) : Unit = {\n+    var out = weightId.toString + \",\" + isFixed.toString + \",\"\n+    if (isFixed) out += initialValue.toString\n+    else out += \"0\"\n+    out += \",\" + desc + \"\\n\"\n+    printStream_weight.print(out)\n+  }\n+\n+  def addVariable(variableId: Long, isEvidence: Boolean, initialValue: Double, \n+    dataType: String, edgeCount: Long, cardinality: Long) : Unit = {\n+\n+    val variableDataType = dataType match {\n+      case \"Boolean\" => 'B'\n+      case \"Multinomial\" => 'M'\n+    } \n+    val out = variableId.toString + \",\" + isEvidence.toString + \",\" +",
        "path": "src/main/scala/org/deepdive/inference/serialization/CSVSerializer.scala",
        "position": null,
        "original_position": 33,
        "commit_id": "15f9b8d17752756316bdc8968a930dc63f138351",
        "original_commit_id": "ae543dc3a55d0bf76f2e9ad6f8c637c0a529d883",
        "user": {
            "login": "dennybritz",
            "id": 403133,
            "node_id": "MDQ6VXNlcjQwMzEzMw==",
            "avatar_url": "https://avatars0.githubusercontent.com/u/403133?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/dennybritz",
            "html_url": "https://github.com/dennybritz",
            "followers_url": "https://api.github.com/users/dennybritz/followers",
            "following_url": "https://api.github.com/users/dennybritz/following{/other_user}",
            "gists_url": "https://api.github.com/users/dennybritz/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/dennybritz/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/dennybritz/subscriptions",
            "organizations_url": "https://api.github.com/users/dennybritz/orgs",
            "repos_url": "https://api.github.com/users/dennybritz/repos",
            "events_url": "https://api.github.com/users/dennybritz/events{/privacy}",
            "received_events_url": "https://api.github.com/users/dennybritz/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "A nicer way to write this: List(x1, x2).mkString(\",\")\n",
        "created_at": "2014-04-02T18:34:45Z",
        "updated_at": "2014-04-03T19:24:37Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/15#discussion_r11220306",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/15",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/11220306"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/15#discussion_r11220306"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/15"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/11238475",
        "pull_request_review_id": null,
        "id": 11238475,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExMjM4NDc1",
        "diff_hunk": "@@ -0,0 +1,79 @@\n+package org.deepdive.inference\n+\n+import org.deepdive.Logging\n+import org.deepdive.settings._\n+import java.io._\n+\n+class BinarySerializer(weightsOuput: OutputStream, variablesOutput: OutputStream, \n+  factorsOutput: OutputStream, edgesOutput: OutputStream, metaDataOutput: OutputStream) extends Serializer with Logging {\n+\n+  val weightStream = new DataOutputStream(weightsOuput)\n+  val variableStream = new DataOutputStream(variablesOutput)\n+  val factorStream = new DataOutputStream(factorsOutput)\n+  val edgeStream = new DataOutputStream(edgesOutput)\n+  val metaStream = new PrintStream(metaDataOutput)\n+\n+\n+\n+  def addWeight(weightId: Long, isFixed: Boolean, initialValue: Double, desc: String) : Unit = {\n+    weightStream.writeLong(weightId)\n+    weightStream.writeBoolean(isFixed)\n+    weightStream.writeDouble(initialValue)\n+    // nodescription\n+  }\n+\n+  def addVariable(variableId: Long, isEvidence: Boolean, initialValue: Double, \n+    dataType: String, edgeCount: Long, cardinality: Long) : Unit = {\n+    val variableDataType = dataType match {\n+      case \"Boolean\" => 'B'\n+      case \"Multinomial\" => 'M'\n+    } \n+    variableStream.writeLong(variableId)\n+    variableStream.writeBoolean(isEvidence)\n+    variableStream.writeDouble(initialValue)\n+    variableStream.writeChar(variableDataType)\n+    variableStream.writeLong(edgeCount)  \n+    variableStream.writeLong(cardinality)  \n+  }\n+\n+  def addFactor(factorId: Long, weightId: Long, factorFunction: String, edgeCount: Long) : Unit = {\n+    val factorFunctionType = factorFunction match {\n+      case \"ImplyFactorFunction\" => 'I'\n+      case \"OrFactorFunction\" => 'O'\n+      case \"AndFactorFunction\" => 'A'\n+      case \"EqualFactorFunction\" => 'E'\n+      case \"IsTrueFactorFunction\" =>  'I'",
        "path": "src/main/scala/org/deepdive/inference/serialization/BinarySerializer.scala",
        "position": null,
        "original_position": 45,
        "commit_id": "15f9b8d17752756316bdc8968a930dc63f138351",
        "original_commit_id": "ae543dc3a55d0bf76f2e9ad6f8c637c0a529d883",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Oh sorry, my fault, will fix it. \nI used char mainly for space reason. If you think integer is cleaner, it's ok for me. \n",
        "created_at": "2014-04-03T04:08:47Z",
        "updated_at": "2014-04-03T19:24:37Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/15#discussion_r11238475",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/15",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/11238475"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/15#discussion_r11238475"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/15"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/11238508",
        "pull_request_review_id": null,
        "id": 11238508,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExMjM4NTA4",
        "diff_hunk": "@@ -0,0 +1,78 @@\n+package org.deepdive.inference\n+\n+import org.deepdive.Logging\n+import org.deepdive.settings._\n+import java.io._\n+\n+class CSVSerializer(weightsOuput: OutputStream, variablesOutput: OutputStream, \n+  factorsOutput: OutputStream, edgesOutput: OutputStream, metaDataOutput: OutputStream) extends Serializer with Logging {",
        "path": "src/main/scala/org/deepdive/inference/serialization/CSVSerializer.scala",
        "position": null,
        "original_position": 8,
        "commit_id": "15f9b8d17752756316bdc8968a930dc63f138351",
        "original_commit_id": "ae543dc3a55d0bf76f2e9ad6f8c637c0a529d883",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Thanks! Will pay attention\n",
        "created_at": "2014-04-03T04:10:39Z",
        "updated_at": "2014-04-03T19:24:37Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/15#discussion_r11238508",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/15",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/11238508"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/15#discussion_r11238508"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/15"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/11238549",
        "pull_request_review_id": null,
        "id": 11238549,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExMjM4NTQ5",
        "diff_hunk": "@@ -0,0 +1,78 @@\n+package org.deepdive.inference\n+\n+import org.deepdive.Logging\n+import org.deepdive.settings._\n+import java.io._\n+\n+class CSVSerializer(weightsOuput: OutputStream, variablesOutput: OutputStream, \n+  factorsOutput: OutputStream, edgesOutput: OutputStream, metaDataOutput: OutputStream) extends Serializer with Logging {\n+\n+  val printStream_weight = new PrintStream(weightsOuput)\n+  val printStream_variable = new PrintStream(variablesOutput)\n+  val printStream_factor = new PrintStream(factorsOutput)\n+  val printStream_edge = new PrintStream(edgesOutput)\n+  val printStream_meta = new PrintStream(metaDataOutput)\n+\n+\n+\n+  def addWeight(weightId: Long, isFixed: Boolean, initialValue: Double, desc: String) : Unit = {\n+    var out = weightId.toString + \",\" + isFixed.toString + \",\"\n+    if (isFixed) out += initialValue.toString\n+    else out += \"0\"\n+    out += \",\" + desc + \"\\n\"\n+    printStream_weight.print(out)\n+  }\n+\n+  def addVariable(variableId: Long, isEvidence: Boolean, initialValue: Double, \n+    dataType: String, edgeCount: Long, cardinality: Long) : Unit = {\n+\n+    val variableDataType = dataType match {\n+      case \"Boolean\" => 'B'\n+      case \"Multinomial\" => 'M'\n+    } \n+    val out = variableId.toString + \",\" + isEvidence.toString + \",\" +",
        "path": "src/main/scala/org/deepdive/inference/serialization/CSVSerializer.scala",
        "position": null,
        "original_position": 33,
        "commit_id": "15f9b8d17752756316bdc8968a930dc63f138351",
        "original_commit_id": "ae543dc3a55d0bf76f2e9ad6f8c637c0a529d883",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Oh nice! Thanks! \n",
        "created_at": "2014-04-03T04:13:48Z",
        "updated_at": "2014-04-03T19:24:37Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/15#discussion_r11238549",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/15",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/11238549"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/15#discussion_r11238549"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/15"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17522368",
        "pull_request_review_id": null,
        "id": 17522368,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTIyMzY4",
        "diff_hunk": "@@ -20,15 +20,20 @@ command to start gpfdist\n     gpfdist -d [directory] -p [port] &\n \n where you specify the directory for storing the files and the HTTP port to run on.\n+**Note that the directory should be an empty directory since DeepDive will clean up\n+this directory or overwrite files.**\n Then, in `application.conf`, specify the gpfdist settings in the `db.default` as\n follows\n \n     db.default {\n       gphost   : [host of gpfdist]\n       gpport   : [port of gpfdist]\n-      gppath   : [path to gpfdist directory]\n+      gppath   : [**absolute path** of gpfdist directory]\n     }\n \n+where gphost, gpport, gppath are the host, port, and abosolute path ",
        "path": "doc/doc/advanced/performance.md",
        "position": null,
        "original_position": 16,
        "commit_id": "98689f9da7035dcdc34d6833fa79a86a05f598df",
        "original_commit_id": "64136274f01402259ee618f6e8e41065172c4a27",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "@feiranwang fix typo \"abosolute\"\n",
        "created_at": "2014-09-14T20:43:56Z",
        "updated_at": "2014-09-15T05:48:43Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17522368",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17522368"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17522368"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17522371",
        "pull_request_review_id": null,
        "id": 17522371,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTIyMzcx",
        "diff_hunk": "@@ -23,7 +25,7 @@ env:\n     - PGPORT=5432\n     - PGHOST=localhost\n     - DBNAME=deepdive_test\n-    - LD_LIBRARY_PATH=/tmp/dw_linux/lib/dw_linux/lib:/tmp/dw_linux/lib/dw_linux/lib64\n+    - LD_LIBRARY_PATH=/tmp/dw_linux/lib:/tmp/dw_linux/lib64:/tmp/dw_linux/lib/numactl-2.0.9",
        "path": ".travis.yml",
        "position": 14,
        "original_position": 14,
        "commit_id": "98689f9da7035dcdc34d6833fa79a86a05f598df",
        "original_commit_id": "64136274f01402259ee618f6e8e41065172c4a27",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "@feiranwang  How come it worked before? Was it because the sampler was never called?\n",
        "created_at": "2014-09-14T20:44:48Z",
        "updated_at": "2014-09-15T05:48:43Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17522371",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17522371"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17522371"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17522377",
        "pull_request_review_id": null,
        "id": 17522377,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTIyMzc3",
        "diff_hunk": "@@ -20,15 +20,20 @@ command to start gpfdist\n     gpfdist -d [directory] -p [port] &\n \n where you specify the directory for storing the files and the HTTP port to run on.\n+**Note that the directory should be an empty directory since DeepDive will clean up\n+this directory or overwrite files.**",
        "path": "doc/doc/advanced/performance.md",
        "position": null,
        "original_position": 5,
        "commit_id": "98689f9da7035dcdc34d6833fa79a86a05f598df",
        "original_commit_id": "64136274f01402259ee618f6e8e41065172c4a27",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Remove \"Note that\". Avoid using bold font for the entire sentence.\n",
        "created_at": "2014-09-14T20:45:42Z",
        "updated_at": "2014-09-15T05:48:43Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17522377",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17522377"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17522377"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17522410",
        "pull_request_review_id": null,
        "id": 17522410,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTIyNDEw",
        "diff_hunk": "@@ -33,7 +33,19 @@ class Sampler extends Actor with ActorLogging {\n       // Depending on the exit value we return success or throw an exception\n       exitValue match {\n         case 0 => sender ! Success()\n-        case _ => throw new RuntimeException(\"sampling failed (see error log for more details)\")\n+        case _ => {\n+          import scala.sys.process._\n+          import java.lang.management\n+          import sun.management.VMManagement;\n+          import java.lang.management.ManagementFactory;\n+          import java.lang.management.RuntimeMXBean;\n+          import java.lang.reflect.Field;\n+          import java.lang.reflect.Method;\n+          var pid = ManagementFactory.getRuntimeMXBean().getName().toString\n+          val pattern = \"\"\"\\d+\"\"\".r\n+          pattern.findAllIn(pid).foreach(id => s\"kill -9 ${id}\".!)\n+          // throw new RuntimeException(\"sampling failed (see error log for more details)\")\n+        }",
        "path": "src/main/scala/org/deepdive/inference/Sampler.scala",
        "position": null,
        "original_position": 17,
        "commit_id": "98689f9da7035dcdc34d6833fa79a86a05f598df",
        "original_commit_id": "64136274f01402259ee618f6e8e41065172c4a27",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "@feiranwang  You must update the comment on line 33. Also please do not just comment out code. If the code is useless/dead, remove it. Don't leave commented out code that \"future people\" will wonder why it is commented (line 47)\n",
        "created_at": "2014-09-14T20:48:00Z",
        "updated_at": "2014-09-15T05:48:43Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17522410",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17522410"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17522410"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17522417",
        "pull_request_review_id": null,
        "id": 17522417,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTIyNDE3",
        "diff_hunk": "@@ -383,12 +383,12 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n \n     // clean up grounding folder (for parallel grounding)\n     if (parallelGrounding) {\n-      //val cleanFile = File.createTempFile(s\"clean\", \".sh\")\n-      //val writer = new PrintWriter(cleanFile)\n-      //writer.println(s\"rm -rf ${groundingPath}/*\")\n-      //writer.close()\n-      //log.info(\"Cleaning up grounding folder...\")\n-      //executeCmd(cleanFile.getAbsolutePath())\n+      val cleanFile = File.createTempFile(s\"clean\", \".sh\")\n+      val writer = new PrintWriter(cleanFile)\n+      writer.println(s\"rm -f ${groundingPath}/dd_*\")\n+      writer.close()\n+      log.info(\"Cleaning up grounding folder...\")\n+      executeCmd(cleanFile.getAbsolutePath())",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 15,
        "original_position": 15,
        "commit_id": "98689f9da7035dcdc34d6833fa79a86a05f598df",
        "original_commit_id": "64136274f01402259ee618f6e8e41065172c4a27",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "@feiranwang : Don't we create some directory in ${groundingPath}? Who is removing tmp, for example?\n",
        "created_at": "2014-09-14T20:49:03Z",
        "updated_at": "2014-09-15T05:48:43Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17522417",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17522417"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17522417"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17522422",
        "pull_request_review_id": null,
        "id": 17522422,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTIyNDIy",
        "diff_hunk": "@@ -398,7 +398,7 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n       executeQuery(createAssignIdFunctionSQL)\n       schema.foreach { case(variable, dataType) =>\n         val Array(relation, column) = variable.split('.')\n-        executeQuery(s\"\"\"SELECT fast_seqassign('${relation}', ${idoffset});\"\"\")\n+        executeQuery(s\"\"\"SELECT fast_seqassign('${relation.toLowerCase()}', ${idoffset});\"\"\")",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 24,
        "original_position": 24,
        "commit_id": "98689f9da7035dcdc34d6833fa79a86a05f598df",
        "original_commit_id": "64136274f01402259ee618f6e8e41065172c4a27",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "@feiranwang why is this needed? Is the bug here or in the definition of fast_seqassign?\n",
        "created_at": "2014-09-14T20:50:07Z",
        "updated_at": "2014-09-15T05:48:43Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17522422",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17522422"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17522422"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17522427",
        "pull_request_review_id": null,
        "id": 17522427,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTIyNDI3",
        "diff_hunk": "@@ -460,7 +460,7 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n \n       // dump variables, \n       // variable table join with holdout table - a variable is an evidence if it has initial value and it is not holdout\n-      du.unload(s\"variables_${relation}\", s\"${groundingPath}/variables_${relation}\", dbSettings, parallelGrounding,\n+      du.unload(s\"dd_variables_${relation}\", s\"${groundingPath}/dd_variables_${relation}\", dbSettings, parallelGrounding,",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 33,
        "original_position": 33,
        "commit_id": "98689f9da7035dcdc34d6833fa79a86a05f598df",
        "original_commit_id": "64136274f01402259ee618f6e8e41065172c4a27",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "@feiranwang  If we agreed that the gpfdist directory should be empty and somewhat _dedicated_ to DD, then there's no point in changing the names. This holds in general, not just for dd_variables. Given that this change seems to be everywhere now, I guess that's ok. \n\nWhat happened to the idea of using a metafile specifying which files we created or not? \n",
        "created_at": "2014-09-14T20:51:40Z",
        "updated_at": "2014-09-15T05:48:43Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17522427",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17522427"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17522427"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17522431",
        "pull_request_review_id": null,
        "id": 17522431,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTIyNDMx",
        "diff_hunk": "@@ -523,7 +523,7 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n \n       // handle factor id\n       if (usingGreenplum) {\n-        executeQuery(s\"SELECT fast_seqassign('${querytable}', ${factorid});\");\n+        executeQuery(s\"SELECT fast_seqassign('${querytable.toLowerCase()}', ${factorid});\");",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 60,
        "original_position": 60,
        "commit_id": "98689f9da7035dcdc34d6833fa79a86a05f598df",
        "original_commit_id": "64136274f01402259ee618f6e8e41065172c4a27",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "@feiranwang  Same comment as before about toLowerCase(), also in the following changes.\n",
        "created_at": "2014-09-14T20:52:00Z",
        "updated_at": "2014-09-15T05:48:43Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17522431",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17522431"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17522431"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17522438",
        "pull_request_review_id": null,
        "id": 17522438,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTIyNDM4",
        "diff_hunk": "@@ -0,0 +1,73 @@\n+package org.deepdive.test.integration",
        "path": "src/test/scala/integration/BiasedCoin.scala",
        "position": 1,
        "original_position": 1,
        "commit_id": "98689f9da7035dcdc34d6833fa79a86a05f598df",
        "original_commit_id": "64136274f01402259ee618f6e8e41065172c4a27",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "@feiranwang  This file needs comments throughout. Especially one explaining what is doing.\n",
        "created_at": "2014-09-14T20:53:27Z",
        "updated_at": "2014-09-15T05:48:43Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17522438",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17522438"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17522438"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17522441",
        "pull_request_review_id": null,
        "id": 17522441,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTIyNDQx",
        "diff_hunk": "@@ -0,0 +1,146 @@\n+// package org.deepdive.test.integration",
        "path": "src/test/scala/integration/Chunking.scala",
        "position": 9,
        "original_position": 1,
        "commit_id": "98689f9da7035dcdc34d6833fa79a86a05f598df",
        "original_commit_id": "64136274f01402259ee618f6e8e41065172c4a27",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "@feiranwang  Add comments, explaining why it is commented out.\n",
        "created_at": "2014-09-14T20:53:48Z",
        "updated_at": "2014-09-15T05:48:43Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17522441",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17522441"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17522441"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17522459",
        "pull_request_review_id": null,
        "id": 17522459,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTIyNDU5",
        "diff_hunk": "@@ -192,8 +192,8 @@ trait SQLInferenceDataStoreSpec extends FunSpec with BeforeAndAfter { this: SQLI\n         inferenceDataStore.init()\n         \n         // Create table with multinomial data\n-        SQL(s\"\"\"CREATE TABLE r1(id bigint, weight text,\n-          value bigint);\"\"\").execute.apply() \n+        SQL(s\"\"\"CREATE TABLE r1(weight text,\n+          value bigint, id bigint);\"\"\").execute.apply() ",
        "path": "src/test/scala/unit/inference/SQLInferenceDataStoreSpec.scala",
        "position": 44,
        "original_position": 29,
        "commit_id": "98689f9da7035dcdc34d6833fa79a86a05f598df",
        "original_commit_id": "64136274f01402259ee618f6e8e41065172c4a27",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "@feiranwang Add comment explaining the importance of columns order.\n",
        "created_at": "2014-09-14T20:55:38Z",
        "updated_at": "2014-09-15T05:48:43Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17522459",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17522459"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17522459"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17522463",
        "pull_request_review_id": null,
        "id": 17522463,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTIyNDYz",
        "diff_hunk": "@@ -228,8 +228,8 @@ trait SQLInferenceDataStoreSpec extends FunSpec with BeforeAndAfter { this: SQLI\n         inferenceDataStore.init()\n         \n         // Create table with multinomial data\n-        SQL(s\"\"\"CREATE TABLE r1(id bigint, weight text,\n-          value bigint);\"\"\").execute.apply() \n+        SQL(s\"\"\"CREATE TABLE r1(weight text,\n+          value bigint, id bigint);\"\"\").execute.apply() ",
        "path": "src/test/scala/unit/inference/SQLInferenceDataStoreSpec.scala",
        "position": 44,
        "original_position": 40,
        "commit_id": "98689f9da7035dcdc34d6833fa79a86a05f598df",
        "original_commit_id": "64136274f01402259ee618f6e8e41065172c4a27",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "@feiranwang  Same here, column order\n",
        "created_at": "2014-09-14T20:55:55Z",
        "updated_at": "2014-09-15T05:48:43Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17522463",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17522463"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17522463"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17526688",
        "pull_request_review_id": null,
        "id": 17526688,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTI2Njg4",
        "diff_hunk": "@@ -23,7 +25,7 @@ env:\n     - PGPORT=5432\n     - PGHOST=localhost\n     - DBNAME=deepdive_test\n-    - LD_LIBRARY_PATH=/tmp/dw_linux/lib/dw_linux/lib:/tmp/dw_linux/lib/dw_linux/lib64\n+    - LD_LIBRARY_PATH=/tmp/dw_linux/lib:/tmp/dw_linux/lib64:/tmp/dw_linux/lib/numactl-2.0.9",
        "path": ".travis.yml",
        "position": 14,
        "original_position": 14,
        "commit_id": "98689f9da7035dcdc34d6833fa79a86a05f598df",
        "original_commit_id": "64136274f01402259ee618f6e8e41065172c4a27",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "The sampler was never called before.\n",
        "created_at": "2014-09-15T05:16:05Z",
        "updated_at": "2014-09-15T05:48:43Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17526688",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17526688"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17526688"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17526699",
        "pull_request_review_id": null,
        "id": 17526699,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTI2Njk5",
        "diff_hunk": "@@ -20,15 +20,20 @@ command to start gpfdist\n     gpfdist -d [directory] -p [port] &\n \n where you specify the directory for storing the files and the HTTP port to run on.\n+**Note that the directory should be an empty directory since DeepDive will clean up\n+this directory or overwrite files.**",
        "path": "doc/doc/advanced/performance.md",
        "position": null,
        "original_position": 5,
        "commit_id": "98689f9da7035dcdc34d6833fa79a86a05f598df",
        "original_commit_id": "64136274f01402259ee618f6e8e41065172c4a27",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "done\n",
        "created_at": "2014-09-15T05:18:05Z",
        "updated_at": "2014-09-15T05:48:43Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17526699",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17526699"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17526699"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17526701",
        "pull_request_review_id": null,
        "id": 17526701,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTI2NzAx",
        "diff_hunk": "@@ -20,15 +20,20 @@ command to start gpfdist\n     gpfdist -d [directory] -p [port] &\n \n where you specify the directory for storing the files and the HTTP port to run on.\n+**Note that the directory should be an empty directory since DeepDive will clean up\n+this directory or overwrite files.**\n Then, in `application.conf`, specify the gpfdist settings in the `db.default` as\n follows\n \n     db.default {\n       gphost   : [host of gpfdist]\n       gpport   : [port of gpfdist]\n-      gppath   : [path to gpfdist directory]\n+      gppath   : [**absolute path** of gpfdist directory]\n     }\n \n+where gphost, gpport, gppath are the host, port, and abosolute path ",
        "path": "doc/doc/advanced/performance.md",
        "position": null,
        "original_position": 16,
        "commit_id": "98689f9da7035dcdc34d6833fa79a86a05f598df",
        "original_commit_id": "64136274f01402259ee618f6e8e41065172c4a27",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "done\n",
        "created_at": "2014-09-15T05:18:10Z",
        "updated_at": "2014-09-15T05:48:43Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17526701",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17526701"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17526701"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17526720",
        "pull_request_review_id": null,
        "id": 17526720,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTI2NzIw",
        "diff_hunk": "@@ -33,7 +33,19 @@ class Sampler extends Actor with ActorLogging {\n       // Depending on the exit value we return success or throw an exception\n       exitValue match {\n         case 0 => sender ! Success()\n-        case _ => throw new RuntimeException(\"sampling failed (see error log for more details)\")\n+        case _ => {\n+          import scala.sys.process._\n+          import java.lang.management\n+          import sun.management.VMManagement;\n+          import java.lang.management.ManagementFactory;\n+          import java.lang.management.RuntimeMXBean;\n+          import java.lang.reflect.Field;\n+          import java.lang.reflect.Method;\n+          var pid = ManagementFactory.getRuntimeMXBean().getName().toString\n+          val pattern = \"\"\"\\d+\"\"\".r\n+          pattern.findAllIn(pid).foreach(id => s\"kill -9 ${id}\".!)\n+          // throw new RuntimeException(\"sampling failed (see error log for more details)\")\n+        }",
        "path": "src/main/scala/org/deepdive/inference/Sampler.scala",
        "position": null,
        "original_position": 17,
        "commit_id": "98689f9da7035dcdc34d6833fa79a86a05f598df",
        "original_commit_id": "64136274f01402259ee618f6e8e41065172c4a27",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "done\n",
        "created_at": "2014-09-15T05:19:59Z",
        "updated_at": "2014-09-15T05:48:43Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17526720",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17526720"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17526720"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17526733",
        "pull_request_review_id": null,
        "id": 17526733,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTI2NzMz",
        "diff_hunk": "@@ -383,12 +383,12 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n \n     // clean up grounding folder (for parallel grounding)\n     if (parallelGrounding) {\n-      //val cleanFile = File.createTempFile(s\"clean\", \".sh\")\n-      //val writer = new PrintWriter(cleanFile)\n-      //writer.println(s\"rm -rf ${groundingPath}/*\")\n-      //writer.close()\n-      //log.info(\"Cleaning up grounding folder...\")\n-      //executeCmd(cleanFile.getAbsolutePath())\n+      val cleanFile = File.createTempFile(s\"clean\", \".sh\")\n+      val writer = new PrintWriter(cleanFile)\n+      writer.println(s\"rm -f ${groundingPath}/dd_*\")\n+      writer.close()\n+      log.info(\"Cleaning up grounding folder...\")\n+      executeCmd(cleanFile.getAbsolutePath())",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 15,
        "original_position": 15,
        "commit_id": "98689f9da7035dcdc34d6833fa79a86a05f598df",
        "original_commit_id": "64136274f01402259ee618f6e8e41065172c4a27",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "These directories are created in tobinary.py and removed in tobinary.py (tmp for example). But in case there already exists such files, remove them first.\n",
        "created_at": "2014-09-15T05:21:04Z",
        "updated_at": "2014-09-15T05:48:43Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17526733",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17526733"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17526733"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17526741",
        "pull_request_review_id": null,
        "id": 17526741,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTI2NzQx",
        "diff_hunk": "@@ -398,7 +398,7 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n       executeQuery(createAssignIdFunctionSQL)\n       schema.foreach { case(variable, dataType) =>\n         val Array(relation, column) = variable.split('.')\n-        executeQuery(s\"\"\"SELECT fast_seqassign('${relation}', ${idoffset});\"\"\")\n+        executeQuery(s\"\"\"SELECT fast_seqassign('${relation.toLowerCase()}', ${idoffset});\"\"\")",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 24,
        "original_position": 24,
        "commit_id": "98689f9da7035dcdc34d6833fa79a86a05f598df",
        "original_commit_id": "64136274f01402259ee618f6e8e41065172c4a27",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "postgres is case insensitive in table names. All table names are stored in lower case. fast_seqassign is case sensitive.\n",
        "created_at": "2014-09-15T05:22:04Z",
        "updated_at": "2014-09-15T05:48:43Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17526741",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17526741"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17526741"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17526766",
        "pull_request_review_id": null,
        "id": 17526766,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTI2NzY2",
        "diff_hunk": "@@ -523,7 +523,7 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n \n       // handle factor id\n       if (usingGreenplum) {\n-        executeQuery(s\"SELECT fast_seqassign('${querytable}', ${factorid});\");\n+        executeQuery(s\"SELECT fast_seqassign('${querytable.toLowerCase()}', ${factorid});\");",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 60,
        "original_position": 60,
        "commit_id": "98689f9da7035dcdc34d6833fa79a86a05f598df",
        "original_commit_id": "64136274f01402259ee618f6e8e41065172c4a27",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "same as above\n",
        "created_at": "2014-09-15T05:23:54Z",
        "updated_at": "2014-09-15T05:48:43Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17526766",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17526766"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17526766"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17526791",
        "pull_request_review_id": null,
        "id": 17526791,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTI2Nzkx",
        "diff_hunk": "@@ -460,7 +460,7 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n \n       // dump variables, \n       // variable table join with holdout table - a variable is an evidence if it has initial value and it is not holdout\n-      du.unload(s\"variables_${relation}\", s\"${groundingPath}/variables_${relation}\", dbSettings, parallelGrounding,\n+      du.unload(s\"dd_variables_${relation}\", s\"${groundingPath}/dd_variables_${relation}\", dbSettings, parallelGrounding,",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 33,
        "original_position": 33,
        "commit_id": "98689f9da7035dcdc34d6833fa79a86a05f598df",
        "original_commit_id": "64136274f01402259ee618f6e8e41065172c4a27",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I found using a specific prefix may be clearer and manageable. Also, if someone did not specify a dedicated directory, DeepDive will not cause dangerous trouble as before (rm -rf ...)\n",
        "created_at": "2014-09-15T05:27:44Z",
        "updated_at": "2014-09-15T05:48:43Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17526791",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17526791"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17526791"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17526829",
        "pull_request_review_id": null,
        "id": 17526829,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTI2ODI5",
        "diff_hunk": "@@ -0,0 +1,73 @@\n+package org.deepdive.test.integration",
        "path": "src/test/scala/integration/BiasedCoin.scala",
        "position": 1,
        "original_position": 1,
        "commit_id": "98689f9da7035dcdc34d6833fa79a86a05f598df",
        "original_commit_id": "64136274f01402259ee618f6e8e41065172c4a27",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "done\n",
        "created_at": "2014-09-15T05:32:27Z",
        "updated_at": "2014-09-15T05:48:43Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17526829",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17526829"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17526829"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17526832",
        "pull_request_review_id": null,
        "id": 17526832,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTI2ODMy",
        "diff_hunk": "@@ -23,7 +25,7 @@ env:\n     - PGPORT=5432\n     - PGHOST=localhost\n     - DBNAME=deepdive_test\n-    - LD_LIBRARY_PATH=/tmp/dw_linux/lib/dw_linux/lib:/tmp/dw_linux/lib/dw_linux/lib64\n+    - LD_LIBRARY_PATH=/tmp/dw_linux/lib:/tmp/dw_linux/lib64:/tmp/dw_linux/lib/numactl-2.0.9",
        "path": ".travis.yml",
        "position": 14,
        "original_position": 14,
        "commit_id": "98689f9da7035dcdc34d6833fa79a86a05f598df",
        "original_commit_id": "64136274f01402259ee618f6e8e41065172c4a27",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "OK, good\n",
        "created_at": "2014-09-15T05:32:37Z",
        "updated_at": "2014-09-15T05:48:43Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17526832",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17526832"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17526832"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17526836",
        "pull_request_review_id": null,
        "id": 17526836,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTI2ODM2",
        "diff_hunk": "@@ -383,12 +383,12 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n \n     // clean up grounding folder (for parallel grounding)\n     if (parallelGrounding) {\n-      //val cleanFile = File.createTempFile(s\"clean\", \".sh\")\n-      //val writer = new PrintWriter(cleanFile)\n-      //writer.println(s\"rm -rf ${groundingPath}/*\")\n-      //writer.close()\n-      //log.info(\"Cleaning up grounding folder...\")\n-      //executeCmd(cleanFile.getAbsolutePath())\n+      val cleanFile = File.createTempFile(s\"clean\", \".sh\")\n+      val writer = new PrintWriter(cleanFile)\n+      writer.println(s\"rm -f ${groundingPath}/dd_*\")\n+      writer.close()\n+      log.info(\"Cleaning up grounding folder...\")\n+      executeCmd(cleanFile.getAbsolutePath())",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 15,
        "original_position": 15,
        "commit_id": "98689f9da7035dcdc34d6833fa79a86a05f598df",
        "original_commit_id": "64136274f01402259ee618f6e8e41065172c4a27",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "ok, good\n",
        "created_at": "2014-09-15T05:32:50Z",
        "updated_at": "2014-09-15T05:48:43Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17526836",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17526836"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17526836"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17526839",
        "pull_request_review_id": null,
        "id": 17526839,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTI2ODM5",
        "diff_hunk": "@@ -398,7 +398,7 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n       executeQuery(createAssignIdFunctionSQL)\n       schema.foreach { case(variable, dataType) =>\n         val Array(relation, column) = variable.split('.')\n-        executeQuery(s\"\"\"SELECT fast_seqassign('${relation}', ${idoffset});\"\"\")\n+        executeQuery(s\"\"\"SELECT fast_seqassign('${relation.toLowerCase()}', ${idoffset});\"\"\")",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 24,
        "original_position": 24,
        "commit_id": "98689f9da7035dcdc34d6833fa79a86a05f598df",
        "original_commit_id": "64136274f01402259ee618f6e8e41065172c4a27",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "ok, good.\n",
        "created_at": "2014-09-15T05:33:20Z",
        "updated_at": "2014-09-15T05:48:43Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17526839",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17526839"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17526839"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17526842",
        "pull_request_review_id": null,
        "id": 17526842,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTI2ODQy",
        "diff_hunk": "@@ -460,7 +460,7 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n \n       // dump variables, \n       // variable table join with holdout table - a variable is an evidence if it has initial value and it is not holdout\n-      du.unload(s\"variables_${relation}\", s\"${groundingPath}/variables_${relation}\", dbSettings, parallelGrounding,\n+      du.unload(s\"dd_variables_${relation}\", s\"${groundingPath}/dd_variables_${relation}\", dbSettings, parallelGrounding,",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 33,
        "original_position": 33,
        "commit_id": "98689f9da7035dcdc34d6833fa79a86a05f598df",
        "original_commit_id": "64136274f01402259ee618f6e8e41065172c4a27",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "ok, good for me.\n",
        "created_at": "2014-09-15T05:33:35Z",
        "updated_at": "2014-09-15T05:48:43Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17526842",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17526842"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17526842"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17526844",
        "pull_request_review_id": null,
        "id": 17526844,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTI2ODQ0",
        "diff_hunk": "@@ -523,7 +523,7 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n \n       // handle factor id\n       if (usingGreenplum) {\n-        executeQuery(s\"SELECT fast_seqassign('${querytable}', ${factorid});\");\n+        executeQuery(s\"SELECT fast_seqassign('${querytable.toLowerCase()}', ${factorid});\");",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 60,
        "original_position": 60,
        "commit_id": "98689f9da7035dcdc34d6833fa79a86a05f598df",
        "original_commit_id": "64136274f01402259ee618f6e8e41065172c4a27",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "ok\n",
        "created_at": "2014-09-15T05:33:42Z",
        "updated_at": "2014-09-15T05:48:43Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17526844",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17526844"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17526844"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17526926",
        "pull_request_review_id": null,
        "id": 17526926,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTI2OTI2",
        "diff_hunk": "@@ -0,0 +1,146 @@\n+// package org.deepdive.test.integration",
        "path": "src/test/scala/integration/Chunking.scala",
        "position": 9,
        "original_position": 1,
        "commit_id": "98689f9da7035dcdc34d6833fa79a86a05f598df",
        "original_commit_id": "64136274f01402259ee618f6e8e41065172c4a27",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "done\n",
        "created_at": "2014-09-15T05:40:38Z",
        "updated_at": "2014-09-15T05:48:43Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17526926",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17526926"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17526926"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17527013",
        "pull_request_review_id": null,
        "id": 17527013,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTI3MDEz",
        "diff_hunk": "@@ -192,8 +192,8 @@ trait SQLInferenceDataStoreSpec extends FunSpec with BeforeAndAfter { this: SQLI\n         inferenceDataStore.init()\n         \n         // Create table with multinomial data\n-        SQL(s\"\"\"CREATE TABLE r1(id bigint, weight text,\n-          value bigint);\"\"\").execute.apply() \n+        SQL(s\"\"\"CREATE TABLE r1(weight text,\n+          value bigint, id bigint);\"\"\").execute.apply() ",
        "path": "src/test/scala/unit/inference/SQLInferenceDataStoreSpec.scala",
        "position": 44,
        "original_position": 29,
        "commit_id": "98689f9da7035dcdc34d6833fa79a86a05f598df",
        "original_commit_id": "64136274f01402259ee618f6e8e41065172c4a27",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "done\n",
        "created_at": "2014-09-15T05:49:07Z",
        "updated_at": "2014-09-15T05:49:07Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17527013",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17527013"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/134#discussion_r17527013"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/134"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17947858",
        "pull_request_review_id": null,
        "id": 17947858,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTQ3ODU4",
        "diff_hunk": "@@ -39,8 +39,8 @@ trait InferenceManager extends Actor with ActorLogging {\n   lazy val factorGraphDumpFileEdges = new File(s\"${Context.outputDir}/graph.edges\")\n   lazy val factorGraphDumpFileMeta = new File(s\"${Context.outputDir}/graph.meta\")\n   lazy val SamplingOutputDir = new File(s\"${Context.outputDir}\")\n-  lazy val SamplingOutputFile = new File(s\"${SamplingOutputDir}/inference_result.out\")\n-  lazy val SamplingOutputFileWeights = new File(s\"${SamplingOutputDir}/inference_result.out.weights\")\n+  lazy val SamplingOutputFile = new File(s\"${SamplingOutputDir}/inference_result.out.text\")\n+  lazy val SamplingOutputFileWeights = new File(s\"${SamplingOutputDir}/inference_result.out.weights.text\")",
        "path": "src/main/scala/org/deepdive/inference/InferenceManager.scala",
        "position": 7,
        "original_position": 7,
        "commit_id": "57a027e4ee1d2fe6bd4110c530de71eab6fc9d83",
        "original_commit_id": "a2adc9ee2a61c9ca24c908ea6db6f0ff2bd8b3a8",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": ".txt?\n",
        "created_at": "2014-09-24T00:16:40Z",
        "updated_at": "2014-09-24T18:43:27Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r17947858",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17947858"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r17947858"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17947869",
        "pull_request_review_id": null,
        "id": 17947869,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTQ3ODY5",
        "diff_hunk": "@@ -688,23 +683,31 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n             cweightid += rs.getLong(1)\n           }\n \n-          execute(s\"\"\"INSERT INTO ${WeightsTable}(id, isfixed, initvalue) SELECT id, isfixed, initvalue FROM ${weighttableForThisFactor};\"\"\")\n+          execute(s\"\"\"INSERT INTO ${WeightsTable}(id, isfixed, initvalue, description) \n+            SELECT id, isfixed, initvalue, ${weightDesc} FROM ${weighttableForThisFactor};\"\"\")\n \n-          // check null weight\n-          val weightChecklist = factorDesc.weight.variables.map(v => s\"\"\" \"${v}\" IS NULL \"\"\").mkString(\"AND\")\n-          issueQuery(s\"SELECT COUNT(*) FROM ${querytable} WHERE ${weightChecklist}\") { rs =>\n-            if (rs.getLong(1) > 0) {\n-              throw new RuntimeException(\"Weight variable has null values\")\n+          // check null weight (only if there are weight variables)\n+          if (hasWeightVariables) {\n+            val weightChecklist = factorDesc.weight.variables.map(v => s\"\"\" \"${v}\" IS NULL \"\"\").mkString(\"AND\")\n+            issueQuery(s\"SELECT COUNT(*) FROM ${querytable} WHERE ${weightChecklist}\") { rs =>\n+              if (rs.getLong(1) > 0) {\n+                throw new RuntimeException(\"Weight variable has null values\")\n+              }\n             }\n           }\n-\n+          ",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": null,
        "original_position": 137,
        "commit_id": "57a027e4ee1d2fe6bd4110c530de71eab6fc9d83",
        "original_commit_id": "a2adc9ee2a61c9ca24c908ea6db6f0ff2bd8b3a8",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Whitespace\n",
        "created_at": "2014-09-24T00:17:12Z",
        "updated_at": "2014-09-24T18:43:27Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r17947869",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17947869"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r17947869"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17947902",
        "pull_request_review_id": null,
        "id": 17947902,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTQ3OTAy",
        "diff_hunk": "@@ -688,23 +683,31 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n             cweightid += rs.getLong(1)\n           }\n \n-          execute(s\"\"\"INSERT INTO ${WeightsTable}(id, isfixed, initvalue) SELECT id, isfixed, initvalue FROM ${weighttableForThisFactor};\"\"\")\n+          execute(s\"\"\"INSERT INTO ${WeightsTable}(id, isfixed, initvalue, description) \n+            SELECT id, isfixed, initvalue, ${weightDesc} FROM ${weighttableForThisFactor};\"\"\")\n \n-          // check null weight\n-          val weightChecklist = factorDesc.weight.variables.map(v => s\"\"\" \"${v}\" IS NULL \"\"\").mkString(\"AND\")\n-          issueQuery(s\"SELECT COUNT(*) FROM ${querytable} WHERE ${weightChecklist}\") { rs =>\n-            if (rs.getLong(1) > 0) {\n-              throw new RuntimeException(\"Weight variable has null values\")\n+          // check null weight (only if there are weight variables)\n+          if (hasWeightVariables) {\n+            val weightChecklist = factorDesc.weight.variables.map(v => s\"\"\" \"${v}\" IS NULL \"\"\").mkString(\"AND\")\n+            issueQuery(s\"SELECT COUNT(*) FROM ${querytable} WHERE ${weightChecklist}\") { rs =>\n+              if (rs.getLong(1) > 0) {\n+                throw new RuntimeException(\"Weight variable has null values\")\n+              }\n             }\n           }\n-\n+          \n           // dump factors\n-          val weightjoinlist = factorDesc.weight.variables.map(v => s\"\"\" t0.\"${v}\" = t1.\"${v}\" \"\"\").mkString(\"AND\")\n+          // do not have join conditions if there are no weight variables, and t1 will only have 1 row\n+          val weightjoinlist = hasWeightVariables match {\n+            case true => \"WHERE \" + factorDesc.weight.variables.map(\n+                v => s\"\"\" t0.\"${v}\" = t1.\"${v}\" \"\"\").mkString(\"AND\")\n+            case false => \"\"\n+          }\n           du.unload(s\"${outfile}\", s\"${groundingPath}/${outfile}\", dbSettings, parallelGrounding,\n             s\"\"\"SELECT t0.id AS factor_id, t1.id AS weight_id, ${idcols}\n              FROM ${querytable} t0, ${weighttableForThisFactor} t1\n-             WHERE ${weightjoinlist};\"\"\")\n-        }\n+             ${weightjoinlist};\"\"\")\n+//        }",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": null,
        "original_position": 152,
        "commit_id": "57a027e4ee1d2fe6bd4110c530de71eab6fc9d83",
        "original_commit_id": "a2adc9ee2a61c9ca24c908ea6db6f0ff2bd8b3a8",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Iss this correct?  Where did the \"WHERE\" go?\n\nWhy is this } just commented out? If it is not needed anymore remove it.\n",
        "created_at": "2014-09-24T00:18:01Z",
        "updated_at": "2014-09-24T18:43:27Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r17947902",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17947902"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r17947902"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17947937",
        "pull_request_review_id": null,
        "id": 17947937,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTQ3OTM3",
        "diff_hunk": "@@ -37,4 +37,9 @@ dropdb $DBNAME\n createdb $DBNAME\n \n # Run the test\n-SBT_OPTS=\"-XX:MaxHeapSize=256m -Xmx512m -XX:MaxPermSize=256m\" sbt \"test\"\n+# SBT_OPTS=\"-XX:MaxHeapSize=256m -Xmx512m -XX:MaxPermSize=256m\" sbt \"test\"",
        "path": "test.sh",
        "position": null,
        "original_position": 5,
        "commit_id": "57a027e4ee1d2fe6bd4110c530de71eab6fc9d83",
        "original_commit_id": "a2adc9ee2a61c9ca24c908ea6db6f0ff2bd8b3a8",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "You export it below, just remove it.\n",
        "created_at": "2014-09-24T00:19:35Z",
        "updated_at": "2014-09-24T18:43:27Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r17947937",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17947937"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r17947937"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17948014",
        "pull_request_review_id": null,
        "id": 17948014,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTQ4MDE0",
        "diff_hunk": "@@ -39,8 +39,8 @@ trait InferenceManager extends Actor with ActorLogging {\n   lazy val factorGraphDumpFileEdges = new File(s\"${Context.outputDir}/graph.edges\")\n   lazy val factorGraphDumpFileMeta = new File(s\"${Context.outputDir}/graph.meta\")\n   lazy val SamplingOutputDir = new File(s\"${Context.outputDir}\")\n-  lazy val SamplingOutputFile = new File(s\"${SamplingOutputDir}/inference_result.out\")\n-  lazy val SamplingOutputFileWeights = new File(s\"${SamplingOutputDir}/inference_result.out.weights\")\n+  lazy val SamplingOutputFile = new File(s\"${SamplingOutputDir}/inference_result.out.text\")\n+  lazy val SamplingOutputFileWeights = new File(s\"${SamplingOutputDir}/inference_result.out.weights.text\")",
        "path": "src/main/scala/org/deepdive/inference/InferenceManager.scala",
        "position": 7,
        "original_position": 7,
        "commit_id": "57a027e4ee1d2fe6bd4110c530de71eab6fc9d83",
        "original_commit_id": "a2adc9ee2a61c9ca24c908ea6db6f0ff2bd8b3a8",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "The sampler output files have suffix \".text\"...\n",
        "created_at": "2014-09-24T00:20:54Z",
        "updated_at": "2014-09-24T18:43:27Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r17948014",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17948014"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r17948014"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17948026",
        "pull_request_review_id": null,
        "id": 17948026,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTQ4MDI2",
        "diff_hunk": "@@ -186,9 +291,9 @@ trait SQLInferenceDataStoreSpec extends FunSpec with BeforeAndAfter { this: SQLI\n           .map(rs => rs.long(\"count\")).single.apply().get\n         assert(numFactors === 100)\n \n-",
        "path": "src/test/scala/unit/inference/SQLInferenceDataStoreSpec.scala",
        "position": 124,
        "original_position": 124,
        "commit_id": "57a027e4ee1d2fe6bd4110c530de71eab6fc9d83",
        "original_commit_id": "a2adc9ee2a61c9ca24c908ea6db6f0ff2bd8b3a8",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Whitespace\n",
        "created_at": "2014-09-24T00:21:12Z",
        "updated_at": "2014-09-24T18:43:27Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r17948026",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17948026"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r17948026"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17948028",
        "pull_request_review_id": null,
        "id": 17948028,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTQ4MDI4",
        "diff_hunk": "@@ -186,9 +291,9 @@ trait SQLInferenceDataStoreSpec extends FunSpec with BeforeAndAfter { this: SQLI\n           .map(rs => rs.long(\"count\")).single.apply().get\n         assert(numFactors === 100)\n \n-\n       }\n \n+",
        "path": "src/test/scala/unit/inference/SQLInferenceDataStoreSpec.scala",
        "position": null,
        "original_position": 127,
        "commit_id": "57a027e4ee1d2fe6bd4110c530de71eab6fc9d83",
        "original_commit_id": "a2adc9ee2a61c9ca24c908ea6db6f0ff2bd8b3a8",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Whitespace\n",
        "created_at": "2014-09-24T00:21:23Z",
        "updated_at": "2014-09-24T18:43:27Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r17948028",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17948028"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r17948028"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17948041",
        "pull_request_review_id": null,
        "id": 17948041,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTQ4MDQx",
        "diff_hunk": "@@ -322,8 +510,8 @@ trait SQLInferenceDataStoreSpec extends FunSpec with BeforeAndAfter { this: SQLI\n \n     describe (\"writing back the inference Result\") {\n \n-      val variablesFile = getClass.getResource(\"/inference/sample_result.variables.pb\").getFile\n-      val weightsFile = getClass.getResource(\"/inference/sample_result.weights.pb\").getFile\n+      val variablesFile = getClass.getResource(\"/inference/sample_result.variables.text\").getFile\n+      val weightsFile = getClass.getResource(\"/inference/sample_result.weights.text\").getFile",
        "path": "src/test/scala/unit/inference/SQLInferenceDataStoreSpec.scala",
        "position": 275,
        "original_position": 228,
        "commit_id": "57a027e4ee1d2fe6bd4110c530de71eab6fc9d83",
        "original_commit_id": "a2adc9ee2a61c9ca24c908ea6db6f0ff2bd8b3a8",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "txt?\n",
        "created_at": "2014-09-24T00:21:37Z",
        "updated_at": "2014-09-24T18:43:27Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r17948041",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17948041"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r17948041"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17948058",
        "pull_request_review_id": null,
        "id": 17948058,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTQ4MDU4",
        "diff_hunk": "@@ -688,23 +683,31 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n             cweightid += rs.getLong(1)\n           }\n \n-          execute(s\"\"\"INSERT INTO ${WeightsTable}(id, isfixed, initvalue) SELECT id, isfixed, initvalue FROM ${weighttableForThisFactor};\"\"\")\n+          execute(s\"\"\"INSERT INTO ${WeightsTable}(id, isfixed, initvalue, description) \n+            SELECT id, isfixed, initvalue, ${weightDesc} FROM ${weighttableForThisFactor};\"\"\")\n \n-          // check null weight\n-          val weightChecklist = factorDesc.weight.variables.map(v => s\"\"\" \"${v}\" IS NULL \"\"\").mkString(\"AND\")\n-          issueQuery(s\"SELECT COUNT(*) FROM ${querytable} WHERE ${weightChecklist}\") { rs =>\n-            if (rs.getLong(1) > 0) {\n-              throw new RuntimeException(\"Weight variable has null values\")\n+          // check null weight (only if there are weight variables)\n+          if (hasWeightVariables) {\n+            val weightChecklist = factorDesc.weight.variables.map(v => s\"\"\" \"${v}\" IS NULL \"\"\").mkString(\"AND\")\n+            issueQuery(s\"SELECT COUNT(*) FROM ${querytable} WHERE ${weightChecklist}\") { rs =>\n+              if (rs.getLong(1) > 0) {\n+                throw new RuntimeException(\"Weight variable has null values\")\n+              }\n             }\n           }\n-\n+          \n           // dump factors\n-          val weightjoinlist = factorDesc.weight.variables.map(v => s\"\"\" t0.\"${v}\" = t1.\"${v}\" \"\"\").mkString(\"AND\")\n+          // do not have join conditions if there are no weight variables, and t1 will only have 1 row\n+          val weightjoinlist = hasWeightVariables match {\n+            case true => \"WHERE \" + factorDesc.weight.variables.map(\n+                v => s\"\"\" t0.\"${v}\" = t1.\"${v}\" \"\"\").mkString(\"AND\")\n+            case false => \"\"\n+          }\n           du.unload(s\"${outfile}\", s\"${groundingPath}/${outfile}\", dbSettings, parallelGrounding,\n             s\"\"\"SELECT t0.id AS factor_id, t1.id AS weight_id, ${idcols}\n              FROM ${querytable} t0, ${weighttableForThisFactor} t1\n-             WHERE ${weightjoinlist};\"\"\")\n-        }\n+             ${weightjoinlist};\"\"\")\n+//        }",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": null,
        "original_position": 152,
        "commit_id": "57a027e4ee1d2fe6bd4110c530de71eab6fc9d83",
        "original_commit_id": "a2adc9ee2a61c9ca24c908ea6db6f0ff2bd8b3a8",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "@zifeishan WHERE is moved into weightjoinlist, right?\n",
        "created_at": "2014-09-24T00:22:24Z",
        "updated_at": "2014-09-24T18:43:27Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r17948058",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17948058"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r17948058"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17948081",
        "pull_request_review_id": null,
        "id": 17948081,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTQ4MDgx",
        "diff_hunk": "@@ -322,8 +510,8 @@ trait SQLInferenceDataStoreSpec extends FunSpec with BeforeAndAfter { this: SQLI\n \n     describe (\"writing back the inference Result\") {\n \n-      val variablesFile = getClass.getResource(\"/inference/sample_result.variables.pb\").getFile\n-      val weightsFile = getClass.getResource(\"/inference/sample_result.weights.pb\").getFile\n+      val variablesFile = getClass.getResource(\"/inference/sample_result.variables.text\").getFile\n+      val weightsFile = getClass.getResource(\"/inference/sample_result.weights.text\").getFile",
        "path": "src/test/scala/unit/inference/SQLInferenceDataStoreSpec.scala",
        "position": 275,
        "original_position": 228,
        "commit_id": "57a027e4ee1d2fe6bd4110c530de71eab6fc9d83",
        "original_commit_id": "a2adc9ee2a61c9ca24c908ea6db6f0ff2bd8b3a8",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Same as above, sampler's output files end with \".text\"...\n",
        "created_at": "2014-09-24T00:23:32Z",
        "updated_at": "2014-09-24T18:43:27Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r17948081",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17948081"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r17948081"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17948102",
        "pull_request_review_id": null,
        "id": 17948102,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTQ4MTAy",
        "diff_hunk": "@@ -39,8 +39,8 @@ trait InferenceManager extends Actor with ActorLogging {\n   lazy val factorGraphDumpFileEdges = new File(s\"${Context.outputDir}/graph.edges\")\n   lazy val factorGraphDumpFileMeta = new File(s\"${Context.outputDir}/graph.meta\")\n   lazy val SamplingOutputDir = new File(s\"${Context.outputDir}\")\n-  lazy val SamplingOutputFile = new File(s\"${SamplingOutputDir}/inference_result.out\")\n-  lazy val SamplingOutputFileWeights = new File(s\"${SamplingOutputDir}/inference_result.out.weights\")\n+  lazy val SamplingOutputFile = new File(s\"${SamplingOutputDir}/inference_result.out.text\")\n+  lazy val SamplingOutputFileWeights = new File(s\"${SamplingOutputDir}/inference_result.out.weights.text\")",
        "path": "src/main/scala/org/deepdive/inference/InferenceManager.scala",
        "position": 7,
        "original_position": 7,
        "commit_id": "57a027e4ee1d2fe6bd4110c530de71eab6fc9d83",
        "original_commit_id": "a2adc9ee2a61c9ca24c908ea6db6f0ff2bd8b3a8",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "ok. How did it work until now? =)\n",
        "created_at": "2014-09-24T00:24:31Z",
        "updated_at": "2014-09-24T18:43:27Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r17948102",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17948102"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r17948102"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17948119",
        "pull_request_review_id": null,
        "id": 17948119,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTQ4MTE5",
        "diff_hunk": "@@ -322,8 +510,8 @@ trait SQLInferenceDataStoreSpec extends FunSpec with BeforeAndAfter { this: SQLI\n \n     describe (\"writing back the inference Result\") {\n \n-      val variablesFile = getClass.getResource(\"/inference/sample_result.variables.pb\").getFile\n-      val weightsFile = getClass.getResource(\"/inference/sample_result.weights.pb\").getFile\n+      val variablesFile = getClass.getResource(\"/inference/sample_result.variables.text\").getFile\n+      val weightsFile = getClass.getResource(\"/inference/sample_result.weights.text\").getFile",
        "path": "src/test/scala/unit/inference/SQLInferenceDataStoreSpec.scala",
        "position": 275,
        "original_position": 228,
        "commit_id": "57a027e4ee1d2fe6bd4110c530de71eab6fc9d83",
        "original_commit_id": "a2adc9ee2a61c9ca24c908ea6db6f0ff2bd8b3a8",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "ok\n",
        "created_at": "2014-09-24T00:24:54Z",
        "updated_at": "2014-09-24T18:43:27Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r17948119",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17948119"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r17948119"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17948142",
        "pull_request_review_id": null,
        "id": 17948142,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTQ4MTQy",
        "diff_hunk": "@@ -688,23 +683,31 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n             cweightid += rs.getLong(1)\n           }\n \n-          execute(s\"\"\"INSERT INTO ${WeightsTable}(id, isfixed, initvalue) SELECT id, isfixed, initvalue FROM ${weighttableForThisFactor};\"\"\")\n+          execute(s\"\"\"INSERT INTO ${WeightsTable}(id, isfixed, initvalue, description) \n+            SELECT id, isfixed, initvalue, ${weightDesc} FROM ${weighttableForThisFactor};\"\"\")\n \n-          // check null weight\n-          val weightChecklist = factorDesc.weight.variables.map(v => s\"\"\" \"${v}\" IS NULL \"\"\").mkString(\"AND\")\n-          issueQuery(s\"SELECT COUNT(*) FROM ${querytable} WHERE ${weightChecklist}\") { rs =>\n-            if (rs.getLong(1) > 0) {\n-              throw new RuntimeException(\"Weight variable has null values\")\n+          // check null weight (only if there are weight variables)\n+          if (hasWeightVariables) {\n+            val weightChecklist = factorDesc.weight.variables.map(v => s\"\"\" \"${v}\" IS NULL \"\"\").mkString(\"AND\")\n+            issueQuery(s\"SELECT COUNT(*) FROM ${querytable} WHERE ${weightChecklist}\") { rs =>\n+              if (rs.getLong(1) > 0) {\n+                throw new RuntimeException(\"Weight variable has null values\")\n+              }\n             }\n           }\n-\n+          \n           // dump factors\n-          val weightjoinlist = factorDesc.weight.variables.map(v => s\"\"\" t0.\"${v}\" = t1.\"${v}\" \"\"\").mkString(\"AND\")\n+          // do not have join conditions if there are no weight variables, and t1 will only have 1 row\n+          val weightjoinlist = hasWeightVariables match {\n+            case true => \"WHERE \" + factorDesc.weight.variables.map(\n+                v => s\"\"\" t0.\"${v}\" = t1.\"${v}\" \"\"\").mkString(\"AND\")\n+            case false => \"\"\n+          }\n           du.unload(s\"${outfile}\", s\"${groundingPath}/${outfile}\", dbSettings, parallelGrounding,\n             s\"\"\"SELECT t0.id AS factor_id, t1.id AS weight_id, ${idcols}\n              FROM ${querytable} t0, ${weighttableForThisFactor} t1\n-             WHERE ${weightjoinlist};\"\"\")\n-        }\n+             ${weightjoinlist};\"\"\")\n+//        }",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": null,
        "original_position": 152,
        "commit_id": "57a027e4ee1d2fe6bd4110c530de71eab6fc9d83",
        "original_commit_id": "a2adc9ee2a61c9ca24c908ea6db6f0ff2bd8b3a8",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "ok, Fix that }. The name of the variable is not great. What about weightjoincondition?\n",
        "created_at": "2014-09-24T00:25:43Z",
        "updated_at": "2014-09-24T18:43:27Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r17948142",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17948142"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r17948142"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17948169",
        "pull_request_review_id": null,
        "id": 17948169,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTQ4MTY5",
        "diff_hunk": "@@ -39,8 +39,8 @@ trait InferenceManager extends Actor with ActorLogging {\n   lazy val factorGraphDumpFileEdges = new File(s\"${Context.outputDir}/graph.edges\")\n   lazy val factorGraphDumpFileMeta = new File(s\"${Context.outputDir}/graph.meta\")\n   lazy val SamplingOutputDir = new File(s\"${Context.outputDir}\")\n-  lazy val SamplingOutputFile = new File(s\"${SamplingOutputDir}/inference_result.out\")\n-  lazy val SamplingOutputFileWeights = new File(s\"${SamplingOutputDir}/inference_result.out.weights\")\n+  lazy val SamplingOutputFile = new File(s\"${SamplingOutputDir}/inference_result.out.text\")\n+  lazy val SamplingOutputFileWeights = new File(s\"${SamplingOutputDir}/inference_result.out.weights.text\")",
        "path": "src/main/scala/org/deepdive/inference/InferenceManager.scala",
        "position": 7,
        "original_position": 7,
        "commit_id": "57a027e4ee1d2fe6bd4110c530de71eab6fc9d83",
        "original_commit_id": "a2adc9ee2a61c9ca24c908ea6db6f0ff2bd8b3a8",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "The \".text\" suffix was originally added in another function, but I feel it should be refactored. So I made this change.\n",
        "created_at": "2014-09-24T00:27:02Z",
        "updated_at": "2014-09-24T18:43:27Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r17948169",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17948169"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r17948169"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17948205",
        "pull_request_review_id": null,
        "id": 17948205,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTQ4MjA1",
        "diff_hunk": "@@ -39,8 +39,8 @@ trait InferenceManager extends Actor with ActorLogging {\n   lazy val factorGraphDumpFileEdges = new File(s\"${Context.outputDir}/graph.edges\")\n   lazy val factorGraphDumpFileMeta = new File(s\"${Context.outputDir}/graph.meta\")\n   lazy val SamplingOutputDir = new File(s\"${Context.outputDir}\")\n-  lazy val SamplingOutputFile = new File(s\"${SamplingOutputDir}/inference_result.out\")\n-  lazy val SamplingOutputFileWeights = new File(s\"${SamplingOutputDir}/inference_result.out.weights\")\n+  lazy val SamplingOutputFile = new File(s\"${SamplingOutputDir}/inference_result.out.text\")\n+  lazy val SamplingOutputFileWeights = new File(s\"${SamplingOutputDir}/inference_result.out.weights.text\")",
        "path": "src/main/scala/org/deepdive/inference/InferenceManager.scala",
        "position": 7,
        "original_position": 7,
        "commit_id": "57a027e4ee1d2fe6bd4110c530de71eab6fc9d83",
        "original_commit_id": "a2adc9ee2a61c9ca24c908ea6db6f0ff2bd8b3a8",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Good choice. It makes more sense here, imho.\n",
        "created_at": "2014-09-24T00:28:17Z",
        "updated_at": "2014-09-24T18:43:27Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r17948205",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17948205"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r17948205"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17955230",
        "pull_request_review_id": null,
        "id": 17955230,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTU1MjMw",
        "diff_hunk": "@@ -265,6 +369,89 @@ trait SQLInferenceDataStoreSpec extends FunSpec with BeforeAndAfter { this: SQLI\n \n       }\n \n+      it(\"should work with fixed weight\")\n+      {\n+        inferenceDataStore.init()\n+        \n+        // Create table with multinomial data\n+        SQL(s\"\"\"CREATE TABLE r1(weight text,\n+          value bigint, id bigint);\"\"\").execute.apply() \n+        val data = (1 to 100).map { i =>\n+          Map(\"id\" -> i, \"value\" -> (i%4))\n+        }\n+        dataStoreHelper.bulkInsert(\"r1\", data.iterator)\n+\n+        // Define the schema\n+        val schema = Map[String, VariableDataType](\"r1.value\" -> MultinomialType(4))\n+\n+        // Build the factor description\n+        val factorDesc = FactorDesc(\"testFactor\", \n+          \"\"\"SELECT id AS \"r1.id\", value AS \"r1.value\" FROM r1\"\"\", \n+          MultinomialFactorFunction(Seq(\"r1.value\")), \n+          KnownFactorWeight(0.37), \"weight_prefix\")\n+        val holdoutFraction = 0.0\n+\n+        // Ground the graph\n+        inferenceDataStore.groundFactorGraph(schema, Seq(factorDesc), holdoutFraction, None, false, \"\", dbSettings, false)\n+\n+        val numWeights = SQL(s\"\"\"SELECT COUNT(*) AS \"count\" FROM ${inferenceDataStore.WeightsTable}\"\"\")\n+          .map(rs => rs.long(\"count\")).single.apply().get\n+        assert(numWeights === 4)\n+\n+        // TODO: what does cardinality mean in this table?\n+\n+        // For fixed weights, all rows should have same initvalue          \n+        assert(SQL(s\"\"\"SELECT initvalue FROM ${inferenceDataStore.WeightsTable} limit 1\"\"\")\n+          .map(rs => rs.double(\"initvalue\")).single.apply().get === 0.37)\n+        assert(SQL(s\"\"\"SELECT isfixed FROM ${inferenceDataStore.WeightsTable} limit 1\"\"\")\n+          .map(rs => rs.int(\"isfixed\")).single.apply().get === 1)\n+\n+        // One factor for each possible predicate assignment\n+        val numFactors = SQL(s\"\"\"SELECT COUNT(*) AS \"count\" FROM dd_query_testFactor\"\"\")\n+          .map(rs => rs.long(\"count\")).single.apply().get\n+        assert(numFactors === 100)\n+\n+      }\n+\n+      it(\"should work with weight to learn without weight variables\")\n+      {\n+        inferenceDataStore.init()\n+        \n+        // Create table with multinomial data\n+        SQL(s\"\"\"CREATE TABLE r1(weight text,\n+          value bigint, id bigint);\"\"\").execute.apply() \n+        val data = (1 to 100).map { i =>\n+          Map(\"id\" -> i, \"value\" -> (i%4))\n+        }\n+        dataStoreHelper.bulkInsert(\"r1\", data.iterator)\n+\n+        // Define the schema\n+        val schema = Map[String, VariableDataType](\"r1.value\" -> MultinomialType(4))\n+\n+        // Build the factor description\n+        val factorDesc = FactorDesc(\"testFactor\", \n+          \"\"\"SELECT id AS \"r1.id\", value AS \"r1.value\" FROM r1\"\"\", \n+          MultinomialFactorFunction(Seq(\"r1.value\")), \n+          UnknownFactorWeight(List()), \"weight_prefix\")\n+        val holdoutFraction = 0.0\n+\n+        // Ground the graph\n+        inferenceDataStore.groundFactorGraph(schema, Seq(factorDesc), holdoutFraction, None, false, \"\", dbSettings, false)\n+\n+        val numWeights = SQL(s\"\"\"SELECT COUNT(*) AS \"count\" FROM ${inferenceDataStore.WeightsTable}\"\"\")\n+          .map(rs => rs.long(\"count\")).single.apply().get\n+        assert(numWeights === 4)\n+\n+        assert(SQL(s\"\"\"SELECT isfixed FROM ${inferenceDataStore.WeightsTable} limit 1\"\"\")\n+          .map(rs => rs.int(\"isfixed\")).single.apply().get === 0)\n+\n+        // One factor for each possible predicate assignment\n+        val numFactors = SQL(s\"\"\"SELECT COUNT(*) AS \"count\" FROM dd_query_testFactor\"\"\")\n+          .map(rs => rs.long(\"count\")).single.apply().get\n+        assert(numFactors === 100)\n+\n+      }\n+\n     }\n \n     // describe(\"dumping the factor graph\") {",
        "path": "src/test/scala/unit/inference/SQLInferenceDataStoreSpec.scala",
        "position": null,
        "original_position": 217,
        "commit_id": "57a027e4ee1d2fe6bd4110c530de71eab6fc9d83",
        "original_commit_id": "791cd54b960463897ebacbd0280f89899fe4f654",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Should this comment block be deleted?\n",
        "created_at": "2014-09-24T06:07:02Z",
        "updated_at": "2014-09-24T18:43:27Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r17955230",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17955230"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r17955230"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18004521",
        "pull_request_review_id": null,
        "id": 18004521,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4MDA0NTIx",
        "diff_hunk": "@@ -537,7 +528,8 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n \n     // weights table\n     execute(s\"\"\"DROP TABLE IF EXISTS ${WeightsTable} CASCADE;\"\"\")\n-    execute(s\"\"\"CREATE TABLE ${WeightsTable} (id bigint, isfixed int, initvalue real, cardinality text);\"\"\")\n+    execute(s\"\"\"CREATE TABLE ${WeightsTable} (id bigint, isfixed int, initvalue real, ",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 63,
        "original_position": 63,
        "commit_id": "57a027e4ee1d2fe6bd4110c530de71eab6fc9d83",
        "original_commit_id": "57a027e4ee1d2fe6bd4110c530de71eab6fc9d83",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "In some other tables the ID is still the first column, for example this table. @feiranwang Is it OK here?\n",
        "created_at": "2014-09-24T22:19:16Z",
        "updated_at": "2014-09-24T22:19:16Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r18004521",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18004521"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r18004521"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18004608",
        "pull_request_review_id": null,
        "id": 18004608,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4MDA0NjA4",
        "diff_hunk": "@@ -537,7 +528,8 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n \n     // weights table\n     execute(s\"\"\"DROP TABLE IF EXISTS ${WeightsTable} CASCADE;\"\"\")\n-    execute(s\"\"\"CREATE TABLE ${WeightsTable} (id bigint, isfixed int, initvalue real, cardinality text);\"\"\")\n+    execute(s\"\"\"CREATE TABLE ${WeightsTable} (id bigint, isfixed int, initvalue real, ",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 63,
        "original_position": 63,
        "commit_id": "57a027e4ee1d2fe6bd4110c530de71eab6fc9d83",
        "original_commit_id": "57a027e4ee1d2fe6bd4110c530de71eab6fc9d83",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "As long as they're not updated it will work, but try to be consistent.\n\nOn Wed, Sep 24, 2014 at 6:19 PM, Zifei Shan notifications@github.com\nwrote:\n\n> In\n> src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala:\n> \n> > @@ -537,7 +528,8 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n> > \n> > ```\n> >  // weights table\n> >  execute(s\"\"\"DROP TABLE IF EXISTS ${WeightsTable} CASCADE;\"\"\")\n> > ```\n> > -    execute(s\"\"\"CREATE TABLE ${WeightsTable} (id bigint, isfixed int, initvalue real, cardinality text);\"\"\")\n> > -    execute(s\"\"\"CREATE TABLE ${WeightsTable} (id bigint, isfixed int, initvalue real,\n> \n> In some other tables the ID is still the first column, for example this\n> table. @feiranwang https://github.com/feiranwang Is it OK here?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/HazyResearch/deepdive/pull/145/files#r18004521.\n\n## \n\nMatteo Riondato, Ph.D.\nPost-doc Researcher\nDepartment of Computer Science\nStanford University\n",
        "created_at": "2014-09-24T22:20:55Z",
        "updated_at": "2014-09-24T22:20:55Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r18004608",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18004608"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r18004608"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18004771",
        "pull_request_review_id": null,
        "id": 18004771,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4MDA0Nzcx",
        "diff_hunk": "@@ -583,6 +575,15 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n         case _ => 0.0\n       }\n \n+      // generate weight description\n+      def generateWeightDesc(weightPrefix: String, weightVariables: Seq[String]) : String = \n+        weightVariables.map ( v => s\"\"\"(CASE WHEN \"${v}\" IS NULL THEN '' ELSE \"${v}\"::text END)\"\"\" )",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 74,
        "original_position": 74,
        "commit_id": "57a027e4ee1d2fe6bd4110c530de71eab6fc9d83",
        "original_commit_id": "57a027e4ee1d2fe6bd4110c530de71eab6fc9d83",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Sorry I missed this earlier. It seems we don't need to use SQL conditions. Conditioning in Scala should already solve the problem, why struggling with SQL?\n\nI will change this in Mysql branch.\n",
        "created_at": "2014-09-24T22:24:23Z",
        "updated_at": "2014-09-24T22:24:23Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r18004771",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18004771"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r18004771"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18004897",
        "pull_request_review_id": null,
        "id": 18004897,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4MDA0ODk3",
        "diff_hunk": "@@ -583,6 +575,15 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n         case _ => 0.0\n       }\n \n+      // generate weight description\n+      def generateWeightDesc(weightPrefix: String, weightVariables: Seq[String]) : String = \n+        weightVariables.map ( v => s\"\"\"(CASE WHEN \"${v}\" IS NULL THEN '' ELSE \"${v}\"::text END)\"\"\" )",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 74,
        "original_position": 74,
        "commit_id": "57a027e4ee1d2fe6bd4110c530de71eab6fc9d83",
        "original_commit_id": "57a027e4ee1d2fe6bd4110c530de71eab6fc9d83",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Branches/Merges are cheap in git. Just commit to this branch and merge\nagain to master and develop\n\nOn Wed, Sep 24, 2014 at 6:24 PM, Zifei Shan notifications@github.com\nwrote:\n\n> In\n> src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala:\n> \n> > @@ -583,6 +575,15 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n> >          case _ => 0.0\n> >        }\n> > -      // generate weight description\n> > -      def generateWeightDesc(weightPrefix: String, weightVariables: Seq[String]) : String =\n> > -        weightVariables.map ( v => s\"\"\"(CASE WHEN \"${v}\" IS NULL THEN '' ELSE \"${v}\"::text END)\"\"\" )\n> \n> Sorry I missed this earlier. It seems we don't need to use SQL conditions.\n> Conditioning in Scala should already solve the problem, why struggling with\n> SQL?\n> \n> I will change this in Mysql branch.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/HazyResearch/deepdive/pull/145/files#r18004771.\n\n## \n\nMatteo Riondato, Ph.D.\nPost-doc Researcher\nDepartment of Computer Science\nStanford University\n",
        "created_at": "2014-09-24T22:28:06Z",
        "updated_at": "2014-09-24T22:28:06Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r18004897",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18004897"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r18004897"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18006061",
        "pull_request_review_id": null,
        "id": 18006061,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4MDA2MDYx",
        "diff_hunk": "@@ -537,7 +528,8 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n \n     // weights table\n     execute(s\"\"\"DROP TABLE IF EXISTS ${WeightsTable} CASCADE;\"\"\")\n-    execute(s\"\"\"CREATE TABLE ${WeightsTable} (id bigint, isfixed int, initvalue real, cardinality text);\"\"\")\n+    execute(s\"\"\"CREATE TABLE ${WeightsTable} (id bigint, isfixed int, initvalue real, ",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 63,
        "original_position": 63,
        "commit_id": "57a027e4ee1d2fe6bd4110c530de71eab6fc9d83",
        "original_commit_id": "57a027e4ee1d2fe6bd4110c530de71eab6fc9d83",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Matteo is right, this table is not updated using fast_seqassign, but better to be consistent.\n",
        "created_at": "2014-09-24T22:58:02Z",
        "updated_at": "2014-09-24T22:58:02Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r18006061",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18006061"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/145#discussion_r18006061"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/145"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18739506",
        "pull_request_review_id": null,
        "id": 18739506,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4NzM5NTA2",
        "diff_hunk": "@@ -9,12 +9,22 @@ import org.deepdive.profiling.Profiler\n import org.deepdive.settings._\n import scala.concurrent.Await\n import scala.concurrent.duration._\n+import scala.util.Try\n \n /* Describes the context of the DeepDive application */\n object Context extends Logging {\n \n   lazy val system = ActorSystem(\"deepdive\")\n   var outputDir = \"out\"\n+  \n+  // Set deepdive home according to environment variable. If not specified, \n+  // use user's current directory\n+  val deepdiveHome = System.getenv(\"DEEPDIVE_HOME\") match {\n+    case null => \".\"",
        "path": "src/main/scala/org/deepdive/Context.scala",
        "position": null,
        "original_position": 15,
        "commit_id": "afeeb9a098c5f482f7a73dacdd3cea8218f030b0",
        "original_commit_id": "8b3941bdd8b03485ebed56ebbca98f8fb64858e7",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "@zifeishan : what about System.getProperty(\"user.dir\") instead?\n",
        "created_at": "2014-10-11T05:12:50Z",
        "updated_at": "2014-10-11T07:43:56Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/153#discussion_r18739506",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/153",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18739506"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/153#discussion_r18739506"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/153"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18739507",
        "pull_request_review_id": null,
        "id": 18739507,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4NzM5NTA3",
        "diff_hunk": "@@ -9,12 +9,22 @@ import org.deepdive.profiling.Profiler\n import org.deepdive.settings._\n import scala.concurrent.Await\n import scala.concurrent.duration._\n+import scala.util.Try\n \n /* Describes the context of the DeepDive application */\n object Context extends Logging {\n \n   lazy val system = ActorSystem(\"deepdive\")\n   var outputDir = \"out\"\n+  \n+  // Set deepdive home according to environment variable. If not specified, \n+  // use user's current directory\n+  val deepdiveHome = System.getenv(\"DEEPDIVE_HOME\") match {\n+    case null => \".\"\n+    case _ => System.getenv(\"DEEPDIVE_HOME\")\n+  }\n+  \n+//      System.getProperty(\"user.dir\"))",
        "path": "src/main/scala/org/deepdive/Context.scala",
        "position": null,
        "original_position": 19,
        "commit_id": "afeeb9a098c5f482f7a73dacdd3cea8218f030b0",
        "original_commit_id": "8b3941bdd8b03485ebed56ebbca98f8fb64858e7",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "@zifeishan Remove if not used.\n",
        "created_at": "2014-10-11T05:12:59Z",
        "updated_at": "2014-10-11T07:43:56Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/153#discussion_r18739507",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/153",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18739507"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/153#discussion_r18739507"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/153"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18740036",
        "pull_request_review_id": null,
        "id": 18740036,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4NzQwMDM2",
        "diff_hunk": "@@ -9,12 +9,22 @@ import org.deepdive.profiling.Profiler\n import org.deepdive.settings._\n import scala.concurrent.Await\n import scala.concurrent.duration._\n+import scala.util.Try\n \n /* Describes the context of the DeepDive application */\n object Context extends Logging {\n \n   lazy val system = ActorSystem(\"deepdive\")\n   var outputDir = \"out\"\n+  \n+  // Set deepdive home according to environment variable. If not specified, \n+  // use user's current directory\n+  val deepdiveHome = System.getenv(\"DEEPDIVE_HOME\") match {\n+    case null => \".\"\n+    case _ => System.getenv(\"DEEPDIVE_HOME\")\n+  }\n+  \n+//      System.getProperty(\"user.dir\"))",
        "path": "src/main/scala/org/deepdive/Context.scala",
        "position": null,
        "original_position": 19,
        "commit_id": "afeeb9a098c5f482f7a73dacdd3cea8218f030b0",
        "original_commit_id": "8b3941bdd8b03485ebed56ebbca98f8fb64858e7",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Removed.\n",
        "created_at": "2014-10-11T07:45:01Z",
        "updated_at": "2014-10-11T07:45:01Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/153#discussion_r18740036",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/153",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18740036"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/153#discussion_r18740036"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/153"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18740041",
        "pull_request_review_id": null,
        "id": 18740041,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4NzQwMDQx",
        "diff_hunk": "@@ -9,12 +9,22 @@ import org.deepdive.profiling.Profiler\n import org.deepdive.settings._\n import scala.concurrent.Await\n import scala.concurrent.duration._\n+import scala.util.Try\n \n /* Describes the context of the DeepDive application */\n object Context extends Logging {\n \n   lazy val system = ActorSystem(\"deepdive\")\n   var outputDir = \"out\"\n+  \n+  // Set deepdive home according to environment variable. If not specified, \n+  // use user's current directory\n+  val deepdiveHome = System.getenv(\"DEEPDIVE_HOME\") match {\n+    case null => \".\"",
        "path": "src/main/scala/org/deepdive/Context.scala",
        "position": null,
        "original_position": 15,
        "commit_id": "afeeb9a098c5f482f7a73dacdd3cea8218f030b0",
        "original_commit_id": "8b3941bdd8b03485ebed56ebbca98f8fb64858e7",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Changed to user.dir and it works.\n",
        "created_at": "2014-10-11T07:45:16Z",
        "updated_at": "2014-10-11T07:45:16Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/153#discussion_r18740041",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/153",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18740041"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/153#discussion_r18740041"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/153"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17926249",
        "pull_request_review_id": null,
        "id": 17926249,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTI2MjQ5",
        "diff_hunk": "@@ -36,10 +36,10 @@ deepdive {\n     poolMaxSize=512\n     poolConnectionTimeoutMillis=1000\n     # Default values: use environment variables\n-    user     : ${PGUSER}\n-    dbname   : ${DBNAME}\n-    host     : ${PGHOST}\n-    port     : ${PGPORT}\n+    # user     : ${PGUSER}\n+    # dbname   : ${DBNAME}\n+    # host     : ${PGHOST}\n+    # port     : ${PGPORT}",
        "path": "src/main/resources/application.conf",
        "position": 11,
        "original_position": 11,
        "commit_id": "1557aa91ca7954afa6afe49d6a62b8f4d36f6982",
        "original_commit_id": "603084d9a6d1f70ecbd3790292fdab8cb7e66422",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Will this affect Postgres' loading/unloading? Some of these dbsettings may be used in data loading/unloading.\n",
        "created_at": "2014-09-23T17:54:02Z",
        "updated_at": "2014-10-20T03:07:00Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17926249",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17926249"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17926249"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17926457",
        "pull_request_review_id": null,
        "id": 17926457,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTI2NDU3",
        "diff_hunk": "@@ -12,15 +13,24 @@ class DataLoader extends JdbcDataStore with Logging {\n \n   // def ds : JdbcDataStore\n \n+  /**\n+   * Split the query so that it can execute multiple queries in both \n+   * psql and mysql settings\n+   */\n   def executeQuery(sql: String) = {",
        "path": "src/main/scala/org/deepdive/datastore/Dataloader.scala",
        "position": null,
        "original_position": 16,
        "commit_id": "1557aa91ca7954afa6afe49d6a62b8f4d36f6982",
        "original_commit_id": "603084d9a6d1f70ecbd3790292fdab8cb7e66422",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Should we refactor out this function? It seems defined in multiple places...\n",
        "created_at": "2014-09-23T17:57:13Z",
        "updated_at": "2014-10-20T03:07:00Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17926457",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17926457"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17926457"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17926997",
        "pull_request_review_id": null,
        "id": 17926997,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTI2OTk3",
        "diff_hunk": "@@ -80,29 +98,38 @@ class DataLoader extends JdbcDataStore with Logging {\n       val outfile = new File(filepath)\n       outfile.getParentFile().mkdirs()\n \n-      val cmdfile = File.createTempFile(s\"copy\", \".sh\")\n+      val cmdfile = File.createTempFile(s\"unload\", \".sh\")\n       val writer = new PrintWriter(cmdfile)\n-      val sql = \"\"\"\\COPY \"\"\" + s\"(SELECT * FROM _${filename}_view) TO '${filepath}'\"\n-      val copyStr = Helpers.buildPsqlCmd(dbSettings, sql)\n+      \n+      val copyStr = dbtype match {\n+        case Psql => List(sqlQueryPrefixRun + \"\\\"\", ",
        "path": "src/main/scala/org/deepdive/datastore/Dataloader.scala",
        "position": null,
        "original_position": 84,
        "commit_id": "1557aa91ca7954afa6afe49d6a62b8f4d36f6982",
        "original_commit_id": "603084d9a6d1f70ecbd3790292fdab8cb7e66422",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Maybe better to use the same pattern (copy + redirection)?\n",
        "created_at": "2014-09-23T18:05:48Z",
        "updated_at": "2014-10-20T03:07:00Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17926997",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17926997"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17926997"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17927480",
        "pull_request_review_id": null,
        "id": 17927480,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTI3NDgw",
        "diff_hunk": "@@ -117,6 +108,10 @@ class ExtractorRunner(dataStore: JsonExtractionDataStore, dbSettings: DbSettings\n       task.extractor.style match {\n \n         case \"json_extractor\" =>\n+          if (dbtype != Psql) {\n+            failTask(s\"do not support ${task.extractor.style} on ${dbtype}.\", taskSender)\n+          }\n+          ",
        "path": "src/main/scala/org/deepdive/extraction/ExtractorRunner.scala",
        "position": 77,
        "original_position": 64,
        "commit_id": "1557aa91ca7954afa6afe49d6a62b8f4d36f6982",
        "original_commit_id": "603084d9a6d1f70ecbd3790292fdab8cb7e66422",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Will this terminate/hang DeepDive?\n",
        "created_at": "2014-09-23T18:12:18Z",
        "updated_at": "2014-10-20T03:07:00Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17927480",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17927480"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17927480"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17927782",
        "pull_request_review_id": null,
        "id": 17927782,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTI3Nzgy",
        "diff_hunk": "@@ -512,7 +545,13 @@ class ExtractorRunner(dataStore: JsonExtractionDataStore, dbSettings: DbSettings\n       case null => \"\"\n       case _ => \" > \" + pipeOutFilePath\n     }\n-    val cmd =  sqlQueryPrefix + \" -c \" + \"\\\"\\\"\\\"\" + query + \"\\\"\\\"\\\"\" + pipeOutStr\n+    val cmd = dbtype match {\n+      case Psql => sqlQueryPrefix +\n+        \" -c \" + \"\\\"\" + query + \"\\\"\" + pipeOutStr",
        "path": "src/main/scala/org/deepdive/extraction/ExtractorRunner.scala",
        "position": null,
        "original_position": 269,
        "commit_id": "1557aa91ca7954afa6afe49d6a62b8f4d36f6982",
        "original_commit_id": "603084d9a6d1f70ecbd3790292fdab8cb7e66422",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "the pattern psql/mysql [option string] -c \"\" seems to appear in multiple palces. Shall we make it a helper function?\n",
        "created_at": "2014-09-23T18:16:47Z",
        "updated_at": "2014-10-20T03:07:00Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17927782",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17927782"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17927782"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17928059",
        "pull_request_review_id": null,
        "id": 17928059,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTI4MDU5",
        "diff_hunk": "@@ -520,17 +559,15 @@ class ExtractorRunner(dataStore: JsonExtractionDataStore, dbSettings: DbSettings\n     executeScriptOrFail(file.getAbsolutePath(), failureReceiver)\n     // executeScriptOrFail(cmd, failureReceiver)\n     \n-    // executeCmd(s\"psql -d ${dbname} -c \" + \"\\\"\\\"\\\"\" + query + \"\\\"\\\"\\\"\")\n+    // executeCmd(s\"psql -d ${dbname} -c \" + \"\\\"\" + query + \"\\\"\")",
        "path": "src/main/scala/org/deepdive/extraction/ExtractorRunner.scala",
        "position": null,
        "original_position": 282,
        "commit_id": "1557aa91ca7954afa6afe49d6a62b8f4d36f6982",
        "original_commit_id": "603084d9a6d1f70ecbd3790292fdab8cb7e66422",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "query could contain \"\" (e.g., select \"id\" from ...). I think triple quote is better here.\n",
        "created_at": "2014-09-23T18:20:41Z",
        "updated_at": "2014-10-20T03:07:00Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17928059",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17928059"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17928059"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17928356",
        "pull_request_review_id": null,
        "id": 17928356,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTI4MzU2",
        "diff_hunk": "@@ -24,30 +98,21 @@ object Helpers extends Logging {\n   }\n \n   /** Build a psql command */\n-  def buildPsqlCmd(dbSettings: DbSettings, query: String) : String = {\n-    // Get Database-related settings\n-    val dbname = dbSettings.dbname\n-    val pguser = dbSettings.user\n-    val pgport = dbSettings.port\n-    val pghost = dbSettings.host\n-    // TODO do not use password for now\n-    val dbnameStr = dbname match {\n-      case null => \"\"\n-      case _ => s\" -d ${dbname}\"\n-    }\n-    val pguserStr = pguser match {\n-      case null => \"\"\n-      case _ => s\" -U ${pguser}\"\n-    }\n-    val pgportStr = pgport match {\n-      case null => \"\"\n-      case _ => s\" -p ${pgport}\"\n+  def buildSqlCmd(dbSettings: DbSettings, query: String) : String = {\n+\n+    // Branch by database driver type\n+    val dbtype = getDbType(dbSettings)\n+    \n+    val sqlQueryPrefix = dbtype match {\n+      case Psql => \"psql \" + getOptionString(dbSettings)\n+      case Mysql => \"mysql \" + getOptionString(dbSettings)\n     }\n-    val pghostStr = pghost match {\n-      case null => \"\"\n-      case _ => s\" -h ${pghost}\"\n+\n+    // Return the command below\n+    dbtype match {\n+      case Psql => sqlQueryPrefix + \" -c \" + \"\\\"\" + query + \"\\\"\"\n+      case Mysql => sqlQueryPrefix + \" --silent -e \" + \"\\\"\" + query + \"\\\"\"\n+      // TODO what happens if \" is in query?",
        "path": "src/main/scala/org/deepdive/helpers/Helpers.scala",
        "position": null,
        "original_position": 121,
        "commit_id": "1557aa91ca7954afa6afe49d6a62b8f4d36f6982",
        "original_commit_id": "603084d9a6d1f70ecbd3790292fdab8cb7e66422",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "same as above. Triple quotes is better for psql. Not sure whether mysql supports triple quteos\n",
        "created_at": "2014-09-23T18:24:26Z",
        "updated_at": "2014-10-20T03:07:00Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17928356",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17928356"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17928356"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17928864",
        "pull_request_review_id": null,
        "id": 17928864,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTI4ODY0",
        "diff_hunk": "@@ -24,30 +98,21 @@ object Helpers extends Logging {\n   }\n \n   /** Build a psql command */\n-  def buildPsqlCmd(dbSettings: DbSettings, query: String) : String = {\n-    // Get Database-related settings\n-    val dbname = dbSettings.dbname\n-    val pguser = dbSettings.user\n-    val pgport = dbSettings.port\n-    val pghost = dbSettings.host\n-    // TODO do not use password for now\n-    val dbnameStr = dbname match {\n-      case null => \"\"\n-      case _ => s\" -d ${dbname}\"\n-    }\n-    val pguserStr = pguser match {\n-      case null => \"\"\n-      case _ => s\" -U ${pguser}\"\n-    }\n-    val pgportStr = pgport match {\n-      case null => \"\"\n-      case _ => s\" -p ${pgport}\"\n+  def buildSqlCmd(dbSettings: DbSettings, query: String) : String = {\n+\n+    // Branch by database driver type\n+    val dbtype = getDbType(dbSettings)\n+    \n+    val sqlQueryPrefix = dbtype match {\n+      case Psql => \"psql \" + getOptionString(dbSettings)\n+      case Mysql => \"mysql \" + getOptionString(dbSettings)\n     }\n-    val pghostStr = pghost match {\n-      case null => \"\"\n-      case _ => s\" -h ${pghost}\"\n+\n+    // Return the command below\n+    dbtype match {\n+      case Psql => sqlQueryPrefix + \" -c \" + \"\\\"\" + query + \"\\\"\"\n+      case Mysql => sqlQueryPrefix + \" --silent -e \" + \"\\\"\" + query + \"\\\"\"\n+      // TODO what happens if \" is in query?",
        "path": "src/main/scala/org/deepdive/helpers/Helpers.scala",
        "position": null,
        "original_position": 121,
        "commit_id": "1557aa91ca7954afa6afe49d6a62b8f4d36f6982",
        "original_commit_id": "603084d9a6d1f70ecbd3790292fdab8cb7e66422",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Why is triple quotes better? I think it is a historical issue, and It turned out that Truple quotes are a wrong syntax in bash. It is same with single quotes. I think we should use single quotes all the way. \n\nI got the advice from @netj .\n",
        "created_at": "2014-09-23T18:31:11Z",
        "updated_at": "2014-10-20T03:07:00Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17928864",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17928864"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17928864"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17928985",
        "pull_request_review_id": null,
        "id": 17928985,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTI4OTg1",
        "diff_hunk": "@@ -24,30 +98,21 @@ object Helpers extends Logging {\n   }\n \n   /** Build a psql command */\n-  def buildPsqlCmd(dbSettings: DbSettings, query: String) : String = {\n-    // Get Database-related settings\n-    val dbname = dbSettings.dbname\n-    val pguser = dbSettings.user\n-    val pgport = dbSettings.port\n-    val pghost = dbSettings.host\n-    // TODO do not use password for now\n-    val dbnameStr = dbname match {\n-      case null => \"\"\n-      case _ => s\" -d ${dbname}\"\n-    }\n-    val pguserStr = pguser match {\n-      case null => \"\"\n-      case _ => s\" -U ${pguser}\"\n-    }\n-    val pgportStr = pgport match {\n-      case null => \"\"\n-      case _ => s\" -p ${pgport}\"\n+  def buildSqlCmd(dbSettings: DbSettings, query: String) : String = {\n+\n+    // Branch by database driver type\n+    val dbtype = getDbType(dbSettings)\n+    \n+    val sqlQueryPrefix = dbtype match {\n+      case Psql => \"psql \" + getOptionString(dbSettings)\n+      case Mysql => \"mysql \" + getOptionString(dbSettings)\n     }\n-    val pghostStr = pghost match {\n-      case null => \"\"\n-      case _ => s\" -h ${pghost}\"\n+\n+    // Return the command below\n+    dbtype match {\n+      case Psql => sqlQueryPrefix + \" -c \" + \"\\\"\" + query + \"\\\"\"\n+      case Mysql => sqlQueryPrefix + \" --silent -e \" + \"\\\"\" + query + \"\\\"\"\n+      // TODO what happens if \" is in query?",
        "path": "src/main/scala/org/deepdive/helpers/Helpers.scala",
        "position": null,
        "original_position": 121,
        "commit_id": "1557aa91ca7954afa6afe49d6a62b8f4d36f6982",
        "original_commit_id": "603084d9a6d1f70ecbd3790292fdab8cb7e66422",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "query may contain quotes, e.g. select \"id\" from relation...\n",
        "created_at": "2014-09-23T18:32:53Z",
        "updated_at": "2014-10-20T03:07:00Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17928985",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17928985"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17928985"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17929133",
        "pull_request_review_id": null,
        "id": 17929133,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTI5MTMz",
        "diff_hunk": "@@ -36,10 +36,10 @@ deepdive {\n     poolMaxSize=512\n     poolConnectionTimeoutMillis=1000\n     # Default values: use environment variables\n-    user     : ${PGUSER}\n-    dbname   : ${DBNAME}\n-    host     : ${PGHOST}\n-    port     : ${PGPORT}\n+    # user     : ${PGUSER}\n+    # dbname   : ${DBNAME}\n+    # host     : ${PGHOST}\n+    # port     : ${PGPORT}",
        "path": "src/main/resources/application.conf",
        "position": 11,
        "original_position": 11,
        "commit_id": "1557aa91ca7954afa6afe49d6a62b8f4d36f6982",
        "original_commit_id": "603084d9a6d1f70ecbd3790292fdab8cb7e66422",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "These are default values and users can still set them in their own application.conf. My confusion is just that the \"PG\" convention is not good for Mysql.\n\nI will change them back for now.\n",
        "created_at": "2014-09-23T18:34:43Z",
        "updated_at": "2014-10-20T03:07:00Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17929133",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17929133"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17929133"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17930525",
        "pull_request_review_id": null,
        "id": 17930525,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTMwNTI1",
        "diff_hunk": "@@ -520,17 +559,15 @@ class ExtractorRunner(dataStore: JsonExtractionDataStore, dbSettings: DbSettings\n     executeScriptOrFail(file.getAbsolutePath(), failureReceiver)\n     // executeScriptOrFail(cmd, failureReceiver)\n     \n-    // executeCmd(s\"psql -d ${dbname} -c \" + \"\\\"\\\"\\\"\" + query + \"\\\"\\\"\\\"\")\n+    // executeCmd(s\"psql -d ${dbname} -c \" + \"\\\"\" + query + \"\\\"\")",
        "path": "src/main/scala/org/deepdive/extraction/ExtractorRunner.scala",
        "position": null,
        "original_position": 282,
        "commit_id": "1557aa91ca7954afa6afe49d6a62b8f4d36f6982",
        "original_commit_id": "603084d9a6d1f70ecbd3790292fdab8cb7e66422",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I think if query contains \"\" the bash script won't work. Let me try it today.\n",
        "created_at": "2014-09-23T18:54:05Z",
        "updated_at": "2014-10-20T03:07:00Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17930525",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17930525"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17930525"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17932869",
        "pull_request_review_id": null,
        "id": 17932869,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTMyODY5",
        "diff_hunk": "@@ -520,17 +559,15 @@ class ExtractorRunner(dataStore: JsonExtractionDataStore, dbSettings: DbSettings\n     executeScriptOrFail(file.getAbsolutePath(), failureReceiver)\n     // executeScriptOrFail(cmd, failureReceiver)\n     \n-    // executeCmd(s\"psql -d ${dbname} -c \" + \"\\\"\\\"\\\"\" + query + \"\\\"\\\"\\\"\")\n+    // executeCmd(s\"psql -d ${dbname} -c \" + \"\\\"\" + query + \"\\\"\")",
        "path": "src/main/scala/org/deepdive/extraction/ExtractorRunner.scala",
        "position": null,
        "original_position": 282,
        "commit_id": "1557aa91ca7954afa6afe49d6a62b8f4d36f6982",
        "original_commit_id": "603084d9a6d1f70ecbd3790292fdab8cb7e66422",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Bash escaping can go crazy if you're not principled.  Surrounding with single quote is the most reliable way for bash.  But then you need to escape whatever single quote already in the argument (`'` --> `'\\''`).\n\nTry this:\n\n```\nexecuteCmd(s\"\"\"psql -d ${dbname} -c '${query.replaceAll(\"'\", \"'\\\\''\")}'\"\"\")\n```\n\n(Highly likely to have errors as I'm not fluent in scala.)\n",
        "created_at": "2014-09-23T19:28:44Z",
        "updated_at": "2014-10-20T03:07:00Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17932869",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17932869"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17932869"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17933068",
        "pull_request_review_id": null,
        "id": 17933068,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTMzMDY4",
        "diff_hunk": "@@ -24,30 +98,21 @@ object Helpers extends Logging {\n   }\n \n   /** Build a psql command */\n-  def buildPsqlCmd(dbSettings: DbSettings, query: String) : String = {\n-    // Get Database-related settings\n-    val dbname = dbSettings.dbname\n-    val pguser = dbSettings.user\n-    val pgport = dbSettings.port\n-    val pghost = dbSettings.host\n-    // TODO do not use password for now\n-    val dbnameStr = dbname match {\n-      case null => \"\"\n-      case _ => s\" -d ${dbname}\"\n-    }\n-    val pguserStr = pguser match {\n-      case null => \"\"\n-      case _ => s\" -U ${pguser}\"\n-    }\n-    val pgportStr = pgport match {\n-      case null => \"\"\n-      case _ => s\" -p ${pgport}\"\n+  def buildSqlCmd(dbSettings: DbSettings, query: String) : String = {\n+\n+    // Branch by database driver type\n+    val dbtype = getDbType(dbSettings)\n+    \n+    val sqlQueryPrefix = dbtype match {\n+      case Psql => \"psql \" + getOptionString(dbSettings)\n+      case Mysql => \"mysql \" + getOptionString(dbSettings)\n     }\n-    val pghostStr = pghost match {\n-      case null => \"\"\n-      case _ => s\" -h ${pghost}\"\n+\n+    // Return the command below\n+    dbtype match {\n+      case Psql => sqlQueryPrefix + \" -c \" + \"\\\"\" + query + \"\\\"\"\n+      case Mysql => sqlQueryPrefix + \" --silent -e \" + \"\\\"\" + query + \"\\\"\"\n+      // TODO what happens if \" is in query?",
        "path": "src/main/scala/org/deepdive/helpers/Helpers.scala",
        "position": null,
        "original_position": 121,
        "commit_id": "1557aa91ca7954afa6afe49d6a62b8f4d36f6982",
        "original_commit_id": "603084d9a6d1f70ecbd3790292fdab8cb7e66422",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "See https://github.com/HazyResearch/deepdive/pull/143/files#r17932869\n",
        "created_at": "2014-09-23T19:30:46Z",
        "updated_at": "2014-10-20T03:07:00Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17933068",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17933068"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17933068"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17945464",
        "pull_request_review_id": null,
        "id": 17945464,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTQ1NDY0",
        "diff_hunk": "@@ -36,10 +36,10 @@ deepdive {\n     poolMaxSize=512\n     poolConnectionTimeoutMillis=1000\n     # Default values: use environment variables\n-    user     : ${PGUSER}\n-    dbname   : ${DBNAME}\n-    host     : ${PGHOST}\n-    port     : ${PGPORT}\n+    # user     : ${PGUSER}\n+    # dbname   : ${DBNAME}\n+    # host     : ${PGHOST}\n+    # port     : ${PGPORT}",
        "path": "src/main/resources/application.conf",
        "position": 11,
        "original_position": 11,
        "commit_id": "1557aa91ca7954afa6afe49d6a62b8f4d36f6982",
        "original_commit_id": "603084d9a6d1f70ecbd3790292fdab8cb7e66422",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "It turns out that if we keep the values, DeepDive will force users to set these environment variables, which should not happen.\n\nDevelopers can set db.default.{dbname, host, port, user} to make deepdive work. The system should not be directly dependent on these env vars.\n\n@zhangce Thoughts?\n",
        "created_at": "2014-09-23T23:10:56Z",
        "updated_at": "2014-10-20T03:07:00Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17945464",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17945464"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17945464"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17946242",
        "pull_request_review_id": null,
        "id": 17946242,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTQ2MjQy",
        "diff_hunk": "@@ -117,6 +108,10 @@ class ExtractorRunner(dataStore: JsonExtractionDataStore, dbSettings: DbSettings\n       task.extractor.style match {\n \n         case \"json_extractor\" =>\n+          if (dbtype != Psql) {\n+            failTask(s\"do not support ${task.extractor.style} on ${dbtype}.\", taskSender)\n+          }\n+          ",
        "path": "src/main/scala/org/deepdive/extraction/ExtractorRunner.scala",
        "position": 77,
        "original_position": 64,
        "commit_id": "1557aa91ca7954afa6afe49d6a62b8f4d36f6982",
        "original_commit_id": "603084d9a6d1f70ecbd3790292fdab8cb7e66422",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "This will terminate DeepDive.\n",
        "created_at": "2014-09-23T23:33:21Z",
        "updated_at": "2014-10-20T03:07:00Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17946242",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17946242"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17946242"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17954693",
        "pull_request_review_id": null,
        "id": 17954693,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTU0Njkz",
        "diff_hunk": "@@ -39,15 +39,15 @@\n   # If so, set is_correct to true or false\n   is_true = '\\N'\n   if spouses[p1_text_lower] == p2_text_lower:\n-    is_true = 'true'\n+    is_true = '1'\n   if spouses[p2_text_lower] == p1_text_lower:\n-    is_true = 'true'\n+    is_true = '1'\n   elif (p1_text == p2_text) or (p1_text in p2_text) or (p2_text in p1_text):\n-    is_true = 'false'\n+    is_true = '0'\n   elif (p1_text_lower, p2_text_lower) in non_spouses:\n-    is_true = 'false'\n+    is_true = '0'\n   elif (p2_text_lower, p1_text_lower) in non_spouses:\n-    is_true = 'false'\n+    is_true = '0'",
        "path": "examples/spouse_example/tsv_extractor/udf/ext_has_spouse.py",
        "position": 17,
        "original_position": 17,
        "commit_id": "1557aa91ca7954afa6afe49d6a62b8f4d36f6982",
        "original_commit_id": "9ad9491d0a8feceeeab208eef6b60b82b8453c94",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "While psql still supports \"true / false\" in TSV files, mysql requires 0/1 for successful type conversion to boolean.\n",
        "created_at": "2014-09-24T05:34:41Z",
        "updated_at": "2014-10-20T03:07:00Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17954693",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17954693"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17954693"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17954756",
        "pull_request_review_id": null,
        "id": 17954756,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTU0NzU2",
        "diff_hunk": "@@ -49,30 +39,38 @@ class DataLoader extends JdbcDataStore with Logging {\n       }\n \n       // hacky way to get schema from a query...\n-      executeQuery(s\"\"\"\n+      executeSqlQuery(s\"\"\"\n         DROP VIEW IF EXISTS _${filename}_view CASCADE;\n         DROP TABLE IF EXISTS _${filename}_tmp CASCADE;\n         CREATE VIEW _${filename}_view AS ${query};\n         CREATE TABLE _${filename}_tmp AS SELECT * FROM _${filename}_view LIMIT 0;\n         \"\"\")\n \n-      executeQuery(s\"\"\"\n+      executeSqlQuery(s\"\"\"\n         DROP EXTERNAL TABLE IF EXISTS _${filename} CASCADE;\n         CREATE WRITABLE EXTERNAL TABLE _${filename} (LIKE _${filename}_tmp)\n         LOCATION ('gpfdist://${hostname}:${port}/${filename}')\n         FORMAT 'TEXT';\n         \"\"\")\n \n-      executeQuery(s\"\"\"\n+      executeSqlQuery(s\"\"\"\n         DROP VIEW _${filename}_view CASCADE;\n         DROP TABLE _${filename}_tmp CASCADE;\"\"\")\n \n-      executeQuery(s\"\"\"\n+      executeSqlQuery(s\"\"\"",
        "path": "src/main/scala/org/deepdive/datastore/Dataloader.scala",
        "position": null,
        "original_position": 68,
        "commit_id": "1557aa91ca7954afa6afe49d6a62b8f4d36f6982",
        "original_commit_id": "9ad9491d0a8feceeeab208eef6b60b82b8453c94",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Code refactor: trait JdbcDatastore now has method `executeSqlQuery`. Not sure if this can be combined with other functions (some of them only allows update).\n",
        "created_at": "2014-09-24T05:38:43Z",
        "updated_at": "2014-10-20T03:07:00Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17954756",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17954756"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17954756"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17954776",
        "pull_request_review_id": null,
        "id": 17954776,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTU0Nzc2",
        "diff_hunk": "@@ -125,204 +124,32 @@ class ExtractorRunner(dataStore: JsonExtractionDataStore, dbSettings: DbSettings\n           Future { sendData(task, workers, taskSender) }\n           goto(Running) using Task(task, sender, workers)\n \n-        // Zifei: New code path for faster extraction\n-        //   Get rid of scala file operations\n-        //   COPY to a file, split files, and send to extractors\n+        // TSV extractor: Get rid of scala file operations\n+        // COPY to a file, split files, and send to extractors\n         case \"tsv_extractor\" =>\n-",
        "path": "src/main/scala/org/deepdive/extraction/ExtractorRunner.scala",
        "position": 91,
        "original_position": 91,
        "commit_id": "1557aa91ca7954afa6afe49d6a62b8f4d36f6982",
        "original_commit_id": "9ad9491d0a8feceeeab208eef6b60b82b8453c94",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Refactor ExtractorRunner to move tsv_extractor and plpy_extractor execution to separate functions, to prevent one method to be too long\n",
        "created_at": "2014-09-24T05:39:51Z",
        "updated_at": "2014-10-20T03:07:00Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17954776",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17954776"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17954776"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17954800",
        "pull_request_review_id": null,
        "id": 17954800,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTU0ODAw",
        "diff_hunk": "@@ -512,32 +355,233 @@ class ExtractorRunner(dataStore: JsonExtractionDataStore, dbSettings: DbSettings\n       case null => \"\"\n       case _ => \" > \" + pipeOutFilePath\n     }\n-    val cmd =  sqlQueryPrefix + \" -c \" + \"\\\"\\\"\\\"\" + query + \"\\\"\\\"\\\"\" + pipeOutStr\n+    // Use single-quote in bash for reliability. Escape all ' into '\\'' inside query.\n+    val cmd = dbtype match {\n+      case Psql => sqlQueryPrefix +\n+        s\"\"\" -c '${query.replaceAll(\"'\", \"'\\\\\\\\''\")}' ${pipeOutStr}\"\"\"\n+      case Mysql => sqlQueryPrefix +\n+        s\"\"\" --silent -e '${query.replaceAll(\"'\", \"'\\\\\\\\''\")}' ${pipeOutStr}\"\"\"\n+    } ",
        "path": "src/main/scala/org/deepdive/extraction/ExtractorRunner.scala",
        "position": null,
        "original_position": 409,
        "commit_id": "1557aa91ca7954afa6afe49d6a62b8f4d36f6982",
        "original_commit_id": "9ad9491d0a8feceeeab208eef6b60b82b8453c94",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "This takes Jaeho's suggestion to use single-quote (') to execute queries.\n\"\\\\\" is needed to actually make a \"\\\" in the final bash script.\n",
        "created_at": "2014-09-24T05:41:23Z",
        "updated_at": "2014-10-20T03:07:00Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17954800",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17954800"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17954800"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17954976",
        "pull_request_review_id": null,
        "id": 17954976,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3OTU0OTc2",
        "diff_hunk": "@@ -11,6 +11,12 @@ export GPHOST=${GPHOST:-localhost}\n export GPPORT=${GPPORT:-8082}\n export GPPATH=${GPPATH:-/tmp}\n \n+# Mysql specific\n+export MYUSER=\"root\"\n+export MYPASSWORD=\n+export MYHOST=127.0.0.1\n+export MYPORT=3306\n+",
        "path": "test.sh",
        "position": 9,
        "original_position": 9,
        "commit_id": "1557aa91ca7954afa6afe49d6a62b8f4d36f6982",
        "original_commit_id": "9ad9491d0a8feceeeab208eef6b60b82b8453c94",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "These env vars are not used now, just for future mysql tests.\n",
        "created_at": "2014-09-24T05:52:03Z",
        "updated_at": "2014-10-20T03:07:00Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17954976",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/17954976"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r17954976"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18125742",
        "pull_request_review_id": null,
        "id": 18125742,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4MTI1NzQy",
        "diff_hunk": "@@ -13,6 +13,7 @@ import org.deepdive.profiling._\n import org.deepdive.calibration._\n import scala.concurrent.duration._\n import scala.concurrent.{Future, Await}\n+import scala.language.postfixOps",
        "path": "src/main/scala/org/deepdive/DeepDive.scala",
        "position": 4,
        "original_position": 4,
        "commit_id": "1557aa91ca7954afa6afe49d6a62b8f4d36f6982",
        "original_commit_id": "4f9c8aec2514f9352d7dd320838ebe54712b279a",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "These new imports are used to fix compile warnings.\n",
        "created_at": "2014-09-27T20:55:13Z",
        "updated_at": "2014-10-20T03:07:00Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r18125742",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18125742"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/143#discussion_r18125742"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/143"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18989211",
        "pull_request_review_id": null,
        "id": 18989211,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4OTg5MjEx",
        "diff_hunk": "@@ -221,8 +221,8 @@ trait SQLInferenceDataStoreSpec extends FunSpec with BeforeAndAfter { this: SQLI\n         // Ground the graph with custom holdout\n         val customHoldoutQuery = \"\"\"\n           INSERT INTO dd_graph_variables_holdout(variable_id)\n-          SELECT id FROM r1 WHERE weight <= 10;\"\"\"\n-        inferenceDataStore.groundFactorGraph(schema, Seq(factorDesc), holdoutFraction, Option(customHoldoutQuery), false, \"\", dbSettings, false)\n+          SELECT id FROM r1 WHERE id <= 10;\"\"\"\n+        inferenceDataStore.groundFactorGraph(schema, Seq(factorDesc), CalibrationSettings(holdoutFraction, Option(customHoldoutQuery), None), false, \"\", dbSettings, false)",
        "path": "src/test/scala/unit/inference/SQLInferenceDataStoreSpec.scala",
        "position": null,
        "original_position": 34,
        "commit_id": "8f5da3a258c9c20fb5b21bde1995697c280b1b0e",
        "original_commit_id": "8a6ba47eb385d3f7f96097352695bc33d60d676d",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "@feiranwang @gengyl08 : Is this change correct? from \"weight\" to \"id\" ???\n",
        "created_at": "2014-10-16T21:46:25Z",
        "updated_at": "2014-10-24T01:02:16Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/158#discussion_r18989211",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/158",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18989211"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/158#discussion_r18989211"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/158"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18989323",
        "pull_request_review_id": null,
        "id": 18989323,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4OTg5MzIz",
        "diff_hunk": "@@ -320,7 +389,7 @@ trait SQLInferenceDataStoreSpec extends FunSpec with BeforeAndAfter { this: SQLI\n         val holdoutFraction = 0.0\n \n         // Ground the graph\n-        inferenceDataStore.groundFactorGraph(schema, Seq(factorDesc), holdoutFraction, None, false, \"\", dbSettings, false)\n+        inferenceDataStore.groundFactorGraph(schema, Seq(factorDesc), CalibrationSettings(0.0, None, None), false, \"\", dbSettings, false)",
        "path": "src/test/scala/unit/inference/SQLInferenceDataStoreSpec.scala",
        "position": null,
        "original_position": 137,
        "commit_id": "8f5da3a258c9c20fb5b21bde1995697c280b1b0e",
        "original_commit_id": "8a6ba47eb385d3f7f96097352695bc33d60d676d",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "@feiranwang @gengyl08 Why was this changed from holdoutFraction to 0.0? I understand that holdoutFraction is 0.0, but try to avoid magic numbers. Please use holdoutFraction.\n",
        "created_at": "2014-10-16T21:48:17Z",
        "updated_at": "2014-10-24T01:02:16Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/158#discussion_r18989323",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/158",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18989323"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/158#discussion_r18989323"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/158"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18989338",
        "pull_request_review_id": null,
        "id": 18989338,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4OTg5MzM4",
        "diff_hunk": "@@ -356,7 +425,7 @@ trait SQLInferenceDataStoreSpec extends FunSpec with BeforeAndAfter { this: SQLI\n         val holdoutFraction = 0.0\n \n         // Ground the graph\n-        inferenceDataStore.groundFactorGraph(schema, Seq(factorDesc), holdoutFraction, None, false, \"\", dbSettings, false)\n+        inferenceDataStore.groundFactorGraph(schema, Seq(factorDesc), CalibrationSettings(0.0, None, None), false, \"\", dbSettings, false)",
        "path": "src/test/scala/unit/inference/SQLInferenceDataStoreSpec.scala",
        "position": null,
        "original_position": 146,
        "commit_id": "8f5da3a258c9c20fb5b21bde1995697c280b1b0e",
        "original_commit_id": "8a6ba47eb385d3f7f96097352695bc33d60d676d",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "@feiranwang @gengyl08 : Same as above.\n",
        "created_at": "2014-10-16T21:48:33Z",
        "updated_at": "2014-10-24T01:02:16Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/158#discussion_r18989338",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/158",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18989338"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/158#discussion_r18989338"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/158"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18989443",
        "pull_request_review_id": null,
        "id": 18989443,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4OTg5NDQz",
        "diff_hunk": "@@ -221,8 +221,8 @@ trait SQLInferenceDataStoreSpec extends FunSpec with BeforeAndAfter { this: SQLI\n         // Ground the graph with custom holdout\n         val customHoldoutQuery = \"\"\"\n           INSERT INTO dd_graph_variables_holdout(variable_id)\n-          SELECT id FROM r1 WHERE weight <= 10;\"\"\"\n-        inferenceDataStore.groundFactorGraph(schema, Seq(factorDesc), holdoutFraction, Option(customHoldoutQuery), false, \"\", dbSettings, false)\n+          SELECT id FROM r1 WHERE id <= 10;\"\"\"\n+        inferenceDataStore.groundFactorGraph(schema, Seq(factorDesc), CalibrationSettings(holdoutFraction, Option(customHoldoutQuery), None), false, \"\", dbSettings, false)",
        "path": "src/test/scala/unit/inference/SQLInferenceDataStoreSpec.scala",
        "position": null,
        "original_position": 34,
        "commit_id": "8f5da3a258c9c20fb5b21bde1995697c280b1b0e",
        "original_commit_id": "8a6ba47eb385d3f7f96097352695bc33d60d676d",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "yes, this change is due to the merge (already exists in master). Since id is unreliable (will be assigned on the fly).\n",
        "created_at": "2014-10-16T21:50:44Z",
        "updated_at": "2014-10-24T01:02:16Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/158#discussion_r18989443",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/158",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18989443"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/158#discussion_r18989443"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/158"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18989736",
        "pull_request_review_id": null,
        "id": 18989736,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4OTg5NzM2",
        "diff_hunk": "@@ -221,8 +221,8 @@ trait SQLInferenceDataStoreSpec extends FunSpec with BeforeAndAfter { this: SQLI\n         // Ground the graph with custom holdout\n         val customHoldoutQuery = \"\"\"\n           INSERT INTO dd_graph_variables_holdout(variable_id)\n-          SELECT id FROM r1 WHERE weight <= 10;\"\"\"\n-        inferenceDataStore.groundFactorGraph(schema, Seq(factorDesc), holdoutFraction, Option(customHoldoutQuery), false, \"\", dbSettings, false)\n+          SELECT id FROM r1 WHERE id <= 10;\"\"\"\n+        inferenceDataStore.groundFactorGraph(schema, Seq(factorDesc), CalibrationSettings(holdoutFraction, Option(customHoldoutQuery), None), false, \"\", dbSettings, false)",
        "path": "src/test/scala/unit/inference/SQLInferenceDataStoreSpec.scala",
        "position": null,
        "original_position": 34,
        "commit_id": "8f5da3a258c9c20fb5b21bde1995697c280b1b0e",
        "original_commit_id": "8a6ba47eb385d3f7f96097352695bc33d60d676d",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "It exists in master but not in develop?How did this happen? \n\nWhat do you mean by \"unreliable\"? Do we want \"id\" because it's random or we do _not_ want it because it's random?\n",
        "created_at": "2014-10-16T21:56:20Z",
        "updated_at": "2014-10-24T01:02:16Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/158#discussion_r18989736",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/158",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18989736"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/158#discussion_r18989736"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/158"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18990110",
        "pull_request_review_id": null,
        "id": 18990110,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4OTkwMTEw",
        "diff_hunk": "@@ -221,8 +221,8 @@ trait SQLInferenceDataStoreSpec extends FunSpec with BeforeAndAfter { this: SQLI\n         // Ground the graph with custom holdout\n         val customHoldoutQuery = \"\"\"\n           INSERT INTO dd_graph_variables_holdout(variable_id)\n-          SELECT id FROM r1 WHERE weight <= 10;\"\"\"\n-        inferenceDataStore.groundFactorGraph(schema, Seq(factorDesc), holdoutFraction, Option(customHoldoutQuery), false, \"\", dbSettings, false)\n+          SELECT id FROM r1 WHERE id <= 10;\"\"\"\n+        inferenceDataStore.groundFactorGraph(schema, Seq(factorDesc), CalibrationSettings(holdoutFraction, Option(customHoldoutQuery), None), false, \"\", dbSettings, false)",
        "path": "src/test/scala/unit/inference/SQLInferenceDataStoreSpec.scala",
        "position": null,
        "original_position": 34,
        "commit_id": "8f5da3a258c9c20fb5b21bde1995697c280b1b0e",
        "original_commit_id": "8a6ba47eb385d3f7f96097352695bc33d60d676d",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "It's also in develop. The problem is Yilong's development is based on an earlier version of deepdive. During the period, we made several changes (fixing unit tests, etc), which are not included in Yilong's branch. This one is the result of merginf from develop into Yilong's branch\n",
        "created_at": "2014-10-16T22:04:21Z",
        "updated_at": "2014-10-24T01:02:16Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/158#discussion_r18990110",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/158",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18990110"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/158#discussion_r18990110"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/158"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18990154",
        "pull_request_review_id": null,
        "id": 18990154,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4OTkwMTU0",
        "diff_hunk": "@@ -221,8 +221,8 @@ trait SQLInferenceDataStoreSpec extends FunSpec with BeforeAndAfter { this: SQLI\n         // Ground the graph with custom holdout\n         val customHoldoutQuery = \"\"\"\n           INSERT INTO dd_graph_variables_holdout(variable_id)\n-          SELECT id FROM r1 WHERE weight <= 10;\"\"\"\n-        inferenceDataStore.groundFactorGraph(schema, Seq(factorDesc), holdoutFraction, Option(customHoldoutQuery), false, \"\", dbSettings, false)\n+          SELECT id FROM r1 WHERE id <= 10;\"\"\"\n+        inferenceDataStore.groundFactorGraph(schema, Seq(factorDesc), CalibrationSettings(holdoutFraction, Option(customHoldoutQuery), None), false, \"\", dbSettings, false)",
        "path": "src/test/scala/unit/inference/SQLInferenceDataStoreSpec.scala",
        "position": null,
        "original_position": 34,
        "commit_id": "8f5da3a258c9c20fb5b21bde1995697c280b1b0e",
        "original_commit_id": "8a6ba47eb385d3f7f96097352695bc33d60d676d",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "we do not want to use id because it's random. We want the number to be deterministic such that we can test on.\n",
        "created_at": "2014-10-16T22:05:20Z",
        "updated_at": "2014-10-24T01:02:16Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/158#discussion_r18990154",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/158",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18990154"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/158#discussion_r18990154"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/158"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18990344",
        "pull_request_review_id": null,
        "id": 18990344,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4OTkwMzQ0",
        "diff_hunk": "@@ -221,8 +221,8 @@ trait SQLInferenceDataStoreSpec extends FunSpec with BeforeAndAfter { this: SQLI\n         // Ground the graph with custom holdout\n         val customHoldoutQuery = \"\"\"\n           INSERT INTO dd_graph_variables_holdout(variable_id)\n-          SELECT id FROM r1 WHERE weight <= 10;\"\"\"\n-        inferenceDataStore.groundFactorGraph(schema, Seq(factorDesc), holdoutFraction, Option(customHoldoutQuery), false, \"\", dbSettings, false)\n+          SELECT id FROM r1 WHERE id <= 10;\"\"\"\n+        inferenceDataStore.groundFactorGraph(schema, Seq(factorDesc), CalibrationSettings(holdoutFraction, Option(customHoldoutQuery), None), false, \"\", dbSettings, false)",
        "path": "src/test/scala/unit/inference/SQLInferenceDataStoreSpec.scala",
        "position": null,
        "original_position": 34,
        "commit_id": "8f5da3a258c9c20fb5b21bde1995697c280b1b0e",
        "original_commit_id": "8a6ba47eb385d3f7f96097352695bc33d60d676d",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "From what I can see, develop has weight (which is correct), while\nyilong-tuffy_port has id. yilong-tuffy_port must be fixed to have weight.\n\nOn Thu, Oct 16, 2014 at 3:05 PM, Feiran Wang notifications@github.com\nwrote:\n\n> In src/test/scala/unit/inference/SQLInferenceDataStoreSpec.scala:\n> \n> > @@ -221,8 +221,8 @@ trait SQLInferenceDataStoreSpec extends FunSpec with BeforeAndAfter { this: SQLI\n> >          // Ground the graph with custom holdout\n> >          val customHoldoutQuery = \"\"\"\n> >            INSERT INTO dd_graph_variables_holdout(variable_id)\n> > -          SELECT id FROM r1 WHERE weight <= 10;\"\"\"\n> > -        inferenceDataStore.groundFactorGraph(schema, Seq(factorDesc), holdoutFraction, Option(customHoldoutQuery), false, \"\", dbSettings, false)\n> > -          SELECT id FROM r1 WHERE id <= 10;\"\"\"\n> > -        inferenceDataStore.groundFactorGraph(schema, Seq(factorDesc), CalibrationSettings(holdoutFraction, Option(customHoldoutQuery), None), false, \"\", dbSettings, false)\n> \n> we do not want to use id because it's random. We want the number to be\n> deterministic such that we can test on.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/HazyResearch/deepdive/pull/158/files#r18990154.\n\n## \n\nMatteo Riondato, Ph.D.\nPost-doc Researcher\nDepartment of Computer Science\nStanford University\n",
        "created_at": "2014-10-16T22:09:56Z",
        "updated_at": "2014-10-24T01:02:16Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/158#discussion_r18990344",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/158",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18990344"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/158#discussion_r18990344"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/158"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18990585",
        "pull_request_review_id": null,
        "id": 18990585,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4OTkwNTg1",
        "diff_hunk": "@@ -221,8 +221,8 @@ trait SQLInferenceDataStoreSpec extends FunSpec with BeforeAndAfter { this: SQLI\n         // Ground the graph with custom holdout\n         val customHoldoutQuery = \"\"\"\n           INSERT INTO dd_graph_variables_holdout(variable_id)\n-          SELECT id FROM r1 WHERE weight <= 10;\"\"\"\n-        inferenceDataStore.groundFactorGraph(schema, Seq(factorDesc), holdoutFraction, Option(customHoldoutQuery), false, \"\", dbSettings, false)\n+          SELECT id FROM r1 WHERE id <= 10;\"\"\"\n+        inferenceDataStore.groundFactorGraph(schema, Seq(factorDesc), CalibrationSettings(holdoutFraction, Option(customHoldoutQuery), None), false, \"\", dbSettings, false)",
        "path": "src/test/scala/unit/inference/SQLInferenceDataStoreSpec.scala",
        "position": null,
        "original_position": 34,
        "commit_id": "8f5da3a258c9c20fb5b21bde1995697c280b1b0e",
        "original_commit_id": "8a6ba47eb385d3f7f96097352695bc33d60d676d",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "yes weight is the correct one. I misunderstand the direction... should be from \nid->weight instead of the reverse.\n",
        "created_at": "2014-10-16T22:14:22Z",
        "updated_at": "2014-10-24T01:02:16Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/158#discussion_r18990585",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/158",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/18990585"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/158#discussion_r18990585"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/158"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20055175",
        "pull_request_review_id": null,
        "id": 20055175,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMDU1MTc1",
        "diff_hunk": "@@ -0,0 +1,11 @@\n+source:      .\n+safe:        true\n+baseurl: http://www.stanford.edu/~zifei/deepdive\n+# TODO let's consider using redcarpet, as it seems to be the most similar to GitHub-flavored Markdown\n+markdown:    redcarpet\n+exclude:\n+  - deploy.sh\n+  - host.sh\n+  - params.json\n+  - \"*.graffle\"\n+  - Makefile",
        "path": "doc/_config-zifei.yml",
        "position": null,
        "original_position": 11,
        "commit_id": "8913cf14f17ce1e55c6c5abc612ca6bdeee0731a",
        "original_commit_id": "ffebf55e54a2718f46fadcdde6cf57a4a82fc361",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Do we need this file?\n",
        "created_at": "2014-11-08T23:08:37Z",
        "updated_at": "2014-11-09T00:30:46Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/161#discussion_r20055175",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/161",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20055175"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/161#discussion_r20055175"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/161"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20055178",
        "pull_request_review_id": null,
        "id": 20055178,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMDU1MTc4",
        "diff_hunk": "@@ -0,0 +1,11 @@\n+#! /usr/bin/env bash\n+\n+HOST=zifei@corn\n+WWW_DIR=/afs/.ir.stanford.edu/users/z/i/zifei/WWW/deepdive\n+HTML_DIR=\"$(dirname $0)/_site\";\n+\n+echo \"Building site...\"\n+jekyll build --config _config-zifei.yml\n+\n+echo \"Deplying site...\"\n+scp -r $(dirname $0)/_site/* $HOST:$WWW_DIR/",
        "path": "doc/_deploy_zifei.sh",
        "position": null,
        "original_position": 11,
        "commit_id": "8913cf14f17ce1e55c6c5abc612ca6bdeee0731a",
        "original_commit_id": "ffebf55e54a2718f46fadcdde6cf57a4a82fc361",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Do we need this file? (2)\n",
        "created_at": "2014-11-08T23:08:48Z",
        "updated_at": "2014-11-09T00:30:46Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/161#discussion_r20055178",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/161",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20055178"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/161#discussion_r20055178"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/161"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20055194",
        "pull_request_review_id": null,
        "id": 20055194,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMDU1MTk0",
        "diff_hunk": "@@ -198,8 +167,24 @@ class DataLoader extends JdbcDataStore with Logging {\n         case Psql => writebackPrefix + s\"'psql \" + Helpers.getOptionString(dbSettings) + \n           \"-c \\\"COPY \" + s\"${tablename} FROM STDIN;\" + \n           \" \\\" < $0'\"\n-        case Mysql => writebackPrefix +\n-          s\"'mysqlimport \" + Helpers.getOptionString(dbSettings) + \" $0'\"\n+        case Mysql => \n+          writebackPrefix + s\"'mysqlimport --local \" + Helpers.getOptionString(dbSettings) +\n+          \" $0'\"\n+          // // for MySQL (cluster), it is faster to load data with many threads\n+          // // (use as much as number of extractor) than single-threaded load.\n+          // s\"find ${filepath} -print0 | xargs -0 -P ${parallelism} -L 1 bash -c \" +\n+          // s\"'mysql \" + Helpers.getOptionString(dbSettings) + \n+          //   \"-e \\\"LOAD DATA \" + \n+          //   // branch if user want to load data from client or server\n+          //   s\"${System.getenv(\"MYSQL_LOCAL_INFILE\") match {\n+          //     case \"1\" => \"LOCAL\"\n+          //     case _ => \"\"\n+          //   }}\" + \" INFILE \" + \n+          //   \"\"\" '\"'\"'$0'\"'\"' \"\"\" + s\" INTO TABLE ${tablename};\" +\n+          //   \"\"\" \"' \"\"\" \n+          // // mysqlimport requires input file to have basename that is same as \n+          // // tablename. Do not use it now.\n+          // // s\"'mysqlimport --local \" + Helpers.getOptionString(dbSettings) + \" $0'\"",
        "path": "src/main/scala/org/deepdive/datastore/Dataloader.scala",
        "position": null,
        "original_position": 97,
        "commit_id": "8913cf14f17ce1e55c6c5abc612ca6bdeee0731a",
        "original_commit_id": "ffebf55e54a2718f46fadcdde6cf57a4a82fc361",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Why is all the above commented? If we do not need it, just remove it.\n",
        "created_at": "2014-11-08T23:11:40Z",
        "updated_at": "2014-11-09T00:30:46Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/161#discussion_r20055194",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/161",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20055194"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/161#discussion_r20055194"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/161"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20055195",
        "pull_request_review_id": null,
        "id": 20055195,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMDU1MTk1",
        "diff_hunk": "@@ -417,6 +422,8 @@ class ExtractorRunner(dataStore: JsonExtractionDataStore, dbSettings: DbSettings\n \n     // val maxParallel = \"0\"  // As many as possible, which is dangerous\n     val maxParallel = task.extractor.parallelism\n+    \n+//    val loader = task.extractor.loader",
        "path": "src/main/scala/org/deepdive/extraction/ExtractorRunner.scala",
        "position": null,
        "original_position": 33,
        "commit_id": "8913cf14f17ce1e55c6c5abc612ca6bdeee0731a",
        "original_commit_id": "ffebf55e54a2718f46fadcdde6cf57a4a82fc361",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "If not needed just remove.\n",
        "created_at": "2014-11-08T23:12:02Z",
        "updated_at": "2014-11-09T00:30:46Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/161#discussion_r20055195",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/161",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20055195"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/161#discussion_r20055195"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/161"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20055197",
        "pull_request_review_id": null,
        "id": 20055197,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMDU1MTk3",
        "diff_hunk": "@@ -31,17 +31,18 @@ trait MysqlInferenceDataStoreComponent extends SQLInferenceDataStoreComponent {\n     val BatchSize = Some(250000)\n     \n     /**\n-     * Internal utility to copy a file to a table. uses LOAD DATA LOCAL INFILE \n+     * Internal utility to copy a file to a table. uses LOAD DATA INFILE \n      * to retrieve the file in client-side rather than server-side. \n      */\n     private def copyFileToTable(filePath: String, tableName: String) : Unit = {\n \n       val srcFile = new File(filePath)\n \n+      // TODO test against different locations of client and server",
        "path": "src/main/scala/org/deepdive/inference/datastore/MysqlInferenceDataStore.scala",
        "position": null,
        "original_position": 12,
        "commit_id": "8913cf14f17ce1e55c6c5abc612ca6bdeee0731a",
        "original_commit_id": "ffebf55e54a2718f46fadcdde6cf57a4a82fc361",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "What does this comment mean?\n",
        "created_at": "2014-11-08T23:12:51Z",
        "updated_at": "2014-11-09T00:30:46Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/161#discussion_r20055197",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/161",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20055197"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/161#discussion_r20055197"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/161"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20055209",
        "pull_request_review_id": null,
        "id": 20055209,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMDU1MjA5",
        "diff_hunk": "@@ -187,11 +188,15 @@ trait MysqlInferenceDataStoreComponent extends SQLInferenceDataStoreComponent {\n     /**\n      * Dumb: mysql cannot drop index \"if exists\"...\n      * We use 4 statements to implement that.\n+     * \n+     * Note: @exist will be NULL if the table do not exist, and \"if\" still branches into second.\n+     * Note: dbname must be providen to locate the index correctly",
        "path": "src/main/scala/org/deepdive/inference/datastore/MysqlInferenceDataStore.scala",
        "position": null,
        "original_position": 27,
        "commit_id": "8913cf14f17ce1e55c6c5abc612ca6bdeee0731a",
        "original_commit_id": "ffebf55e54a2718f46fadcdde6cf57a4a82fc361",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "typo: provided\n",
        "created_at": "2014-11-08T23:14:50Z",
        "updated_at": "2014-11-09T00:30:46Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/161#discussion_r20055209",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/161",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20055209"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/161#discussion_r20055209"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/161"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20055213",
        "pull_request_review_id": null,
        "id": 20055213,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMDU1MjEz",
        "diff_hunk": "@@ -961,8 +963,6 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n \n     relationsColumns.foreach { case(relationName, columnName) =>\n       execute(createInferenceViewSQL(relationName, columnName))\n-    //   // TODO\n-    //   execute(createVariableWeightsViewSQL(relationName, columnName))",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 25,
        "original_position": 25,
        "commit_id": "8913cf14f17ce1e55c6c5abc612ca6bdeee0731a",
        "original_commit_id": "ffebf55e54a2718f46fadcdde6cf57a4a82fc361",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "What happened to this?\n",
        "created_at": "2014-11-08T23:15:33Z",
        "updated_at": "2014-11-09T00:30:46Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/161#discussion_r20055213",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/161",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20055213"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/161#discussion_r20055213"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/161"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20055238",
        "pull_request_review_id": null,
        "id": 20055238,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMDU1MjM4",
        "diff_hunk": "@@ -1,55 +1,14 @@\n-#! /usr/bin/env bash\n+set -e\n \n-# Set username and password\n-export PGUSER=${PGUSER:-`whoami`}\n-export PGPASSWORD=${PGPASSWORD:-}\n-export PGPORT=${PGPORT:-5432}\n-export PGHOST=${PGHOST:-localhost}\n-export DBNAME=deepdive_test\n-export PGDATABASE=$DBNAME  # for testing to work with null settings\n-export GPHOST=${GPHOST:-localhost}\n-export GPPORT=${GPPORT:-8082}\n-export GPPATH=${GPPATH:-/tmp}\n+# if command psql exist, then test psql\n+if hash psql 2>/dev/null; then",
        "path": "test.sh",
        "position": 15,
        "original_position": 15,
        "commit_id": "8913cf14f17ce1e55c6c5abc612ca6bdeee0731a",
        "original_commit_id": "ffebf55e54a2718f46fadcdde6cf57a4a82fc361",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Never seen \"hash command\" before. What about using \"which\" instead? \n",
        "created_at": "2014-11-08T23:18:57Z",
        "updated_at": "2014-11-09T00:30:46Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/161#discussion_r20055238",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/161",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20055238"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/161#discussion_r20055238"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/161"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20055561",
        "pull_request_review_id": null,
        "id": 20055561,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMDU1NTYx",
        "diff_hunk": "@@ -961,8 +963,6 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n \n     relationsColumns.foreach { case(relationName, columnName) =>\n       execute(createInferenceViewSQL(relationName, columnName))\n-    //   // TODO\n-    //   execute(createVariableWeightsViewSQL(relationName, columnName))",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 25,
        "original_position": 25,
        "commit_id": "8913cf14f17ce1e55c6c5abc612ca6bdeee0731a",
        "original_commit_id": "ffebf55e54a2718f46fadcdde6cf57a4a82fc361",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "It was originally commented out, introduced in cc18ddc2d0421fce5c1152f6762a8fec9c0782ca on March. I just cleaned this up.\n",
        "created_at": "2014-11-09T00:17:36Z",
        "updated_at": "2014-11-09T00:30:46Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/161#discussion_r20055561",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/161",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20055561"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/161#discussion_r20055561"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/161"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20055573",
        "pull_request_review_id": null,
        "id": 20055573,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMDU1NTcz",
        "diff_hunk": "@@ -1,55 +1,14 @@\n-#! /usr/bin/env bash\n+set -e\n \n-# Set username and password\n-export PGUSER=${PGUSER:-`whoami`}\n-export PGPASSWORD=${PGPASSWORD:-}\n-export PGPORT=${PGPORT:-5432}\n-export PGHOST=${PGHOST:-localhost}\n-export DBNAME=deepdive_test\n-export PGDATABASE=$DBNAME  # for testing to work with null settings\n-export GPHOST=${GPHOST:-localhost}\n-export GPPORT=${GPPORT:-8082}\n-export GPPATH=${GPPATH:-/tmp}\n+# if command psql exist, then test psql\n+if hash psql 2>/dev/null; then",
        "path": "test.sh",
        "position": 15,
        "original_position": 15,
        "commit_id": "8913cf14f17ce1e55c6c5abc612ca6bdeee0731a",
        "original_commit_id": "ffebf55e54a2718f46fadcdde6cf57a4a82fc361",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I saw the usage from here: http://stackoverflow.com/questions/592620/how-to-check-if-a-program-exists-from-a-bash-script\n\nThey said to avoid `which`, so I choosed `hash` which seems working fine.\n",
        "created_at": "2014-11-09T00:18:58Z",
        "updated_at": "2014-11-09T00:30:46Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/161#discussion_r20055573",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/161",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20055573"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/161#discussion_r20055573"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/161"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20055817",
        "pull_request_review_id": null,
        "id": 20055817,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMDU1ODE3",
        "diff_hunk": "@@ -1,55 +1,14 @@\n-#! /usr/bin/env bash\n+set -e\n \n-# Set username and password\n-export PGUSER=${PGUSER:-`whoami`}\n-export PGPASSWORD=${PGPASSWORD:-}\n-export PGPORT=${PGPORT:-5432}\n-export PGHOST=${PGHOST:-localhost}\n-export DBNAME=deepdive_test\n-export PGDATABASE=$DBNAME  # for testing to work with null settings\n-export GPHOST=${GPHOST:-localhost}\n-export GPPORT=${GPPORT:-8082}\n-export GPPATH=${GPPATH:-/tmp}\n+# if command psql exist, then test psql\n+if hash psql 2>/dev/null; then",
        "path": "test.sh",
        "position": 15,
        "original_position": 15,
        "commit_id": "8913cf14f17ce1e55c6c5abc612ca6bdeee0731a",
        "original_commit_id": "ffebf55e54a2718f46fadcdde6cf57a4a82fc361",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Nice. I didn't know about it. Cool.\n",
        "created_at": "2014-11-09T01:02:43Z",
        "updated_at": "2014-11-09T01:02:43Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/161#discussion_r20055817",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/161",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20055817"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/161#discussion_r20055817"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/161"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20055824",
        "pull_request_review_id": null,
        "id": 20055824,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMDU1ODI0",
        "diff_hunk": "@@ -961,8 +963,6 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n \n     relationsColumns.foreach { case(relationName, columnName) =>\n       execute(createInferenceViewSQL(relationName, columnName))\n-    //   // TODO\n-    //   execute(createVariableWeightsViewSQL(relationName, columnName))",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 25,
        "original_position": 25,
        "commit_id": "8913cf14f17ce1e55c6c5abc612ca6bdeee0731a",
        "original_commit_id": "ffebf55e54a2718f46fadcdde6cf57a4a82fc361",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Good. That's the reason why I don't like commented out lines, as you can't really tell why they are commented.\n",
        "created_at": "2014-11-09T01:03:40Z",
        "updated_at": "2014-11-09T01:03:40Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/161#discussion_r20055824",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/161",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20055824"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/161#discussion_r20055824"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/161"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20055828",
        "pull_request_review_id": null,
        "id": 20055828,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMDU1ODI4",
        "diff_hunk": "@@ -31,17 +31,18 @@ trait MysqlInferenceDataStoreComponent extends SQLInferenceDataStoreComponent {\n     val BatchSize = Some(250000)\n     \n     /**\n-     * Internal utility to copy a file to a table. uses LOAD DATA LOCAL INFILE \n+     * Internal utility to copy a file to a table. uses LOAD DATA INFILE \n      * to retrieve the file in client-side rather than server-side. \n      */\n     private def copyFileToTable(filePath: String, tableName: String) : Unit = {\n \n       val srcFile = new File(filePath)\n \n+      // TODO test against different locations of client and server",
        "path": "src/main/scala/org/deepdive/inference/datastore/MysqlInferenceDataStore.scala",
        "position": null,
        "original_position": 12,
        "commit_id": "8913cf14f17ce1e55c6c5abc612ca6bdeee0731a",
        "original_commit_id": "ffebf55e54a2718f46fadcdde6cf57a4a82fc361",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Like psql `COPY table FROM 'path_to_file'`, `LOAD DATA INFILE` may not work when the client that runs `mysql` is on a different machine with the server. I just changed it to `LOAD DATA LOCAL INFILE` here and it will support reading data from client.\n",
        "created_at": "2014-11-09T01:04:44Z",
        "updated_at": "2014-11-09T01:04:44Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/161#discussion_r20055828",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/161",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20055828"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/161#discussion_r20055828"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/161"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20124981",
        "pull_request_review_id": null,
        "id": 20124981,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMTI0OTgx",
        "diff_hunk": "@@ -120,7 +120,12 @@ object DeepDive extends Logging {\n       \n \n     // Create a default pipeline that executes all tasks\n-    val defaultPipeline = Pipeline(\"_default\", allTasks.map(_.id).toSet)\n+    val defaultPipeline = Pipeline(\"_default\", \n+      activeFactors.size match {\n+        // If no factors are active, do not run inference tasks\n+        case 0 => allTasks.map(_.id).toSet -- Set(\"inference_grounding\", \"inference\", \"calibration\")\n+        case _ => allTasks.map(_.id).toSet\n+      })",
        "path": "src/main/scala/org/deepdive/DeepDive.scala",
        "position": 10,
        "original_position": 10,
        "commit_id": "41361477251ed49af4004127ddd3020c502663c8",
        "original_commit_id": "41361477251ed49af4004127ddd3020c502663c8",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "@zifeishan This change does not seem related. What is it?\n",
        "created_at": "2014-11-11T00:33:34Z",
        "updated_at": "2014-11-11T00:33:34Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/163#discussion_r20124981",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/163",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20124981"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/163#discussion_r20124981"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/163"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20125030",
        "pull_request_review_id": null,
        "id": 20125030,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMTI1MDMw",
        "diff_hunk": "@@ -120,7 +120,12 @@ object DeepDive extends Logging {\n       \n \n     // Create a default pipeline that executes all tasks\n-    val defaultPipeline = Pipeline(\"_default\", allTasks.map(_.id).toSet)\n+    val defaultPipeline = Pipeline(\"_default\", \n+      activeFactors.size match {\n+        // If no factors are active, do not run inference tasks\n+        case 0 => allTasks.map(_.id).toSet -- Set(\"inference_grounding\", \"inference\", \"calibration\")\n+        case _ => allTasks.map(_.id).toSet\n+      })",
        "path": "src/main/scala/org/deepdive/DeepDive.scala",
        "position": 10,
        "original_position": 10,
        "commit_id": "41361477251ed49af4004127ddd3020c502663c8",
        "original_commit_id": "41361477251ed49af4004127ddd3020c502663c8",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Sorry it is described here:\n\nInclude another bug fix: in default pipeline, when no factors are active, do not run the sampler.\n\nPreviously when we specify no factors in the pipeline, the sampler won't run. But when we specify no pipeline, the sampler will run even there are no factors. Now we fix it.\n",
        "created_at": "2014-11-11T00:34:53Z",
        "updated_at": "2014-11-11T00:34:53Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/163#discussion_r20125030",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/163",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20125030"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/163#discussion_r20125030"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/163"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20272797",
        "pull_request_review_id": null,
        "id": 20272797,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMjcyNzk3",
        "diff_hunk": "@@ -461,6 +461,8 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n     if (parallelGrounding) {\n       val cleanFile = File.createTempFile(s\"clean\", \".sh\")\n       val writer = new PrintWriter(cleanFile)\n+      // cleaning up remaining tmp folder for tobinary\n+      writer.println(s\"rm -rf ${groundingPath}/dd_tmp\")\n       writer.println(s\"rm -f ${groundingPath}/dd_*\")",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 6,
        "original_position": 6,
        "commit_id": "59f262c00f13b035784b1db0558dba9a3a0d4ca4",
        "original_commit_id": "59f262c00f13b035784b1db0558dba9a3a0d4ca4",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Why not converting line 466 to -rf ?\n",
        "created_at": "2014-11-13T05:06:18Z",
        "updated_at": "2014-11-13T05:06:18Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/164#discussion_r20272797",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/164",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20272797"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/164#discussion_r20272797"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/164"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20273248",
        "pull_request_review_id": null,
        "id": 20273248,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMjczMjQ4",
        "diff_hunk": "@@ -461,6 +461,8 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n     if (parallelGrounding) {\n       val cleanFile = File.createTempFile(s\"clean\", \".sh\")\n       val writer = new PrintWriter(cleanFile)\n+      // cleaning up remaining tmp folder for tobinary\n+      writer.println(s\"rm -rf ${groundingPath}/dd_tmp\")\n       writer.println(s\"rm -f ${groundingPath}/dd_*\")",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 6,
        "original_position": 6,
        "commit_id": "59f262c00f13b035784b1db0558dba9a3a0d4ca4",
        "original_commit_id": "59f262c00f13b035784b1db0558dba9a3a0d4ca4",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I thought it would be a bit safer to do this... e.g. if groundingPath is \"\", it could be a bit dangerous?\n",
        "created_at": "2014-11-13T05:32:21Z",
        "updated_at": "2014-11-13T05:32:21Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/164#discussion_r20273248",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/164",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20273248"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/164#discussion_r20273248"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/164"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20273272",
        "pull_request_review_id": null,
        "id": 20273272,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMjczMjcy",
        "diff_hunk": "@@ -461,6 +461,8 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n     if (parallelGrounding) {\n       val cleanFile = File.createTempFile(s\"clean\", \".sh\")\n       val writer = new PrintWriter(cleanFile)\n+      // cleaning up remaining tmp folder for tobinary\n+      writer.println(s\"rm -rf ${groundingPath}/dd_tmp\")\n       writer.println(s\"rm -f ${groundingPath}/dd_*\")",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 6,
        "original_position": 6,
        "commit_id": "59f262c00f13b035784b1db0558dba9a3a0d4ca4",
        "original_commit_id": "59f262c00f13b035784b1db0558dba9a3a0d4ca4",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Good point. We went over it in the past. Ok, I'm going to merge.\n",
        "created_at": "2014-11-13T05:33:20Z",
        "updated_at": "2014-11-13T05:33:20Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/164#discussion_r20273272",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/164",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20273272"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/164#discussion_r20273272"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/164"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20635831",
        "pull_request_review_id": null,
        "id": 20635831,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNjM1ODMx",
        "diff_hunk": "@@ -14,6 +14,9 @@ import scala.util.Try\n /* Describes the context of the DeepDive application */\n object Context extends Logging {\n \n+  // The akka actor is initialized when \"Context.system\" is first accessed.\n+  // TODO: it might not be best to use a lazy val here, since we may want \n+  // to run \"DeepDive.run\" multiple times, e.g. in tests.",
        "path": "src/main/scala/org/deepdive/Context.scala",
        "position": 6,
        "original_position": 6,
        "commit_id": "6ea19fa7875fa5f309db9c7896d9b278bc0e7997",
        "original_commit_id": "6e83158b0a4d3345c9a802115224ed26b20beda2",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "@zifeishan : Give me more details, please, as I don't know what \"lazy val\" does. Can you also suggest an alternative, please?\n",
        "created_at": "2014-11-20T09:53:40Z",
        "updated_at": "2014-11-20T22:49:48Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20635831",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20635831"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20635831"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20635856",
        "pull_request_review_id": null,
        "id": 20635856,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNjM1ODU2",
        "diff_hunk": "@@ -170,6 +173,14 @@ object DeepDive extends Logging {\n \n     // Clean up resources\n     Context.shutdown()\n+\n+    // end try\n+    } catch {\n+      case e: Exception =>\n+        // In case of any exception\n+        Context.shutdown()\n+        throw e\n+    }",
        "path": "src/main/scala/org/deepdive/DeepDive.scala",
        "position": 35,
        "original_position": 25,
        "commit_id": "6ea19fa7875fa5f309db9c7896d9b278bc0e7997",
        "original_commit_id": "6e83158b0a4d3345c9a802115224ed26b20beda2",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "@zifeishan : Do we have tests for this?\n",
        "created_at": "2014-11-20T09:54:13Z",
        "updated_at": "2014-11-20T22:49:48Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20635856",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20635856"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20635856"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20635898",
        "pull_request_review_id": null,
        "id": 20635898,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNjM1ODk4",
        "diff_hunk": "@@ -25,26 +25,35 @@ class Sampler extends Actor with ActorLogging {\n       val cmd = buildSamplerCmd(samplerCmd, samplerOptions, weightsFile, variablesFile,\n       factorsFile, edgesFile, metaFile, outputDir, parallelGrounding)\n       log.info(s\"Executing: ${cmd.mkString(\" \")}\")\n-      // We run the process, get its exit value, and print its output to the log file\n-      val exitValue = cmd!(ProcessLogger(\n-        out => log.info(out),\n-        err => System.err.println(err)\n-      ))\n-      // Depending on the exit value we return success or kill the program\n-      exitValue match {\n-        case 0 => sender ! Success()\n-        case _ => {\n-          import scala.sys.process._\n-          import java.lang.management\n-          import sun.management.VMManagement;\n-          import java.lang.management.ManagementFactory;\n-          import java.lang.management.RuntimeMXBean;\n-          import java.lang.reflect.Field;\n-          import java.lang.reflect.Method;\n-          var pid = ManagementFactory.getRuntimeMXBean().getName().toString\n-          val pattern = \"\"\"\\d+\"\"\".r\n-          pattern.findAllIn(pid).foreach(id => s\"kill -9 ${id}\".!)\n+      \n+      // Handle the case where cmd! throw exception rather than return a value\n+      try {\n+        // We run the process, get its exit value, and print its output to the log file\n+        val exitValue = cmd!(ProcessLogger(\n+          out => log.info(out),\n+          err => System.err.println(err)\n+        ))        \n+        // Depending on the exit value we return success or kill the program\n+        exitValue match {\n+          case 0 => sender ! Success()\n+          case _ => {\n+            import scala.sys.process._\n+            import java.lang.management\n+            import sun.management.VMManagement;\n+            import java.lang.management.ManagementFactory;\n+            import java.lang.management.RuntimeMXBean;\n+            import java.lang.reflect.Field;\n+            import java.lang.reflect.Method;\n+            var pid = ManagementFactory.getRuntimeMXBean().getName().toString\n+            val pattern = \"\"\"\\d+\"\"\".r\n+            pattern.findAllIn(pid).foreach(id => s\"kill -9 ${id}\".!)\n+          }\n         }\n+      } catch {\n+        // If some exception is thrown, terminate DeepDive\n+        case e: Throwable =>\n+        sender ! Status.Failure(e)\n+        context.stop(self)",
        "path": "src/main/scala/org/deepdive/inference/Sampler.scala",
        "position": 51,
        "original_position": 51,
        "commit_id": "6ea19fa7875fa5f309db9c7896d9b278bc0e7997",
        "original_commit_id": "6e83158b0a4d3345c9a802115224ed26b20beda2",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "@zifeishan : We need tests for this, if not available yet\n",
        "created_at": "2014-11-20T09:54:55Z",
        "updated_at": "2014-11-20T22:49:48Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20635898",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20635898"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20635898"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20635945",
        "pull_request_review_id": null,
        "id": 20635945,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNjM1OTQ1",
        "diff_hunk": "@@ -238,7 +238,8 @@ deepdive {\n   def processResults(): Double = {\n     JdbcDataStore.init(config)\n     var score = 0.0;\n-    val checkQuery = \"\"\"select num_correct / (num_correct + num_incorrect)\n+    val checkQuery = \"\"\"select num_correct / (num_correct + \n+      CASE WHEN num_incorrect IS NULL THEN 0 ELSE num_incorrect END)",
        "path": "src/test/scala/integration/MysqlSpouseExample.scala",
        "position": null,
        "original_position": 6,
        "commit_id": "6ea19fa7875fa5f309db9c7896d9b278bc0e7997",
        "original_commit_id": "6e83158b0a4d3345c9a802115224ed26b20beda2",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "@zifeishan : Please give me details about this change.\n",
        "created_at": "2014-11-20T09:56:11Z",
        "updated_at": "2014-11-20T22:49:48Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20635945",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20635945"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20635945"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20635954",
        "pull_request_review_id": null,
        "id": 20635954,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNjM1OTU0",
        "diff_hunk": "@@ -195,7 +195,8 @@ deepdive {\n   def processResults(): Double = {\n     JdbcDataStore.init(config)\n     var score = 0.0;\n-    val checkQuery = \"\"\"select num_correct::real / (num_correct + num_incorrect) \n+    val checkQuery = \"\"\"select num_correct::real / (num_correct + \n+      CASE WHEN num_incorrect IS NULL THEN 0 ELSE num_incorrect END) ",
        "path": "src/test/scala/integration/PostgresSpouseExample.scala",
        "position": null,
        "original_position": 6,
        "commit_id": "6ea19fa7875fa5f309db9c7896d9b278bc0e7997",
        "original_commit_id": "6e83158b0a4d3345c9a802115224ed26b20beda2",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "@zifeishan : please give me details about this.\n",
        "created_at": "2014-11-20T09:56:31Z",
        "updated_at": "2014-11-20T22:49:48Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20635954",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20635954"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20635954"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20636068",
        "pull_request_review_id": null,
        "id": 20636068,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNjM2MDY4",
        "diff_hunk": "@@ -14,6 +14,9 @@ import scala.util.Try\n /* Describes the context of the DeepDive application */\n object Context extends Logging {\n \n+  // The akka actor is initialized when \"Context.system\" is first accessed.\n+  // TODO: it might not be best to use a lazy val here, since we may want \n+  // to run \"DeepDive.run\" multiple times, e.g. in tests.",
        "path": "src/main/scala/org/deepdive/Context.scala",
        "position": 6,
        "original_position": 6,
        "commit_id": "6ea19fa7875fa5f309db9c7896d9b278bc0e7997",
        "original_commit_id": "6e83158b0a4d3345c9a802115224ed26b20beda2",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "The difference between `lazy val` and `val` is, that a `val` is executed when it is defined, and a `lazy val` is executed when it is accessed the first time.\n\nWe had encountered some problems when executing DeepDive.run several times in integration tests. We have been using some hacks to fix it (running these tests one by one in separate `sbt` commands). If we can fix it, we can run all tests together.\n\nI have to investigate more into the alternative. A possible way might be initializing `Context.system` explicitly every time `DeepDive.run` executes (not sure how), or making `Context` a class rather than an object. But I am not sure.\n",
        "created_at": "2014-11-20T09:59:20Z",
        "updated_at": "2014-11-20T22:49:48Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20636068",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20636068"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20636068"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20636156",
        "pull_request_review_id": null,
        "id": 20636156,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNjM2MTU2",
        "diff_hunk": "@@ -170,6 +173,14 @@ object DeepDive extends Logging {\n \n     // Clean up resources\n     Context.shutdown()\n+\n+    // end try\n+    } catch {\n+      case e: Exception =>\n+        // In case of any exception\n+        Context.shutdown()\n+        throw e\n+    }",
        "path": "src/main/scala/org/deepdive/DeepDive.scala",
        "position": 35,
        "original_position": 25,
        "commit_id": "6ea19fa7875fa5f309db9c7896d9b278bc0e7997",
        "original_commit_id": "6e83158b0a4d3345c9a802115224ed26b20beda2",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Not yet. I can try to add a \"broken\" test for it.\n",
        "created_at": "2014-11-20T10:01:57Z",
        "updated_at": "2014-11-20T22:49:48Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20636156",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20636156"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20636156"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20636266",
        "pull_request_review_id": null,
        "id": 20636266,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNjM2MjY2",
        "diff_hunk": "@@ -238,7 +238,8 @@ deepdive {\n   def processResults(): Double = {\n     JdbcDataStore.init(config)\n     var score = 0.0;\n-    val checkQuery = \"\"\"select num_correct / (num_correct + num_incorrect)\n+    val checkQuery = \"\"\"select num_correct / (num_correct + \n+      CASE WHEN num_incorrect IS NULL THEN 0 ELSE num_incorrect END)",
        "path": "src/test/scala/integration/MysqlSpouseExample.scala",
        "position": null,
        "original_position": 6,
        "commit_id": "6ea19fa7875fa5f309db9c7896d9b278bc0e7997",
        "original_commit_id": "6e83158b0a4d3345c9a802115224ed26b20beda2",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "There is a chance that `num_incorrect` is 0 in `bucket=9`, in this case `num_incorrect` will be NULL rather than 0 in the has_spouse_is_true_calibration table. Not sure if there is a design decision there or is this a bug. But this query will try to compute `X / (X + NULL)` and get NULL as result, which breaks the result.\n",
        "created_at": "2014-11-20T10:04:34Z",
        "updated_at": "2014-11-20T22:49:48Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20636266",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20636266"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20636266"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20636349",
        "pull_request_review_id": null,
        "id": 20636349,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNjM2MzQ5",
        "diff_hunk": "@@ -195,7 +195,8 @@ deepdive {\n   def processResults(): Double = {\n     JdbcDataStore.init(config)\n     var score = 0.0;\n-    val checkQuery = \"\"\"select num_correct::real / (num_correct + num_incorrect) \n+    val checkQuery = \"\"\"select num_correct::real / (num_correct + \n+      CASE WHEN num_incorrect IS NULL THEN 0 ELSE num_incorrect END) ",
        "path": "src/test/scala/integration/PostgresSpouseExample.scala",
        "position": null,
        "original_position": 6,
        "commit_id": "6ea19fa7875fa5f309db9c7896d9b278bc0e7997",
        "original_commit_id": "6e83158b0a4d3345c9a802115224ed26b20beda2",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "(same as above, changing NULL values to 0)\n",
        "created_at": "2014-11-20T10:06:04Z",
        "updated_at": "2014-11-20T22:49:48Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20636349",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20636349"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20636349"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20636365",
        "pull_request_review_id": null,
        "id": 20636365,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNjM2MzY1",
        "diff_hunk": "@@ -266,7 +267,7 @@ deepdive {\n       // DeepDive.run will finally call JdbcDataStore.close()...\n \n       val score = processResults()\n-      assert(score > 0.9)\n+      assert(score > 0.8)",
        "path": "src/test/scala/integration/MysqlSpouseExample.scala",
        "position": null,
        "original_position": 15,
        "commit_id": "6ea19fa7875fa5f309db9c7896d9b278bc0e7997",
        "original_commit_id": "6e83158b0a4d3345c9a802115224ed26b20beda2",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "@rionda This is a mistaken change and I will recover it. Sorry.\n",
        "created_at": "2014-11-20T10:06:27Z",
        "updated_at": "2014-11-20T22:49:48Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20636365",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20636365"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20636365"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20636649",
        "pull_request_review_id": null,
        "id": 20636649,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNjM2NjQ5",
        "diff_hunk": "@@ -25,26 +25,35 @@ class Sampler extends Actor with ActorLogging {\n       val cmd = buildSamplerCmd(samplerCmd, samplerOptions, weightsFile, variablesFile,\n       factorsFile, edgesFile, metaFile, outputDir, parallelGrounding)\n       log.info(s\"Executing: ${cmd.mkString(\" \")}\")\n-      // We run the process, get its exit value, and print its output to the log file\n-      val exitValue = cmd!(ProcessLogger(\n-        out => log.info(out),\n-        err => System.err.println(err)\n-      ))\n-      // Depending on the exit value we return success or kill the program\n-      exitValue match {\n-        case 0 => sender ! Success()\n-        case _ => {\n-          import scala.sys.process._\n-          import java.lang.management\n-          import sun.management.VMManagement;\n-          import java.lang.management.ManagementFactory;\n-          import java.lang.management.RuntimeMXBean;\n-          import java.lang.reflect.Field;\n-          import java.lang.reflect.Method;\n-          var pid = ManagementFactory.getRuntimeMXBean().getName().toString\n-          val pattern = \"\"\"\\d+\"\"\".r\n-          pattern.findAllIn(pid).foreach(id => s\"kill -9 ${id}\".!)\n+      \n+      // Handle the case where cmd! throw exception rather than return a value\n+      try {\n+        // We run the process, get its exit value, and print its output to the log file\n+        val exitValue = cmd!(ProcessLogger(\n+          out => log.info(out),\n+          err => System.err.println(err)\n+        ))        \n+        // Depending on the exit value we return success or kill the program\n+        exitValue match {\n+          case 0 => sender ! Success()\n+          case _ => {\n+            import scala.sys.process._\n+            import java.lang.management\n+            import sun.management.VMManagement;\n+            import java.lang.management.ManagementFactory;\n+            import java.lang.management.RuntimeMXBean;\n+            import java.lang.reflect.Field;\n+            import java.lang.reflect.Method;\n+            var pid = ManagementFactory.getRuntimeMXBean().getName().toString\n+            val pattern = \"\"\"\\d+\"\"\".r\n+            pattern.findAllIn(pid).foreach(id => s\"kill -9 ${id}\".!)\n+          }\n         }\n+      } catch {\n+        // If some exception is thrown, terminate DeepDive\n+        case e: Throwable =>\n+        sender ! Status.Failure(e)\n+        context.stop(self)",
        "path": "src/main/scala/org/deepdive/inference/Sampler.scala",
        "position": 51,
        "original_position": 51,
        "commit_id": "6ea19fa7875fa5f309db9c7896d9b278bc0e7997",
        "original_commit_id": "6e83158b0a4d3345c9a802115224ed26b20beda2",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I will try if I can add a unit test for this. It could be hard. \n\nActually this is also captured by the big catch block in DeepDive.scala. I can change it back if necessary. There are many catch blocks like this that can be removed now that the big try-catch block works.\n",
        "created_at": "2014-11-20T10:12:56Z",
        "updated_at": "2014-11-20T22:49:48Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20636649",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20636649"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20636649"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20637098",
        "pull_request_review_id": null,
        "id": 20637098,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNjM3MDk4",
        "diff_hunk": "@@ -25,26 +25,35 @@ class Sampler extends Actor with ActorLogging {\n       val cmd = buildSamplerCmd(samplerCmd, samplerOptions, weightsFile, variablesFile,\n       factorsFile, edgesFile, metaFile, outputDir, parallelGrounding)\n       log.info(s\"Executing: ${cmd.mkString(\" \")}\")\n-      // We run the process, get its exit value, and print its output to the log file\n-      val exitValue = cmd!(ProcessLogger(\n-        out => log.info(out),\n-        err => System.err.println(err)\n-      ))\n-      // Depending on the exit value we return success or kill the program\n-      exitValue match {\n-        case 0 => sender ! Success()\n-        case _ => {\n-          import scala.sys.process._\n-          import java.lang.management\n-          import sun.management.VMManagement;\n-          import java.lang.management.ManagementFactory;\n-          import java.lang.management.RuntimeMXBean;\n-          import java.lang.reflect.Field;\n-          import java.lang.reflect.Method;\n-          var pid = ManagementFactory.getRuntimeMXBean().getName().toString\n-          val pattern = \"\"\"\\d+\"\"\".r\n-          pattern.findAllIn(pid).foreach(id => s\"kill -9 ${id}\".!)\n+      \n+      // Handle the case where cmd! throw exception rather than return a value\n+      try {\n+        // We run the process, get its exit value, and print its output to the log file\n+        val exitValue = cmd!(ProcessLogger(\n+          out => log.info(out),\n+          err => System.err.println(err)\n+        ))        \n+        // Depending on the exit value we return success or kill the program\n+        exitValue match {\n+          case 0 => sender ! Success()\n+          case _ => {\n+            import scala.sys.process._\n+            import java.lang.management\n+            import sun.management.VMManagement;\n+            import java.lang.management.ManagementFactory;\n+            import java.lang.management.RuntimeMXBean;\n+            import java.lang.reflect.Field;\n+            import java.lang.reflect.Method;\n+            var pid = ManagementFactory.getRuntimeMXBean().getName().toString\n+            val pattern = \"\"\"\\d+\"\"\".r\n+            pattern.findAllIn(pid).foreach(id => s\"kill -9 ${id}\".!)\n+          }\n         }\n+      } catch {\n+        // If some exception is thrown, terminate DeepDive\n+        case e: Throwable =>\n+        sender ! Status.Failure(e)\n+        context.stop(self)",
        "path": "src/main/scala/org/deepdive/inference/Sampler.scala",
        "position": 51,
        "original_position": 51,
        "commit_id": "6ea19fa7875fa5f309db9c7896d9b278bc0e7997",
        "original_commit_id": "6e83158b0a4d3345c9a802115224ed26b20beda2",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "@rionda Should I change this back or leave it here? \n\nI found it hard to test these non-terminating bugs, since exceptions are thrown anyway, even if the system is stuck there. (same for testing the large try block in DeepDive.scala)\n",
        "created_at": "2014-11-20T10:24:04Z",
        "updated_at": "2014-11-20T22:49:48Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20637098",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20637098"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20637098"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20637698",
        "pull_request_review_id": null,
        "id": 20637698,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNjM3Njk4",
        "diff_hunk": "@@ -170,6 +173,14 @@ object DeepDive extends Logging {\n \n     // Clean up resources\n     Context.shutdown()\n+\n+    // end try\n+    } catch {\n+      case e: Exception =>\n+        // In case of any exception\n+        Context.shutdown()\n+        throw e\n+    }",
        "path": "src/main/scala/org/deepdive/DeepDive.scala",
        "position": 35,
        "original_position": 25,
        "commit_id": "6ea19fa7875fa5f309db9c7896d9b278bc0e7997",
        "original_commit_id": "6e83158b0a4d3345c9a802115224ed26b20beda2",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I may have found a way for the broken test (with a timeout).\n",
        "created_at": "2014-11-20T10:37:35Z",
        "updated_at": "2014-11-20T22:49:48Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20637698",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20637698"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20637698"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20638882",
        "pull_request_review_id": null,
        "id": 20638882,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNjM4ODgy",
        "diff_hunk": "@@ -170,6 +173,14 @@ object DeepDive extends Logging {\n \n     // Clean up resources\n     Context.shutdown()\n+\n+    // end try\n+    } catch {\n+      case e: Exception =>\n+        // In case of any exception\n+        Context.shutdown()\n+        throw e\n+    }",
        "path": "src/main/scala/org/deepdive/DeepDive.scala",
        "position": 35,
        "original_position": 25,
        "commit_id": "6ea19fa7875fa5f309db9c7896d9b278bc0e7997",
        "original_commit_id": "6e83158b0a4d3345c9a802115224ed26b20beda2",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Added a test in b2652ef4eaae8aad96dbc24acb6bd8bfcad0178b.\n",
        "created_at": "2014-11-20T11:04:55Z",
        "updated_at": "2014-11-20T22:49:48Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20638882",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20638882"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20638882"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20639118",
        "pull_request_review_id": null,
        "id": 20639118,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNjM5MTE4",
        "diff_hunk": "@@ -25,26 +25,35 @@ class Sampler extends Actor with ActorLogging {\n       val cmd = buildSamplerCmd(samplerCmd, samplerOptions, weightsFile, variablesFile,\n       factorsFile, edgesFile, metaFile, outputDir, parallelGrounding)\n       log.info(s\"Executing: ${cmd.mkString(\" \")}\")\n-      // We run the process, get its exit value, and print its output to the log file\n-      val exitValue = cmd!(ProcessLogger(\n-        out => log.info(out),\n-        err => System.err.println(err)\n-      ))\n-      // Depending on the exit value we return success or kill the program\n-      exitValue match {\n-        case 0 => sender ! Success()\n-        case _ => {\n-          import scala.sys.process._\n-          import java.lang.management\n-          import sun.management.VMManagement;\n-          import java.lang.management.ManagementFactory;\n-          import java.lang.management.RuntimeMXBean;\n-          import java.lang.reflect.Field;\n-          import java.lang.reflect.Method;\n-          var pid = ManagementFactory.getRuntimeMXBean().getName().toString\n-          val pattern = \"\"\"\\d+\"\"\".r\n-          pattern.findAllIn(pid).foreach(id => s\"kill -9 ${id}\".!)\n+      \n+      // Handle the case where cmd! throw exception rather than return a value\n+      try {\n+        // We run the process, get its exit value, and print its output to the log file\n+        val exitValue = cmd!(ProcessLogger(\n+          out => log.info(out),\n+          err => System.err.println(err)\n+        ))        \n+        // Depending on the exit value we return success or kill the program\n+        exitValue match {\n+          case 0 => sender ! Success()\n+          case _ => {\n+            import scala.sys.process._\n+            import java.lang.management\n+            import sun.management.VMManagement;\n+            import java.lang.management.ManagementFactory;\n+            import java.lang.management.RuntimeMXBean;\n+            import java.lang.reflect.Field;\n+            import java.lang.reflect.Method;\n+            var pid = ManagementFactory.getRuntimeMXBean().getName().toString\n+            val pattern = \"\"\"\\d+\"\"\".r\n+            pattern.findAllIn(pid).foreach(id => s\"kill -9 ${id}\".!)\n+          }\n         }\n+      } catch {\n+        // If some exception is thrown, terminate DeepDive\n+        case e: Throwable =>\n+        sender ! Status.Failure(e)\n+        context.stop(self)",
        "path": "src/main/scala/org/deepdive/inference/Sampler.scala",
        "position": 51,
        "original_position": 51,
        "commit_id": "6ea19fa7875fa5f309db9c7896d9b278bc0e7997",
        "original_commit_id": "6e83158b0a4d3345c9a802115224ed26b20beda2",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I could not test this easily, since `context.stop(self)` will terminate the test. \n\nI just experimented and found this is not covered by the try block in DeepDive.scala (it won't throw exceptions to the actor where DeepDive runs, but to some other Akka actor), so it is a necessary fix.\n",
        "created_at": "2014-11-20T11:10:22Z",
        "updated_at": "2014-11-20T22:49:48Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20639118",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20639118"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20639118"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20682881",
        "pull_request_review_id": null,
        "id": 20682881,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNjgyODgx",
        "diff_hunk": "@@ -14,6 +14,9 @@ import scala.util.Try\n /* Describes the context of the DeepDive application */\n object Context extends Logging {\n \n+  // The akka actor is initialized when \"Context.system\" is first accessed.\n+  // TODO: it might not be best to use a lazy val here, since we may want \n+  // to run \"DeepDive.run\" multiple times, e.g. in tests.",
        "path": "src/main/scala/org/deepdive/Context.scala",
        "position": 6,
        "original_position": 6,
        "commit_id": "6ea19fa7875fa5f309db9c7896d9b278bc0e7997",
        "original_commit_id": "6e83158b0a4d3345c9a802115224ed26b20beda2",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I see. Ok, please add your reply to the source code, so we have this information for the future.\n",
        "created_at": "2014-11-20T22:30:28Z",
        "updated_at": "2014-11-20T22:49:48Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20682881",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20682881"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20682881"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20682898",
        "pull_request_review_id": null,
        "id": 20682898,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNjgyODk4",
        "diff_hunk": "@@ -170,6 +173,14 @@ object DeepDive extends Logging {\n \n     // Clean up resources\n     Context.shutdown()\n+\n+    // end try\n+    } catch {\n+      case e: Exception =>\n+        // In case of any exception\n+        Context.shutdown()\n+        throw e\n+    }",
        "path": "src/main/scala/org/deepdive/DeepDive.scala",
        "position": 35,
        "original_position": 25,
        "commit_id": "6ea19fa7875fa5f309db9c7896d9b278bc0e7997",
        "original_commit_id": "6e83158b0a4d3345c9a802115224ed26b20beda2",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Great, thanks\n",
        "created_at": "2014-11-20T22:30:44Z",
        "updated_at": "2014-11-20T22:49:48Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20682898",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20682898"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20682898"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20682940",
        "pull_request_review_id": null,
        "id": 20682940,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNjgyOTQw",
        "diff_hunk": "@@ -25,26 +25,35 @@ class Sampler extends Actor with ActorLogging {\n       val cmd = buildSamplerCmd(samplerCmd, samplerOptions, weightsFile, variablesFile,\n       factorsFile, edgesFile, metaFile, outputDir, parallelGrounding)\n       log.info(s\"Executing: ${cmd.mkString(\" \")}\")\n-      // We run the process, get its exit value, and print its output to the log file\n-      val exitValue = cmd!(ProcessLogger(\n-        out => log.info(out),\n-        err => System.err.println(err)\n-      ))\n-      // Depending on the exit value we return success or kill the program\n-      exitValue match {\n-        case 0 => sender ! Success()\n-        case _ => {\n-          import scala.sys.process._\n-          import java.lang.management\n-          import sun.management.VMManagement;\n-          import java.lang.management.ManagementFactory;\n-          import java.lang.management.RuntimeMXBean;\n-          import java.lang.reflect.Field;\n-          import java.lang.reflect.Method;\n-          var pid = ManagementFactory.getRuntimeMXBean().getName().toString\n-          val pattern = \"\"\"\\d+\"\"\".r\n-          pattern.findAllIn(pid).foreach(id => s\"kill -9 ${id}\".!)\n+      \n+      // Handle the case where cmd! throw exception rather than return a value\n+      try {\n+        // We run the process, get its exit value, and print its output to the log file\n+        val exitValue = cmd!(ProcessLogger(\n+          out => log.info(out),\n+          err => System.err.println(err)\n+        ))        \n+        // Depending on the exit value we return success or kill the program\n+        exitValue match {\n+          case 0 => sender ! Success()\n+          case _ => {\n+            import scala.sys.process._\n+            import java.lang.management\n+            import sun.management.VMManagement;\n+            import java.lang.management.ManagementFactory;\n+            import java.lang.management.RuntimeMXBean;\n+            import java.lang.reflect.Field;\n+            import java.lang.reflect.Method;\n+            var pid = ManagementFactory.getRuntimeMXBean().getName().toString\n+            val pattern = \"\"\"\\d+\"\"\".r\n+            pattern.findAllIn(pid).foreach(id => s\"kill -9 ${id}\".!)\n+          }\n         }\n+      } catch {\n+        // If some exception is thrown, terminate DeepDive\n+        case e: Throwable =>\n+        sender ! Status.Failure(e)\n+        context.stop(self)",
        "path": "src/main/scala/org/deepdive/inference/Sampler.scala",
        "position": 51,
        "original_position": 51,
        "commit_id": "6ea19fa7875fa5f309db9c7896d9b278bc0e7997",
        "original_commit_id": "6e83158b0a4d3345c9a802115224ed26b20beda2",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Ok, fine.\n",
        "created_at": "2014-11-20T22:31:29Z",
        "updated_at": "2014-11-20T22:49:48Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20682940",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20682940"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20682940"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20683011",
        "pull_request_review_id": null,
        "id": 20683011,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNjgzMDEx",
        "diff_hunk": "@@ -238,7 +238,8 @@ deepdive {\n   def processResults(): Double = {\n     JdbcDataStore.init(config)\n     var score = 0.0;\n-    val checkQuery = \"\"\"select num_correct / (num_correct + num_incorrect)\n+    val checkQuery = \"\"\"select num_correct / (num_correct + \n+      CASE WHEN num_incorrect IS NULL THEN 0 ELSE num_incorrect END)",
        "path": "src/test/scala/integration/MysqlSpouseExample.scala",
        "position": null,
        "original_position": 6,
        "commit_id": "6ea19fa7875fa5f309db9c7896d9b278bc0e7997",
        "original_commit_id": "6e83158b0a4d3345c9a802115224ed26b20beda2",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "ok, good workaround, thanks, although it's probably a design bug in the calibration table design, in my opinion.\n",
        "created_at": "2014-11-20T22:32:45Z",
        "updated_at": "2014-11-20T22:49:48Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20683011",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20683011"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20683011"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20683084",
        "pull_request_review_id": null,
        "id": 20683084,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNjgzMDg0",
        "diff_hunk": "@@ -195,7 +195,8 @@ deepdive {\n   def processResults(): Double = {\n     JdbcDataStore.init(config)\n     var score = 0.0;\n-    val checkQuery = \"\"\"select num_correct::real / (num_correct + num_incorrect) \n+    val checkQuery = \"\"\"select num_correct::real / (num_correct + \n+      CASE WHEN num_incorrect IS NULL THEN 0 ELSE num_incorrect END) ",
        "path": "src/test/scala/integration/PostgresSpouseExample.scala",
        "position": null,
        "original_position": 6,
        "commit_id": "6ea19fa7875fa5f309db9c7896d9b278bc0e7997",
        "original_commit_id": "6e83158b0a4d3345c9a802115224ed26b20beda2",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "ok\n",
        "created_at": "2014-11-20T22:33:51Z",
        "updated_at": "2014-11-20T22:49:48Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20683084",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/20683084"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/172#discussion_r20683084"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/172"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/21478195",
        "pull_request_review_id": null,
        "id": 21478195,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNDc4MTk1",
        "diff_hunk": "@@ -424,8 +424,55 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n     execute(s\"UPDATE ${table} SET id = ${nextVal(IdSequence)};\")\n   }\n         \n+  // clean up grounding folder (for parallel grounding)\n+  def cleanParallelGroundingPath(groundingPath: String) {\n+    val cleanFile = File.createTempFile(s\"clean\", \".sh\")\n+    val writer = new PrintWriter(cleanFile)\n+    // cleaning up remaining tmp folder for tobinary\n+    writer.println(s\"rm -rf ${groundingPath}/dd_tmp\")\n+    writer.println(s\"rm -f ${groundingPath}/dd_*\")",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 16,
        "original_position": 10,
        "commit_id": "aa4ac26d798b3b3c2c225dae75503fe9acb0a491",
        "original_commit_id": "ef3e4f4cc596fe86b6b5a6a86c1f2a50a0e97510",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Can `groundingPath` be an empty string? If so, this command is a bit dangerous to me. Probably we could validate it's not empty when we set `groundingPath`?\n",
        "created_at": "2014-12-08T19:34:44Z",
        "updated_at": "2014-12-08T19:58:33Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/179#discussion_r21478195",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/179",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/21478195"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/179#discussion_r21478195"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/179"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/21478361",
        "pull_request_review_id": null,
        "id": 21478361,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNDc4MzYx",
        "diff_hunk": "@@ -424,8 +424,55 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n     execute(s\"UPDATE ${table} SET id = ${nextVal(IdSequence)};\")\n   }\n         \n+  // clean up grounding folder (for parallel grounding)\n+  def cleanParallelGroundingPath(groundingPath: String) {\n+    val cleanFile = File.createTempFile(s\"clean\", \".sh\")\n+    val writer = new PrintWriter(cleanFile)\n+    // cleaning up remaining tmp folder for tobinary\n+    writer.println(s\"rm -rf ${groundingPath}/dd_tmp\")\n+    writer.println(s\"rm -f ${groundingPath}/dd_*\")",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 16,
        "original_position": 10,
        "commit_id": "aa4ac26d798b3b3c2c225dae75503fe9acb0a491",
        "original_commit_id": "ef3e4f4cc596fe86b6b5a6a86c1f2a50a0e97510",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Exactly as much as it could have been empty before. This is bug-to-bug-compatible with the previous version, I just moved the code.\n",
        "created_at": "2014-12-08T19:36:50Z",
        "updated_at": "2014-12-08T19:58:33Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/179#discussion_r21478361",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/179",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/21478361"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/179#discussion_r21478361"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/179"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/21478403",
        "pull_request_review_id": null,
        "id": 21478403,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNDc4NDAz",
        "diff_hunk": "@@ -424,8 +424,55 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n     execute(s\"UPDATE ${table} SET id = ${nextVal(IdSequence)};\")\n   }\n         \n+  // clean up grounding folder (for parallel grounding)\n+  def cleanParallelGroundingPath(groundingPath: String) {\n+    val cleanFile = File.createTempFile(s\"clean\", \".sh\")\n+    val writer = new PrintWriter(cleanFile)\n+    // cleaning up remaining tmp folder for tobinary\n+    writer.println(s\"rm -rf ${groundingPath}/dd_tmp\")\n+    writer.println(s\"rm -f ${groundingPath}/dd_*\")\n+    writer.close()\n+    log.info(\"Cleaning up grounding folder...\")\n+    Helpers.executeCmd(cleanFile.getAbsolutePath())",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 19,
        "original_position": 13,
        "commit_id": "aa4ac26d798b3b3c2c225dae75503fe9acb0a491",
        "original_commit_id": "ef3e4f4cc596fe86b6b5a6a86c1f2a50a0e97510",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Have we tried directly running\n\n```\nHelpers.executeCmd(s\"rm -rf ${groundingPath}/dd_tmp\")\nHelpers.executeCmd(s\"rm -f ${groundingPath}/dd_*\")\n```\n\nInstead of this long paragraph or writing a temporary file? This helper function should be designed for this task unless we have compelling reasons to write a temp file for it.\n",
        "created_at": "2014-12-08T19:37:17Z",
        "updated_at": "2014-12-08T19:58:33Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/179#discussion_r21478403",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/179",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/21478403"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/179#discussion_r21478403"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/179"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/21478603",
        "pull_request_review_id": null,
        "id": 21478603,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNDc4NjAz",
        "diff_hunk": "@@ -424,8 +424,55 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n     execute(s\"UPDATE ${table} SET id = ${nextVal(IdSequence)};\")\n   }\n         \n+  // clean up grounding folder (for parallel grounding)\n+  def cleanParallelGroundingPath(groundingPath: String) {\n+    val cleanFile = File.createTempFile(s\"clean\", \".sh\")\n+    val writer = new PrintWriter(cleanFile)\n+    // cleaning up remaining tmp folder for tobinary\n+    writer.println(s\"rm -rf ${groundingPath}/dd_tmp\")\n+    writer.println(s\"rm -f ${groundingPath}/dd_*\")\n+    writer.close()\n+    log.info(\"Cleaning up grounding folder...\")\n+    Helpers.executeCmd(cleanFile.getAbsolutePath())\n+  }\n+\n+  // assign variable id - sequential and unique\n+  def assignVariablesIds(schema: Map[String, _ <: VariableDataType]) {\n+    // check whether Greenplum is used\n+    var usingGreenplum = false\n+    issueQuery(checkGreenplumSQL) { rs => \n+      usingGreenplum = rs.getBoolean(1) \n+    }\n+\n+    var idoffset : Long = 0\n+    if (usingGreenplum) {\n+      // We use a special homemade function for Greenplum\n+      executeQuery(createAssignIdFunctionGreenplum)\n+      schema.foreach { case(variable, dataType) =>\n+        val Array(relation, column) = variable.split('.')\n+        executeQuery(s\"\"\"SELECT fast_seqassign('${relation.toLowerCase()}', ${idoffset});\"\"\")\n+        var maxid : Long = 0\n+        issueQuery(s\"\"\"SELECT max(id) FROM ${relation}\"\"\") { rs =>",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": null,
        "original_position": 32,
        "commit_id": "aa4ac26d798b3b3c2c225dae75503fe9acb0a491",
        "original_commit_id": "ef3e4f4cc596fe86b6b5a6a86c1f2a50a0e97510",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I remember yesterday @feiranwang  mentioned that we probably want to use `COUNT(*)` instead of `max(id)` to handle NULL case. Has he already fixed this? Which way is better?\n",
        "created_at": "2014-12-08T19:39:57Z",
        "updated_at": "2014-12-08T19:58:33Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/179#discussion_r21478603",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/179",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/21478603"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/179#discussion_r21478603"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/179"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/21478756",
        "pull_request_review_id": null,
        "id": 21478756,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNDc4NzU2",
        "diff_hunk": "@@ -424,8 +424,55 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n     execute(s\"UPDATE ${table} SET id = ${nextVal(IdSequence)};\")\n   }\n         \n+  // clean up grounding folder (for parallel grounding)\n+  def cleanParallelGroundingPath(groundingPath: String) {\n+    val cleanFile = File.createTempFile(s\"clean\", \".sh\")\n+    val writer = new PrintWriter(cleanFile)\n+    // cleaning up remaining tmp folder for tobinary\n+    writer.println(s\"rm -rf ${groundingPath}/dd_tmp\")\n+    writer.println(s\"rm -f ${groundingPath}/dd_*\")\n+    writer.close()\n+    log.info(\"Cleaning up grounding folder...\")\n+    Helpers.executeCmd(cleanFile.getAbsolutePath())\n+  }\n+\n+  // assign variable id - sequential and unique\n+  def assignVariablesIds(schema: Map[String, _ <: VariableDataType]) {\n+    // check whether Greenplum is used\n+    var usingGreenplum = false\n+    issueQuery(checkGreenplumSQL) { rs => \n+      usingGreenplum = rs.getBoolean(1) \n+    }\n+\n+    var idoffset : Long = 0\n+    if (usingGreenplum) {\n+      // We use a special homemade function for Greenplum\n+      executeQuery(createAssignIdFunctionGreenplum)\n+      schema.foreach { case(variable, dataType) =>\n+        val Array(relation, column) = variable.split('.')\n+        executeQuery(s\"\"\"SELECT fast_seqassign('${relation.toLowerCase()}', ${idoffset});\"\"\")\n+        var maxid : Long = 0\n+        issueQuery(s\"\"\"SELECT max(id) FROM ${relation}\"\"\") { rs =>",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": null,
        "original_position": 32,
        "commit_id": "aa4ac26d798b3b3c2c225dae75503fe9acb0a491",
        "original_commit_id": "ef3e4f4cc596fe86b6b5a6a86c1f2a50a0e97510",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Yes, I just converted it to count() (haven't committed it yet). count(*) is the right choice because it takes care of some corner case. I will commit it soon, I'm doing some testing.\n",
        "created_at": "2014-12-08T19:41:45Z",
        "updated_at": "2014-12-08T19:58:33Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/179#discussion_r21478756",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/179",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/21478756"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/179#discussion_r21478756"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/179"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/21478784",
        "pull_request_review_id": null,
        "id": 21478784,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNDc4Nzg0",
        "diff_hunk": "@@ -424,8 +424,55 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n     execute(s\"UPDATE ${table} SET id = ${nextVal(IdSequence)};\")\n   }\n         \n+  // clean up grounding folder (for parallel grounding)\n+  def cleanParallelGroundingPath(groundingPath: String) {\n+    val cleanFile = File.createTempFile(s\"clean\", \".sh\")\n+    val writer = new PrintWriter(cleanFile)\n+    // cleaning up remaining tmp folder for tobinary\n+    writer.println(s\"rm -rf ${groundingPath}/dd_tmp\")\n+    writer.println(s\"rm -f ${groundingPath}/dd_*\")\n+    writer.close()\n+    log.info(\"Cleaning up grounding folder...\")\n+    Helpers.executeCmd(cleanFile.getAbsolutePath())",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 19,
        "original_position": 13,
        "commit_id": "aa4ac26d798b3b3c2c225dae75503fe9acb0a491",
        "original_commit_id": "ef3e4f4cc596fe86b6b5a6a86c1f2a50a0e97510",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Ok, this is a TODO for later.\n",
        "created_at": "2014-12-08T19:42:03Z",
        "updated_at": "2014-12-08T19:58:33Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/179#discussion_r21478784",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/179",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/21478784"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/179#discussion_r21478784"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/179"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/21478785",
        "pull_request_review_id": null,
        "id": 21478785,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNDc4Nzg1",
        "diff_hunk": "@@ -424,8 +424,55 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n     execute(s\"UPDATE ${table} SET id = ${nextVal(IdSequence)};\")\n   }\n         \n+  // clean up grounding folder (for parallel grounding)\n+  def cleanParallelGroundingPath(groundingPath: String) {\n+    val cleanFile = File.createTempFile(s\"clean\", \".sh\")\n+    val writer = new PrintWriter(cleanFile)\n+    // cleaning up remaining tmp folder for tobinary\n+    writer.println(s\"rm -rf ${groundingPath}/dd_tmp\")\n+    writer.println(s\"rm -f ${groundingPath}/dd_*\")",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 16,
        "original_position": 10,
        "commit_id": "aa4ac26d798b3b3c2c225dae75503fe9acb0a491",
        "original_commit_id": "ef3e4f4cc596fe86b6b5a6a86c1f2a50a0e97510",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "OK makes sense. Probably we want to try cleaning these in future develop branch, but not this hotfix.\n",
        "created_at": "2014-12-08T19:42:03Z",
        "updated_at": "2014-12-08T19:58:33Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/179#discussion_r21478785",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/179",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/21478785"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/179#discussion_r21478785"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/179"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/21478965",
        "pull_request_review_id": null,
        "id": 21478965,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNDc4OTY1",
        "diff_hunk": "@@ -424,8 +424,55 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n     execute(s\"UPDATE ${table} SET id = ${nextVal(IdSequence)};\")\n   }\n         \n+  // clean up grounding folder (for parallel grounding)\n+  def cleanParallelGroundingPath(groundingPath: String) {\n+    val cleanFile = File.createTempFile(s\"clean\", \".sh\")\n+    val writer = new PrintWriter(cleanFile)\n+    // cleaning up remaining tmp folder for tobinary\n+    writer.println(s\"rm -rf ${groundingPath}/dd_tmp\")\n+    writer.println(s\"rm -f ${groundingPath}/dd_*\")\n+    writer.close()\n+    log.info(\"Cleaning up grounding folder...\")\n+    Helpers.executeCmd(cleanFile.getAbsolutePath())\n+  }\n+\n+  // assign variable id - sequential and unique\n+  def assignVariablesIds(schema: Map[String, _ <: VariableDataType]) {\n+    // check whether Greenplum is used\n+    var usingGreenplum = false\n+    issueQuery(checkGreenplumSQL) { rs => \n+      usingGreenplum = rs.getBoolean(1) \n+    }\n+\n+    var idoffset : Long = 0\n+    if (usingGreenplum) {\n+      // We use a special homemade function for Greenplum\n+      executeQuery(createAssignIdFunctionGreenplum)\n+      schema.foreach { case(variable, dataType) =>\n+        val Array(relation, column) = variable.split('.')\n+        executeQuery(s\"\"\"SELECT fast_seqassign('${relation.toLowerCase()}', ${idoffset});\"\"\")\n+        var maxid : Long = 0\n+        issueQuery(s\"\"\"SELECT max(id) FROM ${relation}\"\"\") { rs =>",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": null,
        "original_position": 32,
        "commit_id": "aa4ac26d798b3b3c2c225dae75503fe9acb0a491",
        "original_commit_id": "ef3e4f4cc596fe86b6b5a6a86c1f2a50a0e97510",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Great. Make sure to run `make test` on a GP machine since the GP test is currently still not automatically covered by Travis or Jenkins. I just tested this version and it passed all tests. The Jenkins test will be fixed soon with Jaeho's help.\n",
        "created_at": "2014-12-08T19:44:15Z",
        "updated_at": "2014-12-08T19:58:33Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/179#discussion_r21478965",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/179",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/21478965"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/179#discussion_r21478965"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/179"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/21505443",
        "pull_request_review_id": null,
        "id": 21505443,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNTA1NDQz",
        "diff_hunk": "@@ -23,8 +23,6 @@ trait MysqlInferenceDataStoreComponent extends SQLInferenceDataStoreComponent {\n   \n   class MysqlInferenceDataStore(val dbSettings : DbSettings) extends SQLInferenceDataStore with Logging {\n \n-    implicit lazy val connection = MysqlDataStore.borrowConnection()",
        "path": "src/main/scala/org/deepdive/inference/datastore/MysqlInferenceDataStore.scala",
        "position": 4,
        "original_position": 4,
        "commit_id": "35f16532215d654cbda833a1f8a4857632db4fce",
        "original_commit_id": "fb548442a478c05629e597e2ab523934d6ede2b2",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Why did we remove this?\n",
        "created_at": "2014-12-09T03:32:21Z",
        "updated_at": "2014-12-09T17:44:40Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/180#discussion_r21505443",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/180",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/21505443"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/180#discussion_r21505443"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/180"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/21505477",
        "pull_request_review_id": null,
        "id": 21505477,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNTA1NDc3",
        "diff_hunk": "@@ -23,8 +23,6 @@ trait MysqlInferenceDataStoreComponent extends SQLInferenceDataStoreComponent {\n   \n   class MysqlInferenceDataStore(val dbSettings : DbSettings) extends SQLInferenceDataStore with Logging {\n \n-    implicit lazy val connection = MysqlDataStore.borrowConnection()",
        "path": "src/main/scala/org/deepdive/inference/datastore/MysqlInferenceDataStore.scala",
        "position": 4,
        "original_position": 4,
        "commit_id": "35f16532215d654cbda833a1f8a4857632db4fce",
        "original_commit_id": "fb548442a478c05629e597e2ab523934d6ede2b2",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "It doesn't seem to be used. Am I wrong?\nOn Dec 8, 2014 7:32 PM, \"Zifei Shan\" notifications@github.com wrote:\n\n> In\n> src/main/scala/org/deepdive/inference/datastore/MysqlInferenceDataStore.scala:\n> \n> > @@ -23,8 +23,6 @@ trait MysqlInferenceDataStoreComponent extends SQLInferenceDataStoreComponent {\n> > \n> >    class MysqlInferenceDataStore(val dbSettings : DbSettings) extends SQLInferenceDataStore with Logging {\n> > -    implicit lazy val connection = MysqlDataStore.borrowConnection()\n> \n> Why did we remove this?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/HazyResearch/deepdive/pull/180/files#r21505443.\n",
        "created_at": "2014-12-09T03:34:18Z",
        "updated_at": "2014-12-09T17:44:40Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/180#discussion_r21505477",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/180",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/21505477"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/180#discussion_r21505477"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/180"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/21505574",
        "pull_request_review_id": null,
        "id": 21505574,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNTA1NTc0",
        "diff_hunk": "@@ -23,8 +23,6 @@ trait MysqlInferenceDataStoreComponent extends SQLInferenceDataStoreComponent {\n   \n   class MysqlInferenceDataStore(val dbSettings : DbSettings) extends SQLInferenceDataStore with Logging {\n \n-    implicit lazy val connection = MysqlDataStore.borrowConnection()",
        "path": "src/main/scala/org/deepdive/inference/datastore/MysqlInferenceDataStore.scala",
        "position": 4,
        "original_position": 4,
        "commit_id": "35f16532215d654cbda833a1f8a4857632db4fce",
        "original_commit_id": "fb548442a478c05629e597e2ab523934d6ede2b2",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Not sure if this is the problem. Let me check it.\n",
        "created_at": "2014-12-09T03:39:12Z",
        "updated_at": "2014-12-09T17:44:40Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/180#discussion_r21505574",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/180",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/21505574"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/180#discussion_r21505574"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/180"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/21505833",
        "pull_request_review_id": null,
        "id": 21505833,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNTA1ODMz",
        "diff_hunk": "@@ -23,8 +23,6 @@ trait MysqlInferenceDataStoreComponent extends SQLInferenceDataStoreComponent {\n   \n   class MysqlInferenceDataStore(val dbSettings : DbSettings) extends SQLInferenceDataStore with Logging {\n \n-    implicit lazy val connection = MysqlDataStore.borrowConnection()",
        "path": "src/main/scala/org/deepdive/inference/datastore/MysqlInferenceDataStore.scala",
        "position": 4,
        "original_position": 4,
        "commit_id": "35f16532215d654cbda833a1f8a4857632db4fce",
        "original_commit_id": "fb548442a478c05629e597e2ab523934d6ede2b2",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I doubt it, it didn't worked before removing these.\nOn Dec 8, 2014 7:39 PM, \"Zifei Shan\" notifications@github.com wrote:\n\n> In\n> src/main/scala/org/deepdive/inference/datastore/MysqlInferenceDataStore.scala:\n> \n> > @@ -23,8 +23,6 @@ trait MysqlInferenceDataStoreComponent extends SQLInferenceDataStoreComponent {\n> > \n> >    class MysqlInferenceDataStore(val dbSettings : DbSettings) extends SQLInferenceDataStore with Logging {\n> > -    implicit lazy val connection = MysqlDataStore.borrowConnection()\n> \n> Not sure if this is the problem. Let me check it.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/HazyResearch/deepdive/pull/180/files#r21505574.\n",
        "created_at": "2014-12-09T03:50:46Z",
        "updated_at": "2014-12-09T17:44:40Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/180#discussion_r21505833",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/180",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/21505833"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/180#discussion_r21505833"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/180"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/21506129",
        "pull_request_review_id": null,
        "id": 21506129,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNTA2MTI5",
        "diff_hunk": "@@ -23,8 +23,6 @@ trait MysqlInferenceDataStoreComponent extends SQLInferenceDataStoreComponent {\n   \n   class MysqlInferenceDataStore(val dbSettings : DbSettings) extends SQLInferenceDataStore with Logging {\n \n-    implicit lazy val connection = MysqlDataStore.borrowConnection()",
        "path": "src/main/scala/org/deepdive/inference/datastore/MysqlInferenceDataStore.scala",
        "position": 4,
        "original_position": 4,
        "commit_id": "35f16532215d654cbda833a1f8a4857632db4fce",
        "original_commit_id": "fb548442a478c05629e597e2ab523934d6ede2b2",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Now it turned out that this line is useless =) Good job.\n",
        "created_at": "2014-12-09T04:04:22Z",
        "updated_at": "2014-12-09T17:44:40Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/180#discussion_r21506129",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/180",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/21506129"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/180#discussion_r21506129"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/180"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/22774116",
        "pull_request_review_id": null,
        "id": 22774116,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyNzc0MTE2",
        "diff_hunk": "@@ -1,20 +1,19 @@\n set -e\n \n+# if greenplum supported (gpfdist exist), then test GP\n+if hash gpfdist 2>/dev/null; then\n+  echo \"Testing Greenplum...\"\n+  bash test/test_gp.sh $1\n # if command psql exist, then test psql\n-if hash psql 2>/dev/null; then\n+elif hash psql 2>/dev/null; then",
        "path": "test.sh",
        "position": null,
        "original_position": 9,
        "commit_id": "bc2f68b4e15e36bf192604cd6f979f5598a2a380",
        "original_commit_id": "4e54f1f6289c0d512c74205a08c56ebfac6047ca",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "@rionda It was intended to test Greenplum twice, one with \"test_gp.sh\" which tests with parallel grounding, the other uses non-parallel grounding. Maybe we could find a better structure for this?\n",
        "created_at": "2015-01-12T03:29:24Z",
        "updated_at": "2015-01-12T20:19:07Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/199#discussion_r22774116",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/199",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/22774116"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/199#discussion_r22774116"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/199"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/22818313",
        "pull_request_review_id": null,
        "id": 22818313,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyODE4MzEz",
        "diff_hunk": "@@ -1,20 +1,19 @@\n set -e\n \n+# if greenplum supported (gpfdist exist), then test GP\n+if hash gpfdist 2>/dev/null; then\n+  echo \"Testing Greenplum...\"\n+  bash test/test_gp.sh $1\n # if command psql exist, then test psql\n-if hash psql 2>/dev/null; then\n+elif hash psql 2>/dev/null; then",
        "path": "test.sh",
        "position": null,
        "original_position": 9,
        "commit_id": "bc2f68b4e15e36bf192604cd6f979f5598a2a380",
        "original_commit_id": "4e54f1f6289c0d512c74205a08c56ebfac6047ca",
        "user": {
            "login": "rionda",
            "id": 1178354,
            "node_id": "MDQ6VXNlcjExNzgzNTQ=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/1178354?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rionda",
            "html_url": "https://github.com/rionda",
            "followers_url": "https://api.github.com/users/rionda/followers",
            "following_url": "https://api.github.com/users/rionda/following{/other_user}",
            "gists_url": "https://api.github.com/users/rionda/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rionda/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rionda/subscriptions",
            "organizations_url": "https://api.github.com/users/rionda/orgs",
            "repos_url": "https://api.github.com/users/rionda/repos",
            "events_url": "https://api.github.com/users/rionda/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rionda/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "@zifeishan : I see. I didn't get that. Ok, I'm going to revert it back to the original, but I'll add a comment explaining what you told me. \n",
        "created_at": "2015-01-12T20:14:34Z",
        "updated_at": "2015-01-12T20:19:07Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/199#discussion_r22818313",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/199",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/22818313"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/199#discussion_r22818313"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/199"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/23665382",
        "pull_request_review_id": null,
        "id": 23665382,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzNjY1Mzgy",
        "diff_hunk": "@@ -640,10 +569,42 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n         FROM ${relation} t0, ${relation}_vtype t1\n         WHERE t0.id=t1.id\n         \"\"\")\n+    }\n+  }\n \n+  // assign holdout\n+  def assignHoldout(calibrationSettings: CalibrationSettings) {\n+    // variable holdout table - if user defined, execute once\n+    execute(s\"\"\"DROP TABLE IF EXISTS ${VariablesHoldoutTable} CASCADE;\n+      CREATE TABLE ${VariablesHoldoutTable}(variable_id bigint primary key);\n+      \"\"\")\n+    calibrationSettings.holdoutQuery match {\n+      case Some(query) => execute(query)\n+      case None =>\n     }\n \n-    // generate factor meta data\n+    // variable observation table\n+    execute(s\"\"\"DROP TABLE IF EXISTS ${VariablesObservationTable} CASCADE;\n+      CREATE TABLE ${VariablesObservationTable}(variable_id bigint primary key);\n+      \"\"\")\n+    calibrationSettings.observationQuery match {\n+      case Some(query) => execute(query)\n+      case None =>\n+    }\n+  }\n+\n+  // check whether greenplum is used\n+  def isUsingGreenplum() : Boolean = {",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 123,
        "original_position": 123,
        "commit_id": "a422452a5003ca51c2dc9df274c57455f86505a3",
        "original_commit_id": "a422452a5003ca51c2dc9df274c57455f86505a3",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Is it better to move this to Helper.scala? @feiranwang \n\nNot sure how hard it is. To do this, maybe you need to use the issueQuery equivalence in datastore. Maybe pass in a JdbcDataStore and use [this function](https://github.com/HazyResearch/deepdive/blob/master/src/main/scala/org/deepdive/datastore/JdbcDataStore.scala#L46)?\n",
        "created_at": "2015-01-28T04:37:34Z",
        "updated_at": "2015-01-28T04:42:54Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/205#discussion_r23665382",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/205",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/23665382"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/205#discussion_r23665382"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/205"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/23750430",
        "pull_request_review_id": null,
        "id": 23750430,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzNzUwNDMw",
        "diff_hunk": "@@ -640,10 +569,42 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n         FROM ${relation} t0, ${relation}_vtype t1\n         WHERE t0.id=t1.id\n         \"\"\")\n+    }\n+  }\n \n+  // assign holdout\n+  def assignHoldout(calibrationSettings: CalibrationSettings) {\n+    // variable holdout table - if user defined, execute once\n+    execute(s\"\"\"DROP TABLE IF EXISTS ${VariablesHoldoutTable} CASCADE;\n+      CREATE TABLE ${VariablesHoldoutTable}(variable_id bigint primary key);\n+      \"\"\")\n+    calibrationSettings.holdoutQuery match {\n+      case Some(query) => execute(query)\n+      case None =>\n     }\n \n-    // generate factor meta data\n+    // variable observation table\n+    execute(s\"\"\"DROP TABLE IF EXISTS ${VariablesObservationTable} CASCADE;\n+      CREATE TABLE ${VariablesObservationTable}(variable_id bigint primary key);\n+      \"\"\")\n+    calibrationSettings.observationQuery match {\n+      case Some(query) => execute(query)\n+      case None =>\n+    }\n+  }\n+\n+  // check whether greenplum is used\n+  def isUsingGreenplum() : Boolean = {",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 123,
        "original_position": 123,
        "commit_id": "a422452a5003ca51c2dc9df274c57455f86505a3",
        "original_commit_id": "a422452a5003ca51c2dc9df274c57455f86505a3",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I'll check that @zifeishan \n",
        "created_at": "2015-01-29T07:23:44Z",
        "updated_at": "2015-01-29T07:23:44Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/205#discussion_r23750430",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/205",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/23750430"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/205#discussion_r23750430"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/205"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24463855",
        "pull_request_review_id": null,
        "id": 24463855,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI0NDYzODU1",
        "diff_hunk": "@@ -26,8 +26,10 @@ class DataLoader extends JdbcDataStore with Logging {\n    * @param dbSettings: database settings (DD's class)\n    * @param usingGreenplum: whether to use greenplum's gpunload\n    * @param query: the query to be dumped\n+   * @param folder: for greenplum, the relative path to gpfdist\n    */\n-  def unload(filename: String, filepath: String, dbSettings: DbSettings, usingGreenplum: Boolean, query: String) : Unit = {\n+  def unload(filename: String, filepath: String, dbSettings: DbSettings, \n+    usingGreenplum: Boolean, query: String, folder: String) : Unit = {",
        "path": "src/main/scala/org/deepdive/datastore/Dataloader.scala",
        "position": 8,
        "original_position": 8,
        "commit_id": "33a697f46bcc90e003cbbfb2a2343cf0a12a4042",
        "original_commit_id": "0baece37f3eb6ce63d7b0a1f2548a86ba25dd97f",
        "user": {
            "login": "mikecafarella",
            "id": 425838,
            "node_id": "MDQ6VXNlcjQyNTgzOA==",
            "avatar_url": "https://avatars3.githubusercontent.com/u/425838?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mikecafarella",
            "html_url": "https://github.com/mikecafarella",
            "followers_url": "https://api.github.com/users/mikecafarella/followers",
            "following_url": "https://api.github.com/users/mikecafarella/following{/other_user}",
            "gists_url": "https://api.github.com/users/mikecafarella/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mikecafarella/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mikecafarella/subscriptions",
            "organizations_url": "https://api.github.com/users/mikecafarella/orgs",
            "repos_url": "https://api.github.com/users/mikecafarella/repos",
            "events_url": "https://api.github.com/users/mikecafarella/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mikecafarella/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Is the intention to make \"folder\" a single-level directory, or can this be an arbitrary subdirectory structure?\n\nThere's no sanity-checking of 'folder' here.  What if the user screws up and indicates it should contain a semicolon or quote?\n",
        "created_at": "2015-02-11T00:01:38Z",
        "updated_at": "2015-02-11T21:49:26Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24463855",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24463855"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24463855"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24464259",
        "pull_request_review_id": null,
        "id": 24464259,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI0NDY0MjU5",
        "diff_hunk": "@@ -26,8 +26,10 @@ class DataLoader extends JdbcDataStore with Logging {\n    * @param dbSettings: database settings (DD's class)\n    * @param usingGreenplum: whether to use greenplum's gpunload\n    * @param query: the query to be dumped\n+   * @param folder: for greenplum, the relative path to gpfdist\n    */\n-  def unload(filename: String, filepath: String, dbSettings: DbSettings, usingGreenplum: Boolean, query: String) : Unit = {\n+  def unload(filename: String, filepath: String, dbSettings: DbSettings, \n+    usingGreenplum: Boolean, query: String, folder: String) : Unit = {",
        "path": "src/main/scala/org/deepdive/datastore/Dataloader.scala",
        "position": 8,
        "original_position": 8,
        "commit_id": "33a697f46bcc90e003cbbfb2a2343cf0a12a4042",
        "original_commit_id": "0baece37f3eb6ce63d7b0a1f2548a86ba25dd97f",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "It can be an arbitrary subdirectory structure. The folder argument is managed by DeepDive, and won't be exposed to users. If \"folder\" is invalid, \"executeSqlQueries\" will throw an exception when trying to insert into that table.\n",
        "created_at": "2015-02-11T00:09:11Z",
        "updated_at": "2015-02-11T21:49:26Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24464259",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24464259"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24464259"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24464350",
        "pull_request_review_id": null,
        "id": 24464350,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI0NDY0MzUw",
        "diff_hunk": "@@ -169,23 +169,6 @@ trait MysqlInferenceDataStoreComponent extends SQLInferenceDataStoreComponent {\n         ORDER BY b1.bucket ASC;\n       \"\"\"\n \n-    override def WRONGcreateCalibrationViewRealNumberSQL(name: String, bucketedView: String, columnName: String) = s\"\"\"",
        "path": "src/main/scala/org/deepdive/inference/datastore/MysqlInferenceDataStore.scala",
        "position": 4,
        "original_position": 4,
        "commit_id": "33a697f46bcc90e003cbbfb2a2343cf0a12a4042",
        "original_commit_id": "0baece37f3eb6ce63d7b0a1f2548a86ba25dd97f",
        "user": {
            "login": "mikecafarella",
            "id": 425838,
            "node_id": "MDQ6VXNlcjQyNTgzOA==",
            "avatar_url": "https://avatars3.githubusercontent.com/u/425838?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mikecafarella",
            "html_url": "https://github.com/mikecafarella",
            "followers_url": "https://api.github.com/users/mikecafarella/followers",
            "following_url": "https://api.github.com/users/mikecafarella/following{/other_user}",
            "gists_url": "https://api.github.com/users/mikecafarella/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mikecafarella/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mikecafarella/subscriptions",
            "organizations_url": "https://api.github.com/users/mikecafarella/orgs",
            "repos_url": "https://api.github.com/users/mikecafarella/repos",
            "events_url": "https://api.github.com/users/mikecafarella/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mikecafarella/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I fully support removing methods that are prefixed with WRONG.\n",
        "created_at": "2015-02-11T00:10:52Z",
        "updated_at": "2015-02-11T21:49:26Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24464350",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24464350"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24464350"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24464557",
        "pull_request_review_id": null,
        "id": 24464557,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI0NDY0NTU3",
        "diff_hunk": "@@ -104,5 +104,95 @@ trait PostgresInferenceDataStoreComponent extends SQLInferenceDataStoreComponent\n       // do nothing\n     }\n \n+    // assign senquential ids to table's id column\n+    def assignIds(table: String, startId: Long, sequence: String) : Long = {\n+      if (ds.isUsingGreenplum()) {\n+        executeQuery(s\"SELECT fast_seqassign('${table.toLowerCase()}', ${startId});\");\n+      } else {\n+        execute(s\"UPDATE ${table} SET id = ${nextVal(sequence)};\")\n+      }\n+      var count : Long = 0\n+      issueQuery(s\"\"\"SELECT COUNT(*) FROM ${table};\"\"\") { rs =>\n+        count = rs.getLong(1)\n+      }\n+      return count\n+    }\n+    \n+    // create fast sequence assign function for greenplum\n+    def createAssignIdFunctionGreenplum() : Unit = {\n+      if (!ds.isUsingGreenplum()) return\n+\n+      val sql = \"\"\"",
        "path": "src/main/scala/org/deepdive/inference/datastore/PostgresInferenceDataStore.scala",
        "position": null,
        "original_position": 22,
        "commit_id": "33a697f46bcc90e003cbbfb2a2343cf0a12a4042",
        "original_commit_id": "0baece37f3eb6ce63d7b0a1f2548a86ba25dd97f",
        "user": {
            "login": "mikecafarella",
            "id": 425838,
            "node_id": "MDQ6VXNlcjQyNTgzOA==",
            "avatar_url": "https://avatars3.githubusercontent.com/u/425838?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mikecafarella",
            "html_url": "https://github.com/mikecafarella",
            "followers_url": "https://api.github.com/users/mikecafarella/followers",
            "following_url": "https://api.github.com/users/mikecafarella/following{/other_user}",
            "gists_url": "https://api.github.com/users/mikecafarella/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mikecafarella/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mikecafarella/subscriptions",
            "organizations_url": "https://api.github.com/users/mikecafarella/orgs",
            "repos_url": "https://api.github.com/users/mikecafarella/repos",
            "events_url": "https://api.github.com/users/mikecafarella/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mikecafarella/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "This kind of construction makes me extremely nervous.  It's basically a huge amount of code that's entirely opaque to the build system; what if there's a stray keystroke here?  We won't know until something fails, weirdly.  \n\nCan we move these function definitions into their own Scala wrapper objects, with corresponding isolated test code?\n",
        "created_at": "2015-02-11T00:14:54Z",
        "updated_at": "2015-02-11T21:49:26Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24464557",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24464557"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24464557"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24464658",
        "pull_request_review_id": null,
        "id": 24464658,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI0NDY0NjU4",
        "diff_hunk": "@@ -206,8 +156,7 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n     DROP TABLE IF EXISTS ${lastWeightsTable} CASCADE;\n     CREATE TABLE ${lastWeightsTable} AS\n       SELECT X.*, Y.weight\n-      FROM ${WeightsTable} AS X INNER JOIN ${WeightResultTable} AS Y ON X.id = Y.id\n-      ORDER BY id ASC;",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 148,
        "original_position": 141,
        "commit_id": "33a697f46bcc90e003cbbfb2a2343cf0a12a4042",
        "original_commit_id": "0baece37f3eb6ce63d7b0a1f2548a86ba25dd97f",
        "user": {
            "login": "mikecafarella",
            "id": 425838,
            "node_id": "MDQ6VXNlcjQyNTgzOA==",
            "avatar_url": "https://avatars3.githubusercontent.com/u/425838?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mikecafarella",
            "html_url": "https://github.com/mikecafarella",
            "followers_url": "https://api.github.com/users/mikecafarella/followers",
            "following_url": "https://api.github.com/users/mikecafarella/following{/other_user}",
            "gists_url": "https://api.github.com/users/mikecafarella/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mikecafarella/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mikecafarella/subscriptions",
            "organizations_url": "https://api.github.com/users/mikecafarella/orgs",
            "repos_url": "https://api.github.com/users/mikecafarella/repos",
            "events_url": "https://api.github.com/users/mikecafarella/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mikecafarella/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Why remove the ordering?  Was this an unnecessary element of the query?\n",
        "created_at": "2015-02-11T00:16:54Z",
        "updated_at": "2015-02-11T21:49:26Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24464658",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24464658"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24464658"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24464727",
        "pull_request_review_id": null,
        "id": 24464727,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI0NDY0NzI3",
        "diff_hunk": "@@ -345,88 +278,15 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n     ORDER BY abs(weight) DESC;\n   \"\"\"\n \n-  /** \n-   *  TODO: Now this is specific for greenplum and never used elsewhere. needs refactoring. \n-   */\n-  def createAssignIdFunctionGreenplum = \n-    \"\"\"\n-    DROP LANGUAGE IF EXISTS plpgsql CASCADE;\n-    DROP LANGUAGE IF EXISTS plpythonu CASCADE;\n-    CREATE LANGUAGE plpgsql;\n-    CREATE LANGUAGE plpythonu;\n-\n-    CREATE OR REPLACE FUNCTION clear_count_1(sid int) RETURNS int AS \n-    $$\n-    if '__count_1' in SD:\n-      SD['__count_1'] = -1\n-      return 1\n-    return 0\n-    $$ LANGUAGE plpythonu;\n-     \n-     \n-    CREATE OR REPLACE FUNCTION updateid(startid bigint, sid int, sids int[], base_ids bigint[], base_ids_noagg bigint[]) RETURNS bigint AS \n-    $$\n-    if '__count_1' in SD:\n-      a = SD['__count_2']\n-      b = SD['__count_1']\n-      SD['__count_2'] = SD['__count_2'] - 1\n-      if SD['__count_2'] < 0:\n-        SD.pop('__count_1')\n-      return startid+b-a\n-    else:\n-      for i in range(0, len(sids)):\n-        if sids[i] == sid:\n-          SD['__count_1'] = base_ids[i] - 1\n-          SD['__count_2'] = base_ids_noagg[i] - 1\n-      a = SD['__count_2']\n-      b = SD['__count_1']\n-      SD['__count_2'] = SD['__count_2'] - 1\n-      if SD['__count_2'] < 0:\n-        SD.pop('__count_1')\n-      return startid+b-a\n-      \n-    $$ LANGUAGE plpythonu;\n-     \n-    CREATE OR REPLACE FUNCTION fast_seqassign(tname character varying, startid bigint) RETURNS TEXT AS \n-    $$\n-    BEGIN\n-      EXECUTE 'drop table if exists tmp_gpsid_count cascade;';\n-      EXECUTE 'drop table if exists tmp_gpsid_count_noagg cascade;';\n-      EXECUTE 'create table tmp_gpsid_count as select gp_segment_id as sid, count(clear_count_1(gp_segment_id)) as base_id from ' || quote_ident(tname) || ' group by gp_segment_id order by sid distributed by (sid);';\n-      EXECUTE 'create table tmp_gpsid_count_noagg as select * from tmp_gpsid_count distributed by (sid);';\n-      EXECUTE 'update tmp_gpsid_count as t set base_id = (SELECT SUM(base_id) FROM tmp_gpsid_count as t2 WHERE t2.sid <= t.sid);';\n-      RAISE NOTICE 'EXECUTING _fast_seqassign()...';\n-      EXECUTE 'select * from _fast_seqassign(''' || quote_ident(tname) || ''', ' || startid || ');';\n-      RETURN '';\n-    END;\n-    $$ LANGUAGE 'plpgsql';\n-     \n-    CREATE OR REPLACE FUNCTION _fast_seqassign(tname character varying, startid bigint)\n-    RETURNS TEXT AS\n-    $$\n-    DECLARE\n-      sids int[] :=  ARRAY(SELECT sid FROM tmp_gpsid_count ORDER BY sid);\n-      base_ids bigint[] :=  ARRAY(SELECT base_id FROM tmp_gpsid_count ORDER BY sid);\n-      base_ids_noagg bigint[] :=  ARRAY(SELECT base_id FROM tmp_gpsid_count_noagg ORDER BY sid);\n-      tsids text;\n-      tbase_ids text;\n-      tbase_ids_noagg text;\n-    BEGIN\n-      SELECT INTO tsids array_to_string(sids, ',');\n-      SELECT INTO tbase_ids array_to_string(base_ids, ',');\n-      SELECT INTO tbase_ids_noagg array_to_string(base_ids_noagg, ',');\n-      if ('update ' || tname || ' set id = updateid(' || startid || ', gp_segment_id, ARRAY[' || tsids || '], ARRAY[' || tbase_ids || '], ARRAY[' || tbase_ids_noagg || ']);')::text is not null then\n-        EXECUTE 'update ' || tname || ' set id = updateid(' || startid || ', gp_segment_id, ARRAY[' || tsids || '], ARRAY[' || tbase_ids || '], ARRAY[' || tbase_ids_noagg || ']);';\n-      end if;\n-      RETURN '';\n-    END;\n-    $$\n-    LANGUAGE 'plpgsql';\n-    \"\"\"\n-\n   def init() : Unit = {\n   }\n \n+  // given a path, get file/folder name\n+  // e.g., /some/path/to/folder -> folder\n+  def getFileNameFromPath(path: String) : String = {",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 264,
        "original_position": 257,
        "commit_id": "33a697f46bcc90e003cbbfb2a2343cf0a12a4042",
        "original_commit_id": "0baece37f3eb6ce63d7b0a1f2548a86ba25dd97f",
        "user": {
            "login": "mikecafarella",
            "id": 425838,
            "node_id": "MDQ6VXNlcjQyNTgzOA==",
            "avatar_url": "https://avatars3.githubusercontent.com/u/425838?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mikecafarella",
            "html_url": "https://github.com/mikecafarella",
            "followers_url": "https://api.github.com/users/mikecafarella/followers",
            "following_url": "https://api.github.com/users/mikecafarella/following{/other_user}",
            "gists_url": "https://api.github.com/users/mikecafarella/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mikecafarella/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mikecafarella/subscriptions",
            "organizations_url": "https://api.github.com/users/mikecafarella/orgs",
            "repos_url": "https://api.github.com/users/mikecafarella/repos",
            "events_url": "https://api.github.com/users/mikecafarella/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mikecafarella/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "How is calling \"getFileNameFromPath(p)\" better than \"new File(p).getName()\"?  Just to be all Scala-like?\n",
        "created_at": "2015-02-11T00:18:10Z",
        "updated_at": "2015-02-11T21:49:26Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24464727",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24464727"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24464727"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24464734",
        "pull_request_review_id": null,
        "id": 24464734,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI0NDY0NzM0",
        "diff_hunk": "@@ -104,5 +104,95 @@ trait PostgresInferenceDataStoreComponent extends SQLInferenceDataStoreComponent\n       // do nothing\n     }\n \n+    // assign senquential ids to table's id column\n+    def assignIds(table: String, startId: Long, sequence: String) : Long = {\n+      if (ds.isUsingGreenplum()) {\n+        executeQuery(s\"SELECT fast_seqassign('${table.toLowerCase()}', ${startId});\");\n+      } else {\n+        execute(s\"UPDATE ${table} SET id = ${nextVal(sequence)};\")\n+      }\n+      var count : Long = 0\n+      issueQuery(s\"\"\"SELECT COUNT(*) FROM ${table};\"\"\") { rs =>\n+        count = rs.getLong(1)\n+      }\n+      return count\n+    }\n+    \n+    // create fast sequence assign function for greenplum\n+    def createAssignIdFunctionGreenplum() : Unit = {\n+      if (!ds.isUsingGreenplum()) return\n+\n+      val sql = \"\"\"",
        "path": "src/main/scala/org/deepdive/inference/datastore/PostgresInferenceDataStore.scala",
        "position": null,
        "original_position": 22,
        "commit_id": "33a697f46bcc90e003cbbfb2a2343cf0a12a4042",
        "original_commit_id": "0baece37f3eb6ce63d7b0a1f2548a86ba25dd97f",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Sure. Will do.\n",
        "created_at": "2015-02-11T00:18:15Z",
        "updated_at": "2015-02-11T21:49:26Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24464734",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24464734"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24464734"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24464883",
        "pull_request_review_id": null,
        "id": 24464883,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI0NDY0ODgz",
        "diff_hunk": "@@ -459,173 +319,89 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n   def incrementId(table: String, IdSequence: String) {\n     execute(s\"UPDATE ${table} SET id = ${nextVal(IdSequence)};\")\n   }\n-        \n-  // clean up grounding folder (for parallel grounding)\n-  // TODO: we may not need to actually create a script for this and execute it.\n-  // We may want to directly execute the commands:\n-  // Helpers.executeCmd(s\"rm -rf ${groundingPath}/dd_tmp\")\n-  // Helpers.executeCmd(s\"rm -f ${groundingPath}/dd_*\")\n-  // XXX: This function is a little risky because groundingPath may be the empty\n-  // string. Shall we avoid letting the user shoot her own foot?\n-  def cleanParallelGroundingPath(groundingPath: String) {\n-    val cleanFile = File.createTempFile(s\"clean\", \".sh\")\n-    val writer = new PrintWriter(cleanFile)\n-    // cleaning up remaining tmp folder for tobinary\n-    writer.println(s\"rm -rf ${groundingPath}/dd_tmp\")\n-    writer.println(s\"rm -f ${groundingPath}/dd_*\")\n-    writer.close()\n-    log.info(\"Cleaning up grounding folder...\")\n-    Helpers.executeCmd(cleanFile.getAbsolutePath())\n-  }\n \n   // assign variable id - sequential and unique\n   def assignVariablesIds(schema: Map[String, _ <: VariableDataType]) {\n-    // check whether Greenplum is used\n-    var usingGreenplum = false\n-    issueQuery(checkGreenplumSQL) { rs => \n-      usingGreenplum = rs.getBoolean(1) \n-    }\n+    // fast sequential id assign function\n+    createAssignIdFunctionGreenplum()\n+    execute(createSequenceFunction(IdSequence))\n \n     var idoffset : Long = 0\n-    if (usingGreenplum) {\n-      // We use a special homemade function for Greenplum\n-      executeQuery(createAssignIdFunctionGreenplum)\n-      schema.foreach { case(variable, dataType) =>\n-        val Array(relation, column) = variable.split('.')\n-        executeQuery(s\"\"\"SELECT fast_seqassign('${relation.toLowerCase()}', ${idoffset});\"\"\")\n-        issueQuery(s\"\"\"SELECT count(*) FROM ${relation}\"\"\") { rs =>\n-          idoffset = idoffset + rs.getLong(1)\n-        }\n-      }\n-    } else {\n-      // Mysql: use user-defined variables for ID assign;\n-      // Psql: use sequence\n-      execute(createSequenceFunction(IdSequence))\n-      schema.foreach { case(variable, dataType) =>\n-        val Array(relation, column) = variable.split('.')\n-        incrementId(relation, IdSequence)\n-      }\n+    schema.foreach { case(variable, dataType) =>\n+      val Array(relation, column) = variable.split('.')\n+      idoffset += assignIds(relation, idoffset, IdSequence)\n     }\n   }\n-\n-\n-  /** Ground the factor graph to file\n-   *\n-   * Using the schema and inference rules defined in application.conf, construct factor\n-   * graph files.\n-   * Input: variable schema, factor descriptions, holdout configuration, database settings\n-   * Output: factor graph files: variables, factors, edges, weights, meta\n-   *\n-   * NOTE: for this to work in greenplum, do not put id as the first column!\n-   * The first column in greenplum is distribution key by default. \n-   * We need to update this column, but update is not allowed on distribution key. \n-   *\n-   * It is important to remember that we should not modify the user schema,\n-   * e.g., by adding columns to user relations. The right way to do it is\n-   * usually another. For example, an option could be creating a view of the\n-   * user relation, to which we add the needed column.\n-   *\n-   * It is also important to think about corner cases. For example, we cannot\n-   * assume any relation actually contains rows, or the rows are in some\n-   * predefined special order, or anything like that so the code must take care of\n-   * these cases, and there *must* be tests for the corner cases.\n-   * \n-   * TODO: This method is way too long and needs to be split, also for testing\n-   * purposes\n-   */\n-  def groundFactorGraph(schema: Map[String, _ <: VariableDataType], factorDescs: Seq[FactorDesc],\n-    calibrationSettings: CalibrationSettings, skipLearning: Boolean, weightTable: String, \n-    dbSettings: DbSettings, parallelGrounding: Boolean) {\n-\n-    val du = new DataLoader\n-    val groundingPath = if (!parallelGrounding) Context.outputDir else dbSettings.gppath\n-\n-    // check whether Greenplum is used\n-    var usingGreenplum = false\n-    issueQuery(checkGreenplumSQL) { rs => \n-      usingGreenplum = rs.getBoolean(1) \n-    }\n-    \n-    log.info(s\"Using Greenplum = ${usingGreenplum}\")\n-    log.info(s\"Datastore type = ${Helpers.getDbType(dbSettings)}\")\n-    \n-    log.info(s\"Parallel grounding = ${parallelGrounding}\")\n-    log.debug(s\"Grounding Path = ${groundingPath}\")\n-\n-    // clean up grounding folder (for parallel grounding)\n-    if (parallelGrounding) {\n-      cleanParallelGroundingPath(groundingPath)\n-    }\n-\n-    // assign variable id - sequential and unique\n-    assignVariablesIds(schema)\n-\n+  \n+  // assign variable holdout\n+  def assignHoldout(schema: Map[String, _ <: VariableDataType], calibrationSettings: CalibrationSettings) {\n     // variable holdout table - if user defined, execute once\n-    execute(s\"\"\"DROP TABLE IF EXISTS ${VariablesHoldoutTable} CASCADE;\n-      CREATE TABLE ${VariablesHoldoutTable}(variable_id bigint primary key);\n-      \"\"\")\n+    ds.dropAndCreateTable(VariablesHoldoutTable, \"variable_id bigint primary key\")\n     calibrationSettings.holdoutQuery match {\n-      case Some(query) => execute(query)\n-      case None =>\n-    }\n-\n-    // variable observation table\n-    execute(s\"\"\"DROP TABLE IF EXISTS ${VariablesObservationTable} CASCADE;\n-      CREATE TABLE ${VariablesObservationTable}(variable_id bigint primary key);\n-      \"\"\")\n-    calibrationSettings.observationQuery match {\n-      case Some(query) => execute(query)\n-      case None =>\n-    }\n-\n-    // ground variables\n-    schema.foreach { case(variable, dataType) =>\n-      val Array(relation, column) = variable.split('.')\n-\n-      val variableDataType = dataType match {\n-        case BooleanType => 0\n-        case MultinomialType(x) => 1\n-        case RealNumberType => 2\n-        case RealArrayType(x) => 3\n+      case Some(query) => {\n+        log.info(\"Executing user supplied holdout query\")\n+        execute(query)\n       }\n-\n-      val cardinality = dataType match {\n-        case BooleanType => 2\n-        case MultinomialType(x) => x.toInt\n-        case RealNumberType => 2\n-        case RealArrayType(x) => x.toInt\n-      }\n-\n-      if(variableDataType == 2 || variableDataType == 3){\n-\n-      } else {\n-        // This cannot be parsed in def randFunc for now.\n-        // assign holdout - if not user-defined, randomly select from evidence variables of each variable table\n-        calibrationSettings.holdoutQuery match {\n-          case Some(s) =>\n-          case None => execute(s\"\"\"\n+      case None => {\n+        log.info(\"There is no holdout query, will randomly generate holdout set\")\n+         // randomly assign variable holdout\n+        schema.foreach { case(variable, dataType) =>\n+          val Array(relation, column) = variable.split('.')\n+          // This cannot be parsed in def randFunc for now.\n+          // assign holdout - randomly select from evidence variables of each variable table\n+          execute(s\"\"\"\n             INSERT INTO ${VariablesHoldoutTable}\n             SELECT id FROM ${relation}\n             WHERE ${randomFunction} < ${calibrationSettings.holdoutFraction} AND ${column} IS NOT NULL;\n             \"\"\")\n         }\n       }\n+    }\n \n+    // variable observation table\n+    ds.dropAndCreateTable(VariablesObservationTable, \"variable_id bigint primary key\")\n+    calibrationSettings.observationQuery match {\n+      case Some(query) => {\n+        log.info(\"Executing user supplied observation query\")  \n+        execute(query)\n+      }\n+      case None => {\n+        log.info(\"There is no o query\")",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 457,
        "original_position": 450,
        "commit_id": "33a697f46bcc90e003cbbfb2a2343cf0a12a4042",
        "original_commit_id": "0baece37f3eb6ce63d7b0a1f2548a86ba25dd97f",
        "user": {
            "login": "mikecafarella",
            "id": 425838,
            "node_id": "MDQ6VXNlcjQyNTgzOA==",
            "avatar_url": "https://avatars3.githubusercontent.com/u/425838?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mikecafarella",
            "html_url": "https://github.com/mikecafarella",
            "followers_url": "https://api.github.com/users/mikecafarella/followers",
            "following_url": "https://api.github.com/users/mikecafarella/following{/other_user}",
            "gists_url": "https://api.github.com/users/mikecafarella/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mikecafarella/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mikecafarella/subscriptions",
            "organizations_url": "https://api.github.com/users/mikecafarella/orgs",
            "repos_url": "https://api.github.com/users/mikecafarella/repos",
            "events_url": "https://api.github.com/users/mikecafarella/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mikecafarella/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I am not entirely sure what an observation query is, so I don't know if failing to run it is catastrophic or not.  Is this for debugging, or for indicating the evidence variables?\n",
        "created_at": "2015-02-11T00:21:16Z",
        "updated_at": "2015-02-11T21:49:26Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24464883",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24464883"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24464883"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24464982",
        "pull_request_review_id": null,
        "id": 24464982,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI0NDY0OTgy",
        "diff_hunk": "@@ -459,173 +319,89 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n   def incrementId(table: String, IdSequence: String) {\n     execute(s\"UPDATE ${table} SET id = ${nextVal(IdSequence)};\")\n   }\n-        \n-  // clean up grounding folder (for parallel grounding)\n-  // TODO: we may not need to actually create a script for this and execute it.\n-  // We may want to directly execute the commands:\n-  // Helpers.executeCmd(s\"rm -rf ${groundingPath}/dd_tmp\")\n-  // Helpers.executeCmd(s\"rm -f ${groundingPath}/dd_*\")\n-  // XXX: This function is a little risky because groundingPath may be the empty\n-  // string. Shall we avoid letting the user shoot her own foot?\n-  def cleanParallelGroundingPath(groundingPath: String) {\n-    val cleanFile = File.createTempFile(s\"clean\", \".sh\")\n-    val writer = new PrintWriter(cleanFile)\n-    // cleaning up remaining tmp folder for tobinary\n-    writer.println(s\"rm -rf ${groundingPath}/dd_tmp\")\n-    writer.println(s\"rm -f ${groundingPath}/dd_*\")\n-    writer.close()\n-    log.info(\"Cleaning up grounding folder...\")\n-    Helpers.executeCmd(cleanFile.getAbsolutePath())\n-  }\n \n   // assign variable id - sequential and unique\n   def assignVariablesIds(schema: Map[String, _ <: VariableDataType]) {\n-    // check whether Greenplum is used\n-    var usingGreenplum = false\n-    issueQuery(checkGreenplumSQL) { rs => \n-      usingGreenplum = rs.getBoolean(1) \n-    }\n+    // fast sequential id assign function\n+    createAssignIdFunctionGreenplum()\n+    execute(createSequenceFunction(IdSequence))\n \n     var idoffset : Long = 0\n-    if (usingGreenplum) {\n-      // We use a special homemade function for Greenplum\n-      executeQuery(createAssignIdFunctionGreenplum)\n-      schema.foreach { case(variable, dataType) =>\n-        val Array(relation, column) = variable.split('.')\n-        executeQuery(s\"\"\"SELECT fast_seqassign('${relation.toLowerCase()}', ${idoffset});\"\"\")\n-        issueQuery(s\"\"\"SELECT count(*) FROM ${relation}\"\"\") { rs =>\n-          idoffset = idoffset + rs.getLong(1)\n-        }\n-      }\n-    } else {\n-      // Mysql: use user-defined variables for ID assign;\n-      // Psql: use sequence\n-      execute(createSequenceFunction(IdSequence))\n-      schema.foreach { case(variable, dataType) =>\n-        val Array(relation, column) = variable.split('.')\n-        incrementId(relation, IdSequence)\n-      }\n+    schema.foreach { case(variable, dataType) =>\n+      val Array(relation, column) = variable.split('.')\n+      idoffset += assignIds(relation, idoffset, IdSequence)\n     }\n   }\n-\n-\n-  /** Ground the factor graph to file\n-   *\n-   * Using the schema and inference rules defined in application.conf, construct factor\n-   * graph files.\n-   * Input: variable schema, factor descriptions, holdout configuration, database settings\n-   * Output: factor graph files: variables, factors, edges, weights, meta\n-   *\n-   * NOTE: for this to work in greenplum, do not put id as the first column!\n-   * The first column in greenplum is distribution key by default. \n-   * We need to update this column, but update is not allowed on distribution key. \n-   *\n-   * It is important to remember that we should not modify the user schema,\n-   * e.g., by adding columns to user relations. The right way to do it is\n-   * usually another. For example, an option could be creating a view of the\n-   * user relation, to which we add the needed column.\n-   *\n-   * It is also important to think about corner cases. For example, we cannot\n-   * assume any relation actually contains rows, or the rows are in some\n-   * predefined special order, or anything like that so the code must take care of\n-   * these cases, and there *must* be tests for the corner cases.\n-   * \n-   * TODO: This method is way too long and needs to be split, also for testing\n-   * purposes\n-   */\n-  def groundFactorGraph(schema: Map[String, _ <: VariableDataType], factorDescs: Seq[FactorDesc],\n-    calibrationSettings: CalibrationSettings, skipLearning: Boolean, weightTable: String, \n-    dbSettings: DbSettings, parallelGrounding: Boolean) {\n-\n-    val du = new DataLoader\n-    val groundingPath = if (!parallelGrounding) Context.outputDir else dbSettings.gppath\n-\n-    // check whether Greenplum is used\n-    var usingGreenplum = false\n-    issueQuery(checkGreenplumSQL) { rs => \n-      usingGreenplum = rs.getBoolean(1) \n-    }\n-    \n-    log.info(s\"Using Greenplum = ${usingGreenplum}\")\n-    log.info(s\"Datastore type = ${Helpers.getDbType(dbSettings)}\")\n-    \n-    log.info(s\"Parallel grounding = ${parallelGrounding}\")\n-    log.debug(s\"Grounding Path = ${groundingPath}\")\n-\n-    // clean up grounding folder (for parallel grounding)\n-    if (parallelGrounding) {\n-      cleanParallelGroundingPath(groundingPath)\n-    }\n-\n-    // assign variable id - sequential and unique\n-    assignVariablesIds(schema)\n-\n+  \n+  // assign variable holdout\n+  def assignHoldout(schema: Map[String, _ <: VariableDataType], calibrationSettings: CalibrationSettings) {\n     // variable holdout table - if user defined, execute once\n-    execute(s\"\"\"DROP TABLE IF EXISTS ${VariablesHoldoutTable} CASCADE;\n-      CREATE TABLE ${VariablesHoldoutTable}(variable_id bigint primary key);\n-      \"\"\")\n+    ds.dropAndCreateTable(VariablesHoldoutTable, \"variable_id bigint primary key\")\n     calibrationSettings.holdoutQuery match {\n-      case Some(query) => execute(query)\n-      case None =>\n-    }\n-\n-    // variable observation table\n-    execute(s\"\"\"DROP TABLE IF EXISTS ${VariablesObservationTable} CASCADE;\n-      CREATE TABLE ${VariablesObservationTable}(variable_id bigint primary key);\n-      \"\"\")\n-    calibrationSettings.observationQuery match {\n-      case Some(query) => execute(query)\n-      case None =>\n-    }\n-\n-    // ground variables\n-    schema.foreach { case(variable, dataType) =>\n-      val Array(relation, column) = variable.split('.')\n-\n-      val variableDataType = dataType match {\n-        case BooleanType => 0\n-        case MultinomialType(x) => 1\n-        case RealNumberType => 2\n-        case RealArrayType(x) => 3\n+      case Some(query) => {\n+        log.info(\"Executing user supplied holdout query\")\n+        execute(query)\n       }\n-\n-      val cardinality = dataType match {\n-        case BooleanType => 2\n-        case MultinomialType(x) => x.toInt\n-        case RealNumberType => 2\n-        case RealArrayType(x) => x.toInt\n-      }\n-\n-      if(variableDataType == 2 || variableDataType == 3){\n-\n-      } else {\n-        // This cannot be parsed in def randFunc for now.\n-        // assign holdout - if not user-defined, randomly select from evidence variables of each variable table\n-        calibrationSettings.holdoutQuery match {\n-          case Some(s) =>\n-          case None => execute(s\"\"\"\n+      case None => {\n+        log.info(\"There is no holdout query, will randomly generate holdout set\")\n+         // randomly assign variable holdout\n+        schema.foreach { case(variable, dataType) =>\n+          val Array(relation, column) = variable.split('.')\n+          // This cannot be parsed in def randFunc for now.\n+          // assign holdout - randomly select from evidence variables of each variable table\n+          execute(s\"\"\"\n             INSERT INTO ${VariablesHoldoutTable}\n             SELECT id FROM ${relation}\n             WHERE ${randomFunction} < ${calibrationSettings.holdoutFraction} AND ${column} IS NOT NULL;\n             \"\"\")\n         }\n       }\n+    }\n \n+    // variable observation table\n+    ds.dropAndCreateTable(VariablesObservationTable, \"variable_id bigint primary key\")\n+    calibrationSettings.observationQuery match {\n+      case Some(query) => {\n+        log.info(\"Executing user supplied observation query\")  \n+        execute(query)\n+      }\n+      case None => {\n+        log.info(\"There is no o query\")\n+      }\n+    }\n \n-      // Create a cardinality table for the variable\n+  }\n+  \n+  // generate cardinality tables for variables in the schema\n+  // cardinality tables is used to indicate the domains of the variables\n+  def generateCardinalityTables(schema: Map[String, _ <: VariableDataType]) {\n+    schema.foreach { case(variable, dataType) =>\n+      val Array(relation, column) = variable.split('.')\n       // note we use five-digit fixed-length representation here\n       val cardinalityValues = dataType match {\n         case BooleanType => \"('00001')\"\n         case MultinomialType(x) => (0 to x-1).map (n => s\"\"\"('${\"%05d\".format(n)}')\"\"\").mkString(\", \")\n-        case RealNumberType => \"('00001')\"\n-        case RealArrayType(x) => \"('00001')\"\n       }\n-      val cardinalityTableName = s\"${relation}_${column}_cardinality\"\n+      val cardinalityTableName = InferenceNamespace.getCardinalityTableName(relation, column)\n+      ds.dropAndCreateTable(cardinalityTableName, \"cardinality text\")\n       execute(s\"\"\"\n-        DROP TABLE IF EXISTS ${cardinalityTableName} CASCADE;\n-        CREATE TABLE ${cardinalityTableName}(cardinality text);\n         INSERT INTO ${cardinalityTableName} VALUES ${cardinalityValues};",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 482,
        "original_position": 475,
        "commit_id": "33a697f46bcc90e003cbbfb2a2343cf0a12a4042",
        "original_commit_id": "0baece37f3eb6ce63d7b0a1f2548a86ba25dd97f",
        "user": {
            "login": "mikecafarella",
            "id": 425838,
            "node_id": "MDQ6VXNlcjQyNTgzOA==",
            "avatar_url": "https://avatars3.githubusercontent.com/u/425838?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mikecafarella",
            "html_url": "https://github.com/mikecafarella",
            "followers_url": "https://api.github.com/users/mikecafarella/followers",
            "following_url": "https://api.github.com/users/mikecafarella/following{/other_user}",
            "gists_url": "https://api.github.com/users/mikecafarella/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mikecafarella/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mikecafarella/subscriptions",
            "organizations_url": "https://api.github.com/users/mikecafarella/orgs",
            "repos_url": "https://api.github.com/users/mikecafarella/repos",
            "events_url": "https://api.github.com/users/mikecafarella/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mikecafarella/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "We might want to promulgate a new coding standard: no direct SQL queries without a function wrapper, into which parameters are passed.  The goal in doing that would be to enforce type checking by the compiler.  As it stands, if there's a type clash you don't find out until you try to run the query.  Not a huge deal for this commit, but probably a good idea in the future.\n",
        "created_at": "2015-02-11T00:23:47Z",
        "updated_at": "2015-02-11T21:49:26Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24464982",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24464982"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24464982"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24465061",
        "pull_request_review_id": null,
        "id": 24465061,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI0NDY1MDYx",
        "diff_hunk": "@@ -459,173 +319,89 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n   def incrementId(table: String, IdSequence: String) {\n     execute(s\"UPDATE ${table} SET id = ${nextVal(IdSequence)};\")\n   }\n-        \n-  // clean up grounding folder (for parallel grounding)\n-  // TODO: we may not need to actually create a script for this and execute it.\n-  // We may want to directly execute the commands:\n-  // Helpers.executeCmd(s\"rm -rf ${groundingPath}/dd_tmp\")\n-  // Helpers.executeCmd(s\"rm -f ${groundingPath}/dd_*\")\n-  // XXX: This function is a little risky because groundingPath may be the empty\n-  // string. Shall we avoid letting the user shoot her own foot?\n-  def cleanParallelGroundingPath(groundingPath: String) {\n-    val cleanFile = File.createTempFile(s\"clean\", \".sh\")\n-    val writer = new PrintWriter(cleanFile)\n-    // cleaning up remaining tmp folder for tobinary\n-    writer.println(s\"rm -rf ${groundingPath}/dd_tmp\")\n-    writer.println(s\"rm -f ${groundingPath}/dd_*\")\n-    writer.close()\n-    log.info(\"Cleaning up grounding folder...\")\n-    Helpers.executeCmd(cleanFile.getAbsolutePath())\n-  }\n \n   // assign variable id - sequential and unique\n   def assignVariablesIds(schema: Map[String, _ <: VariableDataType]) {\n-    // check whether Greenplum is used\n-    var usingGreenplum = false\n-    issueQuery(checkGreenplumSQL) { rs => \n-      usingGreenplum = rs.getBoolean(1) \n-    }\n+    // fast sequential id assign function\n+    createAssignIdFunctionGreenplum()\n+    execute(createSequenceFunction(IdSequence))\n \n     var idoffset : Long = 0\n-    if (usingGreenplum) {\n-      // We use a special homemade function for Greenplum\n-      executeQuery(createAssignIdFunctionGreenplum)\n-      schema.foreach { case(variable, dataType) =>\n-        val Array(relation, column) = variable.split('.')\n-        executeQuery(s\"\"\"SELECT fast_seqassign('${relation.toLowerCase()}', ${idoffset});\"\"\")\n-        issueQuery(s\"\"\"SELECT count(*) FROM ${relation}\"\"\") { rs =>\n-          idoffset = idoffset + rs.getLong(1)\n-        }\n-      }\n-    } else {\n-      // Mysql: use user-defined variables for ID assign;\n-      // Psql: use sequence\n-      execute(createSequenceFunction(IdSequence))\n-      schema.foreach { case(variable, dataType) =>\n-        val Array(relation, column) = variable.split('.')\n-        incrementId(relation, IdSequence)\n-      }\n+    schema.foreach { case(variable, dataType) =>\n+      val Array(relation, column) = variable.split('.')\n+      idoffset += assignIds(relation, idoffset, IdSequence)\n     }\n   }\n-\n-\n-  /** Ground the factor graph to file\n-   *\n-   * Using the schema and inference rules defined in application.conf, construct factor\n-   * graph files.\n-   * Input: variable schema, factor descriptions, holdout configuration, database settings\n-   * Output: factor graph files: variables, factors, edges, weights, meta\n-   *\n-   * NOTE: for this to work in greenplum, do not put id as the first column!\n-   * The first column in greenplum is distribution key by default. \n-   * We need to update this column, but update is not allowed on distribution key. \n-   *\n-   * It is important to remember that we should not modify the user schema,\n-   * e.g., by adding columns to user relations. The right way to do it is\n-   * usually another. For example, an option could be creating a view of the\n-   * user relation, to which we add the needed column.\n-   *\n-   * It is also important to think about corner cases. For example, we cannot\n-   * assume any relation actually contains rows, or the rows are in some\n-   * predefined special order, or anything like that so the code must take care of\n-   * these cases, and there *must* be tests for the corner cases.\n-   * \n-   * TODO: This method is way too long and needs to be split, also for testing\n-   * purposes\n-   */\n-  def groundFactorGraph(schema: Map[String, _ <: VariableDataType], factorDescs: Seq[FactorDesc],\n-    calibrationSettings: CalibrationSettings, skipLearning: Boolean, weightTable: String, \n-    dbSettings: DbSettings, parallelGrounding: Boolean) {\n-\n-    val du = new DataLoader\n-    val groundingPath = if (!parallelGrounding) Context.outputDir else dbSettings.gppath\n-\n-    // check whether Greenplum is used\n-    var usingGreenplum = false\n-    issueQuery(checkGreenplumSQL) { rs => \n-      usingGreenplum = rs.getBoolean(1) \n-    }\n-    \n-    log.info(s\"Using Greenplum = ${usingGreenplum}\")\n-    log.info(s\"Datastore type = ${Helpers.getDbType(dbSettings)}\")\n-    \n-    log.info(s\"Parallel grounding = ${parallelGrounding}\")\n-    log.debug(s\"Grounding Path = ${groundingPath}\")\n-\n-    // clean up grounding folder (for parallel grounding)\n-    if (parallelGrounding) {\n-      cleanParallelGroundingPath(groundingPath)\n-    }\n-\n-    // assign variable id - sequential and unique\n-    assignVariablesIds(schema)\n-\n+  \n+  // assign variable holdout\n+  def assignHoldout(schema: Map[String, _ <: VariableDataType], calibrationSettings: CalibrationSettings) {\n     // variable holdout table - if user defined, execute once\n-    execute(s\"\"\"DROP TABLE IF EXISTS ${VariablesHoldoutTable} CASCADE;\n-      CREATE TABLE ${VariablesHoldoutTable}(variable_id bigint primary key);\n-      \"\"\")\n+    ds.dropAndCreateTable(VariablesHoldoutTable, \"variable_id bigint primary key\")\n     calibrationSettings.holdoutQuery match {\n-      case Some(query) => execute(query)\n-      case None =>\n-    }\n-\n-    // variable observation table\n-    execute(s\"\"\"DROP TABLE IF EXISTS ${VariablesObservationTable} CASCADE;\n-      CREATE TABLE ${VariablesObservationTable}(variable_id bigint primary key);\n-      \"\"\")\n-    calibrationSettings.observationQuery match {\n-      case Some(query) => execute(query)\n-      case None =>\n-    }\n-\n-    // ground variables\n-    schema.foreach { case(variable, dataType) =>\n-      val Array(relation, column) = variable.split('.')\n-\n-      val variableDataType = dataType match {\n-        case BooleanType => 0\n-        case MultinomialType(x) => 1\n-        case RealNumberType => 2\n-        case RealArrayType(x) => 3\n+      case Some(query) => {\n+        log.info(\"Executing user supplied holdout query\")\n+        execute(query)\n       }\n-\n-      val cardinality = dataType match {\n-        case BooleanType => 2\n-        case MultinomialType(x) => x.toInt\n-        case RealNumberType => 2\n-        case RealArrayType(x) => x.toInt\n-      }\n-\n-      if(variableDataType == 2 || variableDataType == 3){\n-\n-      } else {\n-        // This cannot be parsed in def randFunc for now.\n-        // assign holdout - if not user-defined, randomly select from evidence variables of each variable table\n-        calibrationSettings.holdoutQuery match {\n-          case Some(s) =>\n-          case None => execute(s\"\"\"\n+      case None => {\n+        log.info(\"There is no holdout query, will randomly generate holdout set\")\n+         // randomly assign variable holdout\n+        schema.foreach { case(variable, dataType) =>\n+          val Array(relation, column) = variable.split('.')\n+          // This cannot be parsed in def randFunc for now.\n+          // assign holdout - randomly select from evidence variables of each variable table\n+          execute(s\"\"\"\n             INSERT INTO ${VariablesHoldoutTable}\n             SELECT id FROM ${relation}\n             WHERE ${randomFunction} < ${calibrationSettings.holdoutFraction} AND ${column} IS NOT NULL;\n             \"\"\")\n         }\n       }\n+    }\n \n+    // variable observation table\n+    ds.dropAndCreateTable(VariablesObservationTable, \"variable_id bigint primary key\")\n+    calibrationSettings.observationQuery match {\n+      case Some(query) => {\n+        log.info(\"Executing user supplied observation query\")  \n+        execute(query)\n+      }\n+      case None => {\n+        log.info(\"There is no o query\")\n+      }\n+    }\n \n-      // Create a cardinality table for the variable\n+  }\n+  \n+  // generate cardinality tables for variables in the schema\n+  // cardinality tables is used to indicate the domains of the variables\n+  def generateCardinalityTables(schema: Map[String, _ <: VariableDataType]) {\n+    schema.foreach { case(variable, dataType) =>\n+      val Array(relation, column) = variable.split('.')\n       // note we use five-digit fixed-length representation here\n       val cardinalityValues = dataType match {\n         case BooleanType => \"('00001')\"\n         case MultinomialType(x) => (0 to x-1).map (n => s\"\"\"('${\"%05d\".format(n)}')\"\"\").mkString(\", \")\n-        case RealNumberType => \"('00001')\"\n-        case RealArrayType(x) => \"('00001')\"\n       }\n-      val cardinalityTableName = s\"${relation}_${column}_cardinality\"\n+      val cardinalityTableName = InferenceNamespace.getCardinalityTableName(relation, column)\n+      ds.dropAndCreateTable(cardinalityTableName, \"cardinality text\")\n       execute(s\"\"\"\n-        DROP TABLE IF EXISTS ${cardinalityTableName} CASCADE;\n-        CREATE TABLE ${cardinalityTableName}(cardinality text);\n         INSERT INTO ${cardinalityTableName} VALUES ${cardinalityValues};\n         \"\"\")\n+    }\n+  }\n+  \n+  // ground variables\n+  def groundVariables(schema: Map[String, _ <: VariableDataType], du: DataLoader, \n+      dbSettings: DbSettings, parallelGrounding: Boolean, groundingPath: String) {\n+        schema.foreach { case(variable, dataType) =>\n+      val Array(relation, column) = variable.split('.')\n+      \n+      val variableDataType = InferenceNamespace.getVariableDataTypeId(dataType)\n+\n+      val cardinality = dataType match {\n+        case BooleanType => 2",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": null,
        "original_position": 489,
        "commit_id": "33a697f46bcc90e003cbbfb2a2343cf0a12a4042",
        "original_commit_id": "0baece37f3eb6ce63d7b0a1f2548a86ba25dd97f",
        "user": {
            "login": "mikecafarella",
            "id": 425838,
            "node_id": "MDQ6VXNlcjQyNTgzOA==",
            "avatar_url": "https://avatars3.githubusercontent.com/u/425838?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mikecafarella",
            "html_url": "https://github.com/mikecafarella",
            "followers_url": "https://api.github.com/users/mikecafarella/followers",
            "following_url": "https://api.github.com/users/mikecafarella/following{/other_user}",
            "gists_url": "https://api.github.com/users/mikecafarella/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mikecafarella/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mikecafarella/subscriptions",
            "organizations_url": "https://api.github.com/users/mikecafarella/orgs",
            "repos_url": "https://api.github.com/users/mikecafarella/repos",
            "events_url": "https://api.github.com/users/mikecafarella/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mikecafarella/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Can we use a reserved word for the BooleanType constant here?  In another part of the code, the magic number is \"0\" instead of \"2\" and it is not clear whether that's a bug or not.\n",
        "created_at": "2015-02-11T00:25:21Z",
        "updated_at": "2015-02-11T21:49:26Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24465061",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24465061"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24465061"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24465075",
        "pull_request_review_id": null,
        "id": 24465075,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI0NDY1MDc1",
        "diff_hunk": "@@ -0,0 +1,55 @@\n+package org.deepdive.inference\n+import org.deepdive.settings._\n+\n+object InferenceNamespace {\n+\n+  def WeightsTable = \"dd_graph_weights\"\n+  def lastWeightsTable = \"dd_graph_last_weights\"\n+  def FactorsTable = \"dd_graph_factors\"\n+  def VariablesTable = \"dd_graph_variables\"\n+  def VariablesMapTable = \"dd_graph_variables_map\"\n+  def WeightResultTable = \"dd_inference_result_weights\"\n+  def VariablesHoldoutTable = \"dd_graph_variables_holdout\"\n+  def VariableResultTable = \"dd_inference_result_variables\"\n+  def MappedInferenceResultView = \"dd_mapped_inference_result\"\n+  def IdSequence = \"dd_variable_sequence\"\n+  def FactorMetaTable = \"dd_graph_factormeta\"\n+  def VariablesObservationTable = \"dd_graph_variables_observation\"\n+  def LearnedWeightsTable = \"dd_inference_result_weights_mapping\"\n+  def FeatureStatsSupportTable = \"dd_feature_statistics_support\"\n+  def FeatureStatsView = \"dd_feature_statistics\"\n+  \n+  // internal tables\n+  def getWeightTableName(tableName: String) = s\"dd_weights_${tableName}\"\n+  def getQueryTableName(tableName: String) = s\"dd_query_${tableName}\"\n+  def getFactorTableName(tableName: String) = s\"dd_factors_${tableName}\"\n+  def getCardinalityTableName(relation: String, column: String) = s\"${relation}_${column}_cardinality\"\n+\n+  // files\n+  def getVariableFileName(relation: String) = s\"dd_variables_${relation}\"\n+  def getFactorFileName(name: String) = s\"dd_factors_${name}_out\"\n+  def getWeightFileName = s\"dd_weights\"\n+  def getFactorMetaFileName = s\"dd_factormeta\"\n+  def getBackupFolderName = s\"_dd_backup\"\n+\n+  // variable data type id\n+  def getVariableDataTypeId(variable: VariableDataType) : Int = {\n+    variable match {\n+      case BooleanType => 0",
        "path": "src/main/scala/org/deepdive/inference/datastore/InferenceNamespace.scala",
        "position": null,
        "original_position": 38,
        "commit_id": "33a697f46bcc90e003cbbfb2a2343cf0a12a4042",
        "original_commit_id": "0baece37f3eb6ce63d7b0a1f2548a86ba25dd97f",
        "user": {
            "login": "mikecafarella",
            "id": 425838,
            "node_id": "MDQ6VXNlcjQyNTgzOA==",
            "avatar_url": "https://avatars3.githubusercontent.com/u/425838?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mikecafarella",
            "html_url": "https://github.com/mikecafarella",
            "followers_url": "https://api.github.com/users/mikecafarella/followers",
            "following_url": "https://api.github.com/users/mikecafarella/following{/other_user}",
            "gists_url": "https://api.github.com/users/mikecafarella/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mikecafarella/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mikecafarella/subscriptions",
            "organizations_url": "https://api.github.com/users/mikecafarella/orgs",
            "repos_url": "https://api.github.com/users/mikecafarella/repos",
            "events_url": "https://api.github.com/users/mikecafarella/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mikecafarella/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "This is what I'm talking about, regarding BooleanType reserved word.  See the \"2\" comment elsewhere\n",
        "created_at": "2015-02-11T00:25:43Z",
        "updated_at": "2015-02-11T21:49:26Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24465075",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24465075"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24465075"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24465113",
        "pull_request_review_id": null,
        "id": 24465113,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI0NDY1MTEz",
        "diff_hunk": "@@ -345,88 +278,15 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n     ORDER BY abs(weight) DESC;\n   \"\"\"\n \n-  /** \n-   *  TODO: Now this is specific for greenplum and never used elsewhere. needs refactoring. \n-   */\n-  def createAssignIdFunctionGreenplum = \n-    \"\"\"\n-    DROP LANGUAGE IF EXISTS plpgsql CASCADE;\n-    DROP LANGUAGE IF EXISTS plpythonu CASCADE;\n-    CREATE LANGUAGE plpgsql;\n-    CREATE LANGUAGE plpythonu;\n-\n-    CREATE OR REPLACE FUNCTION clear_count_1(sid int) RETURNS int AS \n-    $$\n-    if '__count_1' in SD:\n-      SD['__count_1'] = -1\n-      return 1\n-    return 0\n-    $$ LANGUAGE plpythonu;\n-     \n-     \n-    CREATE OR REPLACE FUNCTION updateid(startid bigint, sid int, sids int[], base_ids bigint[], base_ids_noagg bigint[]) RETURNS bigint AS \n-    $$\n-    if '__count_1' in SD:\n-      a = SD['__count_2']\n-      b = SD['__count_1']\n-      SD['__count_2'] = SD['__count_2'] - 1\n-      if SD['__count_2'] < 0:\n-        SD.pop('__count_1')\n-      return startid+b-a\n-    else:\n-      for i in range(0, len(sids)):\n-        if sids[i] == sid:\n-          SD['__count_1'] = base_ids[i] - 1\n-          SD['__count_2'] = base_ids_noagg[i] - 1\n-      a = SD['__count_2']\n-      b = SD['__count_1']\n-      SD['__count_2'] = SD['__count_2'] - 1\n-      if SD['__count_2'] < 0:\n-        SD.pop('__count_1')\n-      return startid+b-a\n-      \n-    $$ LANGUAGE plpythonu;\n-     \n-    CREATE OR REPLACE FUNCTION fast_seqassign(tname character varying, startid bigint) RETURNS TEXT AS \n-    $$\n-    BEGIN\n-      EXECUTE 'drop table if exists tmp_gpsid_count cascade;';\n-      EXECUTE 'drop table if exists tmp_gpsid_count_noagg cascade;';\n-      EXECUTE 'create table tmp_gpsid_count as select gp_segment_id as sid, count(clear_count_1(gp_segment_id)) as base_id from ' || quote_ident(tname) || ' group by gp_segment_id order by sid distributed by (sid);';\n-      EXECUTE 'create table tmp_gpsid_count_noagg as select * from tmp_gpsid_count distributed by (sid);';\n-      EXECUTE 'update tmp_gpsid_count as t set base_id = (SELECT SUM(base_id) FROM tmp_gpsid_count as t2 WHERE t2.sid <= t.sid);';\n-      RAISE NOTICE 'EXECUTING _fast_seqassign()...';\n-      EXECUTE 'select * from _fast_seqassign(''' || quote_ident(tname) || ''', ' || startid || ');';\n-      RETURN '';\n-    END;\n-    $$ LANGUAGE 'plpgsql';\n-     \n-    CREATE OR REPLACE FUNCTION _fast_seqassign(tname character varying, startid bigint)\n-    RETURNS TEXT AS\n-    $$\n-    DECLARE\n-      sids int[] :=  ARRAY(SELECT sid FROM tmp_gpsid_count ORDER BY sid);\n-      base_ids bigint[] :=  ARRAY(SELECT base_id FROM tmp_gpsid_count ORDER BY sid);\n-      base_ids_noagg bigint[] :=  ARRAY(SELECT base_id FROM tmp_gpsid_count_noagg ORDER BY sid);\n-      tsids text;\n-      tbase_ids text;\n-      tbase_ids_noagg text;\n-    BEGIN\n-      SELECT INTO tsids array_to_string(sids, ',');\n-      SELECT INTO tbase_ids array_to_string(base_ids, ',');\n-      SELECT INTO tbase_ids_noagg array_to_string(base_ids_noagg, ',');\n-      if ('update ' || tname || ' set id = updateid(' || startid || ', gp_segment_id, ARRAY[' || tsids || '], ARRAY[' || tbase_ids || '], ARRAY[' || tbase_ids_noagg || ']);')::text is not null then\n-        EXECUTE 'update ' || tname || ' set id = updateid(' || startid || ', gp_segment_id, ARRAY[' || tsids || '], ARRAY[' || tbase_ids || '], ARRAY[' || tbase_ids_noagg || ']);';\n-      end if;\n-      RETURN '';\n-    END;\n-    $$\n-    LANGUAGE 'plpgsql';\n-    \"\"\"\n-\n   def init() : Unit = {\n   }\n \n+  // given a path, get file/folder name\n+  // e.g., /some/path/to/folder -> folder\n+  def getFileNameFromPath(path: String) : String = {",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 264,
        "original_position": 257,
        "commit_id": "33a697f46bcc90e003cbbfb2a2343cf0a12a4042",
        "original_commit_id": "0baece37f3eb6ce63d7b0a1f2548a86ba25dd97f",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I think it's the same... just to wrap it up as a helper function and give it a better name...\n",
        "created_at": "2015-02-11T00:26:29Z",
        "updated_at": "2015-02-11T21:49:26Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24465113",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24465113"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24465113"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24465197",
        "pull_request_review_id": null,
        "id": 24465197,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI0NDY1MTk3",
        "diff_hunk": "@@ -656,41 +432,102 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n       createIndexForJoinOptimization(variableTypeTable, \"id\")\n \n       // dump variables\n-      val initvalueCast = variableDataType match {\n-        case 2 | 3 => cast(column, \"float\")\n-        case _ => cast(cast(column, \"int\"), \"float\")\n-      }\n-      du.unload(s\"dd_variables_${relation}\", s\"${groundingPath}/dd_variables_${relation}\",\n+      val initvalueCast = cast(cast(column, \"int\"), \"float\")\n+      // Sen\n+      // du.unload(s\"dd_variables_${relation}\", s\"${groundingPath}/dd_variables_${relation}\",\n+      val groundingDir = getFileNameFromPath(groundingPath)\n+      du.unload(InferenceNamespace.getVariableFileName(relation),\n+        s\"${groundingPath}/${InferenceNamespace.getVariableFileName(relation)}\",\n         dbSettings, parallelGrounding,\n         s\"\"\"SELECT t0.id, t1.${variableTypeColumn},\n         CASE WHEN t1.${variableTypeColumn} = 0 THEN 0 ELSE ${initvalueCast} END AS initvalue,\n         ${variableDataType} AS type, ${cardinality} AS cardinality\n         FROM ${relation} t0, ${relation}_vtype t1\n         WHERE t0.id=t1.id\n-        \"\"\")\n-\n+        \"\"\", groundingDir)\n     }\n+  }\n \n-    // generate factor meta data\n-    execute(s\"\"\"DROP TABLE IF EXISTS ${FactorMetaTable} CASCADE;\n-      CREATE TABLE ${FactorMetaTable} (name text, funcid int, sign text);\n-      \"\"\")\n+  // ground factor meta data\n+  def groundFactorMeta(du: DataLoader, factorDescs: Seq[FactorDesc], groundingPath: String, \n+    parallelGrounding: Boolean) {\n+    ds.dropAndCreateTable(FactorMetaTable, \"name text, funcid int, sign text\")\n \n     // generate a string containing the signs (whether negated) of variables for each factor\n     factorDescs.foreach { factorDesc =>\n       val signString = factorDesc.func.variables.map(v => !v.isNegated).mkString(\" \")\n-      val funcid = getFactorFunctionTypeid(factorDesc.func.getClass.getSimpleName)\n+      val funcid = InferenceNamespace.getFactorFunctionTypeid(factorDesc.func.getClass.getSimpleName)\n       execute(s\"INSERT INTO ${FactorMetaTable} VALUES ('${factorDesc.name}', ${funcid}, '${signString}')\")\n     }\n \n     // dump factor meta data\n-    du.unload(s\"dd_factormeta\", s\"${groundingPath}/dd_factormeta\", dbSettings, parallelGrounding,\n-      s\"SELECT * FROM ${FactorMetaTable}\")\n+    val groundingDir = getFileNameFromPath(groundingPath)\n+    du.unload(InferenceNamespace.getFactorMetaFileName, \n+      s\"${groundingPath}/${InferenceNamespace.getFactorMetaFileName}\", \n+      dbSettings, parallelGrounding, s\"SELECT * FROM ${FactorMetaTable}\",\n+      groundingDir)\n+  }\n+\n+  // create feature stats for boolean LR function\n+  def createFeatureStats(factorDesc: FactorDesc, querytable: String, weightlist: String,\n+    weightDesc: String) {\n+    // Create feature statistics support tables for error analysis, \n+    // only if it's boolean LR feature (the most common one)\n+    if (factorDesc.func.variables.length == 1 && factorDesc.func.variableDataType == \"Boolean\") {\n+      // This should be a single variable, e.g. \"is_true\"\n+      val variableName = factorDesc.func.variables.map(v => \n+          s\"\"\" ${quoteColumn(v.toString)} \"\"\").mkString(\",\")\n+      val groupByClause = weightlist match {\n+        case \"\" => \"\"\n+        case _ => s\"GROUP BY ${weightlist}\"\n+      }\n+      execute(s\"\"\"\n+      INSERT INTO ${FeatureStatsSupportTable}\n+      SELECT ${weightDesc} as description,\n+             count(CASE WHEN ${variableName}=TRUE THEN 1 ELSE NULL END) AS pos_examples,\n+             count(CASE WHEN ${variableName}=FALSE THEN 1 ELSE NULL END) AS neg_examples,\n+             count(CASE WHEN ${variableName} IS NULL THEN 1 ELSE NULL END) AS queries\n+      FROM ${querytable}\n+      ${groupByClause};\n+      \"\"\")\n+      execute(analyzeTable(FeatureStatsSupportTable))\n+    }\n+  }\n+\n+  // convert grounding file format to be compatible with sampler\n+  // for more information about format, please refer to deepdive's website\n+  def convertGroundingFormat(groundingPath: String) {\n+    log.info(\"Converting grounding file format...\")\n+    val ossuffix = if (System.getProperty(\"os.name\").startsWith(\"Linux\")) \"linux\" else \"mac\"\n+    // TODO: this python script is dangerous and ugly. It changes too many states!\n+    val cmd = s\"python ${Context.deepdiveHome}/util/tobinary.py ${groundingPath} \" + ",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": null,
        "original_position": 581,
        "commit_id": "33a697f46bcc90e003cbbfb2a2343cf0a12a4042",
        "original_commit_id": "0baece37f3eb6ce63d7b0a1f2548a86ba25dd97f",
        "user": {
            "login": "mikecafarella",
            "id": 425838,
            "node_id": "MDQ6VXNlcjQyNTgzOA==",
            "avatar_url": "https://avatars3.githubusercontent.com/u/425838?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mikecafarella",
            "html_url": "https://github.com/mikecafarella",
            "followers_url": "https://api.github.com/users/mikecafarella/followers",
            "following_url": "https://api.github.com/users/mikecafarella/following{/other_user}",
            "gists_url": "https://api.github.com/users/mikecafarella/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mikecafarella/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mikecafarella/subscriptions",
            "organizations_url": "https://api.github.com/users/mikecafarella/orgs",
            "repos_url": "https://api.github.com/users/mikecafarella/repos",
            "events_url": "https://api.github.com/users/mikecafarella/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mikecafarella/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Use Java File object to build paths like this, not manual \"/\" chars and hard-coded strings.  File will take care of directory separators and control characters, etc.  Also, \"util\" and \"tobinary.py\" should be reserved constants\n",
        "created_at": "2015-02-11T00:28:08Z",
        "updated_at": "2015-02-11T21:49:26Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24465197",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24465197"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24465197"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24465299",
        "pull_request_review_id": null,
        "id": 24465299,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI0NDY1Mjk5",
        "diff_hunk": "@@ -656,41 +432,102 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n       createIndexForJoinOptimization(variableTypeTable, \"id\")\n \n       // dump variables\n-      val initvalueCast = variableDataType match {\n-        case 2 | 3 => cast(column, \"float\")\n-        case _ => cast(cast(column, \"int\"), \"float\")\n-      }\n-      du.unload(s\"dd_variables_${relation}\", s\"${groundingPath}/dd_variables_${relation}\",\n+      val initvalueCast = cast(cast(column, \"int\"), \"float\")\n+      // Sen\n+      // du.unload(s\"dd_variables_${relation}\", s\"${groundingPath}/dd_variables_${relation}\",\n+      val groundingDir = getFileNameFromPath(groundingPath)\n+      du.unload(InferenceNamespace.getVariableFileName(relation),\n+        s\"${groundingPath}/${InferenceNamespace.getVariableFileName(relation)}\",\n         dbSettings, parallelGrounding,\n         s\"\"\"SELECT t0.id, t1.${variableTypeColumn},\n         CASE WHEN t1.${variableTypeColumn} = 0 THEN 0 ELSE ${initvalueCast} END AS initvalue,\n         ${variableDataType} AS type, ${cardinality} AS cardinality\n         FROM ${relation} t0, ${relation}_vtype t1\n         WHERE t0.id=t1.id\n-        \"\"\")\n-\n+        \"\"\", groundingDir)\n     }\n+  }\n \n-    // generate factor meta data\n-    execute(s\"\"\"DROP TABLE IF EXISTS ${FactorMetaTable} CASCADE;\n-      CREATE TABLE ${FactorMetaTable} (name text, funcid int, sign text);\n-      \"\"\")\n+  // ground factor meta data\n+  def groundFactorMeta(du: DataLoader, factorDescs: Seq[FactorDesc], groundingPath: String, \n+    parallelGrounding: Boolean) {\n+    ds.dropAndCreateTable(FactorMetaTable, \"name text, funcid int, sign text\")\n \n     // generate a string containing the signs (whether negated) of variables for each factor\n     factorDescs.foreach { factorDesc =>\n       val signString = factorDesc.func.variables.map(v => !v.isNegated).mkString(\" \")\n-      val funcid = getFactorFunctionTypeid(factorDesc.func.getClass.getSimpleName)\n+      val funcid = InferenceNamespace.getFactorFunctionTypeid(factorDesc.func.getClass.getSimpleName)\n       execute(s\"INSERT INTO ${FactorMetaTable} VALUES ('${factorDesc.name}', ${funcid}, '${signString}')\")\n     }\n \n     // dump factor meta data\n-    du.unload(s\"dd_factormeta\", s\"${groundingPath}/dd_factormeta\", dbSettings, parallelGrounding,\n-      s\"SELECT * FROM ${FactorMetaTable}\")\n+    val groundingDir = getFileNameFromPath(groundingPath)\n+    du.unload(InferenceNamespace.getFactorMetaFileName, \n+      s\"${groundingPath}/${InferenceNamespace.getFactorMetaFileName}\", \n+      dbSettings, parallelGrounding, s\"SELECT * FROM ${FactorMetaTable}\",\n+      groundingDir)\n+  }\n+\n+  // create feature stats for boolean LR function\n+  def createFeatureStats(factorDesc: FactorDesc, querytable: String, weightlist: String,\n+    weightDesc: String) {\n+    // Create feature statistics support tables for error analysis, \n+    // only if it's boolean LR feature (the most common one)\n+    if (factorDesc.func.variables.length == 1 && factorDesc.func.variableDataType == \"Boolean\") {\n+      // This should be a single variable, e.g. \"is_true\"\n+      val variableName = factorDesc.func.variables.map(v => \n+          s\"\"\" ${quoteColumn(v.toString)} \"\"\").mkString(\",\")\n+      val groupByClause = weightlist match {\n+        case \"\" => \"\"\n+        case _ => s\"GROUP BY ${weightlist}\"\n+      }\n+      execute(s\"\"\"\n+      INSERT INTO ${FeatureStatsSupportTable}\n+      SELECT ${weightDesc} as description,\n+             count(CASE WHEN ${variableName}=TRUE THEN 1 ELSE NULL END) AS pos_examples,\n+             count(CASE WHEN ${variableName}=FALSE THEN 1 ELSE NULL END) AS neg_examples,\n+             count(CASE WHEN ${variableName} IS NULL THEN 1 ELSE NULL END) AS queries\n+      FROM ${querytable}\n+      ${groupByClause};\n+      \"\"\")\n+      execute(analyzeTable(FeatureStatsSupportTable))\n+    }\n+  }\n+\n+  // convert grounding file format to be compatible with sampler\n+  // for more information about format, please refer to deepdive's website\n+  def convertGroundingFormat(groundingPath: String) {\n+    log.info(\"Converting grounding file format...\")\n+    val ossuffix = if (System.getProperty(\"os.name\").startsWith(\"Linux\")) \"linux\" else \"mac\"\n+    // TODO: this python script is dangerous and ugly. It changes too many states!\n+    val cmd = s\"python ${Context.deepdiveHome}/util/tobinary.py ${groundingPath} \" + \n+        s\"${Context.deepdiveHome}/util/format_converter_${ossuffix} ${Context.outputDir}\"\n+    log.debug(\"Executing: \" + cmd)\n+    val exitValue = cmd!(ProcessLogger(\n+      out => log.info(out),\n+      err => log.info(err)\n+    ))\n+\n+    exitValue match {\n+      case 0 => \n+      case _ => throw new RuntimeException(\"Converting format failed.\")\n+    }\n+  }\n+\n+  def groundFactorsAndWeights(factorDescs: Seq[FactorDesc],\n+    calibrationSettings: CalibrationSettings, du: DataLoader,\n+    dbSettings: DbSettings, groundingPath: String, parallelGrounding: Boolean,\n+    skipLearning: Boolean, weightTable: String) {\n+    val groundingDir = getFileNameFromPath(groundingPath)\n+\n+    // save last weights\n+    if (skipLearning && weightTable.isEmpty()) {\n+      execute(copyLastWeightsSQL)",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 637,
        "original_position": 603,
        "commit_id": "33a697f46bcc90e003cbbfb2a2343cf0a12a4042",
        "original_commit_id": "0baece37f3eb6ce63d7b0a1f2548a86ba25dd97f",
        "user": {
            "login": "mikecafarella",
            "id": 425838,
            "node_id": "MDQ6VXNlcjQyNTgzOA==",
            "avatar_url": "https://avatars3.githubusercontent.com/u/425838?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mikecafarella",
            "html_url": "https://github.com/mikecafarella",
            "followers_url": "https://api.github.com/users/mikecafarella/followers",
            "following_url": "https://api.github.com/users/mikecafarella/following{/other_user}",
            "gists_url": "https://api.github.com/users/mikecafarella/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mikecafarella/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mikecafarella/subscriptions",
            "organizations_url": "https://api.github.com/users/mikecafarella/orgs",
            "repos_url": "https://api.github.com/users/mikecafarella/repos",
            "events_url": "https://api.github.com/users/mikecafarella/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mikecafarella/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "If this clause is executed, does that mean that you will copy the last weights, and then immediately drop the destination table?  Not clear what the desired behavior is here.\n",
        "created_at": "2015-02-11T00:30:04Z",
        "updated_at": "2015-02-11T21:49:26Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24465299",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24465299"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24465299"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24465417",
        "pull_request_review_id": null,
        "id": 24465417,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI0NDY1NDE3",
        "diff_hunk": "@@ -839,67 +660,53 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n         if (isFixed || weightlist == \"\"){\n           execute(s\"\"\"DROP TABLE IF EXISTS ${weighttableForThisFactor} CASCADE;",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": null,
        "original_position": 710,
        "commit_id": "33a697f46bcc90e003cbbfb2a2343cf0a12a4042",
        "original_commit_id": "0baece37f3eb6ce63d7b0a1f2548a86ba25dd97f",
        "user": {
            "login": "mikecafarella",
            "id": 425838,
            "node_id": "MDQ6VXNlcjQyNTgzOA==",
            "avatar_url": "https://avatars3.githubusercontent.com/u/425838?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mikecafarella",
            "html_url": "https://github.com/mikecafarella",
            "followers_url": "https://api.github.com/users/mikecafarella/followers",
            "following_url": "https://api.github.com/users/mikecafarella/following{/other_user}",
            "gists_url": "https://api.github.com/users/mikecafarella/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mikecafarella/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mikecafarella/subscriptions",
            "organizations_url": "https://api.github.com/users/mikecafarella/orgs",
            "repos_url": "https://api.github.com/users/mikecafarella/repos",
            "events_url": "https://api.github.com/users/mikecafarella/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mikecafarella/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "DROP TABLE is such a toxic operation that we should consider making it a method that one has to call in order to generate the SQL.  Inside that method, we would have operations that ensure we are only dropping tables inside the DeepDive namespace.  I am worried that a typo in a tablename somewhere yields a name clash with user tables, and then catastrophe\n",
        "created_at": "2015-02-11T00:32:20Z",
        "updated_at": "2015-02-11T21:49:26Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24465417",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24465417"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24465417"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24465539",
        "pull_request_review_id": null,
        "id": 24465539,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI0NDY1NTM5",
        "diff_hunk": "@@ -459,173 +319,89 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n   def incrementId(table: String, IdSequence: String) {\n     execute(s\"UPDATE ${table} SET id = ${nextVal(IdSequence)};\")\n   }\n-        \n-  // clean up grounding folder (for parallel grounding)\n-  // TODO: we may not need to actually create a script for this and execute it.\n-  // We may want to directly execute the commands:\n-  // Helpers.executeCmd(s\"rm -rf ${groundingPath}/dd_tmp\")\n-  // Helpers.executeCmd(s\"rm -f ${groundingPath}/dd_*\")\n-  // XXX: This function is a little risky because groundingPath may be the empty\n-  // string. Shall we avoid letting the user shoot her own foot?\n-  def cleanParallelGroundingPath(groundingPath: String) {\n-    val cleanFile = File.createTempFile(s\"clean\", \".sh\")\n-    val writer = new PrintWriter(cleanFile)\n-    // cleaning up remaining tmp folder for tobinary\n-    writer.println(s\"rm -rf ${groundingPath}/dd_tmp\")\n-    writer.println(s\"rm -f ${groundingPath}/dd_*\")\n-    writer.close()\n-    log.info(\"Cleaning up grounding folder...\")\n-    Helpers.executeCmd(cleanFile.getAbsolutePath())\n-  }\n \n   // assign variable id - sequential and unique\n   def assignVariablesIds(schema: Map[String, _ <: VariableDataType]) {\n-    // check whether Greenplum is used\n-    var usingGreenplum = false\n-    issueQuery(checkGreenplumSQL) { rs => \n-      usingGreenplum = rs.getBoolean(1) \n-    }\n+    // fast sequential id assign function\n+    createAssignIdFunctionGreenplum()\n+    execute(createSequenceFunction(IdSequence))\n \n     var idoffset : Long = 0\n-    if (usingGreenplum) {\n-      // We use a special homemade function for Greenplum\n-      executeQuery(createAssignIdFunctionGreenplum)\n-      schema.foreach { case(variable, dataType) =>\n-        val Array(relation, column) = variable.split('.')\n-        executeQuery(s\"\"\"SELECT fast_seqassign('${relation.toLowerCase()}', ${idoffset});\"\"\")\n-        issueQuery(s\"\"\"SELECT count(*) FROM ${relation}\"\"\") { rs =>\n-          idoffset = idoffset + rs.getLong(1)\n-        }\n-      }\n-    } else {\n-      // Mysql: use user-defined variables for ID assign;\n-      // Psql: use sequence\n-      execute(createSequenceFunction(IdSequence))\n-      schema.foreach { case(variable, dataType) =>\n-        val Array(relation, column) = variable.split('.')\n-        incrementId(relation, IdSequence)\n-      }\n+    schema.foreach { case(variable, dataType) =>\n+      val Array(relation, column) = variable.split('.')\n+      idoffset += assignIds(relation, idoffset, IdSequence)\n     }\n   }\n-\n-\n-  /** Ground the factor graph to file\n-   *\n-   * Using the schema and inference rules defined in application.conf, construct factor\n-   * graph files.\n-   * Input: variable schema, factor descriptions, holdout configuration, database settings\n-   * Output: factor graph files: variables, factors, edges, weights, meta\n-   *\n-   * NOTE: for this to work in greenplum, do not put id as the first column!\n-   * The first column in greenplum is distribution key by default. \n-   * We need to update this column, but update is not allowed on distribution key. \n-   *\n-   * It is important to remember that we should not modify the user schema,\n-   * e.g., by adding columns to user relations. The right way to do it is\n-   * usually another. For example, an option could be creating a view of the\n-   * user relation, to which we add the needed column.\n-   *\n-   * It is also important to think about corner cases. For example, we cannot\n-   * assume any relation actually contains rows, or the rows are in some\n-   * predefined special order, or anything like that so the code must take care of\n-   * these cases, and there *must* be tests for the corner cases.\n-   * \n-   * TODO: This method is way too long and needs to be split, also for testing\n-   * purposes\n-   */\n-  def groundFactorGraph(schema: Map[String, _ <: VariableDataType], factorDescs: Seq[FactorDesc],\n-    calibrationSettings: CalibrationSettings, skipLearning: Boolean, weightTable: String, \n-    dbSettings: DbSettings, parallelGrounding: Boolean) {\n-\n-    val du = new DataLoader\n-    val groundingPath = if (!parallelGrounding) Context.outputDir else dbSettings.gppath\n-\n-    // check whether Greenplum is used\n-    var usingGreenplum = false\n-    issueQuery(checkGreenplumSQL) { rs => \n-      usingGreenplum = rs.getBoolean(1) \n-    }\n-    \n-    log.info(s\"Using Greenplum = ${usingGreenplum}\")\n-    log.info(s\"Datastore type = ${Helpers.getDbType(dbSettings)}\")\n-    \n-    log.info(s\"Parallel grounding = ${parallelGrounding}\")\n-    log.debug(s\"Grounding Path = ${groundingPath}\")\n-\n-    // clean up grounding folder (for parallel grounding)\n-    if (parallelGrounding) {\n-      cleanParallelGroundingPath(groundingPath)\n-    }\n-\n-    // assign variable id - sequential and unique\n-    assignVariablesIds(schema)\n-\n+  \n+  // assign variable holdout\n+  def assignHoldout(schema: Map[String, _ <: VariableDataType], calibrationSettings: CalibrationSettings) {\n     // variable holdout table - if user defined, execute once\n-    execute(s\"\"\"DROP TABLE IF EXISTS ${VariablesHoldoutTable} CASCADE;\n-      CREATE TABLE ${VariablesHoldoutTable}(variable_id bigint primary key);\n-      \"\"\")\n+    ds.dropAndCreateTable(VariablesHoldoutTable, \"variable_id bigint primary key\")\n     calibrationSettings.holdoutQuery match {\n-      case Some(query) => execute(query)\n-      case None =>\n-    }\n-\n-    // variable observation table\n-    execute(s\"\"\"DROP TABLE IF EXISTS ${VariablesObservationTable} CASCADE;\n-      CREATE TABLE ${VariablesObservationTable}(variable_id bigint primary key);\n-      \"\"\")\n-    calibrationSettings.observationQuery match {\n-      case Some(query) => execute(query)\n-      case None =>\n-    }\n-\n-    // ground variables\n-    schema.foreach { case(variable, dataType) =>\n-      val Array(relation, column) = variable.split('.')\n-\n-      val variableDataType = dataType match {\n-        case BooleanType => 0\n-        case MultinomialType(x) => 1\n-        case RealNumberType => 2\n-        case RealArrayType(x) => 3\n+      case Some(query) => {\n+        log.info(\"Executing user supplied holdout query\")\n+        execute(query)\n       }\n-\n-      val cardinality = dataType match {\n-        case BooleanType => 2\n-        case MultinomialType(x) => x.toInt\n-        case RealNumberType => 2\n-        case RealArrayType(x) => x.toInt\n-      }\n-\n-      if(variableDataType == 2 || variableDataType == 3){\n-\n-      } else {\n-        // This cannot be parsed in def randFunc for now.\n-        // assign holdout - if not user-defined, randomly select from evidence variables of each variable table\n-        calibrationSettings.holdoutQuery match {\n-          case Some(s) =>\n-          case None => execute(s\"\"\"\n+      case None => {\n+        log.info(\"There is no holdout query, will randomly generate holdout set\")\n+         // randomly assign variable holdout\n+        schema.foreach { case(variable, dataType) =>\n+          val Array(relation, column) = variable.split('.')\n+          // This cannot be parsed in def randFunc for now.\n+          // assign holdout - randomly select from evidence variables of each variable table\n+          execute(s\"\"\"\n             INSERT INTO ${VariablesHoldoutTable}\n             SELECT id FROM ${relation}\n             WHERE ${randomFunction} < ${calibrationSettings.holdoutFraction} AND ${column} IS NOT NULL;\n             \"\"\")\n         }\n       }\n+    }\n \n+    // variable observation table\n+    ds.dropAndCreateTable(VariablesObservationTable, \"variable_id bigint primary key\")\n+    calibrationSettings.observationQuery match {\n+      case Some(query) => {\n+        log.info(\"Executing user supplied observation query\")  \n+        execute(query)\n+      }\n+      case None => {\n+        log.info(\"There is no o query\")",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 457,
        "original_position": 450,
        "commit_id": "33a697f46bcc90e003cbbfb2a2343cf0a12a4042",
        "original_commit_id": "0baece37f3eb6ce63d7b0a1f2548a86ba25dd97f",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "It is marking some variables as \"observed\" and fixed. Technically, these are not variables. The difference between observed and evidence is during learning, we will fit observed variables (not sampled in the sampler). This is a feature ported from an earlier engine called Tuffy...\n",
        "created_at": "2015-02-11T00:33:55Z",
        "updated_at": "2015-02-11T21:49:26Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24465539",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24465539"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24465539"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24465853",
        "pull_request_review_id": null,
        "id": 24465853,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI0NDY1ODUz",
        "diff_hunk": "@@ -459,173 +319,89 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n   def incrementId(table: String, IdSequence: String) {\n     execute(s\"UPDATE ${table} SET id = ${nextVal(IdSequence)};\")\n   }\n-        \n-  // clean up grounding folder (for parallel grounding)\n-  // TODO: we may not need to actually create a script for this and execute it.\n-  // We may want to directly execute the commands:\n-  // Helpers.executeCmd(s\"rm -rf ${groundingPath}/dd_tmp\")\n-  // Helpers.executeCmd(s\"rm -f ${groundingPath}/dd_*\")\n-  // XXX: This function is a little risky because groundingPath may be the empty\n-  // string. Shall we avoid letting the user shoot her own foot?\n-  def cleanParallelGroundingPath(groundingPath: String) {\n-    val cleanFile = File.createTempFile(s\"clean\", \".sh\")\n-    val writer = new PrintWriter(cleanFile)\n-    // cleaning up remaining tmp folder for tobinary\n-    writer.println(s\"rm -rf ${groundingPath}/dd_tmp\")\n-    writer.println(s\"rm -f ${groundingPath}/dd_*\")\n-    writer.close()\n-    log.info(\"Cleaning up grounding folder...\")\n-    Helpers.executeCmd(cleanFile.getAbsolutePath())\n-  }\n \n   // assign variable id - sequential and unique\n   def assignVariablesIds(schema: Map[String, _ <: VariableDataType]) {\n-    // check whether Greenplum is used\n-    var usingGreenplum = false\n-    issueQuery(checkGreenplumSQL) { rs => \n-      usingGreenplum = rs.getBoolean(1) \n-    }\n+    // fast sequential id assign function\n+    createAssignIdFunctionGreenplum()\n+    execute(createSequenceFunction(IdSequence))\n \n     var idoffset : Long = 0\n-    if (usingGreenplum) {\n-      // We use a special homemade function for Greenplum\n-      executeQuery(createAssignIdFunctionGreenplum)\n-      schema.foreach { case(variable, dataType) =>\n-        val Array(relation, column) = variable.split('.')\n-        executeQuery(s\"\"\"SELECT fast_seqassign('${relation.toLowerCase()}', ${idoffset});\"\"\")\n-        issueQuery(s\"\"\"SELECT count(*) FROM ${relation}\"\"\") { rs =>\n-          idoffset = idoffset + rs.getLong(1)\n-        }\n-      }\n-    } else {\n-      // Mysql: use user-defined variables for ID assign;\n-      // Psql: use sequence\n-      execute(createSequenceFunction(IdSequence))\n-      schema.foreach { case(variable, dataType) =>\n-        val Array(relation, column) = variable.split('.')\n-        incrementId(relation, IdSequence)\n-      }\n+    schema.foreach { case(variable, dataType) =>\n+      val Array(relation, column) = variable.split('.')\n+      idoffset += assignIds(relation, idoffset, IdSequence)\n     }\n   }\n-\n-\n-  /** Ground the factor graph to file\n-   *\n-   * Using the schema and inference rules defined in application.conf, construct factor\n-   * graph files.\n-   * Input: variable schema, factor descriptions, holdout configuration, database settings\n-   * Output: factor graph files: variables, factors, edges, weights, meta\n-   *\n-   * NOTE: for this to work in greenplum, do not put id as the first column!\n-   * The first column in greenplum is distribution key by default. \n-   * We need to update this column, but update is not allowed on distribution key. \n-   *\n-   * It is important to remember that we should not modify the user schema,\n-   * e.g., by adding columns to user relations. The right way to do it is\n-   * usually another. For example, an option could be creating a view of the\n-   * user relation, to which we add the needed column.\n-   *\n-   * It is also important to think about corner cases. For example, we cannot\n-   * assume any relation actually contains rows, or the rows are in some\n-   * predefined special order, or anything like that so the code must take care of\n-   * these cases, and there *must* be tests for the corner cases.\n-   * \n-   * TODO: This method is way too long and needs to be split, also for testing\n-   * purposes\n-   */\n-  def groundFactorGraph(schema: Map[String, _ <: VariableDataType], factorDescs: Seq[FactorDesc],\n-    calibrationSettings: CalibrationSettings, skipLearning: Boolean, weightTable: String, \n-    dbSettings: DbSettings, parallelGrounding: Boolean) {\n-\n-    val du = new DataLoader\n-    val groundingPath = if (!parallelGrounding) Context.outputDir else dbSettings.gppath\n-\n-    // check whether Greenplum is used\n-    var usingGreenplum = false\n-    issueQuery(checkGreenplumSQL) { rs => \n-      usingGreenplum = rs.getBoolean(1) \n-    }\n-    \n-    log.info(s\"Using Greenplum = ${usingGreenplum}\")\n-    log.info(s\"Datastore type = ${Helpers.getDbType(dbSettings)}\")\n-    \n-    log.info(s\"Parallel grounding = ${parallelGrounding}\")\n-    log.debug(s\"Grounding Path = ${groundingPath}\")\n-\n-    // clean up grounding folder (for parallel grounding)\n-    if (parallelGrounding) {\n-      cleanParallelGroundingPath(groundingPath)\n-    }\n-\n-    // assign variable id - sequential and unique\n-    assignVariablesIds(schema)\n-\n+  \n+  // assign variable holdout\n+  def assignHoldout(schema: Map[String, _ <: VariableDataType], calibrationSettings: CalibrationSettings) {\n     // variable holdout table - if user defined, execute once\n-    execute(s\"\"\"DROP TABLE IF EXISTS ${VariablesHoldoutTable} CASCADE;\n-      CREATE TABLE ${VariablesHoldoutTable}(variable_id bigint primary key);\n-      \"\"\")\n+    ds.dropAndCreateTable(VariablesHoldoutTable, \"variable_id bigint primary key\")\n     calibrationSettings.holdoutQuery match {\n-      case Some(query) => execute(query)\n-      case None =>\n-    }\n-\n-    // variable observation table\n-    execute(s\"\"\"DROP TABLE IF EXISTS ${VariablesObservationTable} CASCADE;\n-      CREATE TABLE ${VariablesObservationTable}(variable_id bigint primary key);\n-      \"\"\")\n-    calibrationSettings.observationQuery match {\n-      case Some(query) => execute(query)\n-      case None =>\n-    }\n-\n-    // ground variables\n-    schema.foreach { case(variable, dataType) =>\n-      val Array(relation, column) = variable.split('.')\n-\n-      val variableDataType = dataType match {\n-        case BooleanType => 0\n-        case MultinomialType(x) => 1\n-        case RealNumberType => 2\n-        case RealArrayType(x) => 3\n+      case Some(query) => {\n+        log.info(\"Executing user supplied holdout query\")\n+        execute(query)\n       }\n-\n-      val cardinality = dataType match {\n-        case BooleanType => 2\n-        case MultinomialType(x) => x.toInt\n-        case RealNumberType => 2\n-        case RealArrayType(x) => x.toInt\n-      }\n-\n-      if(variableDataType == 2 || variableDataType == 3){\n-\n-      } else {\n-        // This cannot be parsed in def randFunc for now.\n-        // assign holdout - if not user-defined, randomly select from evidence variables of each variable table\n-        calibrationSettings.holdoutQuery match {\n-          case Some(s) =>\n-          case None => execute(s\"\"\"\n+      case None => {\n+        log.info(\"There is no holdout query, will randomly generate holdout set\")\n+         // randomly assign variable holdout\n+        schema.foreach { case(variable, dataType) =>\n+          val Array(relation, column) = variable.split('.')\n+          // This cannot be parsed in def randFunc for now.\n+          // assign holdout - randomly select from evidence variables of each variable table\n+          execute(s\"\"\"\n             INSERT INTO ${VariablesHoldoutTable}\n             SELECT id FROM ${relation}\n             WHERE ${randomFunction} < ${calibrationSettings.holdoutFraction} AND ${column} IS NOT NULL;\n             \"\"\")\n         }\n       }\n+    }\n \n+    // variable observation table\n+    ds.dropAndCreateTable(VariablesObservationTable, \"variable_id bigint primary key\")\n+    calibrationSettings.observationQuery match {\n+      case Some(query) => {\n+        log.info(\"Executing user supplied observation query\")  \n+        execute(query)\n+      }\n+      case None => {\n+        log.info(\"There is no o query\")\n+      }\n+    }\n \n-      // Create a cardinality table for the variable\n+  }\n+  \n+  // generate cardinality tables for variables in the schema\n+  // cardinality tables is used to indicate the domains of the variables\n+  def generateCardinalityTables(schema: Map[String, _ <: VariableDataType]) {\n+    schema.foreach { case(variable, dataType) =>\n+      val Array(relation, column) = variable.split('.')\n       // note we use five-digit fixed-length representation here\n       val cardinalityValues = dataType match {\n         case BooleanType => \"('00001')\"\n         case MultinomialType(x) => (0 to x-1).map (n => s\"\"\"('${\"%05d\".format(n)}')\"\"\").mkString(\", \")\n-        case RealNumberType => \"('00001')\"\n-        case RealArrayType(x) => \"('00001')\"\n       }\n-      val cardinalityTableName = s\"${relation}_${column}_cardinality\"\n+      val cardinalityTableName = InferenceNamespace.getCardinalityTableName(relation, column)\n+      ds.dropAndCreateTable(cardinalityTableName, \"cardinality text\")\n       execute(s\"\"\"\n-        DROP TABLE IF EXISTS ${cardinalityTableName} CASCADE;\n-        CREATE TABLE ${cardinalityTableName}(cardinality text);\n         INSERT INTO ${cardinalityTableName} VALUES ${cardinalityValues};\n         \"\"\")\n+    }\n+  }\n+  \n+  // ground variables\n+  def groundVariables(schema: Map[String, _ <: VariableDataType], du: DataLoader, \n+      dbSettings: DbSettings, parallelGrounding: Boolean, groundingPath: String) {\n+        schema.foreach { case(variable, dataType) =>\n+      val Array(relation, column) = variable.split('.')\n+      \n+      val variableDataType = InferenceNamespace.getVariableDataTypeId(dataType)\n+\n+      val cardinality = dataType match {\n+        case BooleanType => 2",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": null,
        "original_position": 489,
        "commit_id": "33a697f46bcc90e003cbbfb2a2343cf0a12a4042",
        "original_commit_id": "0baece37f3eb6ce63d7b0a1f2548a86ba25dd97f",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Here it means the domain size (cardinality) of a boolean variable is 2. The enum types are defined in InferenceNamespace, where we define the variable type mapping: type -> enum integer. \n",
        "created_at": "2015-02-11T00:39:13Z",
        "updated_at": "2015-02-11T21:49:26Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24465853",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24465853"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24465853"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24465930",
        "pull_request_review_id": null,
        "id": 24465930,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI0NDY1OTMw",
        "diff_hunk": "@@ -656,41 +432,102 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n       createIndexForJoinOptimization(variableTypeTable, \"id\")\n \n       // dump variables\n-      val initvalueCast = variableDataType match {\n-        case 2 | 3 => cast(column, \"float\")\n-        case _ => cast(cast(column, \"int\"), \"float\")\n-      }\n-      du.unload(s\"dd_variables_${relation}\", s\"${groundingPath}/dd_variables_${relation}\",\n+      val initvalueCast = cast(cast(column, \"int\"), \"float\")\n+      // Sen\n+      // du.unload(s\"dd_variables_${relation}\", s\"${groundingPath}/dd_variables_${relation}\",\n+      val groundingDir = getFileNameFromPath(groundingPath)\n+      du.unload(InferenceNamespace.getVariableFileName(relation),\n+        s\"${groundingPath}/${InferenceNamespace.getVariableFileName(relation)}\",\n         dbSettings, parallelGrounding,\n         s\"\"\"SELECT t0.id, t1.${variableTypeColumn},\n         CASE WHEN t1.${variableTypeColumn} = 0 THEN 0 ELSE ${initvalueCast} END AS initvalue,\n         ${variableDataType} AS type, ${cardinality} AS cardinality\n         FROM ${relation} t0, ${relation}_vtype t1\n         WHERE t0.id=t1.id\n-        \"\"\")\n-\n+        \"\"\", groundingDir)\n     }\n+  }\n \n-    // generate factor meta data\n-    execute(s\"\"\"DROP TABLE IF EXISTS ${FactorMetaTable} CASCADE;\n-      CREATE TABLE ${FactorMetaTable} (name text, funcid int, sign text);\n-      \"\"\")\n+  // ground factor meta data\n+  def groundFactorMeta(du: DataLoader, factorDescs: Seq[FactorDesc], groundingPath: String, \n+    parallelGrounding: Boolean) {\n+    ds.dropAndCreateTable(FactorMetaTable, \"name text, funcid int, sign text\")\n \n     // generate a string containing the signs (whether negated) of variables for each factor\n     factorDescs.foreach { factorDesc =>\n       val signString = factorDesc.func.variables.map(v => !v.isNegated).mkString(\" \")\n-      val funcid = getFactorFunctionTypeid(factorDesc.func.getClass.getSimpleName)\n+      val funcid = InferenceNamespace.getFactorFunctionTypeid(factorDesc.func.getClass.getSimpleName)\n       execute(s\"INSERT INTO ${FactorMetaTable} VALUES ('${factorDesc.name}', ${funcid}, '${signString}')\")\n     }\n \n     // dump factor meta data\n-    du.unload(s\"dd_factormeta\", s\"${groundingPath}/dd_factormeta\", dbSettings, parallelGrounding,\n-      s\"SELECT * FROM ${FactorMetaTable}\")\n+    val groundingDir = getFileNameFromPath(groundingPath)\n+    du.unload(InferenceNamespace.getFactorMetaFileName, \n+      s\"${groundingPath}/${InferenceNamespace.getFactorMetaFileName}\", \n+      dbSettings, parallelGrounding, s\"SELECT * FROM ${FactorMetaTable}\",\n+      groundingDir)\n+  }\n+\n+  // create feature stats for boolean LR function\n+  def createFeatureStats(factorDesc: FactorDesc, querytable: String, weightlist: String,\n+    weightDesc: String) {\n+    // Create feature statistics support tables for error analysis, \n+    // only if it's boolean LR feature (the most common one)\n+    if (factorDesc.func.variables.length == 1 && factorDesc.func.variableDataType == \"Boolean\") {\n+      // This should be a single variable, e.g. \"is_true\"\n+      val variableName = factorDesc.func.variables.map(v => \n+          s\"\"\" ${quoteColumn(v.toString)} \"\"\").mkString(\",\")\n+      val groupByClause = weightlist match {\n+        case \"\" => \"\"\n+        case _ => s\"GROUP BY ${weightlist}\"\n+      }\n+      execute(s\"\"\"\n+      INSERT INTO ${FeatureStatsSupportTable}\n+      SELECT ${weightDesc} as description,\n+             count(CASE WHEN ${variableName}=TRUE THEN 1 ELSE NULL END) AS pos_examples,\n+             count(CASE WHEN ${variableName}=FALSE THEN 1 ELSE NULL END) AS neg_examples,\n+             count(CASE WHEN ${variableName} IS NULL THEN 1 ELSE NULL END) AS queries\n+      FROM ${querytable}\n+      ${groupByClause};\n+      \"\"\")\n+      execute(analyzeTable(FeatureStatsSupportTable))\n+    }\n+  }\n+\n+  // convert grounding file format to be compatible with sampler\n+  // for more information about format, please refer to deepdive's website\n+  def convertGroundingFormat(groundingPath: String) {\n+    log.info(\"Converting grounding file format...\")\n+    val ossuffix = if (System.getProperty(\"os.name\").startsWith(\"Linux\")) \"linux\" else \"mac\"\n+    // TODO: this python script is dangerous and ugly. It changes too many states!\n+    val cmd = s\"python ${Context.deepdiveHome}/util/tobinary.py ${groundingPath} \" + ",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": null,
        "original_position": 581,
        "commit_id": "33a697f46bcc90e003cbbfb2a2343cf0a12a4042",
        "original_commit_id": "0baece37f3eb6ce63d7b0a1f2548a86ba25dd97f",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Will fix this.\n",
        "created_at": "2015-02-11T00:40:32Z",
        "updated_at": "2015-02-11T21:49:26Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24465930",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24465930"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24465930"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24466104",
        "pull_request_review_id": null,
        "id": 24466104,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI0NDY2MTA0",
        "diff_hunk": "@@ -656,41 +432,102 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n       createIndexForJoinOptimization(variableTypeTable, \"id\")\n \n       // dump variables\n-      val initvalueCast = variableDataType match {\n-        case 2 | 3 => cast(column, \"float\")\n-        case _ => cast(cast(column, \"int\"), \"float\")\n-      }\n-      du.unload(s\"dd_variables_${relation}\", s\"${groundingPath}/dd_variables_${relation}\",\n+      val initvalueCast = cast(cast(column, \"int\"), \"float\")\n+      // Sen\n+      // du.unload(s\"dd_variables_${relation}\", s\"${groundingPath}/dd_variables_${relation}\",\n+      val groundingDir = getFileNameFromPath(groundingPath)\n+      du.unload(InferenceNamespace.getVariableFileName(relation),\n+        s\"${groundingPath}/${InferenceNamespace.getVariableFileName(relation)}\",\n         dbSettings, parallelGrounding,\n         s\"\"\"SELECT t0.id, t1.${variableTypeColumn},\n         CASE WHEN t1.${variableTypeColumn} = 0 THEN 0 ELSE ${initvalueCast} END AS initvalue,\n         ${variableDataType} AS type, ${cardinality} AS cardinality\n         FROM ${relation} t0, ${relation}_vtype t1\n         WHERE t0.id=t1.id\n-        \"\"\")\n-\n+        \"\"\", groundingDir)\n     }\n+  }\n \n-    // generate factor meta data\n-    execute(s\"\"\"DROP TABLE IF EXISTS ${FactorMetaTable} CASCADE;\n-      CREATE TABLE ${FactorMetaTable} (name text, funcid int, sign text);\n-      \"\"\")\n+  // ground factor meta data\n+  def groundFactorMeta(du: DataLoader, factorDescs: Seq[FactorDesc], groundingPath: String, \n+    parallelGrounding: Boolean) {\n+    ds.dropAndCreateTable(FactorMetaTable, \"name text, funcid int, sign text\")\n \n     // generate a string containing the signs (whether negated) of variables for each factor\n     factorDescs.foreach { factorDesc =>\n       val signString = factorDesc.func.variables.map(v => !v.isNegated).mkString(\" \")\n-      val funcid = getFactorFunctionTypeid(factorDesc.func.getClass.getSimpleName)\n+      val funcid = InferenceNamespace.getFactorFunctionTypeid(factorDesc.func.getClass.getSimpleName)\n       execute(s\"INSERT INTO ${FactorMetaTable} VALUES ('${factorDesc.name}', ${funcid}, '${signString}')\")\n     }\n \n     // dump factor meta data\n-    du.unload(s\"dd_factormeta\", s\"${groundingPath}/dd_factormeta\", dbSettings, parallelGrounding,\n-      s\"SELECT * FROM ${FactorMetaTable}\")\n+    val groundingDir = getFileNameFromPath(groundingPath)\n+    du.unload(InferenceNamespace.getFactorMetaFileName, \n+      s\"${groundingPath}/${InferenceNamespace.getFactorMetaFileName}\", \n+      dbSettings, parallelGrounding, s\"SELECT * FROM ${FactorMetaTable}\",\n+      groundingDir)\n+  }\n+\n+  // create feature stats for boolean LR function\n+  def createFeatureStats(factorDesc: FactorDesc, querytable: String, weightlist: String,\n+    weightDesc: String) {\n+    // Create feature statistics support tables for error analysis, \n+    // only if it's boolean LR feature (the most common one)\n+    if (factorDesc.func.variables.length == 1 && factorDesc.func.variableDataType == \"Boolean\") {\n+      // This should be a single variable, e.g. \"is_true\"\n+      val variableName = factorDesc.func.variables.map(v => \n+          s\"\"\" ${quoteColumn(v.toString)} \"\"\").mkString(\",\")\n+      val groupByClause = weightlist match {\n+        case \"\" => \"\"\n+        case _ => s\"GROUP BY ${weightlist}\"\n+      }\n+      execute(s\"\"\"\n+      INSERT INTO ${FeatureStatsSupportTable}\n+      SELECT ${weightDesc} as description,\n+             count(CASE WHEN ${variableName}=TRUE THEN 1 ELSE NULL END) AS pos_examples,\n+             count(CASE WHEN ${variableName}=FALSE THEN 1 ELSE NULL END) AS neg_examples,\n+             count(CASE WHEN ${variableName} IS NULL THEN 1 ELSE NULL END) AS queries\n+      FROM ${querytable}\n+      ${groupByClause};\n+      \"\"\")\n+      execute(analyzeTable(FeatureStatsSupportTable))\n+    }\n+  }\n+\n+  // convert grounding file format to be compatible with sampler\n+  // for more information about format, please refer to deepdive's website\n+  def convertGroundingFormat(groundingPath: String) {\n+    log.info(\"Converting grounding file format...\")\n+    val ossuffix = if (System.getProperty(\"os.name\").startsWith(\"Linux\")) \"linux\" else \"mac\"\n+    // TODO: this python script is dangerous and ugly. It changes too many states!\n+    val cmd = s\"python ${Context.deepdiveHome}/util/tobinary.py ${groundingPath} \" + \n+        s\"${Context.deepdiveHome}/util/format_converter_${ossuffix} ${Context.outputDir}\"\n+    log.debug(\"Executing: \" + cmd)\n+    val exitValue = cmd!(ProcessLogger(\n+      out => log.info(out),\n+      err => log.info(err)\n+    ))\n+\n+    exitValue match {\n+      case 0 => \n+      case _ => throw new RuntimeException(\"Converting format failed.\")\n+    }\n+  }\n+\n+  def groundFactorsAndWeights(factorDescs: Seq[FactorDesc],\n+    calibrationSettings: CalibrationSettings, du: DataLoader,\n+    dbSettings: DbSettings, groundingPath: String, parallelGrounding: Boolean,\n+    skipLearning: Boolean, weightTable: String) {\n+    val groundingDir = getFileNameFromPath(groundingPath)\n+\n+    // save last weights\n+    if (skipLearning && weightTable.isEmpty()) {\n+      execute(copyLastWeightsSQL)",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 637,
        "original_position": 603,
        "commit_id": "33a697f46bcc90e003cbbfb2a2343cf0a12a4042",
        "original_commit_id": "0baece37f3eb6ce63d7b0a1f2548a86ba25dd97f",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "The desired behavior is before dropping the weight table, save the learned weights from last execution to another table, and use it to fill in the new weight table. \n",
        "created_at": "2015-02-11T00:44:10Z",
        "updated_at": "2015-02-11T21:49:26Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24466104",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24466104"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24466104"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24466120",
        "pull_request_review_id": null,
        "id": 24466120,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI0NDY2MTIw",
        "diff_hunk": "@@ -929,58 +729,111 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n           du.unload(s\"${outfile}\", s\"${groundingPath}/${outfile}\", dbSettings, parallelGrounding,\n             s\"\"\"SELECT t0.id AS factor_id, t1.id AS weight_id, ${idcols}\n              FROM ${querytable} t0, ${weighttableForThisFactor} t1\n-             WHERE ${weightjoinlist} AND t1.cardinality = '${cardinalityKey}';\"\"\")\n-        }\n-      }\n-      \n-      // Create feature statistics support tables for error analysis, \n-      // only if it's boolean LR feature (the most common one)\n-      if (factorDesc.func.variables.length == 1 && factorDesc.func.variableDataType == \"Boolean\") {\n-        // This should be a single variable, e.g. \"is_true\"\n-        val variableName = factorDesc.func.variables.map(v => \n-            s\"\"\" ${quoteColumn(v.toString)} \"\"\").mkString(\",\")\n-        val groupByClause = weightlist match {\n-          case \"\" => \"\"\n-          case _ => s\"GROUP BY ${weightlist}\"\n+             WHERE ${weightjoinlist} AND t1.cardinality = '${cardinalityKey}';\"\"\",\n+             groundingDir)\n         }\n-        execute(s\"\"\"\n-        INSERT INTO ${FeatureStatsSupportTable}\n-        SELECT ${weightDesc} as description,\n-               count(CASE WHEN ${variableName}=TRUE THEN 1 ELSE NULL END) AS pos_examples,\n-               count(CASE WHEN ${variableName}=FALSE THEN 1 ELSE NULL END) AS neg_examples,\n-               count(CASE WHEN ${variableName} IS NULL THEN 1 ELSE NULL END) AS queries\n-        FROM ${querytable}\n-        ${groupByClause};\n-        \"\"\")\n-        execute(analyzeTable(FeatureStatsSupportTable))\n       }\n+      // create feature stats for boolean LR\n+      createFeatureStats(factorDesc, querytable, weightlist, weightDesc)\n+    }\n+\n+    if (skipLearning) {\n+      reuseWeights(weightTable)\n     }\n \n     // dump weights\n-    du.unload(\"dd_weights\", s\"${groundingPath}/dd_weights\",dbSettings, parallelGrounding,\n-      s\"SELECT id, isfixed, COALESCE(initvalue, 0) FROM ${WeightsTable}\")\n+    du.unload(InferenceNamespace.getWeightFileName,\n+      s\"${groundingPath}/${InferenceNamespace.getWeightFileName}\",dbSettings, parallelGrounding,\n+      s\"SELECT id, isfixed, COALESCE(initvalue, 0) FROM ${WeightsTable}\",\n+      groundingDir)\n+  }\n+\n+  // handle reusing last weights\n+  def reuseWeights(weightTable: String) {\n+    // skip learning: choose a table to copy weights from\n+    val fromWeightTable = weightTable.isEmpty() match {\n+      case true => lastWeightsTable\n+      case false => weightTable\n+    }\n+    log.info(s\"\"\"Using weights in TABLE ${fromWeightTable} by matching description\"\"\")\n+\n+    // Already set -l 0 for sampler\n+    execute(s\"\"\"\n+      UPDATE ${WeightsTable} SET initvalue = weight \n+      FROM ${fromWeightTable} \n+      WHERE ${WeightsTable}.description = ${fromWeightTable}.description;\n+      \"\"\")\n+  }\n+\n+\n+  /** Ground the factor graph to file\n+   *\n+   * Using the schema and inference rules defined in application.conf, construct factor\n+   * graph files.\n+   * Input: variable schema, factor descriptions, holdout configuration, database settings\n+   * Output: factor graph files: variables, factors, edges, weights, meta\n+   *\n+   * NOTE: for this to work in greenplum, do not put id as the first column!\n+   * The first column in greenplum is distribution key by default. \n+   * We need to update this column, but update is not allowed on distribution key. \n+   *\n+   * It is important to remember that we should not modify the user schema,\n+   * e.g., by adding columns to user relations. The right way to do it is\n+   * usually another. For example, an option could be creating a view of the\n+   * user relation, to which we add the needed column.\n+   *\n+   * It is also important to think about corner cases. For example, we cannot\n+   * assume any relation actually contains rows, or the rows are in some\n+   * predefined special order, or anything like that so the code must take care of\n+   * these cases, and there *must* be tests for the corner cases.\n+   * \n+   * TODO: This method is way too long and needs to be split, also for testing\n+   * purposes\n+   */\n+  def groundFactorGraph(schema: Map[String, _ <: VariableDataType], factorDescs: Seq[FactorDesc],\n+    calibrationSettings: CalibrationSettings, skipLearning: Boolean, weightTable: String, \n+    dbSettings: DbSettings, parallelGrounding: Boolean) {\n+\n+    val du = new DataLoader\n+    val groundingDir = getFileNameFromPath(Context.outputDir)\n+    val groundingPath = parallelGrounding match {\n+      case false => Context.outputDir \n+      case true => dbSettings.gppath + s\"/${groundingDir}\"",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": null,
        "original_position": 908,
        "commit_id": "33a697f46bcc90e003cbbfb2a2343cf0a12a4042",
        "original_commit_id": "0baece37f3eb6ce63d7b0a1f2548a86ba25dd97f",
        "user": {
            "login": "mikecafarella",
            "id": 425838,
            "node_id": "MDQ6VXNlcjQyNTgzOA==",
            "avatar_url": "https://avatars3.githubusercontent.com/u/425838?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mikecafarella",
            "html_url": "https://github.com/mikecafarella",
            "followers_url": "https://api.github.com/users/mikecafarella/followers",
            "following_url": "https://api.github.com/users/mikecafarella/following{/other_user}",
            "gists_url": "https://api.github.com/users/mikecafarella/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mikecafarella/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mikecafarella/subscriptions",
            "organizations_url": "https://api.github.com/users/mikecafarella/orgs",
            "repos_url": "https://api.github.com/users/mikecafarella/repos",
            "events_url": "https://api.github.com/users/mikecafarella/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mikecafarella/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Again, use File here.\n",
        "created_at": "2015-02-11T00:44:23Z",
        "updated_at": "2015-02-11T21:49:26Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24466120",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24466120"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24466120"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24466347",
        "pull_request_review_id": null,
        "id": 24466347,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI0NDY2MzQ3",
        "diff_hunk": "@@ -206,8 +156,7 @@ trait SQLInferenceDataStore extends InferenceDataStore with Logging {\n     DROP TABLE IF EXISTS ${lastWeightsTable} CASCADE;\n     CREATE TABLE ${lastWeightsTable} AS\n       SELECT X.*, Y.weight\n-      FROM ${WeightsTable} AS X INNER JOIN ${WeightResultTable} AS Y ON X.id = Y.id\n-      ORDER BY id ASC;",
        "path": "src/main/scala/org/deepdive/inference/datastore/SQLInferenceDataStore.scala",
        "position": 148,
        "original_position": 141,
        "commit_id": "33a697f46bcc90e003cbbfb2a2343cf0a12a4042",
        "original_commit_id": "0baece37f3eb6ce63d7b0a1f2548a86ba25dd97f",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I think it's unnecessary.\n",
        "created_at": "2015-02-11T00:48:25Z",
        "updated_at": "2015-02-11T21:49:26Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24466347",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24466347"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/230#discussion_r24466347"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/230"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24561645",
        "pull_request_review_id": null,
        "id": 24561645,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI0NTYxNjQ1",
        "diff_hunk": "@@ -400,7 +398,7 @@ class ExtractorRunner(dataStore: JsonExtractionDataStore, dbSettings: DbSettings\n \n     // NEW: for mysqlimport compatibility, the file basename must be same as table name.\n     val queryOutputFile = new File(queryOutputPath + s\"${outputRel}.copy_query_${funcName}.tsv\")\n-    val gpFileName = s\"${outputRel}_unload_${funcName}\"\n+    val gpFileName = s\"${outputRel}_query_unload\"",
        "path": "src/main/scala/org/deepdive/extraction/ExtractorRunner.scala",
        "position": null,
        "original_position": 38,
        "commit_id": "d4bdc8d0af344349f1b03e1f7a8f7e664af71ef9",
        "original_commit_id": "dfe749dc1b0ac36bf0087efaf502112dd1937bc8",
        "user": {
            "login": "mikecafarella",
            "id": 425838,
            "node_id": "MDQ6VXNlcjQyNTgzOA==",
            "avatar_url": "https://avatars3.githubusercontent.com/u/425838?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mikecafarella",
            "html_url": "https://github.com/mikecafarella",
            "followers_url": "https://api.github.com/users/mikecafarella/followers",
            "following_url": "https://api.github.com/users/mikecafarella/following{/other_user}",
            "gists_url": "https://api.github.com/users/mikecafarella/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mikecafarella/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mikecafarella/subscriptions",
            "organizations_url": "https://api.github.com/users/mikecafarella/orgs",
            "repos_url": "https://api.github.com/users/mikecafarella/repos",
            "events_url": "https://api.github.com/users/mikecafarella/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mikecafarella/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Zifei, I'm not sure what's happening on this line.  Can you weigh in?\n",
        "created_at": "2015-02-12T06:46:40Z",
        "updated_at": "2015-02-16T14:50:27Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/209#discussion_r24561645",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/209",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24561645"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/209#discussion_r24561645"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/209"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24562427",
        "pull_request_review_id": null,
        "id": 24562427,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI0NTYyNDI3",
        "diff_hunk": "@@ -400,7 +398,7 @@ class ExtractorRunner(dataStore: JsonExtractionDataStore, dbSettings: DbSettings\n \n     // NEW: for mysqlimport compatibility, the file basename must be same as table name.\n     val queryOutputFile = new File(queryOutputPath + s\"${outputRel}.copy_query_${funcName}.tsv\")\n-    val gpFileName = s\"${outputRel}_unload_${funcName}\"\n+    val gpFileName = s\"${outputRel}_query_unload\"",
        "path": "src/main/scala/org/deepdive/extraction/ExtractorRunner.scala",
        "position": null,
        "original_position": 38,
        "commit_id": "d4bdc8d0af344349f1b03e1f7a8f7e664af71ef9",
        "original_commit_id": "dfe749dc1b0ac36bf0087efaf502112dd1937bc8",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I made this change because sometimes table names are too long (more than 64 chars) with the previous naming convention, and in this case the table will be incorrectly created with a wrong name, and some procedures in DeepDive will crash. Sorry for not commenting it in the code.\n",
        "created_at": "2015-02-12T07:11:12Z",
        "updated_at": "2015-02-16T14:50:27Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/209#discussion_r24562427",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/209",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24562427"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/209#discussion_r24562427"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/209"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24562470",
        "pull_request_review_id": null,
        "id": 24562470,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI0NTYyNDcw",
        "diff_hunk": "@@ -400,7 +398,7 @@ class ExtractorRunner(dataStore: JsonExtractionDataStore, dbSettings: DbSettings\n \n     // NEW: for mysqlimport compatibility, the file basename must be same as table name.\n     val queryOutputFile = new File(queryOutputPath + s\"${outputRel}.copy_query_${funcName}.tsv\")\n-    val gpFileName = s\"${outputRel}_unload_${funcName}\"\n+    val gpFileName = s\"${outputRel}_query_unload\"",
        "path": "src/main/scala/org/deepdive/extraction/ExtractorRunner.scala",
        "position": null,
        "original_position": 38,
        "commit_id": "d4bdc8d0af344349f1b03e1f7a8f7e664af71ef9",
        "original_commit_id": "dfe749dc1b0ac36bf0087efaf502112dd1937bc8",
        "user": {
            "login": "mikecafarella",
            "id": 425838,
            "node_id": "MDQ6VXNlcjQyNTgzOA==",
            "avatar_url": "https://avatars3.githubusercontent.com/u/425838?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mikecafarella",
            "html_url": "https://github.com/mikecafarella",
            "followers_url": "https://api.github.com/users/mikecafarella/followers",
            "following_url": "https://api.github.com/users/mikecafarella/following{/other_user}",
            "gists_url": "https://api.github.com/users/mikecafarella/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mikecafarella/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mikecafarella/subscriptions",
            "organizations_url": "https://api.github.com/users/mikecafarella/orgs",
            "repos_url": "https://api.github.com/users/mikecafarella/repos",
            "events_url": "https://api.github.com/users/mikecafarella/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mikecafarella/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "So the loss of 'funcName' isn't crucial?  I am not sure where funcName comes from, so I can't tell if its loss will possibly cause a name clash here.  \n",
        "created_at": "2015-02-12T07:12:31Z",
        "updated_at": "2015-02-16T14:50:27Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/209#discussion_r24562470",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/209",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24562470"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/209#discussion_r24562470"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/209"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24562611",
        "pull_request_review_id": null,
        "id": 24562611,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI0NTYyNjEx",
        "diff_hunk": "@@ -400,7 +398,7 @@ class ExtractorRunner(dataStore: JsonExtractionDataStore, dbSettings: DbSettings\n \n     // NEW: for mysqlimport compatibility, the file basename must be same as table name.\n     val queryOutputFile = new File(queryOutputPath + s\"${outputRel}.copy_query_${funcName}.tsv\")\n-    val gpFileName = s\"${outputRel}_unload_${funcName}\"\n+    val gpFileName = s\"${outputRel}_query_unload\"",
        "path": "src/main/scala/org/deepdive/extraction/ExtractorRunner.scala",
        "position": null,
        "original_position": 38,
        "commit_id": "d4bdc8d0af344349f1b03e1f7a8f7e664af71ef9",
        "original_commit_id": "dfe749dc1b0ac36bf0087efaf502112dd1937bc8",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I just further updated the naming with `${funcName}_query_unload`, since it is unique and could prevent potential parallelism problems. I also added comments to this change. Waiting for the test.\n",
        "created_at": "2015-02-12T07:17:08Z",
        "updated_at": "2015-02-16T14:50:27Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/209#discussion_r24562611",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/209",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24562611"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/209#discussion_r24562611"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/209"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24756470",
        "pull_request_review_id": null,
        "id": 24756470,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI0NzU2NDcw",
        "diff_hunk": "@@ -28,4 +29,138 @@ object PostgresDataStore extends JdbcDataStore with Logging {\n       dataReader.close()\n     }\n \n+  /**\n+   * input: iterator (of what?)  \n+   * \n+   * - Create a temp CSV file\n+   * - run writeCopyData to write   \n+   */\n+  override def addBatch(result: Iterator[JsObject], outputRelation: String) : Unit = {\n+    val file = File.createTempFile(s\"deepdive_$outputRelation\", \".csv\")",
        "path": "src/main/scala/org/deepdive/datastore/PostgresDataStore.scala",
        "position": null,
        "original_position": 45,
        "commit_id": "0260c1e18bc9e021231194fd4a7f70d377f177ee",
        "original_commit_id": "3c67caf83683d692bf33ee38136e9da8fabaa391",
        "user": {
            "login": "mikecafarella",
            "id": 425838,
            "node_id": "MDQ6VXNlcjQyNTgzOA==",
            "avatar_url": "https://avatars3.githubusercontent.com/u/425838?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mikecafarella",
            "html_url": "https://github.com/mikecafarella",
            "followers_url": "https://api.github.com/users/mikecafarella/followers",
            "following_url": "https://api.github.com/users/mikecafarella/following{/other_user}",
            "gists_url": "https://api.github.com/users/mikecafarella/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mikecafarella/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mikecafarella/subscriptions",
            "organizations_url": "https://api.github.com/users/mikecafarella/orgs",
            "repos_url": "https://api.github.com/users/mikecafarella/repos",
            "events_url": "https://api.github.com/users/mikecafarella/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mikecafarella/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "This temp file could be huge.  But File.createTempFile() will put something in /tmp, which is often on a small volume.  This code is thus extremely prone to failing for lack of disk space.\n\nWe should have our own method called DD.createTempFile(), which is configured to store in a \".tmp\" subdirectory where the config file has told us there is a lot of space.\n",
        "created_at": "2015-02-16T15:19:48Z",
        "updated_at": "2015-02-16T22:09:57Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/235#discussion_r24756470",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/235",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/24756470"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/235#discussion_r24756470"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/235"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/27910619",
        "pull_request_review_id": null,
        "id": 27910619,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI3OTEwNjE5",
        "diff_hunk": "@@ -0,0 +1,98 @@\n+#! /usr/bin/env python\n+#! /usr/bin/env python",
        "path": "src/test/python/test_ddext/input-1.py",
        "position": null,
        "original_position": 2,
        "commit_id": "2ea487e551063728a113325ccf0baf903bd3c76d",
        "original_commit_id": "f42b973eeaf293f3fc3f2dbb16b44f1b0145fd33",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Why is this line repeated?\n",
        "created_at": "2015-04-07T19:20:03Z",
        "updated_at": "2015-04-07T22:54:56Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/251#discussion_r27910619",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/251",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/27910619"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/251#discussion_r27910619"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/251"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/27910641",
        "pull_request_review_id": null,
        "id": 27910641,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDI3OTEwNjQx",
        "diff_hunk": "@@ -0,0 +1,98 @@\n+#! /usr/bin/env python\n+#! /usr/bin/env python",
        "path": "src/test/python/test_ddext/input-2.py",
        "position": null,
        "original_position": 2,
        "commit_id": "2ea487e551063728a113325ccf0baf903bd3c76d",
        "original_commit_id": "f42b973eeaf293f3fc3f2dbb16b44f1b0145fd33",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Why is this line repeated as well?\n",
        "created_at": "2015-04-07T19:20:16Z",
        "updated_at": "2015-04-07T22:54:56Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/251#discussion_r27910641",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/251",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/27910641"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/251#discussion_r27910641"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/251"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/30460082",
        "pull_request_review_id": null,
        "id": 30460082,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMwNDYwMDgy",
        "diff_hunk": "@@ -0,0 +1,10 @@\n+#! /usr/bin/env bash\n+\n+# Create a link for the code to current dir\n+ln -s src/main/scala/org ./",
        "path": "test/test_coverage.sh",
        "position": null,
        "original_position": 4,
        "commit_id": "0ae4355575f29bef63e02bd7ab0ebc6755ad6fa6",
        "original_commit_id": "607082f7a4436087889d942cc0b572dbc2b717ed",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Is this the only way to make `sbt coveralls` work?  They should have some configurable settings.  I guess you probably tried everything, so let's write a clear comment **why** we do this business, not what it's doing. These comments aren't useful because they're longer than the actual commands and only repeat what they do.  Also, you probably want to say something about the required sbt and scala versions, here or in the build files or somewhere in test scripts.\n",
        "created_at": "2015-05-16T10:14:21Z",
        "updated_at": "2015-05-19T03:03:56Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/286#discussion_r30460082",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/286",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/30460082"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/286#discussion_r30460082"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/286"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/30460097",
        "pull_request_review_id": null,
        "id": 30460097,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMwNDYwMDk3",
        "diff_hunk": "@@ -2,7 +2,8 @@\n \n Licensed under the Apache License, Version 2.0. http://www.apache.org/licenses/LICENSE-2.0.txt\n \n-Tested with Travis CI.\n+Tested with Travis CI. and COVERALLS",
        "path": "README.md",
        "position": null,
        "original_position": 5,
        "commit_id": "0ae4355575f29bef63e02bd7ab0ebc6755ad6fa6",
        "original_commit_id": "607082f7a4436087889d942cc0b572dbc2b717ed",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "All caps and the period looks a bit weird.  Can we just place the badges right under the heading without any text?  The text seems redundant.\n",
        "created_at": "2015-05-16T10:17:01Z",
        "updated_at": "2015-05-19T03:03:56Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/286#discussion_r30460097",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/286",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/30460097"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/286#discussion_r30460097"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/286"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/30460112",
        "pull_request_review_id": null,
        "id": 30460112,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMwNDYwMTEy",
        "diff_hunk": "@@ -0,0 +1,10 @@\n+#! /usr/bin/env bash\n+\n+# Create a link for the code to current dir\n+ln -s src/main/scala/org ./",
        "path": "test/test_coverage.sh",
        "position": null,
        "original_position": 4,
        "commit_id": "0ae4355575f29bef63e02bd7ab0ebc6755ad6fa6",
        "original_commit_id": "607082f7a4436087889d942cc0b572dbc2b717ed",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "And perhaps more importantly, a link to coveralls.io as a comment here maybe very helpful for those who aren't familiar with this stuff.\n",
        "created_at": "2015-05-16T10:19:28Z",
        "updated_at": "2015-05-19T03:03:56Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/286#discussion_r30460112",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/286",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/30460112"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/286#discussion_r30460112"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/286"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/30460113",
        "pull_request_review_id": null,
        "id": 30460113,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMwNDYwMTEz",
        "diff_hunk": "@@ -0,0 +1,10 @@\n+#! /usr/bin/env bash\n+\n+# Create a link for the code to current dir\n+ln -s src/main/scala/org ./",
        "path": "test/test_coverage.sh",
        "position": null,
        "original_position": 4,
        "commit_id": "0ae4355575f29bef63e02bd7ab0ebc6755ad6fa6",
        "original_commit_id": "607082f7a4436087889d942cc0b572dbc2b717ed",
        "user": {
            "login": "SenWu",
            "id": 5580008,
            "node_id": "MDQ6VXNlcjU1ODAwMDg=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5580008?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/SenWu",
            "html_url": "https://github.com/SenWu",
            "followers_url": "https://api.github.com/users/SenWu/followers",
            "following_url": "https://api.github.com/users/SenWu/following{/other_user}",
            "gists_url": "https://api.github.com/users/SenWu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/SenWu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/SenWu/subscriptions",
            "organizations_url": "https://api.github.com/users/SenWu/orgs",
            "repos_url": "https://api.github.com/users/SenWu/repos",
            "events_url": "https://api.github.com/users/SenWu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/SenWu/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I tried several ways, but this is only way to work I think. Yes, let me add some comments about versions. I am not clear about the sentence \"because they're longer than the actual commands and only repeat what they do.\"? @netj \n",
        "created_at": "2015-05-16T10:19:38Z",
        "updated_at": "2015-05-19T03:03:56Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/286#discussion_r30460113",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/286",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/30460113"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/286#discussion_r30460113"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/286"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/30460136",
        "pull_request_review_id": null,
        "id": 30460136,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMwNDYwMTM2",
        "diff_hunk": "@@ -0,0 +1,10 @@\n+#! /usr/bin/env bash\n+\n+# Create a link for the code to current dir\n+ln -s src/main/scala/org ./",
        "path": "test/test_coverage.sh",
        "position": null,
        "original_position": 4,
        "commit_id": "0ae4355575f29bef63e02bd7ab0ebc6755ad6fa6",
        "original_commit_id": "607082f7a4436087889d942cc0b572dbc2b717ed",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I meant you don't have to explain each line in English.  It's much more useful to write the non-obvious: why you do a ln and rm around sbt coveralls and what other dependencies are there.\n\nBtw should the `sbt coverageAggregate` be moved here?  Right before it's submitted?  And the file name is a bit misleading because it's not collecting coverage but only submitting, right?\n",
        "created_at": "2015-05-16T10:25:18Z",
        "updated_at": "2015-05-19T03:03:56Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/286#discussion_r30460136",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/286",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/30460136"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/286#discussion_r30460136"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/286"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/30460146",
        "pull_request_review_id": null,
        "id": 30460146,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMwNDYwMTQ2",
        "diff_hunk": "@@ -1,3 +1,9 @@\n+resolvers += Classpaths.sbtPluginReleases\n+\n addSbtPlugin(\"org.xerial.sbt\" % \"sbt-pack\" % \"0.5.1\")  // for sbt-0.13.x or higher\n \n-addSbtPlugin(\"de.johoop\" % \"jacoco4sbt\" % \"2.1.6\")\n\\ No newline at end of file\n+addSbtPlugin(\"de.johoop\" % \"jacoco4sbt\" % \"2.1.6\")\n+\n+addSbtPlugin(\"org.scoverage\" % \"sbt-scoverage\" % \"1.1.0\")\n+\n+addSbtPlugin(\"org.scoverage\" % \"sbt-coveralls\" % \"1.0.0\")",
        "path": "project/plugins.sbt",
        "position": null,
        "original_position": 11,
        "commit_id": "0ae4355575f29bef63e02bd7ab0ebc6755ad6fa6",
        "original_commit_id": "607082f7a4436087889d942cc0b572dbc2b717ed",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "We should state the sbt/scala requirements right here.\n",
        "created_at": "2015-05-16T10:27:34Z",
        "updated_at": "2015-05-19T03:03:56Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/286#discussion_r30460146",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/286",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/30460146"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/286#discussion_r30460146"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/286"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/30460960",
        "pull_request_review_id": null,
        "id": 30460960,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMwNDYwOTYw",
        "diff_hunk": "@@ -2,7 +2,8 @@\n \n Licensed under the Apache License, Version 2.0. http://www.apache.org/licenses/LICENSE-2.0.txt\n \n-Tested with Travis CI.\n+Tested with Travis CI. and COVERALLS",
        "path": "README.md",
        "position": null,
        "original_position": 5,
        "commit_id": "0ae4355575f29bef63e02bd7ab0ebc6755ad6fa6",
        "original_commit_id": "607082f7a4436087889d942cc0b572dbc2b717ed",
        "user": {
            "login": "SenWu",
            "id": 5580008,
            "node_id": "MDQ6VXNlcjU1ODAwMDg=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5580008?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/SenWu",
            "html_url": "https://github.com/SenWu",
            "followers_url": "https://api.github.com/users/SenWu/followers",
            "following_url": "https://api.github.com/users/SenWu/following{/other_user}",
            "gists_url": "https://api.github.com/users/SenWu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/SenWu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/SenWu/subscriptions",
            "organizations_url": "https://api.github.com/users/SenWu/orgs",
            "repos_url": "https://api.github.com/users/SenWu/repos",
            "events_url": "https://api.github.com/users/SenWu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/SenWu/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "They use this name COVERALLS, but I removed it in later commit.\n",
        "created_at": "2015-05-16T13:02:53Z",
        "updated_at": "2015-05-19T03:03:56Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/286#discussion_r30460960",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/286",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/30460960"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/286#discussion_r30460960"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/286"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/30460991",
        "pull_request_review_id": null,
        "id": 30460991,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMwNDYwOTkx",
        "diff_hunk": "@@ -0,0 +1,10 @@\n+#! /usr/bin/env bash\n+\n+# Create a link for the code to current dir\n+ln -s src/main/scala/org ./",
        "path": "test/test_coverage.sh",
        "position": null,
        "original_position": 4,
        "commit_id": "0ae4355575f29bef63e02bd7ab0ebc6755ad6fa6",
        "original_commit_id": "607082f7a4436087889d942cc0b572dbc2b717ed",
        "user": {
            "login": "SenWu",
            "id": 5580008,
            "node_id": "MDQ6VXNlcjU1ODAwMDg=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5580008?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/SenWu",
            "html_url": "https://github.com/SenWu",
            "followers_url": "https://api.github.com/users/SenWu/followers",
            "following_url": "https://api.github.com/users/SenWu/following{/other_user}",
            "gists_url": "https://api.github.com/users/SenWu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/SenWu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/SenWu/subscriptions",
            "organizations_url": "https://api.github.com/users/SenWu/orgs",
            "repos_url": "https://api.github.com/users/SenWu/repos",
            "events_url": "https://api.github.com/users/SenWu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/SenWu/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Although I think this is a bad idea, I change it in later commit. Think if you write a program and when you finish it you will collect the data. But if you it move here, it looks like we don't collect data. People can test coverage on his own machine, and they need covaregeAggregate.\n",
        "created_at": "2015-05-16T13:07:14Z",
        "updated_at": "2015-05-19T03:03:56Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/286#discussion_r30460991",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/286",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/30460991"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/286#discussion_r30460991"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/286"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/30461012",
        "pull_request_review_id": null,
        "id": 30461012,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMwNDYxMDEy",
        "diff_hunk": "@@ -0,0 +1,10 @@\n+#! /usr/bin/env bash\n+\n+# Create a link for the code to current dir\n+ln -s src/main/scala/org ./",
        "path": "test/test_coverage.sh",
        "position": null,
        "original_position": 4,
        "commit_id": "0ae4355575f29bef63e02bd7ab0ebc6755ad6fa6",
        "original_commit_id": "607082f7a4436087889d942cc0b572dbc2b717ed",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I see. Then, let's just rename the script to `submit_coverage.sh` or so to make it clear.\n",
        "created_at": "2015-05-16T13:09:27Z",
        "updated_at": "2015-05-19T03:03:56Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/286#discussion_r30461012",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/286",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/30461012"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/286#discussion_r30461012"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/286"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/30749081",
        "pull_request_review_id": null,
        "id": 30749081,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMwNzQ5MDgx",
        "diff_hunk": "@@ -0,0 +1,8 @@\n+#! /usr/bin/env bash\n+\n+# Create a symbolic link at src/main/scala/org to let coverage test retrieve all source,\n+# and remove it after test\n+# Ref: https://github.com/scoverage/sbt-coveralls\n+ln -s src/main/scala/org ./\n+COVERALLS_REPO_TOKEN=QknRiHqsMIzaOEbmSYtikFuxuVWEiPAJe sbt coveralls",
        "path": "test/submit_coverage.sh",
        "position": null,
        "original_position": 7,
        "commit_id": "2d3fef5a470953836a9dbc70371af5549828a394",
        "original_commit_id": "6c6a0158930f8a526179c4165477da9d2c4d146b",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Why is the token exposed here?  To my understanding, we don't need it when running through TravisCI.\nPlease remove the line, and reset the token since it's been compromised.\n",
        "created_at": "2015-05-20T21:03:15Z",
        "updated_at": "2015-05-23T23:27:23Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/293#discussion_r30749081",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/293",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/30749081"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/293#discussion_r30749081"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/293"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/30749125",
        "pull_request_review_id": null,
        "id": 30749125,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMwNzQ5MTI1",
        "diff_hunk": "@@ -62,6 +62,7 @@ sbt \"test-only org.deepdive.test.integration.BrokenTest -- -oF\" && \\\n sbt \"test-only org.deepdive.test.integration.BiasedCoin -- -oF\" && \\\n sbt \"test-only org.deepdive.test.integration.MysqlSpouseExample -- -oF\" && \\\n sbt \"test-only org.deepdive.test.integration.ChunkingApp -- -oF\"\n+sbt coverageAggregate",
        "path": "test/test_mysql.sh",
        "position": null,
        "original_position": 4,
        "commit_id": "2d3fef5a470953836a9dbc70371af5549828a394",
        "original_commit_id": "6c6a0158930f8a526179c4165477da9d2c4d146b",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Where are the `sbt coverage` lines for mysql?\n",
        "created_at": "2015-05-20T21:03:43Z",
        "updated_at": "2015-05-23T23:27:23Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/293#discussion_r30749125",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/293",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/30749125"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/293#discussion_r30749125"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/293"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/31005421",
        "pull_request_review_id": null,
        "id": 31005421,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMxMDA1NDIx",
        "diff_hunk": "@@ -0,0 +1,310 @@\n+---\n+layout: default\n+---\n+\n+# Using DeepDive with Postgres-XL \n+\n+This document describes how to install and configure\n+[Postgres-XL](http://www.postgres-xl.org/) to work\n+with DeepDive. It also describes two caveats needed in writing queries when using\n+XL, and some [FAQs](#faq) about using XL.\n+\n+After installing XL, DeepDive should work well with it. Apart from the\n+following caveat below, you should not observe any difference than if you\n+were running PostgreSQL.\n+\n+## Important Caveats\n+\n+You should add a `DISTRIBUTE BY HASH` clause in all `CREATE TABLE` commands. **Do\n+not use the column `id`** as the distribution key. **Do not use** a distribution\n+key that is **not initially assigned**.\n+\n+Refer to the [XL\n+manual](http://files.postgres-xl.org/documentation/) for\n+more information.\n+\n+You should always create tables as `CREATE UNLOGGED TABLE ...`. The unlogged\n+keyword turns off write-ahead logging (WAL), and gives a significant speedup\n+in writes (often 10X). WAL is not needed for DeepDive apps, because the\n+database is only used for processing, not for persisting data.\n+\n+## Installation\n+We now describe how to install XL and configure it to be used with\n+DeepDive. The steps were tested to install XL on Ubuntu 15.04. \n+\n+### Setting OS Parameters\n+\n+Set the following parameters in the `/etc/sysctl.conf` file: \n+```\n+kernel.sem = 1000  32000  32  1000\n+kernel.shmmax = 429496729600  #400GB\n+```\n+\n+After making these changes, run\n+```bash\n+sudo sysctl -p /etc/sysctl.conf\n+```\n+\n+Set the following parameter in the `/etc/ssh/ssh_config` file:\n+```\n+StrictHostKeyChecking no\n+```\n+\n+Finally, set the following parameter in the `/etc/ssh/sshd_config` file:\n+```\n+MaxStartups 100\n+```\n+After that, run\n+```bash\n+sudo service ssh restart\n+```\n+\n+### Building XL from Source\n+\n+```bash\n+sudo apt-get update\n+sudo apt-get -y install -y screen curl git rsync openssl locales openssh-server openssh-client\n+sudo apt-get -y install -y gcc flex bison make cmake jade openjade docbook docbook-dsssl\n+sudo apt-get -y install zlib1g-dev libreadline6-dev python-dev libssl-dev\n+sudo localedef -i en_US -c -f UTF-8 -A /usr/share/locale/locale.alias en_US.UTF-8\n+```\n+\n+Now set `$TARGET_DIR` to the directory into which you would like to install XL.\n+```bash\n+TARGET_DIR=/opt/pgxl\n+```\n+\n+Then add the following line(s) to `/etc/ld.so.conf` (replace `$TARGET_DIR` with dir):\n+```bash\n+/usr/local/lib\n+$TARGET_DIR/lib\n+```\n+\n+The remaining parts of the installation do not require sudo rights. However, make sure\n+the user account doing the installation has write permissions to `$TARGET_DIR`.\n+\n+Create a build directory and download the XL sources.\n+```bash\n+BUILD_DIR=~/pgxl_install\n+mkdir -p $BUILD_DIR\n+cd $BUILD_DIR\n+wget -O pgxl-v9.2.tar.gz http://sourceforge.net/projects/postgres-xl/files/Releases/Version_9.2rc/postgres-xl-v9.2-src.tar.gz/download\n+tar -xzf pgxl-v9.2.tar.gz\n+```\n+\n+Now, you can build the sources.\n+```bash\n+cd $BUILD_DIR/postgres-xl/\n+./configure --with-python --with-openssl --prefix $TARGET_DIR\n+make -j 4\n+make install\n+```\n+\n+Additionally modules can be added as follows.\n+```bash\n+for pkg in btree_gin btree_gist earthdistance fuzzystrmatch hstore intagg intarray oid2name \\\n+pg_buffercache pgcrypto pgxc_clean pgxc_ctl pgxc_ddl pgxc_monitor stormstats \\\n+tablefunc tsearch2 unaccent; do\n+    cd $BUILD_DIR/postgres-xl/contrib/$pkg\n+    make; make install\n+done\n+```\n+\n+### Set XL related session variables\n+\n+To set Greenplum related session variables, modify your",
        "path": "doc/doc/advanced/pgxl.md",
        "position": null,
        "original_position": 115,
        "commit_id": "81cd9c21d7d118a02d92fad7f78b6872a7ee455a",
        "original_commit_id": "01c273ebb2f225b7f16b5f74e98271dd735a998b",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "\"Greenplum\" still remains here\n",
        "created_at": "2015-05-26T05:31:44Z",
        "updated_at": "2015-06-03T06:37:44Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/301#discussion_r31005421",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/301",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/31005421"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/301#discussion_r31005421"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/301"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/31005446",
        "pull_request_review_id": null,
        "id": 31005446,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMxMDA1NDQ2",
        "diff_hunk": "@@ -0,0 +1,310 @@\n+---\n+layout: default\n+---\n+\n+# Using DeepDive with Postgres-XL \n+\n+This document describes how to install and configure\n+[Postgres-XL](http://www.postgres-xl.org/) to work\n+with DeepDive. It also describes two caveats needed in writing queries when using\n+XL, and some [FAQs](#faq) about using XL.\n+\n+After installing XL, DeepDive should work well with it. Apart from the\n+following caveat below, you should not observe any difference than if you\n+were running PostgreSQL.\n+\n+## Important Caveats\n+\n+You should add a `DISTRIBUTE BY HASH` clause in all `CREATE TABLE` commands. **Do\n+not use the column `id`** as the distribution key. **Do not use** a distribution\n+key that is **not initially assigned**.\n+\n+Refer to the [XL\n+manual](http://files.postgres-xl.org/documentation/) for\n+more information.\n+\n+You should always create tables as `CREATE UNLOGGED TABLE ...`. The unlogged\n+keyword turns off write-ahead logging (WAL), and gives a significant speedup\n+in writes (often 10X). WAL is not needed for DeepDive apps, because the\n+database is only used for processing, not for persisting data.\n+\n+## Installation\n+We now describe how to install XL and configure it to be used with\n+DeepDive. The steps were tested to install XL on Ubuntu 15.04. \n+\n+### Setting OS Parameters\n+\n+Set the following parameters in the `/etc/sysctl.conf` file: \n+```\n+kernel.sem = 1000  32000  32  1000\n+kernel.shmmax = 429496729600  #400GB\n+```\n+\n+After making these changes, run\n+```bash\n+sudo sysctl -p /etc/sysctl.conf\n+```\n+\n+Set the following parameter in the `/etc/ssh/ssh_config` file:\n+```\n+StrictHostKeyChecking no\n+```\n+\n+Finally, set the following parameter in the `/etc/ssh/sshd_config` file:\n+```\n+MaxStartups 100\n+```\n+After that, run\n+```bash\n+sudo service ssh restart\n+```\n+\n+### Building XL from Source\n+\n+```bash\n+sudo apt-get update\n+sudo apt-get -y install -y screen curl git rsync openssl locales openssh-server openssh-client\n+sudo apt-get -y install -y gcc flex bison make cmake jade openjade docbook docbook-dsssl\n+sudo apt-get -y install zlib1g-dev libreadline6-dev python-dev libssl-dev\n+sudo localedef -i en_US -c -f UTF-8 -A /usr/share/locale/locale.alias en_US.UTF-8\n+```\n+\n+Now set `$TARGET_DIR` to the directory into which you would like to install XL.\n+```bash\n+TARGET_DIR=/opt/pgxl\n+```\n+\n+Then add the following line(s) to `/etc/ld.so.conf` (replace `$TARGET_DIR` with dir):\n+```bash\n+/usr/local/lib\n+$TARGET_DIR/lib\n+```\n+\n+The remaining parts of the installation do not require sudo rights. However, make sure\n+the user account doing the installation has write permissions to `$TARGET_DIR`.\n+\n+Create a build directory and download the XL sources.\n+```bash\n+BUILD_DIR=~/pgxl_install\n+mkdir -p $BUILD_DIR\n+cd $BUILD_DIR\n+wget -O pgxl-v9.2.tar.gz http://sourceforge.net/projects/postgres-xl/files/Releases/Version_9.2rc/postgres-xl-v9.2-src.tar.gz/download\n+tar -xzf pgxl-v9.2.tar.gz\n+```\n+\n+Now, you can build the sources.\n+```bash\n+cd $BUILD_DIR/postgres-xl/\n+./configure --with-python --with-openssl --prefix $TARGET_DIR\n+make -j 4\n+make install\n+```\n+\n+Additionally modules can be added as follows.\n+```bash\n+for pkg in btree_gin btree_gist earthdistance fuzzystrmatch hstore intagg intarray oid2name \\\n+pg_buffercache pgcrypto pgxc_clean pgxc_ctl pgxc_ddl pgxc_monitor stormstats \\\n+tablefunc tsearch2 unaccent; do\n+    cd $BUILD_DIR/postgres-xl/contrib/$pkg\n+    make; make install\n+done\n+```\n+\n+### Set XL related session variables\n+\n+To set Greenplum related session variables, modify your\n+`~/.bashrc` script and add the line:\n+\n+```bash\n+export PATH=$TARGET_DIR/bin:\\$PATH",
        "path": "doc/doc/advanced/pgxl.md",
        "position": null,
        "original_position": 119,
        "commit_id": "81cd9c21d7d118a02d92fad7f78b6872a7ee455a",
        "original_commit_id": "01c273ebb2f225b7f16b5f74e98271dd735a998b",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "There shouldn't be a backslash before the last `$PATH`.  Surrounding quote may be safer.\n\n``` bash\nexport PATH=\"$TARGET_DIR/bin:$PATH\"\n```\n",
        "created_at": "2015-05-26T05:32:57Z",
        "updated_at": "2015-06-03T06:37:44Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/301#discussion_r31005446",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/301",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/31005446"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/301#discussion_r31005446"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/301"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/31006400",
        "pull_request_review_id": null,
        "id": 31006400,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMxMDA2NDAw",
        "diff_hunk": "@@ -0,0 +1,310 @@\n+---\n+layout: default\n+---\n+\n+# Using DeepDive with Postgres-XL \n+\n+This document describes how to install and configure\n+[Postgres-XL](http://www.postgres-xl.org/) to work\n+with DeepDive. It also describes two caveats needed in writing queries when using\n+XL, and some [FAQs](#faq) about using XL.\n+\n+After installing XL, DeepDive should work well with it. Apart from the\n+following caveat below, you should not observe any difference than if you\n+were running PostgreSQL.\n+\n+## Important Caveats\n+\n+You should add a `DISTRIBUTE BY HASH` clause in all `CREATE TABLE` commands. **Do\n+not use the column `id`** as the distribution key. **Do not use** a distribution\n+key that is **not initially assigned**.\n+\n+Refer to the [XL\n+manual](http://files.postgres-xl.org/documentation/) for\n+more information.\n+\n+You should always create tables as `CREATE UNLOGGED TABLE ...`. The unlogged\n+keyword turns off write-ahead logging (WAL), and gives a significant speedup\n+in writes (often 10X). WAL is not needed for DeepDive apps, because the\n+database is only used for processing, not for persisting data.\n+\n+## Installation\n+We now describe how to install XL and configure it to be used with\n+DeepDive. The steps were tested to install XL on Ubuntu 15.04. \n+\n+### Setting OS Parameters\n+\n+Set the following parameters in the `/etc/sysctl.conf` file: \n+```\n+kernel.sem = 1000  32000  32  1000\n+kernel.shmmax = 429496729600  #400GB\n+```\n+\n+After making these changes, run\n+```bash\n+sudo sysctl -p /etc/sysctl.conf\n+```\n+\n+Set the following parameter in the `/etc/ssh/ssh_config` file:\n+```\n+StrictHostKeyChecking no\n+```\n+\n+Finally, set the following parameter in the `/etc/ssh/sshd_config` file:\n+```\n+MaxStartups 100\n+```\n+After that, run\n+```bash\n+sudo service ssh restart\n+```\n+\n+### Building XL from Source\n+\n+```bash\n+sudo apt-get update\n+sudo apt-get -y install -y screen curl git rsync openssl locales openssh-server openssh-client\n+sudo apt-get -y install -y gcc flex bison make cmake jade openjade docbook docbook-dsssl\n+sudo apt-get -y install zlib1g-dev libreadline6-dev python-dev libssl-dev\n+sudo localedef -i en_US -c -f UTF-8 -A /usr/share/locale/locale.alias en_US.UTF-8\n+```\n+\n+Now set `$TARGET_DIR` to the directory into which you would like to install XL.\n+```bash\n+TARGET_DIR=/opt/pgxl\n+```\n+\n+Then add the following line(s) to `/etc/ld.so.conf` (replace `$TARGET_DIR` with dir):\n+```bash\n+/usr/local/lib\n+$TARGET_DIR/lib\n+```\n+\n+The remaining parts of the installation do not require sudo rights. However, make sure\n+the user account doing the installation has write permissions to `$TARGET_DIR`.\n+\n+Create a build directory and download the XL sources.\n+```bash\n+BUILD_DIR=~/pgxl_install\n+mkdir -p $BUILD_DIR\n+cd $BUILD_DIR\n+wget -O pgxl-v9.2.tar.gz http://sourceforge.net/projects/postgres-xl/files/Releases/Version_9.2rc/postgres-xl-v9.2-src.tar.gz/download\n+tar -xzf pgxl-v9.2.tar.gz\n+```\n+\n+Now, you can build the sources.\n+```bash\n+cd $BUILD_DIR/postgres-xl/\n+./configure --with-python --with-openssl --prefix $TARGET_DIR\n+make -j 4\n+make install\n+```\n+\n+Additionally modules can be added as follows.\n+```bash\n+for pkg in btree_gin btree_gist earthdistance fuzzystrmatch hstore intagg intarray oid2name \\\n+pg_buffercache pgcrypto pgxc_clean pgxc_ctl pgxc_ddl pgxc_monitor stormstats \\\n+tablefunc tsearch2 unaccent; do\n+    cd $BUILD_DIR/postgres-xl/contrib/$pkg\n+    make; make install\n+done\n+```\n+\n+### Set XL related session variables\n+\n+To set Greenplum related session variables, modify your\n+`~/.bashrc` script and add the line:\n+\n+```bash\n+export PATH=$TARGET_DIR/bin:\\$PATH\n+export PGXC_CTL_HOME=$TARGET_DIR/conf\n+```\n+\n+### Configure ssh with localhost\n+\n+Now you need to generate ssh keys for `localhost`. Run: \n+```bash\n+mkdir -p ~/.ssh\n+chmod 700 ~/.ssh\n+cd ~/.ssh\n+if [ ! -f id_rsa.pub ]; then\n+    ssh-keygen -t rsa -N \"\" -f id_rsa\n+fi\n+cat id_rsa.pub >> authorized_keys\n+chmod 600 authorized_keys\n+```\n+\n+Then you should be able to `ssh` into `localhost` without password, and you can\n+move on.\n+\n+### Create directories for the database\n+\n+Now set `$DATA_DIR` to the directory, where the database files will be stored.\n+**Be sure that you have write permission to this directory**.\n+\n+```bash\n+mkdir -p $DATA_DIR\n+```\n+\n+### Configure the XL database\n+\n+In order to run XL, we still need to create a file with configuration settings. Run\n+```bash\n+mkdir -p $TARGET_DIR/conf\n+```\n+\n+Now, assuming that `$USER` is set to the account under which pgxl should execute, the\n+following settings should provide a good starting point.\n+```bash\n+USER_DB_PORT=5432\n+MAX_USER_CONNECTIONS=100\n+DATA_NODE_SHARED_BUFFERS=\"2000MB\"\n+DATA_NODE_WORK_MEM=\"128MB\"\n+DATA_NODE_MAINTENANCE_MEM=\"128MB\"\n+DATA_NODE_WAL_BUFFERS=\"16MB\"\n+DATA_NODE_CHECKPOINT_SEGMENTS=\"256\"\n+```\n+\n+Create a file `$TARGET_DIR/conf/pgxc_ctl.conf` with the following contents (replace\n+variables with their values):\n+\n+```\n+pgxcOwner=$USER\n+pgxcUser=\\$pgxcOwner",
        "path": "doc/doc/advanced/pgxl.md",
        "position": null,
        "original_position": 173,
        "commit_id": "81cd9c21d7d118a02d92fad7f78b6872a7ee455a",
        "original_commit_id": "01c273ebb2f225b7f16b5f74e98271dd735a998b",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "There seems many unnecessary backslashes here for pgxc_ctl.conf.  Is this intentional?\n\nI'd argue most steps involving config file creation should be written in the following style:\n1. first ask the user to set several shell variables, \n2. then run a huge `cat` command with heredoc that produces the file.\n\nIt's easy for the reader since they can blindly copy/paste the whole thing into their terminal to just get things work and also clear how advanced readers could customize if they wanted.\n\nFor example:\n\n``` bash\n# some customizable variables\nUSER=mylogin\npgxcOwner=mylogin\n# more variables ...\n\n# create pgxc_ctl.conf\ncat >\"$TARGET_DIR\"/conf/pgxc_ctl.conf <<EOF\npgxcOwner=$USER\npgxcUser=$pgxcOwner\n...\nEOF\n```\n",
        "created_at": "2015-05-26T06:05:51Z",
        "updated_at": "2015-06-03T06:37:44Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/301#discussion_r31006400",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/301",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/31006400"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/301#discussion_r31006400"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/301"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/31006424",
        "pull_request_review_id": null,
        "id": 31006424,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMxMDA2NDI0",
        "diff_hunk": "@@ -232,14 +232,16 @@ trait JdbcDataStore extends Logging {\n   }\n \n   // check whether postgres-xl is used\n-  def isUsingPostgresXL() : Boolean = {\n+  lazy val isUsingPostgresXL : Boolean = {",
        "path": "src/main/scala/org/deepdive/datastore/JdbcDataStore.scala",
        "position": 23,
        "original_position": 23,
        "commit_id": "81cd9c21d7d118a02d92fad7f78b6872a7ee455a",
        "original_commit_id": "01c273ebb2f225b7f16b5f74e98271dd735a998b",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "`lazy val` seems good for these checks!\n",
        "created_at": "2015-05-26T06:06:47Z",
        "updated_at": "2015-06-03T06:37:44Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/301#discussion_r31006424",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/301",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/31006424"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/301#discussion_r31006424"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/301"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/31006494",
        "pull_request_review_id": null,
        "id": 31006494,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMxMDA2NDk0",
        "diff_hunk": "@@ -0,0 +1,310 @@\n+---\n+layout: default\n+---\n+\n+# Using DeepDive with Postgres-XL \n+\n+This document describes how to install and configure\n+[Postgres-XL](http://www.postgres-xl.org/) to work\n+with DeepDive. It also describes two caveats needed in writing queries when using\n+XL, and some [FAQs](#faq) about using XL.\n+\n+After installing XL, DeepDive should work well with it. Apart from the\n+following caveat below, you should not observe any difference than if you\n+were running PostgreSQL.\n+\n+## Important Caveats\n+\n+You should add a `DISTRIBUTE BY HASH` clause in all `CREATE TABLE` commands. **Do\n+not use the column `id`** as the distribution key. **Do not use** a distribution\n+key that is **not initially assigned**.\n+\n+Refer to the [XL\n+manual](http://files.postgres-xl.org/documentation/) for\n+more information.\n+\n+You should always create tables as `CREATE UNLOGGED TABLE ...`. The unlogged\n+keyword turns off write-ahead logging (WAL), and gives a significant speedup\n+in writes (often 10X). WAL is not needed for DeepDive apps, because the\n+database is only used for processing, not for persisting data.\n+\n+## Installation\n+We now describe how to install XL and configure it to be used with\n+DeepDive. The steps were tested to install XL on Ubuntu 15.04. \n+\n+### Setting OS Parameters\n+\n+Set the following parameters in the `/etc/sysctl.conf` file: \n+```\n+kernel.sem = 1000  32000  32  1000\n+kernel.shmmax = 429496729600  #400GB\n+```\n+\n+After making these changes, run\n+```bash\n+sudo sysctl -p /etc/sysctl.conf\n+```\n+\n+Set the following parameter in the `/etc/ssh/ssh_config` file:\n+```\n+StrictHostKeyChecking no\n+```\n+\n+Finally, set the following parameter in the `/etc/ssh/sshd_config` file:\n+```\n+MaxStartups 100\n+```\n+After that, run\n+```bash\n+sudo service ssh restart\n+```\n+\n+### Building XL from Source\n+\n+```bash\n+sudo apt-get update\n+sudo apt-get -y install -y screen curl git rsync openssl locales openssh-server openssh-client\n+sudo apt-get -y install -y gcc flex bison make cmake jade openjade docbook docbook-dsssl\n+sudo apt-get -y install zlib1g-dev libreadline6-dev python-dev libssl-dev\n+sudo localedef -i en_US -c -f UTF-8 -A /usr/share/locale/locale.alias en_US.UTF-8\n+```\n+\n+Now set `$TARGET_DIR` to the directory into which you would like to install XL.\n+```bash\n+TARGET_DIR=/opt/pgxl\n+```\n+\n+Then add the following line(s) to `/etc/ld.so.conf` (replace `$TARGET_DIR` with dir):\n+```bash\n+/usr/local/lib\n+$TARGET_DIR/lib\n+```\n+\n+The remaining parts of the installation do not require sudo rights. However, make sure\n+the user account doing the installation has write permissions to `$TARGET_DIR`.\n+\n+Create a build directory and download the XL sources.\n+```bash\n+BUILD_DIR=~/pgxl_install\n+mkdir -p $BUILD_DIR\n+cd $BUILD_DIR\n+wget -O pgxl-v9.2.tar.gz http://sourceforge.net/projects/postgres-xl/files/Releases/Version_9.2rc/postgres-xl-v9.2-src.tar.gz/download\n+tar -xzf pgxl-v9.2.tar.gz\n+```\n+\n+Now, you can build the sources.\n+```bash\n+cd $BUILD_DIR/postgres-xl/\n+./configure --with-python --with-openssl --prefix $TARGET_DIR\n+make -j 4\n+make install\n+```\n+\n+Additionally modules can be added as follows.\n+```bash\n+for pkg in btree_gin btree_gist earthdistance fuzzystrmatch hstore intagg intarray oid2name \\\n+pg_buffercache pgcrypto pgxc_clean pgxc_ctl pgxc_ddl pgxc_monitor stormstats \\\n+tablefunc tsearch2 unaccent; do\n+    cd $BUILD_DIR/postgres-xl/contrib/$pkg\n+    make; make install\n+done\n+```\n+\n+### Set XL related session variables\n+\n+To set Greenplum related session variables, modify your\n+`~/.bashrc` script and add the line:\n+\n+```bash\n+export PATH=$TARGET_DIR/bin:\\$PATH\n+export PGXC_CTL_HOME=$TARGET_DIR/conf\n+```\n+\n+### Configure ssh with localhost\n+\n+Now you need to generate ssh keys for `localhost`. Run: \n+```bash\n+mkdir -p ~/.ssh\n+chmod 700 ~/.ssh\n+cd ~/.ssh\n+if [ ! -f id_rsa.pub ]; then\n+    ssh-keygen -t rsa -N \"\" -f id_rsa\n+fi\n+cat id_rsa.pub >> authorized_keys\n+chmod 600 authorized_keys\n+```\n+\n+Then you should be able to `ssh` into `localhost` without password, and you can\n+move on.\n+\n+### Create directories for the database\n+\n+Now set `$DATA_DIR` to the directory, where the database files will be stored.\n+**Be sure that you have write permission to this directory**.\n+\n+```bash\n+mkdir -p $DATA_DIR\n+```\n+\n+### Configure the XL database\n+\n+In order to run XL, we still need to create a file with configuration settings. Run\n+```bash\n+mkdir -p $TARGET_DIR/conf\n+```\n+\n+Now, assuming that `$USER` is set to the account under which pgxl should execute, the\n+following settings should provide a good starting point.\n+```bash\n+USER_DB_PORT=5432\n+MAX_USER_CONNECTIONS=100\n+DATA_NODE_SHARED_BUFFERS=\"2000MB\"\n+DATA_NODE_WORK_MEM=\"128MB\"\n+DATA_NODE_MAINTENANCE_MEM=\"128MB\"\n+DATA_NODE_WAL_BUFFERS=\"16MB\"\n+DATA_NODE_CHECKPOINT_SEGMENTS=\"256\"\n+```\n+\n+Create a file `$TARGET_DIR/conf/pgxc_ctl.conf` with the following contents (replace\n+variables with their values):\n+\n+```\n+pgxcOwner=$USER\n+pgxcUser=\\$pgxcOwner\n+tmpDir=/tmp\n+localTmpDir=\\$tmpDir\n+configBackup=n\n+pgxcInstallDir=$TARGET_DIR\n+\n+gtmName=gtm\n+gtmMasterServer=localhost\n+gtmMasterPort=20001\n+gtmMasterDir=$DATA_DIR/gtm\n+gtmSlave=n\n+gtmProxy=n\n+\n+coordMasterDirs=($DATA_DIR/coord)\n+coordNames=(coord)\n+coordMasterServers=(localhost)\n+coordPorts=($USER_DB_PORT)\n+poolerPorts=(20002)\n+coordMaxWALSenders=(5)\n+coordSlave=n\n+\n+datanodeMasterDirs=($DATA_DIR/data1 $DATA_DIR/data2)\n+datanodeNames=(data1 data2)\n+datanodeMasterServers=(localhost localhost)\n+datanodePorts=(3001 3002)\n+datanodePoolerPorts=(4001 4002)\n+datanodeMaxWALSenders=(5 5)\n+datanodeSpecificExtraConfig=(none none)\n+datanodeSpecificExtraPgHba=(none none)\n+datanodeSlave=n\n+\n+coordExtraConfig=coordExtraConfig\n+cat > \\$coordExtraConfig <<EOF\n+#================================================\n+# Added to all the coordinator postgresql.conf\n+log_destination = 'stderr'\n+logging_collector = on\n+log_directory = 'logs'\n+listen_addresses = '*'\n+log_filename = 'coordinator.log'\n+max_connections = $MAX_USER_CONNECTIONS\n+shared_buffers = $DATA_NODE_SHARED_BUFFERS\n+checkpoint_segments = $DATA_NODE_CHECKPOINT_SEGMENTS\n+work_mem = $DATA_NODE_WORK_MEM\n+maintenance_work_mem = $DATA_NODE_MAINTENANCE_MEM\n+wal_buffers = $DATA_NODE_WAL_BUFFERS\n+EOF\n+\n+datanodeExtraConfig=datanodeExtraConfig\n+cat > \\$datanodeExtraConfig <<EOF\n+#================================================\n+# Added to all the datanode postgresql.conf\n+log_destination = 'stderr'\n+logging_collector = on\n+log_directory = 'logs'\n+log_filename = 'datanode.log'\n+max_connections = $MAX_USER_CONNECTIONS\n+shared_buffers = $DATA_NODE_SHARED_BUFFERS\n+checkpoint_segments = $DATA_NODE_CHECKPOINT_SEGMENTS\n+work_mem = $DATA_NODE_WORK_MEM\n+maintenance_work_mem = $DATA_NODE_MAINTENANCE_MEM\n+wal_buffers = $DATA_NODE_WAL_BUFFERS\n+EOF\n+```\n+\n+Above configuration creates a cluster with two data nodes, one coordinator,\n+and one manager.\n+\n+### Configure PATH\n+\n+Configure the following paths into your `~/.bashrc`:\n+\n+```bash\n+export PATH=$TARGET_DIR/bin:$PATH\n+export PGXC_CTL_HOME=$TARGET_DIR/conf\n+```\n+\n+Run `source ~/.bashrc`.",
        "path": "doc/doc/advanced/pgxl.md",
        "position": null,
        "original_position": 250,
        "commit_id": "81cd9c21d7d118a02d92fad7f78b6872a7ee455a",
        "original_commit_id": "01c273ebb2f225b7f16b5f74e98271dd735a998b",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "This section is repeated (L113: Set XL related session variable).\n",
        "created_at": "2015-05-26T06:09:06Z",
        "updated_at": "2015-06-03T06:37:44Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/301#discussion_r31006494",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/301",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/31006494"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/301#discussion_r31006494"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/301"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32304136",
        "pull_request_review_id": null,
        "id": 32304136,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMyMzA0MTM2",
        "diff_hunk": "@@ -17,10 +17,15 @@ class contains different extractor *styles*:\n \n - Row-wise extractors:\n   - [`json_extractor`](#json_extractor): highly flexible and compatible with\n-  previous systems, but with limited performance \n+  previous systems, but with limited performance",
        "path": "doc/doc/basics/extractors.md",
        "position": 5,
        "original_position": 5,
        "commit_id": "897f96ff379fa91fdb2b04f75bca957c932b35c7",
        "original_commit_id": "c83ceabdeaead8e4efda66bdc1577c000782763f",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Could you please keep the trailing whitespace cleanup in a separate PR?  It's really hard to see what's really changed.  They should've not introduced in the first place, but I think it's even worse if we clean them up in random commits.\n",
        "created_at": "2015-06-12T10:03:24Z",
        "updated_at": "2015-06-17T20:38:40Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/319#discussion_r32304136",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/319",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32304136"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/319#discussion_r32304136"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/319"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32328055",
        "pull_request_review_id": null,
        "id": 32328055,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMyMzI4MDU1",
        "diff_hunk": "@@ -17,10 +17,15 @@ class contains different extractor *styles*:\n \n - Row-wise extractors:\n   - [`json_extractor`](#json_extractor): highly flexible and compatible with\n-  previous systems, but with limited performance \n+  previous systems, but with limited performance",
        "path": "doc/doc/basics/extractors.md",
        "position": 5,
        "original_position": 5,
        "commit_id": "897f96ff379fa91fdb2b04f75bca957c932b35c7",
        "original_commit_id": "c83ceabdeaead8e4efda66bdc1577c000782763f",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Last time I tried reverting space changes, I ended up destroying my working copy...\n\nGood news: Adding 'w=1' to the URL suppresses whitespace differences: https://github.com/tiimgreen/github-cheat-sheet#ignore-whitespace\n\nAlso, if we switch to Phabricator, we get smarter diff by default, together with lots of configurable linters.\n",
        "created_at": "2015-06-12T15:51:53Z",
        "updated_at": "2015-06-17T20:38:40Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/319#discussion_r32328055",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/319",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32328055"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/319#discussion_r32328055"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/319"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32353885",
        "pull_request_review_id": null,
        "id": 32353885,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMyMzUzODg1",
        "diff_hunk": "@@ -17,10 +17,15 @@ class contains different extractor *styles*:\n \n - Row-wise extractors:\n   - [`json_extractor`](#json_extractor): highly flexible and compatible with\n-  previous systems, but with limited performance \n+  previous systems, but with limited performance",
        "path": "doc/doc/basics/extractors.md",
        "position": 5,
        "original_position": 5,
        "commit_id": "897f96ff379fa91fdb2b04f75bca957c932b35c7",
        "original_commit_id": "c83ceabdeaead8e4efda66bdc1577c000782763f",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I'm worried because the whitespace confuses git.  Yes, we humans can use brain or tools to figure out what's different, but once such commit mixed with a lot of unrelated changes is put into history, it's really hard to backport, merge with other branches, etc.  Sorry, but it was a big pain to merge conflicts created by some of your past commits again and again: it's really a disaster for integrators and release engineers.  If your editor is stripping all the trailing whitespaces, I would ask you to disable that setting temporarily for DeepDive until we do a full cleanup on every file and add a style check to ban them forever :)  For this one I'll try to merge as it is right after the release, hopefully without much trouble.\n",
        "created_at": "2015-06-12T20:51:39Z",
        "updated_at": "2015-06-17T20:38:40Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/319#discussion_r32353885",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/319",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32353885"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/319#discussion_r32353885"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/319"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32354842",
        "pull_request_review_id": null,
        "id": 32354842,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMyMzU0ODQy",
        "diff_hunk": "@@ -17,10 +17,15 @@ class contains different extractor *styles*:\n \n - Row-wise extractors:\n   - [`json_extractor`](#json_extractor): highly flexible and compatible with\n-  previous systems, but with limited performance \n+  previous systems, but with limited performance",
        "path": "doc/doc/basics/extractors.md",
        "position": 5,
        "original_position": 5,
        "commit_id": "897f96ff379fa91fdb2b04f75bca957c932b35c7",
        "original_commit_id": "c83ceabdeaead8e4efda66bdc1577c000782763f",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I see. Sorry about the trouble... I'll rebase after the release and handle the conflicts from my side so that this change stays automatically mergeable.\n\nIf the conflicts you mentioned were b/w master and develop, I think the real solution is to have one conceptual \"master\" branch and avoid changes flying in both directions (colliding with each other).\n",
        "created_at": "2015-06-12T21:04:35Z",
        "updated_at": "2015-06-17T20:38:40Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/319#discussion_r32354842",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/319",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32354842"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/319#discussion_r32354842"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/319"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32355223",
        "pull_request_review_id": null,
        "id": 32355223,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMyMzU1MjIz",
        "diff_hunk": "@@ -17,10 +17,15 @@ class contains different extractor *styles*:\n \n - Row-wise extractors:\n   - [`json_extractor`](#json_extractor): highly flexible and compatible with\n-  previous systems, but with limited performance \n+  previous systems, but with limited performance",
        "path": "doc/doc/basics/extractors.md",
        "position": 5,
        "original_position": 5,
        "commit_id": "897f96ff379fa91fdb2b04f75bca957c932b35c7",
        "original_commit_id": "c83ceabdeaead8e4efda66bdc1577c000782763f",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "That'd be great!  The conflicts were mainly between ones trying to get into develop.  Well, master and develop is another thing we need to iron out, and we'll have a true \"master\" after this release with a clear doc on the branching/release policy.\n",
        "created_at": "2015-06-12T21:09:34Z",
        "updated_at": "2015-06-17T20:38:40Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/319#discussion_r32355223",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/319",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32355223"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/319#discussion_r32355223"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/319"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32355881",
        "pull_request_review_id": null,
        "id": 32355881,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMyMzU1ODgx",
        "diff_hunk": "@@ -17,10 +17,15 @@ class contains different extractor *styles*:\n \n - Row-wise extractors:\n   - [`json_extractor`](#json_extractor): highly flexible and compatible with\n-  previous systems, but with limited performance \n+  previous systems, but with limited performance",
        "path": "doc/doc/basics/extractors.md",
        "position": 5,
        "original_position": 5,
        "commit_id": "897f96ff379fa91fdb2b04f75bca957c932b35c7",
        "original_commit_id": "c83ceabdeaead8e4efda66bdc1577c000782763f",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Start adopting Phabricator today, and it'll become the contributor's responsibility (as opposed to the maintainer's) to handle conflicts (rebase etc.)! :)\n",
        "created_at": "2015-06-12T21:18:25Z",
        "updated_at": "2015-06-17T20:38:40Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/319#discussion_r32355881",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/319",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32355881"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/319#discussion_r32355881"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/319"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32356154",
        "pull_request_review_id": null,
        "id": 32356154,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMyMzU2MTU0",
        "diff_hunk": "@@ -17,10 +17,15 @@ class contains different extractor *styles*:\n \n - Row-wise extractors:\n   - [`json_extractor`](#json_extractor): highly flexible and compatible with\n-  previous systems, but with limited performance \n+  previous systems, but with limited performance",
        "path": "doc/doc/basics/extractors.md",
        "position": 5,
        "original_position": 5,
        "commit_id": "897f96ff379fa91fdb2b04f75bca957c932b35c7",
        "original_commit_id": "c83ceabdeaead8e4efda66bdc1577c000782763f",
        "user": {
            "login": "chrismre",
            "id": 7597496,
            "node_id": "MDQ6VXNlcjc1OTc0OTY=",
            "avatar_url": "https://avatars1.githubusercontent.com/u/7597496?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/chrismre",
            "html_url": "https://github.com/chrismre",
            "followers_url": "https://api.github.com/users/chrismre/followers",
            "following_url": "https://api.github.com/users/chrismre/following{/other_user}",
            "gists_url": "https://api.github.com/users/chrismre/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/chrismre/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/chrismre/subscriptions",
            "organizations_url": "https://api.github.com/users/chrismre/orgs",
            "repos_url": "https://api.github.com/users/chrismre/repos",
            "events_url": "https://api.github.com/users/chrismre/events{/privacy}",
            "received_events_url": "https://api.github.com/users/chrismre/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Feng, with strong opinions about tools? At least I have no strong opinions\n:)\n\nJaeho is the boss :)\n\nOn Fri, Jun 12, 2015 at 2:18 PM, alldefector notifications@github.com\nwrote:\n\n> In doc/doc/basics/extractors.md\n> https://github.com/HazyResearch/deepdive/pull/319#discussion_r32355881:\n> \n> > @@ -17,10 +17,15 @@ class contains different extractor _styles_:\n> > - Row-wise extractors:\n> >   - [`json_extractor`](#json_extractor): highly flexible and compatible with\n> >   -  previous systems, but with limited performance\n> >   -  previous systems, but with limited performance\n> \n> Start adopting Phabricator today, and it'll become the contributor's\n> responsibility (as opposed to the maintainer's) to handle conflicts (rebase\n> etc.)! :)\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/HazyResearch/deepdive/pull/319/files#r32355881.\n",
        "created_at": "2015-06-12T21:21:34Z",
        "updated_at": "2015-06-17T20:38:40Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/319#discussion_r32356154",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/319",
        "author_association": "NONE",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32356154"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/319#discussion_r32356154"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/319"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32356553",
        "pull_request_review_id": null,
        "id": 32356553,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMyMzU2NTUz",
        "diff_hunk": "@@ -17,10 +17,15 @@ class contains different extractor *styles*:\n \n - Row-wise extractors:\n   - [`json_extractor`](#json_extractor): highly flexible and compatible with\n-  previous systems, but with limited performance \n+  previous systems, but with limited performance",
        "path": "doc/doc/basics/extractors.md",
        "position": 5,
        "original_position": 5,
        "commit_id": "897f96ff379fa91fdb2b04f75bca957c932b35c7",
        "original_commit_id": "c83ceabdeaead8e4efda66bdc1577c000782763f",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I was mostly joking :) And github isn't bad at all! I was just really impressed by FB's tool that I couldn't help but to peddle it all the time :)\n",
        "created_at": "2015-06-12T21:26:01Z",
        "updated_at": "2015-06-17T20:38:40Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/319#discussion_r32356553",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/319",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32356553"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/319#discussion_r32356553"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/319"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32807574",
        "pull_request_review_id": null,
        "id": 32807574,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMyODA3NTc0",
        "diff_hunk": "@@ -20,7 +20,12 @@ class contains different extractor *styles*:\n   previous systems, but with limited performance\n   - [`tsv_extractor`](#tsv_extractor): moderate flexibility and performance\n   - [`plpy_extractor`](#plpy_extractor): parallel database-built-in extractors\n-  with restricted flexibility\n+  with seriously restricted flexibility because only the UDF function definition\n+  is shipped to the DB\n+  - [`piggy_extractor`](#piggy_extractor): a distributed language-agnostic pipe-based extractor driver.\n+  We ship the entire runtime environment (code, data, and pip libraries) to\n+  the DB server(s) and avoid multi-user collision by staging separate environments\n+  for concurrent users. See `examples/spouse_example/piggy_extractor/`.",
        "path": "doc/doc/basics/extractors.md",
        "position": null,
        "original_position": 10,
        "commit_id": "c904e82c9b44bd1140b566e286bec6ca2771e29d",
        "original_commit_id": "1fdb4fa2ac434edd5646d826e91e5622f1c8a654",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Could you add more detail about using piggy?  There's no `#piggy_extractor` section defined here.  For example, what extra config is needed, how piggy knows what to ship, which databases are supported, etc.  It's probably a bad idea to have users stare at the piggy example and expect them to figure out what's changed from tsv or other examples.\n",
        "created_at": "2015-06-19T07:46:09Z",
        "updated_at": "2015-06-20T00:39:30Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/326#discussion_r32807574",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/326",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32807574"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/326#discussion_r32807574"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/326"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32807615",
        "pull_request_review_id": null,
        "id": 32807615,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMyODA3NjE1",
        "diff_hunk": "@@ -0,0 +1,56 @@\n+deepdive {\n+\n+  db.default {\n+    driver: \"org.postgresql.Driver\"\n+    url: \"jdbc:postgresql://\"${PGHOST}\":\"${PGPORT}\"/\"${DBNAME} # \"\n+    user: ${PGUSER}\n+    password: ${PGPASSWORD}\n+    dbname: ${DBNAME}\n+    host: ${PGHOST}\n+    port: ${PGPORT}\n+  }\n+\n+  pipeline.run: \"all\"\n+  pipeline.pipelines.all: [\n+    \"ext_clear_table\",\n+    \"ext_people\",\n+    \"ext_spouses\",\n+  ]\n+\n+  extraction.extractors {\n+\n+    ext_clear_table {\n+      style: \"sql_extractor\"\n+      sql: \"\"\"\n+        DELETE FROM people_mentions;\n+        DELETE FROM has_spouse;\n+        DELETE FROM has_spouse_features;\n+        \"\"\"\n+    }\n+\n+    ext_people {\n+      input: \"\"\"\n+          SELECT  sentence_id,\n+                  words,\n+                  ner_tags\n+          FROM    sentences\n+          \"\"\"\n+      output_relation: \"people_mentions\"\n+      udf_dir: ${APP_HOME}\"/udfs\"\n+      udf: \"python people.py\"\n+      dependencies: [\"ext_clear_table\"]\n+      style: \"piggy_extractor\"\n+    }\n+\n+    ext_spouses {\n+      input: \"people_mentions\"\n+      output_relation: \"has_spouse(sentence_id, person1_id, person2_id)\"\n+      udf_dir: ${APP_HOME}\"/udfs\"",
        "path": "examples/spouse_example/piggy_extractor/application.conf",
        "position": null,
        "original_position": 48,
        "commit_id": "c904e82c9b44bd1140b566e286bec6ca2771e29d",
        "original_commit_id": "1fdb4fa2ac434edd5646d826e91e5622f1c8a654",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "It seems `udf_dir` is what's being shipped by piggy, right?  A comment around this line would be extremely helpful if so.\n",
        "created_at": "2015-06-19T07:47:08Z",
        "updated_at": "2015-06-20T00:39:30Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/326#discussion_r32807615",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/326",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32807615"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/326#discussion_r32807615"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/326"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32807683",
        "pull_request_review_id": null,
        "id": 32807683,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMyODA3Njgz",
        "diff_hunk": "@@ -17,4 +20,31 @@ object FileDataUtils extends Logging {\n       reader.close()\n     }\n   }\n+\n+  def recursiveListFiles(dir: File): Array[File] = {",
        "path": "src/main/scala/org/deepdive/datastore/FileDataUtils.scala",
        "position": 15,
        "original_position": 15,
        "commit_id": "c904e82c9b44bd1140b566e286bec6ca2771e29d",
        "original_commit_id": "1fdb4fa2ac434edd5646d826e91e5622f1c8a654",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Could you add unit tests for these extra functions?  The low test coverage got even lower. :(\n",
        "created_at": "2015-06-19T07:48:28Z",
        "updated_at": "2015-06-20T00:39:30Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/326#discussion_r32807683",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/326",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32807683"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/326#discussion_r32807683"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/326"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32807862",
        "pull_request_review_id": null,
        "id": 32807862,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMyODA3ODYy",
        "diff_hunk": "@@ -171,4 +172,23 @@ object Helpers extends Logging {\n         s\"\"\" --silent -N -e '${query.replaceAll(\"'\", \"'\\\\\\\\''\")}' \"\"\"\n     }\n   }\n+\n+\n+  def slugify(input: String): String = {",
        "path": "src/main/scala/org/deepdive/helpers/Helpers.scala",
        "position": 14,
        "original_position": 14,
        "commit_id": "c904e82c9b44bd1140b566e286bec6ca2771e29d",
        "original_commit_id": "1fdb4fa2ac434edd5646d826e91e5622f1c8a654",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Unit tests for these as well.  Not sure why this helper is needed from where.  Would be helpful if there's a brief comment.\n",
        "created_at": "2015-06-19T07:51:19Z",
        "updated_at": "2015-06-20T00:39:30Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/326#discussion_r32807862",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/326",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32807862"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/326#discussion_r32807862"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/326"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32808660",
        "pull_request_review_id": null,
        "id": 32808660,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMyODA4NjYw",
        "diff_hunk": "@@ -0,0 +1,339 @@\n+import hashlib\n+import inspect\n+import json\n+import os\n+import sys\n+import time\n+\n+\n+def piggy_prepare_run(dir, script, source, target, is_pgxl):\n+\n+    date = time.strftime('%Y%m%d')\n+    fp = date + '_' + hashlib.sha1('\\t'.join([dir, script, source, target])).hexdigest()[:8]\n+    fname = 'piggy_func_' + fp\n+    sview = 'piggy_source_' + fp\n+    tview = 'piggy_target_' + fp\n+\n+    if ' ' not in source:\n+        source_query = 'SELECT * FROM ' + source\n+    else:\n+        source_query = source\n+\n+    if '(' not in target:\n+        target_query = 'SELECT * FROM ' + target\n+    else:\n+        tname, tcols = target.strip(' )').split('(')\n+        target_query = 'SELECT %s FROM %s' % (tcols, tname)\n+\n+    logfile_path = os.path.join(dir, fname + '.log')\n+    if os.path.isfile(logfile_path):\n+        os.remove(logfile_path)\n+\n+    ts_format = '%m-%d %H:%M:%S'\n+\n+    # Why TABLE not VIEW?\n+    # Because PGXL does not create views on data nodes.\n+\n+    sql_create_tables = '''\n+    DROP TABLE IF EXISTS %(sview)s CASCADE;\n+    DROP TABLE IF EXISTS %(tview)s CASCADE;\n+    CREATE TABLE %(sview)s AS SELECT * FROM (%(source_query)s) _sview_source LIMIT 0;\n+    CREATE TABLE %(tview)s AS %(target_query)s LIMIT 0;\n+    ''' % {\n+        'sview': sview,\n+        'tview': tview,\n+        'source_query': source_query,\n+        'target_query': target_query,\n+    }\n+\n+\n+    # Why log to file instead of a DB table?\n+    # PGXL would complain something like 'no XID'.\n+    # This may have fixed it though: https://github.com/snaga/postgres-xl/commit/d4136935a1d741c61ada13ef8c5ae44f68162cc9\n+\n+    sql_create_functions = '''\n+    CREATE OR REPLACE FUNCTION %(fname)s (record %(sview)s)\n+        RETURNS SETOF %(tview)s\n+    AS $func$\n+\n+        # NULL input tuple indicates end of stream.\n+        # It's time to flush output and clean up.\n+        if record is None:\n+            import os\n+            if 'piggy' not in SD:\n+                raise StopIteration\n+            piggy = SD['piggy']\n+            try:\n+                piggy.log('Grabbing final tuples from the buffer.')\n+                for x in piggy.pull_final():\n+                    yield x\n+            finally:\n+                del SD['piggy']\n+            raise StopIteration\n+\n+        # First input tuple; we setup the worker process.\n+        if 'piggy' not in SD:\n+            import errno\n+            import fcntl\n+            import json\n+            import os\n+            import socket\n+            import subprocess\n+            import time\n+            from datetime import datetime\n+            from Queue import Queue, Empty, Full\n+            from threading import Thread\n+\n+            # Create a pipe for logs.\n+            # Only one process would succeed on the same host.\n+            log_pipe_name = os.path.join('%(dir)s', '%(fname)s.pipe')\n+            try:\n+                os.mkfifo(log_pipe_name)",
        "path": "util/piggy_prepare.py",
        "position": 91,
        "original_position": 91,
        "commit_id": "c904e82c9b44bd1140b566e286bec6ca2771e29d",
        "original_commit_id": "1fdb4fa2ac434edd5646d826e91e5622f1c8a654",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I don't think it's a good idea to use named pipes for logging.  These log pipes will quickly become the bottleneck for any real app because all udfs will block on their `piggy.log()` calls until the `*_getlog` FUNCTION drains the pipe.  It seems the `GetLogThread` will only consume up to a megabyte from the pipe only once every second.  I think buffering in a temporary file is better in this case.  Also providing an easy way to toggle `piggy.log()` would be better than asking users to comment out all those calls.  Another way could be optionally starting GetLogThread based on DeepDive's log level and `piggy.log()` ignoring the passed lines if no thread will be reading, but this sounds too complicated already.\n",
        "created_at": "2015-06-19T08:05:53Z",
        "updated_at": "2015-06-20T00:39:30Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/326#discussion_r32808660",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/326",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32808660"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/326#discussion_r32808660"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/326"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32811770",
        "pull_request_review_id": null,
        "id": 32811770,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMyODExNzcw",
        "diff_hunk": "@@ -17,4 +20,31 @@ object FileDataUtils extends Logging {\n       reader.close()\n     }\n   }\n+\n+  def recursiveListFiles(dir: File): Array[File] = {",
        "path": "src/main/scala/org/deepdive/datastore/FileDataUtils.scala",
        "position": 15,
        "original_position": 15,
        "commit_id": "c904e82c9b44bd1140b566e286bec6ca2771e29d",
        "original_commit_id": "1fdb4fa2ac434edd5646d826e91e5622f1c8a654",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "done\n",
        "created_at": "2015-06-19T08:55:03Z",
        "updated_at": "2015-06-20T00:39:30Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/326#discussion_r32811770",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/326",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32811770"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/326#discussion_r32811770"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/326"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32811826",
        "pull_request_review_id": null,
        "id": 32811826,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMyODExODI2",
        "diff_hunk": "@@ -171,4 +172,23 @@ object Helpers extends Logging {\n         s\"\"\" --silent -N -e '${query.replaceAll(\"'\", \"'\\\\\\\\''\")}' \"\"\"\n     }\n   }\n+\n+\n+  def slugify(input: String): String = {",
        "path": "src/main/scala/org/deepdive/helpers/Helpers.scala",
        "position": 14,
        "original_position": 14,
        "commit_id": "c904e82c9b44bd1140b566e286bec6ca2771e29d",
        "original_commit_id": "1fdb4fa2ac434edd5646d826e91e5622f1c8a654",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "It's used for use client hostname as part of the path of distributed package copies. It's a standard procedure in web apps. Added a test.\n",
        "created_at": "2015-06-19T08:55:54Z",
        "updated_at": "2015-06-20T00:39:30Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/326#discussion_r32811826",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/326",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32811826"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/326#discussion_r32811826"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/326"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32812138",
        "pull_request_review_id": null,
        "id": 32812138,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMyODEyMTM4",
        "diff_hunk": "@@ -0,0 +1,339 @@\n+import hashlib\n+import inspect\n+import json\n+import os\n+import sys\n+import time\n+\n+\n+def piggy_prepare_run(dir, script, source, target, is_pgxl):\n+\n+    date = time.strftime('%Y%m%d')\n+    fp = date + '_' + hashlib.sha1('\\t'.join([dir, script, source, target])).hexdigest()[:8]\n+    fname = 'piggy_func_' + fp\n+    sview = 'piggy_source_' + fp\n+    tview = 'piggy_target_' + fp\n+\n+    if ' ' not in source:\n+        source_query = 'SELECT * FROM ' + source\n+    else:\n+        source_query = source\n+\n+    if '(' not in target:\n+        target_query = 'SELECT * FROM ' + target\n+    else:\n+        tname, tcols = target.strip(' )').split('(')\n+        target_query = 'SELECT %s FROM %s' % (tcols, tname)\n+\n+    logfile_path = os.path.join(dir, fname + '.log')\n+    if os.path.isfile(logfile_path):\n+        os.remove(logfile_path)\n+\n+    ts_format = '%m-%d %H:%M:%S'\n+\n+    # Why TABLE not VIEW?\n+    # Because PGXL does not create views on data nodes.\n+\n+    sql_create_tables = '''\n+    DROP TABLE IF EXISTS %(sview)s CASCADE;\n+    DROP TABLE IF EXISTS %(tview)s CASCADE;\n+    CREATE TABLE %(sview)s AS SELECT * FROM (%(source_query)s) _sview_source LIMIT 0;\n+    CREATE TABLE %(tview)s AS %(target_query)s LIMIT 0;\n+    ''' % {\n+        'sview': sview,\n+        'tview': tview,\n+        'source_query': source_query,\n+        'target_query': target_query,\n+    }\n+\n+\n+    # Why log to file instead of a DB table?\n+    # PGXL would complain something like 'no XID'.\n+    # This may have fixed it though: https://github.com/snaga/postgres-xl/commit/d4136935a1d741c61ada13ef8c5ae44f68162cc9\n+\n+    sql_create_functions = '''\n+    CREATE OR REPLACE FUNCTION %(fname)s (record %(sview)s)\n+        RETURNS SETOF %(tview)s\n+    AS $func$\n+\n+        # NULL input tuple indicates end of stream.\n+        # It's time to flush output and clean up.\n+        if record is None:\n+            import os\n+            if 'piggy' not in SD:\n+                raise StopIteration\n+            piggy = SD['piggy']\n+            try:\n+                piggy.log('Grabbing final tuples from the buffer.')\n+                for x in piggy.pull_final():\n+                    yield x\n+            finally:\n+                del SD['piggy']\n+            raise StopIteration\n+\n+        # First input tuple; we setup the worker process.\n+        if 'piggy' not in SD:\n+            import errno\n+            import fcntl\n+            import json\n+            import os\n+            import socket\n+            import subprocess\n+            import time\n+            from datetime import datetime\n+            from Queue import Queue, Empty, Full\n+            from threading import Thread\n+\n+            # Create a pipe for logs.\n+            # Only one process would succeed on the same host.\n+            log_pipe_name = os.path.join('%(dir)s', '%(fname)s.pipe')\n+            try:\n+                os.mkfifo(log_pipe_name)",
        "path": "util/piggy_prepare.py",
        "position": 91,
        "original_position": 91,
        "commit_id": "c904e82c9b44bd1140b566e286bec6ca2771e29d",
        "original_commit_id": "1fdb4fa2ac434edd5646d826e91e5622f1c8a654",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Is there a pipe-like mechanism not subject to the small capacity limit that I could use? I used temp files earlier but it requires repeated scans (or complicated error-prone offset bookkeeping); so ideally I'd like to not go back to that approach. Any suggestions? Anything from unix that's like an append-only sorted string table?\n\nI could use `tail -c XXX` and keep the running offset XXX. But log flushing is an issue (too often can be inefficient; too infrequent results in delays. Maybe flush every 1-2 seconds?\n\nAnother idea would be to have the `_getlog` function start a standing thread to suck from the log pipe while accumulating in memory (periodically drained by function calls). We can cap the size of the buffer and evict old logs if limit is reached; but that's only when more than say 10MB of log is generated in one second.\n",
        "created_at": "2015-06-19T09:00:05Z",
        "updated_at": "2015-06-20T00:39:30Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/326#discussion_r32812138",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/326",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32812138"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/326#discussion_r32812138"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/326"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32819163",
        "pull_request_review_id": null,
        "id": 32819163,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMyODE5MTYz",
        "diff_hunk": "@@ -0,0 +1,339 @@\n+import hashlib\n+import inspect\n+import json\n+import os\n+import sys\n+import time\n+\n+\n+def piggy_prepare_run(dir, script, source, target, is_pgxl):\n+\n+    date = time.strftime('%Y%m%d')\n+    fp = date + '_' + hashlib.sha1('\\t'.join([dir, script, source, target])).hexdigest()[:8]\n+    fname = 'piggy_func_' + fp\n+    sview = 'piggy_source_' + fp\n+    tview = 'piggy_target_' + fp\n+\n+    if ' ' not in source:\n+        source_query = 'SELECT * FROM ' + source\n+    else:\n+        source_query = source\n+\n+    if '(' not in target:\n+        target_query = 'SELECT * FROM ' + target\n+    else:\n+        tname, tcols = target.strip(' )').split('(')\n+        target_query = 'SELECT %s FROM %s' % (tcols, tname)\n+\n+    logfile_path = os.path.join(dir, fname + '.log')\n+    if os.path.isfile(logfile_path):\n+        os.remove(logfile_path)\n+\n+    ts_format = '%m-%d %H:%M:%S'\n+\n+    # Why TABLE not VIEW?\n+    # Because PGXL does not create views on data nodes.\n+\n+    sql_create_tables = '''\n+    DROP TABLE IF EXISTS %(sview)s CASCADE;\n+    DROP TABLE IF EXISTS %(tview)s CASCADE;\n+    CREATE TABLE %(sview)s AS SELECT * FROM (%(source_query)s) _sview_source LIMIT 0;\n+    CREATE TABLE %(tview)s AS %(target_query)s LIMIT 0;\n+    ''' % {\n+        'sview': sview,\n+        'tview': tview,\n+        'source_query': source_query,\n+        'target_query': target_query,\n+    }\n+\n+\n+    # Why log to file instead of a DB table?\n+    # PGXL would complain something like 'no XID'.\n+    # This may have fixed it though: https://github.com/snaga/postgres-xl/commit/d4136935a1d741c61ada13ef8c5ae44f68162cc9\n+\n+    sql_create_functions = '''\n+    CREATE OR REPLACE FUNCTION %(fname)s (record %(sview)s)\n+        RETURNS SETOF %(tview)s\n+    AS $func$\n+\n+        # NULL input tuple indicates end of stream.\n+        # It's time to flush output and clean up.\n+        if record is None:\n+            import os\n+            if 'piggy' not in SD:\n+                raise StopIteration\n+            piggy = SD['piggy']\n+            try:\n+                piggy.log('Grabbing final tuples from the buffer.')\n+                for x in piggy.pull_final():\n+                    yield x\n+            finally:\n+                del SD['piggy']\n+            raise StopIteration\n+\n+        # First input tuple; we setup the worker process.\n+        if 'piggy' not in SD:\n+            import errno\n+            import fcntl\n+            import json\n+            import os\n+            import socket\n+            import subprocess\n+            import time\n+            from datetime import datetime\n+            from Queue import Queue, Empty, Full\n+            from threading import Thread\n+\n+            # Create a pipe for logs.\n+            # Only one process would succeed on the same host.\n+            log_pipe_name = os.path.join('%(dir)s', '%(fname)s.pipe')\n+            try:\n+                os.mkfifo(log_pipe_name)",
        "path": "util/piggy_prepare.py",
        "position": 91,
        "original_position": 91,
        "commit_id": "c904e82c9b44bd1140b566e286bec6ca2771e29d",
        "original_commit_id": "1fdb4fa2ac434edd5646d826e91e5622f1c8a654",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Implemented the last option: https://github.com/HazyResearch/deepdive/commit/f6f6ef5f9525c8623e83e7f55c51c5a72e3db8ae\n\nHowever, DD _always_ gets complete log -- apparently due to some lack of flushing? Could you take a look and see what might be wrong? (I've tried various buffering options for both reader and writer of the named pipe.)\n",
        "created_at": "2015-06-19T10:51:29Z",
        "updated_at": "2015-06-20T00:39:30Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/326#discussion_r32819163",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/326",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32819163"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/326#discussion_r32819163"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/326"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32868802",
        "pull_request_review_id": null,
        "id": 32868802,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMyODY4ODAy",
        "diff_hunk": "@@ -0,0 +1,339 @@\n+import hashlib\n+import inspect\n+import json\n+import os\n+import sys\n+import time\n+\n+\n+def piggy_prepare_run(dir, script, source, target, is_pgxl):\n+\n+    date = time.strftime('%Y%m%d')\n+    fp = date + '_' + hashlib.sha1('\\t'.join([dir, script, source, target])).hexdigest()[:8]\n+    fname = 'piggy_func_' + fp\n+    sview = 'piggy_source_' + fp\n+    tview = 'piggy_target_' + fp\n+\n+    if ' ' not in source:\n+        source_query = 'SELECT * FROM ' + source\n+    else:\n+        source_query = source\n+\n+    if '(' not in target:\n+        target_query = 'SELECT * FROM ' + target\n+    else:\n+        tname, tcols = target.strip(' )').split('(')\n+        target_query = 'SELECT %s FROM %s' % (tcols, tname)\n+\n+    logfile_path = os.path.join(dir, fname + '.log')\n+    if os.path.isfile(logfile_path):\n+        os.remove(logfile_path)\n+\n+    ts_format = '%m-%d %H:%M:%S'\n+\n+    # Why TABLE not VIEW?\n+    # Because PGXL does not create views on data nodes.\n+\n+    sql_create_tables = '''\n+    DROP TABLE IF EXISTS %(sview)s CASCADE;\n+    DROP TABLE IF EXISTS %(tview)s CASCADE;\n+    CREATE TABLE %(sview)s AS SELECT * FROM (%(source_query)s) _sview_source LIMIT 0;\n+    CREATE TABLE %(tview)s AS %(target_query)s LIMIT 0;\n+    ''' % {\n+        'sview': sview,\n+        'tview': tview,\n+        'source_query': source_query,\n+        'target_query': target_query,\n+    }\n+\n+\n+    # Why log to file instead of a DB table?\n+    # PGXL would complain something like 'no XID'.\n+    # This may have fixed it though: https://github.com/snaga/postgres-xl/commit/d4136935a1d741c61ada13ef8c5ae44f68162cc9\n+\n+    sql_create_functions = '''\n+    CREATE OR REPLACE FUNCTION %(fname)s (record %(sview)s)\n+        RETURNS SETOF %(tview)s\n+    AS $func$\n+\n+        # NULL input tuple indicates end of stream.\n+        # It's time to flush output and clean up.\n+        if record is None:\n+            import os\n+            if 'piggy' not in SD:\n+                raise StopIteration\n+            piggy = SD['piggy']\n+            try:\n+                piggy.log('Grabbing final tuples from the buffer.')\n+                for x in piggy.pull_final():\n+                    yield x\n+            finally:\n+                del SD['piggy']\n+            raise StopIteration\n+\n+        # First input tuple; we setup the worker process.\n+        if 'piggy' not in SD:\n+            import errno\n+            import fcntl\n+            import json\n+            import os\n+            import socket\n+            import subprocess\n+            import time\n+            from datetime import datetime\n+            from Queue import Queue, Empty, Full\n+            from threading import Thread\n+\n+            # Create a pipe for logs.\n+            # Only one process would succeed on the same host.\n+            log_pipe_name = os.path.join('%(dir)s', '%(fname)s.pipe')\n+            try:\n+                os.mkfifo(log_pipe_name)",
        "path": "util/piggy_prepare.py",
        "position": 91,
        "original_position": 91,
        "commit_id": "c904e82c9b44bd1140b566e286bec6ca2771e29d",
        "original_commit_id": "1fdb4fa2ac434edd5646d826e91e5622f1c8a654",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I'm not sure I understand what you mean by log flushing.  This seems to be getting overcomplicated.  I think we shouldn't waste time reinventing a very limited remote shell as a Python hack.  It appears to me most of these issues would just go away with a simple `tail -F` over `ssh` to each host.  Can you make DD simply spawn a `for h in ${hosts}; do ssh $h tail -f ${log_pipe_name} & done; wait` command from GetLogThread and hang up all ssh processes once the thread has to die?\n",
        "created_at": "2015-06-19T20:58:39Z",
        "updated_at": "2015-06-20T00:39:30Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/326#discussion_r32868802",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/326",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32868802"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/326#discussion_r32868802"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/326"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32870964",
        "pull_request_review_id": null,
        "id": 32870964,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMyODcwOTY0",
        "diff_hunk": "@@ -0,0 +1,339 @@\n+import hashlib\n+import inspect\n+import json\n+import os\n+import sys\n+import time\n+\n+\n+def piggy_prepare_run(dir, script, source, target, is_pgxl):\n+\n+    date = time.strftime('%Y%m%d')\n+    fp = date + '_' + hashlib.sha1('\\t'.join([dir, script, source, target])).hexdigest()[:8]\n+    fname = 'piggy_func_' + fp\n+    sview = 'piggy_source_' + fp\n+    tview = 'piggy_target_' + fp\n+\n+    if ' ' not in source:\n+        source_query = 'SELECT * FROM ' + source\n+    else:\n+        source_query = source\n+\n+    if '(' not in target:\n+        target_query = 'SELECT * FROM ' + target\n+    else:\n+        tname, tcols = target.strip(' )').split('(')\n+        target_query = 'SELECT %s FROM %s' % (tcols, tname)\n+\n+    logfile_path = os.path.join(dir, fname + '.log')\n+    if os.path.isfile(logfile_path):\n+        os.remove(logfile_path)\n+\n+    ts_format = '%m-%d %H:%M:%S'\n+\n+    # Why TABLE not VIEW?\n+    # Because PGXL does not create views on data nodes.\n+\n+    sql_create_tables = '''\n+    DROP TABLE IF EXISTS %(sview)s CASCADE;\n+    DROP TABLE IF EXISTS %(tview)s CASCADE;\n+    CREATE TABLE %(sview)s AS SELECT * FROM (%(source_query)s) _sview_source LIMIT 0;\n+    CREATE TABLE %(tview)s AS %(target_query)s LIMIT 0;\n+    ''' % {\n+        'sview': sview,\n+        'tview': tview,\n+        'source_query': source_query,\n+        'target_query': target_query,\n+    }\n+\n+\n+    # Why log to file instead of a DB table?\n+    # PGXL would complain something like 'no XID'.\n+    # This may have fixed it though: https://github.com/snaga/postgres-xl/commit/d4136935a1d741c61ada13ef8c5ae44f68162cc9\n+\n+    sql_create_functions = '''\n+    CREATE OR REPLACE FUNCTION %(fname)s (record %(sview)s)\n+        RETURNS SETOF %(tview)s\n+    AS $func$\n+\n+        # NULL input tuple indicates end of stream.\n+        # It's time to flush output and clean up.\n+        if record is None:\n+            import os\n+            if 'piggy' not in SD:\n+                raise StopIteration\n+            piggy = SD['piggy']\n+            try:\n+                piggy.log('Grabbing final tuples from the buffer.')\n+                for x in piggy.pull_final():\n+                    yield x\n+            finally:\n+                del SD['piggy']\n+            raise StopIteration\n+\n+        # First input tuple; we setup the worker process.\n+        if 'piggy' not in SD:\n+            import errno\n+            import fcntl\n+            import json\n+            import os\n+            import socket\n+            import subprocess\n+            import time\n+            from datetime import datetime\n+            from Queue import Queue, Empty, Full\n+            from threading import Thread\n+\n+            # Create a pipe for logs.\n+            # Only one process would succeed on the same host.\n+            log_pipe_name = os.path.join('%(dir)s', '%(fname)s.pipe')\n+            try:\n+                os.mkfifo(log_pipe_name)",
        "path": "util/piggy_prepare.py",
        "position": 91,
        "original_position": 91,
        "commit_id": "c904e82c9b44bd1140b566e286bec6ca2771e29d",
        "original_commit_id": "1fdb4fa2ac434edd5646d826e91e5622f1c8a654",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Never mind. Found the issue -- DD was creating a new DB connection every time we call get_log, and the shared dict in play does not go beyond one process / connection, so the sucker thread became an orphan, and subsequently calls try to create more sucker threads.\n\nDirect SSH access would definitely make things much easier, but the downside with it is that 1) the DB server / cluster doesn't always allows ssh access to all DD developers who'd like to use the DB; 2) we'd have to manage the SSH keys / passwords in addition to the JDBC params in the DD client. Longer term we may well have SSH access as a deployment requirement. I'd be as happy as you will be to switch to ssh when that day comes :)\n",
        "created_at": "2015-06-19T21:28:12Z",
        "updated_at": "2015-06-20T00:39:30Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/326#discussion_r32870964",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/326",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32870964"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/326#discussion_r32870964"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/326"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32871798",
        "pull_request_review_id": null,
        "id": 32871798,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMyODcxNzk4",
        "diff_hunk": "@@ -0,0 +1,339 @@\n+import hashlib\n+import inspect\n+import json\n+import os\n+import sys\n+import time\n+\n+\n+def piggy_prepare_run(dir, script, source, target, is_pgxl):\n+\n+    date = time.strftime('%Y%m%d')\n+    fp = date + '_' + hashlib.sha1('\\t'.join([dir, script, source, target])).hexdigest()[:8]\n+    fname = 'piggy_func_' + fp\n+    sview = 'piggy_source_' + fp\n+    tview = 'piggy_target_' + fp\n+\n+    if ' ' not in source:\n+        source_query = 'SELECT * FROM ' + source\n+    else:\n+        source_query = source\n+\n+    if '(' not in target:\n+        target_query = 'SELECT * FROM ' + target\n+    else:\n+        tname, tcols = target.strip(' )').split('(')\n+        target_query = 'SELECT %s FROM %s' % (tcols, tname)\n+\n+    logfile_path = os.path.join(dir, fname + '.log')\n+    if os.path.isfile(logfile_path):\n+        os.remove(logfile_path)\n+\n+    ts_format = '%m-%d %H:%M:%S'\n+\n+    # Why TABLE not VIEW?\n+    # Because PGXL does not create views on data nodes.\n+\n+    sql_create_tables = '''\n+    DROP TABLE IF EXISTS %(sview)s CASCADE;\n+    DROP TABLE IF EXISTS %(tview)s CASCADE;\n+    CREATE TABLE %(sview)s AS SELECT * FROM (%(source_query)s) _sview_source LIMIT 0;\n+    CREATE TABLE %(tview)s AS %(target_query)s LIMIT 0;\n+    ''' % {\n+        'sview': sview,\n+        'tview': tview,\n+        'source_query': source_query,\n+        'target_query': target_query,\n+    }\n+\n+\n+    # Why log to file instead of a DB table?\n+    # PGXL would complain something like 'no XID'.\n+    # This may have fixed it though: https://github.com/snaga/postgres-xl/commit/d4136935a1d741c61ada13ef8c5ae44f68162cc9\n+\n+    sql_create_functions = '''\n+    CREATE OR REPLACE FUNCTION %(fname)s (record %(sview)s)\n+        RETURNS SETOF %(tview)s\n+    AS $func$\n+\n+        # NULL input tuple indicates end of stream.\n+        # It's time to flush output and clean up.\n+        if record is None:\n+            import os\n+            if 'piggy' not in SD:\n+                raise StopIteration\n+            piggy = SD['piggy']\n+            try:\n+                piggy.log('Grabbing final tuples from the buffer.')\n+                for x in piggy.pull_final():\n+                    yield x\n+            finally:\n+                del SD['piggy']\n+            raise StopIteration\n+\n+        # First input tuple; we setup the worker process.\n+        if 'piggy' not in SD:\n+            import errno\n+            import fcntl\n+            import json\n+            import os\n+            import socket\n+            import subprocess\n+            import time\n+            from datetime import datetime\n+            from Queue import Queue, Empty, Full\n+            from threading import Thread\n+\n+            # Create a pipe for logs.\n+            # Only one process would succeed on the same host.\n+            log_pipe_name = os.path.join('%(dir)s', '%(fname)s.pipe')\n+            try:\n+                os.mkfifo(log_pipe_name)",
        "path": "util/piggy_prepare.py",
        "position": 91,
        "original_position": 91,
        "commit_id": "c904e82c9b44bd1140b566e286bec6ca2771e29d",
        "original_commit_id": "1fdb4fa2ac434edd5646d826e91e5622f1c8a654",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "@raphaelhoffmann and I had a chat about the SSH requirement and we concluded it doesn't make sense to support the case where the database nodes disallow SSH access, because the DD user will most likely be the one who sets up the database as well. (We're making it easier so this is plausible.)  Therefore, the user has to have SSH access to even start the datanode on such host. (Sharing a parallel database between users never worked in practice anyway, let alone the case where a single user tries to run multiple queries..)\n\nSSH authentication issues can always be handled transparently with ssh-agent or Kerberos.  Adding additional config params to DD will only add unnecessary complexity and I know such approach always break at some point.\n\nAnyway, so is it possible to fix the issue with minimal change?\n",
        "created_at": "2015-06-19T21:40:10Z",
        "updated_at": "2015-06-20T00:39:30Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/326#discussion_r32871798",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/326",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32871798"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/326#discussion_r32871798"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/326"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32874226",
        "pull_request_review_id": null,
        "id": 32874226,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDMyODc0MjI2",
        "diff_hunk": "@@ -0,0 +1,339 @@\n+import hashlib\n+import inspect\n+import json\n+import os\n+import sys\n+import time\n+\n+\n+def piggy_prepare_run(dir, script, source, target, is_pgxl):\n+\n+    date = time.strftime('%Y%m%d')\n+    fp = date + '_' + hashlib.sha1('\\t'.join([dir, script, source, target])).hexdigest()[:8]\n+    fname = 'piggy_func_' + fp\n+    sview = 'piggy_source_' + fp\n+    tview = 'piggy_target_' + fp\n+\n+    if ' ' not in source:\n+        source_query = 'SELECT * FROM ' + source\n+    else:\n+        source_query = source\n+\n+    if '(' not in target:\n+        target_query = 'SELECT * FROM ' + target\n+    else:\n+        tname, tcols = target.strip(' )').split('(')\n+        target_query = 'SELECT %s FROM %s' % (tcols, tname)\n+\n+    logfile_path = os.path.join(dir, fname + '.log')\n+    if os.path.isfile(logfile_path):\n+        os.remove(logfile_path)\n+\n+    ts_format = '%m-%d %H:%M:%S'\n+\n+    # Why TABLE not VIEW?\n+    # Because PGXL does not create views on data nodes.\n+\n+    sql_create_tables = '''\n+    DROP TABLE IF EXISTS %(sview)s CASCADE;\n+    DROP TABLE IF EXISTS %(tview)s CASCADE;\n+    CREATE TABLE %(sview)s AS SELECT * FROM (%(source_query)s) _sview_source LIMIT 0;\n+    CREATE TABLE %(tview)s AS %(target_query)s LIMIT 0;\n+    ''' % {\n+        'sview': sview,\n+        'tview': tview,\n+        'source_query': source_query,\n+        'target_query': target_query,\n+    }\n+\n+\n+    # Why log to file instead of a DB table?\n+    # PGXL would complain something like 'no XID'.\n+    # This may have fixed it though: https://github.com/snaga/postgres-xl/commit/d4136935a1d741c61ada13ef8c5ae44f68162cc9\n+\n+    sql_create_functions = '''\n+    CREATE OR REPLACE FUNCTION %(fname)s (record %(sview)s)\n+        RETURNS SETOF %(tview)s\n+    AS $func$\n+\n+        # NULL input tuple indicates end of stream.\n+        # It's time to flush output and clean up.\n+        if record is None:\n+            import os\n+            if 'piggy' not in SD:\n+                raise StopIteration\n+            piggy = SD['piggy']\n+            try:\n+                piggy.log('Grabbing final tuples from the buffer.')\n+                for x in piggy.pull_final():\n+                    yield x\n+            finally:\n+                del SD['piggy']\n+            raise StopIteration\n+\n+        # First input tuple; we setup the worker process.\n+        if 'piggy' not in SD:\n+            import errno\n+            import fcntl\n+            import json\n+            import os\n+            import socket\n+            import subprocess\n+            import time\n+            from datetime import datetime\n+            from Queue import Queue, Empty, Full\n+            from threading import Thread\n+\n+            # Create a pipe for logs.\n+            # Only one process would succeed on the same host.\n+            log_pipe_name = os.path.join('%(dir)s', '%(fname)s.pipe')\n+            try:\n+                os.mkfifo(log_pipe_name)",
        "path": "util/piggy_prepare.py",
        "position": 91,
        "original_position": 91,
        "commit_id": "c904e82c9b44bd1140b566e286bec6ca2771e29d",
        "original_commit_id": "1fdb4fa2ac434edd5646d826e91e5622f1c8a654",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Yes, it should be an easy fix.\n\nI guess the fundamental question is what role the machine running DD plays (the master process orchestrating everything vs. a small client that should eventually run on a Chromebook). There is actually a third topology -- a monolithic one where one host runs the DB, the DD driver, extractions, and inference -- that's closest to the current status quo.\n\nIn the end, we'll likely have both the orchestrating master and the thin client. One way to move toward that direction is to start defining different services and clear interfaces between them. Those interfaces should probably be higher level than arbitrary local (psql) or remote commands (ssh) even if ssh is the simplest way to run things remotely. That said, streaming logs via SQL is clearly not a good interface either, though it does stick to the interface constraint that everything happens on JDBC (as opposed to having psql and ssh as well).\n",
        "created_at": "2015-06-19T22:18:01Z",
        "updated_at": "2015-06-20T00:39:30Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/326#discussion_r32874226",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/326",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/32874226"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/326#discussion_r32874226"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/326"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35480128",
        "pull_request_review_id": null,
        "id": 35480128,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM1NDgwMTI4",
        "diff_hunk": "@@ -56,7 +56,7 @@ class MysqlDataStore extends JdbcDataStore with Logging {\n       case \"text\" | \"varchar\" => s\"convert(${expr.toString()}, char)\"\n       // in mysql, convert to unsigned guarantees bigint.\n       // @see http://stackoverflow.com/questions/4660383/how-do-i-cast-a-type-to-a-bigint-in-mysql\n-      case \"bigint\" | \"int\" => s\"convert(${expr.toString()}, unsigned)\"\n+      case \"bigint\" | \"int\" => s\"convert(${expr.toString()}, signed)\"",
        "path": "src/main/scala/org/deepdive/datastore/MysqlDataStore.scala",
        "position": 5,
        "original_position": 5,
        "commit_id": "859c17d4eeeb7f7811087efe100204d221aeca3b",
        "original_commit_id": "dd46a0a1bed8ee7964f0c887715d70c52c8b5f5f",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I'm unsure if this retains the bigint conversion effect.  Do the ids need be -1?\n",
        "created_at": "2015-07-25T05:38:54Z",
        "updated_at": "2015-07-25T08:28:33Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/342#discussion_r35480128",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/342",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35480128"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/342#discussion_r35480128"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/342"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35480272",
        "pull_request_review_id": null,
        "id": 35480272,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM1NDgwMjcy",
        "diff_hunk": "@@ -547,11 +547,7 @@ trait SQLInferenceRunner extends InferenceRunner with Logging {\n                 |= t1.${dataStore.quoteColumn(s\"${InferenceNamespace.getBaseTableName(v.relation)}.id\")}\n                 |\"\"\".stripMargin.replaceAll(\"\\n\", \" \")).mkString(\"AND\")\n             val weightJoinlist = factorDesc.weight.variables.map(v => {\n-              // split column to get relation name\n-              val colSplit = v.split('.')\n-              val lastvrel = InferenceNamespace.getBaseTableName(colSplit(0))\n-              val lastv = s\"${lastvrel}.${colSplit.takeRight(colSplit.length-1).mkString(\".\")}\"\n-              s\"\"\" t0.${dataStore.quoteColumn(v)} = t1.${dataStore.quoteColumn(lastv)}\"\"\"\n+              s\"\"\" t0.${dataStore.quoteColumn(v)} = t1.${dataStore.quoteColumn(v)}\"\"\"",
        "path": "src/main/scala/org/deepdive/inference/SQLInferenceRunner.scala",
        "position": 9,
        "original_position": 9,
        "commit_id": "859c17d4eeeb7f7811087efe100204d221aeca3b",
        "original_commit_id": "dd46a0a1bed8ee7964f0c887715d70c52c8b5f5f",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Is this backwards compatible?  Why did we need such complicated column name manipulation before if the same name could be just shared across t0 and t1?  Does this require any change to t0 or t1?\n",
        "created_at": "2015-07-25T06:01:59Z",
        "updated_at": "2015-07-25T08:28:33Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/342#discussion_r35480272",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/342",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35480272"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/342#discussion_r35480272"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/342"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35480292",
        "pull_request_review_id": null,
        "id": 35480292,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM1NDgwMjky",
        "diff_hunk": "@@ -586,6 +586,120 @@ trait SQLInferenceRunnerSpec extends FunSpec with BeforeAndAfter { this: SQLInfe\n           .map(rs => rs.long(\"count\")).single.apply().get\n         assert(numVariablesInc === 20)\n       }\n+\n+      it(\"should work with materialization and incremental modes (multinomial)\") {\n+        cancelUnlessPostgres()\n+        // XXX Incremental workflow not supported on Postgres-XL as inference.PostgresInferenceRunner.groundVariables raises the following error:\n+        //   org.postgresql.util.PSQLException: ERROR: could not plan this distributed update\n+        //   Detail: correlated UPDATE or updating distribution column currently not supported in Postgres-XL.\n+        if (dataStoreHelper.isUsingPostgresXL) pending\n+        // XXX Incremental workflow not supported on Greenplum as datastore.PostgresDataStore.createTableIfNotExists raising following error:",
        "path": "src/test/scala/unit/inference/SQLInferenceDataStoreSpec.scala",
        "position": null,
        "original_position": 47,
        "commit_id": "859c17d4eeeb7f7811087efe100204d221aeca3b",
        "original_commit_id": "dd46a0a1bed8ee7964f0c887715d70c52c8b5f5f",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I'm wondering if this is still the case.  Didn't we fix at least for GP?\n",
        "created_at": "2015-07-25T06:04:01Z",
        "updated_at": "2015-07-25T08:28:33Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/342#discussion_r35480292",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/342",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35480292"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/342#discussion_r35480292"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/342"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35480320",
        "pull_request_review_id": null,
        "id": 35480320,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM1NDgwMzIw",
        "diff_hunk": "@@ -586,6 +586,120 @@ trait SQLInferenceRunnerSpec extends FunSpec with BeforeAndAfter { this: SQLInfe\n           .map(rs => rs.long(\"count\")).single.apply().get\n         assert(numVariablesInc === 20)\n       }\n+\n+      it(\"should work with materialization and incremental modes (multinomial)\") {\n+        cancelUnlessPostgres()\n+        // XXX Incremental workflow not supported on Postgres-XL as inference.PostgresInferenceRunner.groundVariables raises the following error:\n+        //   org.postgresql.util.PSQLException: ERROR: could not plan this distributed update\n+        //   Detail: correlated UPDATE or updating distribution column currently not supported in Postgres-XL.\n+        if (dataStoreHelper.isUsingPostgresXL) pending\n+        // XXX Incremental workflow not supported on Greenplum as datastore.PostgresDataStore.createTableIfNotExists raising following error:\n+        //   org.postgresql.util.PSQLException: ERROR: syntax error at or near \"NOT\"\n+        //   Position: 17\n+        if (dataStoreHelper.isUsingGreenplum) pending\n+\n+        inferenceRunner.init()\n+\n+        // Insert sample data\n+        SQL(s\"\"\"CREATE TABLE r1(weight text,\n+          is_correct int, id bigint);\"\"\").execute.apply()\n+        val data = (1 to 100).map { i =>\n+          Map(\"id\" -> i, \"weight\" -> s\"weight_${i}\", \"is_correct\" -> i%2)\n+        }\n+        dataStoreHelper.bulkInsert(\"r1\", data.iterator)\n+\n+        val dbSettingsMat = DbSettings(dbSettings.driver, dbSettings.url, dbSettings.user,\n+          dbSettings.password, dbSettings.dbname, dbSettings.host, dbSettings.port,\n+          dbSettings.gphost, dbSettings.gppath, dbSettings.gpport, dbSettings.gpload,\n+          IncrementalMode.MATERIALIZATION, null)\n+\n+        val schema = Map[String, VariableDataType](\"r1.is_correct\" -> MultinomialType(2))",
        "path": "src/test/scala/unit/inference/SQLInferenceDataStoreSpec.scala",
        "position": null,
        "original_position": 67,
        "commit_id": "859c17d4eeeb7f7811087efe100204d221aeca3b",
        "original_commit_id": "dd46a0a1bed8ee7964f0c887715d70c52c8b5f5f",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Shouldn't we test a multinomial type with more than two classes?\n",
        "created_at": "2015-07-25T06:07:23Z",
        "updated_at": "2015-07-25T08:28:33Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/342#discussion_r35480320",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/342",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35480320"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/342#discussion_r35480320"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/342"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35480341",
        "pull_request_review_id": null,
        "id": 35480341,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM1NDgwMzQx",
        "diff_hunk": "@@ -547,11 +547,7 @@ trait SQLInferenceRunner extends InferenceRunner with Logging {\n                 |= t1.${dataStore.quoteColumn(s\"${InferenceNamespace.getBaseTableName(v.relation)}.id\")}\n                 |\"\"\".stripMargin.replaceAll(\"\\n\", \" \")).mkString(\"AND\")\n             val weightJoinlist = factorDesc.weight.variables.map(v => {\n-              // split column to get relation name\n-              val colSplit = v.split('.')\n-              val lastvrel = InferenceNamespace.getBaseTableName(colSplit(0))\n-              val lastv = s\"${lastvrel}.${colSplit.takeRight(colSplit.length-1).mkString(\".\")}\"\n-              s\"\"\" t0.${dataStore.quoteColumn(v)} = t1.${dataStore.quoteColumn(lastv)}\"\"\"\n+              s\"\"\" t0.${dataStore.quoteColumn(v)} = t1.${dataStore.quoteColumn(v)}\"\"\"",
        "path": "src/main/scala/org/deepdive/inference/SQLInferenceRunner.scala",
        "position": 9,
        "original_position": 9,
        "commit_id": "859c17d4eeeb7f7811087efe100204d221aeca3b",
        "original_commit_id": "dd46a0a1bed8ee7964f0c887715d70c52c8b5f5f",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "The grounding code matches ddlog.jar within deepdive.\n\nPreviously, in the ddlog generated application.conf, we name weight column as \"relation.relationAlias.columnName\", where for incremental case relation needs to be prefixed with dd_delta_ or dd_new. This is very awkward.\n\nNow, we use consistent naming for weight column. The name for weight column would be \"dd_weight_column_idx\", and is consistent for increnental or non-incremental run.\n",
        "created_at": "2015-07-25T06:11:49Z",
        "updated_at": "2015-07-25T08:28:33Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/342#discussion_r35480341",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/342",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35480341"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/342#discussion_r35480341"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/342"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35480349",
        "pull_request_review_id": null,
        "id": 35480349,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM1NDgwMzQ5",
        "diff_hunk": "@@ -586,6 +586,120 @@ trait SQLInferenceRunnerSpec extends FunSpec with BeforeAndAfter { this: SQLInfe\n           .map(rs => rs.long(\"count\")).single.apply().get\n         assert(numVariablesInc === 20)\n       }\n+\n+      it(\"should work with materialization and incremental modes (multinomial)\") {\n+        cancelUnlessPostgres()\n+        // XXX Incremental workflow not supported on Postgres-XL as inference.PostgresInferenceRunner.groundVariables raises the following error:\n+        //   org.postgresql.util.PSQLException: ERROR: could not plan this distributed update\n+        //   Detail: correlated UPDATE or updating distribution column currently not supported in Postgres-XL.\n+        if (dataStoreHelper.isUsingPostgresXL) pending\n+        // XXX Incremental workflow not supported on Greenplum as datastore.PostgresDataStore.createTableIfNotExists raising following error:",
        "path": "src/test/scala/unit/inference/SQLInferenceDataStoreSpec.scala",
        "position": null,
        "original_position": 47,
        "commit_id": "859c17d4eeeb7f7811087efe100204d221aeca3b",
        "original_commit_id": "dd46a0a1bed8ee7964f0c887715d70c52c8b5f5f",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "These lines were in master, so I used it here. I think incremental has been fixed for greenplum and postgresxl.\n",
        "created_at": "2015-07-25T06:14:05Z",
        "updated_at": "2015-07-25T08:28:33Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/342#discussion_r35480349",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/342",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35480349"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/342#discussion_r35480349"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/342"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35480357",
        "pull_request_review_id": null,
        "id": 35480357,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM1NDgwMzU3",
        "diff_hunk": "@@ -586,6 +586,120 @@ trait SQLInferenceRunnerSpec extends FunSpec with BeforeAndAfter { this: SQLInfe\n           .map(rs => rs.long(\"count\")).single.apply().get\n         assert(numVariablesInc === 20)\n       }\n+\n+      it(\"should work with materialization and incremental modes (multinomial)\") {\n+        cancelUnlessPostgres()\n+        // XXX Incremental workflow not supported on Postgres-XL as inference.PostgresInferenceRunner.groundVariables raises the following error:\n+        //   org.postgresql.util.PSQLException: ERROR: could not plan this distributed update\n+        //   Detail: correlated UPDATE or updating distribution column currently not supported in Postgres-XL.\n+        if (dataStoreHelper.isUsingPostgresXL) pending\n+        // XXX Incremental workflow not supported on Greenplum as datastore.PostgresDataStore.createTableIfNotExists raising following error:\n+        //   org.postgresql.util.PSQLException: ERROR: syntax error at or near \"NOT\"\n+        //   Position: 17\n+        if (dataStoreHelper.isUsingGreenplum) pending\n+\n+        inferenceRunner.init()\n+\n+        // Insert sample data\n+        SQL(s\"\"\"CREATE TABLE r1(weight text,\n+          is_correct int, id bigint);\"\"\").execute.apply()\n+        val data = (1 to 100).map { i =>\n+          Map(\"id\" -> i, \"weight\" -> s\"weight_${i}\", \"is_correct\" -> i%2)\n+        }\n+        dataStoreHelper.bulkInsert(\"r1\", data.iterator)\n+\n+        val dbSettingsMat = DbSettings(dbSettings.driver, dbSettings.url, dbSettings.user,\n+          dbSettings.password, dbSettings.dbname, dbSettings.host, dbSettings.port,\n+          dbSettings.gphost, dbSettings.gppath, dbSettings.gpport, dbSettings.gpload,\n+          IncrementalMode.MATERIALIZATION, null)\n+\n+        val schema = Map[String, VariableDataType](\"r1.is_correct\" -> MultinomialType(2))",
        "path": "src/test/scala/unit/inference/SQLInferenceDataStoreSpec.scala",
        "position": null,
        "original_position": 67,
        "commit_id": "859c17d4eeeb7f7811087efe100204d221aeca3b",
        "original_commit_id": "dd46a0a1bed8ee7964f0c887715d70c52c8b5f5f",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "There's no difference in grounding for different multinomial domain sizes. Multinomial takes a different path from boolean in grounding factors and weights.\n",
        "created_at": "2015-07-25T06:16:38Z",
        "updated_at": "2015-07-25T08:28:33Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/342#discussion_r35480357",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/342",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35480357"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/342#discussion_r35480357"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/342"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35480360",
        "pull_request_review_id": null,
        "id": 35480360,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM1NDgwMzYw",
        "diff_hunk": "@@ -547,11 +547,7 @@ trait SQLInferenceRunner extends InferenceRunner with Logging {\n                 |= t1.${dataStore.quoteColumn(s\"${InferenceNamespace.getBaseTableName(v.relation)}.id\")}\n                 |\"\"\".stripMargin.replaceAll(\"\\n\", \" \")).mkString(\"AND\")\n             val weightJoinlist = factorDesc.weight.variables.map(v => {\n-              // split column to get relation name\n-              val colSplit = v.split('.')\n-              val lastvrel = InferenceNamespace.getBaseTableName(colSplit(0))\n-              val lastv = s\"${lastvrel}.${colSplit.takeRight(colSplit.length-1).mkString(\".\")}\"\n-              s\"\"\" t0.${dataStore.quoteColumn(v)} = t1.${dataStore.quoteColumn(lastv)}\"\"\"\n+              s\"\"\" t0.${dataStore.quoteColumn(v)} = t1.${dataStore.quoteColumn(v)}\"\"\"",
        "path": "src/main/scala/org/deepdive/inference/SQLInferenceRunner.scala",
        "position": 9,
        "original_position": 9,
        "commit_id": "859c17d4eeeb7f7811087efe100204d221aeca3b",
        "original_commit_id": "dd46a0a1bed8ee7964f0c887715d70c52c8b5f5f",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I see, I like that consistency through simplicity.  Actually, I was asking whether this change is harmless to the existing apps, not originating from ddlog.\n",
        "created_at": "2015-07-25T06:16:57Z",
        "updated_at": "2015-07-25T08:28:33Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/342#discussion_r35480360",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/342",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35480360"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/342#discussion_r35480360"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/342"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35480361",
        "pull_request_review_id": null,
        "id": 35480361,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM1NDgwMzYx",
        "diff_hunk": "@@ -56,7 +56,7 @@ class MysqlDataStore extends JdbcDataStore with Logging {\n       case \"text\" | \"varchar\" => s\"convert(${expr.toString()}, char)\"\n       // in mysql, convert to unsigned guarantees bigint.\n       // @see http://stackoverflow.com/questions/4660383/how-do-i-cast-a-type-to-a-bigint-in-mysql\n-      case \"bigint\" | \"int\" => s\"convert(${expr.toString()}, unsigned)\"\n+      case \"bigint\" | \"int\" => s\"convert(${expr.toString()}, signed)\"",
        "path": "src/main/scala/org/deepdive/datastore/MysqlDataStore.scala",
        "position": 5,
        "original_position": 5,
        "commit_id": "859c17d4eeeb7f7811087efe100204d221aeca3b",
        "original_commit_id": "dd46a0a1bed8ee7964f0c887715d70c52c8b5f5f",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Casting to signed in mysql should be also bigint. \nA related post is http://dba.stackexchange.com/questions/57363/why-does-cast1-as-signed-integer-return-a-bigint-on-mysql.\n\nI'm installing mysql to confirm this.\n",
        "created_at": "2015-07-25T06:17:10Z",
        "updated_at": "2015-07-25T08:28:33Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/342#discussion_r35480361",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/342",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35480361"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/342#discussion_r35480361"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/342"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35480372",
        "pull_request_review_id": null,
        "id": 35480372,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM1NDgwMzcy",
        "diff_hunk": "@@ -586,6 +586,120 @@ trait SQLInferenceRunnerSpec extends FunSpec with BeforeAndAfter { this: SQLInfe\n           .map(rs => rs.long(\"count\")).single.apply().get\n         assert(numVariablesInc === 20)\n       }\n+\n+      it(\"should work with materialization and incremental modes (multinomial)\") {\n+        cancelUnlessPostgres()\n+        // XXX Incremental workflow not supported on Postgres-XL as inference.PostgresInferenceRunner.groundVariables raises the following error:\n+        //   org.postgresql.util.PSQLException: ERROR: could not plan this distributed update\n+        //   Detail: correlated UPDATE or updating distribution column currently not supported in Postgres-XL.\n+        if (dataStoreHelper.isUsingPostgresXL) pending\n+        // XXX Incremental workflow not supported on Greenplum as datastore.PostgresDataStore.createTableIfNotExists raising following error:",
        "path": "src/test/scala/unit/inference/SQLInferenceDataStoreSpec.scala",
        "position": null,
        "original_position": 47,
        "commit_id": "859c17d4eeeb7f7811087efe100204d221aeca3b",
        "original_commit_id": "dd46a0a1bed8ee7964f0c887715d70c52c8b5f5f",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Hmm, then whoever fixed that forgot to update the tests.  Or it could be my fault while merging for releases.  Could you please remove these guards?\n",
        "created_at": "2015-07-25T06:18:26Z",
        "updated_at": "2015-07-25T08:28:33Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/342#discussion_r35480372",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/342",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35480372"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/342#discussion_r35480372"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/342"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35480380",
        "pull_request_review_id": null,
        "id": 35480380,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM1NDgwMzgw",
        "diff_hunk": "@@ -547,11 +547,7 @@ trait SQLInferenceRunner extends InferenceRunner with Logging {\n                 |= t1.${dataStore.quoteColumn(s\"${InferenceNamespace.getBaseTableName(v.relation)}.id\")}\n                 |\"\"\".stripMargin.replaceAll(\"\\n\", \" \")).mkString(\"AND\")\n             val weightJoinlist = factorDesc.weight.variables.map(v => {\n-              // split column to get relation name\n-              val colSplit = v.split('.')\n-              val lastvrel = InferenceNamespace.getBaseTableName(colSplit(0))\n-              val lastv = s\"${lastvrel}.${colSplit.takeRight(colSplit.length-1).mkString(\".\")}\"\n-              s\"\"\" t0.${dataStore.quoteColumn(v)} = t1.${dataStore.quoteColumn(lastv)}\"\"\"\n+              s\"\"\" t0.${dataStore.quoteColumn(v)} = t1.${dataStore.quoteColumn(v)}\"\"\"",
        "path": "src/main/scala/org/deepdive/inference/SQLInferenceRunner.scala",
        "position": 9,
        "original_position": 9,
        "commit_id": "859c17d4eeeb7f7811087efe100204d221aeca3b",
        "original_commit_id": "dd46a0a1bed8ee7964f0c887715d70c52c8b5f5f",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "No, this will not affect existing apps not originating from ddlog. It only affects incremental programs generated from ddlog. \n",
        "created_at": "2015-07-25T06:19:17Z",
        "updated_at": "2015-07-25T08:28:33Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/342#discussion_r35480380",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/342",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35480380"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/342#discussion_r35480380"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/342"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35480394",
        "pull_request_review_id": null,
        "id": 35480394,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM1NDgwMzk0",
        "diff_hunk": "@@ -586,6 +586,120 @@ trait SQLInferenceRunnerSpec extends FunSpec with BeforeAndAfter { this: SQLInfe\n           .map(rs => rs.long(\"count\")).single.apply().get\n         assert(numVariablesInc === 20)\n       }\n+\n+      it(\"should work with materialization and incremental modes (multinomial)\") {\n+        cancelUnlessPostgres()\n+        // XXX Incremental workflow not supported on Postgres-XL as inference.PostgresInferenceRunner.groundVariables raises the following error:\n+        //   org.postgresql.util.PSQLException: ERROR: could not plan this distributed update\n+        //   Detail: correlated UPDATE or updating distribution column currently not supported in Postgres-XL.\n+        if (dataStoreHelper.isUsingPostgresXL) pending\n+        // XXX Incremental workflow not supported on Greenplum as datastore.PostgresDataStore.createTableIfNotExists raising following error:\n+        //   org.postgresql.util.PSQLException: ERROR: syntax error at or near \"NOT\"\n+        //   Position: 17\n+        if (dataStoreHelper.isUsingGreenplum) pending\n+\n+        inferenceRunner.init()\n+\n+        // Insert sample data\n+        SQL(s\"\"\"CREATE TABLE r1(weight text,\n+          is_correct int, id bigint);\"\"\").execute.apply()\n+        val data = (1 to 100).map { i =>\n+          Map(\"id\" -> i, \"weight\" -> s\"weight_${i}\", \"is_correct\" -> i%2)\n+        }\n+        dataStoreHelper.bulkInsert(\"r1\", data.iterator)\n+\n+        val dbSettingsMat = DbSettings(dbSettings.driver, dbSettings.url, dbSettings.user,\n+          dbSettings.password, dbSettings.dbname, dbSettings.host, dbSettings.port,\n+          dbSettings.gphost, dbSettings.gppath, dbSettings.gpport, dbSettings.gpload,\n+          IncrementalMode.MATERIALIZATION, null)\n+\n+        val schema = Map[String, VariableDataType](\"r1.is_correct\" -> MultinomialType(2))",
        "path": "src/test/scala/unit/inference/SQLInferenceDataStoreSpec.scala",
        "position": null,
        "original_position": 67,
        "commit_id": "859c17d4eeeb7f7811087efe100204d221aeca3b",
        "original_commit_id": "dd46a0a1bed8ee7964f0c887715d70c52c8b5f5f",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I know, but testing for two may not be a meaningful test case because it's the same as Boolean.\n",
        "created_at": "2015-07-25T06:21:33Z",
        "updated_at": "2015-07-25T08:28:33Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/342#discussion_r35480394",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/342",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35480394"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/342#discussion_r35480394"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/342"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35480481",
        "pull_request_review_id": null,
        "id": 35480481,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM1NDgwNDgx",
        "diff_hunk": "@@ -586,6 +586,120 @@ trait SQLInferenceRunnerSpec extends FunSpec with BeforeAndAfter { this: SQLInfe\n           .map(rs => rs.long(\"count\")).single.apply().get\n         assert(numVariablesInc === 20)\n       }\n+\n+      it(\"should work with materialization and incremental modes (multinomial)\") {\n+        cancelUnlessPostgres()\n+        // XXX Incremental workflow not supported on Postgres-XL as inference.PostgresInferenceRunner.groundVariables raises the following error:\n+        //   org.postgresql.util.PSQLException: ERROR: could not plan this distributed update\n+        //   Detail: correlated UPDATE or updating distribution column currently not supported in Postgres-XL.\n+        if (dataStoreHelper.isUsingPostgresXL) pending\n+        // XXX Incremental workflow not supported on Greenplum as datastore.PostgresDataStore.createTableIfNotExists raising following error:\n+        //   org.postgresql.util.PSQLException: ERROR: syntax error at or near \"NOT\"\n+        //   Position: 17\n+        if (dataStoreHelper.isUsingGreenplum) pending\n+\n+        inferenceRunner.init()\n+\n+        // Insert sample data\n+        SQL(s\"\"\"CREATE TABLE r1(weight text,\n+          is_correct int, id bigint);\"\"\").execute.apply()\n+        val data = (1 to 100).map { i =>\n+          Map(\"id\" -> i, \"weight\" -> s\"weight_${i}\", \"is_correct\" -> i%2)\n+        }\n+        dataStoreHelper.bulkInsert(\"r1\", data.iterator)\n+\n+        val dbSettingsMat = DbSettings(dbSettings.driver, dbSettings.url, dbSettings.user,\n+          dbSettings.password, dbSettings.dbname, dbSettings.host, dbSettings.port,\n+          dbSettings.gphost, dbSettings.gppath, dbSettings.gpport, dbSettings.gpload,\n+          IncrementalMode.MATERIALIZATION, null)\n+\n+        val schema = Map[String, VariableDataType](\"r1.is_correct\" -> MultinomialType(2))",
        "path": "src/test/scala/unit/inference/SQLInferenceDataStoreSpec.scala",
        "position": null,
        "original_position": 67,
        "commit_id": "859c17d4eeeb7f7811087efe100204d221aeca3b",
        "original_commit_id": "dd46a0a1bed8ee7964f0c887715d70c52c8b5f5f",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "A multinomial with domain size two may feel like boolean, but it is not the same as boolean. The weights are indexed by variable cardinality. For example, if there's a `weight: ?`, for boolean, it's just one weight, but for multinomial, the number of weights is the variable domain size. \n\nThis test has considered the semantics of multinomial, and the assert conditions are different from boolean case.\n",
        "created_at": "2015-07-25T06:37:43Z",
        "updated_at": "2015-07-25T08:28:33Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/342#discussion_r35480481",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/342",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35480481"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/342#discussion_r35480481"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/342"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35480483",
        "pull_request_review_id": null,
        "id": 35480483,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM1NDgwNDgz",
        "diff_hunk": "@@ -586,6 +586,120 @@ trait SQLInferenceRunnerSpec extends FunSpec with BeforeAndAfter { this: SQLInfe\n           .map(rs => rs.long(\"count\")).single.apply().get\n         assert(numVariablesInc === 20)\n       }\n+\n+      it(\"should work with materialization and incremental modes (multinomial)\") {\n+        cancelUnlessPostgres()\n+        // XXX Incremental workflow not supported on Postgres-XL as inference.PostgresInferenceRunner.groundVariables raises the following error:\n+        //   org.postgresql.util.PSQLException: ERROR: could not plan this distributed update\n+        //   Detail: correlated UPDATE or updating distribution column currently not supported in Postgres-XL.\n+        if (dataStoreHelper.isUsingPostgresXL) pending\n+        // XXX Incremental workflow not supported on Greenplum as datastore.PostgresDataStore.createTableIfNotExists raising following error:",
        "path": "src/test/scala/unit/inference/SQLInferenceDataStoreSpec.scala",
        "position": null,
        "original_position": 47,
        "commit_id": "859c17d4eeeb7f7811087efe100204d221aeca3b",
        "original_commit_id": "dd46a0a1bed8ee7964f0c887715d70c52c8b5f5f",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Sure\n",
        "created_at": "2015-07-25T06:37:55Z",
        "updated_at": "2015-07-25T08:28:33Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/342#discussion_r35480483",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/342",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35480483"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/342#discussion_r35480483"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/342"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35480564",
        "pull_request_review_id": null,
        "id": 35480564,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM1NDgwNTY0",
        "diff_hunk": "@@ -56,7 +56,7 @@ class MysqlDataStore extends JdbcDataStore with Logging {\n       case \"text\" | \"varchar\" => s\"convert(${expr.toString()}, char)\"\n       // in mysql, convert to unsigned guarantees bigint.\n       // @see http://stackoverflow.com/questions/4660383/how-do-i-cast-a-type-to-a-bigint-in-mysql\n-      case \"bigint\" | \"int\" => s\"convert(${expr.toString()}, unsigned)\"\n+      case \"bigint\" | \"int\" => s\"convert(${expr.toString()}, signed)\"",
        "path": "src/main/scala/org/deepdive/datastore/MysqlDataStore.scala",
        "position": 5,
        "original_position": 5,
        "commit_id": "859c17d4eeeb7f7811087efe100204d221aeca3b",
        "original_commit_id": "dd46a0a1bed8ee7964f0c887715d70c52c8b5f5f",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Locally, either casting 0 to unsigned or signed will become int typed...\n\nThe problem of mysql is it doesn't have control over bigint vs int when casting.\n",
        "created_at": "2015-07-25T06:51:00Z",
        "updated_at": "2015-07-25T08:28:33Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/342#discussion_r35480564",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/342",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35480564"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/342#discussion_r35480564"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/342"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35480569",
        "pull_request_review_id": null,
        "id": 35480569,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM1NDgwNTY5",
        "diff_hunk": "@@ -586,6 +586,120 @@ trait SQLInferenceRunnerSpec extends FunSpec with BeforeAndAfter { this: SQLInfe\n           .map(rs => rs.long(\"count\")).single.apply().get\n         assert(numVariablesInc === 20)\n       }\n+\n+      it(\"should work with materialization and incremental modes (multinomial)\") {\n+        cancelUnlessPostgres()\n+        // XXX Incremental workflow not supported on Postgres-XL as inference.PostgresInferenceRunner.groundVariables raises the following error:\n+        //   org.postgresql.util.PSQLException: ERROR: could not plan this distributed update\n+        //   Detail: correlated UPDATE or updating distribution column currently not supported in Postgres-XL.\n+        if (dataStoreHelper.isUsingPostgresXL) pending\n+        // XXX Incremental workflow not supported on Greenplum as datastore.PostgresDataStore.createTableIfNotExists raising following error:\n+        //   org.postgresql.util.PSQLException: ERROR: syntax error at or near \"NOT\"\n+        //   Position: 17\n+        if (dataStoreHelper.isUsingGreenplum) pending\n+\n+        inferenceRunner.init()\n+\n+        // Insert sample data\n+        SQL(s\"\"\"CREATE TABLE r1(weight text,\n+          is_correct int, id bigint);\"\"\").execute.apply()\n+        val data = (1 to 100).map { i =>\n+          Map(\"id\" -> i, \"weight\" -> s\"weight_${i}\", \"is_correct\" -> i%2)\n+        }\n+        dataStoreHelper.bulkInsert(\"r1\", data.iterator)\n+\n+        val dbSettingsMat = DbSettings(dbSettings.driver, dbSettings.url, dbSettings.user,\n+          dbSettings.password, dbSettings.dbname, dbSettings.host, dbSettings.port,\n+          dbSettings.gphost, dbSettings.gppath, dbSettings.gpport, dbSettings.gpload,\n+          IncrementalMode.MATERIALIZATION, null)\n+\n+        val schema = Map[String, VariableDataType](\"r1.is_correct\" -> MultinomialType(2))",
        "path": "src/test/scala/unit/inference/SQLInferenceDataStoreSpec.scala",
        "position": null,
        "original_position": 67,
        "commit_id": "859c17d4eeeb7f7811087efe100204d221aeca3b",
        "original_commit_id": "dd46a0a1bed8ee7964f0c887715d70c52c8b5f5f",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Right, what you said can be seen exactly from the test code.  However, the test seems too unrealistic (even the multinomial variable name is `is_correct`!) and makes the reader doubt whether this really is a valid test case.  I was just asking if it could be changed to look more real.  For example, increasing the number from 2 to 3, changing the name to class or category, and fixing the corresponding numbers in the assertions should make it completely look normal.  That's not a huge change, is it?\n",
        "created_at": "2015-07-25T06:52:03Z",
        "updated_at": "2015-07-25T08:28:33Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/342#discussion_r35480569",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/342",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35480569"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/342#discussion_r35480569"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/342"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35480630",
        "pull_request_review_id": null,
        "id": 35480630,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM1NDgwNjMw",
        "diff_hunk": "@@ -586,6 +586,120 @@ trait SQLInferenceRunnerSpec extends FunSpec with BeforeAndAfter { this: SQLInfe\n           .map(rs => rs.long(\"count\")).single.apply().get\n         assert(numVariablesInc === 20)\n       }\n+\n+      it(\"should work with materialization and incremental modes (multinomial)\") {\n+        cancelUnlessPostgres()\n+        // XXX Incremental workflow not supported on Postgres-XL as inference.PostgresInferenceRunner.groundVariables raises the following error:\n+        //   org.postgresql.util.PSQLException: ERROR: could not plan this distributed update\n+        //   Detail: correlated UPDATE or updating distribution column currently not supported in Postgres-XL.\n+        if (dataStoreHelper.isUsingPostgresXL) pending\n+        // XXX Incremental workflow not supported on Greenplum as datastore.PostgresDataStore.createTableIfNotExists raising following error:\n+        //   org.postgresql.util.PSQLException: ERROR: syntax error at or near \"NOT\"\n+        //   Position: 17\n+        if (dataStoreHelper.isUsingGreenplum) pending\n+\n+        inferenceRunner.init()\n+\n+        // Insert sample data\n+        SQL(s\"\"\"CREATE TABLE r1(weight text,\n+          is_correct int, id bigint);\"\"\").execute.apply()\n+        val data = (1 to 100).map { i =>\n+          Map(\"id\" -> i, \"weight\" -> s\"weight_${i}\", \"is_correct\" -> i%2)\n+        }\n+        dataStoreHelper.bulkInsert(\"r1\", data.iterator)\n+\n+        val dbSettingsMat = DbSettings(dbSettings.driver, dbSettings.url, dbSettings.user,\n+          dbSettings.password, dbSettings.dbname, dbSettings.host, dbSettings.port,\n+          dbSettings.gphost, dbSettings.gppath, dbSettings.gpport, dbSettings.gpload,\n+          IncrementalMode.MATERIALIZATION, null)\n+\n+        val schema = Map[String, VariableDataType](\"r1.is_correct\" -> MultinomialType(2))",
        "path": "src/test/scala/unit/inference/SQLInferenceDataStoreSpec.scala",
        "position": null,
        "original_position": 67,
        "commit_id": "859c17d4eeeb7f7811087efe100204d221aeca3b",
        "original_commit_id": "dd46a0a1bed8ee7964f0c887715d70c52c8b5f5f",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Sure, I'll do it. Sorry I just wanted to clarify the concepts... not worrying about the amount of changes...\n",
        "created_at": "2015-07-25T06:59:48Z",
        "updated_at": "2015-07-25T08:28:33Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/342#discussion_r35480630",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/342",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35480630"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/342#discussion_r35480630"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/342"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35935124",
        "pull_request_review_id": null,
        "id": 35935124,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM1OTM1MTI0",
        "diff_hunk": "@@ -37,10 +43,11 @@ A DeepDive application is a directory that contains the following files and dire\n \n * `run/`\n \n-    Each run of the DeepDive application has a corresponding subdirectory under this directory whose name contains the timestamp when the run was started, e.g., `run/20150618-223344.567890/`.\n+    Each run of the DeepDive application has a corresponding subdirectory under this directory whose name contains the timestamp when the run was started, e.g., `run/20150618/223344.567890/`.\n     All output and log files that belong to the run are kept under that subdirectory.\n     There are a few symbolic links with mnemonic names to the most recently started run, last successful run, last failed run for handy access.\n \n+[DDlog]: ../basics/ddlog.html",
        "path": "doc/doc/advanced/deepdiveapp.md",
        "position": 45,
        "original_position": 45,
        "commit_id": "44b9be2238750bb1d9d65d9b8b357abeed516b0f",
        "original_commit_id": "69d6cf7e8354eb7cd6477ea3b4746ede1f239fee",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Should be ddlog.**md** here.\n",
        "created_at": "2015-07-30T23:57:54Z",
        "updated_at": "2015-07-31T06:46:53Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/344#discussion_r35935124",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/344",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35935124"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/344#discussion_r35935124"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/344"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35935546",
        "pull_request_review_id": null,
        "id": 35935546,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM1OTM1NTQ2",
        "diff_hunk": "@@ -0,0 +1,67 @@\n+#! /usr/bin/env python\n+\n+import csv\n+import os\n+import sys\n+from collections import defaultdict\n+\n+APP_HOME = os.environ['APP_HOME']\n+\n+# Load the spouse dictionary for distant supervision\n+spouses = defaultdict(lambda: None)\n+with open (APP_HOME + \"/input/spouses.csv\") as csvfile:\n+  reader = csv.reader(csvfile)\n+  for line in reader:\n+    spouses[line[0].strip().lower()] = line[1].strip().lower()\n+\n+# Load relations of people that are not spouse\n+non_spouses = set()\n+lines = open(APP_HOME + '/input/non-spouses.tsv').readlines()\n+for line in lines:\n+  name1, name2, relation = line.strip().split('\\t')\n+  non_spouses.add((name1, name2))  # Add a non-spouse relation pair\n+\n+# For each input tuple\n+for row in sys.stdin:\n+  parts = row.strip().split('\\t')\n+  if len(parts) != 5:\n+    print >>sys.stderr, 'Failed to parse row:', row\n+    continue\n+\n+  sentence_id, p1_id, p1_text, p2_id, p2_text = parts\n+\n+  p1_text = p1_text.strip()\n+  p2_text = p2_text.strip()\n+  p1_text_lower = p1_text.lower()\n+  p2_text_lower = p2_text.lower()\n+\n+  # See if the combination of people is in our supervision dictionary\n+  # If so, set is_correct to true or false\n+  is_true = '\\N'\n+  if spouses[p1_text_lower] == p2_text_lower:\n+    is_true = '1'\n+  if spouses[p2_text_lower] == p1_text_lower:\n+    is_true = '1'\n+  elif (p1_text == p2_text) or (p1_text in p2_text) or (p2_text in p1_text):\n+    is_true = '0'\n+  elif (p1_text_lower, p2_text_lower) in non_spouses:\n+    is_true = '0'\n+  elif (p2_text_lower, p1_text_lower) in non_spouses:\n+    is_true = '0'\n+\n+  print '\\t'.join([\n+    p1_id, p2_id, sentence_id,\n+    \"%s-%s\" %(p1_text, p2_text),\n+    \"%s-%s\" %(p1_id, p2_id),\n+    is_true\n+    ])\n+\n+  # TABLE FORMAT: CREATE TABLE has_spouse(\n+  # person1_id bigint,\n+  # person2_id bigint,\n+  # sentence_id bigint,\n+  # description text,\n+  # is_true boolean,\n+  # relation_id bigint, -- unique identifier for has_spouse\n+  # id bigint   -- reserved for DeepDive\n+  # );",
        "path": "examples/spouse_example/postgres/ddlog/udf/ext_has_spouse.py",
        "position": null,
        "original_position": 67,
        "commit_id": "44b9be2238750bb1d9d65d9b8b357abeed516b0f",
        "original_commit_id": "69d6cf7e8354eb7cd6477ea3b4746ede1f239fee",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "This may be wrong in the original example. The table schema information does not correspond to the output schema.\n",
        "created_at": "2015-07-31T00:04:21Z",
        "updated_at": "2015-07-31T06:46:53Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/344#discussion_r35935546",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/344",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35935546"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/344#discussion_r35935546"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/344"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35937975",
        "pull_request_review_id": null,
        "id": 35937975,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM1OTM3OTc1",
        "diff_hunk": "@@ -37,10 +43,11 @@ A DeepDive application is a directory that contains the following files and dire\n \n * `run/`\n \n-    Each run of the DeepDive application has a corresponding subdirectory under this directory whose name contains the timestamp when the run was started, e.g., `run/20150618-223344.567890/`.\n+    Each run of the DeepDive application has a corresponding subdirectory under this directory whose name contains the timestamp when the run was started, e.g., `run/20150618/223344.567890/`.\n     All output and log files that belong to the run are kept under that subdirectory.\n     There are a few symbolic links with mnemonic names to the most recently started run, last successful run, last failed run for handy access.\n \n+[DDlog]: ../basics/ddlog.html",
        "path": "doc/doc/advanced/deepdiveapp.md",
        "position": 45,
        "original_position": 45,
        "commit_id": "44b9be2238750bb1d9d65d9b8b357abeed516b0f",
        "original_commit_id": "69d6cf7e8354eb7cd6477ea3b4746ede1f239fee",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Well, I also wish we could use `.md`, but Jekyll, our website compiler doesn't convert that automatically into `.html`.\n",
        "created_at": "2015-07-31T00:49:58Z",
        "updated_at": "2015-07-31T06:46:53Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/344#discussion_r35937975",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/344",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35937975"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/344#discussion_r35937975"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/344"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35938040",
        "pull_request_review_id": null,
        "id": 35938040,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM1OTM4MDQw",
        "diff_hunk": "@@ -0,0 +1,67 @@\n+#! /usr/bin/env python\n+\n+import csv\n+import os\n+import sys\n+from collections import defaultdict\n+\n+APP_HOME = os.environ['APP_HOME']\n+\n+# Load the spouse dictionary for distant supervision\n+spouses = defaultdict(lambda: None)\n+with open (APP_HOME + \"/input/spouses.csv\") as csvfile:\n+  reader = csv.reader(csvfile)\n+  for line in reader:\n+    spouses[line[0].strip().lower()] = line[1].strip().lower()\n+\n+# Load relations of people that are not spouse\n+non_spouses = set()\n+lines = open(APP_HOME + '/input/non-spouses.tsv').readlines()\n+for line in lines:\n+  name1, name2, relation = line.strip().split('\\t')\n+  non_spouses.add((name1, name2))  # Add a non-spouse relation pair\n+\n+# For each input tuple\n+for row in sys.stdin:\n+  parts = row.strip().split('\\t')\n+  if len(parts) != 5:\n+    print >>sys.stderr, 'Failed to parse row:', row\n+    continue\n+\n+  sentence_id, p1_id, p1_text, p2_id, p2_text = parts\n+\n+  p1_text = p1_text.strip()\n+  p2_text = p2_text.strip()\n+  p1_text_lower = p1_text.lower()\n+  p2_text_lower = p2_text.lower()\n+\n+  # See if the combination of people is in our supervision dictionary\n+  # If so, set is_correct to true or false\n+  is_true = '\\N'\n+  if spouses[p1_text_lower] == p2_text_lower:\n+    is_true = '1'\n+  if spouses[p2_text_lower] == p1_text_lower:\n+    is_true = '1'\n+  elif (p1_text == p2_text) or (p1_text in p2_text) or (p2_text in p1_text):\n+    is_true = '0'\n+  elif (p1_text_lower, p2_text_lower) in non_spouses:\n+    is_true = '0'\n+  elif (p2_text_lower, p1_text_lower) in non_spouses:\n+    is_true = '0'\n+\n+  print '\\t'.join([\n+    p1_id, p2_id, sentence_id,\n+    \"%s-%s\" %(p1_text, p2_text),\n+    \"%s-%s\" %(p1_id, p2_id),\n+    is_true\n+    ])\n+\n+  # TABLE FORMAT: CREATE TABLE has_spouse(\n+  # person1_id bigint,\n+  # person2_id bigint,\n+  # sentence_id bigint,\n+  # description text,\n+  # is_true boolean,\n+  # relation_id bigint, -- unique identifier for has_spouse\n+  # id bigint   -- reserved for DeepDive\n+  # );",
        "path": "examples/spouse_example/postgres/ddlog/udf/ext_has_spouse.py",
        "position": null,
        "original_position": 67,
        "commit_id": "44b9be2238750bb1d9d65d9b8b357abeed516b0f",
        "original_commit_id": "69d6cf7e8354eb7cd6477ea3b4746ede1f239fee",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Thanks for point this out.  I didn't pay careful attention to the comments.  The `print` statement do match the schema in the `.ddlog`.  Should we just remove the comments, or update to the corresponding ddlog line?\n",
        "created_at": "2015-07-31T00:51:31Z",
        "updated_at": "2015-07-31T06:46:53Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/344#discussion_r35938040",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/344",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35938040"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/344#discussion_r35938040"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/344"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35946302",
        "pull_request_review_id": null,
        "id": 35946302,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM1OTQ2MzAy",
        "diff_hunk": "@@ -37,10 +43,11 @@ A DeepDive application is a directory that contains the following files and dire\n \n * `run/`\n \n-    Each run of the DeepDive application has a corresponding subdirectory under this directory whose name contains the timestamp when the run was started, e.g., `run/20150618-223344.567890/`.\n+    Each run of the DeepDive application has a corresponding subdirectory under this directory whose name contains the timestamp when the run was started, e.g., `run/20150618/223344.567890/`.\n     All output and log files that belong to the run are kept under that subdirectory.\n     There are a few symbolic links with mnemonic names to the most recently started run, last successful run, last failed run for handy access.\n \n+[DDlog]: ../basics/ddlog.html",
        "path": "doc/doc/advanced/deepdiveapp.md",
        "position": 45,
        "original_position": 45,
        "commit_id": "44b9be2238750bb1d9d65d9b8b357abeed516b0f",
        "original_commit_id": "69d6cf7e8354eb7cd6477ea3b4746ede1f239fee",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "The current one is a dead link, but there's a page ending in `.md` (https://github.com/HazyResearch/deepdive/blob/netj-builtin-ddlog-support/doc/doc/basics/ddlog.md).\n",
        "created_at": "2015-07-31T04:44:56Z",
        "updated_at": "2015-07-31T06:46:53Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/344#discussion_r35946302",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/344",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35946302"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/344#discussion_r35946302"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/344"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35946559",
        "pull_request_review_id": null,
        "id": 35946559,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM1OTQ2NTU5",
        "diff_hunk": "@@ -0,0 +1,67 @@\n+#! /usr/bin/env python\n+\n+import csv\n+import os\n+import sys\n+from collections import defaultdict\n+\n+APP_HOME = os.environ['APP_HOME']\n+\n+# Load the spouse dictionary for distant supervision\n+spouses = defaultdict(lambda: None)\n+with open (APP_HOME + \"/input/spouses.csv\") as csvfile:\n+  reader = csv.reader(csvfile)\n+  for line in reader:\n+    spouses[line[0].strip().lower()] = line[1].strip().lower()\n+\n+# Load relations of people that are not spouse\n+non_spouses = set()\n+lines = open(APP_HOME + '/input/non-spouses.tsv').readlines()\n+for line in lines:\n+  name1, name2, relation = line.strip().split('\\t')\n+  non_spouses.add((name1, name2))  # Add a non-spouse relation pair\n+\n+# For each input tuple\n+for row in sys.stdin:\n+  parts = row.strip().split('\\t')\n+  if len(parts) != 5:\n+    print >>sys.stderr, 'Failed to parse row:', row\n+    continue\n+\n+  sentence_id, p1_id, p1_text, p2_id, p2_text = parts\n+\n+  p1_text = p1_text.strip()\n+  p2_text = p2_text.strip()\n+  p1_text_lower = p1_text.lower()\n+  p2_text_lower = p2_text.lower()\n+\n+  # See if the combination of people is in our supervision dictionary\n+  # If so, set is_correct to true or false\n+  is_true = '\\N'\n+  if spouses[p1_text_lower] == p2_text_lower:\n+    is_true = '1'\n+  if spouses[p2_text_lower] == p1_text_lower:\n+    is_true = '1'\n+  elif (p1_text == p2_text) or (p1_text in p2_text) or (p2_text in p1_text):\n+    is_true = '0'\n+  elif (p1_text_lower, p2_text_lower) in non_spouses:\n+    is_true = '0'\n+  elif (p2_text_lower, p1_text_lower) in non_spouses:\n+    is_true = '0'\n+\n+  print '\\t'.join([\n+    p1_id, p2_id, sentence_id,\n+    \"%s-%s\" %(p1_text, p2_text),\n+    \"%s-%s\" %(p1_id, p2_id),\n+    is_true\n+    ])\n+\n+  # TABLE FORMAT: CREATE TABLE has_spouse(\n+  # person1_id bigint,\n+  # person2_id bigint,\n+  # sentence_id bigint,\n+  # description text,\n+  # is_true boolean,\n+  # relation_id bigint, -- unique identifier for has_spouse\n+  # id bigint   -- reserved for DeepDive\n+  # );",
        "path": "examples/spouse_example/postgres/ddlog/udf/ext_has_spouse.py",
        "position": null,
        "original_position": 67,
        "commit_id": "44b9be2238750bb1d9d65d9b8b357abeed516b0f",
        "original_commit_id": "69d6cf7e8354eb7cd6477ea3b4746ede1f239fee",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I think we can just remove those comments, since the program is correct.\n",
        "created_at": "2015-07-31T04:54:09Z",
        "updated_at": "2015-07-31T06:46:53Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/344#discussion_r35946559",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/344",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35946559"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/344#discussion_r35946559"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/344"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35946625",
        "pull_request_review_id": null,
        "id": 35946625,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM1OTQ2NjI1",
        "diff_hunk": "@@ -37,10 +43,11 @@ A DeepDive application is a directory that contains the following files and dire\n \n * `run/`\n \n-    Each run of the DeepDive application has a corresponding subdirectory under this directory whose name contains the timestamp when the run was started, e.g., `run/20150618-223344.567890/`.\n+    Each run of the DeepDive application has a corresponding subdirectory under this directory whose name contains the timestamp when the run was started, e.g., `run/20150618/223344.567890/`.\n     All output and log files that belong to the run are kept under that subdirectory.\n     There are a few symbolic links with mnemonic names to the most recently started run, last successful run, last failed run for handy access.\n \n+[DDlog]: ../basics/ddlog.html",
        "path": "doc/doc/advanced/deepdiveapp.md",
        "position": 45,
        "original_position": 45,
        "commit_id": "44b9be2238750bb1d9d65d9b8b357abeed516b0f",
        "original_commit_id": "69d6cf7e8354eb7cd6477ea3b4746ede1f239fee",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Sorry I found out that the deployed website actually ends in .html\n",
        "created_at": "2015-07-31T04:57:06Z",
        "updated_at": "2015-07-31T06:46:53Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/344#discussion_r35946625",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/344",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/35946625"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/344#discussion_r35946625"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/344"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36270221",
        "pull_request_review_id": null,
        "id": 36270221,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjcwMjIx",
        "diff_hunk": "@@ -0,0 +1,59 @@\n+#! /usr/bin/env python\n+\n+import json, sys, csv, os, pipes, subprocess\n+\n+def convert_type(value, column_type):\n+  if value == \"\": return None\n+  column_type = column_type.lower()\n+  if column_type == \"integer\" or column_type == \"int\" or column_type == \"bigint\":\n+    return int(value)\n+  elif column_type == \"float\" or column_type == \"numeric\":\n+    return float(value)\n+  elif column_type == \"text\":\n+    return value\n+  elif column_type == \"boolean\":\n+    return value == \"t\"\n+  elif column_type.endswith(\"[]\"):\n+    # csv array starts and ends with curly braces\n+    value = value[1:-1]\n+    column_type = column_type[:-2]\n+    # string unescaping\n+    # note this is an array as a csv field, we first unescape csv escaping,\n+    # and then do array unescaping, apply csv escaping, and parse the arry\n+    # elements as csv\n+    # csv escapes double quote with two double quotes\n+    # array escapes double quote using backslashes\n+    if column_type == \"text\":\n+      value = value.replace('\"\"', '\"').replace('\\\\\"', '\"\"')\n+    arr = csv.reader([value], delimiter=',', quotechar='\"').next()\n+    return [convert_type(x, column_type) for x in arr]\n+  else:\n+    raise ValueError(\"Unsupported data type %s\" %column_type)\n+\n+\n+def main():\n+  # get column types\n+  sql = \"CREATE TEMP TABLE __to_json_temp AS (%s) LIMIT 0; \\\n+  COPY (SELECT format_type(atttypid, atttypmod) AS type \\\n+  FROM pg_attribute \\\n+  WHERE attrelid = '__to_json_temp'::regclass \\\n+  AND attnum > 0 \\\n+  AND NOT attisdropped \\\n+  ORDER BY attnum) TO STDOUT\" %(sys.argv[1].rstrip(\";\"))\n+  types = subprocess.check_output(\"deepdive sql %s\" %pipes.quote(sql), shell=True).strip().split(\"\\n\")",
        "path": "util/csvtojson.py",
        "position": null,
        "original_position": 43,
        "commit_id": "456f610515b447ad5e0b1e67f9fd2be9cb7d336e",
        "original_commit_id": "5ca52fe5cb109c0a585acd33f0fabf76294e230e",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Nice! Looks like you found the magic SQL!\n\nCalling `deepdive sql` within the implementation of itself seems pretty weird and inefficient.  I think this part that runs the fancy SQL should be moved out of python, and run directly from the `db-query`.  The types can be passed as arguments to this python script.  That way, we can also keep the python code much simpler, reusable, and having less dependencies.\n",
        "created_at": "2015-08-05T05:19:15Z",
        "updated_at": "2015-08-08T01:38:21Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36270221",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36270221"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36270221"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36270270",
        "pull_request_review_id": null,
        "id": 36270270,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjcwMjcw",
        "diff_hunk": "@@ -0,0 +1,59 @@\n+#! /usr/bin/env python\n+\n+import json, sys, csv, os, pipes, subprocess\n+\n+def convert_type(value, column_type):\n+  if value == \"\": return None\n+  column_type = column_type.lower()\n+  if column_type == \"integer\" or column_type == \"int\" or column_type == \"bigint\":\n+    return int(value)\n+  elif column_type == \"float\" or column_type == \"numeric\":\n+    return float(value)\n+  elif column_type == \"text\":\n+    return value\n+  elif column_type == \"boolean\":\n+    return value == \"t\"\n+  elif column_type.endswith(\"[]\"):\n+    # csv array starts and ends with curly braces\n+    value = value[1:-1]\n+    column_type = column_type[:-2]\n+    # string unescaping\n+    # note this is an array as a csv field, we first unescape csv escaping,\n+    # and then do array unescaping, apply csv escaping, and parse the arry\n+    # elements as csv\n+    # csv escapes double quote with two double quotes\n+    # array escapes double quote using backslashes\n+    if column_type == \"text\":\n+      value = value.replace('\"\"', '\"').replace('\\\\\"', '\"\"')\n+    arr = csv.reader([value], delimiter=',', quotechar='\"').next()\n+    return [convert_type(x, column_type) for x in arr]\n+  else:\n+    raise ValueError(\"Unsupported data type %s\" %column_type)\n+\n+\n+def main():\n+  # get column types\n+  sql = \"CREATE TEMP TABLE __to_json_temp AS (%s) LIMIT 0; \\\n+  COPY (SELECT format_type(atttypid, atttypmod) AS type \\\n+  FROM pg_attribute \\\n+  WHERE attrelid = '__to_json_temp'::regclass \\\n+  AND attnum > 0 \\\n+  AND NOT attisdropped \\\n+  ORDER BY attnum) TO STDOUT\" %(sys.argv[1].rstrip(\";\"))\n+  types = subprocess.check_output(\"deepdive sql %s\" %pipes.quote(sql), shell=True).strip().split(\"\\n\")\n+\n+  # read the contents\n+  reader = csv.reader(sys.stdin, delimiter = ',', quotechar = '\"')\n+  fieldnames = reader.next()\n+  if len(types) != len(fieldnames):\n+    raise ValueError(\"Number of columns does not match schema\")\n+  for line in reader:\n+    if len(line) != len(fieldnames):\n+      raise ValueError(\"Number of columns does not match schema\")\n+    obj = {}\n+    for i in range(len(fieldnames)):\n+      obj[fieldnames[i]] = convert_type(line[i], types[i])",
        "path": "util/csvtojson.py",
        "position": null,
        "original_position": 55,
        "commit_id": "456f610515b447ad5e0b1e67f9fd2be9cb7d336e",
        "original_commit_id": "5ca52fe5cb109c0a585acd33f0fabf76294e230e",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "`xrange` is better if you're just iterating over it once as it won't create a temporary list. (It seems some consider [`range` as anti-pattern](http://lignos.org/py_antipatterns/).)  I'm unsure how fast `len` is, but keeping the number of columns in a local variable is probably a good idea.\nThis loop body is the hot path, so we should try our best to make it as efficient as possible.  Actually, instead of having `convert_type` go over multiple `if`-branches with `types[i]` on every input line, it should be significantly better to construct a list of functions based on the types ahead of time, to simply apply them on the values we see for each line without any branching.\n",
        "created_at": "2015-08-05T05:20:46Z",
        "updated_at": "2015-08-08T01:38:21Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36270270",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36270270"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36270270"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36270564",
        "pull_request_review_id": null,
        "id": 36270564,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjcwNTY0",
        "diff_hunk": "@@ -0,0 +1,28 @@\n+#!/usr/bin/env bash\n+# db-query -- Evaluates given SQL query against the PostgreSQL database configured for a DeepDive application\n+# > eval \"$(db-parse \"$url\")\"\n+# > db-query SQL FORMAT HEADER\n+##\n+set -eu\n+\n+sql=${1:?No SQL given}; shift\n+format=${1:?No FORMAT given}; shift\n+header=${1:?No HEADER given}; shift\n+\n+copy_option=\n+case $header in\n+    0) ;;\n+    1) copy_option+=\" HEADER\" ;;\n+    *) error \"$header: unrecognized value for HEADER\"\n+esac\n+case $format in\n+    tsv) ;;\n+    csv) copy_option+=\" csv\" ;;\n+    json)\n+        db-query \"$sql\" csv 1 | python $DEEPDIVE_HOME/util/csvtojson.py \"$sql\"",
        "path": "shell/driver.greenplum/db-query",
        "position": null,
        "original_position": 22,
        "commit_id": "456f610515b447ad5e0b1e67f9fd2be9cb7d336e",
        "original_commit_id": "5ca52fe5cb109c0a585acd33f0fabf76294e230e",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "You don't need to call the python script with full path as long as it stays in either util/ or bin/.  I don't think explicitly calling python is a good idea, either.  So, this can be written simply as:\n\n``` bash\ndb-query \"$sql\" csv 1 | csvtojson.py \"$sql\"\n```\n\nHowever, since we want to call the magic sql directly from this script and pass the types to python, it should be something like this:\n\n``` bash\ntypes=$(db-prompt -c \"CREATE TEMP TABLE ... ($sql LIMIT 0); ... magic SQL ...\")\ndb-query \"$sql\" csv 1 | csvtojson.py $types  # XXX assuming no spaces in type names\n```\n",
        "created_at": "2015-08-05T05:29:36Z",
        "updated_at": "2015-08-08T01:38:21Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36270564",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36270564"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36270564"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36270601",
        "pull_request_review_id": null,
        "id": 36270601,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjcwNjAx",
        "diff_hunk": "@@ -0,0 +1,59 @@\n+#! /usr/bin/env python\n+\n+import json, sys, csv, os, pipes, subprocess",
        "path": "util/csvtojson.py",
        "position": null,
        "original_position": 3,
        "commit_id": "456f610515b447ad5e0b1e67f9fd2be9cb7d336e",
        "original_commit_id": "5ca52fe5cb109c0a585acd33f0fabf76294e230e",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "os, pipes, subprocess will be unnecessary once we take out the `deepdive sql` invocation.\n",
        "created_at": "2015-08-05T05:30:48Z",
        "updated_at": "2015-08-08T01:38:21Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36270601",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36270601"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36270601"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36380884",
        "pull_request_review_id": null,
        "id": 36380884,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzgwODg0",
        "diff_hunk": "@@ -0,0 +1,10 @@\n+#!/usr/bin/env bats\n+# Tests for json exporter\n+\n+. \"$BATS_TEST_DIRNAME\"/env.sh >&2\n+\n+@test \"$DBVARIANT json exporter\" {\n+    q=\"SELECT 123::bigint as i, 45.678 as float, TRUE as t, FALSE as f, 'foo bar baz'::text as s, NULL::text as n, ARRAY['asdf qwer\"$'\\t'\"zxcv\"$'\\n'\"1234', '\\\"I''m your father,\\\" said Darth Vader.'] as a, ARRAY[1,2,3] as b\"\n+    expected=\"{\\\"a\\\": [\\\"asdf qwer\\\\tzxcv\\\\n1234\\\", \\\"\\\\\\\"I'm your father,\\\\\\\" said Darth Vader.\\\"], \\\"b\\\": [1, 2, 3], \\\"f\\\": false, \\\"i\\\": 123, \\\"float\\\": 45.678, \\\"n\\\": null, \\\"s\\\": \\\"foo bar baz\\\", \\\"t\\\": true}\"\n+    [[ $(db-query \"$q\" json 0) = \"$expected\" ]]\n+}",
        "path": "test/greenplum/export_json.bats",
        "position": null,
        "original_position": 10,
        "commit_id": "456f610515b447ad5e0b1e67f9fd2be9cb7d336e",
        "original_commit_id": "9c4dae8a4cb09f9874471eac447c3504b2061ff5",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Let's keep this as a symlink to the postgresql one since they should be identical.\n",
        "created_at": "2015-08-06T04:42:00Z",
        "updated_at": "2015-08-08T01:38:21Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36380884",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36380884"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36380884"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36380921",
        "pull_request_review_id": null,
        "id": 36380921,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzgwOTIx",
        "diff_hunk": "@@ -0,0 +1,10 @@\n+#!/usr/bin/env bats\n+# Tests for json exporter\n+\n+. \"$BATS_TEST_DIRNAME\"/env.sh >&2\n+\n+@test \"$DBVARIANT json exporter\" {\n+    q=\"SELECT 123::bigint as i, 45.678 as float, TRUE as t, FALSE as f, 'foo bar baz'::text as s, NULL::text as n, ARRAY['asdf qwer\"$'\\t'\"zxcv\"$'\\n'\"1234', '\\\"I''m your father,\\\" said Darth Vader.'] as a, ARRAY[1,2,3] as b\"\n+    expected=\"{\\\"i\\\":123,\\\"float\\\":45.678,\\\"t\\\":true,\\\"f\\\":false,\\\"s\\\":\\\"foo bar baz\\\",\\\"n\\\":null,\\\"a\\\":[\\\"asdf qwer\\\\tzxcv\\\\n1234\\\",\\\"\\\\\\\"I'm your father,\\\\\\\" said Darth Vader.\\\"],\\\"b\\\":[1,2,3]}\"\n+    [[ $(db-query \"$q\" json 0) = \"$expected\" ]]",
        "path": "test/postgresql/export_json.bats",
        "position": null,
        "original_position": 9,
        "commit_id": "456f610515b447ad5e0b1e67f9fd2be9cb7d336e",
        "original_commit_id": "9c4dae8a4cb09f9874471eac447c3504b2061ff5",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Can we use `deepdive sql eval ... format=json` instead?\n",
        "created_at": "2015-08-06T04:43:11Z",
        "updated_at": "2015-08-08T01:38:21Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36380921",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36380921"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36380921"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36381191",
        "pull_request_review_id": null,
        "id": 36381191,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzgxMTkx",
        "diff_hunk": "@@ -0,0 +1,64 @@\n+#! /usr/bin/env python\n+\n+import json, sys, csv\n+\n+def convert_flat_type_func(column_type):\n+  column_type = column_type.lower()\n+  if column_type == \"integer\" or column_type == \"int\" or column_type == \"bigint\":\n+    return lambda x: None if x == \"\" else int(x)\n+  elif column_type == \"float\" or column_type == \"numeric\":\n+    return lambda x: None if x == \"\" else float(x)\n+  elif column_type == \"text\":\n+    return lambda x: None if x == \"\" else x\n+  elif column_type == \"boolean\":\n+    return lambda x: None if x == \"\" else x == \"t\"\n+  else:\n+    raise ValueError(\"Unsupported data type %s\" %column_type)\n+\n+# given a column type, returns a function that takes a string input\n+# and output with the correct type\n+def convert_type_func(column_type):\n+  column_type = column_type.lower()\n+  if column_type.endswith(\"[]\"):\n+    # csv array starts and ends with curly braces\n+    column_type = column_type[:-2]\n+    # string unescaping\n+    # note this is an array as a csv field, we first unescape csv escaping,\n+    # and then do array unescaping, apply csv escaping, and parse the arry\n+    # elements as csv\n+    # csv escapes double quote with two double quotes\n+    # array escapes double quote using backslashes\n+    flat_func = convert_flat_type_func(column_type)\n+    def convert_text_array_func(value):\n+      if value == \"\": return None\n+      arr = csv.reader([value[1:-1].replace('\"\"', '\"').replace('\\\\\"', '\"\"')], delimiter=',', quotechar='\"').next()",
        "path": "util/csvtojson.py",
        "position": null,
        "original_position": 34,
        "commit_id": "456f610515b447ad5e0b1e67f9fd2be9cb7d336e",
        "original_commit_id": "9c4dae8a4cb09f9874471eac447c3504b2061ff5",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I don't understand why you need the first csv unescaping?  Doesn't that mess up some cases?  How do you deal with escaped backslashes?  It'd be nice to include the corner case examples step by step in the comment.\n",
        "created_at": "2015-08-06T04:51:29Z",
        "updated_at": "2015-08-08T01:38:21Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36381191",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36381191"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36381191"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36381260",
        "pull_request_review_id": null,
        "id": 36381260,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzgxMjYw",
        "diff_hunk": "@@ -0,0 +1,64 @@\n+#! /usr/bin/env python\n+\n+import json, sys, csv\n+\n+def convert_flat_type_func(column_type):\n+  column_type = column_type.lower()\n+  if column_type == \"integer\" or column_type == \"int\" or column_type == \"bigint\":\n+    return lambda x: None if x == \"\" else int(x)\n+  elif column_type == \"float\" or column_type == \"numeric\":\n+    return lambda x: None if x == \"\" else float(x)\n+  elif column_type == \"text\":\n+    return lambda x: None if x == \"\" else x\n+  elif column_type == \"boolean\":\n+    return lambda x: None if x == \"\" else x == \"t\"\n+  else:\n+    raise ValueError(\"Unsupported data type %s\" %column_type)\n+\n+# given a column type, returns a function that takes a string input\n+# and output with the correct type\n+def convert_type_func(column_type):\n+  column_type = column_type.lower()\n+  if column_type.endswith(\"[]\"):\n+    # csv array starts and ends with curly braces\n+    column_type = column_type[:-2]\n+    # string unescaping\n+    # note this is an array as a csv field, we first unescape csv escaping,\n+    # and then do array unescaping, apply csv escaping, and parse the arry\n+    # elements as csv\n+    # csv escapes double quote with two double quotes\n+    # array escapes double quote using backslashes\n+    flat_func = convert_flat_type_func(column_type)\n+    def convert_text_array_func(value):\n+      if value == \"\": return None\n+      arr = csv.reader([value[1:-1].replace('\"\"', '\"').replace('\\\\\"', '\"\"')], delimiter=',', quotechar='\"').next()",
        "path": "util/csvtojson.py",
        "position": null,
        "original_position": 34,
        "commit_id": "456f610515b447ad5e0b1e67f9fd2be9cb7d336e",
        "original_commit_id": "9c4dae8a4cb09f9874471eac447c3504b2061ff5",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "When exporting to csv, array escape is first applied, and then csv escape is applied. To unescape, we need to do the reverse.\n",
        "created_at": "2015-08-06T04:53:37Z",
        "updated_at": "2015-08-08T01:38:21Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36381260",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36381260"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36381260"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36381268",
        "pull_request_review_id": null,
        "id": 36381268,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzgxMjY4",
        "diff_hunk": "@@ -0,0 +1,64 @@\n+#! /usr/bin/env python\n+\n+import json, sys, csv\n+\n+def convert_flat_type_func(column_type):\n+  column_type = column_type.lower()\n+  if column_type == \"integer\" or column_type == \"int\" or column_type == \"bigint\":\n+    return lambda x: None if x == \"\" else int(x)\n+  elif column_type == \"float\" or column_type == \"numeric\":\n+    return lambda x: None if x == \"\" else float(x)\n+  elif column_type == \"text\":\n+    return lambda x: None if x == \"\" else x\n+  elif column_type == \"boolean\":\n+    return lambda x: None if x == \"\" else x == \"t\"\n+  else:\n+    raise ValueError(\"Unsupported data type %s\" %column_type)\n+\n+# given a column type, returns a function that takes a string input\n+# and output with the correct type\n+def convert_type_func(column_type):\n+  column_type = column_type.lower()\n+  if column_type.endswith(\"[]\"):\n+    # csv array starts and ends with curly braces\n+    column_type = column_type[:-2]\n+    # string unescaping\n+    # note this is an array as a csv field, we first unescape csv escaping,\n+    # and then do array unescaping, apply csv escaping, and parse the arry\n+    # elements as csv\n+    # csv escapes double quote with two double quotes\n+    # array escapes double quote using backslashes\n+    flat_func = convert_flat_type_func(column_type)\n+    def convert_text_array_func(value):\n+      if value == \"\": return None\n+      arr = csv.reader([value[1:-1].replace('\"\"', '\"').replace('\\\\\"', '\"\"')], delimiter=',', quotechar='\"').next()\n+      return [flat_func(x) for x in arr]\n+    def convert_other_array_func(value):\n+      if value == \"\": return None\n+      arr = csv.reader([value[1:-1]], delimiter=',', quotechar='\"').next()\n+      return [flat_func(x) for x in arr]\n+    if column_type == \"text\":",
        "path": "util/csvtojson.py",
        "position": null,
        "original_position": 40,
        "commit_id": "456f610515b447ad5e0b1e67f9fd2be9cb7d336e",
        "original_commit_id": "9c4dae8a4cb09f9874471eac447c3504b2061ff5",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I think you're shortcircuiting here for performance, right?  However, is it correct to do so for just text type?  Shouldn't the escape handling be the safe default and use the faster one only for the known types that are safe, e.g., int, float, double, number, etc.?\n",
        "created_at": "2015-08-06T04:53:52Z",
        "updated_at": "2015-08-08T01:38:21Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36381268",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36381268"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36381268"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36381305",
        "pull_request_review_id": null,
        "id": 36381305,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzgxMzA1",
        "diff_hunk": "@@ -0,0 +1,64 @@\n+#! /usr/bin/env python\n+\n+import json, sys, csv\n+\n+def convert_flat_type_func(column_type):\n+  column_type = column_type.lower()\n+  if column_type == \"integer\" or column_type == \"int\" or column_type == \"bigint\":\n+    return lambda x: None if x == \"\" else int(x)\n+  elif column_type == \"float\" or column_type == \"numeric\":\n+    return lambda x: None if x == \"\" else float(x)\n+  elif column_type == \"text\":\n+    return lambda x: None if x == \"\" else x\n+  elif column_type == \"boolean\":\n+    return lambda x: None if x == \"\" else x == \"t\"\n+  else:\n+    raise ValueError(\"Unsupported data type %s\" %column_type)\n+\n+# given a column type, returns a function that takes a string input\n+# and output with the correct type\n+def convert_type_func(column_type):\n+  column_type = column_type.lower()\n+  if column_type.endswith(\"[]\"):\n+    # csv array starts and ends with curly braces\n+    column_type = column_type[:-2]\n+    # string unescaping\n+    # note this is an array as a csv field, we first unescape csv escaping,\n+    # and then do array unescaping, apply csv escaping, and parse the arry\n+    # elements as csv\n+    # csv escapes double quote with two double quotes\n+    # array escapes double quote using backslashes\n+    flat_func = convert_flat_type_func(column_type)\n+    def convert_text_array_func(value):\n+      if value == \"\": return None\n+      arr = csv.reader([value[1:-1].replace('\"\"', '\"').replace('\\\\\"', '\"\"')], delimiter=',', quotechar='\"').next()",
        "path": "util/csvtojson.py",
        "position": null,
        "original_position": 34,
        "commit_id": "456f610515b447ad5e0b1e67f9fd2be9cb7d336e",
        "original_commit_id": "9c4dae8a4cb09f9874471eac447c3504b2061ff5",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Will add unescaping for backslash.\n",
        "created_at": "2015-08-06T04:54:52Z",
        "updated_at": "2015-08-08T01:38:21Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36381305",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36381305"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36381305"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36381317",
        "pull_request_review_id": null,
        "id": 36381317,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzgxMzE3",
        "diff_hunk": "@@ -0,0 +1,10 @@\n+#!/usr/bin/env bats\n+# Tests for json exporter\n+\n+. \"$BATS_TEST_DIRNAME\"/env.sh >&2\n+\n+@test \"$DBVARIANT json exporter\" {\n+    q=\"SELECT 123::bigint as i, 45.678 as float, TRUE as t, FALSE as f, 'foo bar baz'::text as s, NULL::text as n, ARRAY['asdf qwer\"$'\\t'\"zxcv\"$'\\n'\"1234', '\\\"I''m your father,\\\" said Darth Vader.'] as a, ARRAY[1,2,3] as b\"\n+    expected=\"{\\\"i\\\":123,\\\"float\\\":45.678,\\\"t\\\":true,\\\"f\\\":false,\\\"s\\\":\\\"foo bar baz\\\",\\\"n\\\":null,\\\"a\\\":[\\\"asdf qwer\\\\tzxcv\\\\n1234\\\",\\\"\\\\\\\"I'm your father,\\\\\\\" said Darth Vader.\\\"],\\\"b\\\":[1,2,3]}\"\n+    [[ $(db-query \"$q\" json 0) = \"$expected\" ]]\n+}",
        "path": "test/postgresql/export_json.bats",
        "position": null,
        "original_position": 10,
        "commit_id": "456f610515b447ad5e0b1e67f9fd2be9cb7d336e",
        "original_commit_id": "9c4dae8a4cb09f9874471eac447c3504b2061ff5",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Let's add some tests that also include backslashes, curly braces in the array.\n",
        "created_at": "2015-08-06T04:55:15Z",
        "updated_at": "2015-08-08T01:38:21Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36381317",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36381317"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36381317"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36381348",
        "pull_request_review_id": null,
        "id": 36381348,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzgxMzQ4",
        "diff_hunk": "@@ -0,0 +1,64 @@\n+#! /usr/bin/env python\n+\n+import json, sys, csv\n+\n+def convert_flat_type_func(column_type):\n+  column_type = column_type.lower()\n+  if column_type == \"integer\" or column_type == \"int\" or column_type == \"bigint\":\n+    return lambda x: None if x == \"\" else int(x)\n+  elif column_type == \"float\" or column_type == \"numeric\":\n+    return lambda x: None if x == \"\" else float(x)\n+  elif column_type == \"text\":\n+    return lambda x: None if x == \"\" else x\n+  elif column_type == \"boolean\":\n+    return lambda x: None if x == \"\" else x == \"t\"\n+  else:\n+    raise ValueError(\"Unsupported data type %s\" %column_type)\n+\n+# given a column type, returns a function that takes a string input\n+# and output with the correct type\n+def convert_type_func(column_type):\n+  column_type = column_type.lower()\n+  if column_type.endswith(\"[]\"):\n+    # csv array starts and ends with curly braces\n+    column_type = column_type[:-2]\n+    # string unescaping\n+    # note this is an array as a csv field, we first unescape csv escaping,\n+    # and then do array unescaping, apply csv escaping, and parse the arry\n+    # elements as csv\n+    # csv escapes double quote with two double quotes\n+    # array escapes double quote using backslashes\n+    flat_func = convert_flat_type_func(column_type)\n+    def convert_text_array_func(value):\n+      if value == \"\": return None\n+      arr = csv.reader([value[1:-1].replace('\"\"', '\"').replace('\\\\\"', '\"\"')], delimiter=',', quotechar='\"').next()\n+      return [flat_func(x) for x in arr]\n+    def convert_other_array_func(value):\n+      if value == \"\": return None\n+      arr = csv.reader([value[1:-1]], delimiter=',', quotechar='\"').next()\n+      return [flat_func(x) for x in arr]\n+    if column_type == \"text\":",
        "path": "util/csvtojson.py",
        "position": null,
        "original_position": 40,
        "commit_id": "456f610515b447ad5e0b1e67f9fd2be9cb7d336e",
        "original_commit_id": "9c4dae8a4cb09f9874471eac447c3504b2061ff5",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Only text array needs unescaping, and int, float, boolean arrays do not need unescaping. Here I'm testing if it's a text array\n",
        "created_at": "2015-08-06T04:56:29Z",
        "updated_at": "2015-08-08T01:38:21Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36381348",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36381348"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36381348"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36381372",
        "pull_request_review_id": null,
        "id": 36381372,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzgxMzcy",
        "diff_hunk": "@@ -0,0 +1,35 @@\n+#!/usr/bin/env bash\n+# db-query -- Evaluates given SQL query against the PostgreSQL database configured for a DeepDive application\n+# > eval \"$(db-parse \"$url\")\"\n+# > db-query SQL FORMAT HEADER\n+##\n+set -eu\n+\n+sql=${1:?No SQL given}; shift\n+format=${1:?No FORMAT given}; shift\n+header=${1:?No HEADER given}; shift\n+\n+copy_option=\n+case $header in\n+    0) ;;\n+    1) copy_option+=\" HEADER\" ;;\n+    *) error \"$header: unrecognized value for HEADER\"\n+esac\n+case $format in\n+    tsv) ;;\n+    csv) copy_option+=\" csv\" ;;\n+    json)\n+        types=\"$(db-prompt -c \"CREATE TEMP TABLE __to_json_temp AS ($sql) LIMIT 0;\n+            COPY (SELECT format_type(atttypid, atttypmod) AS type\n+            FROM pg_attribute\n+            WHERE attrelid = '__to_json_temp'::regclass\n+            AND attnum > 0\n+            AND NOT attisdropped\n+            ORDER BY attnum) TO STDOUT\" | tr '\\n' ',' | sed 's/,$//')\"\n+        db-query \"$sql\" csv 1 | csvtojson.py \"$types\"",
        "path": "shell/driver.greenplum/db-query",
        "position": null,
        "original_position": 29,
        "commit_id": "456f610515b447ad5e0b1e67f9fd2be9cb7d336e",
        "original_commit_id": "9c4dae8a4cb09f9874471eac447c3504b2061ff5",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "No need to turn the whole thing into a single comma-separated string.  I think it's better to pass the types as multiple arguments without quoting `$types` here.\n",
        "created_at": "2015-08-06T04:57:24Z",
        "updated_at": "2015-08-08T01:38:21Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36381372",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36381372"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36381372"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36381380",
        "pull_request_review_id": null,
        "id": 36381380,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzgxMzgw",
        "diff_hunk": "@@ -0,0 +1,10 @@\n+#!/usr/bin/env bats\n+# Tests for json exporter\n+\n+. \"$BATS_TEST_DIRNAME\"/env.sh >&2\n+\n+@test \"$DBVARIANT json exporter\" {\n+    q=\"SELECT 123::bigint as i, 45.678 as float, TRUE as t, FALSE as f, 'foo bar baz'::text as s, NULL::text as n, ARRAY['asdf qwer\"$'\\t'\"zxcv\"$'\\n'\"1234', '\\\"I''m your father,\\\" said Darth Vader.'] as a, ARRAY[1,2,3] as b\"\n+    expected=\"{\\\"i\\\":123,\\\"float\\\":45.678,\\\"t\\\":true,\\\"f\\\":false,\\\"s\\\":\\\"foo bar baz\\\",\\\"n\\\":null,\\\"a\\\":[\\\"asdf qwer\\\\tzxcv\\\\n1234\\\",\\\"\\\\\\\"I'm your father,\\\\\\\" said Darth Vader.\\\"],\\\"b\\\":[1,2,3]}\"\n+    [[ $(db-query \"$q\" json 0) = \"$expected\" ]]",
        "path": "test/postgresql/export_json.bats",
        "position": null,
        "original_position": 9,
        "commit_id": "456f610515b447ad5e0b1e67f9fd2be9cb7d336e",
        "original_commit_id": "9c4dae8a4cb09f9874471eac447c3504b2061ff5",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "`deepdive sql` command requires `db.url`, `schema.sql`, etc., which are not present for tests.\n",
        "created_at": "2015-08-06T04:57:36Z",
        "updated_at": "2015-08-08T01:38:21Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36381380",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36381380"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36381380"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36381398",
        "pull_request_review_id": null,
        "id": 36381398,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzgxMzk4",
        "diff_hunk": "@@ -0,0 +1,64 @@\n+#! /usr/bin/env python\n+\n+import json, sys, csv\n+\n+def convert_flat_type_func(column_type):\n+  column_type = column_type.lower()\n+  if column_type == \"integer\" or column_type == \"int\" or column_type == \"bigint\":\n+    return lambda x: None if x == \"\" else int(x)\n+  elif column_type == \"float\" or column_type == \"numeric\":\n+    return lambda x: None if x == \"\" else float(x)\n+  elif column_type == \"text\":\n+    return lambda x: None if x == \"\" else x\n+  elif column_type == \"boolean\":\n+    return lambda x: None if x == \"\" else x == \"t\"\n+  else:\n+    raise ValueError(\"Unsupported data type %s\" %column_type)\n+\n+# given a column type, returns a function that takes a string input\n+# and output with the correct type\n+def convert_type_func(column_type):\n+  column_type = column_type.lower()\n+  if column_type.endswith(\"[]\"):\n+    # csv array starts and ends with curly braces\n+    column_type = column_type[:-2]\n+    # string unescaping\n+    # note this is an array as a csv field, we first unescape csv escaping,\n+    # and then do array unescaping, apply csv escaping, and parse the arry\n+    # elements as csv\n+    # csv escapes double quote with two double quotes\n+    # array escapes double quote using backslashes\n+    flat_func = convert_flat_type_func(column_type)\n+    def convert_text_array_func(value):\n+      if value == \"\": return None\n+      arr = csv.reader([value[1:-1].replace('\"\"', '\"').replace('\\\\\"', '\"\"')], delimiter=',', quotechar='\"').next()\n+      return [flat_func(x) for x in arr]\n+    def convert_other_array_func(value):\n+      if value == \"\": return None\n+      arr = csv.reader([value[1:-1]], delimiter=',', quotechar='\"').next()\n+      return [flat_func(x) for x in arr]\n+    if column_type == \"text\":\n+      return convert_text_array_func\n+    else:\n+      return convert_other_array_func\n+  else:\n+    return convert_flat_type_func(column_type)\n+\n+def main():\n+  # get column types\n+  types = sys.argv[1].split(\",\")",
        "path": "util/csvtojson.py",
        "position": null,
        "original_position": 49,
        "commit_id": "456f610515b447ad5e0b1e67f9fd2be9cb7d336e",
        "original_commit_id": "9c4dae8a4cb09f9874471eac447c3504b2061ff5",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Let's use multiple arguments.\n\n``` python\ntypes = sys.argv[1:]\n```\n",
        "created_at": "2015-08-06T04:58:16Z",
        "updated_at": "2015-08-08T01:38:21Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36381398",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36381398"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36381398"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36381407",
        "pull_request_review_id": null,
        "id": 36381407,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzgxNDA3",
        "diff_hunk": "@@ -0,0 +1,10 @@\n+#!/usr/bin/env bats\n+# Tests for json exporter\n+\n+. \"$BATS_TEST_DIRNAME\"/env.sh >&2\n+\n+@test \"$DBVARIANT json exporter\" {\n+    q=\"SELECT 123::bigint as i, 45.678 as float, TRUE as t, FALSE as f, 'foo bar baz'::text as s, NULL::text as n, ARRAY['asdf qwer\"$'\\t'\"zxcv\"$'\\n'\"1234', '\\\"I''m your father,\\\" said Darth Vader.'] as a, ARRAY[1,2,3] as b\"\n+    expected=\"{\\\"a\\\": [\\\"asdf qwer\\\\tzxcv\\\\n1234\\\", \\\"\\\\\\\"I'm your father,\\\\\\\" said Darth Vader.\\\"], \\\"b\\\": [1, 2, 3], \\\"f\\\": false, \\\"i\\\": 123, \\\"float\\\": 45.678, \\\"n\\\": null, \\\"s\\\": \\\"foo bar baz\\\", \\\"t\\\": true}\"\n+    [[ $(db-query \"$q\" json 0) = \"$expected\" ]]\n+}",
        "path": "test/greenplum/export_json.bats",
        "position": null,
        "original_position": 10,
        "commit_id": "456f610515b447ad5e0b1e67f9fd2be9cb7d336e",
        "original_commit_id": "9c4dae8a4cb09f9874471eac447c3504b2061ff5",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "postgresql and greenplum's output is different, postgresql's output does not have spaces after the key colon, and value delimiter.\n",
        "created_at": "2015-08-06T04:58:28Z",
        "updated_at": "2015-08-08T01:38:21Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36381407",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36381407"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36381407"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36381435",
        "pull_request_review_id": null,
        "id": 36381435,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzgxNDM1",
        "diff_hunk": "@@ -0,0 +1,35 @@\n+#!/usr/bin/env bash\n+# db-query -- Evaluates given SQL query against the PostgreSQL database configured for a DeepDive application\n+# > eval \"$(db-parse \"$url\")\"\n+# > db-query SQL FORMAT HEADER\n+##\n+set -eu\n+\n+sql=${1:?No SQL given}; shift\n+format=${1:?No FORMAT given}; shift\n+header=${1:?No HEADER given}; shift\n+\n+copy_option=\n+case $header in\n+    0) ;;\n+    1) copy_option+=\" HEADER\" ;;\n+    *) error \"$header: unrecognized value for HEADER\"\n+esac\n+case $format in\n+    tsv) ;;\n+    csv) copy_option+=\" csv\" ;;\n+    json)\n+        types=\"$(db-prompt -c \"CREATE TEMP TABLE __to_json_temp AS ($sql) LIMIT 0;\n+            COPY (SELECT format_type(atttypid, atttypmod) AS type\n+            FROM pg_attribute\n+            WHERE attrelid = '__to_json_temp'::regclass\n+            AND attnum > 0\n+            AND NOT attisdropped\n+            ORDER BY attnum) TO STDOUT\" | tr '\\n' ',' | sed 's/,$//')\"\n+        db-query \"$sql\" csv 1 | csvtojson.py \"$types\"",
        "path": "shell/driver.greenplum/db-query",
        "position": null,
        "original_position": 29,
        "commit_id": "456f610515b447ad5e0b1e67f9fd2be9cb7d336e",
        "original_commit_id": "9c4dae8a4cb09f9874471eac447c3504b2061ff5",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Sure\n",
        "created_at": "2015-08-06T04:59:29Z",
        "updated_at": "2015-08-08T01:38:21Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36381435",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36381435"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36381435"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36381438",
        "pull_request_review_id": null,
        "id": 36381438,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzgxNDM4",
        "diff_hunk": "@@ -0,0 +1,64 @@\n+#! /usr/bin/env python\n+\n+import json, sys, csv\n+\n+def convert_flat_type_func(column_type):\n+  column_type = column_type.lower()\n+  if column_type == \"integer\" or column_type == \"int\" or column_type == \"bigint\":\n+    return lambda x: None if x == \"\" else int(x)\n+  elif column_type == \"float\" or column_type == \"numeric\":\n+    return lambda x: None if x == \"\" else float(x)\n+  elif column_type == \"text\":\n+    return lambda x: None if x == \"\" else x\n+  elif column_type == \"boolean\":\n+    return lambda x: None if x == \"\" else x == \"t\"\n+  else:\n+    raise ValueError(\"Unsupported data type %s\" %column_type)\n+\n+# given a column type, returns a function that takes a string input\n+# and output with the correct type\n+def convert_type_func(column_type):\n+  column_type = column_type.lower()\n+  if column_type.endswith(\"[]\"):\n+    # csv array starts and ends with curly braces\n+    column_type = column_type[:-2]\n+    # string unescaping\n+    # note this is an array as a csv field, we first unescape csv escaping,\n+    # and then do array unescaping, apply csv escaping, and parse the arry\n+    # elements as csv\n+    # csv escapes double quote with two double quotes\n+    # array escapes double quote using backslashes\n+    flat_func = convert_flat_type_func(column_type)\n+    def convert_text_array_func(value):\n+      if value == \"\": return None\n+      arr = csv.reader([value[1:-1].replace('\"\"', '\"').replace('\\\\\"', '\"\"')], delimiter=',', quotechar='\"').next()",
        "path": "util/csvtojson.py",
        "position": null,
        "original_position": 34,
        "commit_id": "456f610515b447ad5e0b1e67f9fd2be9cb7d336e",
        "original_commit_id": "9c4dae8a4cb09f9874471eac447c3504b2061ff5",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "But isn't the `csv.reader` supposed to handle the first csv escapes?\nThe one in the outer loop that reads `sys.stdin`?\n",
        "created_at": "2015-08-06T04:59:39Z",
        "updated_at": "2015-08-08T01:38:21Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36381438",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36381438"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36381438"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36381546",
        "pull_request_review_id": null,
        "id": 36381546,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzgxNTQ2",
        "diff_hunk": "@@ -0,0 +1,10 @@\n+#!/usr/bin/env bats\n+# Tests for json exporter\n+\n+. \"$BATS_TEST_DIRNAME\"/env.sh >&2\n+\n+@test \"$DBVARIANT json exporter\" {\n+    q=\"SELECT 123::bigint as i, 45.678 as float, TRUE as t, FALSE as f, 'foo bar baz'::text as s, NULL::text as n, ARRAY['asdf qwer\"$'\\t'\"zxcv\"$'\\n'\"1234', '\\\"I''m your father,\\\" said Darth Vader.'] as a, ARRAY[1,2,3] as b\"\n+    expected=\"{\\\"a\\\": [\\\"asdf qwer\\\\tzxcv\\\\n1234\\\", \\\"\\\\\\\"I'm your father,\\\\\\\" said Darth Vader.\\\"], \\\"b\\\": [1, 2, 3], \\\"f\\\": false, \\\"i\\\": 123, \\\"float\\\": 45.678, \\\"n\\\": null, \\\"s\\\": \\\"foo bar baz\\\", \\\"t\\\": true}\"\n+    [[ $(db-query \"$q\" json 0) = \"$expected\" ]]\n+}",
        "path": "test/greenplum/export_json.bats",
        "position": null,
        "original_position": 10,
        "commit_id": "456f610515b447ad5e0b1e67f9fd2be9cb7d336e",
        "original_commit_id": "9c4dae8a4cb09f9874471eac447c3504b2061ff5",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Hmm.. that sounds like a weird reason to keep the two separate.  Since the whitespaces outside string values are not the test's concern, it should rather normalize the json before checking, right?\n",
        "created_at": "2015-08-06T05:03:30Z",
        "updated_at": "2015-08-08T01:38:21Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36381546",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36381546"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36381546"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36381554",
        "pull_request_review_id": null,
        "id": 36381554,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzgxNTU0",
        "diff_hunk": "@@ -0,0 +1,64 @@\n+#! /usr/bin/env python\n+\n+import json, sys, csv\n+\n+def convert_flat_type_func(column_type):\n+  column_type = column_type.lower()\n+  if column_type == \"integer\" or column_type == \"int\" or column_type == \"bigint\":\n+    return lambda x: None if x == \"\" else int(x)\n+  elif column_type == \"float\" or column_type == \"numeric\":\n+    return lambda x: None if x == \"\" else float(x)\n+  elif column_type == \"text\":\n+    return lambda x: None if x == \"\" else x\n+  elif column_type == \"boolean\":\n+    return lambda x: None if x == \"\" else x == \"t\"\n+  else:\n+    raise ValueError(\"Unsupported data type %s\" %column_type)\n+\n+# given a column type, returns a function that takes a string input\n+# and output with the correct type\n+def convert_type_func(column_type):\n+  column_type = column_type.lower()\n+  if column_type.endswith(\"[]\"):\n+    # csv array starts and ends with curly braces\n+    column_type = column_type[:-2]\n+    # string unescaping\n+    # note this is an array as a csv field, we first unescape csv escaping,\n+    # and then do array unescaping, apply csv escaping, and parse the arry\n+    # elements as csv\n+    # csv escapes double quote with two double quotes\n+    # array escapes double quote using backslashes\n+    flat_func = convert_flat_type_func(column_type)\n+    def convert_text_array_func(value):\n+      if value == \"\": return None\n+      arr = csv.reader([value[1:-1].replace('\"\"', '\"').replace('\\\\\"', '\"\"')], delimiter=',', quotechar='\"').next()",
        "path": "util/csvtojson.py",
        "position": null,
        "original_position": 34,
        "commit_id": "456f610515b447ad5e0b1e67f9fd2be9cb7d336e",
        "original_commit_id": "9c4dae8a4cb09f9874471eac447c3504b2061ff5",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Here, the array is inside the csv file, so the array itself is double quoted. Every double quote inside the array is doubled. After we unescape the csv and psql array, the elements of array become normal csv escaped elements.\n",
        "created_at": "2015-08-06T05:03:46Z",
        "updated_at": "2015-08-08T01:38:21Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36381554",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36381554"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36381554"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36381576",
        "pull_request_review_id": null,
        "id": 36381576,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzgxNTc2",
        "diff_hunk": "@@ -0,0 +1,10 @@\n+#!/usr/bin/env bats\n+# Tests for json exporter\n+\n+. \"$BATS_TEST_DIRNAME\"/env.sh >&2\n+\n+@test \"$DBVARIANT json exporter\" {\n+    q=\"SELECT 123::bigint as i, 45.678 as float, TRUE as t, FALSE as f, 'foo bar baz'::text as s, NULL::text as n, ARRAY['asdf qwer\"$'\\t'\"zxcv\"$'\\n'\"1234', '\\\"I''m your father,\\\" said Darth Vader.'] as a, ARRAY[1,2,3] as b\"\n+    expected=\"{\\\"i\\\":123,\\\"float\\\":45.678,\\\"t\\\":true,\\\"f\\\":false,\\\"s\\\":\\\"foo bar baz\\\",\\\"n\\\":null,\\\"a\\\":[\\\"asdf qwer\\\\tzxcv\\\\n1234\\\",\\\"\\\\\\\"I'm your father,\\\\\\\" said Darth Vader.\\\"],\\\"b\\\":[1,2,3]}\"\n+    [[ $(db-query \"$q\" json 0) = \"$expected\" ]]",
        "path": "test/postgresql/export_json.bats",
        "position": null,
        "original_position": 9,
        "commit_id": "456f610515b447ad5e0b1e67f9fd2be9cb7d336e",
        "original_commit_id": "9c4dae8a4cb09f9874471eac447c3504b2061ff5",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I see.  Sounds reasonable for now.  I should relax that requirement at some point.\n",
        "created_at": "2015-08-06T05:04:27Z",
        "updated_at": "2015-08-08T01:38:21Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36381576",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36381576"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36381576"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36383812",
        "pull_request_review_id": null,
        "id": 36383812,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzgzODEy",
        "diff_hunk": "@@ -0,0 +1,10 @@\n+#!/usr/bin/env bats\n+# Tests for json exporter\n+\n+. \"$BATS_TEST_DIRNAME\"/env.sh >&2\n+\n+@test \"$DBVARIANT json exporter\" {\n+    q=\"SELECT 123::bigint as i, 45.678 as float, TRUE as t, FALSE as f, 'foo bar baz'::text as s, NULL::text as n, ARRAY['asdf qwer\"$'\\t'\"zxcv\"$'\\n'\"1234', '\\\"I''m your father,\\\" said Darth Vader.'] as a, ARRAY[1,2,3] as b\"\n+    expected=\"{\\\"a\\\": [\\\"asdf qwer\\\\tzxcv\\\\n1234\\\", \\\"\\\\\\\"I'm your father,\\\\\\\" said Darth Vader.\\\"], \\\"b\\\": [1, 2, 3], \\\"f\\\": false, \\\"i\\\": 123, \\\"float\\\": 45.678, \\\"n\\\": null, \\\"s\\\": \\\"foo bar baz\\\", \\\"t\\\": true}\"\n+    [[ $(db-query \"$q\" json 0) = \"$expected\" ]]\n+}",
        "path": "test/greenplum/export_json.bats",
        "position": null,
        "original_position": 10,
        "commit_id": "456f610515b447ad5e0b1e67f9fd2be9cb7d336e",
        "original_commit_id": "9c4dae8a4cb09f9874471eac447c3504b2061ff5",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I searched a bit, and it seems there is no easy way to normalize json in bash...\n",
        "created_at": "2015-08-06T06:13:23Z",
        "updated_at": "2015-08-08T01:38:21Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36383812",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36383812"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36383812"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36384706",
        "pull_request_review_id": null,
        "id": 36384706,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mzg0NzA2",
        "diff_hunk": "@@ -0,0 +1,10 @@\n+#!/usr/bin/env bats\n+# Tests for json exporter\n+\n+. \"$BATS_TEST_DIRNAME\"/env.sh >&2\n+\n+@test \"$DBVARIANT json exporter\" {\n+    q=\"SELECT 123::bigint as i, 45.678 as float, TRUE as t, FALSE as f, 'foo bar baz'::text as s, NULL::text as n, ARRAY['asdf qwer\"$'\\t'\"zxcv\"$'\\n'\"1234', '\\\"I''m your father,\\\" said Darth Vader.'] as a, ARRAY[1,2,3] as b\"\n+    expected=\"{\\\"a\\\": [\\\"asdf qwer\\\\tzxcv\\\\n1234\\\", \\\"\\\\\\\"I'm your father,\\\\\\\" said Darth Vader.\\\"], \\\"b\\\": [1, 2, 3], \\\"f\\\": false, \\\"i\\\": 123, \\\"float\\\": 45.678, \\\"n\\\": null, \\\"s\\\": \\\"foo bar baz\\\", \\\"t\\\": true}\"\n+    [[ $(db-query \"$q\" json 0) = \"$expected\" ]]\n+}",
        "path": "test/greenplum/export_json.bats",
        "position": null,
        "original_position": 10,
        "commit_id": "456f610515b447ad5e0b1e67f9fd2be9cb7d336e",
        "original_commit_id": "9c4dae8a4cb09f9874471eac447c3504b2061ff5",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Yeah, python is probably a good choice for that and bash is good friend with python.  Actually I just updated the tests myself.  Hope it's helpful.\n",
        "created_at": "2015-08-06T06:33:37Z",
        "updated_at": "2015-08-08T01:38:21Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36384706",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36384706"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36384706"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36384738",
        "pull_request_review_id": null,
        "id": 36384738,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mzg0NzM4",
        "diff_hunk": "@@ -0,0 +1,10 @@\n+#!/usr/bin/env bats\n+# Tests for json exporter\n+\n+. \"$BATS_TEST_DIRNAME\"/env.sh >&2\n+\n+@test \"$DBVARIANT json exporter\" {\n+    q=\"SELECT 123::bigint as i, 45.678 as float, TRUE as t, FALSE as f, 'foo bar baz'::text as s, NULL::text as n, ARRAY['asdf qwer\"$'\\t'\"zxcv\"$'\\n'\"1234', '\\\"I''m your father,\\\" said Darth Vader.'] as a, ARRAY[1,2,3] as b\"\n+    expected=\"{\\\"i\\\":123,\\\"float\\\":45.678,\\\"t\\\":true,\\\"f\\\":false,\\\"s\\\":\\\"foo bar baz\\\",\\\"n\\\":null,\\\"a\\\":[\\\"asdf qwer\\\\tzxcv\\\\n1234\\\",\\\"\\\\\\\"I'm your father,\\\\\\\" said Darth Vader.\\\"],\\\"b\\\":[1,2,3]}\"\n+    [[ $(db-query \"$q\" json 0) = \"$expected\" ]]\n+}",
        "path": "test/postgresql/export_json.bats",
        "position": null,
        "original_position": 10,
        "commit_id": "456f610515b447ad5e0b1e67f9fd2be9cb7d336e",
        "original_commit_id": "9c4dae8a4cb09f9874471eac447c3504b2061ff5",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Done.\n",
        "created_at": "2015-08-06T06:34:23Z",
        "updated_at": "2015-08-08T01:38:21Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36384738",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36384738"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36384738"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36385055",
        "pull_request_review_id": null,
        "id": 36385055,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mzg1MDU1",
        "diff_hunk": "@@ -0,0 +1,64 @@\n+#! /usr/bin/env python\n+\n+import json, sys, csv\n+\n+def convert_flat_type_func(column_type):\n+  column_type = column_type.lower()\n+  if column_type == \"integer\" or column_type == \"int\" or column_type == \"bigint\":\n+    return lambda x: None if x == \"\" else int(x)\n+  elif column_type == \"float\" or column_type == \"numeric\":\n+    return lambda x: None if x == \"\" else float(x)\n+  elif column_type == \"text\":\n+    return lambda x: None if x == \"\" else x\n+  elif column_type == \"boolean\":\n+    return lambda x: None if x == \"\" else x == \"t\"\n+  else:\n+    raise ValueError(\"Unsupported data type %s\" %column_type)\n+\n+# given a column type, returns a function that takes a string input\n+# and output with the correct type\n+def convert_type_func(column_type):\n+  column_type = column_type.lower()\n+  if column_type.endswith(\"[]\"):\n+    # csv array starts and ends with curly braces\n+    column_type = column_type[:-2]\n+    # string unescaping\n+    # note this is an array as a csv field, we first unescape csv escaping,\n+    # and then do array unescaping, apply csv escaping, and parse the arry\n+    # elements as csv\n+    # csv escapes double quote with two double quotes\n+    # array escapes double quote using backslashes\n+    flat_func = convert_flat_type_func(column_type)\n+    def convert_text_array_func(value):\n+      if value == \"\": return None\n+      arr = csv.reader([value[1:-1].replace('\"\"', '\"').replace('\\\\\"', '\"\"')], delimiter=',', quotechar='\"').next()",
        "path": "util/csvtojson.py",
        "position": null,
        "original_position": 34,
        "commit_id": "456f610515b447ad5e0b1e67f9fd2be9cb7d336e",
        "original_commit_id": "9c4dae8a4cb09f9874471eac447c3504b2061ff5",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Please take a look at https://github.com/HazyResearch/mindbender/blob/3f0e24ed0851907302c49ebe1c1d619e7c243df2/gui/frontend/src/mindtagger/array-parsers.coffee#L10\nYou'll see some incorrect cases with the updated tests.\n",
        "created_at": "2015-08-06T06:40:59Z",
        "updated_at": "2015-08-08T01:38:21Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36385055",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/36385055"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/348#discussion_r36385055"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/348"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/37061995",
        "pull_request_review_id": null,
        "id": 37061995,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDYxOTk1",
        "diff_hunk": "@@ -1,253 +1,536 @@\n package org.deepdive.settings\n \n-import org.deepdive.Logging\n-import org.deepdive.Context\n-import org.deepdive.helpers.Helpers\n import com.typesafe.config._\n+import org.deepdive.{Context, Logging}\n+\n import scala.collection.JavaConversions._\n+import scala.language.postfixOps\n import scala.util.Try\n+import scala.util.parsing.combinator.RegexParsers\n \n object SettingsParser extends Logging {\n \n-   def loadFromConfig(rootConfig: Config) : Settings = {\n-    val config = rootConfig.getConfig(\"deepdive\")\n-\n-    val inferenceSettings = loadInferenceSettings(config)\n-    val dbSettings = loadDbSettings(config)\n-    val schemaSettings = loadSchemaSettings(config)\n-    val extractors = loadExtractionSettings(config)\n-    val calibrationSettings = loadCalibrationSettings(config)\n-    val samplerSettings = loadSamplerSettings(config)\n-    val pipelineSettings = loadPipelineSettings(config)\n+  val loadFromConfig =\n+    getDeepDiveConfig _ andThen\n+      loadDatabaseSettings andThen\n+      loadSchemaSettings andThen\n+      loadExtractionSettings andThen\n+      loadInferenceSettings andThen\n+      loadSamplerSettings andThen\n+      loadCalibrationSettings andThen\n+      loadPipelineSettings andThen\n+      checkSettings\n \n-    // Make sure that the variables related to the Greenplum distributed\n-    // filesystem are set if the user wants to use parallel grounding\n-    if (dbSettings.gpload) {\n-      if (dbSettings.gphost == \"\" || dbSettings.gpport == \"\" || dbSettings.gppath == \"\") {\n-        throw new RuntimeException(s\"Parallel Loading is set to true, but one of db.default.gphost, db.default.gpport, or db.default.gppath is not specified\")\n-      }\n-    }\n+  def configEntries(config: Config): Iterable[(String, Config)] = {\n+    config.root.keys map { key => key -> config.getConfig(key) }\n+  }\n \n-    Settings(schemaSettings, extractors, inferenceSettings,\n-      calibrationSettings, samplerSettings, pipelineSettings, dbSettings)\n+  private def getDeepDiveConfig(rootConfig: Config): Settings = {\n+    Settings(config = (rootConfig withFallback ConfigFactory.load()) getConfig(\"deepdive\"))\n   }\n \n-  // Returns: case class DbSetting(driver: String, url: String, user: String, password: String, dbname: String, host: String, port: String)\n-  private def loadDbSettings(config: Config) : DbSettings = {\n-    val dbConfig = Try(config.getConfig(\"db.default\")).getOrElse {\n-      log.warning(\"No schema defined.\")\n-      return DbSettings(Helpers.PsqlDriver, null, null, null, null, null, null,\n-        null, null, null, false, null)\n-    }\n-    val driver = Try(dbConfig.getString(\"driver\")).getOrElse(null)\n-    val url = Try(dbConfig.getString(\"url\")).getOrElse(null)\n-    val user = Try(dbConfig.getString(\"user\")).getOrElse(null)\n-    val password = Try(dbConfig.getString(\"password\")).getOrElse(null)\n-    val dbname = Try(dbConfig.getString(\"dbname\")).getOrElse(null)\n-    val host = Try(dbConfig.getString(\"host\")).getOrElse(null)\n-    val port = Try(dbConfig.getString(\"port\")).getOrElse(null)\n-    val gphost = Try(dbConfig.getString(\"gphost\")).getOrElse(\"\")\n-    val gpport = Try(dbConfig.getString(\"gpport\")).getOrElse(\"\")\n-    var gppath = Try(dbConfig.getString(\"gppath\")).getOrElse(\"\")\n-    val parallelGrounding = Try(config.getConfig(\"inference\").getBoolean(\"parallel_grounding\")).getOrElse(false)\n-    var gpload = Try(dbConfig.getBoolean(\"gpload\")).getOrElse(false) || parallelGrounding\n-    if (gppath.takeRight(1) == \"/\") gppath = gppath.take(gppath.length -1)\n-    log.info(s\"Database settings: user ${user}, dbname ${dbname}, host ${host}, port ${port}.\")\n-    if (gphost != \"\") {\n-      log.info(s\"GPFDIST settings: host ${gphost} port ${gpport} path ${gppath}\")\n-    }\n-    val incrementalModeStr = Try(dbConfig.getString(\"incremental_mode\")).getOrElse(\"ORIGINAL\")\n-    val incrementalMode = incrementalModeStr match {\n-      case \"INCREMENTAL\" => IncrementalMode.INCREMENTAL\n-      case \"MATERIALIZATION\" => IncrementalMode.MATERIALIZATION\n-      case _ => IncrementalMode.ORIGINAL\n+  private def checkSettings(settings: Settings): Settings = {\n+    // val dbDriver = config.getString(\"deepdive.db.default.driver\")\n+    val dbSettings = settings.dbSettings\n+    dbSettings.dbname match {\n+      case \"\" =>\n+        sys.error(s\"parsing dbname failed\")\n+      case _ =>\n     }\n-    val schemaConfig = Try(config.getConfig(\"schema\")).getOrElse {\n-      log.warning(\"No schema defined.\")\n-      null\n+\n+    // check incremental settings\n+    dbSettings.incrementalMode match {\n+      case IncrementalMode.MATERIALIZATION | IncrementalMode.INCREMENTAL =>\n+        if (!settings.pipelineSettings.baseDir.isDefined) {\n+          log.error(s\"base folder not set for incremental run\")\n+          Context.shutdown()\n+        }\n+      case _ =>\n     }\n-    val keyConfig = Try(schemaConfig.getConfig(\"keys\")).getOrElse(null)\n-    val keyMap = keyConfig match {\n-      case null => null\n-      case _ => {\n-        val keyRelations = keyConfig.root.keySet.toList\n-        val keyRelationsWithConfig = keyRelations.zip(keyRelations.map(keyConfig.getStringList))\n-        keyRelationsWithConfig.groupBy(_._1).map { case (k,v) => (k,v.map(_._2).flatten.distinct)}\n-      }\n+\n+    // Make sure the activePipelineName is defined\n+    if (settings.pipelineSettings.activePipelineName.isDefined && settings.pipelineSettings.activePipeline.isEmpty) {\n+      sys.error(s\"${settings.pipelineSettings.activePipelineName.get}: No such pipeline defined\")\n     }\n-    return DbSettings(driver, url, user, password, dbname, host, port,\n-      gphost, gppath, gpport, gpload, incrementalMode, keyMap)\n+\n+    log.info(settings.config.root.render(ConfigRenderOptions.concise().setFormatted(true)))\n+    log.info(settings toString)\n+\n+    settings\n   }\n \n+  private def loadDatabaseSettings(settings: Settings): Settings = {\n+    var config = settings.config withFallback ConfigFactory.parseString(\n+      \"\"\"",
        "path": "src/main/scala/org/deepdive/settings/SettingsParser.scala",
        "position": 128,
        "original_position": 128,
        "commit_id": "243899588acf15fce921ff85bb7481708a1e73d3",
        "original_commit_id": "243899588acf15fce921ff85bb7481708a1e73d3",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "What is this string used for?\n",
        "created_at": "2015-08-14T09:32:24Z",
        "updated_at": "2015-08-14T09:32:24Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/353#discussion_r37061995",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/353",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/37061995"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/353#discussion_r37061995"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/353"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/37062252",
        "pull_request_review_id": null,
        "id": 37062252,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDYyMjUy",
        "diff_hunk": "@@ -1,253 +1,536 @@\n package org.deepdive.settings\n \n-import org.deepdive.Logging\n-import org.deepdive.Context\n-import org.deepdive.helpers.Helpers\n import com.typesafe.config._\n+import org.deepdive.{Context, Logging}\n+\n import scala.collection.JavaConversions._\n+import scala.language.postfixOps\n import scala.util.Try\n+import scala.util.parsing.combinator.RegexParsers\n \n object SettingsParser extends Logging {\n \n-   def loadFromConfig(rootConfig: Config) : Settings = {\n-    val config = rootConfig.getConfig(\"deepdive\")\n-\n-    val inferenceSettings = loadInferenceSettings(config)\n-    val dbSettings = loadDbSettings(config)\n-    val schemaSettings = loadSchemaSettings(config)\n-    val extractors = loadExtractionSettings(config)\n-    val calibrationSettings = loadCalibrationSettings(config)\n-    val samplerSettings = loadSamplerSettings(config)\n-    val pipelineSettings = loadPipelineSettings(config)\n+  val loadFromConfig =\n+    getDeepDiveConfig _ andThen\n+      loadDatabaseSettings andThen\n+      loadSchemaSettings andThen\n+      loadExtractionSettings andThen\n+      loadInferenceSettings andThen\n+      loadSamplerSettings andThen\n+      loadCalibrationSettings andThen\n+      loadPipelineSettings andThen\n+      checkSettings\n \n-    // Make sure that the variables related to the Greenplum distributed\n-    // filesystem are set if the user wants to use parallel grounding\n-    if (dbSettings.gpload) {\n-      if (dbSettings.gphost == \"\" || dbSettings.gpport == \"\" || dbSettings.gppath == \"\") {\n-        throw new RuntimeException(s\"Parallel Loading is set to true, but one of db.default.gphost, db.default.gpport, or db.default.gppath is not specified\")\n-      }\n-    }\n+  def configEntries(config: Config): Iterable[(String, Config)] = {\n+    config.root.keys map { key => key -> config.getConfig(key) }\n+  }\n \n-    Settings(schemaSettings, extractors, inferenceSettings,\n-      calibrationSettings, samplerSettings, pipelineSettings, dbSettings)\n+  private def getDeepDiveConfig(rootConfig: Config): Settings = {\n+    Settings(config = (rootConfig withFallback ConfigFactory.load()) getConfig(\"deepdive\"))\n   }\n \n-  // Returns: case class DbSetting(driver: String, url: String, user: String, password: String, dbname: String, host: String, port: String)\n-  private def loadDbSettings(config: Config) : DbSettings = {\n-    val dbConfig = Try(config.getConfig(\"db.default\")).getOrElse {\n-      log.warning(\"No schema defined.\")\n-      return DbSettings(Helpers.PsqlDriver, null, null, null, null, null, null,\n-        null, null, null, false, null)\n-    }\n-    val driver = Try(dbConfig.getString(\"driver\")).getOrElse(null)\n-    val url = Try(dbConfig.getString(\"url\")).getOrElse(null)\n-    val user = Try(dbConfig.getString(\"user\")).getOrElse(null)\n-    val password = Try(dbConfig.getString(\"password\")).getOrElse(null)\n-    val dbname = Try(dbConfig.getString(\"dbname\")).getOrElse(null)\n-    val host = Try(dbConfig.getString(\"host\")).getOrElse(null)\n-    val port = Try(dbConfig.getString(\"port\")).getOrElse(null)\n-    val gphost = Try(dbConfig.getString(\"gphost\")).getOrElse(\"\")\n-    val gpport = Try(dbConfig.getString(\"gpport\")).getOrElse(\"\")\n-    var gppath = Try(dbConfig.getString(\"gppath\")).getOrElse(\"\")\n-    val parallelGrounding = Try(config.getConfig(\"inference\").getBoolean(\"parallel_grounding\")).getOrElse(false)\n-    var gpload = Try(dbConfig.getBoolean(\"gpload\")).getOrElse(false) || parallelGrounding\n-    if (gppath.takeRight(1) == \"/\") gppath = gppath.take(gppath.length -1)\n-    log.info(s\"Database settings: user ${user}, dbname ${dbname}, host ${host}, port ${port}.\")\n-    if (gphost != \"\") {\n-      log.info(s\"GPFDIST settings: host ${gphost} port ${gpport} path ${gppath}\")\n-    }\n-    val incrementalModeStr = Try(dbConfig.getString(\"incremental_mode\")).getOrElse(\"ORIGINAL\")\n-    val incrementalMode = incrementalModeStr match {\n-      case \"INCREMENTAL\" => IncrementalMode.INCREMENTAL\n-      case \"MATERIALIZATION\" => IncrementalMode.MATERIALIZATION\n-      case _ => IncrementalMode.ORIGINAL\n+  private def checkSettings(settings: Settings): Settings = {\n+    // val dbDriver = config.getString(\"deepdive.db.default.driver\")\n+    val dbSettings = settings.dbSettings\n+    dbSettings.dbname match {\n+      case \"\" =>\n+        sys.error(s\"parsing dbname failed\")\n+      case _ =>\n     }\n-    val schemaConfig = Try(config.getConfig(\"schema\")).getOrElse {\n-      log.warning(\"No schema defined.\")\n-      null\n+\n+    // check incremental settings\n+    dbSettings.incrementalMode match {\n+      case IncrementalMode.MATERIALIZATION | IncrementalMode.INCREMENTAL =>\n+        if (!settings.pipelineSettings.baseDir.isDefined) {\n+          log.error(s\"base folder not set for incremental run\")\n+          Context.shutdown()\n+        }\n+      case _ =>\n     }\n-    val keyConfig = Try(schemaConfig.getConfig(\"keys\")).getOrElse(null)\n-    val keyMap = keyConfig match {\n-      case null => null\n-      case _ => {\n-        val keyRelations = keyConfig.root.keySet.toList\n-        val keyRelationsWithConfig = keyRelations.zip(keyRelations.map(keyConfig.getStringList))\n-        keyRelationsWithConfig.groupBy(_._1).map { case (k,v) => (k,v.map(_._2).flatten.distinct)}\n-      }\n+\n+    // Make sure the activePipelineName is defined\n+    if (settings.pipelineSettings.activePipelineName.isDefined && settings.pipelineSettings.activePipeline.isEmpty) {\n+      sys.error(s\"${settings.pipelineSettings.activePipelineName.get}: No such pipeline defined\")\n     }\n-    return DbSettings(driver, url, user, password, dbname, host, port,\n-      gphost, gppath, gpport, gpload, incrementalMode, keyMap)\n+\n+    log.info(settings.config.root.render(ConfigRenderOptions.concise().setFormatted(true)))\n+    log.info(settings toString)\n+\n+    settings\n   }\n \n+  private def loadDatabaseSettings(settings: Settings): Settings = {\n+    var config = settings.config withFallback ConfigFactory.parseString(\n+      \"\"\"",
        "path": "src/main/scala/org/deepdive/settings/SettingsParser.scala",
        "position": 128,
        "original_position": 128,
        "commit_id": "243899588acf15fce921ff85bb7481708a1e73d3",
        "original_commit_id": "243899588acf15fce921ff85bb7481708a1e73d3",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Oh I got it. `withFallback` combines this string with the input config.\n",
        "created_at": "2015-08-14T09:36:31Z",
        "updated_at": "2015-08-14T09:36:31Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/353#discussion_r37062252",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/353",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/37062252"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/353#discussion_r37062252"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/353"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/37062429",
        "pull_request_review_id": null,
        "id": 37062429,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDYyNDI5",
        "diff_hunk": "@@ -1,253 +1,536 @@\n package org.deepdive.settings",
        "path": "src/main/scala/org/deepdive/settings/SettingsParser.scala",
        "position": 1,
        "original_position": 1,
        "commit_id": "243899588acf15fce921ff85bb7481708a1e73d3",
        "original_commit_id": "243899588acf15fce921ff85bb7481708a1e73d3",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Is it better to combine the settings in one file or in separate files? It seems after combining, this file is getting big and comprehensive.\n",
        "created_at": "2015-08-14T09:39:15Z",
        "updated_at": "2015-08-14T09:39:15Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/353#discussion_r37062429",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/353",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/37062429"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/353#discussion_r37062429"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/353"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/37062846",
        "pull_request_review_id": null,
        "id": 37062846,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDYyODQ2",
        "diff_hunk": "@@ -35,18 +35,20 @@ object TestHelper {\n   }\n \n   def getDbSettings() =\n-      DbSettings(getDriverFromEnv,      // driver\n-          System.getenv(\"DBCONNSTRING\"),  // nrl\n-          System.getenv(\"DBUSER\"),        // user\n-          System.getenv(\"DBPASSWORD\"),    // password\n-          System.getenv(\"DBNAME\"),        // dbname\n-          System.getenv(\"DBHOST\"),\n-          System.getenv(\"DBPORT\"),\n-          System.getenv(\"GPHOST\"),\n-          System.getenv(\"GPPATH\"),\n-          System.getenv(\"GPPORT\"),\n-          getGPLOADEnv(),\n-          getIsIncrementalEnv())\n+      DbSettings(\n+        driver = getDriverFromEnv, // driver\n+        url = System.getenv(\"DBCONNSTRING\"), // nrl\n+        user = System.getenv(\"DBUSER\"), // user\n+        password = System.getenv(\"DBPASSWORD\"), // password\n+        dbname = System.getenv(\"DBNAME\"), // dbname\n+        host = System.getenv(\"DBHOST\"),\n+        port = System.getenv(\"DBPORT\"),\n+        gphost = System.getenv(\"GPHOST\"),\n+        gppath = System.getenv(\"GPPATH\"),\n+        gpport = System.getenv(\"GPPORT\"),\n+        gpload = getGPLOADEnv(),\n+        incrementalMode = getIsIncrementalEnv()",
        "path": "src/test/scala/helpers/TestHelper.scala",
        "position": 28,
        "original_position": 28,
        "commit_id": "243899588acf15fce921ff85bb7481708a1e73d3",
        "original_commit_id": "243899588acf15fce921ff85bb7481708a1e73d3",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Great! Named arguments are much clearer.\n",
        "created_at": "2015-08-14T09:46:00Z",
        "updated_at": "2015-08-14T09:46:00Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/353#discussion_r37062846",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/353",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/37062846"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/353#discussion_r37062846"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/353"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/37062736",
        "pull_request_review_id": null,
        "id": 37062736,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDYyNzM2",
        "diff_hunk": "@@ -35,18 +35,20 @@ object TestHelper {\n   }\n \n   def getDbSettings() =\n-      DbSettings(getDriverFromEnv,      // driver\n-          System.getenv(\"DBCONNSTRING\"),  // nrl\n-          System.getenv(\"DBUSER\"),        // user\n-          System.getenv(\"DBPASSWORD\"),    // password\n-          System.getenv(\"DBNAME\"),        // dbname\n-          System.getenv(\"DBHOST\"),\n-          System.getenv(\"DBPORT\"),\n-          System.getenv(\"GPHOST\"),\n-          System.getenv(\"GPPATH\"),\n-          System.getenv(\"GPPORT\"),\n-          getGPLOADEnv(),\n-          getIsIncrementalEnv())\n+      DbSettings(",
        "path": "src/test/scala/helpers/TestHelper.scala",
        "position": 16,
        "original_position": 16,
        "commit_id": "243899588acf15fce921ff85bb7481708a1e73d3",
        "original_commit_id": "243899588acf15fce921ff85bb7481708a1e73d3",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "(Not related to this PR)\nDo you think using a map for `DbSettings` is better than a case class. When adding new settings inside `DbSettings`, we need to to change all the initializations for the case class (the copy constructors below solved much of this problem). A map may be easier to maintain.\n",
        "created_at": "2015-08-14T09:43:52Z",
        "updated_at": "2015-08-14T09:48:31Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/353#discussion_r37062736",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/353",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/37062736"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/353#discussion_r37062736"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/353"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/37111286",
        "pull_request_review_id": null,
        "id": 37111286,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTExMjg2",
        "diff_hunk": "@@ -1,253 +1,536 @@\n package org.deepdive.settings",
        "path": "src/main/scala/org/deepdive/settings/SettingsParser.scala",
        "position": 1,
        "original_position": 1,
        "commit_id": "243899588acf15fce921ff85bb7481708a1e73d3",
        "original_commit_id": "243899588acf15fce921ff85bb7481708a1e73d3",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Larger functions are no doubt bad, but I don't think that necessarily applies to files.  I found it much easier to stare at one file than getting lost while trying to guess where something might exist.  Since Scala doesn't force us to sync the filenames with its contents, I think it's better to keep the number of files minimal.  The only downside I see is slower compilation due to putting too many independent classes in the same file.  But I believe settings classes don't need frequent changes.  Does this sound convincing?\n",
        "created_at": "2015-08-14T19:34:09Z",
        "updated_at": "2015-08-14T19:34:09Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/353#discussion_r37111286",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/353",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/37111286"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/353#discussion_r37111286"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/353"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/37111862",
        "pull_request_review_id": null,
        "id": 37111862,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTExODYy",
        "diff_hunk": "@@ -35,18 +35,20 @@ object TestHelper {\n   }\n \n   def getDbSettings() =\n-      DbSettings(getDriverFromEnv,      // driver\n-          System.getenv(\"DBCONNSTRING\"),  // nrl\n-          System.getenv(\"DBUSER\"),        // user\n-          System.getenv(\"DBPASSWORD\"),    // password\n-          System.getenv(\"DBNAME\"),        // dbname\n-          System.getenv(\"DBHOST\"),\n-          System.getenv(\"DBPORT\"),\n-          System.getenv(\"GPHOST\"),\n-          System.getenv(\"GPPATH\"),\n-          System.getenv(\"GPPORT\"),\n-          getGPLOADEnv(),\n-          getIsIncrementalEnv())\n+      DbSettings(",
        "path": "src/test/scala/helpers/TestHelper.scala",
        "position": 16,
        "original_position": 16,
        "commit_id": "243899588acf15fce921ff85bb7481708a1e73d3",
        "original_commit_id": "243899588acf15fce921ff85bb7481708a1e73d3",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I'm against using Map.  I initially thought we should directly use the Config object instead of creating an array of Settings classes.  But it seems Config is awkward to use (throws exception for null) and since we already have this typed intermediate representation it's good to maintain it cleanly.\nUsing case classes with named argument syntax and default values eliminates the need for impacting all constructors.  And right, using `.copy()` on top of that further reduces the impact.  This PR is actually all about these two to make it easier to further refactor things.\n",
        "created_at": "2015-08-14T19:40:15Z",
        "updated_at": "2015-08-14T19:40:15Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/353#discussion_r37111862",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/353",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/37111862"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/353#discussion_r37111862"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/353"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/37111965",
        "pull_request_review_id": null,
        "id": 37111965,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTExOTY1",
        "diff_hunk": "@@ -35,18 +35,20 @@ object TestHelper {\n   }\n \n   def getDbSettings() =\n-      DbSettings(getDriverFromEnv,      // driver\n-          System.getenv(\"DBCONNSTRING\"),  // nrl\n-          System.getenv(\"DBUSER\"),        // user\n-          System.getenv(\"DBPASSWORD\"),    // password\n-          System.getenv(\"DBNAME\"),        // dbname\n-          System.getenv(\"DBHOST\"),\n-          System.getenv(\"DBPORT\"),\n-          System.getenv(\"GPHOST\"),\n-          System.getenv(\"GPPATH\"),\n-          System.getenv(\"GPPORT\"),\n-          getGPLOADEnv(),\n-          getIsIncrementalEnv())\n+      DbSettings(\n+        driver = getDriverFromEnv, // driver\n+        url = System.getenv(\"DBCONNSTRING\"), // nrl\n+        user = System.getenv(\"DBUSER\"), // user\n+        password = System.getenv(\"DBPASSWORD\"), // password\n+        dbname = System.getenv(\"DBNAME\"), // dbname\n+        host = System.getenv(\"DBHOST\"),\n+        port = System.getenv(\"DBPORT\"),\n+        gphost = System.getenv(\"GPHOST\"),\n+        gppath = System.getenv(\"GPPATH\"),\n+        gpport = System.getenv(\"GPPORT\"),\n+        gpload = getGPLOADEnv(),\n+        incrementalMode = getIsIncrementalEnv()",
        "path": "src/test/scala/helpers/TestHelper.scala",
        "position": 28,
        "original_position": 28,
        "commit_id": "243899588acf15fce921ff85bb7481708a1e73d3",
        "original_commit_id": "243899588acf15fce921ff85bb7481708a1e73d3",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Sure, everyone should use this style unless there are only one or two args.\n",
        "created_at": "2015-08-14T19:41:23Z",
        "updated_at": "2015-08-14T19:41:23Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/353#discussion_r37111965",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/353",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/37111965"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/353#discussion_r37111965"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/353"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/37684864",
        "pull_request_review_id": null,
        "id": 37684864,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0ODY0",
        "diff_hunk": "@@ -42,8 +42,8 @@ You can follow the [corresponding section in the original walkthrough](../basics\n Alternatively, you can try the handy scripts included incremental example we include in the source tree.\n \n ```bash\n-cd examples/spouse_example/postgres/incremental\n-./0-setup.sh spouse_example.f1.ddl inc-base.out\n+cd examples/spouse_example/incremental",
        "path": "doc/doc/advanced/incremental.md",
        "position": null,
        "original_position": 6,
        "commit_id": "95d699f660d5d3702e0c716d0895a59502680504",
        "original_commit_id": "832ddbfbd4db39e1345ba22b1fd05fd72279438a",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "It seems your merge/rebase reverted some of my fixes on the docs.  Could you make sure your commits are just renaming .ddl -> ddlog?  Unfortunately, we can't check whether the gist-it stuff works until the PR is merged (something to be fixed using Jekyll's variables), but you can easily see the paths without `postgres/` will be incorrect.\n\nThis is why it's super important to always base your work off of origin/master to avoid merge headaches.\n\n``` bash\ngit remote update\ngit checkout -b work origin/master\n```\n",
        "created_at": "2015-08-21T22:39:04Z",
        "updated_at": "2015-08-23T22:44:35Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/363#discussion_r37684864",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/363",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/37684864"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/363#discussion_r37684864"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/363"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/37685310",
        "pull_request_review_id": null,
        "id": 37685310,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg1MzEw",
        "diff_hunk": "@@ -5,7 +5,7 @@ layout: default\n # Writing Applications in DDlog\n \n DDlog is a higher-level language for writing DeepDive applications in succinct Datalog-like syntax.\n-We are gradually extending the language to allow expression of advanced SQL queries used by complex extractors as well as a variety of factors with Markov Logic-like syntax.\n+We are gradually extending the language to allow expression of advanced SQL queries used by complex extractors as well as a variety of factors with Markov Logic-like syntax. A reference for ddlog lanugage features can be found [here](https://github.com/HazyResearch/ddlog/wiki/DDlog-Language-Features).",
        "path": "doc/doc/basics/ddlog.md",
        "position": null,
        "original_position": 5,
        "commit_id": "95d699f660d5d3702e0c716d0895a59502680504",
        "original_commit_id": "832ddbfbd4db39e1345ba22b1fd05fd72279438a",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Sorry for being picky here, but I think [keeping one sentence per line](https://news.ycombinator.com/item?id=4642395) makes it easy to see what's changed.\n",
        "created_at": "2015-08-21T22:47:03Z",
        "updated_at": "2015-08-23T22:44:35Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/363#discussion_r37685310",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/363",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/37685310"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/363#discussion_r37685310"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/363"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/37685383",
        "pull_request_review_id": null,
        "id": 37685383,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg1Mzgz",
        "diff_hunk": "@@ -102,59 +102,69 @@ The function is supposed to take as input a triple of sentence id, array of word\n The Python implementation `udf/ext_people.py` takes the input as tab-separated values in each line and  outputs in the same format.\n \n ```\n-function ext_people over like ext_people_input\n-                 returns like people_mentions\n+function ext_people over (sentence_id text, words text, ner_tags text)\n+                 returns rows like people_mentions\n   implementation \"udf/ext_people.py\" handles tsv lines.\n ```\n \n Then this user-defined function `ext_people` can be called in the following way to add more tuples to `people_mentions` taking triples from the `sentences` relation:\n \n ```\n-people_mentions :- !ext_people(ext_people_input).\n-ext_people_input(s, ARRAY_TO_STRING(words, \"~^~\"), ARRAY_TO_STRING(ner_tags, \"~^~\")) :-\n-  sentences(_1, _2, words, _3, _4, _5, ner_tags, _6, s).\n+people_mentions +=\n+  ext_people(s, ARRAY_TO_STRING(words, \"~^~\"), ARRAY_TO_STRING(ner_tags, \"~^~\")) :-\n+  sentences(_, _, words, _, _, _, ner_tags, _, s).\n ```\n \n In a similar way, we can have another UDF map candidate relationships and supervise them.\n \n ```\n-function ext_has_spouse over like ext_has_spouse_input\n-                     returns like has_spouse_candidates\n+function ext_has_spouse over (sentence_id text, mid1 text, text1 text, mid2 text, text2 text)\n+                     returns rows like has_spouse_candidates\n   implementation \"udf/ext_has_spouse.py\" handles tsv lines.\n \n-has_spouse_candidates :- !ext_has_spouse(ext_has_spouse_input).\n-ext_has_spouse_input(s, p1_id, p1_text, p2_id, p2_text) :-\n-  people_mentions(s, _1, _2, p1_text, p1_id),\n-  people_mentions(s, _3, _4, p2_text, p2_id).\n+has_spouse_candidates +=\n+  ext_has_spouse(s, p1_id, p1_text, p2_id, p2_text) :-\n+  people_mentions(s, _, _, p1_text, p1_id),\n+  people_mentions(s, _, _, p2_text, p2_id).\n ```\n \n ### Feature Extraction Rules\n Also, we use a UDF to extract features for the candidate relationships.\n \n ```\n-function ext_has_spouse_features over like ext_has_spouse_features_input\n-                              returns like has_spouse_features\n+function ext_has_spouse_features over rows like ext_has_spouse_features_input\n+                              returns rows like has_spouse_features\n   implementation \"udf/ext_has_spouse_features.py\" handles tsv lines.\n \n-has_spouse_features :- !ext_has_spouse_features(ext_has_spouse_features_input).\n-ext_has_spouse_features_input(ARRAY_TO_STRING(words, \"~^~\"), rid, p1idx, p1len, p2idx, p2len) :-\n-  sentences(_1, _2, words, _3, _4, _5, _6, _7, s),\n-  has_spouse_candidates(person1_id, person2_id, s, _8, rid, _9),\n-  people_mentions(s, p1idx, p1len, _10, person1_id),\n-  people_mentions(s, p2idx, p2len, _11, person2_id).\n+function ext_has_spouse_features over (words text, rid text, position1 int, length1 int, position2 int, length2 int)\n+                              returns rows like has_spouse_features\n+  implementation \"udf/ext_has_spouse_features.py\" handles tsv lines.\n+\n+has_spouse_features +=\n+  ext_has_spouse_features(ARRAY_TO_STRING(words, \"~^~\"), rid, p1idx, p1len, p2idx, p2len) :-\n+  sentences(_, _, words, _, _, _, _, _, s),\n+  has_spouse_candidates(person1_id, person2_id, s, _, rid, _),\n+  people_mentions(s, p1idx, p1len, _, person1_id),\n+  people_mentions(s, p2idx, p2len, _, person2_id).\n ```\n \n \n ### Inference Rules\n-Finally, we define a binary classifier for our boolean variable `has_spouse`.\n+Now we need to generate variables from the candidate relation. This is through\n+the *scoping rule*/*supervision rule* below. In the scoping rule, variables are generated from the\n+body and distinct on the key given in the head. The supervision for the variables\n+is annotated with `@label(l)`.\n \n ```\n-has_spouse(rid) :- has_spouse_candidates(_1, _2, _3, _4, rid, l)\n-label = l.\n+@label(l)\n+has_spouse(rid) :- has_spouse_candidates(_1, _2, _3, _4, rid, l).",
        "path": "doc/doc/basics/ddlog.md",
        "position": null,
        "original_position": 90,
        "commit_id": "95d699f660d5d3702e0c716d0895a59502680504",
        "original_commit_id": "832ddbfbd4db39e1345ba22b1fd05fd72279438a",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Shouldn't we remove the numbers from here as well?\n",
        "created_at": "2015-08-21T22:48:25Z",
        "updated_at": "2015-08-23T22:44:35Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/363#discussion_r37685383",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/363",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/37685383"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/363#discussion_r37685383"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/363"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/37685563",
        "pull_request_review_id": null,
        "id": 37685563,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg1NTYz",
        "diff_hunk": "@@ -102,59 +102,69 @@ The function is supposed to take as input a triple of sentence id, array of word\n The Python implementation `udf/ext_people.py` takes the input as tab-separated values in each line and  outputs in the same format.\n \n ```\n-function ext_people over like ext_people_input\n-                 returns like people_mentions\n+function ext_people over (sentence_id text, words text, ner_tags text)\n+                 returns rows like people_mentions\n   implementation \"udf/ext_people.py\" handles tsv lines.\n ```\n \n Then this user-defined function `ext_people` can be called in the following way to add more tuples to `people_mentions` taking triples from the `sentences` relation:\n \n ```\n-people_mentions :- !ext_people(ext_people_input).\n-ext_people_input(s, ARRAY_TO_STRING(words, \"~^~\"), ARRAY_TO_STRING(ner_tags, \"~^~\")) :-\n-  sentences(_1, _2, words, _3, _4, _5, ner_tags, _6, s).\n+people_mentions +=\n+  ext_people(s, ARRAY_TO_STRING(words, \"~^~\"), ARRAY_TO_STRING(ner_tags, \"~^~\")) :-\n+  sentences(_, _, words, _, _, _, ner_tags, _, s).\n ```\n \n In a similar way, we can have another UDF map candidate relationships and supervise them.\n \n ```\n-function ext_has_spouse over like ext_has_spouse_input\n-                     returns like has_spouse_candidates\n+function ext_has_spouse over (sentence_id text, mid1 text, text1 text, mid2 text, text2 text)\n+                     returns rows like has_spouse_candidates\n   implementation \"udf/ext_has_spouse.py\" handles tsv lines.\n \n-has_spouse_candidates :- !ext_has_spouse(ext_has_spouse_input).\n-ext_has_spouse_input(s, p1_id, p1_text, p2_id, p2_text) :-\n-  people_mentions(s, _1, _2, p1_text, p1_id),\n-  people_mentions(s, _3, _4, p2_text, p2_id).\n+has_spouse_candidates +=\n+  ext_has_spouse(s, p1_id, p1_text, p2_id, p2_text) :-\n+  people_mentions(s, _, _, p1_text, p1_id),\n+  people_mentions(s, _, _, p2_text, p2_id).\n ```\n \n ### Feature Extraction Rules\n Also, we use a UDF to extract features for the candidate relationships.\n \n ```\n-function ext_has_spouse_features over like ext_has_spouse_features_input\n-                              returns like has_spouse_features\n+function ext_has_spouse_features over rows like ext_has_spouse_features_input\n+                              returns rows like has_spouse_features\n   implementation \"udf/ext_has_spouse_features.py\" handles tsv lines.\n \n-has_spouse_features :- !ext_has_spouse_features(ext_has_spouse_features_input).\n-ext_has_spouse_features_input(ARRAY_TO_STRING(words, \"~^~\"), rid, p1idx, p1len, p2idx, p2len) :-\n-  sentences(_1, _2, words, _3, _4, _5, _6, _7, s),\n-  has_spouse_candidates(person1_id, person2_id, s, _8, rid, _9),\n-  people_mentions(s, p1idx, p1len, _10, person1_id),\n-  people_mentions(s, p2idx, p2len, _11, person2_id).\n+function ext_has_spouse_features over (words text, rid text, position1 int, length1 int, position2 int, length2 int)\n+                              returns rows like has_spouse_features\n+  implementation \"udf/ext_has_spouse_features.py\" handles tsv lines.\n+\n+has_spouse_features +=\n+  ext_has_spouse_features(ARRAY_TO_STRING(words, \"~^~\"), rid, p1idx, p1len, p2idx, p2len) :-\n+  sentences(_, _, words, _, _, _, _, _, s),\n+  has_spouse_candidates(person1_id, person2_id, s, _, rid, _),\n+  people_mentions(s, p1idx, p1len, _, person1_id),\n+  people_mentions(s, p2idx, p2len, _, person2_id).",
        "path": "doc/doc/basics/ddlog.md",
        "position": null,
        "original_position": 75,
        "commit_id": "95d699f660d5d3702e0c716d0895a59502680504",
        "original_commit_id": "832ddbfbd4db39e1345ba22b1fd05fd72279438a",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "It'd be nice if we could use gist-it here as well instead of cut/pasting the code.  Keeping the line numbers in sync will be a bit tricky though..  Maybe we can put markers as comment in the actual code and grab their line numbers to always generate an accurate gist-it url to embed?\n",
        "created_at": "2015-08-21T22:52:20Z",
        "updated_at": "2015-08-23T22:44:35Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/363#discussion_r37685563",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/363",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/37685563"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/363#discussion_r37685563"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/363"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/37685657",
        "pull_request_review_id": null,
        "id": 37685657,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg1NjU3",
        "diff_hunk": "@@ -89,23 +89,23 @@ It will do the following:\n ### Incremental Phase\n \n In the incremental phase, suppose you added feature 2 to your udf, `ext_has_spouse_features.f2.py`.\n-Let's say you saved this modified udf in a file named `ext_has_spouse_features.f2.ddl`.\n+Let's say you saved this modified udf in a file named `ext_has_spouse_features.f2.ddlog`.\n \n <script src=\"https://gist-it.appspot.com/github.com/HazyResearch/deepdive/blob/master/examples/spouse_example/postgres/incremental/udf/ext_has_spouse_features.f2.py?footer=minimal&slice=27:39\">\n </script>\n \n \n You need to mark in the DDlog program which udf has been modified to produce the correct incremental pipeline.\n-Let's say you saved this modified DDlog program in a file named `spouse_example.f2.ddl`.\n+Let's say you saved this modified DDlog program in a file named `spouse_example.f2.ddlog`.\n \n-<script src=\"https://gist-it.appspot.com/github.com/HazyResearch/deepdive/blob/master/examples/spouse_example/postgres/incremental/spouse_example.f2.ddl?footer=minimal&slice=67:70\">\n+<script src=\"http://gist-it.appspot.com/https://github.com/HazyResearch/deepdive/blob/master/examples/spouse_example/incremental/spouse_example.f2.ddlog?footer=minimal&slice=67:70\">",
        "path": "doc/doc/advanced/incremental.md",
        "position": null,
        "original_position": 53,
        "commit_id": "95d699f660d5d3702e0c716d0895a59502680504",
        "original_commit_id": "832ddbfbd4db39e1345ba22b1fd05fd72279438a",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Make sure the line numbers are in sync with the modified code (67-70 probably moved)\n",
        "created_at": "2015-08-21T22:54:13Z",
        "updated_at": "2015-08-23T22:44:35Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/363#discussion_r37685657",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/363",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/37685657"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/363#discussion_r37685657"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/363"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/37718175",
        "pull_request_review_id": null,
        "id": 37718175,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzE4MTc1",
        "diff_hunk": "@@ -102,6 +148,15 @@ case $(uname) in\n         #elif [[ -e /etc/redhat-release ]]; then\n         #    # CentOS/RedHat\n         #    os=RedHat\n+        else # unrecognized Linux distro\n+            os=\n+            # try to grab a Linux distro identifier\n+            {\n+                set -o pipefail\n+                hint=$(lsb_release -i | cut -f2) ||",
        "path": "util/install.sh",
        "position": 76,
        "original_position": 76,
        "commit_id": "4597e4a8149dcd089c4c43e90ef45b849995b429",
        "original_commit_id": "b5595da160441ddd72f708a59bbd5a0e8eb4c0be",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Will there be a problem if `lsb_release` or `/etc/redhat-release` does not exist?\n",
        "created_at": "2015-08-24T04:04:31Z",
        "updated_at": "2015-08-24T08:41:48Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/365#discussion_r37718175",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/365",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/37718175"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/365#discussion_r37718175"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/365"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/37718371",
        "pull_request_review_id": null,
        "id": 37718371,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzE4Mzcx",
        "diff_hunk": "@@ -1,53 +1,40 @@\n #!/usr/bin/env bash\n-# DeepDive installers for Ubuntu Linux\n+# DeepDive installers for Debian/Ubuntu Linux\n \n-LSB=$(lsb_release -r 2>/dev/null)\n-case ${LSB##*\t} in\n-    12.04|14.04|15.04) true ;;\n-    *) error \"Ubuntu $LSB found: This installer only works with Ubuntu 12.04, 14.04, and 15.04.\"\n+LSB=$(lsb_release -ir | cut -f2)",
        "path": "util/install/install.Ubuntu.sh",
        "position": null,
        "original_position": 9,
        "commit_id": "4597e4a8149dcd089c4c43e90ef45b849995b429",
        "original_commit_id": "b5595da160441ddd72f708a59bbd5a0e8eb4c0be",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "On raiders4 and raiders5 (Ubuntu 12.04), I got `No LSB modules are available` when trying `lsb_release`. Should we deal with `lsb_release` before using it?\n",
        "created_at": "2015-08-24T04:14:17Z",
        "updated_at": "2015-08-24T08:41:48Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/365#discussion_r37718371",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/365",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/37718371"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/365#discussion_r37718371"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/365"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/37729295",
        "pull_request_review_id": null,
        "id": 37729295,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzI5Mjk1",
        "diff_hunk": "@@ -1,53 +1,40 @@\n #!/usr/bin/env bash\n-# DeepDive installers for Ubuntu Linux\n+# DeepDive installers for Debian/Ubuntu Linux\n \n-LSB=$(lsb_release -r 2>/dev/null)\n-case ${LSB##*\t} in\n-    12.04|14.04|15.04) true ;;\n-    *) error \"Ubuntu $LSB found: This installer only works with Ubuntu 12.04, 14.04, and 15.04.\"\n+LSB=$(lsb_release -ir | cut -f2)",
        "path": "util/install/install.Ubuntu.sh",
        "position": null,
        "original_position": 9,
        "commit_id": "4597e4a8149dcd089c4c43e90ef45b849995b429",
        "original_commit_id": "b5595da160441ddd72f708a59bbd5a0e8eb4c0be",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I think you need to add `-ri` flags to get normal output.  `lsb_release` seems to be always available on Debian-based systems.  Most base packages are depending on it, so should be installed.  Btw when `lsb_release` goes wrong, the current code will just show warnings and still let the user run the defined installers.  Maybe I should make it more explicit by adding a `|| true`.\n",
        "created_at": "2015-08-24T08:34:59Z",
        "updated_at": "2015-08-24T08:41:48Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/365#discussion_r37729295",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/365",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/37729295"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/365#discussion_r37729295"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/365"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/37729436",
        "pull_request_review_id": null,
        "id": 37729436,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzI5NDM2",
        "diff_hunk": "@@ -102,6 +148,15 @@ case $(uname) in\n         #elif [[ -e /etc/redhat-release ]]; then\n         #    # CentOS/RedHat\n         #    os=RedHat\n+        else # unrecognized Linux distro\n+            os=\n+            # try to grab a Linux distro identifier\n+            {\n+                set -o pipefail\n+                hint=$(lsb_release -i | cut -f2) ||",
        "path": "util/install.sh",
        "position": 76,
        "original_position": 76,
        "commit_id": "4597e4a8149dcd089c4c43e90ef45b849995b429",
        "original_commit_id": "b5595da160441ddd72f708a59bbd5a0e8eb4c0be",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Good point.  The intention was to then set hint empty and just display a warning without it.  It seems the second line may fail if `redhat-release` doesn't exist.  I'll bulletproof it.\n",
        "created_at": "2015-08-24T08:36:55Z",
        "updated_at": "2015-08-24T08:41:48Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/365#discussion_r37729436",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/365",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/37729436"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/365#discussion_r37729436"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/365"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39349289",
        "pull_request_review_id": null,
        "id": 39349289,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ5Mjg5",
        "diff_hunk": "@@ -0,0 +1,28 @@\n+#!/usr/bin/env bash\n+# deepdive-load -- Loads a table from a file or Pipe's output.\n+# You must either have the DEEPDIVE_DB_URL environment set to a proper URL or\n+# run this command under a DeepDive application where the URL is set in a db.url file.\n+#\n+# > deepdive load table file format\n+#\n+# > deepdive load sentences sentences.tsv tsv\n+# > deepdive load sentences sentences.csv csv\n+#\n+##\n+set -eu\n+\n+# parse database settings\n+. load-db-driver.sh\n+\n+if [[ $# -gt 1 ]]; then\n+    table=$1; shift\n+    file=$1; shift\n+    if [[ $# -gt 0 ]]; then\n+        format=$1; shift\n+    else\n+        format=tsv\n+    fi\n+    db-load $table $file $format\n+else\n+    error \"Missing table name or file name\"",
        "path": "shell/deepdive-load",
        "position": null,
        "original_position": 27,
        "commit_id": "82ec76177eac5615c178a8837f490250ca0ec0fc",
        "original_commit_id": "a57c32ac0be448fdc5062bb2f93e66a9c7707c26",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Please move this usage error checking to the top to avoid unnecessary global if blocks:\n\n``` bash\n# ...\n. load-db-driver.sh\n\n[[ $# -gt 1 ]] || error \"Missing table name or file name\"\n\ntable=$1; shift\n# ...\n```\n",
        "created_at": "2015-09-13T16:38:27Z",
        "updated_at": "2015-09-16T00:41:36Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/368#discussion_r39349289",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/368",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39349289"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/368#discussion_r39349289"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/368"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39349296",
        "pull_request_review_id": null,
        "id": 39349296,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ5Mjk2",
        "diff_hunk": "@@ -0,0 +1,20 @@\n+#!/usr/bin/env bash\n+# db-load -- Loads file to the PostgreSQL database configured for a DeepDive application",
        "path": "shell/driver.postgresql/db-load",
        "position": 2,
        "original_position": 2,
        "commit_id": "82ec76177eac5615c178a8837f490250ca0ec0fc",
        "original_commit_id": "a57c32ac0be448fdc5062bb2f93e66a9c7707c26",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I think you need to add this new `load` operation to `load-db-driver.sh` as well.\n",
        "created_at": "2015-09-13T16:38:31Z",
        "updated_at": "2015-09-16T00:41:36Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/368#discussion_r39349296",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/368",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39349296"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/368#discussion_r39349296"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/368"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39349244",
        "pull_request_review_id": null,
        "id": 39349244,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ5MjQ0",
        "diff_hunk": "@@ -91,6 +91,15 @@ fullConfig=$run_dir/deepdive.conf\n     ddlog compile \"${ddlogFiles[@]}\"\n     export PIPELINE=  # XXX ddlog shouldn't emit this\n     : ${Pipeline:=endtoend}\n+\n+    # set PARALLELISM env var, use max parallelism if the variable is not set\n+    if [[ $(uname) = 'Linux' ]]; then",
        "path": "shell/deepdive-run",
        "position": null,
        "original_position": 6,
        "commit_id": "2bec6330f5d5d7c0412490fc7ca3c1df4c505a32",
        "original_commit_id": "70e8342c2bafcdf7f69f5f6d448acaa643945c42",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Checking if `nproc` or `sysctl` is available makes more sense than relying on uname.  You could chain the options with something like:\n\n``` bash\n: ${PARALLELISM:=$({\n    # Linux typically has coreutils which includes nproc\n    nproc ||\n    # OS X\n    sysctl -n hw.ncpu ||\n    # fall back to 1\n    echo 1\n} 2>/dev/null)}\n```\n",
        "created_at": "2015-09-13T16:35:13Z",
        "updated_at": "2015-09-20T10:40:11Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/376#discussion_r39349244",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/376",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39349244"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/376#discussion_r39349244"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/376"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39349264",
        "pull_request_review_id": null,
        "id": 39349264,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ5MjY0",
        "diff_hunk": "@@ -16,8 +16,19 @@ db-init \"$@\"\n \n # make sure the necessary tables are all created\n if [[ -e app.ddlog ]]; then\n-    # TODO export schema.sql from ddlog instead of running initdb pipeline\n-    deepdive-run initdb\n+    if [[ $# -gt 0 ]]; then",
        "path": "shell/deepdive-initdb",
        "position": null,
        "original_position": 6,
        "commit_id": "2bec6330f5d5d7c0412490fc7ca3c1df4c505a32",
        "original_commit_id": "70e8342c2bafcdf7f69f5f6d448acaa643945c42",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Rather than having these argument count checks buried deep inside, I think it's much clearer to define initdb's behavior entirely differently when arguments are specified.  Please see my comment on the PR for reorganizing.\n",
        "created_at": "2015-09-13T16:37:00Z",
        "updated_at": "2015-09-20T10:40:11Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/376#discussion_r39349264",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/376",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39349264"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/376#discussion_r39349264"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/376"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39349268",
        "pull_request_review_id": null,
        "id": 39349268,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ5MjY4",
        "diff_hunk": "@@ -0,0 +1,23 @@\n+#! /usr/bin/env python\n+# Generate create table statement given a ddlog exported schema and a table name.\n+# Usage: ddlog_initdb SCHEMA.JSON TABLE_NAME",
        "path": "util/ddlog_initdb",
        "position": null,
        "original_position": 3,
        "commit_id": "2bec6330f5d5d7c0412490fc7ca3c1df4c505a32",
        "original_commit_id": "70e8342c2bafcdf7f69f5f6d448acaa643945c42",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Let's give a better name to this script.  How about `schema_json_to_sql`?\n\nI was originally thinking this SQL generator should go under each driver, e.g.,  to handle `DISTRIBUTED BY` in GP and so on.  If you agree, I think it'll be a matter of just moving this to `shell/driver.postgresql/` and keeping a symlink or clone under `driver.mysql/`.\n",
        "created_at": "2015-09-13T16:37:08Z",
        "updated_at": "2015-09-20T10:40:11Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/376#discussion_r39349268",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/376",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39349268"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/376#discussion_r39349268"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/376"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39349270",
        "pull_request_review_id": null,
        "id": 39349270,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ5Mjcw",
        "diff_hunk": "@@ -0,0 +1,15 @@\n+#!/usr/bin/env bats\n+# Tests for initdb",
        "path": "test/postgresql/deepdive_initdb.bats",
        "position": 2,
        "original_position": 2,
        "commit_id": "2bec6330f5d5d7c0412490fc7ca3c1df4c505a32",
        "original_commit_id": "70e8342c2bafcdf7f69f5f6d448acaa643945c42",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Maybe it's more important to test the python script for generating CREATE TABLE statements?\n",
        "created_at": "2015-09-13T16:37:13Z",
        "updated_at": "2015-09-20T10:40:11Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/376#discussion_r39349270",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/376",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39349270"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/376#discussion_r39349270"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/376"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39936185",
        "pull_request_review_id": null,
        "id": 39936185,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTM2MTg1",
        "diff_hunk": "@@ -198,7 +203,7 @@ class PostgresDataStore extends JdbcDataStore with Logging {\n           WHERE (host, port) IN (SELECT host, min(port) FROM pgxl_dual group by host);\n       \"\"\")\n     }\n-    executeSqlQueries(SQLFunctions.piggyExtractorDriverDeclaration, false)\n+    //executeSqlQueries(SQLFunctions.piggyExtractorDriverDeclaration, false)",
        "path": "src/main/scala/org/deepdive/datastore/PostgresDataStore.scala",
        "position": null,
        "original_position": 21,
        "commit_id": "62a8de48f4ac4fc594087f71be3f202139c1e69b",
        "original_commit_id": "b0e94afcc60fcdf380d011223acc3adfff875b51",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Can you delete this line instead of commenting?\n",
        "created_at": "2015-09-20T23:13:05Z",
        "updated_at": "2015-09-20T23:15:09Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/397#discussion_r39936185",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/397",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39936185"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/397#discussion_r39936185"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/397"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39349280",
        "pull_request_review_id": null,
        "id": 39349280,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ5Mjgw",
        "diff_hunk": "@@ -18,4 +18,7 @@ port=${port#:}\n user=${user_password%:*}\n password=${user_password#$user}\n password=${password#:}\n-dbname=${rest}  # TODO do we need to strip any parameters from the rest of the URL?\n+dbname=${rest%\\?*}\n+rest1=${rest#$dbname?}\n+ssl=${rest1#ssl=} # TODO we only support ssl parameter from the rest of the URL",
        "path": "shell/parse-url.sh",
        "position": null,
        "original_position": 7,
        "commit_id": "f6145f22174bb78606f07292803f4f02e50ae356",
        "original_commit_id": "4d39e685af090af776fbca5ccb0fa15a9cbcb602",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "This is fine for now, but spending a bit more effort to handle general query string parsing may be a wise move.  Well, even without doing any smart parsing, I think it may be more robust to pass the entire query string down to the JDBC url (say, via `$querystring`) and set the `PGSSLMODE=require` from db-parse if it contains `*ssl=true*`.  How does that sound?\n",
        "created_at": "2015-09-13T16:37:42Z",
        "updated_at": "2015-09-22T18:10:04Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/380#discussion_r39349280",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/380",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39349280"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/380#discussion_r39349280"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/380"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39349283",
        "pull_request_review_id": null,
        "id": 39349283,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ5Mjgz",
        "diff_hunk": "@@ -21,7 +21,11 @@ echo \"export DBHOST DBPORT DBUSER DBPASSWORD DBNAME\"\n \n # more variables for Scala application.conf\n echo \"DEEPDIVE_JDBC_DRIVER=org.postgresql.Driver\"\n-echo \"DEEPDIVE_JDBC_URL=jdbc:postgresql://$host${port:+:$port}/$dbname\"\n+if [ ${ssl} = \"true\" ]; then\n+    echo \"DEEPDIVE_JDBC_URL=jdbc:postgresql://$host${port:+:$port}/$dbname?ssl=true\\&sslfactory=org.postgresql.ssl.NonValidatingFactory\"\n+else\n+    echo \"DEEPDIVE_JDBC_URL=jdbc:postgresql://$host${port:+:$port}/$dbname\"\n+fi",
        "path": "shell/driver.postgresql/db-parse",
        "position": null,
        "original_position": 9,
        "commit_id": "f6145f22174bb78606f07292803f4f02e50ae356",
        "original_commit_id": "4d39e685af090af776fbca5ccb0fa15a9cbcb602",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "If the entire query string were available as `$querystring`, no need for duplicating the long full URLs:\n\n``` bash\necho \"DEEPDIVE_JDBC_URL=jdbc:postgresql://$host${port:+:$port}/$dbname$querystring\"\n```\n\nI think the `&sslfactory=org.postgresql.ssl.NonValidatingFactory` option should be moved to documentation.  This is for getting self-signed certs to work, right?\n",
        "created_at": "2015-09-13T16:37:52Z",
        "updated_at": "2015-09-22T18:10:04Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/380#discussion_r39349283",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/380",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39349283"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/380#discussion_r39349283"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/380"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39349284",
        "pull_request_review_id": null,
        "id": 39349284,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ5Mjg0",
        "diff_hunk": "@@ -29,4 +33,9 @@ echo \"PGHOST=${host}\"\n echo \"PGPORT=${port}\"\n echo \"PGUSER=${user}\"\n echo \"PGPASSWORD=\\$DBPASSWORD\"\n-echo \"export PGHOST PGPORT PGUSER PGPASSWORD\"\n+if [ ${ssl} = \"true\"  ]; then\n+    echo \"PGSSLMODE=require\"\n+else\n+    echo \"PGSSLMODE=disable\"\n+fi",
        "path": "shell/driver.postgresql/db-parse",
        "position": null,
        "original_position": 22,
        "commit_id": "f6145f22174bb78606f07292803f4f02e50ae356",
        "original_commit_id": "4d39e685af090af776fbca5ccb0fa15a9cbcb602",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Using the `$querystring`, we could do something like:\n\n``` bash\ncase $querystring in\n    *ssl=true*) echo \"PGSSLMODE=require\" ;;\n    *) echo \"PGSSLMODE=disable\"\nesac\n```\n",
        "created_at": "2015-09-13T16:37:58Z",
        "updated_at": "2015-09-22T18:10:04Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/380#discussion_r39349284",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/380",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39349284"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/380#discussion_r39349284"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/380"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39349285",
        "pull_request_review_id": null,
        "id": 39349285,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ5Mjg1",
        "diff_hunk": "@@ -18,4 +18,7 @@ port=${port#:}\n user=${user_password%:*}\n password=${user_password#$user}\n password=${password#:}\n-dbname=${rest}  # TODO do we need to strip any parameters from the rest of the URL?\n+dbname=${rest%\\?*}\n+rest1=${rest#$dbname?}\n+ssl=${rest1#ssl=} # TODO we only support ssl parameter from the rest of the URL\n+#dbname=${rest}  # TODO do we need to strip any parameters from the rest of the URL?",
        "path": "shell/parse-url.sh",
        "position": null,
        "original_position": 8,
        "commit_id": "f6145f22174bb78606f07292803f4f02e50ae356",
        "original_commit_id": "4d39e685af090af776fbca5ccb0fa15a9cbcb602",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Please delete this last line instead of commenting out.\n",
        "created_at": "2015-09-13T16:38:03Z",
        "updated_at": "2015-09-22T18:10:04Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/380#discussion_r39349285",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/380",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39349285"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/380#discussion_r39349285"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/380"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39803619",
        "pull_request_review_id": null,
        "id": 39803619,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODAzNjE5",
        "diff_hunk": "@@ -21,7 +21,11 @@ echo \"export DBHOST DBPORT DBUSER DBPASSWORD DBNAME\"\n \n # more variables for Scala application.conf\n echo \"DEEPDIVE_JDBC_DRIVER=org.postgresql.Driver\"\n-echo \"DEEPDIVE_JDBC_URL=jdbc:postgresql://$host${port:+:$port}/$dbname\"\n+if [ ${ssl} = \"true\" ]; then\n+    echo \"DEEPDIVE_JDBC_URL=jdbc:postgresql://$host${port:+:$port}/$dbname?ssl=true\\&sslfactory=org.postgresql.ssl.NonValidatingFactory\"\n+else\n+    echo \"DEEPDIVE_JDBC_URL=jdbc:postgresql://$host${port:+:$port}/$dbname\"\n+fi",
        "path": "shell/driver.postgresql/db-parse",
        "position": null,
        "original_position": 9,
        "commit_id": "f6145f22174bb78606f07292803f4f02e50ae356",
        "original_commit_id": "4d39e685af090af776fbca5ccb0fa15a9cbcb602",
        "user": {
            "login": "SenWu",
            "id": 5580008,
            "node_id": "MDQ6VXNlcjU1ODAwMDg=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5580008?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/SenWu",
            "html_url": "https://github.com/SenWu",
            "followers_url": "https://api.github.com/users/SenWu/followers",
            "following_url": "https://api.github.com/users/SenWu/following{/other_user}",
            "gists_url": "https://api.github.com/users/SenWu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/SenWu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/SenWu/subscriptions",
            "organizations_url": "https://api.github.com/users/SenWu/orgs",
            "repos_url": "https://api.github.com/users/SenWu/repos",
            "events_url": "https://api.github.com/users/SenWu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/SenWu/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Sorry, I don't understand 'I think the &sslfactory=org.postgresql.ssl.NonValidatingFactory option should be moved to documentation.'. \nYes,  `&sslfactory=org.postgresql.ssl.NonValidatingFactory` is for getting self-signed cert, and we add it in to `DEEPDIVE_JDBC_URL` for that purpose.\n",
        "created_at": "2015-09-17T21:51:08Z",
        "updated_at": "2015-09-22T18:10:04Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/380#discussion_r39803619",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/380",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39803619"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/380#discussion_r39803619"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/380"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39814825",
        "pull_request_review_id": null,
        "id": 39814825,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODE0ODI1",
        "diff_hunk": "@@ -21,7 +21,11 @@ echo \"export DBHOST DBPORT DBUSER DBPASSWORD DBNAME\"\n \n # more variables for Scala application.conf\n echo \"DEEPDIVE_JDBC_DRIVER=org.postgresql.Driver\"\n-echo \"DEEPDIVE_JDBC_URL=jdbc:postgresql://$host${port:+:$port}/$dbname\"\n+if [ ${ssl} = \"true\" ]; then\n+    echo \"DEEPDIVE_JDBC_URL=jdbc:postgresql://$host${port:+:$port}/$dbname?ssl=true\\&sslfactory=org.postgresql.ssl.NonValidatingFactory\"\n+else\n+    echo \"DEEPDIVE_JDBC_URL=jdbc:postgresql://$host${port:+:$port}/$dbname\"\n+fi",
        "path": "shell/driver.postgresql/db-parse",
        "position": null,
        "original_position": 9,
        "commit_id": "f6145f22174bb78606f07292803f4f02e50ae356",
        "original_commit_id": "4d39e685af090af776fbca5ccb0fa15a9cbcb602",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I was saying user should decide whether to disable certificate validation, not DeepDive. If we always disable validation, it's not real ssl support. \n",
        "created_at": "2015-09-18T00:29:17Z",
        "updated_at": "2015-09-22T18:10:04Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/380#discussion_r39814825",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/380",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39814825"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/380#discussion_r39814825"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/380"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39815489",
        "pull_request_review_id": null,
        "id": 39815489,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODE1NDg5",
        "diff_hunk": "@@ -21,7 +21,11 @@ echo \"export DBHOST DBPORT DBUSER DBPASSWORD DBNAME\"\n \n # more variables for Scala application.conf\n echo \"DEEPDIVE_JDBC_DRIVER=org.postgresql.Driver\"\n-echo \"DEEPDIVE_JDBC_URL=jdbc:postgresql://$host${port:+:$port}/$dbname\"\n+if [ ${ssl} = \"true\" ]; then\n+    echo \"DEEPDIVE_JDBC_URL=jdbc:postgresql://$host${port:+:$port}/$dbname?ssl=true\\&sslfactory=org.postgresql.ssl.NonValidatingFactory\"\n+else\n+    echo \"DEEPDIVE_JDBC_URL=jdbc:postgresql://$host${port:+:$port}/$dbname\"\n+fi",
        "path": "shell/driver.postgresql/db-parse",
        "position": null,
        "original_position": 9,
        "commit_id": "f6145f22174bb78606f07292803f4f02e50ae356",
        "original_commit_id": "4d39e685af090af776fbca5ccb0fa15a9cbcb602",
        "user": {
            "login": "SenWu",
            "id": 5580008,
            "node_id": "MDQ6VXNlcjU1ODAwMDg=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5580008?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/SenWu",
            "html_url": "https://github.com/SenWu",
            "followers_url": "https://api.github.com/users/SenWu/followers",
            "following_url": "https://api.github.com/users/SenWu/following{/other_user}",
            "gists_url": "https://api.github.com/users/SenWu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/SenWu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/SenWu/subscriptions",
            "organizations_url": "https://api.github.com/users/SenWu/orgs",
            "repos_url": "https://api.github.com/users/SenWu/repos",
            "events_url": "https://api.github.com/users/SenWu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/SenWu/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "OK. Should I just keep `querystring` in `DEEPDIVE_JDBC_URL` , e.g., ``echo\"DEEPDIVE_JDBC_URL=jdbc:postgresql://$host${port:+:$port}/$dbname$querystring\"`?\n",
        "created_at": "2015-09-18T00:41:47Z",
        "updated_at": "2015-09-22T18:10:04Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/380#discussion_r39815489",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/380",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39815489"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/380#discussion_r39815489"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/380"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39815993",
        "pull_request_review_id": null,
        "id": 39815993,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODE1OTkz",
        "diff_hunk": "@@ -21,7 +21,11 @@ echo \"export DBHOST DBPORT DBUSER DBPASSWORD DBNAME\"\n \n # more variables for Scala application.conf\n echo \"DEEPDIVE_JDBC_DRIVER=org.postgresql.Driver\"\n-echo \"DEEPDIVE_JDBC_URL=jdbc:postgresql://$host${port:+:$port}/$dbname\"\n+if [ ${ssl} = \"true\" ]; then\n+    echo \"DEEPDIVE_JDBC_URL=jdbc:postgresql://$host${port:+:$port}/$dbname?ssl=true\\&sslfactory=org.postgresql.ssl.NonValidatingFactory\"\n+else\n+    echo \"DEEPDIVE_JDBC_URL=jdbc:postgresql://$host${port:+:$port}/$dbname\"\n+fi",
        "path": "shell/driver.postgresql/db-parse",
        "position": null,
        "original_position": 9,
        "commit_id": "f6145f22174bb78606f07292803f4f02e50ae356",
        "original_commit_id": "4d39e685af090af776fbca5ccb0fa15a9cbcb602",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Yup. I think that's better because user can supply extra options we are not aware of when necessary. \n",
        "created_at": "2015-09-18T00:52:46Z",
        "updated_at": "2015-09-22T18:10:04Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/380#discussion_r39815993",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/380",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39815993"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/380#discussion_r39815993"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/380"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39924183",
        "pull_request_review_id": null,
        "id": 39924183,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTI0MTgz",
        "diff_hunk": "@@ -21,7 +21,11 @@ echo \"export DBHOST DBPORT DBUSER DBPASSWORD DBNAME\"\n \n # more variables for Scala application.conf\n echo \"DEEPDIVE_JDBC_DRIVER=org.postgresql.Driver\"\n-echo \"DEEPDIVE_JDBC_URL=jdbc:postgresql://$host${port:+:$port}/$dbname\"\n+if [ ${ssl} = \"true\" ]; then\n+    echo \"DEEPDIVE_JDBC_URL=jdbc:postgresql://$host${port:+:$port}/$dbname?ssl=true\\&sslfactory=org.postgresql.ssl.NonValidatingFactory\"\n+else\n+    echo \"DEEPDIVE_JDBC_URL=jdbc:postgresql://$host${port:+:$port}/$dbname\"\n+fi",
        "path": "shell/driver.postgresql/db-parse",
        "position": null,
        "original_position": 9,
        "commit_id": "f6145f22174bb78606f07292803f4f02e50ae356",
        "original_commit_id": "4d39e685af090af776fbca5ccb0fa15a9cbcb602",
        "user": {
            "login": "SenWu",
            "id": 5580008,
            "node_id": "MDQ6VXNlcjU1ODAwMDg=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5580008?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/SenWu",
            "html_url": "https://github.com/SenWu",
            "followers_url": "https://api.github.com/users/SenWu/followers",
            "following_url": "https://api.github.com/users/SenWu/following{/other_user}",
            "gists_url": "https://api.github.com/users/SenWu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/SenWu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/SenWu/subscriptions",
            "organizations_url": "https://api.github.com/users/SenWu/orgs",
            "repos_url": "https://api.github.com/users/SenWu/repos",
            "events_url": "https://api.github.com/users/SenWu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/SenWu/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Sorry about that. Fixed.\n",
        "created_at": "2015-09-19T22:06:59Z",
        "updated_at": "2015-09-22T18:10:04Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/380#discussion_r39924183",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/380",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39924183"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/380#discussion_r39924183"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/380"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39935925",
        "pull_request_review_id": null,
        "id": 39935925,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTM1OTI1",
        "diff_hunk": "@@ -49,7 +49,7 @@ Java Virtual Machine. The default heap size is the minimum between one quarter o\n the physical memory and 1GB. If you use [SBT](http://www.scala-sbt.org/) to run\n a DeepDive application , you can set the heap size as follows:\n ```bash\n-    SBT_OPTS=\"-Xmx8g\" sbt \"run -c path_to_application.conf\"\n+    SBT_OPTS=\"-Xmx8g\" deepdive run -c path_to_deepdive.conf",
        "path": "doc/doc/advanced/performance.md",
        "position": null,
        "original_position": 23,
        "commit_id": "6c03643bb5c2b3020a002eb9262d4367f8f30230",
        "original_commit_id": "321154e9500a0b1415057e8b38a2b4de2198dea0",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "SBT is no longer used in `deepdive run`, so perhaps this section should be removed?\n",
        "created_at": "2015-09-20T22:48:26Z",
        "updated_at": "2015-09-28T07:58:31Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/396#discussion_r39935925",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/396",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39935925"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/396#discussion_r39935925"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/396"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39935949",
        "pull_request_review_id": null,
        "id": 39935949,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTM1OTQ5",
        "diff_hunk": "@@ -38,7 +38,7 @@ where gphost, gpport, gppath are the host, port, and absolute path\n gpfdist is running on (specified when starting gpfdist server).",
        "path": "doc/doc/advanced/performance.md",
        "position": null,
        "original_position": 10,
        "commit_id": "6c03643bb5c2b3020a002eb9262d4367f8f30230",
        "original_commit_id": "321154e9500a0b1415057e8b38a2b4de2198dea0",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Can you see if we can move this section to Using Greenplum?\n",
        "created_at": "2015-09-20T22:50:23Z",
        "updated_at": "2015-09-28T07:58:31Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/396#discussion_r39935949",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/396",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39935949"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/396#discussion_r39935949"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/396"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39935972",
        "pull_request_review_id": null,
        "id": 39935972,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTM1OTcy",
        "diff_hunk": "@@ -49,7 +49,7 @@ Java Virtual Machine. The default heap size is the minimum between one quarter o\n the physical memory and 1GB. If you use [SBT](http://www.scala-sbt.org/) to run\n a DeepDive application , you can set the heap size as follows:\n ```bash\n-    SBT_OPTS=\"-Xmx8g\" sbt \"run -c path_to_application.conf\"\n+    SBT_OPTS=\"-Xmx8g\" deepdive run -c path_to_deepdive.conf\n ```\n \n ### Setting extractor parallelism and batch sizes",
        "path": "doc/doc/advanced/performance.md",
        "position": null,
        "original_position": 26,
        "commit_id": "6c03643bb5c2b3020a002eb9262d4367f8f30230",
        "original_commit_id": "321154e9500a0b1415057e8b38a2b4de2198dea0",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I think this section is no longer relevant since data never goes through our Scala code.  Can we delete it?\n",
        "created_at": "2015-09-20T22:51:44Z",
        "updated_at": "2015-09-28T07:58:31Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/396#discussion_r39935972",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/396",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39935972"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/396#discussion_r39935972"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/396"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39936077",
        "pull_request_review_id": null,
        "id": 39936077,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTM2MDc3",
        "diff_hunk": "@@ -321,79 +319,7 @@ The following directives are only for the `json_extractor`, `tsv_extractor`, and\n   guide](extractors.html) for details about the requirements for the UDF for\n   different styles of extractors.\n \n-In the following subsections we present the directives that are specific to each\n-extractor style, if any.\n-\n-#### <a name=\"json\" href=\"#\"></a> json\\_extractor / tsv\\_extractor\n-\n-- `parallelism`: specifies the number of instances of this extractor to be\n-  executed in parallel. Tuples generated by the `input` query are sent (in\n-  batches of size controlled by the `input_batch_size` directive) to each\n-  instance in a round-robin fashion. This is an optional directive. Example\n-  usage:\n-\n-    ```bash\n-    myExtractor {\n-      # ...\n-      style: \"json_extractor\"\n-      # ...\n-      parallelism: 5\n-      # ...\n-    }\n-    ```\n-\n-- `input_batch_size`: specifies the size of the batch of tuples to feed to each\n-  instance of the extractor at each round-robin iteration. This is an optional directive. **The default value is 10000.**  Example usage:\n-\n-    ```bash\n-    myExtractor {\n-      # ...\n-      style: \"json_extractor\"\n-      # ...\n-      parallelism: 5\n-      # ...\n-      input_batch_size: 5000\n-      # ...\n-    }\n-    ```\n-\n-- `output_batch_size`: specifies how many tuples in the output of the extractor\n-  are inserted into the `output_relation` at once. If the tuples are large, a\n-  smaller value may avoid incurring in out of memory errors. This is an optional directive. **The default value is 50000.** Example usage:\n-\n-    ```bash\n-    myExtractor {\n-      # ...\n-      style: \"json_extractor\"\n-      # ...\n-      output_batch_size: 5000\n-      # ...\n-    }\n-    ```\n-\n-#### sql_extractor\n-\n-- `sql`: specifies the SQL command to execute. This option is mandatory for this\n-  extractor style. Example usage:\n-\n-    ```bash\n-    myExtractor {\n-      style: \"sql_extractor\"\n-      sql: \"\"\"INSERT INTO titles VALUES (1, 'Moby Dick')\"\"\"\n-    }\n-    ```\n-\n-#### cmd_extractor\n-\n-- `cmd`: specifies the shell command to execte. This option is mandatory for\n-  this extractor style. Example usage:\n-\n-    ```bash\n-    myExtractor {\n-      style: \"cmd_extractor\"\n-      cmd: \"\"\"python words.py\"\"\"\n-    }\n-    ```\n+For more information about different types of extractors can be found in ['Writing extractors' guide](extractors.html).",
        "path": "doc/doc/basics/configuration.md",
        "position": null,
        "original_position": 88,
        "commit_id": "6c03643bb5c2b3020a002eb9262d4367f8f30230",
        "original_commit_id": "321154e9500a0b1415057e8b38a2b4de2198dea0",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "This seems to be just repeating [L219](https://github.com/HazyResearch/deepdive/pull/396/files#diff-c2706fbfe92b4785b1ecee704a4952c8R219).  I think what needs to be mentioned/summarized here is that there are other directives (e.g., `parallelism`, `sql`, `cmd`) for different `style`.\n",
        "created_at": "2015-09-20T23:01:47Z",
        "updated_at": "2015-09-28T07:58:31Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/396#discussion_r39936077",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/396",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39936077"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/396#discussion_r39936077"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/396"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/40540204",
        "pull_request_review_id": null,
        "id": 40540204,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTQwMjA0",
        "diff_hunk": "@@ -30,3 +30,4 @@\n .mindtagger.conf.json\n /out\n /log\n+/lib",
        "path": ".gitignore",
        "position": null,
        "original_position": 4,
        "commit_id": "df76d5ba98f71e19ec99e98057f636ab60e03773",
        "original_commit_id": "97a38828720546fe4612c06fb6b51ed36ef545dd",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Why is this added back?\n",
        "created_at": "2015-09-28T11:03:30Z",
        "updated_at": "2015-09-28T11:34:12Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/418#discussion_r40540204",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/418",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/40540204"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/418#discussion_r40540204"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/418"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/40540293",
        "pull_request_review_id": null,
        "id": 40540293,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTQwMjkz",
        "diff_hunk": "@@ -12,9 +12,11 @@ fi\n echo \"Set DB_NAME to ${DBNAME}.\"\n echo \"HOST is ${PGHOST}, PORT is ${PGPORT}.\"\n \n-dropdb $DBNAME || true\n-createdb $DBNAME\n+# dropdb $DBNAME || true\n+# createdb $DBNAME\n \n-psql -d $DBNAME <./schema.sql\n+# psql -d $DBNAME <./schema.sql\n bzcat ../../data/articles_dump.csv.bz2  | psql -d $DBNAME -c \"COPY articles  FROM STDIN CSV\"\n-bzcat ../../data/sentences_dump.csv.bz2 | psql -d $DBNAME -c \"COPY sentences FROM STDIN CSV\"\n+bzcat ../../data/sentences_dump.csv.bz2 |\n+if [[ -z ${INCREMENTAL_SPOUSE_EXAMPLE_LIMIT_SENTENCES:-} ]]; then cat; else head -n ${INCREMENTAL_SPOUSE_EXAMPLE_LIMIT_SENTENCES}; fi |\n+psql -d $DBNAME -c \"COPY sentences FROM STDIN CSV\"",
        "path": "examples/spouse_example/postgres/incremental/setup_database.sh",
        "position": null,
        "original_position": 15,
        "commit_id": "df76d5ba98f71e19ec99e98057f636ab60e03773",
        "original_commit_id": "97a38828720546fe4612c06fb6b51ed36ef545dd",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Let's use `deepdive sql`.\n",
        "created_at": "2015-09-28T11:04:37Z",
        "updated_at": "2015-09-28T11:34:12Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/418#discussion_r40540293",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/418",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/40540293"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/418#discussion_r40540293"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/418"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/40540349",
        "pull_request_review_id": null,
        "id": 40540349,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTQwMzQ5",
        "diff_hunk": "@@ -1,3 +1,3 @@\n # List names of the inference rule to mark as active in materialization phase here.\n # It's in the form of the relation name of the head in the inference rule followed by an underscore and the zero-based index of the rule.\n-has_spouse_0  # You can put comments after a hash letter.\n+inference_rule_0  # You can put comments after a hash letter.",
        "path": "examples/spouse_example/postgres/incremental/spouse_example.f1.active.rules",
        "position": 4,
        "original_position": 4,
        "commit_id": "df76d5ba98f71e19ec99e98057f636ab60e03773",
        "original_commit_id": "97a38828720546fe4612c06fb6b51ed36ef545dd",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "This is another place where the compiler naming convention needs to be fixed as pointed out in #411.\n",
        "created_at": "2015-09-28T11:05:25Z",
        "updated_at": "2015-09-28T11:34:12Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/418#discussion_r40540349",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/418",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/40540349"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/418#discussion_r40540349"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/418"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/40540430",
        "pull_request_review_id": null,
        "id": 40540430,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTQwNDMw",
        "diff_hunk": "@@ -73,6 +73,7 @@ else\n     run_dir=\"$DEEPDIVE_APP/run/$run_id\"\n fi\n mkdir -p \"$run_dir\"\n+mkdir -p run",
        "path": "shell/deepdive-run",
        "position": null,
        "original_position": 4,
        "commit_id": "df76d5ba98f71e19ec99e98057f636ab60e03773",
        "original_commit_id": "97a38828720546fe4612c06fb6b51ed36ef545dd",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Is this necessary?  If so, let's add a comment.\n",
        "created_at": "2015-09-28T11:06:29Z",
        "updated_at": "2015-09-28T11:34:12Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/418#discussion_r40540430",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/418",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/40540430"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/418#discussion_r40540430"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/418"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/40540509",
        "pull_request_review_id": null,
        "id": 40540509,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTQwNTA5",
        "diff_hunk": "@@ -30,3 +30,4 @@\n .mindtagger.conf.json\n /out\n /log\n+/lib",
        "path": ".gitignore",
        "position": null,
        "original_position": 4,
        "commit_id": "df76d5ba98f71e19ec99e98057f636ab60e03773",
        "original_commit_id": "97a38828720546fe4612c06fb6b51ed36ef545dd",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I was thinking we don't want to track the library files. `library/` is also not included in the source tree, so we are currently not tracking it.\n",
        "created_at": "2015-09-28T11:07:58Z",
        "updated_at": "2015-09-28T11:34:12Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/418#discussion_r40540509",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/418",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/40540509"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/418#discussion_r40540509"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/418"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/40541300",
        "pull_request_review_id": null,
        "id": 40541300,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTQxMzAw",
        "diff_hunk": "@@ -30,3 +30,4 @@\n .mindtagger.conf.json\n /out\n /log\n+/lib",
        "path": ".gitignore",
        "position": null,
        "original_position": 4,
        "commit_id": "df76d5ba98f71e19ec99e98057f636ab60e03773",
        "original_commit_id": "97a38828720546fe4612c06fb6b51ed36ef545dd",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "But do any of our source or build produce `lib/` or `library/`?  I don't think we need to over ignore.\n",
        "created_at": "2015-09-28T11:19:35Z",
        "updated_at": "2015-09-28T11:34:12Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/418#discussion_r40541300",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/418",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/40541300"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/418#discussion_r40541300"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/418"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/40542026",
        "pull_request_review_id": null,
        "id": 40542026,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTQyMDI2",
        "diff_hunk": "@@ -30,3 +30,4 @@\n .mindtagger.conf.json\n /out\n /log\n+/lib",
        "path": ".gitignore",
        "position": null,
        "original_position": 4,
        "commit_id": "df76d5ba98f71e19ec99e98057f636ab60e03773",
        "original_commit_id": "97a38828720546fe4612c06fb6b51ed36ef545dd",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I see. It's probably old files on my laptop. I'll remove this. \n",
        "created_at": "2015-09-28T11:30:09Z",
        "updated_at": "2015-09-28T11:34:12Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/418#discussion_r40542026",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/418",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/40542026"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/418#discussion_r40542026"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/418"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/43072839",
        "pull_request_review_id": null,
        "id": 43072839,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcyODM5",
        "diff_hunk": "@@ -108,6 +108,9 @@ fullConfig=$run_dir/deepdive.conf\n         echo 1\n     ) 2>/dev/null)}\n     export PARALLELISM\n+    \n+    # set INPUT_BATCH_SIZE env var \n+    export INPUT_BATCH_SIZE=10000",
        "path": "shell/deepdive-run",
        "position": null,
        "original_position": 6,
        "commit_id": "19402cbb66d3b4353f3816dbd32509db6da289a5",
        "original_commit_id": "781d455807038a8a67641b83f633d165c788a285",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Always settings this prevents the user from overriding the value.  Please fix it to respect user's environment.\n",
        "created_at": "2015-10-27T00:36:21Z",
        "updated_at": "2015-10-27T16:51:26Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/435#discussion_r43072839",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/435",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/43072839"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/435#discussion_r43072839"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/435"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/43146987",
        "pull_request_review_id": null,
        "id": 43146987,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQ2OTg3",
        "diff_hunk": "@@ -108,6 +108,9 @@ fullConfig=$run_dir/deepdive.conf\n         echo 1\n     ) 2>/dev/null)}\n     export PARALLELISM\n+    \n+    # set INPUT_BATCH_SIZE env var \n+    ! [[ -e \"$INPUT_BATCH_SIZE\" ]] || export INPUT_BATCH_SIZE=10000",
        "path": "shell/deepdive-run",
        "position": null,
        "original_position": 6,
        "commit_id": "19402cbb66d3b4353f3816dbd32509db6da289a5",
        "original_commit_id": "6829267513df2213689cffdba25be21914cb9a64",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "First, when `INPUT_BATCH_SIZE` is not defined, the entire script aborts.  Second, this test with `-e` is checking whether there _exists_ a file named with that value which doesn't make sense.\n\nHere's the right way to do this:\n\n``` bash\n: ${INPUT_BATCH_SIZE:=10000}\nexport INPUT_BATCH_SIZE\n```\n",
        "created_at": "2015-10-27T16:29:24Z",
        "updated_at": "2015-10-27T16:51:26Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/435#discussion_r43146987",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/435",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/43146987"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/435#discussion_r43146987"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/435"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/43149587",
        "pull_request_review_id": null,
        "id": 43149587,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQ5NTg3",
        "diff_hunk": "@@ -108,6 +108,9 @@ fullConfig=$run_dir/deepdive.conf\n         echo 1\n     ) 2>/dev/null)}\n     export PARALLELISM\n+    \n+    # set INPUT_BATCH_SIZE env var \n+    ! [[ -e \"$INPUT_BATCH_SIZE\" ]] || export INPUT_BATCH_SIZE=10000",
        "path": "shell/deepdive-run",
        "position": null,
        "original_position": 6,
        "commit_id": "19402cbb66d3b4353f3816dbd32509db6da289a5",
        "original_commit_id": "6829267513df2213689cffdba25be21914cb9a64",
        "user": {
            "login": "xiaoling",
            "id": 474885,
            "node_id": "MDQ6VXNlcjQ3NDg4NQ==",
            "avatar_url": "https://avatars0.githubusercontent.com/u/474885?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/xiaoling",
            "html_url": "https://github.com/xiaoling",
            "followers_url": "https://api.github.com/users/xiaoling/followers",
            "following_url": "https://api.github.com/users/xiaoling/following{/other_user}",
            "gists_url": "https://api.github.com/users/xiaoling/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/xiaoling/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/xiaoling/subscriptions",
            "organizations_url": "https://api.github.com/users/xiaoling/orgs",
            "repos_url": "https://api.github.com/users/xiaoling/repos",
            "events_url": "https://api.github.com/users/xiaoling/events{/privacy}",
            "received_events_url": "https://api.github.com/users/xiaoling/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "You're right. Sorry my bash is a bit rusty. I should've been more careful.\n",
        "created_at": "2015-10-27T16:46:46Z",
        "updated_at": "2015-10-27T16:51:26Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/435#discussion_r43149587",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/435",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/43149587"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/435#discussion_r43149587"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/435"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/43149838",
        "pull_request_review_id": null,
        "id": 43149838,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQ5ODM4",
        "diff_hunk": "@@ -108,6 +108,9 @@ fullConfig=$run_dir/deepdive.conf\n         echo 1\n     ) 2>/dev/null)}\n     export PARALLELISM\n+    \n+    # set INPUT_BATCH_SIZE env var \n+    ! [[ -e \"$INPUT_BATCH_SIZE\" ]] || export INPUT_BATCH_SIZE=10000",
        "path": "shell/deepdive-run",
        "position": null,
        "original_position": 6,
        "commit_id": "19402cbb66d3b4353f3816dbd32509db6da289a5",
        "original_commit_id": "6829267513df2213689cffdba25be21914cb9a64",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "No worries.  Can you also update the ddlog submodule to point to the commit relevant to this?\n",
        "created_at": "2015-10-27T16:48:38Z",
        "updated_at": "2015-10-27T16:51:26Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/435#discussion_r43149838",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/435",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/43149838"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/435#discussion_r43149838"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/435"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39349298",
        "pull_request_review_id": null,
        "id": 39349298,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ5Mjk4",
        "diff_hunk": "@@ -449,12 +449,20 @@ class ExtractorRunner(dataStore: JdbcDataStore, dbSettings: DbSettings) extends\n     log.info(s\"File dumped to ${actualDumpedPath}\")\n     val splitPrefix = s\"${actualDumpedPath}-\"\n     val linesPerSplit = task.extractor.inputBatchSize\n+    val maxParallel = task.extractor.parallelism\n+    val splitNum = maxParallel * 10",
        "path": "src/main/scala/org/deepdive/extraction/ExtractorRunner.scala",
        "position": null,
        "original_position": 5,
        "commit_id": "894285f50441cdd26691cb891135ae086e9ab1b6",
        "original_commit_id": "147adce3aa43a56df0bca375d71f743d31b8b681",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Why do we need `* 10` here?  If I understood correctly, your fix is trying to avoid the startup overhead of UDFs, right?  Then why do we still pay 9 more times if we can just start once and handle the chunk?\n",
        "created_at": "2015-09-13T16:38:41Z",
        "updated_at": "2015-10-31T17:07:29Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/370#discussion_r39349298",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/370",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/39349298"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/370#discussion_r39349298"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/370"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/43069054",
        "pull_request_review_id": null,
        "id": 43069054,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY5MDU0",
        "diff_hunk": "@@ -449,12 +449,20 @@ class ExtractorRunner(dataStore: JdbcDataStore, dbSettings: DbSettings) extends\n     log.info(s\"File dumped to ${actualDumpedPath}\")\n     val splitPrefix = s\"${actualDumpedPath}-\"\n     val linesPerSplit = task.extractor.inputBatchSize\n+    val maxParallel = task.extractor.parallelism\n+    val splitNum = maxParallel * 10",
        "path": "src/main/scala/org/deepdive/extraction/ExtractorRunner.scala",
        "position": null,
        "original_position": 5,
        "commit_id": "894285f50441cdd26691cb891135ae086e9ab1b6",
        "original_commit_id": "147adce3aa43a56df0bca375d71f743d31b8b681",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "@netj Sorry for the late response. Here I was trying to reduce the effect of the data skew:  sometimes one thread might be stuck in a chunk of data and all other threads are waiting for the slowest one to finish. With smaller splits the data skew should have less impact.  So the idea here is to find a balance between UDF start overhead and the data skew.\n\nI also believed that `xargs` has some load balancing to distribute the input chunks but I am not sure there.\n",
        "created_at": "2015-10-26T23:44:13Z",
        "updated_at": "2015-10-31T17:07:29Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/370#discussion_r43069054",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/370",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/43069054"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/370#discussion_r43069054"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/370"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/45150616",
        "pull_request_review_id": null,
        "id": 45150616,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTUwNjE2",
        "diff_hunk": "@@ -79,7 +79,6 @@ class Sampler extends Actor with ActorLogging {\n           \"-w\", weightsFile,\n           \"-v\", variablesFile,\n           \"-f\", factorsFile,\n-          \"-e\", edgesFile,",
        "path": "src/main/scala/org/deepdive/inference/Sampler.scala",
        "position": 4,
        "original_position": 4,
        "commit_id": "d102eaae2dbfbb9e97d62e510512dc5e4f6f14f8",
        "original_commit_id": "d102eaae2dbfbb9e97d62e510512dc5e4f6f14f8",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "We should probably update the sampler documentation as well.  Could you open an issue for that so we don't forget?\n",
        "created_at": "2015-11-18T01:55:31Z",
        "updated_at": "2015-11-18T01:55:31Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/437#discussion_r45150616",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/437",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/45150616"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/437#discussion_r45150616"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/437"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/45150932",
        "pull_request_review_id": null,
        "id": 45150932,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTUwOTMy",
        "diff_hunk": "@@ -79,7 +79,6 @@ class Sampler extends Actor with ActorLogging {\n           \"-w\", weightsFile,\n           \"-v\", variablesFile,\n           \"-f\", factorsFile,\n-          \"-e\", edgesFile,",
        "path": "src/main/scala/org/deepdive/inference/Sampler.scala",
        "position": 4,
        "original_position": 4,
        "commit_id": "d102eaae2dbfbb9e97d62e510512dc5e4f6f14f8",
        "original_commit_id": "d102eaae2dbfbb9e97d62e510512dc5e4f6f14f8",
        "user": {
            "login": "SenWu",
            "id": 5580008,
            "node_id": "MDQ6VXNlcjU1ODAwMDg=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5580008?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/SenWu",
            "html_url": "https://github.com/SenWu",
            "followers_url": "https://api.github.com/users/SenWu/followers",
            "following_url": "https://api.github.com/users/SenWu/following{/other_user}",
            "gists_url": "https://api.github.com/users/SenWu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/SenWu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/SenWu/subscriptions",
            "organizations_url": "https://api.github.com/users/SenWu/orgs",
            "repos_url": "https://api.github.com/users/SenWu/repos",
            "events_url": "https://api.github.com/users/SenWu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/SenWu/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Sure, let me do it.\n",
        "created_at": "2015-11-18T02:00:20Z",
        "updated_at": "2015-11-18T02:00:20Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/437#discussion_r45150932",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/437",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/45150932"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/437#discussion_r45150932"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/437"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/45264398",
        "pull_request_review_id": null,
        "id": 45264398,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjY0Mzk4",
        "diff_hunk": "@@ -73,10 +73,10 @@ stage ddlib/ddlib                                                 lib/python/\n # DimmWitted sampler\n case $(uname) in\n Linux)\n+    # copy sampler libraries\n+    ldd util/sampler-dw-linux | grep '=>' | awk '{print $3}' | sort -u | grep -v '^(' | xargs cp -t \"$STAGE_DIR\"/lib/",
        "path": "stage.sh",
        "position": 5,
        "original_position": 5,
        "commit_id": "6e9e36a00bc5230e84e7ca201a36387cd119dfb7",
        "original_commit_id": "bb691af7421982cdfef59cb1119051d2f5295f97",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I think you should also modify the sampler-dw-linux.sh script to add only the lib directory to LD_LIBRARY_PATH.  I remember we were adding a bunch of subdirs.\n",
        "created_at": "2015-11-18T21:39:33Z",
        "updated_at": "2015-11-20T02:12:44Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/440#discussion_r45264398",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/440",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/45264398"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/440#discussion_r45264398"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/440"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/48086017",
        "pull_request_review_id": null,
        "id": 48086017,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg2MDE3",
        "diff_hunk": "@@ -0,0 +1,136 @@\n+#!/usr/bin/env bash\n+# local/compute-execute -- Executes a process locally using all available processors\n+# $ compute-execute input_sql=... command=... output_relation=...\n+#\n+# To limit the number of parallel processes, set the DEEPDIVE_NUM_PROCESSES\n+# environment or the 'deepdive.computers.local.num_processes' in\n+# computers.conf:\n+# $ export DEEPDIVE_NUM_PROCESSES=2\n+# $ compute-execute input_sql=... command=... output_relation=...\n+##\n+set -euo pipefail\n+\n+: ${DEEPDIVE_PREFIX_TABLE_TEMPORARY:=dd_tmp_} ${DEEPDIVE_PREFIX_TABLE_OLD:=dd_old_}\n+\n+# load compute configuration\n+eval \"$(jq2sh <<<\"$DEEPDIVE_COMPUTER_CONFIG\" \\\n+    num_processes='.num_processes' \\\n+    #\n+)\"\n+# respect the DEEPDIVE_NUM_PROCESSES environment\n+num_processes=${DEEPDIVE_NUM_PROCESSES:-${num_processes:-$(\n+        # detect number of processor cores\n+        nproc=$(\n+            # Linux typically has coreutils which includes nproc\n+            nproc ||\n+            # OS X\n+            sysctl -n hw.ncpu ||\n+            # fall back to 1\n+            echo 1\n+        )\n+        if [[ $nproc -gt 1 ]]; then\n+            # leave one processor out\n+            let nproc-=1\n+        elif [[ $nproc -lt 1 ]]; then\n+            nproc=1\n+        fi\n+        echo $nproc\n+    )}}\n+\n+# declare all input arguments\n+declare -- \"$@\"\n+\n+# show configuration\n+echo \"Executing with the following configuration:\"\n+echo \" num_processes=$num_processes\"\n+\n+# XXX there are conditional branches below depending on whether input_sql\n+# and/or output_relation is given, to support four use cases:\n+# 1) executing command while streaming data from/to the database\n+# 2) input-only command which has no output to the database and streams from the database\n+# 3) output-only command which has no input from the database and streams to the database\n+# 4) database-independent command which simply runs in parallel\n+\n+# prepare a temporary output table when output_relation is given\n+if [[ -n $output_relation ]]; then\n+    # some derived values\n+    output_relation_tmp=\"${DEEPDIVE_PREFIX_TABLE_TEMPORARY}${output_relation}\"\n+\n+    # show configuration\n+    echo \" output_relation_tmp=$output_relation_tmp\"\n+    echo\n+\n+    # use an empty temporary table as a sink instead of TRUNCATE'ing the output_relation\n+    deepdive-initdb \"$output_relation\"\n+    deepdive-sql \"\n+        DROP TABLE IF EXISTS $output_relation_tmp;\n+        CREATE TABLE $output_relation_tmp (LIKE $output_relation);",
        "path": "runner/compute-driver/local/compute-execute",
        "position": null,
        "original_position": 67,
        "commit_id": "1815fa8bd346746b171a3cae622c27cb3a162a31",
        "original_commit_id": "140da9c38e1fc380eb50a14f1e3d86992d956dc8",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Maybe add `INCLUDING ALL` to make sure all other bells and whistles are retained?\n",
        "created_at": "2015-12-19T01:18:51Z",
        "updated_at": "2016-01-01T19:42:03Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/445#discussion_r48086017",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/445",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/48086017"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/445#discussion_r48086017"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/445"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/48086227",
        "pull_request_review_id": null,
        "id": 48086227,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDg2MjI3",
        "diff_hunk": "@@ -0,0 +1,136 @@\n+#!/usr/bin/env bash\n+# local/compute-execute -- Executes a process locally using all available processors\n+# $ compute-execute input_sql=... command=... output_relation=...\n+#\n+# To limit the number of parallel processes, set the DEEPDIVE_NUM_PROCESSES\n+# environment or the 'deepdive.computers.local.num_processes' in\n+# computers.conf:\n+# $ export DEEPDIVE_NUM_PROCESSES=2\n+# $ compute-execute input_sql=... command=... output_relation=...\n+##\n+set -euo pipefail\n+\n+: ${DEEPDIVE_PREFIX_TABLE_TEMPORARY:=dd_tmp_} ${DEEPDIVE_PREFIX_TABLE_OLD:=dd_old_}\n+\n+# load compute configuration\n+eval \"$(jq2sh <<<\"$DEEPDIVE_COMPUTER_CONFIG\" \\\n+    num_processes='.num_processes' \\\n+    #\n+)\"\n+# respect the DEEPDIVE_NUM_PROCESSES environment\n+num_processes=${DEEPDIVE_NUM_PROCESSES:-${num_processes:-$(\n+        # detect number of processor cores\n+        nproc=$(\n+            # Linux typically has coreutils which includes nproc\n+            nproc ||\n+            # OS X\n+            sysctl -n hw.ncpu ||\n+            # fall back to 1\n+            echo 1\n+        )\n+        if [[ $nproc -gt 1 ]]; then\n+            # leave one processor out\n+            let nproc-=1\n+        elif [[ $nproc -lt 1 ]]; then\n+            nproc=1\n+        fi\n+        echo $nproc\n+    )}}\n+\n+# declare all input arguments\n+declare -- \"$@\"\n+\n+# show configuration\n+echo \"Executing with the following configuration:\"\n+echo \" num_processes=$num_processes\"\n+\n+# XXX there are conditional branches below depending on whether input_sql\n+# and/or output_relation is given, to support four use cases:\n+# 1) executing command while streaming data from/to the database\n+# 2) input-only command which has no output to the database and streams from the database\n+# 3) output-only command which has no input from the database and streams to the database\n+# 4) database-independent command which simply runs in parallel\n+\n+# prepare a temporary output table when output_relation is given\n+if [[ -n $output_relation ]]; then\n+    # some derived values\n+    output_relation_tmp=\"${DEEPDIVE_PREFIX_TABLE_TEMPORARY}${output_relation}\"\n+\n+    # show configuration\n+    echo \" output_relation_tmp=$output_relation_tmp\"\n+    echo\n+\n+    # use an empty temporary table as a sink instead of TRUNCATE'ing the output_relation\n+    deepdive-initdb \"$output_relation\"\n+    deepdive-sql \"\n+        DROP TABLE IF EXISTS $output_relation_tmp;\n+        CREATE TABLE $output_relation_tmp (LIKE $output_relation);\n+    \"\n+fi\n+\n+# set up named pipes for parallel processes and make sure they are cleaned up upon exit\n+[[ -z $input_sql       ]] || for i in $(seq $num_processes); do rm -f process-$i.input ; mkfifo process-$i.input ; done\n+[[ -z $output_relation ]] || for i in $(seq $num_processes); do rm -f process-$i.output; mkfifo process-$i.output; done\n+trap 'rm -f process-*.{input,output}' EXIT\n+# now spawn processes attached to the named pipes in reverse order (from sink to source)\n+\n+if [[ -n $output_relation ]]; then\n+    # use mkmimo again to merge outputs of multiple processes into a single stream\n+    mkmimo process-*.output \\> /dev/stdout |\n+    # load the output data to the temporary table in the database\n+    # XXX hiding default progress bar from deepdive-load\n+    # TODO abbreviate this env into a show_progress option, e.g., recursive=false\n+    show_progress input_to \"$DEEPDIVE_CURRENT_PROCESS_NAME output\" -- \\\n+    env DEEPDIVE_PROGRESS_FD=2 \\\n+    deepdive-load \"$output_relation_tmp\" /dev/stdin &\n+fi\n+\n+# spawn multiple processes attached to the pipes\n+if [[ -n $output_relation && -n $input_sql ]]; then # process with input from/output to database\n+    for i in $(seq $num_processes); do\n+        DEEPDIVE_CURRENT_PROCESS_INDEX=$i \\\n+        \"$SHELL\" -c \"$command\" <process-$i.input >process-$i.output &\n+    done\n+elif [[ -n $input_sql ]]; then # input-only process\n+    for i in $(seq $num_processes); do\n+        DEEPDIVE_CURRENT_PROCESS_INDEX=$i \\\n+        \"$SHELL\" -c \"$command\" <process-$i.input &\n+    done\n+elif [[ -n $output_relation ]]; then # output-only process\n+    for i in $(seq $num_processes); do\n+        DEEPDIVE_CURRENT_PROCESS_INDEX=$i \\\n+        \"$SHELL\" -c \"$command\" >process-$i.output &\n+    done\n+else # neither output_relation nor input_sql specified\n+    for i in $(seq $num_processes); do\n+        DEEPDIVE_CURRENT_PROCESS_INDEX=$i \\\n+        \"$SHELL\" -c \"$command\" &\n+    done\n+fi\n+\n+if [[ -n $input_sql ]]; then\n+    # unload data from the database and pour into the pipes\n+    show_progress output_from \"$DEEPDIVE_CURRENT_PROCESS_NAME input\" -- \\\n+    deepdive-sql eval \"$input_sql\" format=\"$DEEPDIVE_LOAD_FORMAT\" |\n+    # use mkmimo to distribute input data to multiple processes\n+    mkmimo /dev/stdin \\> process-*.input &\n+fi\n+\n+wait  # until everything is done ##############################################\n+\n+if [[ -n $output_relation ]]; then\n+    # rename the new temporary table\n+    # TODO maybe use PostgreSQL's schema support here?\n+    echo \"Replacing $output_relation with $output_relation_tmp\"",
        "path": "runner/compute-driver/local/compute-execute",
        "position": null,
        "original_position": 124,
        "commit_id": "1815fa8bd346746b171a3cae622c27cb3a162a31",
        "original_commit_id": "140da9c38e1fc380eb50a14f1e3d86992d956dc8",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Is inserting into (as opposed to replacing) the target table something we need?\n",
        "created_at": "2015-12-19T01:26:43Z",
        "updated_at": "2016-01-01T19:42:03Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/445#discussion_r48086227",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/445",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/48086227"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/445#discussion_r48086227"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/445"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/48148988",
        "pull_request_review_id": null,
        "id": 48148988,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ4OTg4",
        "diff_hunk": "@@ -0,0 +1,136 @@\n+#!/usr/bin/env bash\n+# local/compute-execute -- Executes a process locally using all available processors\n+# $ compute-execute input_sql=... command=... output_relation=...\n+#\n+# To limit the number of parallel processes, set the DEEPDIVE_NUM_PROCESSES\n+# environment or the 'deepdive.computers.local.num_processes' in\n+# computers.conf:\n+# $ export DEEPDIVE_NUM_PROCESSES=2\n+# $ compute-execute input_sql=... command=... output_relation=...\n+##\n+set -euo pipefail\n+\n+: ${DEEPDIVE_PREFIX_TABLE_TEMPORARY:=dd_tmp_} ${DEEPDIVE_PREFIX_TABLE_OLD:=dd_old_}\n+\n+# load compute configuration\n+eval \"$(jq2sh <<<\"$DEEPDIVE_COMPUTER_CONFIG\" \\\n+    num_processes='.num_processes' \\\n+    #\n+)\"\n+# respect the DEEPDIVE_NUM_PROCESSES environment\n+num_processes=${DEEPDIVE_NUM_PROCESSES:-${num_processes:-$(\n+        # detect number of processor cores\n+        nproc=$(\n+            # Linux typically has coreutils which includes nproc\n+            nproc ||\n+            # OS X\n+            sysctl -n hw.ncpu ||\n+            # fall back to 1\n+            echo 1\n+        )\n+        if [[ $nproc -gt 1 ]]; then\n+            # leave one processor out\n+            let nproc-=1\n+        elif [[ $nproc -lt 1 ]]; then\n+            nproc=1\n+        fi\n+        echo $nproc\n+    )}}\n+\n+# declare all input arguments\n+declare -- \"$@\"\n+\n+# show configuration\n+echo \"Executing with the following configuration:\"\n+echo \" num_processes=$num_processes\"\n+\n+# XXX there are conditional branches below depending on whether input_sql\n+# and/or output_relation is given, to support four use cases:\n+# 1) executing command while streaming data from/to the database\n+# 2) input-only command which has no output to the database and streams from the database\n+# 3) output-only command which has no input from the database and streams to the database\n+# 4) database-independent command which simply runs in parallel\n+\n+# prepare a temporary output table when output_relation is given\n+if [[ -n $output_relation ]]; then\n+    # some derived values\n+    output_relation_tmp=\"${DEEPDIVE_PREFIX_TABLE_TEMPORARY}${output_relation}\"\n+\n+    # show configuration\n+    echo \" output_relation_tmp=$output_relation_tmp\"\n+    echo\n+\n+    # use an empty temporary table as a sink instead of TRUNCATE'ing the output_relation\n+    deepdive-initdb \"$output_relation\"\n+    deepdive-sql \"\n+        DROP TABLE IF EXISTS $output_relation_tmp;\n+        CREATE TABLE $output_relation_tmp (LIKE $output_relation);",
        "path": "runner/compute-driver/local/compute-execute",
        "position": null,
        "original_position": 67,
        "commit_id": "1815fa8bd346746b171a3cae622c27cb3a162a31",
        "original_commit_id": "140da9c38e1fc380eb50a14f1e3d86992d956dc8",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Agreed and done.  Thanks for suggesting this.\n",
        "created_at": "2015-12-21T14:18:16Z",
        "updated_at": "2016-01-01T19:42:03Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/445#discussion_r48148988",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/445",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/48148988"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/445#discussion_r48148988"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/445"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/48150515",
        "pull_request_review_id": null,
        "id": 48150515,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTUwNTE1",
        "diff_hunk": "@@ -0,0 +1,136 @@\n+#!/usr/bin/env bash\n+# local/compute-execute -- Executes a process locally using all available processors\n+# $ compute-execute input_sql=... command=... output_relation=...\n+#\n+# To limit the number of parallel processes, set the DEEPDIVE_NUM_PROCESSES\n+# environment or the 'deepdive.computers.local.num_processes' in\n+# computers.conf:\n+# $ export DEEPDIVE_NUM_PROCESSES=2\n+# $ compute-execute input_sql=... command=... output_relation=...\n+##\n+set -euo pipefail\n+\n+: ${DEEPDIVE_PREFIX_TABLE_TEMPORARY:=dd_tmp_} ${DEEPDIVE_PREFIX_TABLE_OLD:=dd_old_}\n+\n+# load compute configuration\n+eval \"$(jq2sh <<<\"$DEEPDIVE_COMPUTER_CONFIG\" \\\n+    num_processes='.num_processes' \\\n+    #\n+)\"\n+# respect the DEEPDIVE_NUM_PROCESSES environment\n+num_processes=${DEEPDIVE_NUM_PROCESSES:-${num_processes:-$(\n+        # detect number of processor cores\n+        nproc=$(\n+            # Linux typically has coreutils which includes nproc\n+            nproc ||\n+            # OS X\n+            sysctl -n hw.ncpu ||\n+            # fall back to 1\n+            echo 1\n+        )\n+        if [[ $nproc -gt 1 ]]; then\n+            # leave one processor out\n+            let nproc-=1\n+        elif [[ $nproc -lt 1 ]]; then\n+            nproc=1\n+        fi\n+        echo $nproc\n+    )}}\n+\n+# declare all input arguments\n+declare -- \"$@\"\n+\n+# show configuration\n+echo \"Executing with the following configuration:\"\n+echo \" num_processes=$num_processes\"\n+\n+# XXX there are conditional branches below depending on whether input_sql\n+# and/or output_relation is given, to support four use cases:\n+# 1) executing command while streaming data from/to the database\n+# 2) input-only command which has no output to the database and streams from the database\n+# 3) output-only command which has no input from the database and streams to the database\n+# 4) database-independent command which simply runs in parallel\n+\n+# prepare a temporary output table when output_relation is given\n+if [[ -n $output_relation ]]; then\n+    # some derived values\n+    output_relation_tmp=\"${DEEPDIVE_PREFIX_TABLE_TEMPORARY}${output_relation}\"\n+\n+    # show configuration\n+    echo \" output_relation_tmp=$output_relation_tmp\"\n+    echo\n+\n+    # use an empty temporary table as a sink instead of TRUNCATE'ing the output_relation\n+    deepdive-initdb \"$output_relation\"\n+    deepdive-sql \"\n+        DROP TABLE IF EXISTS $output_relation_tmp;\n+        CREATE TABLE $output_relation_tmp (LIKE $output_relation);\n+    \"\n+fi\n+\n+# set up named pipes for parallel processes and make sure they are cleaned up upon exit\n+[[ -z $input_sql       ]] || for i in $(seq $num_processes); do rm -f process-$i.input ; mkfifo process-$i.input ; done\n+[[ -z $output_relation ]] || for i in $(seq $num_processes); do rm -f process-$i.output; mkfifo process-$i.output; done\n+trap 'rm -f process-*.{input,output}' EXIT\n+# now spawn processes attached to the named pipes in reverse order (from sink to source)\n+\n+if [[ -n $output_relation ]]; then\n+    # use mkmimo again to merge outputs of multiple processes into a single stream\n+    mkmimo process-*.output \\> /dev/stdout |\n+    # load the output data to the temporary table in the database\n+    # XXX hiding default progress bar from deepdive-load\n+    # TODO abbreviate this env into a show_progress option, e.g., recursive=false\n+    show_progress input_to \"$DEEPDIVE_CURRENT_PROCESS_NAME output\" -- \\\n+    env DEEPDIVE_PROGRESS_FD=2 \\\n+    deepdive-load \"$output_relation_tmp\" /dev/stdin &\n+fi\n+\n+# spawn multiple processes attached to the pipes\n+if [[ -n $output_relation && -n $input_sql ]]; then # process with input from/output to database\n+    for i in $(seq $num_processes); do\n+        DEEPDIVE_CURRENT_PROCESS_INDEX=$i \\\n+        \"$SHELL\" -c \"$command\" <process-$i.input >process-$i.output &\n+    done\n+elif [[ -n $input_sql ]]; then # input-only process\n+    for i in $(seq $num_processes); do\n+        DEEPDIVE_CURRENT_PROCESS_INDEX=$i \\\n+        \"$SHELL\" -c \"$command\" <process-$i.input &\n+    done\n+elif [[ -n $output_relation ]]; then # output-only process\n+    for i in $(seq $num_processes); do\n+        DEEPDIVE_CURRENT_PROCESS_INDEX=$i \\\n+        \"$SHELL\" -c \"$command\" >process-$i.output &\n+    done\n+else # neither output_relation nor input_sql specified\n+    for i in $(seq $num_processes); do\n+        DEEPDIVE_CURRENT_PROCESS_INDEX=$i \\\n+        \"$SHELL\" -c \"$command\" &\n+    done\n+fi\n+\n+if [[ -n $input_sql ]]; then\n+    # unload data from the database and pour into the pipes\n+    show_progress output_from \"$DEEPDIVE_CURRENT_PROCESS_NAME input\" -- \\\n+    deepdive-sql eval \"$input_sql\" format=\"$DEEPDIVE_LOAD_FORMAT\" |\n+    # use mkmimo to distribute input data to multiple processes\n+    mkmimo /dev/stdin \\> process-*.input &\n+fi\n+\n+wait  # until everything is done ##############################################\n+\n+if [[ -n $output_relation ]]; then\n+    # rename the new temporary table\n+    # TODO maybe use PostgreSQL's schema support here?\n+    echo \"Replacing $output_relation with $output_relation_tmp\"",
        "path": "runner/compute-driver/local/compute-execute",
        "position": null,
        "original_position": 124,
        "commit_id": "1815fa8bd346746b171a3cae622c27cb3a162a31",
        "original_commit_id": "140da9c38e1fc380eb50a14f1e3d86992d956dc8",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I'd say no for running UDFs, as it makes part of the data flow non-idempotent, hence unwieldy for the user as well as the system.  Whenever a user needs to append the output of some UDF to somewhere or to take a UNION of multiple UDFs, sql_extractor or cmd_extractor is there.  DDlog compiler may need to be adjusted to automatically compile a UNION VIEW when multiple UDFs define the same head.  Related to this, I added a check that ensures all output_relation is defined by a single extractor, as it'll surprise all users when multiple extractors are defined to output to the same relation but only one extractor's output remains.\n",
        "created_at": "2015-12-21T14:33:17Z",
        "updated_at": "2016-01-01T19:42:03Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/445#discussion_r48150515",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/445",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/48150515"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/445#discussion_r48150515"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/445"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/48516109",
        "pull_request_review_id": null,
        "id": 48516109,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTE2MTA5",
        "diff_hunk": "@@ -0,0 +1,9 @@\n+#!/usr/bin/env jq-f\n+# compile-code-plpy_extractor -- TODO Compiles plpy_extractors in the normalized JSON into code executable by the runner\n+##\n+.deepdive_ as $deepdive\n+\n+| $deepdive.execution.processes | to_entries[]\n+| .value.name = .key | .value\n+| select(.style == \"plpy_extractor\")",
        "path": "compiler/compile-code/compile-code-piggy_extractor",
        "position": null,
        "original_position": 8,
        "commit_id": "1815fa8bd346746b171a3cae622c27cb3a162a31",
        "original_commit_id": "328a620b4606b77d1b5feba20d038cfd68e83988",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "It seems that the content in `compiler/compile-code/compile-code-piggy_extractor` and `compiler/compile-code/compile-code-plpy_extractor` should switch?\n",
        "created_at": "2015-12-29T01:00:29Z",
        "updated_at": "2016-01-01T19:42:03Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/445#discussion_r48516109",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/445",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/48516109"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/445#discussion_r48516109"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/445"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/48516210",
        "pull_request_review_id": null,
        "id": 48516210,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTE2MjEw",
        "diff_hunk": "@@ -0,0 +1,9 @@\n+#!/usr/bin/env jq-f\n+# compile-code-plpy_extractor -- TODO Compiles plpy_extractors in the normalized JSON into code executable by the runner\n+##\n+.deepdive_ as $deepdive\n+\n+| $deepdive.execution.processes | to_entries[]\n+| .value.name = .key | .value\n+| select(.style == \"plpy_extractor\")",
        "path": "compiler/compile-code/compile-code-piggy_extractor",
        "position": null,
        "original_position": 8,
        "commit_id": "1815fa8bd346746b171a3cae622c27cb3a162a31",
        "original_commit_id": "328a620b4606b77d1b5feba20d038cfd68e83988",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Good catch!  I'm actually planning to drop these in a later commit: f5186538f47e5a1f5020ce2420f3ec49b30a5b11 so maybe it's not too critical.\n",
        "created_at": "2015-12-29T01:03:40Z",
        "updated_at": "2016-01-01T19:42:03Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/445#discussion_r48516210",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/445",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/48516210"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/445#discussion_r48516210"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/445"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/48516243",
        "pull_request_review_id": null,
        "id": 48516243,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTE2MjQz",
        "diff_hunk": "@@ -130,14 +137,17 @@ util/format_converter_mac: src/main/c/binarize.cpp\n endif\n \n .PHONY: build-mindbender\n+MINDBENDER=mindbender\n build-mindbender:\n-\tgit submodule update --init mindbender\n-\t$(MAKE) -C mindbender clean-packages\n-\t$(MAKE) -C mindbender package\n+\t@util/build/build-submodule-if-needed $(MINDBENDER) mindbender-LATEST-Darwin-x86_64.sh",
        "path": "Makefile",
        "position": null,
        "original_position": 88,
        "commit_id": "1815fa8bd346746b171a3cae622c27cb3a162a31",
        "original_commit_id": "328a620b4606b77d1b5feba20d038cfd68e83988",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Seems missing `test-build build: build-mindbender` here\n",
        "created_at": "2015-12-29T01:04:34Z",
        "updated_at": "2016-01-01T19:42:03Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/445#discussion_r48516243",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/445",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/48516243"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/445#discussion_r48516243"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/445"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/48516452",
        "pull_request_review_id": null,
        "id": 48516452,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTE2NDUy",
        "diff_hunk": "@@ -0,0 +1,739 @@\n+#!/usr/bin/env jq-f\n+# compile-config-2.01-grounding -- Adds processes for grounding the factor graph\n+##\n+\n+include \"util\";\n+include \"sql\";\n+\n+.deepdive_ as $deepdive\n+\n+# some names for constants\n+| \"id\"                             as $deepdiveVariableIdColumn\n+| \"dd_graph_variables_holdout\"     as $deepdiveGlobalHoldoutTable\n+| \"dd_graph_variables_observation\" as $deepdiveGlobalObservationTable\n+| \"dd_graph_weights\"               as $deepdiveGlobalWeightsTable\n+| \"dd_query_\"                      as $deepdivePrefixForFactorsTable # TODO correct prefix to dd_factors_?\n+| \"dd_weights_\"                    as $deepdivePrefixForWeightsTable\n+| \"dd_weightsmulti_\"               as $deepdivePrefixForMultinomialWeightsTable\n+| \"dd_categories_\"                 as $deepdivePrefixForVariableCategoriesTable\n+\n+###############################################################################\n+\n+# parse variable schema to define some names (mainly to find Multinomial cardinality)\n+| .deepdive_.schema.variables_ = [\n+    $deepdive.schema.variables | to_entries[]\n+    | .key as $relationName | .value | to_entries\n+        | if length != 1 then error(\"deepdive.schema.variables.\\($relationName) has \\(length\n+            ) columns declared as variable. Every relation can have up to one column declared as a variable\") else .[] end\n+        | .key as $columnName | .value as $variableType |\n+        { variableName         : $relationName\n+        , variablesTable       : $relationName\n+        , variablesLabelColumn : $columnName\n+        , variableCategoriesTable : \"\\($deepdivePrefixForVariableCategoriesTable)\\($relationName)\"\n+        } + (\n+            # parse variable type\n+            $variableType | trimWhitespace | ascii_downcase |\n+            capture(\"^(?:\n+                        (?<isBooleanType>     boolean     )\n+                      | (?<isCategoricalType> categorical )\n+                            \\\\s* \\\\(\n+                            \\\\s* (?<variableCardinality> \\\\d+)\n+                            \\\\s* \\\\)\n+                      )$\"; \"xmi\") //\n+                error(\"deepdive.schema.variables.\\($relationName).\\($columnName\n+                    ) has an unrecognized type: \\($variableType | @json)\")\n+        ) |\n+        if   .isBooleanType     then .variableType = \"boolean\"     | .variableCardinality = 2\n+        elif .isCategoricalType then .variableType = \"categorical\" | .variableCardinality |= tonumber\n+        else .\n+        end\n+]\n+# create a map to make it easy to access a variable by its name\n+| .deepdive_.schema.variables_byName = (.deepdive_.schema.variables_ | map({key: .variableName, value: .}) | from_entries)\n+| .deepdive_ as $deepdive  # necessary since we just mutated it\n+\n+# parse inference rules, especially the function and weight fields as function_ and weight_\n+| .deepdive_.inference.factors_ = [\n+    $deepdive.inference.factors | to_entries[]\n+    | .key as $factorNameQualified\n+    | (.key | ltrimstr(\"factor/\")) as $factorName | .value\n+\n+    # some useful names for compilation\n+    | .factorNameQualified = $factorNameQualified\n+    | .factorName = $factorName\n+    | .factorsTable = \"\\($deepdivePrefixForFactorsTable)\\($factorName)\"\n+    | .weightsTable = \"\\($deepdivePrefixForWeightsTable)\\($factorName)\"\n+\n+    # parse the weight field\n+    | .weight_ = (.weight | trimWhitespace\n+        | if startswith(\"?\")? then\n+            # unknown weight, find parameters\n+            { is_fixed: false\n+            , params: (ltrimstr(\"?\") | trimWhitespace\n+                | ltrimstr(\"(\") | rtrimstr(\")\") | trimWhitespace\n+                | if length == 0 then [] else  split(\"\\\\s*,\\\\s*\") end)\n+            , init_value: 0.0\n+            }\n+        else\n+            # fixed weight\n+            { is_fixed: true\n+            , params: []\n+            , init_value: tonumber\n+            }\n+        end\n+        )\n+\n+    # parse the function field\n+    | .function_ = (.function | trimWhitespace\n+        | capture(\"^ (?<name>.+)\n+                \\\\s* \\\\(\n+                \\\\s* (?<variables>.+)\n+                \\\\s* \\\\)\n+                   $\"; \"x\") // error(\"deepdive.inference.factors.\\($factorName\n+                        ) has an unrecognized function: \\(@json)\")\n+        | .name |= ascii_downcase\n+        # parse arguments to the function or predicate (variables)\n+        | .variables |= [ trimWhitespace | splits(\"\\\\s*,\\\\s*\")\n+            | capture(\"^ (?<isNegated> !      )?\n+                    \\\\s* (?<columnLabel>\n+                            (?<columnPrefix>\n+                                (?<name> [^.]+ )\n+                    \\\\s*        \\\\. (?: [^.]+ \\\\. )*\n+                            )\n+                    \\\\s*    (?: [^.]+ )\n+                         )\n+                    \\\\s* (?<field>     .+     )\n+                    \\\\s* (?<isArray>   \\\\[\\\\] )?\n+                    \\\\s* (?: = (?<equalsTo> \\\\d+))?\n+                       $\"; \"x\") // error(\"deepdive.inference.factors.\\($factorName\n+                            ) has an unrecognized variable argument: \\(@json)\")\n+            | .isNegated |= (length > 0)\n+            | .isArray   |= (length > 0)\n+            | .equalsTo  |= (if . then tonumber else null end)\n+            | .columnId   = \"\\(.columnPrefix)\\($deepdiveVariableIdColumn)\"\n+            # link this variable reference to its schema definition to simplify compilation\n+            | .schema     = $deepdive.schema.variables_byName[.name] //\n+                error(\"deepdive.inference.factors.\\($factorName).function refers to an undefined variable: \\(.name)\")\n+            ]\n+        # assign the ordinal index to each variable\n+        | .variables |= [ . as $vars | range($vars | length) | . as $i | $vars[$i] | .ordinal = $i ]\n+        # map function name (case insensitive) to the code used in the binary format for the inference engine\n+        | .id =\n+            { imply       : 0\n+            , or          : 1\n+            , and         : 2\n+            , equal       : 3\n+            , istrue      : 4\n+            , multinomial : 5\n+            , linear      : 7\n+            , ratio       : 8\n+            , logical     : 9\n+            , imply3      : 11\n+            }[.name] //\n+                error(\"deepdive.inference.factors.\\($factorName\n+                    ) uses an unrecognized function: \\(.name | @json)\")\n+        )\n+        # TODO check if all .variables type are categorical for multinomial function\n+\n+    # XXX set up an exploded table to dump for multinomial factors\n+    | .weightsTableForDumping =\n+        if .function_.name != \"multinomial\" then .weightsTable\n+        else \"\\($deepdivePrefixForMultinomialWeightsTable)\\($factorName)\"\n+        end\n+]\n+# create a map to make it easy to access a factor by its name\n+| .deepdive_.inference.factors_byName = (.deepdive_.inference.factors_ | map({key: .factorName, value: .}) | from_entries)\n+| .deepdive_ as $deepdive  # necessary since we just mutated it\n+\n+\n+###############################################################################\n+\n+# how to ground the factor graph in the old way with legacy Scala code\n+# TODO remove me as we drop the scala codebase\n+| .deepdive_.execution.processes += {\n+    \"process/grounding/legacy\": {\n+        dependencies_: ($deepdive.inference.factors_ | map(.factorNameQualified)),\n+        output_: \"model/factorgraph.legacy\",\n+        style: \"cmd_extractor\",\n+        cmd: \"mkdir -p ../../../model && cd ../../../model\n+            mkdir -p factorgraph\n+\n+            set +x; . load-db-driver.sh; set -x\n+            export DEEPDIVE_LOGFILE=factorgraph/grounding.log\n+            [[ ! -e \\\"$DEEPDIVE_LOGFILE\\\" ]] || mv -f \\\"$DEEPDIVE_LOGFILE\\\" \\\"$DEEPDIVE_LOGFILE\\\"~\n+            java org.deepdive.Main -c <(\n+                set +x\n+                echo \\(\"deepdive \\(.deepdive | @json)\" | @sh)\n+                echo \\(\"deepdive.pipeline.pipelines.grounding: [\\($deepdive.inference.factors_ | map(.factorName) | join(\", \"))]\" | @sh)\n+                echo \\(\"deepdive.pipeline.run: grounding\" | @sh)\n+            ) -o factorgraph -t inference_grounding\n+\n+            # drop graph. prefix from file names\n+            cd factorgraph\n+            mv -f graph.variables variables\n+            mv -f graph.factors   factors\n+            mv -f graph.weights   weights\n+            mv -f graph.meta      meta\n+        \"\n+    }\n+}\n+\n+\n+###############################################################################\n+\n+## variable_id_partition\n+# Grounding begins by counting the variables to partition a range of\n+# non-negative integers for assigning the variable ids.\n+| .deepdive_.execution.processes += {\n+    \"process/grounding/variable_id_partition\": {\n+        dependencies_: [\n+            # id partition depends on all variable tables\n+            $deepdive.schema.variables_[] | \"data/\\(.variablesTable)\"\n+        ],\n+        style: \"cmd_extractor\", cmd: \"\n+        : ${DEEPDIVE_GROUNDING_DIR:=\\\"$DEEPDIVE_APP\\\"/run/model/grounding}\n+\n+        RANGE_BEGIN=0 \\\\\n+        partition_id_range \\($deepdive.schema.variables_ | map(.variablesTable | @sh) | join(\" \")) | {\n+            # record the base\n+            variableCountTotal=0\n+            while read table begin excludeEnd; do\n+                varPath=\\\"$DEEPDIVE_GROUNDING_DIR\\\"/variable/${table}\n+                mkdir -p \\\"$varPath\\\"\n+                cd \\\"$varPath\\\"\n+                echo $begin                      >id_begin\n+                echo $excludeEnd                 >id_exclude_end\n+                echo $(( $excludeEnd - $begin )) >count\n+                variableCountTotal=$excludeEnd\n+            done\n+            # record the final count\n+            echo $variableCountTotal >\\\"$DEEPDIVE_GROUNDING_DIR\\\"/variable_count\n+        }\n+        \"\n+    }\n+}\n+\n+\n+## variable/*/assign_id\n+# Each variable table then gets the range of integers assigned to the id column\n+# of every row.\n+| .deepdive_.execution.processes += merge($deepdive.schema.variables_[] | {\n+    \"process/grounding/variable/\\(.variableName)/assign_id\": {\n+        dependencies_: [\n+            \"process/grounding/variable_id_partition\"\n+        ],\n+        style: \"cmd_extractor\", cmd: \"\n+        : ${DEEPDIVE_GROUNDING_DIR:=\\\"$DEEPDIVE_APP\\\"/run/model/grounding}\n+        table=\\(.variablesTable | @sh)\n+\n+        cd \\\"$DEEPDIVE_GROUNDING_DIR\\\"/variable/${table}\n+        baseId=$(cat id_begin)\n+\n+        # assign id to all rows according to the paritition\n+        deepdive db assign_sequential_id $table \\($deepdiveVariableIdColumn | @sh) $baseId\n+\n+        \\(\n+        if .variableType == \"categorical\" then",
        "path": "compiler/compile-config/compile-config-2.01-grounding",
        "position": null,
        "original_position": 236,
        "commit_id": "1815fa8bd346746b171a3cae622c27cb3a162a31",
        "original_commit_id": "328a620b4606b77d1b5feba20d038cfd68e83988",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "The \"categorical\" branch seems like a separate part from id assignment.\n",
        "created_at": "2015-12-29T01:10:39Z",
        "updated_at": "2016-01-01T19:42:03Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/445#discussion_r48516452",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/445",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/48516452"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/445#discussion_r48516452"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/445"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/48516578",
        "pull_request_review_id": null,
        "id": 48516578,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTE2NTc4",
        "diff_hunk": "@@ -130,14 +137,17 @@ util/format_converter_mac: src/main/c/binarize.cpp\n endif\n \n .PHONY: build-mindbender\n+MINDBENDER=mindbender\n build-mindbender:\n-\tgit submodule update --init mindbender\n-\t$(MAKE) -C mindbender clean-packages\n-\t$(MAKE) -C mindbender package\n+\t@util/build/build-submodule-if-needed $(MINDBENDER) mindbender-LATEST-Darwin-x86_64.sh",
        "path": "Makefile",
        "position": null,
        "original_position": 88,
        "commit_id": "1815fa8bd346746b171a3cae622c27cb3a162a31",
        "original_commit_id": "328a620b4606b77d1b5feba20d038cfd68e83988",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "mindbender build has been optional in the past as it's quite a big one, so I kept it as it is.  (stage.sh also optionally stages the built binary.)  I feel we should do that in a separate PR\n",
        "created_at": "2015-12-29T01:14:48Z",
        "updated_at": "2016-01-01T19:42:03Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/445#discussion_r48516578",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/445",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/48516578"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/445#discussion_r48516578"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/445"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/48517141",
        "pull_request_review_id": null,
        "id": 48517141,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTE3MTQx",
        "diff_hunk": "@@ -0,0 +1,739 @@\n+#!/usr/bin/env jq-f\n+# compile-config-2.01-grounding -- Adds processes for grounding the factor graph\n+##\n+\n+include \"util\";\n+include \"sql\";\n+\n+.deepdive_ as $deepdive\n+\n+# some names for constants\n+| \"id\"                             as $deepdiveVariableIdColumn\n+| \"dd_graph_variables_holdout\"     as $deepdiveGlobalHoldoutTable\n+| \"dd_graph_variables_observation\" as $deepdiveGlobalObservationTable\n+| \"dd_graph_weights\"               as $deepdiveGlobalWeightsTable\n+| \"dd_query_\"                      as $deepdivePrefixForFactorsTable # TODO correct prefix to dd_factors_?\n+| \"dd_weights_\"                    as $deepdivePrefixForWeightsTable\n+| \"dd_weightsmulti_\"               as $deepdivePrefixForMultinomialWeightsTable\n+| \"dd_categories_\"                 as $deepdivePrefixForVariableCategoriesTable\n+\n+###############################################################################\n+\n+# parse variable schema to define some names (mainly to find Multinomial cardinality)\n+| .deepdive_.schema.variables_ = [\n+    $deepdive.schema.variables | to_entries[]\n+    | .key as $relationName | .value | to_entries\n+        | if length != 1 then error(\"deepdive.schema.variables.\\($relationName) has \\(length\n+            ) columns declared as variable. Every relation can have up to one column declared as a variable\") else .[] end\n+        | .key as $columnName | .value as $variableType |\n+        { variableName         : $relationName\n+        , variablesTable       : $relationName\n+        , variablesLabelColumn : $columnName\n+        , variableCategoriesTable : \"\\($deepdivePrefixForVariableCategoriesTable)\\($relationName)\"\n+        } + (\n+            # parse variable type\n+            $variableType | trimWhitespace | ascii_downcase |\n+            capture(\"^(?:\n+                        (?<isBooleanType>     boolean     )\n+                      | (?<isCategoricalType> categorical )\n+                            \\\\s* \\\\(\n+                            \\\\s* (?<variableCardinality> \\\\d+)\n+                            \\\\s* \\\\)\n+                      )$\"; \"xmi\") //\n+                error(\"deepdive.schema.variables.\\($relationName).\\($columnName\n+                    ) has an unrecognized type: \\($variableType | @json)\")\n+        ) |\n+        if   .isBooleanType     then .variableType = \"boolean\"     | .variableCardinality = 2\n+        elif .isCategoricalType then .variableType = \"categorical\" | .variableCardinality |= tonumber\n+        else .\n+        end\n+]\n+# create a map to make it easy to access a variable by its name\n+| .deepdive_.schema.variables_byName = (.deepdive_.schema.variables_ | map({key: .variableName, value: .}) | from_entries)\n+| .deepdive_ as $deepdive  # necessary since we just mutated it\n+\n+# parse inference rules, especially the function and weight fields as function_ and weight_\n+| .deepdive_.inference.factors_ = [\n+    $deepdive.inference.factors | to_entries[]\n+    | .key as $factorNameQualified\n+    | (.key | ltrimstr(\"factor/\")) as $factorName | .value\n+\n+    # some useful names for compilation\n+    | .factorNameQualified = $factorNameQualified\n+    | .factorName = $factorName\n+    | .factorsTable = \"\\($deepdivePrefixForFactorsTable)\\($factorName)\"\n+    | .weightsTable = \"\\($deepdivePrefixForWeightsTable)\\($factorName)\"\n+\n+    # parse the weight field\n+    | .weight_ = (.weight | trimWhitespace\n+        | if startswith(\"?\")? then\n+            # unknown weight, find parameters\n+            { is_fixed: false\n+            , params: (ltrimstr(\"?\") | trimWhitespace\n+                | ltrimstr(\"(\") | rtrimstr(\")\") | trimWhitespace\n+                | if length == 0 then [] else  split(\"\\\\s*,\\\\s*\") end)\n+            , init_value: 0.0\n+            }\n+        else\n+            # fixed weight\n+            { is_fixed: true\n+            , params: []\n+            , init_value: tonumber\n+            }\n+        end\n+        )\n+\n+    # parse the function field\n+    | .function_ = (.function | trimWhitespace\n+        | capture(\"^ (?<name>.+)\n+                \\\\s* \\\\(\n+                \\\\s* (?<variables>.+)\n+                \\\\s* \\\\)\n+                   $\"; \"x\") // error(\"deepdive.inference.factors.\\($factorName\n+                        ) has an unrecognized function: \\(@json)\")\n+        | .name |= ascii_downcase\n+        # parse arguments to the function or predicate (variables)\n+        | .variables |= [ trimWhitespace | splits(\"\\\\s*,\\\\s*\")\n+            | capture(\"^ (?<isNegated> !      )?\n+                    \\\\s* (?<columnLabel>\n+                            (?<columnPrefix>\n+                                (?<name> [^.]+ )\n+                    \\\\s*        \\\\. (?: [^.]+ \\\\. )*\n+                            )\n+                    \\\\s*    (?: [^.]+ )\n+                         )\n+                    \\\\s* (?<field>     .+     )\n+                    \\\\s* (?<isArray>   \\\\[\\\\] )?\n+                    \\\\s* (?: = (?<equalsTo> \\\\d+))?\n+                       $\"; \"x\") // error(\"deepdive.inference.factors.\\($factorName\n+                            ) has an unrecognized variable argument: \\(@json)\")\n+            | .isNegated |= (length > 0)\n+            | .isArray   |= (length > 0)\n+            | .equalsTo  |= (if . then tonumber else null end)\n+            | .columnId   = \"\\(.columnPrefix)\\($deepdiveVariableIdColumn)\"\n+            # link this variable reference to its schema definition to simplify compilation\n+            | .schema     = $deepdive.schema.variables_byName[.name] //\n+                error(\"deepdive.inference.factors.\\($factorName).function refers to an undefined variable: \\(.name)\")\n+            ]\n+        # assign the ordinal index to each variable\n+        | .variables |= [ . as $vars | range($vars | length) | . as $i | $vars[$i] | .ordinal = $i ]\n+        # map function name (case insensitive) to the code used in the binary format for the inference engine\n+        | .id =\n+            { imply       : 0\n+            , or          : 1\n+            , and         : 2\n+            , equal       : 3\n+            , istrue      : 4\n+            , multinomial : 5\n+            , linear      : 7\n+            , ratio       : 8\n+            , logical     : 9\n+            , imply3      : 11\n+            }[.name] //\n+                error(\"deepdive.inference.factors.\\($factorName\n+                    ) uses an unrecognized function: \\(.name | @json)\")\n+        )\n+        # TODO check if all .variables type are categorical for multinomial function\n+\n+    # XXX set up an exploded table to dump for multinomial factors\n+    | .weightsTableForDumping =\n+        if .function_.name != \"multinomial\" then .weightsTable\n+        else \"\\($deepdivePrefixForMultinomialWeightsTable)\\($factorName)\"\n+        end\n+]\n+# create a map to make it easy to access a factor by its name\n+| .deepdive_.inference.factors_byName = (.deepdive_.inference.factors_ | map({key: .factorName, value: .}) | from_entries)\n+| .deepdive_ as $deepdive  # necessary since we just mutated it\n+\n+\n+###############################################################################\n+\n+# how to ground the factor graph in the old way with legacy Scala code\n+# TODO remove me as we drop the scala codebase\n+| .deepdive_.execution.processes += {\n+    \"process/grounding/legacy\": {\n+        dependencies_: ($deepdive.inference.factors_ | map(.factorNameQualified)),\n+        output_: \"model/factorgraph.legacy\",\n+        style: \"cmd_extractor\",\n+        cmd: \"mkdir -p ../../../model && cd ../../../model\n+            mkdir -p factorgraph\n+\n+            set +x; . load-db-driver.sh; set -x\n+            export DEEPDIVE_LOGFILE=factorgraph/grounding.log\n+            [[ ! -e \\\"$DEEPDIVE_LOGFILE\\\" ]] || mv -f \\\"$DEEPDIVE_LOGFILE\\\" \\\"$DEEPDIVE_LOGFILE\\\"~\n+            java org.deepdive.Main -c <(\n+                set +x\n+                echo \\(\"deepdive \\(.deepdive | @json)\" | @sh)\n+                echo \\(\"deepdive.pipeline.pipelines.grounding: [\\($deepdive.inference.factors_ | map(.factorName) | join(\", \"))]\" | @sh)\n+                echo \\(\"deepdive.pipeline.run: grounding\" | @sh)\n+            ) -o factorgraph -t inference_grounding\n+\n+            # drop graph. prefix from file names\n+            cd factorgraph\n+            mv -f graph.variables variables\n+            mv -f graph.factors   factors\n+            mv -f graph.weights   weights\n+            mv -f graph.meta      meta\n+        \"\n+    }\n+}\n+\n+\n+###############################################################################\n+\n+## variable_id_partition\n+# Grounding begins by counting the variables to partition a range of\n+# non-negative integers for assigning the variable ids.\n+| .deepdive_.execution.processes += {\n+    \"process/grounding/variable_id_partition\": {\n+        dependencies_: [\n+            # id partition depends on all variable tables\n+            $deepdive.schema.variables_[] | \"data/\\(.variablesTable)\"\n+        ],\n+        style: \"cmd_extractor\", cmd: \"\n+        : ${DEEPDIVE_GROUNDING_DIR:=\\\"$DEEPDIVE_APP\\\"/run/model/grounding}\n+\n+        RANGE_BEGIN=0 \\\\\n+        partition_id_range \\($deepdive.schema.variables_ | map(.variablesTable | @sh) | join(\" \")) | {\n+            # record the base\n+            variableCountTotal=0\n+            while read table begin excludeEnd; do\n+                varPath=\\\"$DEEPDIVE_GROUNDING_DIR\\\"/variable/${table}\n+                mkdir -p \\\"$varPath\\\"\n+                cd \\\"$varPath\\\"\n+                echo $begin                      >id_begin\n+                echo $excludeEnd                 >id_exclude_end\n+                echo $(( $excludeEnd - $begin )) >count\n+                variableCountTotal=$excludeEnd\n+            done\n+            # record the final count\n+            echo $variableCountTotal >\\\"$DEEPDIVE_GROUNDING_DIR\\\"/variable_count\n+        }\n+        \"\n+    }\n+}\n+\n+\n+## variable/*/assign_id\n+# Each variable table then gets the range of integers assigned to the id column\n+# of every row.\n+| .deepdive_.execution.processes += merge($deepdive.schema.variables_[] | {\n+    \"process/grounding/variable/\\(.variableName)/assign_id\": {\n+        dependencies_: [\n+            \"process/grounding/variable_id_partition\"\n+        ],\n+        style: \"cmd_extractor\", cmd: \"\n+        : ${DEEPDIVE_GROUNDING_DIR:=\\\"$DEEPDIVE_APP\\\"/run/model/grounding}\n+        table=\\(.variablesTable | @sh)\n+\n+        cd \\\"$DEEPDIVE_GROUNDING_DIR\\\"/variable/${table}\n+        baseId=$(cat id_begin)\n+\n+        # assign id to all rows according to the paritition\n+        deepdive db assign_sequential_id $table \\($deepdiveVariableIdColumn | @sh) $baseId\n+\n+        \\(\n+        if .variableType == \"categorical\" then",
        "path": "compiler/compile-config/compile-config-2.01-grounding",
        "position": null,
        "original_position": 236,
        "commit_id": "1815fa8bd346746b171a3cae622c27cb3a162a31",
        "original_commit_id": "328a620b4606b77d1b5feba20d038cfd68e83988",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I agree.  Maybe we should rename this step to a more general one?  Any good idea?  Alternatively, we could create optional process nodes, but I didn't want to mess up with the dependency graph as it requires another branch when enumerating dependencies for the `\"process/grounding/variable_holdout\"` node.\n",
        "created_at": "2015-12-29T01:33:30Z",
        "updated_at": "2016-01-01T19:42:03Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/445#discussion_r48517141",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/445",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/48517141"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/445#discussion_r48517141"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/445"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/48517150",
        "pull_request_review_id": null,
        "id": 48517150,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTE3MTUw",
        "diff_hunk": "@@ -0,0 +1,136 @@\n+#!/usr/bin/env bash\n+# local/compute-execute -- Executes a process locally using all available processors\n+# $ compute-execute input_sql=... command=... output_relation=...\n+#\n+# To limit the number of parallel processes, set the DEEPDIVE_NUM_PROCESSES\n+# environment or the 'deepdive.computers.local.num_processes' in\n+# computers.conf:\n+# $ export DEEPDIVE_NUM_PROCESSES=2\n+# $ compute-execute input_sql=... command=... output_relation=...\n+##\n+set -euo pipefail\n+\n+: ${DEEPDIVE_PREFIX_TABLE_TEMPORARY:=dd_tmp_} ${DEEPDIVE_PREFIX_TABLE_OLD:=dd_old_}\n+\n+# load compute configuration\n+eval \"$(jq2sh <<<\"$DEEPDIVE_COMPUTER_CONFIG\" \\\n+    num_processes='.num_processes' \\\n+    #\n+)\"\n+# respect the DEEPDIVE_NUM_PROCESSES environment\n+num_processes=${DEEPDIVE_NUM_PROCESSES:-${num_processes:-$(\n+        # detect number of processor cores\n+        nproc=$(\n+            # Linux typically has coreutils which includes nproc\n+            nproc ||\n+            # OS X\n+            sysctl -n hw.ncpu ||\n+            # fall back to 1\n+            echo 1\n+        )\n+        if [[ $nproc -gt 1 ]]; then\n+            # leave one processor out\n+            let nproc-=1\n+        elif [[ $nproc -lt 1 ]]; then\n+            nproc=1\n+        fi\n+        echo $nproc\n+    )}}\n+\n+# declare all input arguments\n+declare -- \"$@\"\n+\n+# show configuration\n+echo \"Executing with the following configuration:\"\n+echo \" num_processes=$num_processes\"\n+\n+# XXX there are conditional branches below depending on whether input_sql\n+# and/or output_relation is given, to support four use cases:\n+# 1) executing command while streaming data from/to the database\n+# 2) input-only command which has no output to the database and streams from the database\n+# 3) output-only command which has no input from the database and streams to the database\n+# 4) database-independent command which simply runs in parallel\n+\n+# prepare a temporary output table when output_relation is given\n+if [[ -n $output_relation ]]; then\n+    # some derived values\n+    output_relation_tmp=\"${DEEPDIVE_PREFIX_TABLE_TEMPORARY}${output_relation}\"\n+\n+    # show configuration\n+    echo \" output_relation_tmp=$output_relation_tmp\"\n+    echo\n+\n+    # use an empty temporary table as a sink instead of TRUNCATE'ing the output_relation\n+    deepdive-initdb \"$output_relation\"\n+    deepdive-sql \"\n+        DROP TABLE IF EXISTS $output_relation_tmp;\n+        CREATE TABLE $output_relation_tmp (LIKE $output_relation);\n+    \"\n+fi\n+\n+# set up named pipes for parallel processes and make sure they are cleaned up upon exit\n+[[ -z $input_sql       ]] || for i in $(seq $num_processes); do rm -f process-$i.input ; mkfifo process-$i.input ; done\n+[[ -z $output_relation ]] || for i in $(seq $num_processes); do rm -f process-$i.output; mkfifo process-$i.output; done\n+trap 'rm -f process-*.{input,output}' EXIT\n+# now spawn processes attached to the named pipes in reverse order (from sink to source)\n+\n+if [[ -n $output_relation ]]; then\n+    # use mkmimo again to merge outputs of multiple processes into a single stream\n+    mkmimo process-*.output \\> /dev/stdout |\n+    # load the output data to the temporary table in the database\n+    # XXX hiding default progress bar from deepdive-load\n+    # TODO abbreviate this env into a show_progress option, e.g., recursive=false\n+    show_progress input_to \"$DEEPDIVE_CURRENT_PROCESS_NAME output\" -- \\\n+    env DEEPDIVE_PROGRESS_FD=2 \\\n+    deepdive-load \"$output_relation_tmp\" /dev/stdin &\n+fi\n+\n+# spawn multiple processes attached to the pipes\n+if [[ -n $output_relation && -n $input_sql ]]; then # process with input from/output to database\n+    for i in $(seq $num_processes); do\n+        DEEPDIVE_CURRENT_PROCESS_INDEX=$i \\\n+        \"$SHELL\" -c \"$command\" <process-$i.input >process-$i.output &\n+    done\n+elif [[ -n $input_sql ]]; then # input-only process\n+    for i in $(seq $num_processes); do\n+        DEEPDIVE_CURRENT_PROCESS_INDEX=$i \\\n+        \"$SHELL\" -c \"$command\" <process-$i.input &\n+    done\n+elif [[ -n $output_relation ]]; then # output-only process\n+    for i in $(seq $num_processes); do\n+        DEEPDIVE_CURRENT_PROCESS_INDEX=$i \\\n+        \"$SHELL\" -c \"$command\" >process-$i.output &\n+    done\n+else # neither output_relation nor input_sql specified\n+    for i in $(seq $num_processes); do\n+        DEEPDIVE_CURRENT_PROCESS_INDEX=$i \\\n+        \"$SHELL\" -c \"$command\" &\n+    done\n+fi\n+\n+if [[ -n $input_sql ]]; then\n+    # unload data from the database and pour into the pipes\n+    show_progress output_from \"$DEEPDIVE_CURRENT_PROCESS_NAME input\" -- \\\n+    deepdive-sql eval \"$input_sql\" format=\"$DEEPDIVE_LOAD_FORMAT\" |\n+    # use mkmimo to distribute input data to multiple processes\n+    mkmimo /dev/stdin \\> process-*.input &\n+fi\n+\n+wait  # until everything is done ##############################################\n+\n+if [[ -n $output_relation ]]; then\n+    # rename the new temporary table\n+    # TODO maybe use PostgreSQL's schema support here?\n+    echo \"Replacing $output_relation with $output_relation_tmp\"",
        "path": "runner/compute-driver/local/compute-execute",
        "position": null,
        "original_position": 124,
        "commit_id": "1815fa8bd346746b171a3cae622c27cb3a162a31",
        "original_commit_id": "140da9c38e1fc380eb50a14f1e3d86992d956dc8",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Makes sense. Though the `+=` syntax in ddlog may give people the false impression that the output would be appended...\n",
        "created_at": "2015-12-29T01:33:55Z",
        "updated_at": "2016-01-01T19:42:03Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/445#discussion_r48517150",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/445",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/48517150"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/445#discussion_r48517150"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/445"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/48560850",
        "pull_request_review_id": null,
        "id": 48560850,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTYwODUw",
        "diff_hunk": "@@ -130,14 +137,17 @@ util/format_converter_mac: src/main/c/binarize.cpp\n endif\n \n .PHONY: build-mindbender\n+MINDBENDER=mindbender\n build-mindbender:\n-\tgit submodule update --init mindbender\n-\t$(MAKE) -C mindbender clean-packages\n-\t$(MAKE) -C mindbender package\n+\t@util/build/build-submodule-if-needed $(MINDBENDER) mindbender-LATEST-Darwin-x86_64.sh",
        "path": "Makefile",
        "position": null,
        "original_position": 88,
        "commit_id": "1815fa8bd346746b171a3cae622c27cb3a162a31",
        "original_commit_id": "328a620b4606b77d1b5feba20d038cfd68e83988",
        "user": {
            "login": "zifeishan",
            "id": 1811701,
            "node_id": "MDQ6VXNlcjE4MTE3MDE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1811701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zifeishan",
            "html_url": "https://github.com/zifeishan",
            "followers_url": "https://api.github.com/users/zifeishan/followers",
            "following_url": "https://api.github.com/users/zifeishan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zifeishan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zifeishan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zifeishan/subscriptions",
            "organizations_url": "https://api.github.com/users/zifeishan/orgs",
            "repos_url": "https://api.github.com/users/zifeishan/repos",
            "events_url": "https://api.github.com/users/zifeishan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zifeishan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Also, why are we forcing the `Darwin` release here?\n",
        "created_at": "2015-12-29T18:48:45Z",
        "updated_at": "2016-01-01T19:42:03Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/445#discussion_r48560850",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/445",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/48560850"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/445#discussion_r48560850"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/445"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/48576337",
        "pull_request_review_id": null,
        "id": 48576337,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTc2MzM3",
        "diff_hunk": "@@ -0,0 +1,136 @@\n+#!/usr/bin/env bash\n+# local/compute-execute -- Executes a process locally using all available processors\n+# $ compute-execute input_sql=... command=... output_relation=...\n+#\n+# To limit the number of parallel processes, set the DEEPDIVE_NUM_PROCESSES\n+# environment or the 'deepdive.computers.local.num_processes' in\n+# computers.conf:\n+# $ export DEEPDIVE_NUM_PROCESSES=2\n+# $ compute-execute input_sql=... command=... output_relation=...\n+##\n+set -euo pipefail\n+\n+: ${DEEPDIVE_PREFIX_TABLE_TEMPORARY:=dd_tmp_} ${DEEPDIVE_PREFIX_TABLE_OLD:=dd_old_}\n+\n+# load compute configuration\n+eval \"$(jq2sh <<<\"$DEEPDIVE_COMPUTER_CONFIG\" \\\n+    num_processes='.num_processes' \\\n+    #\n+)\"\n+# respect the DEEPDIVE_NUM_PROCESSES environment\n+num_processes=${DEEPDIVE_NUM_PROCESSES:-${num_processes:-$(\n+        # detect number of processor cores\n+        nproc=$(\n+            # Linux typically has coreutils which includes nproc\n+            nproc ||\n+            # OS X\n+            sysctl -n hw.ncpu ||\n+            # fall back to 1\n+            echo 1\n+        )\n+        if [[ $nproc -gt 1 ]]; then\n+            # leave one processor out\n+            let nproc-=1\n+        elif [[ $nproc -lt 1 ]]; then\n+            nproc=1\n+        fi\n+        echo $nproc\n+    )}}\n+\n+# declare all input arguments\n+declare -- \"$@\"\n+\n+# show configuration\n+echo \"Executing with the following configuration:\"\n+echo \" num_processes=$num_processes\"\n+\n+# XXX there are conditional branches below depending on whether input_sql\n+# and/or output_relation is given, to support four use cases:\n+# 1) executing command while streaming data from/to the database\n+# 2) input-only command which has no output to the database and streams from the database\n+# 3) output-only command which has no input from the database and streams to the database\n+# 4) database-independent command which simply runs in parallel\n+\n+# prepare a temporary output table when output_relation is given\n+if [[ -n $output_relation ]]; then\n+    # some derived values\n+    output_relation_tmp=\"${DEEPDIVE_PREFIX_TABLE_TEMPORARY}${output_relation}\"\n+\n+    # show configuration\n+    echo \" output_relation_tmp=$output_relation_tmp\"\n+    echo\n+\n+    # use an empty temporary table as a sink instead of TRUNCATE'ing the output_relation\n+    deepdive-initdb \"$output_relation\"\n+    deepdive-sql \"\n+        DROP TABLE IF EXISTS $output_relation_tmp;\n+        CREATE TABLE $output_relation_tmp (LIKE $output_relation);\n+    \"\n+fi\n+\n+# set up named pipes for parallel processes and make sure they are cleaned up upon exit\n+[[ -z $input_sql       ]] || for i in $(seq $num_processes); do rm -f process-$i.input ; mkfifo process-$i.input ; done\n+[[ -z $output_relation ]] || for i in $(seq $num_processes); do rm -f process-$i.output; mkfifo process-$i.output; done\n+trap 'rm -f process-*.{input,output}' EXIT\n+# now spawn processes attached to the named pipes in reverse order (from sink to source)\n+\n+if [[ -n $output_relation ]]; then\n+    # use mkmimo again to merge outputs of multiple processes into a single stream\n+    mkmimo process-*.output \\> /dev/stdout |\n+    # load the output data to the temporary table in the database\n+    # XXX hiding default progress bar from deepdive-load\n+    # TODO abbreviate this env into a show_progress option, e.g., recursive=false\n+    show_progress input_to \"$DEEPDIVE_CURRENT_PROCESS_NAME output\" -- \\\n+    env DEEPDIVE_PROGRESS_FD=2 \\\n+    deepdive-load \"$output_relation_tmp\" /dev/stdin &\n+fi\n+\n+# spawn multiple processes attached to the pipes\n+if [[ -n $output_relation && -n $input_sql ]]; then # process with input from/output to database\n+    for i in $(seq $num_processes); do\n+        DEEPDIVE_CURRENT_PROCESS_INDEX=$i \\\n+        \"$SHELL\" -c \"$command\" <process-$i.input >process-$i.output &\n+    done\n+elif [[ -n $input_sql ]]; then # input-only process\n+    for i in $(seq $num_processes); do\n+        DEEPDIVE_CURRENT_PROCESS_INDEX=$i \\\n+        \"$SHELL\" -c \"$command\" <process-$i.input &\n+    done\n+elif [[ -n $output_relation ]]; then # output-only process\n+    for i in $(seq $num_processes); do\n+        DEEPDIVE_CURRENT_PROCESS_INDEX=$i \\\n+        \"$SHELL\" -c \"$command\" >process-$i.output &\n+    done\n+else # neither output_relation nor input_sql specified\n+    for i in $(seq $num_processes); do\n+        DEEPDIVE_CURRENT_PROCESS_INDEX=$i \\\n+        \"$SHELL\" -c \"$command\" &\n+    done\n+fi\n+\n+if [[ -n $input_sql ]]; then\n+    # unload data from the database and pour into the pipes\n+    show_progress output_from \"$DEEPDIVE_CURRENT_PROCESS_NAME input\" -- \\\n+    deepdive-sql eval \"$input_sql\" format=\"$DEEPDIVE_LOAD_FORMAT\" |\n+    # use mkmimo to distribute input data to multiple processes\n+    mkmimo /dev/stdin \\> process-*.input &\n+fi\n+\n+wait  # until everything is done ##############################################\n+\n+if [[ -n $output_relation ]]; then\n+    # rename the new temporary table\n+    # TODO maybe use PostgreSQL's schema support here?\n+    echo \"Replacing $output_relation with $output_relation_tmp\"",
        "path": "runner/compute-driver/local/compute-execute",
        "position": null,
        "original_position": 124,
        "commit_id": "1815fa8bd346746b171a3cae622c27cb3a162a31",
        "original_commit_id": "140da9c38e1fc380eb50a14f1e3d86992d956dc8",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Would it be clearer if multiple `+=` ddlog rules to the same head relation were compiled into a union of each rule's output?  That way, the semantics still is augmenting the relation with more tuples from the function, but beginning from an empty relation for a given program. Currently, having such repeated heads will result in a deepdive compiler error.\n",
        "created_at": "2015-12-29T22:51:10Z",
        "updated_at": "2016-01-01T19:42:03Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/445#discussion_r48576337",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/445",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/48576337"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/445#discussion_r48576337"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/445"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/48576380",
        "pull_request_review_id": null,
        "id": 48576380,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTc2Mzgw",
        "diff_hunk": "@@ -130,14 +137,17 @@ util/format_converter_mac: src/main/c/binarize.cpp\n endif\n \n .PHONY: build-mindbender\n+MINDBENDER=mindbender\n build-mindbender:\n-\tgit submodule update --init mindbender\n-\t$(MAKE) -C mindbender clean-packages\n-\t$(MAKE) -C mindbender package\n+\t@util/build/build-submodule-if-needed $(MINDBENDER) mindbender-LATEST-Darwin-x86_64.sh",
        "path": "Makefile",
        "position": null,
        "original_position": 88,
        "commit_id": "1815fa8bd346746b171a3cae622c27cb3a162a31",
        "original_commit_id": "328a620b4606b77d1b5feba20d038cfd68e83988",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "That's a mistake. Fix is on the way :)\n",
        "created_at": "2015-12-29T22:52:14Z",
        "updated_at": "2016-01-01T19:42:03Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/445#discussion_r48576380",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/445",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/48576380"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/445#discussion_r48576380"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/445"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/48580144",
        "pull_request_review_id": null,
        "id": 48580144,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTgwMTQ0",
        "diff_hunk": "@@ -0,0 +1,136 @@\n+#!/usr/bin/env bash\n+# local/compute-execute -- Executes a process locally using all available processors\n+# $ compute-execute input_sql=... command=... output_relation=...\n+#\n+# To limit the number of parallel processes, set the DEEPDIVE_NUM_PROCESSES\n+# environment or the 'deepdive.computers.local.num_processes' in\n+# computers.conf:\n+# $ export DEEPDIVE_NUM_PROCESSES=2\n+# $ compute-execute input_sql=... command=... output_relation=...\n+##\n+set -euo pipefail\n+\n+: ${DEEPDIVE_PREFIX_TABLE_TEMPORARY:=dd_tmp_} ${DEEPDIVE_PREFIX_TABLE_OLD:=dd_old_}\n+\n+# load compute configuration\n+eval \"$(jq2sh <<<\"$DEEPDIVE_COMPUTER_CONFIG\" \\\n+    num_processes='.num_processes' \\\n+    #\n+)\"\n+# respect the DEEPDIVE_NUM_PROCESSES environment\n+num_processes=${DEEPDIVE_NUM_PROCESSES:-${num_processes:-$(\n+        # detect number of processor cores\n+        nproc=$(\n+            # Linux typically has coreutils which includes nproc\n+            nproc ||\n+            # OS X\n+            sysctl -n hw.ncpu ||\n+            # fall back to 1\n+            echo 1\n+        )\n+        if [[ $nproc -gt 1 ]]; then\n+            # leave one processor out\n+            let nproc-=1\n+        elif [[ $nproc -lt 1 ]]; then\n+            nproc=1\n+        fi\n+        echo $nproc\n+    )}}\n+\n+# declare all input arguments\n+declare -- \"$@\"\n+\n+# show configuration\n+echo \"Executing with the following configuration:\"\n+echo \" num_processes=$num_processes\"\n+\n+# XXX there are conditional branches below depending on whether input_sql\n+# and/or output_relation is given, to support four use cases:\n+# 1) executing command while streaming data from/to the database\n+# 2) input-only command which has no output to the database and streams from the database\n+# 3) output-only command which has no input from the database and streams to the database\n+# 4) database-independent command which simply runs in parallel\n+\n+# prepare a temporary output table when output_relation is given\n+if [[ -n $output_relation ]]; then\n+    # some derived values\n+    output_relation_tmp=\"${DEEPDIVE_PREFIX_TABLE_TEMPORARY}${output_relation}\"\n+\n+    # show configuration\n+    echo \" output_relation_tmp=$output_relation_tmp\"\n+    echo\n+\n+    # use an empty temporary table as a sink instead of TRUNCATE'ing the output_relation\n+    deepdive-initdb \"$output_relation\"\n+    deepdive-sql \"\n+        DROP TABLE IF EXISTS $output_relation_tmp;\n+        CREATE TABLE $output_relation_tmp (LIKE $output_relation);\n+    \"\n+fi\n+\n+# set up named pipes for parallel processes and make sure they are cleaned up upon exit\n+[[ -z $input_sql       ]] || for i in $(seq $num_processes); do rm -f process-$i.input ; mkfifo process-$i.input ; done\n+[[ -z $output_relation ]] || for i in $(seq $num_processes); do rm -f process-$i.output; mkfifo process-$i.output; done\n+trap 'rm -f process-*.{input,output}' EXIT\n+# now spawn processes attached to the named pipes in reverse order (from sink to source)\n+\n+if [[ -n $output_relation ]]; then\n+    # use mkmimo again to merge outputs of multiple processes into a single stream\n+    mkmimo process-*.output \\> /dev/stdout |\n+    # load the output data to the temporary table in the database\n+    # XXX hiding default progress bar from deepdive-load\n+    # TODO abbreviate this env into a show_progress option, e.g., recursive=false\n+    show_progress input_to \"$DEEPDIVE_CURRENT_PROCESS_NAME output\" -- \\\n+    env DEEPDIVE_PROGRESS_FD=2 \\\n+    deepdive-load \"$output_relation_tmp\" /dev/stdin &\n+fi\n+\n+# spawn multiple processes attached to the pipes\n+if [[ -n $output_relation && -n $input_sql ]]; then # process with input from/output to database\n+    for i in $(seq $num_processes); do\n+        DEEPDIVE_CURRENT_PROCESS_INDEX=$i \\\n+        \"$SHELL\" -c \"$command\" <process-$i.input >process-$i.output &\n+    done\n+elif [[ -n $input_sql ]]; then # input-only process\n+    for i in $(seq $num_processes); do\n+        DEEPDIVE_CURRENT_PROCESS_INDEX=$i \\\n+        \"$SHELL\" -c \"$command\" <process-$i.input &\n+    done\n+elif [[ -n $output_relation ]]; then # output-only process\n+    for i in $(seq $num_processes); do\n+        DEEPDIVE_CURRENT_PROCESS_INDEX=$i \\\n+        \"$SHELL\" -c \"$command\" >process-$i.output &\n+    done\n+else # neither output_relation nor input_sql specified\n+    for i in $(seq $num_processes); do\n+        DEEPDIVE_CURRENT_PROCESS_INDEX=$i \\\n+        \"$SHELL\" -c \"$command\" &\n+    done\n+fi\n+\n+if [[ -n $input_sql ]]; then\n+    # unload data from the database and pour into the pipes\n+    show_progress output_from \"$DEEPDIVE_CURRENT_PROCESS_NAME input\" -- \\\n+    deepdive-sql eval \"$input_sql\" format=\"$DEEPDIVE_LOAD_FORMAT\" |\n+    # use mkmimo to distribute input data to multiple processes\n+    mkmimo /dev/stdin \\> process-*.input &\n+fi\n+\n+wait  # until everything is done ##############################################\n+\n+if [[ -n $output_relation ]]; then\n+    # rename the new temporary table\n+    # TODO maybe use PostgreSQL's schema support here?\n+    echo \"Replacing $output_relation with $output_relation_tmp\"",
        "path": "runner/compute-driver/local/compute-execute",
        "position": null,
        "original_position": 124,
        "commit_id": "1815fa8bd346746b171a3cae622c27cb3a162a31",
        "original_commit_id": "140da9c38e1fc380eb50a14f1e3d86992d956dc8",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "That sounds great :)\n",
        "created_at": "2015-12-30T00:12:35Z",
        "updated_at": "2016-01-01T19:42:03Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/445#discussion_r48580144",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/445",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/48580144"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/445#discussion_r48580144"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/445"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/48901077",
        "pull_request_review_id": null,
        "id": 48901077,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTAxMDc3",
        "diff_hunk": "@@ -1,14 +1,24 @@\n #!/usr/bin/env bash\n # deepdive-load -- Loads a given relation's data\n-# > deepdive load RELATION [SOURCE...]\n-# Initializes given RELATION in the database and loads data from SOURCE.\n-# When SOURCE is unspecified, the data is loaded from files found on path\n-# input/RELATION.* under the DeepDive application.\n+# > deepdive load RELATION[(COLUMN[,COLUMN]...)] [SOURCE...]\n+# Initializes given RELATION in the database and loads data from SOURCE for\n+# optionally specified COLUMNs.  When SOURCE is unspecified, the data is loaded\n+# from files found on path input/RELATION.* under the DeepDive application.\n+#\n+# When RELATION is a random variable and no COLUMN is specified, the SOURCE are\n+# expected to contain the `label` column at the end, in addition to all\n+# user-defined ones.",
        "path": "database/deepdive-load",
        "position": 14,
        "original_position": 14,
        "commit_id": "7275c1489eec862e06ccd80245c019540c45a647",
        "original_commit_id": "7275c1489eec862e06ccd80245c019540c45a647",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "This requires the users to be aware of the `label` column in variable relation, but not declaring it in the schema declaration. IMO it would be better to be more consistent: either the `label` column is explicitly exposed to the users, or it's completely hidden from the users.\n",
        "created_at": "2016-01-05T21:50:41Z",
        "updated_at": "2016-01-05T21:50:41Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/447#discussion_r48901077",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/447",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/48901077"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/447#discussion_r48901077"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/447"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/51383322",
        "pull_request_review_id": null,
        "id": 51383322,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzgzMzIy",
        "diff_hunk": "@@ -0,0 +1,16 @@\n+#!/usr/bin/env python\n+# partition_integers -- Partitions integer range into even ranges\n+##\n+import sys\n+\n+M = int(sys.argv[1])\n+N = int(sys.argv[2])\n+\n+k = float(M)/N",
        "path": "util/partition_integers",
        "position": null,
        "original_position": 9,
        "commit_id": "afeccb4a7fe31c5ef486b43a8004e220c1208faf",
        "original_commit_id": "0987edfc44ff4eb849255747c815c155ee9f3f88",
        "user": {
            "login": "SenWu",
            "id": 5580008,
            "node_id": "MDQ6VXNlcjU1ODAwMDg=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5580008?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/SenWu",
            "html_url": "https://github.com/SenWu",
            "followers_url": "https://api.github.com/users/SenWu/followers",
            "following_url": "https://api.github.com/users/SenWu/following{/other_user}",
            "gists_url": "https://api.github.com/users/SenWu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/SenWu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/SenWu/subscriptions",
            "organizations_url": "https://api.github.com/users/SenWu/orgs",
            "repos_url": "https://api.github.com/users/SenWu/repos",
            "events_url": "https://api.github.com/users/SenWu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/SenWu/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Can we add a check for N here? E.g. N=0.\nAlso, if M=10 and N=20, the answer is\n1 0 0\n0 1 0\n1 1 1\n0 2 1\n1 2 2\n0 3 2\n1 3 3\n0 4 3\n1 4 4\n0 5 4\nIs this answer correct?\n",
        "created_at": "2016-02-01T06:31:58Z",
        "updated_at": "2016-02-01T09:23:06Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/473#discussion_r51383322",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/473",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/51383322"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/473#discussion_r51383322"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/473"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/51389627",
        "pull_request_review_id": null,
        "id": 51389627,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzg5NjI3",
        "diff_hunk": "@@ -0,0 +1,16 @@\n+#!/usr/bin/env python\n+# partition_integers -- Partitions integer range into even ranges\n+##\n+import sys\n+\n+M = int(sys.argv[1])\n+N = int(sys.argv[2])\n+\n+k = float(M)/N",
        "path": "util/partition_integers",
        "position": null,
        "original_position": 9,
        "commit_id": "afeccb4a7fe31c5ef486b43a8004e220c1208faf",
        "original_commit_id": "0987edfc44ff4eb849255747c815c155ee9f3f88",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Thanks for catching this.  I wasn't careful enough to include unit tests for this.  I'll make sure it produces error on such case.  Yes if we want more partitions than the number of elements, it'll end up with something like that.  I think it'll produce the a correct partition that covers all elements, right?\n",
        "created_at": "2016-02-01T08:22:29Z",
        "updated_at": "2016-02-01T09:23:06Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/473#discussion_r51389627",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/473",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/51389627"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/473#discussion_r51389627"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/473"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/51396842",
        "pull_request_review_id": null,
        "id": 51396842,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzk2ODQy",
        "diff_hunk": "@@ -0,0 +1,16 @@\n+#!/usr/bin/env python\n+# partition_integers -- Partitions integer range into even ranges\n+##\n+import sys\n+\n+M = int(sys.argv[1])\n+N = int(sys.argv[2])\n+\n+k = float(M)/N",
        "path": "util/partition_integers",
        "position": null,
        "original_position": 9,
        "commit_id": "afeccb4a7fe31c5ef486b43a8004e220c1208faf",
        "original_commit_id": "0987edfc44ff4eb849255747c815c155ee9f3f88",
        "user": {
            "login": "SenWu",
            "id": 5580008,
            "node_id": "MDQ6VXNlcjU1ODAwMDg=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5580008?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/SenWu",
            "html_url": "https://github.com/SenWu",
            "followers_url": "https://api.github.com/users/SenWu/followers",
            "following_url": "https://api.github.com/users/SenWu/following{/other_user}",
            "gists_url": "https://api.github.com/users/SenWu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/SenWu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/SenWu/subscriptions",
            "organizations_url": "https://api.github.com/users/SenWu/orgs",
            "repos_url": "https://api.github.com/users/SenWu/repos",
            "events_url": "https://api.github.com/users/SenWu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/SenWu/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Yes, it will produce the correct answer. Could it better to set N as M is N>M?\n\nOn Feb 1, 2016, at 12:22 AM, Jaeho Shin <notifications@github.com<mailto:notifications@github.com>> wrote:\n\nIn util/partition_integershttps://github.com/HazyResearch/deepdive/pull/473#discussion_r51389627:\n\n> @@ -0,0 +1,16 @@\n> +#!/usr/bin/env python\n> +# partition_integers -- Partitions integer range into even ranges\n> +##\n> +import sys\n> +\n> +M = int(sys.argv[1])\n> +N = int(sys.argv[2])\n> +\n> +k = float(M)/N\n\nThanks for catching this. I wasn't careful enough to include unit tests for this. I'll make sure it produces error on such case. Yes if we want more partitions than the number of elements, it'll end up with something like that. I think it'll produce the a correct partition that covers all elements, right?\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/HazyResearch/deepdive/pull/473/files#r51389627.\n",
        "created_at": "2016-02-01T09:45:54Z",
        "updated_at": "2016-02-01T09:45:54Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/473#discussion_r51396842",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/473",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/51396842"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/473#discussion_r51396842"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/473"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/51378799",
        "pull_request_review_id": null,
        "id": 51378799,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzc4Nzk5",
        "diff_hunk": "@@ -12,7 +12,7 @@ deepdive {\n   }\n \n   sampler.sampler_cmd: \"sampler-dw\"\n-  sampler.sampler_args: \"-l 1000 -s 1 -i 1000 --alpha 0.01\"\n+  sampler.sampler_args: \"-l 1000 -s 1 -i 1000 --alpha 0.01 --sample_evidence\"",
        "path": "compiler/deepdive-default.conf",
        "position": 5,
        "original_position": 5,
        "commit_id": "e8b3c4c0507f92105166d8d5f52343759d903a92",
        "original_commit_id": "84d95f56f15ffb231816b1840b10911503e331cf",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Seems some different voices there #416. So we're decided to set it as default?\n",
        "created_at": "2016-02-01T04:37:14Z",
        "updated_at": "2016-02-01T19:35:30Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/475#discussion_r51378799",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/475",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/51378799"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/475#discussion_r51378799"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/475"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/51379091",
        "pull_request_review_id": null,
        "id": 51379091,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzc5MDkx",
        "diff_hunk": "@@ -0,0 +1,243 @@\n+## Random variable to predict #################################################\n+\n+# This application's goal is to predict whether a given pair of person mention\n+# are indicating a spouse relationship or not.\n+@extraction\n+has_spouse?(\n+    @key\n+    @references(relation=\"person_mention\", column=\"mention_id\", alias=\"p1\")\n+    p1_id text,\n+    @key\n+    @references(relation=\"person_mention\", column=\"mention_id\", alias=\"p2\")\n+    p2_id text\n+).\n+\n+## Input Data #################################################################\n+\n+# example DeepDive application for finding spouse relationships in news articles\n+@source\n+articles(\n+    @key\n+    @distributed_by\n+    id text,\n+    @searchable\n+    content text\n+    # TODO more fields if needed\n+).\n+\n+@source\n+spouses_dbpedia(\n+    @key\n+    person1_name text,\n+    @key\n+    person2_name text\n+).\n+\n+## NLP markup #################################################################\n+@source\n+sentences(\n+    @key\n+    @distributed_by\n+    #@references(relation=\"articles\", column=\"id\")\n+    doc_id         text,\n+    @key\n+    sentence_index int,\n+    @searchable\n+    sentence_text  text,\n+    tokens         text[],\n+    lemmas         text[],\n+    pos_tags       text[],\n+    ner_tags       text[],\n+    doc_offsets    int[],\n+    dep_types      text[],\n+    dep_tokens     int[]\n+).\n+\n+function nlp_markup over (\n+        doc_id text,\n+        content text\n+    ) returns rows like sentences\n+    implementation \"udf/nlp_markup.sh\" handles tsv lines.\n+\n+sentences +=\n+  nlp_markup(doc_id, content) :-\n+  articles(doc_id, content).\n+\n+\n+## Candidate mapping ##########################################################\n+@extraction\n+person_mention(\n+    @key\n+    mention_id text,\n+    @searchable\n+    mention_text text,\n+    @distributed_by\n+    @references(relation=\"sentences\", column=\"doc_id\",         alias=\"appears_in\")\n+    doc_id text,\n+    @references(relation=\"sentences\", column=\"sentence_index\", alias=\"appears_in\")\n+    sentence_index int,\n+    begin_index int,\n+    end_index int\n+).\n+\n+function map_person_mention over (\n+        doc_id text,\n+        sentence_index int,\n+        tokens text[],\n+        ner_tags text[]\n+    ) returns rows like person_mention\n+    implementation \"udf/map_person_mention.py\" handles tsv lines.\n+\n+person_mention += map_person_mention(\n+    doc_id, sentence_index, tokens, ner_tags\n+) :- sentences(doc_id, sentence_index, _, tokens, _, _, ner_tags, _, _, _).\n+\n+spouse_candidate(\n+    # TODO Here, ideally ddlog should allow us to write something like:\n+    #   p1 person_mention, p2 person_mention\n+    # then expand them into their multi-@key columns by itself.\n+    p1_id text,\n+    p1_name text,\n+    p2_id text,\n+    p2_name text\n+).\n+\n+num_people(doc_id, sentence_index, COUNT(p)) :-\n+    person_mention(p, _, doc_id, sentence_index, _, _).\n+\n+spouse_candidate(p1, p1_name, p2, p2_name) :-\n+    num_people(same_doc, same_sentence, num_p),\n+    person_mention(p1, p1_name, same_doc, same_sentence, p1_begin, _),\n+    person_mention(p2, p2_name, same_doc, same_sentence, p2_begin, _),\n+    num_p < 5,\n+    p1_name != p2_name,\n+    p1_begin != p2_begin.\n+\n+\n+## Feature Extraction #########################################################\n+\n+# Feature extraction (using DDLIB via a UDF) at the relation level\n+# TODO: add @extraction flag here?\n+@extraction\n+spouse_feature(\n+    @key\n+    @references(relation=\"has_spouse\", column=\"p1_id\", alias=\"has_spouse\")\n+    p1_id text,\n+    @key\n+    @references(relation=\"has_spouse\", column=\"p2_id\", alias=\"has_spouse\")\n+    p2_id text,\n+    @key\n+    feature text\n+).\n+\n+function extract_spouse_features over (\n+        p1_id text,\n+        p2_id text,\n+        p1_begin_index int,\n+        p1_end_index int,\n+        p2_begin_index int,\n+        p2_end_index int,\n+        doc_id text,\n+        sent_index int,\n+        tokens text[],\n+        lemmas text[],\n+        pos_tags text[],\n+        ner_tags text[],\n+        dep_types text[],\n+        dep_tokens int[]\n+    ) returns rows like spouse_feature\n+    implementation \"udf/extract_spouse_features.py\" handles tsv lines.\n+\n+spouse_feature += extract_spouse_features(\n+  p1_id, p2_id, p1_begin_index, p1_end_index, p2_begin_index, p2_end_index,\n+  doc_id, sent_index, tokens, lemmas, pos_tags, ner_tags, dep_types, dep_tokens) :-\n+  person_mention(p1_id, _, doc_id, sent_index, p1_begin_index, p1_end_index),\n+  person_mention(p2_id, _, doc_id, sent_index, p2_begin_index, p2_end_index),\n+  sentences(doc_id, sent_index, _, tokens, lemmas, pos_tags, ner_tags, _, dep_types, dep_tokens\n+).\n+\n+\n+## Distant Supervision ########################################################\n+@extraction\n+spouse_label(\n+    @key\n+    @references(relation=\"has_spouse\", column=\"p1_id\", alias=\"has_spouse\")\n+    p1_id text,\n+    @key\n+    @references(relation=\"has_spouse\", column=\"p2_id\", alias=\"has_spouse\")\n+    p2_id text,\n+    @navigable\n+    label int,\n+    @navigable\n+    rule_id text\n+).\n+\n+# make sure all pairs in spouse_candidate are considered as unsupervised examples\n+spouse_label(p1, p2) = NULL :- spouse_candidate(p1, _, p2, _).\n+\n+# distant supervision using data from DBpedia\n+spouse_label(p1,p2, 1, \"from_dbpedia\") :-\n+  spouse_candidate(p1, p1_name, p2, p2_name), spouses_dbpedia(n1, n2),\n+  [ lower(n1) = lower(p1_name), lower(n2) = lower(p2_name) ;\n+    lower(n2) = lower(p1_name), lower(n1) = lower(p2_name) ].\n+\n+# supervision by heuristic rules in a UDF\n+function supervise over (\n+        p1_id text, p1_begin int, p1_end int,\n+        p2_id text, p2_begin int, p2_end int,\n+        doc_id         text,\n+        sentence_index int,\n+        sentence_text  text,\n+        tokens         text[],\n+        lemmas         text[],\n+        pos_tags       text[],\n+        ner_tags       text[],\n+        dep_types      text[],\n+        dep_tokens    int[]\n+    ) returns (\n+        p1_id text, p2_id text, label int, rule_id text\n+    )\n+    implementation \"udf/supervise_spouse.py\" handles tsv lines.\n+\n+spouse_label += supervise(\n+    p1_id, p1_begin, p1_end,\n+    p2_id, p2_begin, p2_end,\n+    doc_id, sentence_index, sentence_text,\n+    tokens, lemmas, pos_tags, ner_tags, dep_types, dep_token_indexes\n+) :- spouse_candidate(p1_id, _, p2_id, _),\n+    person_mention(p1_id, p1_text, doc_id, sentence_index, p1_begin, p1_end),\n+    person_mention(p2_id, p2_text,      _,              _, p2_begin, p2_end),\n+    sentences(\n+        doc_id, sentence_index, sentence_text,\n+        tokens, lemmas, pos_tags, ner_tags, _, dep_types, dep_token_indexes\n+    ).\n+\n+\n+# resolve multiple labels by majority vote (summing the labels in {-1,0,1})\n+spouse_label_resolved(p1_id, p2_id, SUM(vote)) :- spouse_label(p1_id, p2_id, vote, rule_id).\n+\n+# assign the resolved labels for the spouse relation\n+has_spouse(p1_id, p2_id) = if l > 0 then TRUE\n+                      else if l < 0 then FALSE\n+                      else NULL end :- spouse_label_resolved(p1_id, p2_id, l).\n+\n+###############################################################################\n+\n+## Inference Rules ############################################################\n+\n+# Features\n+@weight(f)\n+has_spouse(p1_id, p2_id) :-\n+  spouse_candidate(p1_id, _, p2_id, _),\n+  spouse_feature(p1_id, p2_id, f).\n+\n+# Inference rule: Symmetry\n+@weight(3.0)\n+has_spouse(p1_id, p2_id) => has_spouse(p2_id, p1_id) :-",
        "path": "examples/spouse/app.ddlog",
        "position": 236,
        "original_position": 236,
        "commit_id": "e8b3c4c0507f92105166d8d5f52343759d903a92",
        "original_commit_id": "84d95f56f15ffb231816b1840b10911503e331cf",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Should this be equal factor?\n",
        "created_at": "2016-02-01T04:46:45Z",
        "updated_at": "2016-02-01T19:35:30Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/475#discussion_r51379091",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/475",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/51379091"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/475#discussion_r51379091"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/475"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/51379176",
        "pull_request_review_id": null,
        "id": 51379176,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzc5MTc2",
        "diff_hunk": "@@ -0,0 +1,2 @@\n+deepdive.calibration.holdout_fraction:0.6",
        "path": "examples/spouse/deepdive.conf",
        "position": 1,
        "original_position": 1,
        "commit_id": "e8b3c4c0507f92105166d8d5f52343759d903a92",
        "original_commit_id": "84d95f56f15ffb231816b1840b10911503e331cf",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "60% for calibration is a bit uncommon. Just to double check, is this intended?\n",
        "created_at": "2016-02-01T04:49:50Z",
        "updated_at": "2016-02-01T19:35:30Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/475#discussion_r51379176",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/475",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/51379176"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/475#discussion_r51379176"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/475"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/51379230",
        "pull_request_review_id": null,
        "id": 51379230,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzc5MjMw",
        "diff_hunk": "@@ -0,0 +1,8 @@\n+#!/usr/bin/env bash\n+set -euo pipefail\n+cd \"$(dirname \"$0\")\"\n+\n+cat /dfs/scratch0/thodrek/signalmedia/signalmedia-1m.jsonl |",
        "path": "examples/spouse/input/articles.tsv.sh",
        "position": 5,
        "original_position": 5,
        "commit_id": "e8b3c4c0507f92105166d8d5f52343759d903a92",
        "original_commit_id": "84d95f56f15ffb231816b1840b10911503e331cf",
        "user": {
            "login": "feiranwang",
            "id": 5935656,
            "node_id": "MDQ6VXNlcjU5MzU2NTY=",
            "avatar_url": "https://avatars3.githubusercontent.com/u/5935656?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/feiranwang",
            "html_url": "https://github.com/feiranwang",
            "followers_url": "https://api.github.com/users/feiranwang/followers",
            "following_url": "https://api.github.com/users/feiranwang/following{/other_user}",
            "gists_url": "https://api.github.com/users/feiranwang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/feiranwang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/feiranwang/subscriptions",
            "organizations_url": "https://api.github.com/users/feiranwang/orgs",
            "repos_url": "https://api.github.com/users/feiranwang/repos",
            "events_url": "https://api.github.com/users/feiranwang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/feiranwang/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Hardcoded path here.\n",
        "created_at": "2016-02-01T04:51:30Z",
        "updated_at": "2016-02-01T19:35:30Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/475#discussion_r51379230",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/475",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/51379230"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/475#discussion_r51379230"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/475"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/51379711",
        "pull_request_review_id": null,
        "id": 51379711,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzc5NzEx",
        "diff_hunk": "@@ -12,7 +12,7 @@ deepdive {\n   }\n \n   sampler.sampler_cmd: \"sampler-dw\"\n-  sampler.sampler_args: \"-l 1000 -s 1 -i 1000 --alpha 0.01\"\n+  sampler.sampler_args: \"-l 1000 -s 1 -i 1000 --alpha 0.01 --sample_evidence\"",
        "path": "compiler/deepdive-default.conf",
        "position": 5,
        "original_position": 5,
        "commit_id": "e8b3c4c0507f92105166d8d5f52343759d903a92",
        "original_commit_id": "84d95f56f15ffb231816b1840b10911503e331cf",
        "user": {
            "login": "thodrek",
            "id": 1043311,
            "node_id": "MDQ6VXNlcjEwNDMzMTE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1043311?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/thodrek",
            "html_url": "https://github.com/thodrek",
            "followers_url": "https://api.github.com/users/thodrek/followers",
            "following_url": "https://api.github.com/users/thodrek/following{/other_user}",
            "gists_url": "https://api.github.com/users/thodrek/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/thodrek/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/thodrek/subscriptions",
            "organizations_url": "https://api.github.com/users/thodrek/orgs",
            "repos_url": "https://api.github.com/users/thodrek/repos",
            "events_url": "https://api.github.com/users/thodrek/events{/privacy}",
            "received_events_url": "https://api.github.com/users/thodrek/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "We are using it here because of the noisy rules. To be honest I don't think we should have it on by default but that's just my opinion.\n",
        "created_at": "2016-02-01T05:05:48Z",
        "updated_at": "2016-02-01T19:35:30Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/475#discussion_r51379711",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/475",
        "author_association": "NONE",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/51379711"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/475#discussion_r51379711"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/475"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/51379779",
        "pull_request_review_id": null,
        "id": 51379779,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzc5Nzc5",
        "diff_hunk": "@@ -0,0 +1,2 @@\n+deepdive.calibration.holdout_fraction:0.6",
        "path": "examples/spouse/deepdive.conf",
        "position": 1,
        "original_position": 1,
        "commit_id": "e8b3c4c0507f92105166d8d5f52343759d903a92",
        "original_commit_id": "84d95f56f15ffb231816b1840b10911503e331cf",
        "user": {
            "login": "thodrek",
            "id": 1043311,
            "node_id": "MDQ6VXNlcjEwNDMzMTE=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/1043311?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/thodrek",
            "html_url": "https://github.com/thodrek",
            "followers_url": "https://api.github.com/users/thodrek/followers",
            "following_url": "https://api.github.com/users/thodrek/following{/other_user}",
            "gists_url": "https://api.github.com/users/thodrek/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/thodrek/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/thodrek/subscriptions",
            "organizations_url": "https://api.github.com/users/thodrek/orgs",
            "repos_url": "https://api.github.com/users/thodrek/repos",
            "events_url": "https://api.github.com/users/thodrek/events{/privacy}",
            "received_events_url": "https://api.github.com/users/thodrek/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "We started by testing a very small corpus and had that so that we could have a sufficient number of \"spouse\" instances in the test data. This was without using --sample_evidence.\n",
        "created_at": "2016-02-01T05:07:53Z",
        "updated_at": "2016-02-01T19:35:30Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/475#discussion_r51379779",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/475",
        "author_association": "NONE",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/51379779"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/475#discussion_r51379779"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/475"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/51387235",
        "pull_request_review_id": null,
        "id": 51387235,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzg3MjM1",
        "diff_hunk": "@@ -12,7 +12,7 @@ deepdive {\n   }\n \n   sampler.sampler_cmd: \"sampler-dw\"\n-  sampler.sampler_args: \"-l 1000 -s 1 -i 1000 --alpha 0.01\"\n+  sampler.sampler_args: \"-l 1000 -s 1 -i 1000 --alpha 0.01 --sample_evidence\"",
        "path": "compiler/deepdive-default.conf",
        "position": 5,
        "original_position": 5,
        "commit_id": "e8b3c4c0507f92105166d8d5f52343759d903a92",
        "original_commit_id": "84d95f56f15ffb231816b1840b10911503e331cf",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "@feiranwang Good catch!  @thodrek This is actually the default settings we updated, not just our example.\n\nWe've seen too many people bitten by this default setting, and enough is enough.  Whoever that's rigorous enough wanting to exactly evaluate the system's performance won't be misled by the extra inference results.  So I insist on turning this option on by default.  However, I'm all for adding a clear way to prevent misinterpretation, so let's discuss what can be the best additional output, e.g., what extra view can we create and/or should we compute two numbers that excludes/includes holdout?\n",
        "created_at": "2016-02-01T07:40:37Z",
        "updated_at": "2016-02-01T19:35:30Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/475#discussion_r51387235",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/475",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/51387235"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/475#discussion_r51387235"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/475"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/51387351",
        "pull_request_review_id": null,
        "id": 51387351,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzg3MzUx",
        "diff_hunk": "@@ -0,0 +1,243 @@\n+## Random variable to predict #################################################\n+\n+# This application's goal is to predict whether a given pair of person mention\n+# are indicating a spouse relationship or not.\n+@extraction\n+has_spouse?(\n+    @key\n+    @references(relation=\"person_mention\", column=\"mention_id\", alias=\"p1\")\n+    p1_id text,\n+    @key\n+    @references(relation=\"person_mention\", column=\"mention_id\", alias=\"p2\")\n+    p2_id text\n+).\n+\n+## Input Data #################################################################\n+\n+# example DeepDive application for finding spouse relationships in news articles\n+@source\n+articles(\n+    @key\n+    @distributed_by\n+    id text,\n+    @searchable\n+    content text\n+    # TODO more fields if needed\n+).\n+\n+@source\n+spouses_dbpedia(\n+    @key\n+    person1_name text,\n+    @key\n+    person2_name text\n+).\n+\n+## NLP markup #################################################################\n+@source\n+sentences(\n+    @key\n+    @distributed_by\n+    #@references(relation=\"articles\", column=\"id\")\n+    doc_id         text,\n+    @key\n+    sentence_index int,\n+    @searchable\n+    sentence_text  text,\n+    tokens         text[],\n+    lemmas         text[],\n+    pos_tags       text[],\n+    ner_tags       text[],\n+    doc_offsets    int[],\n+    dep_types      text[],\n+    dep_tokens     int[]\n+).\n+\n+function nlp_markup over (\n+        doc_id text,\n+        content text\n+    ) returns rows like sentences\n+    implementation \"udf/nlp_markup.sh\" handles tsv lines.\n+\n+sentences +=\n+  nlp_markup(doc_id, content) :-\n+  articles(doc_id, content).\n+\n+\n+## Candidate mapping ##########################################################\n+@extraction\n+person_mention(\n+    @key\n+    mention_id text,\n+    @searchable\n+    mention_text text,\n+    @distributed_by\n+    @references(relation=\"sentences\", column=\"doc_id\",         alias=\"appears_in\")\n+    doc_id text,\n+    @references(relation=\"sentences\", column=\"sentence_index\", alias=\"appears_in\")\n+    sentence_index int,\n+    begin_index int,\n+    end_index int\n+).\n+\n+function map_person_mention over (\n+        doc_id text,\n+        sentence_index int,\n+        tokens text[],\n+        ner_tags text[]\n+    ) returns rows like person_mention\n+    implementation \"udf/map_person_mention.py\" handles tsv lines.\n+\n+person_mention += map_person_mention(\n+    doc_id, sentence_index, tokens, ner_tags\n+) :- sentences(doc_id, sentence_index, _, tokens, _, _, ner_tags, _, _, _).\n+\n+spouse_candidate(\n+    # TODO Here, ideally ddlog should allow us to write something like:\n+    #   p1 person_mention, p2 person_mention\n+    # then expand them into their multi-@key columns by itself.\n+    p1_id text,\n+    p1_name text,\n+    p2_id text,\n+    p2_name text\n+).\n+\n+num_people(doc_id, sentence_index, COUNT(p)) :-\n+    person_mention(p, _, doc_id, sentence_index, _, _).\n+\n+spouse_candidate(p1, p1_name, p2, p2_name) :-\n+    num_people(same_doc, same_sentence, num_p),\n+    person_mention(p1, p1_name, same_doc, same_sentence, p1_begin, _),\n+    person_mention(p2, p2_name, same_doc, same_sentence, p2_begin, _),\n+    num_p < 5,\n+    p1_name != p2_name,\n+    p1_begin != p2_begin.\n+\n+\n+## Feature Extraction #########################################################\n+\n+# Feature extraction (using DDLIB via a UDF) at the relation level\n+# TODO: add @extraction flag here?\n+@extraction\n+spouse_feature(\n+    @key\n+    @references(relation=\"has_spouse\", column=\"p1_id\", alias=\"has_spouse\")\n+    p1_id text,\n+    @key\n+    @references(relation=\"has_spouse\", column=\"p2_id\", alias=\"has_spouse\")\n+    p2_id text,\n+    @key\n+    feature text\n+).\n+\n+function extract_spouse_features over (\n+        p1_id text,\n+        p2_id text,\n+        p1_begin_index int,\n+        p1_end_index int,\n+        p2_begin_index int,\n+        p2_end_index int,\n+        doc_id text,\n+        sent_index int,\n+        tokens text[],\n+        lemmas text[],\n+        pos_tags text[],\n+        ner_tags text[],\n+        dep_types text[],\n+        dep_tokens int[]\n+    ) returns rows like spouse_feature\n+    implementation \"udf/extract_spouse_features.py\" handles tsv lines.\n+\n+spouse_feature += extract_spouse_features(\n+  p1_id, p2_id, p1_begin_index, p1_end_index, p2_begin_index, p2_end_index,\n+  doc_id, sent_index, tokens, lemmas, pos_tags, ner_tags, dep_types, dep_tokens) :-\n+  person_mention(p1_id, _, doc_id, sent_index, p1_begin_index, p1_end_index),\n+  person_mention(p2_id, _, doc_id, sent_index, p2_begin_index, p2_end_index),\n+  sentences(doc_id, sent_index, _, tokens, lemmas, pos_tags, ner_tags, _, dep_types, dep_tokens\n+).\n+\n+\n+## Distant Supervision ########################################################\n+@extraction\n+spouse_label(\n+    @key\n+    @references(relation=\"has_spouse\", column=\"p1_id\", alias=\"has_spouse\")\n+    p1_id text,\n+    @key\n+    @references(relation=\"has_spouse\", column=\"p2_id\", alias=\"has_spouse\")\n+    p2_id text,\n+    @navigable\n+    label int,\n+    @navigable\n+    rule_id text\n+).\n+\n+# make sure all pairs in spouse_candidate are considered as unsupervised examples\n+spouse_label(p1, p2) = NULL :- spouse_candidate(p1, _, p2, _).\n+\n+# distant supervision using data from DBpedia\n+spouse_label(p1,p2, 1, \"from_dbpedia\") :-\n+  spouse_candidate(p1, p1_name, p2, p2_name), spouses_dbpedia(n1, n2),\n+  [ lower(n1) = lower(p1_name), lower(n2) = lower(p2_name) ;\n+    lower(n2) = lower(p1_name), lower(n1) = lower(p2_name) ].\n+\n+# supervision by heuristic rules in a UDF\n+function supervise over (\n+        p1_id text, p1_begin int, p1_end int,\n+        p2_id text, p2_begin int, p2_end int,\n+        doc_id         text,\n+        sentence_index int,\n+        sentence_text  text,\n+        tokens         text[],\n+        lemmas         text[],\n+        pos_tags       text[],\n+        ner_tags       text[],\n+        dep_types      text[],\n+        dep_tokens    int[]\n+    ) returns (\n+        p1_id text, p2_id text, label int, rule_id text\n+    )\n+    implementation \"udf/supervise_spouse.py\" handles tsv lines.\n+\n+spouse_label += supervise(\n+    p1_id, p1_begin, p1_end,\n+    p2_id, p2_begin, p2_end,\n+    doc_id, sentence_index, sentence_text,\n+    tokens, lemmas, pos_tags, ner_tags, dep_types, dep_token_indexes\n+) :- spouse_candidate(p1_id, _, p2_id, _),\n+    person_mention(p1_id, p1_text, doc_id, sentence_index, p1_begin, p1_end),\n+    person_mention(p2_id, p2_text,      _,              _, p2_begin, p2_end),\n+    sentences(\n+        doc_id, sentence_index, sentence_text,\n+        tokens, lemmas, pos_tags, ner_tags, _, dep_types, dep_token_indexes\n+    ).\n+\n+\n+# resolve multiple labels by majority vote (summing the labels in {-1,0,1})\n+spouse_label_resolved(p1_id, p2_id, SUM(vote)) :- spouse_label(p1_id, p2_id, vote, rule_id).\n+\n+# assign the resolved labels for the spouse relation\n+has_spouse(p1_id, p2_id) = if l > 0 then TRUE\n+                      else if l < 0 then FALSE\n+                      else NULL end :- spouse_label_resolved(p1_id, p2_id, l).\n+\n+###############################################################################\n+\n+## Inference Rules ############################################################\n+\n+# Features\n+@weight(f)\n+has_spouse(p1_id, p2_id) :-\n+  spouse_candidate(p1_id, _, p2_id, _),\n+  spouse_feature(p1_id, p2_id, f).\n+\n+# Inference rule: Symmetry\n+@weight(3.0)\n+has_spouse(p1_id, p2_id) => has_spouse(p2_id, p1_id) :-",
        "path": "examples/spouse/app.ddlog",
        "position": 236,
        "original_position": 236,
        "commit_id": "e8b3c4c0507f92105166d8d5f52343759d903a92",
        "original_commit_id": "84d95f56f15ffb231816b1840b10911503e331cf",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I agree.  I think the only reason implication appeared here is because the other example we looked at was written before we added the Equals-factor syntax.\n",
        "created_at": "2016-02-01T07:42:36Z",
        "updated_at": "2016-02-01T19:35:30Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/475#discussion_r51387351",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/475",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/51387351"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/475#discussion_r51387351"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/475"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/51387506",
        "pull_request_review_id": null,
        "id": 51387506,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzg3NTA2",
        "diff_hunk": "@@ -0,0 +1,8 @@\n+#!/usr/bin/env bash\n+set -euo pipefail\n+cd \"$(dirname \"$0\")\"\n+\n+cat /dfs/scratch0/thodrek/signalmedia/signalmedia-1m.jsonl |",
        "path": "examples/spouse/input/articles.tsv.sh",
        "position": 5,
        "original_position": 5,
        "commit_id": "e8b3c4c0507f92105166d8d5f52343759d903a92",
        "original_commit_id": "84d95f56f15ffb231816b1840b10911503e331cf",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Right.  We should change this to grab from the same dir (`input/`) and put instructions to download the full corpus (people have to go through license agreement and get an email with download link btw).\n\nWe'll grab a subset of articles, get NLP markups and include the sentences.tsv.bz2 instead for running our integration tests.\n",
        "created_at": "2016-02-01T07:44:51Z",
        "updated_at": "2016-02-01T19:35:30Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/475#discussion_r51387506",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/475",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/51387506"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/475#discussion_r51387506"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/475"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/52405088",
        "pull_request_review_id": null,
        "id": 52405088,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDA1MDg4",
        "diff_hunk": "@@ -57,7 +57,7 @@ DeepDive makes good use of uncertainty to improve predictions during the\n [probabilistic inference](inference.md) step. For example, DeepDive may figure\n out that a certain mention of \"Barack\" is only 60% likely to actually refer to\n \"Barack Obama\", and use this fact to discount the impact of that mention on the\n-final result for the entity \"Barack Obama\". DeepDive can also make use the\n-domain knowledge, and allows users to encode rules such as \"If Barack is married\n+final result for the entity \"Barack Obama\". DeepDive can also make use of ",
        "path": "doc/relation_extraction.md",
        "position": 65,
        "original_position": 65,
        "commit_id": "3495a5c17822035121265a597693f11f489a0599",
        "original_commit_id": "3495a5c17822035121265a597693f11f489a0599",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "probably the space at the end of this line\n",
        "created_at": "2016-02-10T01:43:23Z",
        "updated_at": "2016-02-10T01:43:23Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/484#discussion_r52405088",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/484",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/52405088"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/484#discussion_r52405088"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/484"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/52405247",
        "pull_request_review_id": null,
        "id": 52405247,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDA1MjQ3",
        "diff_hunk": "@@ -57,7 +57,7 @@ DeepDive makes good use of uncertainty to improve predictions during the\n [probabilistic inference](inference.md) step. For example, DeepDive may figure\n out that a certain mention of \"Barack\" is only 60% likely to actually refer to\n \"Barack Obama\", and use this fact to discount the impact of that mention on the\n-final result for the entity \"Barack Obama\". DeepDive can also make use the\n-domain knowledge, and allows users to encode rules such as \"If Barack is married\n+final result for the entity \"Barack Obama\". DeepDive can also make use of ",
        "path": "doc/relation_extraction.md",
        "position": 65,
        "original_position": 65,
        "commit_id": "3495a5c17822035121265a597693f11f489a0599",
        "original_commit_id": "3495a5c17822035121265a597693f11f489a0599",
        "user": {
            "login": "bhancock8",
            "id": 12646092,
            "node_id": "MDQ6VXNlcjEyNjQ2MDky",
            "avatar_url": "https://avatars2.githubusercontent.com/u/12646092?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/bhancock8",
            "html_url": "https://github.com/bhancock8",
            "followers_url": "https://api.github.com/users/bhancock8/followers",
            "following_url": "https://api.github.com/users/bhancock8/following{/other_user}",
            "gists_url": "https://api.github.com/users/bhancock8/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/bhancock8/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/bhancock8/subscriptions",
            "organizations_url": "https://api.github.com/users/bhancock8/orgs",
            "repos_url": "https://api.github.com/users/bhancock8/repos",
            "events_url": "https://api.github.com/users/bhancock8/events{/privacy}",
            "received_events_url": "https://api.github.com/users/bhancock8/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Hey Jaeho,\n\nYeah, sorry. I'm still trying to figure out how to make edits properly. I\nwas doing them all inside git's UI; think I'll just move to the command\nline in the future. I saw that two didn't pass Travis, and I figured it was\nbecause of trailing white space, so I closed them. I'll go in and get rid\nof the whitespace offenders and resubmit the pull request. Thanks!\n\nOn Tue, Feb 9, 2016 at 5:43 PM, Jaeho Shin notifications@github.com wrote:\n\n> In doc/relation_extraction.md\n> https://github.com/HazyResearch/deepdive/pull/484#discussion_r52405088:\n> \n> > @@ -57,7 +57,7 @@ DeepDive makes good use of uncertainty to improve predictions during the\n> >  [probabilistic inference](inference.md) step. For example, DeepDive may figure\n> >  out that a certain mention of \"Barack\" is only 60% likely to actually refer to\n> >  \"Barack Obama\", and use this fact to discount the impact of that mention on the\n> > -final result for the entity \"Barack Obama\". DeepDive can also make use the\n> > -domain knowledge, and allows users to encode rules such as \"If Barack is married\n> > +final result for the entity \"Barack Obama\". DeepDive can also make use of\n> \n> probably the space at the end of this line\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/HazyResearch/deepdive/pull/484/files#r52405088.\n",
        "created_at": "2016-02-10T01:45:47Z",
        "updated_at": "2016-02-10T01:45:47Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/484#discussion_r52405247",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/484",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/52405247"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/484#discussion_r52405247"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/484"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/52405543",
        "pull_request_review_id": null,
        "id": 52405543,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDA1NTQz",
        "diff_hunk": "@@ -57,7 +57,7 @@ DeepDive makes good use of uncertainty to improve predictions during the\n [probabilistic inference](inference.md) step. For example, DeepDive may figure\n out that a certain mention of \"Barack\" is only 60% likely to actually refer to\n \"Barack Obama\", and use this fact to discount the impact of that mention on the\n-final result for the entity \"Barack Obama\". DeepDive can also make use the\n-domain knowledge, and allows users to encode rules such as \"If Barack is married\n+final result for the entity \"Barack Obama\". DeepDive can also make use of ",
        "path": "doc/relation_extraction.md",
        "position": 65,
        "original_position": 65,
        "commit_id": "3495a5c17822035121265a597693f11f489a0599",
        "original_commit_id": "3495a5c17822035121265a597693f11f489a0599",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "No worries.  Using GitHub UI is fine, and the actual edits are more important than passing any of those silly checks.  They are just safe guards to keep the code in a clean shape easy for merging.  I think you can simply open the same file from your branch and modify inplace.  That'll probably update this PR and give us a green light.\n",
        "created_at": "2016-02-10T01:50:21Z",
        "updated_at": "2016-02-10T01:50:21Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/484#discussion_r52405543",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/484",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/52405543"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/484#discussion_r52405543"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/484"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/52406504",
        "pull_request_review_id": null,
        "id": 52406504,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDA2NTA0",
        "diff_hunk": "@@ -57,7 +57,7 @@ DeepDive makes good use of uncertainty to improve predictions during the\n [probabilistic inference](inference.md) step. For example, DeepDive may figure\n out that a certain mention of \"Barack\" is only 60% likely to actually refer to\n \"Barack Obama\", and use this fact to discount the impact of that mention on the\n-final result for the entity \"Barack Obama\". DeepDive can also make use the\n-domain knowledge, and allows users to encode rules such as \"If Barack is married\n+final result for the entity \"Barack Obama\". DeepDive can also make use of ",
        "path": "doc/relation_extraction.md",
        "position": 65,
        "original_position": 65,
        "commit_id": "3495a5c17822035121265a597693f11f489a0599",
        "original_commit_id": "3495a5c17822035121265a597693f11f489a0599",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Actually, I just fixed them and merged since I have them handy.  Thanks for your careful reading and editing!\n",
        "created_at": "2016-02-10T02:04:16Z",
        "updated_at": "2016-02-10T02:04:16Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/484#discussion_r52406504",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/484",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/52406504"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/484#discussion_r52406504"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/484"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/52407163",
        "pull_request_review_id": null,
        "id": 52407163,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDA3MTYz",
        "diff_hunk": "@@ -18,13 +24,16 @@ word_features(\n \n tag?(word_id bigint) Categorical(13).\n \n-function ext_training\n-  over rows like words_raw\n-  returns rows like words\n-  implementation \"udf/ext_training.py\" handles tsv lines.\n-\n-words +=\n-  ext_training(wid, word, pos, tag) :- words_raw(wid, word, pos, tag).\n+words(sent_id, word_id, word, pos, true_tag, tag_id) :-\n+  words_raw(sent_id, word_id, word, pos, true_tag),\n+  tags(tag_id, tag),\n+  if true_tag = \"B-UCP\" then \"\"\n+  else if true_tag = \"I-UCP\" then \"\"",
        "path": "examples/chunking/app.ddlog",
        "position": null,
        "original_position": 32,
        "commit_id": "9916610e80ac3900023b06bb34d475deddd9e069",
        "original_commit_id": "6ed0efeb9e0521c0f59d73bfbc0bbbef7fc91440",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "With these two exceptions, did the result become identical to the old example's?\n",
        "created_at": "2016-02-10T02:15:12Z",
        "updated_at": "2016-02-10T02:59:35Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/485#discussion_r52407163",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/485",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/52407163"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/485#discussion_r52407163"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/485"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/52408356",
        "pull_request_review_id": null,
        "id": 52408356,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDA4MzU2",
        "diff_hunk": "@@ -18,13 +24,16 @@ word_features(\n \n tag?(word_id bigint) Categorical(13).\n \n-function ext_training\n-  over rows like words_raw\n-  returns rows like words\n-  implementation \"udf/ext_training.py\" handles tsv lines.\n-\n-words +=\n-  ext_training(wid, word, pos, tag) :- words_raw(wid, word, pos, tag).\n+words(sent_id, word_id, word, pos, true_tag, tag_id) :-\n+  words_raw(sent_id, word_id, word, pos, true_tag),\n+  tags(tag_id, tag),\n+  if true_tag = \"B-UCP\" then \"\"\n+  else if true_tag = \"I-UCP\" then \"\"",
        "path": "examples/chunking/app.ddlog",
        "position": null,
        "original_position": 32,
        "commit_id": "9916610e80ac3900023b06bb34d475deddd9e069",
        "original_commit_id": "6ed0efeb9e0521c0f59d73bfbc0bbbef7fc91440",
        "user": {
            "login": "raphaelhoffmann",
            "id": 991789,
            "node_id": "MDQ6VXNlcjk5MTc4OQ==",
            "avatar_url": "https://avatars2.githubusercontent.com/u/991789?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/raphaelhoffmann",
            "html_url": "https://github.com/raphaelhoffmann",
            "followers_url": "https://api.github.com/users/raphaelhoffmann/followers",
            "following_url": "https://api.github.com/users/raphaelhoffmann/following{/other_user}",
            "gists_url": "https://api.github.com/users/raphaelhoffmann/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/raphaelhoffmann/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/raphaelhoffmann/subscriptions",
            "organizations_url": "https://api.github.com/users/raphaelhoffmann/orgs",
            "repos_url": "https://api.github.com/users/raphaelhoffmann/repos",
            "events_url": "https://api.github.com/users/raphaelhoffmann/events{/privacy}",
            "received_events_url": "https://api.github.com/users/raphaelhoffmann/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Not exactly the same, but close. I think there was already a bug in the old chunking example, where words in one sentence sometimes had different sent_ids (likely because there was no row-order enforced):\n\n```\n sent_id | word_id |   word    | pos | true_tag | tag | id\n---------+---------+-----------+-----+----------+-----+-----\n       5 |     134 | ``        | ``  | O        |   6 | 133\n       5 |     135 | The       | DT  | B-NP     |   0 | 134\n       5 |     136 | risks     | NNS | I-NP     |   0 | 135\n       5 |     137 | for       | IN  | B-PP     |   2 | 136\n       5 |     138 | sterling  | NN  | B-NP     |   0 | 137\n       5 |     139 | of        | IN  | B-PP     |   2 | 138\n       5 |     140 | a         | DT  | B-NP     |   0 | 139\n       5 |     141 | bad       | JJ  | I-NP     |   0 | 140\n       5 |     142 | trade     | NN  | I-NP     |   0 | 141\n       5 |     143 | figure    | NN  | I-NP     |   0 | 142\n       5 |     144 | are       | VBP | B-VP     |   1 | 143\n       5 |     145 | very      | RB  | B-ADVP   |   4 | 144\n       5 |     146 | heavily   | RB  | I-ADVP   |   4 | 145\n       5 |     147 | on        | IN  | B-PP     |   2 | 146\n       5 |     148 | the       | DT  | B-NP     |   0 | 147\n       5 |     149 | down      | JJ  | I-NP     |   0 | 148\n       5 |     150 | side      | NN  | I-NP     |   0 | 149\n       5 |     151 | ,         | ,   | O        |   6 | 150\n       5 |     152 | ''        | ''  | O        |   6 | 151\n       5 |     153 | said      | VBD | B-VP     |   1 | 152\n       5 |     154 | Chris     | NNP | B-NP     |   0 | 153\n       6 |     155 | Dillow    | NNP | I-NP     |   0 | 155\n       6 |     156 | ,         | ,   | O        |   6 | 156\n       6 |     157 | senior    | JJ  | B-NP     |   0 | 157\n       6 |     158 | U.K.      | NNP | I-NP     |   0 | 158\n       6 |     159 | economist | NN  | I-NP     |   0 | 159\n       6 |     160 | at        | IN  | B-PP     |   2 | 160\n       6 |     161 | Nomura    | NNP | B-NP     |   0 | 161\n       6 |     162 | Research  | NNP | I-NP     |   0 | 162\n       6 |     163 | Institute | NNP | I-NP     |   0 | 163\n       6 |     164 | .         | .   | O        |   6 | 164\n```\n\nThe input to the `udf/ext_training.py` looked like this:\n\n```\n     151 | ,             | ,    | O      |\n     152 | ''            | ''   | O      |\n     153 | said          | VBD  | B-VP   |\n     154 | Chris         | NNP  | B-NP   |\n    6553 |               |      |        |\n     155 | Dillow        | NNP  | I-NP   |\n     156 | ,             | ,    | O      |\n     157 | senior        | JJ   | B-NP   |\n     158 | U.K.          | NNP  | I-NP   |\n     159 | economist     | NN   | I-NP   |\n```\n",
        "created_at": "2016-02-10T02:36:03Z",
        "updated_at": "2016-02-10T02:59:35Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/485#discussion_r52408356",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/485",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/52408356"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/485#discussion_r52408356"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/485"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/52408711",
        "pull_request_review_id": null,
        "id": 52408711,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDA4NzEx",
        "diff_hunk": "@@ -18,13 +24,16 @@ word_features(\n \n tag?(word_id bigint) Categorical(13).\n \n-function ext_training\n-  over rows like words_raw\n-  returns rows like words\n-  implementation \"udf/ext_training.py\" handles tsv lines.\n-\n-words +=\n-  ext_training(wid, word, pos, tag) :- words_raw(wid, word, pos, tag).\n+words(sent_id, word_id, word, pos, true_tag, tag_id) :-\n+  words_raw(sent_id, word_id, word, pos, true_tag),\n+  tags(tag_id, tag),\n+  if true_tag = \"B-UCP\" then \"\"\n+  else if true_tag = \"I-UCP\" then \"\"",
        "path": "examples/chunking/app.ddlog",
        "position": null,
        "original_position": 32,
        "commit_id": "9916610e80ac3900023b06bb34d475deddd9e069",
        "original_commit_id": "6ed0efeb9e0521c0f59d73bfbc0bbbef7fc91440",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I see.  I'm getting the following numbers on my side after a little bit of edits.  Do they look good to you?\n\n```\n2016-02-09 18:40:42.179312 ################################################\n2016-02-09 18:40:42.179337 # nvar               : 270052\n2016-02-09 18:40:42.179360 # nfac               : 1049594\n2016-02-09 18:40:42.179392 # nweight            : 268970\n2016-02-09 18:40:42.179413 # nedge              : 1354720\n2016-02-09 18:40:42.179434 ################################################\n2016-02-09 18:40:42.350998 LOADED VARIABLES: #270052\n2016-02-09 18:40:42.351067          N_QUERY: #49389\n2016-02-09 18:40:42.351087          N_EVID : #220663\n2016-02-09 18:40:42.412293 LOADED WEIGHTS: #268970\n2016-02-09 18:40:43.577835 LOADED FACTORS: #1049594\n```\n",
        "created_at": "2016-02-10T02:42:36Z",
        "updated_at": "2016-02-10T02:59:35Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/485#discussion_r52408711",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/485",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/52408711"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/485#discussion_r52408711"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/485"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/52409119",
        "pull_request_review_id": null,
        "id": 52409119,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDA5MTE5",
        "diff_hunk": "@@ -18,13 +24,16 @@ word_features(\n \n tag?(word_id bigint) Categorical(13).\n \n-function ext_training\n-  over rows like words_raw\n-  returns rows like words\n-  implementation \"udf/ext_training.py\" handles tsv lines.\n-\n-words +=\n-  ext_training(wid, word, pos, tag) :- words_raw(wid, word, pos, tag).\n+words(sent_id, word_id, word, pos, true_tag, tag_id) :-\n+  words_raw(sent_id, word_id, word, pos, true_tag),\n+  tags(tag_id, tag),\n+  if true_tag = \"B-UCP\" then \"\"\n+  else if true_tag = \"I-UCP\" then \"\"",
        "path": "examples/chunking/app.ddlog",
        "position": null,
        "original_position": 32,
        "commit_id": "9916610e80ac3900023b06bb34d475deddd9e069",
        "original_commit_id": "6ed0efeb9e0521c0f59d73bfbc0bbbef7fc91440",
        "user": {
            "login": "raphaelhoffmann",
            "id": 991789,
            "node_id": "MDQ6VXNlcjk5MTc4OQ==",
            "avatar_url": "https://avatars2.githubusercontent.com/u/991789?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/raphaelhoffmann",
            "html_url": "https://github.com/raphaelhoffmann",
            "followers_url": "https://api.github.com/users/raphaelhoffmann/followers",
            "following_url": "https://api.github.com/users/raphaelhoffmann/following{/other_user}",
            "gists_url": "https://api.github.com/users/raphaelhoffmann/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/raphaelhoffmann/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/raphaelhoffmann/subscriptions",
            "organizations_url": "https://api.github.com/users/raphaelhoffmann/orgs",
            "repos_url": "https://api.github.com/users/raphaelhoffmann/repos",
            "events_url": "https://api.github.com/users/raphaelhoffmann/events{/privacy}",
            "received_events_url": "https://api.github.com/users/raphaelhoffmann/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Yes, they are identical to my numbers on this example.\n",
        "created_at": "2016-02-10T02:50:09Z",
        "updated_at": "2016-02-10T02:59:35Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/485#discussion_r52409119",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/485",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/52409119"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/485#discussion_r52409119"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/485"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/52413045",
        "pull_request_review_id": null,
        "id": 52413045,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDEzMDQ1",
        "diff_hunk": "@@ -0,0 +1,242 @@\n+#!/usr/bin/env bash\n+# deepdive-model -- Supports working with the statistical inference model\n+#\n+# > deepdive model ground\n+# > deepdive model factorgraph init\n+# > deepdive model factorgraph list\n+# > deepdive model factorgraph keep  [NAME]\n+# > deepdive model factorgraph reuse [NAME]\n+# > deepdive model factorgraph drop  [NAME]\n+#\n+# > deepdive model learn\n+# > deepdive model weights init\n+# > deepdive model weights list\n+# > deepdive model weights keep  [NAME]\n+# > deepdive model weights reuse [NAME]\n+# > deepdive model weights drop  [NAME]\n+#\n+# > deepdive model infer\n+#\n+# > deepdive model calibration\n+##\n+set -euo pipefail\n+\n+DEEPDIVE_APP=$(find-deepdive-app)\n+export DEEPDIVE_APP\n+\n+[[ $# -gt 0 ]] || usage \"$0\" \"Missing COMMAND\"\n+Command=$1; shift\n+\n+case $Command in\n+    # shorthands for repeating groups of processes related to the model\n+    ground)\n+        exec deepdive-redo process/grounding/{variable_id_partition,combine_factorgraph}\n+        ;;\n+\n+    learn)\n+        exec deepdive-redo process/model/learning data/model/{weights,probabilities}\n+        ;;\n+\n+    infer)\n+        exec deepdive-redo process/model/inference data/model/{weights,probabilities}\n+        ;;\n+\n+    calibration)\n+        exec deepdive-redo model/calibration-plots\n+        ;;\n+\n+    # managment of artifacts related to the model\n+    factorgraph|weights)\n+        What=$Command; [[ $# -gt 0 ]] || set -- list  # defaults to listing\n+        Command=$1; shift\n+        case $Command in\n+            list)\n+                container=\"$DEEPDIVE_APP\"/snapshot/model/\"$What\"\n+                if [[ -d \"$container\" ]]; then\n+                    cd \"$container\"\n+                    ls -t\n+                fi\n+                ;;\n+\n+            keep)\n+                if [[ $# -eq 0 ]]; then\n+                    Name=$(date +%Y%m%d.%H%M%S)\n+                else\n+                    Name=$1; shift\n+                fi\n+                # determine where to keep: either in snapshot/model/ or $Name itself if it's a path\n+                case $Name in\n+                    */*)\n+                        mkdir -p \"$Name\"\n+                        dest=$(cd \"$Name\" && pwd)\n+                        cd \"$DEEPDIVE_APP\"\n+                        ;;\n+                    *)\n+                        cd \"$DEEPDIVE_APP\"\n+                        dest=snapshot/model/\"$What\"/\"$Name\"\n+                        mkdir -p \"$dest\"\n+                esac\n+                case $What in\n+                    factorgraph)\n+                        # TODO keep database tables for grounding as well\n+                        # TODO rsync run/model/grounding as well\n+                        # TODO use cp -al instead of duplicating with rsync?\n+                        rsync -avH --delete --copy-unsafe-links run/model/factorgraph{,.done} \"$dest\"/\n+                        ;;\n+                    weights)\n+                        # automatically load weights only if learning is already done\n+                        if ! deepdive-\"done\" data/model/weights &&\n+                             deepdive-\"done\" process/model/learning; then\n+                            DEEPDIVE_PLAN_EDIT=false \\\n+                            deepdive-\"do\" data/model/weights\n+                        fi\n+                        # dump non-zero weights for boolean factors\n+                        if [[ -n $(deepdive-sql eval \"\n+                                SELECT 1 WHERE EXISTS (\n+                                    SELECT id FROM dd_graph_weights\n+                                     WHERE categories IS NULL)\") ]]; then\n+                            # TODO parallel unload\n+                            show_progress output_from \"$dest\" -- \\\n+                            deepdive-sql eval \"\n+                                SELECT description\n+                                     , weight\n+                                  FROM dd_inference_result_weights wo\n+                                     , dd_graph_weights            wi\n+                                 WHERE wo.id = wi.id\n+                                   AND weight <> 0\n+                                   AND categories IS NULL -- to rule out multinomial\n+                            \" format=tsv | pbzip2 >\"$dest\"/dd_weights.tsv.bz2\n+                        fi\n+                        # dump non-zero weights for multinomial factors\n+                        if [[ -n $(deepdive-sql eval \"\n+                                SELECT 1 WHERE EXISTS (\n+                                    SELECT id FROM dd_graph_weights\n+                                     WHERE categories IS NOT NULL)\") ]]; then\n+                            # TODO parallel unload\n+                            # XXX ARRAY_AGG is postgres specific\n+                            show_progress output_from \"$dest\" -- \\\n+                            deepdive-sql eval \"\n+                                SELECT description\n+                                     , ARRAY_AGG(categories::TEXT)\n+                                     , ARRAY_AGG(weight)\n+                                  FROM dd_inference_result_weights wo\n+                                     , dd_graph_weights            wi\n+                                 WHERE wo.id = wi.id\n+                                   AND weight <> 0\n+                                   AND categories IS NOT NULL -- to include multinomial only\n+                                 GROUP BY description\n+                            \" format=tsv | pbzip2 >\"$dest\"/dd_weights.multinomial.tsv.bz2\n+                        fi\n+                        ;;\n+                esac\n+                # keep a LAST symlink pointing to the last\n+                mkdir -p snapshot/model/\"$What\"\n+                ln -sfnv \"$Name\" snapshot/model/\"$What\"/LAST\n+                ;;\n+\n+            reuse|drop)\n+                if [[ $# -eq 0 ]]; then\n+                    Name=LAST\n+                    [[ -e \"$DEEPDIVE_APP/snapshot/model/$What/$Name\" ]] ||\n+                        error \"\\`deepdive $What keep\\` has never been run\"\n+                else\n+                    Name=$1; shift\n+                fi\n+                # determine the actual artifact\n+                if [[ -d \"$Name\" ]]; then\n+                    src=$(cd \"$Name\" && pwd)\n+                    cd \"$DEEPDIVE_APP\"\n+                else\n+                    cd \"$DEEPDIVE_APP\"\n+                    src=\"snapshot/model/$What/$Name\"\n+                    [[ -e $src ]] ||\n+                        error \"\\`deepdive keep $What $Name\\` has never run or $Name got removed\"\n+                fi\n+                case $Command in\n+                    reuse)\n+                        case $What in\n+                            factorgraph)\n+                                cd \"$DEEPDIVE_APP\"\n+                                # TODO use (relative) symlink or cp -al instead of duplicating with rsync?\n+                                rsync -avH --delete --copy-unsafe-links \"$src\"/factorgraph{,.done} run/model/\n+                                # TODO load all table for grounding as well\n+                                ;;\n+                            weights)\n+                                # TODO load into a unique table, then create a view with the following name\n+                                weightTableName=dd_graph_weights_reuse\n+                                weightsReuseFlag=\"$DEEPDIVE_APP\"/run/model/grounding/factor/weights.reuse\n+                                deepdive create table \"$weightTableName\" \"description:TEXT\" \"categories:TEXT\" \"weight:DOUBLE PRECISION\"\n+                                rm -f \"$weightsReuseFlag\"\n+                                for weightFile in \"$src\"/dd_weights{,.multinomial}.tsv.bz2; do\n+                                    [[ -e \"$weightFile\" ]] || continue\n+                                    case $weightFile in\n+                                        *.multinomial.tsv.bz2)\n+                                            # first load the grouped data from file\n+                                            deepdive create table \"${weightTableName}_array\" \"description:TEXT\" \"categories::TEXT[]\" \"weights:DOUBLE PRECISION[]\"\n+                                            # TODO parallel load\n+                                            deepdive load \"${weightTableName}_array\" \"$weightFile\"\n+                                            # explode the group\n+                                            # XXX UNNEST is postgres-specific\n+                                            deepdive sql \"INSERT INTO $weightTableName\n+                                                SELECT description\n+                                                     , UNNEST(categories) AS categories\n+                                                     , UNNEST(weights)    AS weight\n+                                                  FROM ${weightTableName}_array\n+                                            \"\n+                                            deepdive sql \"DROP TABLE ${weightTableName}_array\"\n+                                            ;;\n+                                        *.tsv.bz2)\n+                                            # TODO parallel load\n+                                            deepdive load \"$weightTableName\" \"$weightFile\"",
        "path": "inference/deepdive-model",
        "position": null,
        "original_position": 190,
        "commit_id": "232ae9f6111af4e5a8cbe6f6e9cb2da3d86ee36e",
        "original_commit_id": "6d88393d1059e60b44fecefe8ef6e92aba7411bf",
        "user": {
            "login": "raphaelhoffmann",
            "id": 991789,
            "node_id": "MDQ6VXNlcjk5MTc4OQ==",
            "avatar_url": "https://avatars2.githubusercontent.com/u/991789?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/raphaelhoffmann",
            "html_url": "https://github.com/raphaelhoffmann",
            "followers_url": "https://api.github.com/users/raphaelhoffmann/followers",
            "following_url": "https://api.github.com/users/raphaelhoffmann/following{/other_user}",
            "gists_url": "https://api.github.com/users/raphaelhoffmann/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/raphaelhoffmann/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/raphaelhoffmann/subscriptions",
            "organizations_url": "https://api.github.com/users/raphaelhoffmann/orgs",
            "repos_url": "https://api.github.com/users/raphaelhoffmann/repos",
            "events_url": "https://api.github.com/users/raphaelhoffmann/events{/privacy}",
            "received_events_url": "https://api.github.com/users/raphaelhoffmann/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Does this work? I ran `deepdive model weights reuse foo` on the spouse tsv example, and I got\n\n```\nERROR:  missing data for column \"weight\"\nCONTEXT:  COPY dd_graph_weights_reuse, line 1: \"f_has_spouse_features--word_between=D   0.723347\"\n```\n\nTable `dd_graph_weights_reuse` has three columns, but the dumped file that we are trying to load only has two columns.\n",
        "created_at": "2016-02-10T04:18:22Z",
        "updated_at": "2016-02-10T06:45:49Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/481#discussion_r52413045",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/481",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/52413045"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/481#discussion_r52413045"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/481"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/52414328",
        "pull_request_review_id": null,
        "id": 52414328,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDE0MzI4",
        "diff_hunk": "@@ -0,0 +1,242 @@\n+#!/usr/bin/env bash\n+# deepdive-model -- Supports working with the statistical inference model\n+#\n+# > deepdive model ground\n+# > deepdive model factorgraph init\n+# > deepdive model factorgraph list\n+# > deepdive model factorgraph keep  [NAME]\n+# > deepdive model factorgraph reuse [NAME]\n+# > deepdive model factorgraph drop  [NAME]\n+#\n+# > deepdive model learn\n+# > deepdive model weights init\n+# > deepdive model weights list\n+# > deepdive model weights keep  [NAME]\n+# > deepdive model weights reuse [NAME]\n+# > deepdive model weights drop  [NAME]\n+#\n+# > deepdive model infer\n+#\n+# > deepdive model calibration\n+##\n+set -euo pipefail\n+\n+DEEPDIVE_APP=$(find-deepdive-app)\n+export DEEPDIVE_APP\n+\n+[[ $# -gt 0 ]] || usage \"$0\" \"Missing COMMAND\"\n+Command=$1; shift\n+\n+case $Command in\n+    # shorthands for repeating groups of processes related to the model\n+    ground)\n+        exec deepdive-redo process/grounding/{variable_id_partition,combine_factorgraph}\n+        ;;\n+\n+    learn)\n+        exec deepdive-redo process/model/learning data/model/{weights,probabilities}\n+        ;;\n+\n+    infer)\n+        exec deepdive-redo process/model/inference data/model/{weights,probabilities}\n+        ;;\n+\n+    calibration)\n+        exec deepdive-redo model/calibration-plots\n+        ;;\n+\n+    # managment of artifacts related to the model\n+    factorgraph|weights)\n+        What=$Command; [[ $# -gt 0 ]] || set -- list  # defaults to listing\n+        Command=$1; shift\n+        case $Command in\n+            list)\n+                container=\"$DEEPDIVE_APP\"/snapshot/model/\"$What\"\n+                if [[ -d \"$container\" ]]; then\n+                    cd \"$container\"\n+                    ls -t\n+                fi\n+                ;;\n+\n+            keep)\n+                if [[ $# -eq 0 ]]; then\n+                    Name=$(date +%Y%m%d.%H%M%S)\n+                else\n+                    Name=$1; shift\n+                fi\n+                # determine where to keep: either in snapshot/model/ or $Name itself if it's a path\n+                case $Name in\n+                    */*)\n+                        mkdir -p \"$Name\"\n+                        dest=$(cd \"$Name\" && pwd)\n+                        cd \"$DEEPDIVE_APP\"\n+                        ;;\n+                    *)\n+                        cd \"$DEEPDIVE_APP\"\n+                        dest=snapshot/model/\"$What\"/\"$Name\"\n+                        mkdir -p \"$dest\"\n+                esac\n+                case $What in\n+                    factorgraph)\n+                        # TODO keep database tables for grounding as well\n+                        # TODO rsync run/model/grounding as well\n+                        # TODO use cp -al instead of duplicating with rsync?\n+                        rsync -avH --delete --copy-unsafe-links run/model/factorgraph{,.done} \"$dest\"/\n+                        ;;\n+                    weights)\n+                        # automatically load weights only if learning is already done\n+                        if ! deepdive-\"done\" data/model/weights &&\n+                             deepdive-\"done\" process/model/learning; then\n+                            DEEPDIVE_PLAN_EDIT=false \\\n+                            deepdive-\"do\" data/model/weights\n+                        fi\n+                        # dump non-zero weights for boolean factors\n+                        if [[ -n $(deepdive-sql eval \"\n+                                SELECT 1 WHERE EXISTS (\n+                                    SELECT id FROM dd_graph_weights\n+                                     WHERE categories IS NULL)\") ]]; then\n+                            # TODO parallel unload\n+                            show_progress output_from \"$dest\" -- \\\n+                            deepdive-sql eval \"\n+                                SELECT description\n+                                     , weight\n+                                  FROM dd_inference_result_weights wo\n+                                     , dd_graph_weights            wi\n+                                 WHERE wo.id = wi.id\n+                                   AND weight <> 0\n+                                   AND categories IS NULL -- to rule out multinomial\n+                            \" format=tsv | pbzip2 >\"$dest\"/dd_weights.tsv.bz2\n+                        fi\n+                        # dump non-zero weights for multinomial factors\n+                        if [[ -n $(deepdive-sql eval \"\n+                                SELECT 1 WHERE EXISTS (\n+                                    SELECT id FROM dd_graph_weights\n+                                     WHERE categories IS NOT NULL)\") ]]; then\n+                            # TODO parallel unload\n+                            # XXX ARRAY_AGG is postgres specific\n+                            show_progress output_from \"$dest\" -- \\\n+                            deepdive-sql eval \"\n+                                SELECT description\n+                                     , ARRAY_AGG(categories::TEXT)\n+                                     , ARRAY_AGG(weight)\n+                                  FROM dd_inference_result_weights wo\n+                                     , dd_graph_weights            wi\n+                                 WHERE wo.id = wi.id\n+                                   AND weight <> 0\n+                                   AND categories IS NOT NULL -- to include multinomial only\n+                                 GROUP BY description\n+                            \" format=tsv | pbzip2 >\"$dest\"/dd_weights.multinomial.tsv.bz2\n+                        fi\n+                        ;;\n+                esac\n+                # keep a LAST symlink pointing to the last\n+                mkdir -p snapshot/model/\"$What\"\n+                ln -sfnv \"$Name\" snapshot/model/\"$What\"/LAST\n+                ;;\n+\n+            reuse|drop)\n+                if [[ $# -eq 0 ]]; then\n+                    Name=LAST\n+                    [[ -e \"$DEEPDIVE_APP/snapshot/model/$What/$Name\" ]] ||\n+                        error \"\\`deepdive $What keep\\` has never been run\"\n+                else\n+                    Name=$1; shift\n+                fi\n+                # determine the actual artifact\n+                if [[ -d \"$Name\" ]]; then\n+                    src=$(cd \"$Name\" && pwd)\n+                    cd \"$DEEPDIVE_APP\"\n+                else\n+                    cd \"$DEEPDIVE_APP\"\n+                    src=\"snapshot/model/$What/$Name\"\n+                    [[ -e $src ]] ||\n+                        error \"\\`deepdive keep $What $Name\\` has never run or $Name got removed\"\n+                fi\n+                case $Command in\n+                    reuse)\n+                        case $What in\n+                            factorgraph)\n+                                cd \"$DEEPDIVE_APP\"\n+                                # TODO use (relative) symlink or cp -al instead of duplicating with rsync?\n+                                rsync -avH --delete --copy-unsafe-links \"$src\"/factorgraph{,.done} run/model/\n+                                # TODO load all table for grounding as well\n+                                ;;\n+                            weights)\n+                                # TODO load into a unique table, then create a view with the following name\n+                                weightTableName=dd_graph_weights_reuse\n+                                weightsReuseFlag=\"$DEEPDIVE_APP\"/run/model/grounding/factor/weights.reuse\n+                                deepdive create table \"$weightTableName\" \"description:TEXT\" \"categories:TEXT\" \"weight:DOUBLE PRECISION\"\n+                                rm -f \"$weightsReuseFlag\"\n+                                for weightFile in \"$src\"/dd_weights{,.multinomial}.tsv.bz2; do\n+                                    [[ -e \"$weightFile\" ]] || continue\n+                                    case $weightFile in\n+                                        *.multinomial.tsv.bz2)\n+                                            # first load the grouped data from file\n+                                            deepdive create table \"${weightTableName}_array\" \"description:TEXT\" \"categories::TEXT[]\" \"weights:DOUBLE PRECISION[]\"\n+                                            # TODO parallel load\n+                                            deepdive load \"${weightTableName}_array\" \"$weightFile\"\n+                                            # explode the group\n+                                            # XXX UNNEST is postgres-specific\n+                                            deepdive sql \"INSERT INTO $weightTableName\n+                                                SELECT description\n+                                                     , UNNEST(categories) AS categories\n+                                                     , UNNEST(weights)    AS weight\n+                                                  FROM ${weightTableName}_array\n+                                            \"\n+                                            deepdive sql \"DROP TABLE ${weightTableName}_array\"\n+                                            ;;\n+                                        *.tsv.bz2)\n+                                            # TODO parallel load\n+                                            deepdive load \"$weightTableName\" \"$weightFile\"",
        "path": "inference/deepdive-model",
        "position": null,
        "original_position": 190,
        "commit_id": "232ae9f6111af4e5a8cbe6f6e9cb2da3d86ee36e",
        "original_commit_id": "6d88393d1059e60b44fecefe8ef6e92aba7411bf",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Sorry, I may have left some bugs after last minute changes for multinomial.  Another reason to set up the tests first.  It should probably work if replaced with the following line.\n\n```\ndeepdive load \"$weightTableName(description,weight)\" \"$weightFile\"\n```\n",
        "created_at": "2016-02-10T04:48:13Z",
        "updated_at": "2016-02-10T06:45:49Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/481#discussion_r52414328",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/481",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/52414328"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/481#discussion_r52414328"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/481"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/52514736",
        "pull_request_review_id": null,
        "id": 52514736,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTE0NzM2",
        "diff_hunk": "@@ -0,0 +1,240 @@\n+#!/usr/bin/env bash\n+# deepdive-model -- Supports working with the statistical inference model\n+#\n+# > deepdive model ground\n+# > deepdive model factorgraph init\n+# > deepdive model factorgraph list\n+# > deepdive model factorgraph keep  [NAME]\n+# > deepdive model factorgraph reuse [NAME]\n+# > deepdive model factorgraph drop  [NAME]\n+#\n+# > deepdive model learn\n+# > deepdive model weights init\n+# > deepdive model weights list\n+# > deepdive model weights keep  [NAME]\n+# > deepdive model weights reuse [NAME]\n+# > deepdive model weights drop  [NAME]\n+#\n+# > deepdive model infer\n+#\n+# > deepdive model calibration\n+##\n+set -euo pipefail\n+\n+DEEPDIVE_APP=$(find-deepdive-app)\n+export DEEPDIVE_APP\n+\n+[[ $# -gt 0 ]] || usage \"$0\" \"Missing COMMAND\"\n+Command=$1; shift\n+\n+case $Command in\n+    # shorthands for repeating groups of processes related to the model\n+    ground)\n+        exec deepdive-redo process/grounding/{variable_id_partition,combine_factorgraph}\n+        ;;\n+\n+    learn)\n+        exec deepdive-redo process/model/learning data/model/{weights,probabilities}\n+        ;;\n+\n+    infer)\n+        exec deepdive-redo process/model/inference data/model/{weights,probabilities}\n+        ;;\n+\n+    calibration)\n+        exec deepdive-redo model/calibration-plots\n+        ;;\n+\n+    # managment of artifacts related to the model\n+    factorgraph|weights)\n+        What=$Command; [[ $# -gt 0 ]] || set -- list  # defaults to listing\n+        Command=$1; shift\n+        case $Command in\n+            list)\n+                container=\"$DEEPDIVE_APP\"/snapshot/model/\"$What\"\n+                if [[ -d \"$container\" ]]; then\n+                    cd \"$container\"\n+                    ls -t\n+                fi\n+                ;;\n+\n+            keep)\n+                if [[ $# -eq 0 ]]; then\n+                    Name=$(date +%Y%m%d.%H%M%S)\n+                else\n+                    Name=$1; shift\n+                fi\n+                # determine where to keep: either in snapshot/model/ or $Name itself if it's a path\n+                case $Name in\n+                    */*)\n+                        mkdir -p \"$Name\"\n+                        dest=$(cd \"$Name\" && pwd)\n+                        cd \"$DEEPDIVE_APP\"\n+                        ;;\n+                    *)\n+                        cd \"$DEEPDIVE_APP\"\n+                        dest=snapshot/model/\"$What\"/\"$Name\"\n+                        mkdir -p \"$dest\"\n+                esac\n+                case $What in\n+                    factorgraph)\n+                        # TODO keep database tables for grounding as well\n+                        # TODO rsync run/model/grounding as well\n+                        # TODO use cp -al instead of duplicating with rsync?\n+                        rsync -avH --delete --copy-unsafe-links run/model/factorgraph{,.done} \"$dest\"/\n+                        ;;\n+                    weights)\n+                        # automatically load weights only if learning is already done\n+                        if ! deepdive-\"done\" data/model/weights &&\n+                             deepdive-\"done\" process/model/learning; then\n+                            DEEPDIVE_PLAN_EDIT=false \\\n+                            deepdive-\"do\" data/model/weights\n+                        fi\n+                        # dump non-zero weights for boolean factors\n+                        if [[ -n $(deepdive-sql eval \"\n+                                SELECT 1 WHERE EXISTS (\n+                                    SELECT id FROM dd_graph_weights\n+                                     WHERE categories IS NULL)\") ]]; then\n+                            # TODO parallel unload\n+                            show_progress output_from \"$dest\" -- \\\n+                            deepdive-sql eval \"\n+                                SELECT description\n+                                     , weight\n+                                  FROM dd_inference_result_weights wo\n+                                     , dd_graph_weights            wi\n+                                 WHERE wo.id = wi.id\n+                                   AND weight <> 0\n+                                   AND categories IS NULL -- to rule out multinomial\n+                            \" format=tsv | pbzip2 >\"$dest\"/dd_weights.tsv.bz2\n+                        fi\n+                        # dump non-zero weights for multinomial factors\n+                        if [[ -n $(deepdive-sql eval \"\n+                                SELECT 1 WHERE EXISTS (\n+                                    SELECT id FROM dd_graph_weights\n+                                     WHERE categories IS NOT NULL)\") ]]; then\n+                            # TODO parallel unload\n+                            # XXX ARRAY_AGG is postgres specific\n+                            show_progress output_from \"$dest\" -- \\\n+                            deepdive-sql eval \"\n+                                SELECT description\n+                                     , ARRAY_AGG(categories::TEXT)\n+                                     , ARRAY_AGG(weight)\n+                                  FROM dd_inference_result_weights wo\n+                                     , dd_graph_weights            wi\n+                                 WHERE wo.id = wi.id\n+                                   AND weight <> 0\n+                                   AND categories IS NOT NULL -- to include multinomial only\n+                                 GROUP BY description\n+                            \" format=tsv | pbzip2 >\"$dest\"/dd_weights.multinomial.tsv.bz2\n+                        fi\n+                        ;;\n+                esac\n+                # keep a LAST symlink pointing to the last\n+                mkdir -p snapshot/model/\"$What\"\n+                ln -sfnv \"$Name\" snapshot/model/\"$What\"/LAST\n+                ;;\n+\n+            reuse|drop)\n+                if [[ $# -eq 0 ]]; then\n+                    Name=LAST\n+                    [[ -e \"$DEEPDIVE_APP/snapshot/model/$What/$Name\" ]] ||\n+                        error \"\\`deepdive $What keep\\` has never been run\"\n+                else\n+                    Name=$1; shift\n+                fi\n+                # determine the actual artifact\n+                if [[ -d \"$Name\" ]]; then\n+                    src=$(cd \"$Name\" && pwd)\n+                    cd \"$DEEPDIVE_APP\"\n+                else\n+                    cd \"$DEEPDIVE_APP\"\n+                    src=\"snapshot/model/$What/$Name\"\n+                    [[ -e $src ]] ||\n+                        error \"\\`deepdive keep $What $Name\\` has never run or $Name got removed\"\n+                fi\n+                case $Command in\n+                    reuse)\n+                        case $What in\n+                            factorgraph)\n+                                cd \"$DEEPDIVE_APP\"\n+                                # TODO use (relative) symlink or cp -al instead of duplicating with rsync?\n+                                rsync -avH --delete --copy-unsafe-links \"$src\"/factorgraph{,.done} run/model/\n+                                # TODO load all table for grounding as well\n+                                ;;\n+                            weights)\n+                                # TODO load into a unique table, then create a view with the following name\n+                                weightTableName=dd_graph_weights_reuse\n+                                weightsReuseFlag=\"$DEEPDIVE_APP\"/run/model/grounding/factor/weights.reuse\n+                                deepdive create table \"$weightTableName\" \"description:TEXT\" \"categories:TEXT\" \"weight:DOUBLE PRECISION\"\n+                                rm -f \"$weightsReuseFlag\"\n+                                for weightFile in \"$src\"/dd_weights{,.multinomial}.tsv.bz2; do\n+                                    [[ -e \"$weightFile\" ]] || continue\n+                                    case $weightFile in\n+                                        *.multinomial.tsv.bz2)\n+                                            # first load the grouped data from file\n+                                            deepdive create table \"${weightTableName}_array\" \"description:TEXT\" \"categories::TEXT[]\" \"weights:DOUBLE PRECISION[]\"\n+                                            # TODO parallel load\n+                                            deepdive load \"${weightTableName}_array\" \"$weightFile\"\n+                                            # explode the group\n+                                            # XXX UNNEST is postgres-specific\n+                                            deepdive sql \"INSERT INTO $weightTableName",
        "path": "inference/deepdive-model",
        "position": 180,
        "original_position": 180,
        "commit_id": "232ae9f6111af4e5a8cbe6f6e9cb2da3d86ee36e",
        "original_commit_id": "232ae9f6111af4e5a8cbe6f6e9cb2da3d86ee36e",
        "user": {
            "login": "raphaelhoffmann",
            "id": 991789,
            "node_id": "MDQ6VXNlcjk5MTc4OQ==",
            "avatar_url": "https://avatars2.githubusercontent.com/u/991789?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/raphaelhoffmann",
            "html_url": "https://github.com/raphaelhoffmann",
            "followers_url": "https://api.github.com/users/raphaelhoffmann/followers",
            "following_url": "https://api.github.com/users/raphaelhoffmann/following{/other_user}",
            "gists_url": "https://api.github.com/users/raphaelhoffmann/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/raphaelhoffmann/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/raphaelhoffmann/subscriptions",
            "organizations_url": "https://api.github.com/users/raphaelhoffmann/orgs",
            "repos_url": "https://api.github.com/users/raphaelhoffmann/repos",
            "events_url": "https://api.github.com/users/raphaelhoffmann/events{/privacy}",
            "received_events_url": "https://api.github.com/users/raphaelhoffmann/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "it probably doesn't matter in practice but it might be more efficient to do `create table as`\n",
        "created_at": "2016-02-10T19:52:08Z",
        "updated_at": "2016-02-10T19:52:08Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/481#discussion_r52514736",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/481",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/52514736"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/481#discussion_r52514736"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/481"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/52547671",
        "pull_request_review_id": null,
        "id": 52547671,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTQ3Njcx",
        "diff_hunk": "@@ -0,0 +1,240 @@\n+#!/usr/bin/env bash\n+# deepdive-model -- Supports working with the statistical inference model\n+#\n+# > deepdive model ground\n+# > deepdive model factorgraph init\n+# > deepdive model factorgraph list\n+# > deepdive model factorgraph keep  [NAME]\n+# > deepdive model factorgraph reuse [NAME]\n+# > deepdive model factorgraph drop  [NAME]\n+#\n+# > deepdive model learn\n+# > deepdive model weights init\n+# > deepdive model weights list\n+# > deepdive model weights keep  [NAME]\n+# > deepdive model weights reuse [NAME]\n+# > deepdive model weights drop  [NAME]\n+#\n+# > deepdive model infer\n+#\n+# > deepdive model calibration\n+##\n+set -euo pipefail\n+\n+DEEPDIVE_APP=$(find-deepdive-app)\n+export DEEPDIVE_APP\n+\n+[[ $# -gt 0 ]] || usage \"$0\" \"Missing COMMAND\"\n+Command=$1; shift\n+\n+case $Command in\n+    # shorthands for repeating groups of processes related to the model\n+    ground)\n+        exec deepdive-redo process/grounding/{variable_id_partition,combine_factorgraph}\n+        ;;\n+\n+    learn)\n+        exec deepdive-redo process/model/learning data/model/{weights,probabilities}\n+        ;;\n+\n+    infer)\n+        exec deepdive-redo process/model/inference data/model/{weights,probabilities}\n+        ;;\n+\n+    calibration)\n+        exec deepdive-redo model/calibration-plots\n+        ;;\n+\n+    # managment of artifacts related to the model\n+    factorgraph|weights)\n+        What=$Command; [[ $# -gt 0 ]] || set -- list  # defaults to listing\n+        Command=$1; shift\n+        case $Command in\n+            list)\n+                container=\"$DEEPDIVE_APP\"/snapshot/model/\"$What\"\n+                if [[ -d \"$container\" ]]; then\n+                    cd \"$container\"\n+                    ls -t\n+                fi\n+                ;;\n+\n+            keep)\n+                if [[ $# -eq 0 ]]; then\n+                    Name=$(date +%Y%m%d.%H%M%S)\n+                else\n+                    Name=$1; shift\n+                fi\n+                # determine where to keep: either in snapshot/model/ or $Name itself if it's a path\n+                case $Name in\n+                    */*)\n+                        mkdir -p \"$Name\"\n+                        dest=$(cd \"$Name\" && pwd)\n+                        cd \"$DEEPDIVE_APP\"\n+                        ;;\n+                    *)\n+                        cd \"$DEEPDIVE_APP\"\n+                        dest=snapshot/model/\"$What\"/\"$Name\"\n+                        mkdir -p \"$dest\"\n+                esac\n+                case $What in\n+                    factorgraph)\n+                        # TODO keep database tables for grounding as well\n+                        # TODO rsync run/model/grounding as well\n+                        # TODO use cp -al instead of duplicating with rsync?\n+                        rsync -avH --delete --copy-unsafe-links run/model/factorgraph{,.done} \"$dest\"/\n+                        ;;\n+                    weights)\n+                        # automatically load weights only if learning is already done\n+                        if ! deepdive-\"done\" data/model/weights &&\n+                             deepdive-\"done\" process/model/learning; then\n+                            DEEPDIVE_PLAN_EDIT=false \\\n+                            deepdive-\"do\" data/model/weights\n+                        fi\n+                        # dump non-zero weights for boolean factors\n+                        if [[ -n $(deepdive-sql eval \"\n+                                SELECT 1 WHERE EXISTS (\n+                                    SELECT id FROM dd_graph_weights\n+                                     WHERE categories IS NULL)\") ]]; then\n+                            # TODO parallel unload\n+                            show_progress output_from \"$dest\" -- \\\n+                            deepdive-sql eval \"\n+                                SELECT description\n+                                     , weight\n+                                  FROM dd_inference_result_weights wo\n+                                     , dd_graph_weights            wi\n+                                 WHERE wo.id = wi.id\n+                                   AND weight <> 0\n+                                   AND categories IS NULL -- to rule out multinomial\n+                            \" format=tsv | pbzip2 >\"$dest\"/dd_weights.tsv.bz2\n+                        fi\n+                        # dump non-zero weights for multinomial factors\n+                        if [[ -n $(deepdive-sql eval \"\n+                                SELECT 1 WHERE EXISTS (\n+                                    SELECT id FROM dd_graph_weights\n+                                     WHERE categories IS NOT NULL)\") ]]; then\n+                            # TODO parallel unload\n+                            # XXX ARRAY_AGG is postgres specific\n+                            show_progress output_from \"$dest\" -- \\\n+                            deepdive-sql eval \"\n+                                SELECT description\n+                                     , ARRAY_AGG(categories::TEXT)\n+                                     , ARRAY_AGG(weight)\n+                                  FROM dd_inference_result_weights wo\n+                                     , dd_graph_weights            wi\n+                                 WHERE wo.id = wi.id\n+                                   AND weight <> 0\n+                                   AND categories IS NOT NULL -- to include multinomial only\n+                                 GROUP BY description\n+                            \" format=tsv | pbzip2 >\"$dest\"/dd_weights.multinomial.tsv.bz2\n+                        fi\n+                        ;;\n+                esac\n+                # keep a LAST symlink pointing to the last\n+                mkdir -p snapshot/model/\"$What\"\n+                ln -sfnv \"$Name\" snapshot/model/\"$What\"/LAST\n+                ;;\n+\n+            reuse|drop)\n+                if [[ $# -eq 0 ]]; then\n+                    Name=LAST\n+                    [[ -e \"$DEEPDIVE_APP/snapshot/model/$What/$Name\" ]] ||\n+                        error \"\\`deepdive $What keep\\` has never been run\"\n+                else\n+                    Name=$1; shift\n+                fi\n+                # determine the actual artifact\n+                if [[ -d \"$Name\" ]]; then\n+                    src=$(cd \"$Name\" && pwd)\n+                    cd \"$DEEPDIVE_APP\"\n+                else\n+                    cd \"$DEEPDIVE_APP\"\n+                    src=\"snapshot/model/$What/$Name\"\n+                    [[ -e $src ]] ||\n+                        error \"\\`deepdive keep $What $Name\\` has never run or $Name got removed\"\n+                fi\n+                case $Command in\n+                    reuse)\n+                        case $What in\n+                            factorgraph)\n+                                cd \"$DEEPDIVE_APP\"\n+                                # TODO use (relative) symlink or cp -al instead of duplicating with rsync?\n+                                rsync -avH --delete --copy-unsafe-links \"$src\"/factorgraph{,.done} run/model/\n+                                # TODO load all table for grounding as well\n+                                ;;\n+                            weights)\n+                                # TODO load into a unique table, then create a view with the following name\n+                                weightTableName=dd_graph_weights_reuse\n+                                weightsReuseFlag=\"$DEEPDIVE_APP\"/run/model/grounding/factor/weights.reuse\n+                                deepdive create table \"$weightTableName\" \"description:TEXT\" \"categories:TEXT\" \"weight:DOUBLE PRECISION\"\n+                                rm -f \"$weightsReuseFlag\"\n+                                for weightFile in \"$src\"/dd_weights{,.multinomial}.tsv.bz2; do\n+                                    [[ -e \"$weightFile\" ]] || continue\n+                                    case $weightFile in\n+                                        *.multinomial.tsv.bz2)\n+                                            # first load the grouped data from file\n+                                            deepdive create table \"${weightTableName}_array\" \"description:TEXT\" \"categories::TEXT[]\" \"weights:DOUBLE PRECISION[]\"\n+                                            # TODO parallel load\n+                                            deepdive load \"${weightTableName}_array\" \"$weightFile\"\n+                                            # explode the group\n+                                            # XXX UNNEST is postgres-specific\n+                                            deepdive sql \"INSERT INTO $weightTableName",
        "path": "inference/deepdive-model",
        "position": 180,
        "original_position": 180,
        "commit_id": "232ae9f6111af4e5a8cbe6f6e9cb2da3d86ee36e",
        "original_commit_id": "232ae9f6111af4e5a8cbe6f6e9cb2da3d86ee36e",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Since we had more than one sources of weights (one for boolean, another for multinomial factors) I kept it as INSERT, but we could also create two tables and add a UNION view if you think there's a big difference.\n",
        "created_at": "2016-02-11T00:03:10Z",
        "updated_at": "2016-02-11T00:03:10Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/481#discussion_r52547671",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/481",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/52547671"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/481#discussion_r52547671"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/481"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/53852834",
        "pull_request_review_id": null,
        "id": 53852834,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODUyODM0",
        "diff_hunk": "@@ -1,97 +1,93 @@\n #!/usr/bin/env bash\n # db-assign_sequential_id -- Assigns a unique integer to every row for a table using PostgreSQL sequence generator\n # > eval \"$(db-parse \"$url\")\"\n-# > db-assign_sequential_id TABLE COLUMN BEGIN_ID\n+# > db-assign_sequential_id TABLE COLUMN BEGIN_ID [INCREMENT]\n ##\n set -euo pipefail\n \n [[ $# -gt 0 ]] || usage \"$0\" \"Missing TABLE\"\n [[ $# -gt 1 ]] || usage \"$0\" \"Missing COLUMN\"\n [[ $# -gt 2 ]] || usage \"$0\" \"Missing BEGIN_ID\"\n-Table=$1 Column=$2 BeginId=$3\n+Table=$1 Column=$2 BeginId=$3 Increment=${4:-1}\n \n # Use Greenplum PL/pgSQL and PL/Python UDFs to assign IDs fast\n # See: http://www.postgresql.org/docs/8.2/static/sql-createlanguage.html\n # See: http://www.postgresql.org/docs/8.2/static/plpgsql-overview.html\n # See: http://www.postgresql.org/docs/8.2/static/plpython-funcs.html\n-db-execute \"CREATE LANGUAGE plpgsql;\" || true\n-db-execute \"CREATE LANGUAGE plpythonu;\" || true\n-db-execute \"\n-    CREATE OR REPLACE FUNCTION clear_count_1(sid int) RETURNS int AS\n-    \\$\\$\n-    if '__count_1' in SD:\n-      SD['__count_1'] = -1\n-      return 1\n-    return 0\n+if db-supports_pg_lang \"plpgsql\" && db-supports_pg_lang \"plpythonu\"; then\n+    db-execute \"\n+    CREATE OR REPLACE FUNCTION fast_seqassign_init()\n+    RETURNS INT AS \\$\\$\n+      if 'cumulative_cnt' in SD:\n+        SD.pop('cumulative_cnt')\n+      return 0\n     \\$\\$ LANGUAGE plpythonu;\n \n-    CREATE OR REPLACE FUNCTION updateid(startid bigint, sid int, sids int[], base_ids bigint[], base_ids_noagg bigint[]) RETURNS bigint AS\n-    \\$\\$\n-    if '__count_1' in SD:\n-      a = SD['__count_2']\n-      b = SD['__count_1']\n-      SD['__count_2'] = SD['__count_2'] - 1\n-      if SD['__count_2'] < 0:\n-        SD.pop('__count_1')\n-      return startid+b-a\n-    else:\n-      for i in range(0, len(sids)):\n-        if sids[i] == sid:\n-          SD['__count_1'] = base_ids[i] - 1\n-          SD['__count_2'] = base_ids_noagg[i] - 1\n-      a = SD['__count_2']\n-      b = SD['__count_1']\n-      SD['__count_2'] = SD['__count_2'] - 1\n-      if SD['__count_2'] < 0:\n-        SD.pop('__count_1')\n-      return startid+b-a\n-\n+    CREATE OR REPLACE FUNCTION fast_seqassign_next(startid BIGINT, increment BIGINT, this_gpsid INT, gpsids INT[], cumulative_cnts BIGINT[], cnts BIGINT[])\n+    RETURNS BIGINT AS \\$\\$\n+      if 'cumulative_cnt' not in SD:\n+        for gpsid, cumulative_cnt, cnt in zip(gpsids, cumulative_cnts, cnts):\n+          if gpsid == this_gpsid:\n+            # assignment is done using the cumulative count for this segment and a decrementing counter from the size of this segment down to one\n+            SD['cumulative_cnt'] = cumulative_cnt\n+            SD['remaining'] = cnt\n+            break\n+      if 'cumulative_cnt' in SD and SD['remaining'] > 0:\n+        id = startid + increment * (SD['cumulative_cnt'] - SD['remaining'])\n+        SD['remaining'] = SD['remaining'] - 1\n+        if SD['remaining'] <= 0:\n+          SD.pop('cumulative_cnt')",
        "path": "database/db-driver/greenplum/db-assign_sequential_id",
        "position": null,
        "original_position": 71,
        "commit_id": "e3af172e1da5e306bbc2952823e292c29a1bcc7f",
        "original_commit_id": "a8a8bf61dcf3536a750c2e1af8fa9b66f62ca640",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Looks like these two lines would make the next call re-initialize `SD['cumulative_cnt']`, which in turn would circumvent the `else` block from ever being activated. I think we should just remove them.\n",
        "created_at": "2016-02-23T21:26:48Z",
        "updated_at": "2016-02-24T04:51:10Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/496#discussion_r53852834",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/496",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/53852834"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/496#discussion_r53852834"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/496"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/53852983",
        "pull_request_review_id": null,
        "id": 53852983,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODUyOTgz",
        "diff_hunk": "@@ -1,97 +1,93 @@\n #!/usr/bin/env bash\n # db-assign_sequential_id -- Assigns a unique integer to every row for a table using PostgreSQL sequence generator\n # > eval \"$(db-parse \"$url\")\"\n-# > db-assign_sequential_id TABLE COLUMN BEGIN_ID\n+# > db-assign_sequential_id TABLE COLUMN BEGIN_ID [INCREMENT]\n ##\n set -euo pipefail\n \n [[ $# -gt 0 ]] || usage \"$0\" \"Missing TABLE\"\n [[ $# -gt 1 ]] || usage \"$0\" \"Missing COLUMN\"\n [[ $# -gt 2 ]] || usage \"$0\" \"Missing BEGIN_ID\"\n-Table=$1 Column=$2 BeginId=$3\n+Table=$1 Column=$2 BeginId=$3 Increment=${4:-1}\n \n # Use Greenplum PL/pgSQL and PL/Python UDFs to assign IDs fast\n # See: http://www.postgresql.org/docs/8.2/static/sql-createlanguage.html\n # See: http://www.postgresql.org/docs/8.2/static/plpgsql-overview.html\n # See: http://www.postgresql.org/docs/8.2/static/plpython-funcs.html\n-db-execute \"CREATE LANGUAGE plpgsql;\" || true\n-db-execute \"CREATE LANGUAGE plpythonu;\" || true\n-db-execute \"\n-    CREATE OR REPLACE FUNCTION clear_count_1(sid int) RETURNS int AS\n-    \\$\\$\n-    if '__count_1' in SD:\n-      SD['__count_1'] = -1\n-      return 1\n-    return 0\n+if db-supports_pg_lang \"plpgsql\" && db-supports_pg_lang \"plpythonu\"; then\n+    db-execute \"\n+    CREATE OR REPLACE FUNCTION fast_seqassign_init()\n+    RETURNS INT AS \\$\\$\n+      if 'cumulative_cnt' in SD:\n+        SD.pop('cumulative_cnt')\n+      return 0\n     \\$\\$ LANGUAGE plpythonu;\n \n-    CREATE OR REPLACE FUNCTION updateid(startid bigint, sid int, sids int[], base_ids bigint[], base_ids_noagg bigint[]) RETURNS bigint AS\n-    \\$\\$\n-    if '__count_1' in SD:\n-      a = SD['__count_2']\n-      b = SD['__count_1']\n-      SD['__count_2'] = SD['__count_2'] - 1\n-      if SD['__count_2'] < 0:\n-        SD.pop('__count_1')\n-      return startid+b-a\n-    else:\n-      for i in range(0, len(sids)):\n-        if sids[i] == sid:\n-          SD['__count_1'] = base_ids[i] - 1\n-          SD['__count_2'] = base_ids_noagg[i] - 1\n-      a = SD['__count_2']\n-      b = SD['__count_1']\n-      SD['__count_2'] = SD['__count_2'] - 1\n-      if SD['__count_2'] < 0:\n-        SD.pop('__count_1')\n-      return startid+b-a\n-\n+    CREATE OR REPLACE FUNCTION fast_seqassign_next(startid BIGINT, increment BIGINT, this_gpsid INT, gpsids INT[], cumulative_cnts BIGINT[], cnts BIGINT[])\n+    RETURNS BIGINT AS \\$\\$\n+      if 'cumulative_cnt' not in SD:\n+        for gpsid, cumulative_cnt, cnt in zip(gpsids, cumulative_cnts, cnts):\n+          if gpsid == this_gpsid:\n+            # assignment is done using the cumulative count for this segment and a decrementing counter from the size of this segment down to one\n+            SD['cumulative_cnt'] = cumulative_cnt\n+            SD['remaining'] = cnt\n+            break\n+      if 'cumulative_cnt' in SD and SD['remaining'] > 0:",
        "path": "database/db-driver/greenplum/db-assign_sequential_id",
        "position": null,
        "original_position": 67,
        "commit_id": "e3af172e1da5e306bbc2952823e292c29a1bcc7f",
        "original_commit_id": "a8a8bf61dcf3536a750c2e1af8fa9b66f62ca640",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "This may not be consequential, but since this UDF is invoked once per row, maybe `elif SD['remaining'] > 0:`?\n",
        "created_at": "2016-02-23T21:27:50Z",
        "updated_at": "2016-02-24T04:51:10Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/496#discussion_r53852983",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/496",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/53852983"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/496#discussion_r53852983"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/496"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/53853742",
        "pull_request_review_id": null,
        "id": 53853742,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODUzNzQy",
        "diff_hunk": "@@ -1,97 +1,93 @@\n #!/usr/bin/env bash\n # db-assign_sequential_id -- Assigns a unique integer to every row for a table using PostgreSQL sequence generator\n # > eval \"$(db-parse \"$url\")\"\n-# > db-assign_sequential_id TABLE COLUMN BEGIN_ID\n+# > db-assign_sequential_id TABLE COLUMN BEGIN_ID [INCREMENT]\n ##\n set -euo pipefail\n \n [[ $# -gt 0 ]] || usage \"$0\" \"Missing TABLE\"\n [[ $# -gt 1 ]] || usage \"$0\" \"Missing COLUMN\"\n [[ $# -gt 2 ]] || usage \"$0\" \"Missing BEGIN_ID\"\n-Table=$1 Column=$2 BeginId=$3\n+Table=$1 Column=$2 BeginId=$3 Increment=${4:-1}\n \n # Use Greenplum PL/pgSQL and PL/Python UDFs to assign IDs fast\n # See: http://www.postgresql.org/docs/8.2/static/sql-createlanguage.html\n # See: http://www.postgresql.org/docs/8.2/static/plpgsql-overview.html\n # See: http://www.postgresql.org/docs/8.2/static/plpython-funcs.html\n-db-execute \"CREATE LANGUAGE plpgsql;\" || true\n-db-execute \"CREATE LANGUAGE plpythonu;\" || true\n-db-execute \"\n-    CREATE OR REPLACE FUNCTION clear_count_1(sid int) RETURNS int AS\n-    \\$\\$\n-    if '__count_1' in SD:\n-      SD['__count_1'] = -1\n-      return 1\n-    return 0\n+if db-supports_pg_lang \"plpgsql\" && db-supports_pg_lang \"plpythonu\"; then\n+    db-execute \"\n+    CREATE OR REPLACE FUNCTION fast_seqassign_init()\n+    RETURNS INT AS \\$\\$\n+      if 'cumulative_cnt' in SD:\n+        SD.pop('cumulative_cnt')\n+      return 0\n     \\$\\$ LANGUAGE plpythonu;\n \n-    CREATE OR REPLACE FUNCTION updateid(startid bigint, sid int, sids int[], base_ids bigint[], base_ids_noagg bigint[]) RETURNS bigint AS\n-    \\$\\$\n-    if '__count_1' in SD:\n-      a = SD['__count_2']\n-      b = SD['__count_1']\n-      SD['__count_2'] = SD['__count_2'] - 1\n-      if SD['__count_2'] < 0:\n-        SD.pop('__count_1')\n-      return startid+b-a\n-    else:\n-      for i in range(0, len(sids)):\n-        if sids[i] == sid:\n-          SD['__count_1'] = base_ids[i] - 1\n-          SD['__count_2'] = base_ids_noagg[i] - 1\n-      a = SD['__count_2']\n-      b = SD['__count_1']\n-      SD['__count_2'] = SD['__count_2'] - 1\n-      if SD['__count_2'] < 0:\n-        SD.pop('__count_1')\n-      return startid+b-a\n-\n+    CREATE OR REPLACE FUNCTION fast_seqassign_next(startid BIGINT, increment BIGINT, this_gpsid INT, gpsids INT[], cumulative_cnts BIGINT[], cnts BIGINT[])\n+    RETURNS BIGINT AS \\$\\$\n+      if 'cumulative_cnt' not in SD:\n+        for gpsid, cumulative_cnt, cnt in zip(gpsids, cumulative_cnts, cnts):\n+          if gpsid == this_gpsid:\n+            # assignment is done using the cumulative count for this segment and a decrementing counter from the size of this segment down to one\n+            SD['cumulative_cnt'] = cumulative_cnt\n+            SD['remaining'] = cnt\n+            break\n+      if 'cumulative_cnt' in SD and SD['remaining'] > 0:\n+        id = startid + increment * (SD['cumulative_cnt'] - SD['remaining'])\n+        SD['remaining'] = SD['remaining'] - 1\n+        if SD['remaining'] <= 0:\n+          SD.pop('cumulative_cnt')",
        "path": "database/db-driver/greenplum/db-assign_sequential_id",
        "position": null,
        "original_position": 71,
        "commit_id": "e3af172e1da5e306bbc2952823e292c29a1bcc7f",
        "original_commit_id": "a8a8bf61dcf3536a750c2e1af8fa9b66f62ca640",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I'm not sure if I understood you correctly.  The else block below is for making loud noise when an empty partition is assigned or no partition is assigned for the segment.  Maybe instead of popping, we can put an assertion here that remaining should never go negative.\n",
        "created_at": "2016-02-23T21:32:53Z",
        "updated_at": "2016-02-24T04:51:10Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/496#discussion_r53853742",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/496",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/53853742"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/496#discussion_r53853742"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/496"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/53853909",
        "pull_request_review_id": null,
        "id": 53853909,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODUzOTA5",
        "diff_hunk": "@@ -1,97 +1,93 @@\n #!/usr/bin/env bash\n # db-assign_sequential_id -- Assigns a unique integer to every row for a table using PostgreSQL sequence generator\n # > eval \"$(db-parse \"$url\")\"\n-# > db-assign_sequential_id TABLE COLUMN BEGIN_ID\n+# > db-assign_sequential_id TABLE COLUMN BEGIN_ID [INCREMENT]\n ##\n set -euo pipefail\n \n [[ $# -gt 0 ]] || usage \"$0\" \"Missing TABLE\"\n [[ $# -gt 1 ]] || usage \"$0\" \"Missing COLUMN\"\n [[ $# -gt 2 ]] || usage \"$0\" \"Missing BEGIN_ID\"\n-Table=$1 Column=$2 BeginId=$3\n+Table=$1 Column=$2 BeginId=$3 Increment=${4:-1}\n \n # Use Greenplum PL/pgSQL and PL/Python UDFs to assign IDs fast\n # See: http://www.postgresql.org/docs/8.2/static/sql-createlanguage.html\n # See: http://www.postgresql.org/docs/8.2/static/plpgsql-overview.html\n # See: http://www.postgresql.org/docs/8.2/static/plpython-funcs.html\n-db-execute \"CREATE LANGUAGE plpgsql;\" || true\n-db-execute \"CREATE LANGUAGE plpythonu;\" || true\n-db-execute \"\n-    CREATE OR REPLACE FUNCTION clear_count_1(sid int) RETURNS int AS\n-    \\$\\$\n-    if '__count_1' in SD:\n-      SD['__count_1'] = -1\n-      return 1\n-    return 0\n+if db-supports_pg_lang \"plpgsql\" && db-supports_pg_lang \"plpythonu\"; then\n+    db-execute \"\n+    CREATE OR REPLACE FUNCTION fast_seqassign_init()\n+    RETURNS INT AS \\$\\$\n+      if 'cumulative_cnt' in SD:\n+        SD.pop('cumulative_cnt')\n+      return 0\n     \\$\\$ LANGUAGE plpythonu;\n \n-    CREATE OR REPLACE FUNCTION updateid(startid bigint, sid int, sids int[], base_ids bigint[], base_ids_noagg bigint[]) RETURNS bigint AS\n-    \\$\\$\n-    if '__count_1' in SD:\n-      a = SD['__count_2']\n-      b = SD['__count_1']\n-      SD['__count_2'] = SD['__count_2'] - 1\n-      if SD['__count_2'] < 0:\n-        SD.pop('__count_1')\n-      return startid+b-a\n-    else:\n-      for i in range(0, len(sids)):\n-        if sids[i] == sid:\n-          SD['__count_1'] = base_ids[i] - 1\n-          SD['__count_2'] = base_ids_noagg[i] - 1\n-      a = SD['__count_2']\n-      b = SD['__count_1']\n-      SD['__count_2'] = SD['__count_2'] - 1\n-      if SD['__count_2'] < 0:\n-        SD.pop('__count_1')\n-      return startid+b-a\n-\n+    CREATE OR REPLACE FUNCTION fast_seqassign_next(startid BIGINT, increment BIGINT, this_gpsid INT, gpsids INT[], cumulative_cnts BIGINT[], cnts BIGINT[])\n+    RETURNS BIGINT AS \\$\\$\n+      if 'cumulative_cnt' not in SD:\n+        for gpsid, cumulative_cnt, cnt in zip(gpsids, cumulative_cnts, cnts):\n+          if gpsid == this_gpsid:\n+            # assignment is done using the cumulative count for this segment and a decrementing counter from the size of this segment down to one\n+            SD['cumulative_cnt'] = cumulative_cnt\n+            SD['remaining'] = cnt\n+            break\n+      if 'cumulative_cnt' in SD and SD['remaining'] > 0:",
        "path": "database/db-driver/greenplum/db-assign_sequential_id",
        "position": null,
        "original_position": 67,
        "commit_id": "e3af172e1da5e306bbc2952823e292c29a1bcc7f",
        "original_commit_id": "a8a8bf61dcf3536a750c2e1af8fa9b66f62ca640",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Then most of the code here including `id = ...` and `return id` should be duplicated in the first `if`, no?\n",
        "created_at": "2016-02-23T21:34:09Z",
        "updated_at": "2016-02-24T04:51:10Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/496#discussion_r53853909",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/496",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/53853909"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/496#discussion_r53853909"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/496"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/53853965",
        "pull_request_review_id": null,
        "id": 53853965,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODUzOTY1",
        "diff_hunk": "@@ -1,97 +1,93 @@\n #!/usr/bin/env bash\n # db-assign_sequential_id -- Assigns a unique integer to every row for a table using PostgreSQL sequence generator\n # > eval \"$(db-parse \"$url\")\"\n-# > db-assign_sequential_id TABLE COLUMN BEGIN_ID\n+# > db-assign_sequential_id TABLE COLUMN BEGIN_ID [INCREMENT]\n ##\n set -euo pipefail\n \n [[ $# -gt 0 ]] || usage \"$0\" \"Missing TABLE\"\n [[ $# -gt 1 ]] || usage \"$0\" \"Missing COLUMN\"\n [[ $# -gt 2 ]] || usage \"$0\" \"Missing BEGIN_ID\"\n-Table=$1 Column=$2 BeginId=$3\n+Table=$1 Column=$2 BeginId=$3 Increment=${4:-1}\n \n # Use Greenplum PL/pgSQL and PL/Python UDFs to assign IDs fast\n # See: http://www.postgresql.org/docs/8.2/static/sql-createlanguage.html\n # See: http://www.postgresql.org/docs/8.2/static/plpgsql-overview.html\n # See: http://www.postgresql.org/docs/8.2/static/plpython-funcs.html\n-db-execute \"CREATE LANGUAGE plpgsql;\" || true\n-db-execute \"CREATE LANGUAGE plpythonu;\" || true\n-db-execute \"\n-    CREATE OR REPLACE FUNCTION clear_count_1(sid int) RETURNS int AS\n-    \\$\\$\n-    if '__count_1' in SD:\n-      SD['__count_1'] = -1\n-      return 1\n-    return 0\n+if db-supports_pg_lang \"plpgsql\" && db-supports_pg_lang \"plpythonu\"; then\n+    db-execute \"\n+    CREATE OR REPLACE FUNCTION fast_seqassign_init()\n+    RETURNS INT AS \\$\\$\n+      if 'cumulative_cnt' in SD:\n+        SD.pop('cumulative_cnt')\n+      return 0\n     \\$\\$ LANGUAGE plpythonu;\n \n-    CREATE OR REPLACE FUNCTION updateid(startid bigint, sid int, sids int[], base_ids bigint[], base_ids_noagg bigint[]) RETURNS bigint AS\n-    \\$\\$\n-    if '__count_1' in SD:\n-      a = SD['__count_2']\n-      b = SD['__count_1']\n-      SD['__count_2'] = SD['__count_2'] - 1\n-      if SD['__count_2'] < 0:\n-        SD.pop('__count_1')\n-      return startid+b-a\n-    else:\n-      for i in range(0, len(sids)):\n-        if sids[i] == sid:\n-          SD['__count_1'] = base_ids[i] - 1\n-          SD['__count_2'] = base_ids_noagg[i] - 1\n-      a = SD['__count_2']\n-      b = SD['__count_1']\n-      SD['__count_2'] = SD['__count_2'] - 1\n-      if SD['__count_2'] < 0:\n-        SD.pop('__count_1')\n-      return startid+b-a\n-\n+    CREATE OR REPLACE FUNCTION fast_seqassign_next(startid BIGINT, increment BIGINT, this_gpsid INT, gpsids INT[], cumulative_cnts BIGINT[], cnts BIGINT[])\n+    RETURNS BIGINT AS \\$\\$\n+      if 'cumulative_cnt' not in SD:\n+        for gpsid, cumulative_cnt, cnt in zip(gpsids, cumulative_cnts, cnts):\n+          if gpsid == this_gpsid:\n+            # assignment is done using the cumulative count for this segment and a decrementing counter from the size of this segment down to one\n+            SD['cumulative_cnt'] = cumulative_cnt\n+            SD['remaining'] = cnt\n+            break\n+      if 'cumulative_cnt' in SD and SD['remaining'] > 0:\n+        id = startid + increment * (SD['cumulative_cnt'] - SD['remaining'])\n+        SD['remaining'] = SD['remaining'] - 1\n+        if SD['remaining'] <= 0:\n+          SD.pop('cumulative_cnt')\n+        return id\n+      else:\n+        # XXX no segment was found\n+        raise plpy.ERROR('No non-empty id range was assigned for segment %s in the initial partition' % str(this_gpsid))\n     \\$\\$ LANGUAGE plpythonu;\n \n-    CREATE OR REPLACE FUNCTION fast_seqassign(tname character varying, cname character varying, startid bigint) RETURNS TEXT AS\n-    \\$\\$\n+    CREATE OR REPLACE FUNCTION fast_seqassign(tname CHARACTER VARYING, cname CHARACTER VARYING, startid BIGINT, increment BIGINT)\n+    RETURNS TEXT AS \\$\\$\n     BEGIN\n-      EXECUTE 'drop table if exists tmp_gpsid_count cascade;';\n-      EXECUTE 'drop table if exists tmp_gpsid_count_noagg cascade;';\n-      EXECUTE 'create table tmp_gpsid_count as select gp_segment_id as sid, count(clear_count_1(gp_segment_id)) as base_id from ' || quote_ident(tname) || ' group by gp_segment_id order by sid distributed by (sid);';\n-      EXECUTE 'create table tmp_gpsid_count_noagg as select * from tmp_gpsid_count distributed by (sid);';\n-      EXECUTE 'update tmp_gpsid_count as t set base_id = (SELECT SUM(base_id) FROM tmp_gpsid_count as t2 WHERE t2.sid <= t.sid);';\n-      RAISE NOTICE 'EXECUTING _fast_seqassign()...';\n-      EXECUTE 'select * from _fast_seqassign(''' || quote_ident(tname) || ''', ' || startid || ');';\n-      RETURN '';\n+      EXECUTE 'CREATE TEMPORARY VIEW dd_tmp_id_ranges_by_gpsid AS\n+               SELECT gpsid\n+                    , cnt\n+                    , SUM(cnt) OVER (ORDER BY gpsid) AS cumulative_cnt\n+                 FROM (\n+                    SELECT gp_segment_id AS gpsid\n+                         , COUNT(1)      AS cnt\n+                      FROM ' || QUOTE_IDENT(tname) || '\n+                     GROUP BY gpsid\n+                     ORDER BY gpsid\n+                 ) histogram;';\n+      DECLARE\n+        gpsids          INT[]    := ARRAY(SELECT gpsid          FROM dd_tmp_id_ranges_by_gpsid ORDER BY gpsid);\n+        cumulative_cnts BIGINT[] := ARRAY(SELECT cumulative_cnt FROM dd_tmp_id_ranges_by_gpsid ORDER BY gpsid);\n+        cnts            BIGINT[] := ARRAY(SELECT cnt            FROM dd_tmp_id_ranges_by_gpsid ORDER BY gpsid);\n+      BEGIN\n+        EXECUTE 'SELECT fast_seqassign_init();';\n+        EXECUTE 'UPDATE ' || tname || ' SET ' || cname || ' = fast_seqassign_next(\n+            ' || startid || ',\n+            ' || increment || ',\n+            gp_segment_id,\n+            ARRAY[' || ARRAY_TO_STRING(         gpsids, ',')::TEXT || '],\n+            ARRAY[' || ARRAY_TO_STRING(cumulative_cnts, ',')::TEXT || '],\n+            ARRAY[' || ARRAY_TO_STRING(           cnts, ',')::TEXT || ']\n+          );';\n+        RETURN 'fast_seqassign done for segments [' || ARRAY_TO_STRING(gpsids, ',') || ']' ||\n+               ' with counts [' || ARRAY_TO_STRING(cnts, ',') || ']';\n+      END;\n     END;\n     \\$\\$ LANGUAGE 'plpgsql';\n \n-    CREATE OR REPLACE FUNCTION _fast_seqassign(tname character varying, startid bigint)\n-    RETURNS TEXT AS\n-    \\$\\$\n-    DECLARE\n-      sids int[] :=  ARRAY(SELECT sid FROM tmp_gpsid_count ORDER BY sid);\n-      base_ids bigint[] :=  ARRAY(SELECT base_id FROM tmp_gpsid_count ORDER BY sid);\n-      base_ids_noagg bigint[] :=  ARRAY(SELECT base_id FROM tmp_gpsid_count_noagg ORDER BY sid);\n-      tsids text;\n-      tbase_ids text;\n-      tbase_ids_noagg text;\n-    BEGIN\n-      SELECT INTO tsids array_to_string(sids, ',');\n-      SELECT INTO tbase_ids array_to_string(base_ids, ',');\n-      SELECT INTO tbase_ids_noagg array_to_string(base_ids_noagg, ',');\n-      if ('update ' || tname || ' set ' || cname || ' = updateid(' || startid || ', gp_segment_id, ARRAY[' || tsids || '], ARRAY[' || tbase_ids || '], ARRAY[' || tbase_ids_noagg || ']);')::text is not null then\n-        EXECUTE 'update ' || tname || ' set ' || cname || ' = updateid(' || startid || ', gp_segment_id, ARRAY[' || tsids || '], ARRAY[' || tbase_ids || '], ARRAY[' || tbase_ids_noagg || ']);';\n-      end if;\n-      RETURN '';\n-    END;\n-    \\$\\$\n-    LANGUAGE 'plpgsql';\n-\n-    SELECT fast_seqassign('$Table', '$Column', $BeginId);\n-\" && exit\n+    SELECT fast_seqassign('$Table', '$Column', $BeginId, $Increment);\n+\"\n \n+else # if either plpgsql or plpythonu is not available\n # Fall back to using PostgreSQL sequence generator named after the table and column\n # See: http://www.postgresql.org/docs/current/static/sql-createsequence.html\n seq=\"dd_seq_${Table}_${Column}\"\n deepdive sql \"\n     DROP SEQUENCE IF EXISTS $seq CASCADE;\n-    CREATE TEMPORARY SEQUENCE $seq MINVALUE -1 START $BeginId;\n+    CREATE TEMPORARY SEQUENCE $seq INCREMENT BY $Increment MINVALUE $(($BeginId - 1)) START $BeginId;",
        "path": "database/db-driver/greenplum/db-assign_sequential_id",
        "position": null,
        "original_position": 156,
        "commit_id": "e3af172e1da5e306bbc2952823e292c29a1bcc7f",
        "original_commit_id": "a8a8bf61dcf3536a750c2e1af8fa9b66f62ca640",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "What is `- 1` for? Ditto for the postgresql one.\n",
        "created_at": "2016-02-23T21:34:27Z",
        "updated_at": "2016-02-24T04:51:10Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/496#discussion_r53853965",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/496",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/53853965"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/496#discussion_r53853965"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/496"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/53854339",
        "pull_request_review_id": null,
        "id": 53854339,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODU0MzM5",
        "diff_hunk": "@@ -1,97 +1,93 @@\n #!/usr/bin/env bash\n # db-assign_sequential_id -- Assigns a unique integer to every row for a table using PostgreSQL sequence generator\n # > eval \"$(db-parse \"$url\")\"\n-# > db-assign_sequential_id TABLE COLUMN BEGIN_ID\n+# > db-assign_sequential_id TABLE COLUMN BEGIN_ID [INCREMENT]\n ##\n set -euo pipefail\n \n [[ $# -gt 0 ]] || usage \"$0\" \"Missing TABLE\"\n [[ $# -gt 1 ]] || usage \"$0\" \"Missing COLUMN\"\n [[ $# -gt 2 ]] || usage \"$0\" \"Missing BEGIN_ID\"\n-Table=$1 Column=$2 BeginId=$3\n+Table=$1 Column=$2 BeginId=$3 Increment=${4:-1}\n \n # Use Greenplum PL/pgSQL and PL/Python UDFs to assign IDs fast\n # See: http://www.postgresql.org/docs/8.2/static/sql-createlanguage.html\n # See: http://www.postgresql.org/docs/8.2/static/plpgsql-overview.html\n # See: http://www.postgresql.org/docs/8.2/static/plpython-funcs.html\n-db-execute \"CREATE LANGUAGE plpgsql;\" || true\n-db-execute \"CREATE LANGUAGE plpythonu;\" || true\n-db-execute \"\n-    CREATE OR REPLACE FUNCTION clear_count_1(sid int) RETURNS int AS\n-    \\$\\$\n-    if '__count_1' in SD:\n-      SD['__count_1'] = -1\n-      return 1\n-    return 0\n+if db-supports_pg_lang \"plpgsql\" && db-supports_pg_lang \"plpythonu\"; then\n+    db-execute \"\n+    CREATE OR REPLACE FUNCTION fast_seqassign_init()\n+    RETURNS INT AS \\$\\$\n+      if 'cumulative_cnt' in SD:\n+        SD.pop('cumulative_cnt')\n+      return 0\n     \\$\\$ LANGUAGE plpythonu;\n \n-    CREATE OR REPLACE FUNCTION updateid(startid bigint, sid int, sids int[], base_ids bigint[], base_ids_noagg bigint[]) RETURNS bigint AS\n-    \\$\\$\n-    if '__count_1' in SD:\n-      a = SD['__count_2']\n-      b = SD['__count_1']\n-      SD['__count_2'] = SD['__count_2'] - 1\n-      if SD['__count_2'] < 0:\n-        SD.pop('__count_1')\n-      return startid+b-a\n-    else:\n-      for i in range(0, len(sids)):\n-        if sids[i] == sid:\n-          SD['__count_1'] = base_ids[i] - 1\n-          SD['__count_2'] = base_ids_noagg[i] - 1\n-      a = SD['__count_2']\n-      b = SD['__count_1']\n-      SD['__count_2'] = SD['__count_2'] - 1\n-      if SD['__count_2'] < 0:\n-        SD.pop('__count_1')\n-      return startid+b-a\n-\n+    CREATE OR REPLACE FUNCTION fast_seqassign_next(startid BIGINT, increment BIGINT, this_gpsid INT, gpsids INT[], cumulative_cnts BIGINT[], cnts BIGINT[])\n+    RETURNS BIGINT AS \\$\\$\n+      if 'cumulative_cnt' not in SD:\n+        for gpsid, cumulative_cnt, cnt in zip(gpsids, cumulative_cnts, cnts):\n+          if gpsid == this_gpsid:\n+            # assignment is done using the cumulative count for this segment and a decrementing counter from the size of this segment down to one\n+            SD['cumulative_cnt'] = cumulative_cnt\n+            SD['remaining'] = cnt\n+            break\n+      if 'cumulative_cnt' in SD and SD['remaining'] > 0:\n+        id = startid + increment * (SD['cumulative_cnt'] - SD['remaining'])\n+        SD['remaining'] = SD['remaining'] - 1\n+        if SD['remaining'] <= 0:\n+          SD.pop('cumulative_cnt')\n+        return id\n+      else:\n+        # XXX no segment was found\n+        raise plpy.ERROR('No non-empty id range was assigned for segment %s in the initial partition' % str(this_gpsid))\n     \\$\\$ LANGUAGE plpythonu;\n \n-    CREATE OR REPLACE FUNCTION fast_seqassign(tname character varying, cname character varying, startid bigint) RETURNS TEXT AS\n-    \\$\\$\n+    CREATE OR REPLACE FUNCTION fast_seqassign(tname CHARACTER VARYING, cname CHARACTER VARYING, startid BIGINT, increment BIGINT)\n+    RETURNS TEXT AS \\$\\$\n     BEGIN\n-      EXECUTE 'drop table if exists tmp_gpsid_count cascade;';\n-      EXECUTE 'drop table if exists tmp_gpsid_count_noagg cascade;';\n-      EXECUTE 'create table tmp_gpsid_count as select gp_segment_id as sid, count(clear_count_1(gp_segment_id)) as base_id from ' || quote_ident(tname) || ' group by gp_segment_id order by sid distributed by (sid);';\n-      EXECUTE 'create table tmp_gpsid_count_noagg as select * from tmp_gpsid_count distributed by (sid);';\n-      EXECUTE 'update tmp_gpsid_count as t set base_id = (SELECT SUM(base_id) FROM tmp_gpsid_count as t2 WHERE t2.sid <= t.sid);';\n-      RAISE NOTICE 'EXECUTING _fast_seqassign()...';\n-      EXECUTE 'select * from _fast_seqassign(''' || quote_ident(tname) || ''', ' || startid || ');';\n-      RETURN '';\n+      EXECUTE 'CREATE TEMPORARY VIEW dd_tmp_id_ranges_by_gpsid AS\n+               SELECT gpsid\n+                    , cnt\n+                    , SUM(cnt) OVER (ORDER BY gpsid) AS cumulative_cnt\n+                 FROM (\n+                    SELECT gp_segment_id AS gpsid\n+                         , COUNT(1)      AS cnt\n+                      FROM ' || QUOTE_IDENT(tname) || '\n+                     GROUP BY gpsid\n+                     ORDER BY gpsid\n+                 ) histogram;';\n+      DECLARE\n+        gpsids          INT[]    := ARRAY(SELECT gpsid          FROM dd_tmp_id_ranges_by_gpsid ORDER BY gpsid);\n+        cumulative_cnts BIGINT[] := ARRAY(SELECT cumulative_cnt FROM dd_tmp_id_ranges_by_gpsid ORDER BY gpsid);\n+        cnts            BIGINT[] := ARRAY(SELECT cnt            FROM dd_tmp_id_ranges_by_gpsid ORDER BY gpsid);\n+      BEGIN\n+        EXECUTE 'SELECT fast_seqassign_init();';\n+        EXECUTE 'UPDATE ' || tname || ' SET ' || cname || ' = fast_seqassign_next(\n+            ' || startid || ',\n+            ' || increment || ',\n+            gp_segment_id,\n+            ARRAY[' || ARRAY_TO_STRING(         gpsids, ',')::TEXT || '],\n+            ARRAY[' || ARRAY_TO_STRING(cumulative_cnts, ',')::TEXT || '],\n+            ARRAY[' || ARRAY_TO_STRING(           cnts, ',')::TEXT || ']\n+          );';\n+        RETURN 'fast_seqassign done for segments [' || ARRAY_TO_STRING(gpsids, ',') || ']' ||\n+               ' with counts [' || ARRAY_TO_STRING(cnts, ',') || ']';\n+      END;\n     END;\n     \\$\\$ LANGUAGE 'plpgsql';\n \n-    CREATE OR REPLACE FUNCTION _fast_seqassign(tname character varying, startid bigint)\n-    RETURNS TEXT AS\n-    \\$\\$\n-    DECLARE\n-      sids int[] :=  ARRAY(SELECT sid FROM tmp_gpsid_count ORDER BY sid);\n-      base_ids bigint[] :=  ARRAY(SELECT base_id FROM tmp_gpsid_count ORDER BY sid);\n-      base_ids_noagg bigint[] :=  ARRAY(SELECT base_id FROM tmp_gpsid_count_noagg ORDER BY sid);\n-      tsids text;\n-      tbase_ids text;\n-      tbase_ids_noagg text;\n-    BEGIN\n-      SELECT INTO tsids array_to_string(sids, ',');\n-      SELECT INTO tbase_ids array_to_string(base_ids, ',');\n-      SELECT INTO tbase_ids_noagg array_to_string(base_ids_noagg, ',');\n-      if ('update ' || tname || ' set ' || cname || ' = updateid(' || startid || ', gp_segment_id, ARRAY[' || tsids || '], ARRAY[' || tbase_ids || '], ARRAY[' || tbase_ids_noagg || ']);')::text is not null then\n-        EXECUTE 'update ' || tname || ' set ' || cname || ' = updateid(' || startid || ', gp_segment_id, ARRAY[' || tsids || '], ARRAY[' || tbase_ids || '], ARRAY[' || tbase_ids_noagg || ']);';\n-      end if;\n-      RETURN '';\n-    END;\n-    \\$\\$\n-    LANGUAGE 'plpgsql';\n-\n-    SELECT fast_seqassign('$Table', '$Column', $BeginId);\n-\" && exit\n+    SELECT fast_seqassign('$Table', '$Column', $BeginId, $Increment);\n+\"\n \n+else # if either plpgsql or plpythonu is not available\n # Fall back to using PostgreSQL sequence generator named after the table and column\n # See: http://www.postgresql.org/docs/current/static/sql-createsequence.html\n seq=\"dd_seq_${Table}_${Column}\"\n deepdive sql \"\n     DROP SEQUENCE IF EXISTS $seq CASCADE;\n-    CREATE TEMPORARY SEQUENCE $seq MINVALUE -1 START $BeginId;\n+    CREATE TEMPORARY SEQUENCE $seq INCREMENT BY $Increment MINVALUE $(($BeginId - 1)) START $BeginId;",
        "path": "database/db-driver/greenplum/db-assign_sequential_id",
        "position": null,
        "original_position": 156,
        "commit_id": "e3af172e1da5e306bbc2952823e292c29a1bcc7f",
        "original_commit_id": "a8a8bf61dcf3536a750c2e1af8fa9b66f62ca640",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "It's a relic from the Scala code I based my rewrite on.  I wasn't sure if other parts were depending on the particular value, but we can certainly take that out since I'm pretty sure it's unnecessary.\n",
        "created_at": "2016-02-23T21:37:09Z",
        "updated_at": "2016-02-24T04:51:10Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/496#discussion_r53854339",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/496",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/53854339"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/496#discussion_r53854339"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/496"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/53854793",
        "pull_request_review_id": null,
        "id": 53854793,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODU0Nzkz",
        "diff_hunk": "@@ -1,97 +1,93 @@\n #!/usr/bin/env bash\n # db-assign_sequential_id -- Assigns a unique integer to every row for a table using PostgreSQL sequence generator\n # > eval \"$(db-parse \"$url\")\"\n-# > db-assign_sequential_id TABLE COLUMN BEGIN_ID\n+# > db-assign_sequential_id TABLE COLUMN BEGIN_ID [INCREMENT]\n ##\n set -euo pipefail\n \n [[ $# -gt 0 ]] || usage \"$0\" \"Missing TABLE\"\n [[ $# -gt 1 ]] || usage \"$0\" \"Missing COLUMN\"\n [[ $# -gt 2 ]] || usage \"$0\" \"Missing BEGIN_ID\"\n-Table=$1 Column=$2 BeginId=$3\n+Table=$1 Column=$2 BeginId=$3 Increment=${4:-1}\n \n # Use Greenplum PL/pgSQL and PL/Python UDFs to assign IDs fast\n # See: http://www.postgresql.org/docs/8.2/static/sql-createlanguage.html\n # See: http://www.postgresql.org/docs/8.2/static/plpgsql-overview.html\n # See: http://www.postgresql.org/docs/8.2/static/plpython-funcs.html\n-db-execute \"CREATE LANGUAGE plpgsql;\" || true\n-db-execute \"CREATE LANGUAGE plpythonu;\" || true\n-db-execute \"\n-    CREATE OR REPLACE FUNCTION clear_count_1(sid int) RETURNS int AS\n-    \\$\\$\n-    if '__count_1' in SD:\n-      SD['__count_1'] = -1\n-      return 1\n-    return 0\n+if db-supports_pg_lang \"plpgsql\" && db-supports_pg_lang \"plpythonu\"; then\n+    db-execute \"\n+    CREATE OR REPLACE FUNCTION fast_seqassign_init()\n+    RETURNS INT AS \\$\\$\n+      if 'cumulative_cnt' in SD:\n+        SD.pop('cumulative_cnt')\n+      return 0\n     \\$\\$ LANGUAGE plpythonu;\n \n-    CREATE OR REPLACE FUNCTION updateid(startid bigint, sid int, sids int[], base_ids bigint[], base_ids_noagg bigint[]) RETURNS bigint AS\n-    \\$\\$\n-    if '__count_1' in SD:\n-      a = SD['__count_2']\n-      b = SD['__count_1']\n-      SD['__count_2'] = SD['__count_2'] - 1\n-      if SD['__count_2'] < 0:\n-        SD.pop('__count_1')\n-      return startid+b-a\n-    else:\n-      for i in range(0, len(sids)):\n-        if sids[i] == sid:\n-          SD['__count_1'] = base_ids[i] - 1\n-          SD['__count_2'] = base_ids_noagg[i] - 1\n-      a = SD['__count_2']\n-      b = SD['__count_1']\n-      SD['__count_2'] = SD['__count_2'] - 1\n-      if SD['__count_2'] < 0:\n-        SD.pop('__count_1')\n-      return startid+b-a\n-\n+    CREATE OR REPLACE FUNCTION fast_seqassign_next(startid BIGINT, increment BIGINT, this_gpsid INT, gpsids INT[], cumulative_cnts BIGINT[], cnts BIGINT[])\n+    RETURNS BIGINT AS \\$\\$\n+      if 'cumulative_cnt' not in SD:\n+        for gpsid, cumulative_cnt, cnt in zip(gpsids, cumulative_cnts, cnts):\n+          if gpsid == this_gpsid:\n+            # assignment is done using the cumulative count for this segment and a decrementing counter from the size of this segment down to one\n+            SD['cumulative_cnt'] = cumulative_cnt\n+            SD['remaining'] = cnt\n+            break\n+      if 'cumulative_cnt' in SD and SD['remaining'] > 0:\n+        id = startid + increment * (SD['cumulative_cnt'] - SD['remaining'])\n+        SD['remaining'] = SD['remaining'] - 1\n+        if SD['remaining'] <= 0:\n+          SD.pop('cumulative_cnt')",
        "path": "database/db-driver/greenplum/db-assign_sequential_id",
        "position": null,
        "original_position": 71,
        "commit_id": "e3af172e1da5e306bbc2952823e292c29a1bcc7f",
        "original_commit_id": "a8a8bf61dcf3536a750c2e1af8fa9b66f62ca640",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "`SD['remaining'] == 0` is the acceptable case. After that, there should be no more calls. However, if you do `SD.pop` here, the next call would enter the above `if 'cumulative_cnt' not in SD:` branch instead of the `raise` statement.\n\nIf you just remove these two lines, the next call would go to the `raise` statement.\n",
        "created_at": "2016-02-23T21:40:15Z",
        "updated_at": "2016-02-24T04:51:10Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/496#discussion_r53854793",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/496",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/53854793"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/496#discussion_r53854793"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/496"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/53855575",
        "pull_request_review_id": null,
        "id": 53855575,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODU1NTc1",
        "diff_hunk": "@@ -1,97 +1,93 @@\n #!/usr/bin/env bash\n # db-assign_sequential_id -- Assigns a unique integer to every row for a table using PostgreSQL sequence generator\n # > eval \"$(db-parse \"$url\")\"\n-# > db-assign_sequential_id TABLE COLUMN BEGIN_ID\n+# > db-assign_sequential_id TABLE COLUMN BEGIN_ID [INCREMENT]\n ##\n set -euo pipefail\n \n [[ $# -gt 0 ]] || usage \"$0\" \"Missing TABLE\"\n [[ $# -gt 1 ]] || usage \"$0\" \"Missing COLUMN\"\n [[ $# -gt 2 ]] || usage \"$0\" \"Missing BEGIN_ID\"\n-Table=$1 Column=$2 BeginId=$3\n+Table=$1 Column=$2 BeginId=$3 Increment=${4:-1}\n \n # Use Greenplum PL/pgSQL and PL/Python UDFs to assign IDs fast\n # See: http://www.postgresql.org/docs/8.2/static/sql-createlanguage.html\n # See: http://www.postgresql.org/docs/8.2/static/plpgsql-overview.html\n # See: http://www.postgresql.org/docs/8.2/static/plpython-funcs.html\n-db-execute \"CREATE LANGUAGE plpgsql;\" || true\n-db-execute \"CREATE LANGUAGE plpythonu;\" || true\n-db-execute \"\n-    CREATE OR REPLACE FUNCTION clear_count_1(sid int) RETURNS int AS\n-    \\$\\$\n-    if '__count_1' in SD:\n-      SD['__count_1'] = -1\n-      return 1\n-    return 0\n+if db-supports_pg_lang \"plpgsql\" && db-supports_pg_lang \"plpythonu\"; then\n+    db-execute \"\n+    CREATE OR REPLACE FUNCTION fast_seqassign_init()\n+    RETURNS INT AS \\$\\$\n+      if 'cumulative_cnt' in SD:\n+        SD.pop('cumulative_cnt')\n+      return 0\n     \\$\\$ LANGUAGE plpythonu;\n \n-    CREATE OR REPLACE FUNCTION updateid(startid bigint, sid int, sids int[], base_ids bigint[], base_ids_noagg bigint[]) RETURNS bigint AS\n-    \\$\\$\n-    if '__count_1' in SD:\n-      a = SD['__count_2']\n-      b = SD['__count_1']\n-      SD['__count_2'] = SD['__count_2'] - 1\n-      if SD['__count_2'] < 0:\n-        SD.pop('__count_1')\n-      return startid+b-a\n-    else:\n-      for i in range(0, len(sids)):\n-        if sids[i] == sid:\n-          SD['__count_1'] = base_ids[i] - 1\n-          SD['__count_2'] = base_ids_noagg[i] - 1\n-      a = SD['__count_2']\n-      b = SD['__count_1']\n-      SD['__count_2'] = SD['__count_2'] - 1\n-      if SD['__count_2'] < 0:\n-        SD.pop('__count_1')\n-      return startid+b-a\n-\n+    CREATE OR REPLACE FUNCTION fast_seqassign_next(startid BIGINT, increment BIGINT, this_gpsid INT, gpsids INT[], cumulative_cnts BIGINT[], cnts BIGINT[])\n+    RETURNS BIGINT AS \\$\\$\n+      if 'cumulative_cnt' not in SD:\n+        for gpsid, cumulative_cnt, cnt in zip(gpsids, cumulative_cnts, cnts):\n+          if gpsid == this_gpsid:\n+            # assignment is done using the cumulative count for this segment and a decrementing counter from the size of this segment down to one\n+            SD['cumulative_cnt'] = cumulative_cnt\n+            SD['remaining'] = cnt\n+            break\n+      if 'cumulative_cnt' in SD and SD['remaining'] > 0:",
        "path": "database/db-driver/greenplum/db-assign_sequential_id",
        "position": null,
        "original_position": 67,
        "commit_id": "e3af172e1da5e306bbc2952823e292c29a1bcc7f",
        "original_commit_id": "a8a8bf61dcf3536a750c2e1af8fa9b66f62ca640",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I see... Maybe check `'cumulative_cnt' in SD` at the end of the first `if` block then.\n\nFor further optimization, we could also get rid of the multiplication below. Something like\n\n```\nid = SD['cur_val']\nSD['cur_val'] += increment\nreturn id\n```\n",
        "created_at": "2016-02-23T21:45:30Z",
        "updated_at": "2016-02-24T04:51:10Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/496#discussion_r53855575",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/496",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/53855575"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/496#discussion_r53855575"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/496"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/53855898",
        "pull_request_review_id": null,
        "id": 53855898,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODU1ODk4",
        "diff_hunk": "@@ -1,97 +1,93 @@\n #!/usr/bin/env bash\n # db-assign_sequential_id -- Assigns a unique integer to every row for a table using PostgreSQL sequence generator\n # > eval \"$(db-parse \"$url\")\"\n-# > db-assign_sequential_id TABLE COLUMN BEGIN_ID\n+# > db-assign_sequential_id TABLE COLUMN BEGIN_ID [INCREMENT]\n ##\n set -euo pipefail\n \n [[ $# -gt 0 ]] || usage \"$0\" \"Missing TABLE\"\n [[ $# -gt 1 ]] || usage \"$0\" \"Missing COLUMN\"\n [[ $# -gt 2 ]] || usage \"$0\" \"Missing BEGIN_ID\"\n-Table=$1 Column=$2 BeginId=$3\n+Table=$1 Column=$2 BeginId=$3 Increment=${4:-1}\n \n # Use Greenplum PL/pgSQL and PL/Python UDFs to assign IDs fast\n # See: http://www.postgresql.org/docs/8.2/static/sql-createlanguage.html\n # See: http://www.postgresql.org/docs/8.2/static/plpgsql-overview.html\n # See: http://www.postgresql.org/docs/8.2/static/plpython-funcs.html\n-db-execute \"CREATE LANGUAGE plpgsql;\" || true\n-db-execute \"CREATE LANGUAGE plpythonu;\" || true\n-db-execute \"\n-    CREATE OR REPLACE FUNCTION clear_count_1(sid int) RETURNS int AS\n-    \\$\\$\n-    if '__count_1' in SD:\n-      SD['__count_1'] = -1\n-      return 1\n-    return 0\n+if db-supports_pg_lang \"plpgsql\" && db-supports_pg_lang \"plpythonu\"; then\n+    db-execute \"\n+    CREATE OR REPLACE FUNCTION fast_seqassign_init()\n+    RETURNS INT AS \\$\\$\n+      if 'cumulative_cnt' in SD:\n+        SD.pop('cumulative_cnt')\n+      return 0\n     \\$\\$ LANGUAGE plpythonu;\n \n-    CREATE OR REPLACE FUNCTION updateid(startid bigint, sid int, sids int[], base_ids bigint[], base_ids_noagg bigint[]) RETURNS bigint AS\n-    \\$\\$\n-    if '__count_1' in SD:\n-      a = SD['__count_2']\n-      b = SD['__count_1']\n-      SD['__count_2'] = SD['__count_2'] - 1\n-      if SD['__count_2'] < 0:\n-        SD.pop('__count_1')\n-      return startid+b-a\n-    else:\n-      for i in range(0, len(sids)):\n-        if sids[i] == sid:\n-          SD['__count_1'] = base_ids[i] - 1\n-          SD['__count_2'] = base_ids_noagg[i] - 1\n-      a = SD['__count_2']\n-      b = SD['__count_1']\n-      SD['__count_2'] = SD['__count_2'] - 1\n-      if SD['__count_2'] < 0:\n-        SD.pop('__count_1')\n-      return startid+b-a\n-\n+    CREATE OR REPLACE FUNCTION fast_seqassign_next(startid BIGINT, increment BIGINT, this_gpsid INT, gpsids INT[], cumulative_cnts BIGINT[], cnts BIGINT[])\n+    RETURNS BIGINT AS \\$\\$\n+      if 'cumulative_cnt' not in SD:\n+        for gpsid, cumulative_cnt, cnt in zip(gpsids, cumulative_cnts, cnts):\n+          if gpsid == this_gpsid:\n+            # assignment is done using the cumulative count for this segment and a decrementing counter from the size of this segment down to one\n+            SD['cumulative_cnt'] = cumulative_cnt\n+            SD['remaining'] = cnt\n+            break\n+      if 'cumulative_cnt' in SD and SD['remaining'] > 0:\n+        id = startid + increment * (SD['cumulative_cnt'] - SD['remaining'])\n+        SD['remaining'] = SD['remaining'] - 1\n+        if SD['remaining'] <= 0:\n+          SD.pop('cumulative_cnt')\n+        return id\n+      else:\n+        # XXX no segment was found\n+        raise plpy.ERROR('No non-empty id range was assigned for segment %s in the initial partition' % str(this_gpsid))\n     \\$\\$ LANGUAGE plpythonu;\n \n-    CREATE OR REPLACE FUNCTION fast_seqassign(tname character varying, cname character varying, startid bigint) RETURNS TEXT AS\n-    \\$\\$\n+    CREATE OR REPLACE FUNCTION fast_seqassign(tname CHARACTER VARYING, cname CHARACTER VARYING, startid BIGINT, increment BIGINT)\n+    RETURNS TEXT AS \\$\\$\n     BEGIN\n-      EXECUTE 'drop table if exists tmp_gpsid_count cascade;';\n-      EXECUTE 'drop table if exists tmp_gpsid_count_noagg cascade;';\n-      EXECUTE 'create table tmp_gpsid_count as select gp_segment_id as sid, count(clear_count_1(gp_segment_id)) as base_id from ' || quote_ident(tname) || ' group by gp_segment_id order by sid distributed by (sid);';\n-      EXECUTE 'create table tmp_gpsid_count_noagg as select * from tmp_gpsid_count distributed by (sid);';\n-      EXECUTE 'update tmp_gpsid_count as t set base_id = (SELECT SUM(base_id) FROM tmp_gpsid_count as t2 WHERE t2.sid <= t.sid);';\n-      RAISE NOTICE 'EXECUTING _fast_seqassign()...';\n-      EXECUTE 'select * from _fast_seqassign(''' || quote_ident(tname) || ''', ' || startid || ');';\n-      RETURN '';\n+      EXECUTE 'CREATE TEMPORARY VIEW dd_tmp_id_ranges_by_gpsid AS\n+               SELECT gpsid\n+                    , cnt\n+                    , SUM(cnt) OVER (ORDER BY gpsid) AS cumulative_cnt\n+                 FROM (\n+                    SELECT gp_segment_id AS gpsid\n+                         , COUNT(1)      AS cnt\n+                      FROM ' || QUOTE_IDENT(tname) || '\n+                     GROUP BY gpsid\n+                     ORDER BY gpsid\n+                 ) histogram;';\n+      DECLARE\n+        gpsids          INT[]    := ARRAY(SELECT gpsid          FROM dd_tmp_id_ranges_by_gpsid ORDER BY gpsid);\n+        cumulative_cnts BIGINT[] := ARRAY(SELECT cumulative_cnt FROM dd_tmp_id_ranges_by_gpsid ORDER BY gpsid);\n+        cnts            BIGINT[] := ARRAY(SELECT cnt            FROM dd_tmp_id_ranges_by_gpsid ORDER BY gpsid);\n+      BEGIN\n+        EXECUTE 'SELECT fast_seqassign_init();';\n+        EXECUTE 'UPDATE ' || tname || ' SET ' || cname || ' = fast_seqassign_next(\n+            ' || startid || ',\n+            ' || increment || ',\n+            gp_segment_id,\n+            ARRAY[' || ARRAY_TO_STRING(         gpsids, ',')::TEXT || '],\n+            ARRAY[' || ARRAY_TO_STRING(cumulative_cnts, ',')::TEXT || '],\n+            ARRAY[' || ARRAY_TO_STRING(           cnts, ',')::TEXT || ']\n+          );';\n+        RETURN 'fast_seqassign done for segments [' || ARRAY_TO_STRING(gpsids, ',') || ']' ||\n+               ' with counts [' || ARRAY_TO_STRING(cnts, ',') || ']';\n+      END;\n     END;\n     \\$\\$ LANGUAGE 'plpgsql';\n \n-    CREATE OR REPLACE FUNCTION _fast_seqassign(tname character varying, startid bigint)\n-    RETURNS TEXT AS\n-    \\$\\$\n-    DECLARE\n-      sids int[] :=  ARRAY(SELECT sid FROM tmp_gpsid_count ORDER BY sid);\n-      base_ids bigint[] :=  ARRAY(SELECT base_id FROM tmp_gpsid_count ORDER BY sid);\n-      base_ids_noagg bigint[] :=  ARRAY(SELECT base_id FROM tmp_gpsid_count_noagg ORDER BY sid);\n-      tsids text;\n-      tbase_ids text;\n-      tbase_ids_noagg text;\n-    BEGIN\n-      SELECT INTO tsids array_to_string(sids, ',');\n-      SELECT INTO tbase_ids array_to_string(base_ids, ',');\n-      SELECT INTO tbase_ids_noagg array_to_string(base_ids_noagg, ',');\n-      if ('update ' || tname || ' set ' || cname || ' = updateid(' || startid || ', gp_segment_id, ARRAY[' || tsids || '], ARRAY[' || tbase_ids || '], ARRAY[' || tbase_ids_noagg || ']);')::text is not null then\n-        EXECUTE 'update ' || tname || ' set ' || cname || ' = updateid(' || startid || ', gp_segment_id, ARRAY[' || tsids || '], ARRAY[' || tbase_ids || '], ARRAY[' || tbase_ids_noagg || ']);';\n-      end if;\n-      RETURN '';\n-    END;\n-    \\$\\$\n-    LANGUAGE 'plpgsql';\n-\n-    SELECT fast_seqassign('$Table', '$Column', $BeginId);\n-\" && exit\n+    SELECT fast_seqassign('$Table', '$Column', $BeginId, $Increment);\n+\"\n \n+else # if either plpgsql or plpythonu is not available\n # Fall back to using PostgreSQL sequence generator named after the table and column\n # See: http://www.postgresql.org/docs/current/static/sql-createsequence.html\n seq=\"dd_seq_${Table}_${Column}\"\n deepdive sql \"\n     DROP SEQUENCE IF EXISTS $seq CASCADE;\n-    CREATE TEMPORARY SEQUENCE $seq MINVALUE -1 START $BeginId;\n+    CREATE TEMPORARY SEQUENCE $seq INCREMENT BY $Increment MINVALUE $(($BeginId - 1)) START $BeginId;",
        "path": "database/db-driver/greenplum/db-assign_sequential_id",
        "position": null,
        "original_position": 156,
        "commit_id": "e3af172e1da5e306bbc2952823e292c29a1bcc7f",
        "original_commit_id": "a8a8bf61dcf3536a750c2e1af8fa9b66f62ca640",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Yeah, the docs suggest that `minvalue` is usually equal to `start`.\nhttp://www.postgresql.org/docs/8.3/static/sql-createsequence.html\n",
        "created_at": "2016-02-23T21:47:42Z",
        "updated_at": "2016-02-24T04:51:10Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/496#discussion_r53855898",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/496",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/53855898"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/496#discussion_r53855898"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/496"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/53856344",
        "pull_request_review_id": null,
        "id": 53856344,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODU2MzQ0",
        "diff_hunk": "@@ -1,97 +1,93 @@\n #!/usr/bin/env bash\n # db-assign_sequential_id -- Assigns a unique integer to every row for a table using PostgreSQL sequence generator\n # > eval \"$(db-parse \"$url\")\"\n-# > db-assign_sequential_id TABLE COLUMN BEGIN_ID\n+# > db-assign_sequential_id TABLE COLUMN BEGIN_ID [INCREMENT]\n ##\n set -euo pipefail\n \n [[ $# -gt 0 ]] || usage \"$0\" \"Missing TABLE\"\n [[ $# -gt 1 ]] || usage \"$0\" \"Missing COLUMN\"\n [[ $# -gt 2 ]] || usage \"$0\" \"Missing BEGIN_ID\"\n-Table=$1 Column=$2 BeginId=$3\n+Table=$1 Column=$2 BeginId=$3 Increment=${4:-1}\n \n # Use Greenplum PL/pgSQL and PL/Python UDFs to assign IDs fast\n # See: http://www.postgresql.org/docs/8.2/static/sql-createlanguage.html\n # See: http://www.postgresql.org/docs/8.2/static/plpgsql-overview.html\n # See: http://www.postgresql.org/docs/8.2/static/plpython-funcs.html\n-db-execute \"CREATE LANGUAGE plpgsql;\" || true\n-db-execute \"CREATE LANGUAGE plpythonu;\" || true\n-db-execute \"\n-    CREATE OR REPLACE FUNCTION clear_count_1(sid int) RETURNS int AS\n-    \\$\\$\n-    if '__count_1' in SD:\n-      SD['__count_1'] = -1\n-      return 1\n-    return 0\n+if db-supports_pg_lang \"plpgsql\" && db-supports_pg_lang \"plpythonu\"; then\n+    db-execute \"\n+    CREATE OR REPLACE FUNCTION fast_seqassign_init()\n+    RETURNS INT AS \\$\\$\n+      if 'cumulative_cnt' in SD:\n+        SD.pop('cumulative_cnt')\n+      return 0\n     \\$\\$ LANGUAGE plpythonu;\n \n-    CREATE OR REPLACE FUNCTION updateid(startid bigint, sid int, sids int[], base_ids bigint[], base_ids_noagg bigint[]) RETURNS bigint AS\n-    \\$\\$\n-    if '__count_1' in SD:\n-      a = SD['__count_2']\n-      b = SD['__count_1']\n-      SD['__count_2'] = SD['__count_2'] - 1\n-      if SD['__count_2'] < 0:\n-        SD.pop('__count_1')\n-      return startid+b-a\n-    else:\n-      for i in range(0, len(sids)):\n-        if sids[i] == sid:\n-          SD['__count_1'] = base_ids[i] - 1\n-          SD['__count_2'] = base_ids_noagg[i] - 1\n-      a = SD['__count_2']\n-      b = SD['__count_1']\n-      SD['__count_2'] = SD['__count_2'] - 1\n-      if SD['__count_2'] < 0:\n-        SD.pop('__count_1')\n-      return startid+b-a\n-\n+    CREATE OR REPLACE FUNCTION fast_seqassign_next(startid BIGINT, increment BIGINT, this_gpsid INT, gpsids INT[], cumulative_cnts BIGINT[], cnts BIGINT[])\n+    RETURNS BIGINT AS \\$\\$\n+      if 'cumulative_cnt' not in SD:\n+        for gpsid, cumulative_cnt, cnt in zip(gpsids, cumulative_cnts, cnts):\n+          if gpsid == this_gpsid:\n+            # assignment is done using the cumulative count for this segment and a decrementing counter from the size of this segment down to one\n+            SD['cumulative_cnt'] = cumulative_cnt\n+            SD['remaining'] = cnt\n+            break\n+      if 'cumulative_cnt' in SD and SD['remaining'] > 0:\n+        id = startid + increment * (SD['cumulative_cnt'] - SD['remaining'])\n+        SD['remaining'] = SD['remaining'] - 1\n+        if SD['remaining'] <= 0:\n+          SD.pop('cumulative_cnt')",
        "path": "database/db-driver/greenplum/db-assign_sequential_id",
        "position": null,
        "original_position": 71,
        "commit_id": "e3af172e1da5e306bbc2952823e292c29a1bcc7f",
        "original_commit_id": "a8a8bf61dcf3536a750c2e1af8fa9b66f62ca640",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Makes sense. We can turn the else block to handle both out-of-range cases if you think that's clearer.\n",
        "created_at": "2016-02-23T21:50:25Z",
        "updated_at": "2016-02-24T04:51:10Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/496#discussion_r53856344",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/496",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/53856344"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/496#discussion_r53856344"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/496"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/53856831",
        "pull_request_review_id": null,
        "id": 53856831,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODU2ODMx",
        "diff_hunk": "@@ -1,97 +1,93 @@\n #!/usr/bin/env bash\n # db-assign_sequential_id -- Assigns a unique integer to every row for a table using PostgreSQL sequence generator\n # > eval \"$(db-parse \"$url\")\"\n-# > db-assign_sequential_id TABLE COLUMN BEGIN_ID\n+# > db-assign_sequential_id TABLE COLUMN BEGIN_ID [INCREMENT]\n ##\n set -euo pipefail\n \n [[ $# -gt 0 ]] || usage \"$0\" \"Missing TABLE\"\n [[ $# -gt 1 ]] || usage \"$0\" \"Missing COLUMN\"\n [[ $# -gt 2 ]] || usage \"$0\" \"Missing BEGIN_ID\"\n-Table=$1 Column=$2 BeginId=$3\n+Table=$1 Column=$2 BeginId=$3 Increment=${4:-1}\n \n # Use Greenplum PL/pgSQL and PL/Python UDFs to assign IDs fast\n # See: http://www.postgresql.org/docs/8.2/static/sql-createlanguage.html\n # See: http://www.postgresql.org/docs/8.2/static/plpgsql-overview.html\n # See: http://www.postgresql.org/docs/8.2/static/plpython-funcs.html\n-db-execute \"CREATE LANGUAGE plpgsql;\" || true\n-db-execute \"CREATE LANGUAGE plpythonu;\" || true\n-db-execute \"\n-    CREATE OR REPLACE FUNCTION clear_count_1(sid int) RETURNS int AS\n-    \\$\\$\n-    if '__count_1' in SD:\n-      SD['__count_1'] = -1\n-      return 1\n-    return 0\n+if db-supports_pg_lang \"plpgsql\" && db-supports_pg_lang \"plpythonu\"; then\n+    db-execute \"\n+    CREATE OR REPLACE FUNCTION fast_seqassign_init()\n+    RETURNS INT AS \\$\\$\n+      if 'cumulative_cnt' in SD:\n+        SD.pop('cumulative_cnt')\n+      return 0\n     \\$\\$ LANGUAGE plpythonu;\n \n-    CREATE OR REPLACE FUNCTION updateid(startid bigint, sid int, sids int[], base_ids bigint[], base_ids_noagg bigint[]) RETURNS bigint AS\n-    \\$\\$\n-    if '__count_1' in SD:\n-      a = SD['__count_2']\n-      b = SD['__count_1']\n-      SD['__count_2'] = SD['__count_2'] - 1\n-      if SD['__count_2'] < 0:\n-        SD.pop('__count_1')\n-      return startid+b-a\n-    else:\n-      for i in range(0, len(sids)):\n-        if sids[i] == sid:\n-          SD['__count_1'] = base_ids[i] - 1\n-          SD['__count_2'] = base_ids_noagg[i] - 1\n-      a = SD['__count_2']\n-      b = SD['__count_1']\n-      SD['__count_2'] = SD['__count_2'] - 1\n-      if SD['__count_2'] < 0:\n-        SD.pop('__count_1')\n-      return startid+b-a\n-\n+    CREATE OR REPLACE FUNCTION fast_seqassign_next(startid BIGINT, increment BIGINT, this_gpsid INT, gpsids INT[], cumulative_cnts BIGINT[], cnts BIGINT[])\n+    RETURNS BIGINT AS \\$\\$\n+      if 'cumulative_cnt' not in SD:\n+        for gpsid, cumulative_cnt, cnt in zip(gpsids, cumulative_cnts, cnts):\n+          if gpsid == this_gpsid:\n+            # assignment is done using the cumulative count for this segment and a decrementing counter from the size of this segment down to one\n+            SD['cumulative_cnt'] = cumulative_cnt\n+            SD['remaining'] = cnt\n+            break\n+      if 'cumulative_cnt' in SD and SD['remaining'] > 0:\n+        id = startid + increment * (SD['cumulative_cnt'] - SD['remaining'])\n+        SD['remaining'] = SD['remaining'] - 1\n+        if SD['remaining'] <= 0:\n+          SD.pop('cumulative_cnt')",
        "path": "database/db-driver/greenplum/db-assign_sequential_id",
        "position": null,
        "original_position": 71,
        "commit_id": "e3af172e1da5e306bbc2952823e292c29a1bcc7f",
        "original_commit_id": "a8a8bf61dcf3536a750c2e1af8fa9b66f62ca640",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Yep :) (Looks like it current does that already...)\n",
        "created_at": "2016-02-23T21:53:45Z",
        "updated_at": "2016-02-24T04:51:10Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/496#discussion_r53856831",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/496",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/53856831"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/496#discussion_r53856831"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/496"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/53857320",
        "pull_request_review_id": null,
        "id": 53857320,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODU3MzIw",
        "diff_hunk": "@@ -1,97 +1,93 @@\n #!/usr/bin/env bash\n # db-assign_sequential_id -- Assigns a unique integer to every row for a table using PostgreSQL sequence generator\n # > eval \"$(db-parse \"$url\")\"\n-# > db-assign_sequential_id TABLE COLUMN BEGIN_ID\n+# > db-assign_sequential_id TABLE COLUMN BEGIN_ID [INCREMENT]\n ##\n set -euo pipefail\n \n [[ $# -gt 0 ]] || usage \"$0\" \"Missing TABLE\"\n [[ $# -gt 1 ]] || usage \"$0\" \"Missing COLUMN\"\n [[ $# -gt 2 ]] || usage \"$0\" \"Missing BEGIN_ID\"\n-Table=$1 Column=$2 BeginId=$3\n+Table=$1 Column=$2 BeginId=$3 Increment=${4:-1}\n \n # Use Greenplum PL/pgSQL and PL/Python UDFs to assign IDs fast\n # See: http://www.postgresql.org/docs/8.2/static/sql-createlanguage.html\n # See: http://www.postgresql.org/docs/8.2/static/plpgsql-overview.html\n # See: http://www.postgresql.org/docs/8.2/static/plpython-funcs.html\n-db-execute \"CREATE LANGUAGE plpgsql;\" || true\n-db-execute \"CREATE LANGUAGE plpythonu;\" || true\n-db-execute \"\n-    CREATE OR REPLACE FUNCTION clear_count_1(sid int) RETURNS int AS\n-    \\$\\$\n-    if '__count_1' in SD:\n-      SD['__count_1'] = -1\n-      return 1\n-    return 0\n+if db-supports_pg_lang \"plpgsql\" && db-supports_pg_lang \"plpythonu\"; then\n+    db-execute \"\n+    CREATE OR REPLACE FUNCTION fast_seqassign_init()\n+    RETURNS INT AS \\$\\$\n+      if 'cumulative_cnt' in SD:\n+        SD.pop('cumulative_cnt')\n+      return 0\n     \\$\\$ LANGUAGE plpythonu;\n \n-    CREATE OR REPLACE FUNCTION updateid(startid bigint, sid int, sids int[], base_ids bigint[], base_ids_noagg bigint[]) RETURNS bigint AS\n-    \\$\\$\n-    if '__count_1' in SD:\n-      a = SD['__count_2']\n-      b = SD['__count_1']\n-      SD['__count_2'] = SD['__count_2'] - 1\n-      if SD['__count_2'] < 0:\n-        SD.pop('__count_1')\n-      return startid+b-a\n-    else:\n-      for i in range(0, len(sids)):\n-        if sids[i] == sid:\n-          SD['__count_1'] = base_ids[i] - 1\n-          SD['__count_2'] = base_ids_noagg[i] - 1\n-      a = SD['__count_2']\n-      b = SD['__count_1']\n-      SD['__count_2'] = SD['__count_2'] - 1\n-      if SD['__count_2'] < 0:\n-        SD.pop('__count_1')\n-      return startid+b-a\n-\n+    CREATE OR REPLACE FUNCTION fast_seqassign_next(startid BIGINT, increment BIGINT, this_gpsid INT, gpsids INT[], cumulative_cnts BIGINT[], cnts BIGINT[])\n+    RETURNS BIGINT AS \\$\\$\n+      if 'cumulative_cnt' not in SD:\n+        for gpsid, cumulative_cnt, cnt in zip(gpsids, cumulative_cnts, cnts):\n+          if gpsid == this_gpsid:\n+            # assignment is done using the cumulative count for this segment and a decrementing counter from the size of this segment down to one\n+            SD['cumulative_cnt'] = cumulative_cnt\n+            SD['remaining'] = cnt\n+            break\n+      if 'cumulative_cnt' in SD and SD['remaining'] > 0:",
        "path": "database/db-driver/greenplum/db-assign_sequential_id",
        "position": null,
        "original_position": 67,
        "commit_id": "e3af172e1da5e306bbc2952823e292c29a1bcc7f",
        "original_commit_id": "a8a8bf61dcf3536a750c2e1af8fa9b66f62ca640",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Sounds good.  I see your concern for checking SD twice on the hot-path.  I'll also get rid of redundant ops.  I guess that was my original intention, but my brain got overheated decrypting the original code... :)\n",
        "created_at": "2016-02-23T21:57:02Z",
        "updated_at": "2016-02-24T04:51:10Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/496#discussion_r53857320",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/496",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/53857320"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/496#discussion_r53857320"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/496"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/54305457",
        "pull_request_review_id": null,
        "id": 54305457,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzA1NDU3",
        "diff_hunk": "@@ -88,12 +88,17 @@ Then we compile our application, as we must do whenever we change `app.ddlog`:\n deepdive compile\n ```\n \n-Finally, we tell DeepDive to execute the steps to load the `articles` table:\n+Finally, we tell DeepDive to execute the steps to load the `articles` table using the `input/articles.tsv.sh` script. You must have the [full corpus](http://research.signalmedia.co/newsir16/signal-dataset.html) downloaded.\n \n ```bash\n deepdive do articles\n ```\n \n+Alternatively, a sample of 1000 articles can be loaded directly by issuing the following command:\n+```bash\n+deepdive load articles input/articles-1000.tsv.bz2",
        "path": "doc/example-spouse.md",
        "position": 13,
        "original_position": 13,
        "commit_id": "7cd5885d43bee20a07e663d69d33b35d4ba763f8",
        "original_commit_id": "eb3d44fe962d399bc64c9894c81fa2901e6ff8b2",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Thanks for the edits!  This is a great!\n\nIn fact you need a `deepdive mark done articles` after the `deepdive load` to make it equivalent to `deepdive do articles`.  Could you revise this block as well as the one in quickstart?\n",
        "created_at": "2016-02-26T21:36:51Z",
        "updated_at": "2016-05-25T08:13:58Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/506#discussion_r54305457",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/506",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/54305457"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/506#discussion_r54305457"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/506"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/54305726",
        "pull_request_review_id": null,
        "id": 54305726,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzA1NzI2",
        "diff_hunk": "@@ -165,7 +170,7 @@ Note that we declare a compound key of `(doc_id, sentence_index)` for each sente\n -->\n \n Next we declare a DDlog function which takes in the `doc_id` and `content` for an article and returns rows conforming to the sentences schema we just declared, using the **user-defined function (UDF)** in `udf/nlp_markup.sh`.\n-This UDF is a Bash script which calls [our own wrapper around CoreNLP](https://github.com/HazyResearch/bazaar/tree/master/parser).\n+This UDF is a Bash script which calls [our own wrapper around CoreNLP](https://github.com/HazyResearch/bazaar/tree/master/parser). The CoreNLP library requires Java 8 to run.",
        "path": "doc/example-spouse.md",
        "position": 24,
        "original_position": 24,
        "commit_id": "7cd5885d43bee20a07e663d69d33b35d4ba763f8",
        "original_commit_id": "eb3d44fe962d399bc64c9894c81fa2901e6ff8b2",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "It'd be nice to mention this Java 8 requirement in the first section for preparation/installation as well as in the quickstart, perhaps pointing to this page? http://www.oracle.com/technetwork/java/javase/downloads/jre8-downloads-2133155.html\n",
        "created_at": "2016-02-26T21:39:32Z",
        "updated_at": "2016-05-25T08:13:58Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/506#discussion_r54305726",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/506",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/54305726"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/506#discussion_r54305726"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/506"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/54306009",
        "pull_request_review_id": null,
        "id": 54306009,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzA2MDA5",
        "diff_hunk": "@@ -68,18 +68,18 @@ app.ddlog  db.url  deepdive.conf  input/  labeling/  mindbender/  README.md  udf\n ```\n \n ### 1. Load input\n-\n-You can find some of our sampled datasets under `input/`.\n-You can also [download the full corpus][corpus], but let's proceed with the one that has 1000 sampled articles:\n+First, you have to compile the DeepDive application using the following command:\n \n ```bash\n-ln -s articles-1000.tsv.bz2 input/articles.tsv.bz2\n+deepdive compile\n ```\n+Once it has compiled with no error, you can run the following ```deepdive``` commands.\n+\n+You can find some of our sampled datasets under `input/`.\n+You can also [download the full corpus][corpus], but let's proceed with the one that has 1000 sampled articles. Run the following command to load the sampled articles into DeepDive:\n ```bash\n-deepdive do articles\n+deepdive load articles input/articles-1000.tsv.bz2",
        "path": "doc/quickstart.md",
        "position": 19,
        "original_position": 19,
        "commit_id": "7cd5885d43bee20a07e663d69d33b35d4ba763f8",
        "original_commit_id": "eb3d44fe962d399bc64c9894c81fa2901e6ff8b2",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Maybe for a quicker quickstart, we could also just tell them to load the preprocessed sentences, not having to mention Java 8, etc.\n\n``` bash\ndeepdive do init/app\ndeepdive create table sentences\ndeepdive load sentences input/sentences-1000.tsv.bz2\ndeepdive mark done sentences\n```\n\n(Which doesn't look as simple as before though..)\n",
        "created_at": "2016-02-26T21:42:02Z",
        "updated_at": "2016-05-25T08:13:58Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/506#discussion_r54306009",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/506",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/54306009"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/506#discussion_r54306009"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/506"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/54306397",
        "pull_request_review_id": null,
        "id": 54306397,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzA2Mzk3",
        "diff_hunk": "@@ -88,12 +88,17 @@ Then we compile our application, as we must do whenever we change `app.ddlog`:\n deepdive compile\n ```\n \n-Finally, we tell DeepDive to execute the steps to load the `articles` table:\n+Finally, we tell DeepDive to execute the steps to load the `articles` table using the `input/articles.tsv.sh` script. You must have the [full corpus](http://research.signalmedia.co/newsir16/signal-dataset.html) downloaded.\n \n ```bash\n deepdive do articles\n ```\n \n+Alternatively, a sample of 1000 articles can be loaded directly by issuing the following command:\n+```bash\n+deepdive load articles input/articles-1000.tsv.bz2",
        "path": "doc/example-spouse.md",
        "position": 13,
        "original_position": 13,
        "commit_id": "7cd5885d43bee20a07e663d69d33b35d4ba763f8",
        "original_commit_id": "eb3d44fe962d399bc64c9894c81fa2901e6ff8b2",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I forgot `deepdive do` was actually doing much more and you need these commands to run before/after `deepdive load`:\n\n``` bash\ndeepdive do init/app\ndeepdive create table articles\ndeepdive load articles input/articles-1000.tsv.bz2\ndeepdive mark done articles\n```\n\nWe could come up with a new shorthand for this kind of side-loading operation or revise existing `deepdive load` or `deepdive do`.\n",
        "created_at": "2016-02-26T21:45:10Z",
        "updated_at": "2016-05-25T08:13:58Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/506#discussion_r54306397",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/506",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/54306397"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/506#discussion_r54306397"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/506"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/54316364",
        "pull_request_review_id": null,
        "id": 54316364,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzE2MzY0",
        "diff_hunk": "@@ -88,12 +88,17 @@ Then we compile our application, as we must do whenever we change `app.ddlog`:\n deepdive compile\n ```\n \n-Finally, we tell DeepDive to execute the steps to load the `articles` table:\n+Finally, we tell DeepDive to execute the steps to load the `articles` table using the `input/articles.tsv.sh` script. You must have the [full corpus](http://research.signalmedia.co/newsir16/signal-dataset.html) downloaded.\n \n ```bash\n deepdive do articles\n ```\n \n+Alternatively, a sample of 1000 articles can be loaded directly by issuing the following command:\n+```bash\n+deepdive load articles input/articles-1000.tsv.bz2",
        "path": "doc/example-spouse.md",
        "position": 13,
        "original_position": 13,
        "commit_id": "7cd5885d43bee20a07e663d69d33b35d4ba763f8",
        "original_commit_id": "eb3d44fe962d399bc64c9894c81fa2901e6ff8b2",
        "user": {
            "login": "henryre",
            "id": 7783678,
            "node_id": "MDQ6VXNlcjc3ODM2Nzg=",
            "avatar_url": "https://avatars1.githubusercontent.com/u/7783678?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/henryre",
            "html_url": "https://github.com/henryre",
            "followers_url": "https://api.github.com/users/henryre/followers",
            "following_url": "https://api.github.com/users/henryre/following{/other_user}",
            "gists_url": "https://api.github.com/users/henryre/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/henryre/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/henryre/subscriptions",
            "organizations_url": "https://api.github.com/users/henryre/orgs",
            "repos_url": "https://api.github.com/users/henryre/repos",
            "events_url": "https://api.github.com/users/henryre/events{/privacy}",
            "received_events_url": "https://api.github.com/users/henryre/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Agreed. I think I had a script doing this and forgot about it. Built-in shorthand is a great idea\n",
        "created_at": "2016-02-26T23:23:39Z",
        "updated_at": "2016-05-25T08:13:58Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/506#discussion_r54316364",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/506",
        "author_association": "MEMBER",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/54316364"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/506#discussion_r54316364"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/506"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/58185594",
        "pull_request_review_id": null,
        "id": 58185594,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MTg1NTk0",
        "diff_hunk": "@@ -165,7 +170,7 @@ Note that we declare a compound key of `(doc_id, sentence_index)` for each sente\n -->\n \n Next we declare a DDlog function which takes in the `doc_id` and `content` for an article and returns rows conforming to the sentences schema we just declared, using the **user-defined function (UDF)** in `udf/nlp_markup.sh`.\n-This UDF is a Bash script which calls [our own wrapper around CoreNLP](https://github.com/HazyResearch/bazaar/tree/master/parser).\n+This UDF is a Bash script which calls [our own wrapper around CoreNLP](https://github.com/HazyResearch/bazaar/tree/master/parser). The CoreNLP library requires Java 8 to run.",
        "path": "doc/example-spouse.md",
        "position": 24,
        "original_position": 24,
        "commit_id": "7cd5885d43bee20a07e663d69d33b35d4ba763f8",
        "original_commit_id": "eb3d44fe962d399bc64c9894c81fa2901e6ff8b2",
        "user": {
            "login": "lanphan",
            "id": 360654,
            "node_id": "MDQ6VXNlcjM2MDY1NA==",
            "avatar_url": "https://avatars3.githubusercontent.com/u/360654?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/lanphan",
            "html_url": "https://github.com/lanphan",
            "followers_url": "https://api.github.com/users/lanphan/followers",
            "following_url": "https://api.github.com/users/lanphan/following{/other_user}",
            "gists_url": "https://api.github.com/users/lanphan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/lanphan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/lanphan/subscriptions",
            "organizations_url": "https://api.github.com/users/lanphan/orgs",
            "repos_url": "https://api.github.com/users/lanphan/repos",
            "events_url": "https://api.github.com/users/lanphan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/lanphan/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "What's about using \"curl -fsSL git.io/getdeepdive | bash -s _deepdive_runtime_deps\" to install jre8 as dependencies, @netj?\n",
        "created_at": "2016-04-01T10:00:44Z",
        "updated_at": "2016-05-25T08:13:58Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/506#discussion_r58185594",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/506",
        "author_association": "NONE",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/58185594"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/506#discussion_r58185594"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/506"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67228299",
        "pull_request_review_id": null,
        "id": 67228299,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3MjI4Mjk5",
        "diff_hunk": "@@ -73,11 +37,12 @@ def withRedefinitionsWithJoinsForIdColumns:\n         ],\n         style: \"cmd_extractor\", cmd: \"\n         : ${DEEPDIVE_GROUNDING_DIR:=\\\"$DEEPDIVE_APP\\\"/run/model/grounding}\n-        : ${DEEPDIVE_GROUNDING_DENSE_MULTINOMIAL:=false}\n         varPath=\\\"$DEEPDIVE_GROUNDING_DIR\\\"/variable/\\(.variableName | @sh)\n         mkdir -p \\\"$varPath\\\"\n         cd \\\"$varPath\\\"\n \n+        deepdive sql \\\"ANALYZE VERBOSE \\(.variablesTable)\\\"",
        "path": "compiler/compile-config/compile-config-2.01-grounding",
        "position": null,
        "original_position": 52,
        "commit_id": "ef844744a017d673f6a1bbf84e0f8c8cf98b9ae4",
        "original_commit_id": "ba7c50215f8a9170f12b320558b8c5719f122128",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Postgres-specific.  Use [`db-analyze`](https://github.com/HazyResearch/deepdive/blob/master/database/db-driver/postgresql/db-analyze) instead.  Adding `VERBOSE` to postgres' may be a good idea.\n",
        "created_at": "2016-06-15T19:21:35Z",
        "updated_at": "2016-06-17T06:56:32Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/540#discussion_r67228299",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/540",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67228299"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/540#discussion_r67228299"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/540"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67235683",
        "pull_request_review_id": null,
        "id": 67235683,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3MjM1Njgz",
        "diff_hunk": "@@ -454,65 +413,10 @@ def withRedefinitionsWithJoinsForIdColumns:\n                     | { alias: \"f\", table: .factorsTable }\n                     ]\n                 } | asSql | asPrettySqlArg) as $weightsMaterializeSqlArg |\n-            if .function_.isMultinomial then # factors over categorical variables\n-            \"\n-            # use dense representation only if all variables of the factor are dense\n-            isDenseMultinomial=true\n-            for v in \\([ .function_.variables[] | .schema.variableName | @sh ] | unique | join(\" \")); do\n-                ! [[ -e \\\"$DEEPDIVE_GROUNDING_DIR/variable/$v/isDenseCategorical\\\" ]] || continue\n-                isDenseMultinomial=false\n-                break\n-            done\n-            if $isDenseMultinomial; then\n-                touch isDenseMultinomial\n-                # find distinct weight parameters for factors\n-                # without considering the exact combinations of category values,\n-                # assuming they'll all be useful to put in a dense representation\n-                deepdive create table \\(.weightGroupsTable | @sh) as \\($weightsMaterializeSqlArg)\n-            else\n-                rm -f isDenseMultinomial\n-                # find distinct weight parameters for factors\n-                # considering all the exact combintations of category values present in the factors\n-                # assuming they'll be sparse\n-                deepdive create table \\(.weightsTable | @sh) as \\(\n-                    { SELECT:\n-                        # weight parameters\n-                        [ ( .weight_.params[] | { column: . } )\n-                        # include category columns for every categorical variable\n-                        , ( .function_.variables[]\n-                        | .columnId as $columnIdForThisVar\n-                        | .schema.variablesCategoryColumns[]\n-                        | { alias: \"\\($columnIdForThisVar)_\\(.)\", table: \"v.\\($columnIdForThisVar)\", column: . } )\n-                        # weight attributes\n-                        , { alias: \"isfixed\"  , expr: .weight_.is_fixed      }\n-                        , { alias: \"initvalue\", expr: .weight_.init_value    }\n-                        , { alias: \"wid\"      , expr: \"CAST(NULL AS BIGINT)\" } # to be assigned later by the assign_weight_id process\n-                        ]\n-                    , DISTINCT: true  # XXX this is inevitable\n-                    # weights are exploded for the factor's variables' category values\n-                    , FROM: [ { alias: \"f\", table: .factorsTable } ]\n-                    # FIXME joins below can go away if DDlog could project them as well in the .input_query\n-                    # or we could inject those pushed-down projections if it at least compiled into a sql.jq object\n-# XXX this join prevents us from defining a partial weight distribution\n-                    , JOIN:\n-                        [ ( .function_.variables[]\n-                        | .columnId as $columnIdForThisVar\n-                        | { INNER: { alias: \"i.\\($columnIdForThisVar)\", table: .schema.variablesIdsTable }\n-                          , ON: { eq: [ { table: \"f\"                          , column: $columnIdForThisVar }\n-                                      , { table: \"i.\\($columnIdForThisVar)\", column: deepdiveVariableIdColumn } ] }\n-                          }\n-                        , { INNER: { alias: \"v.\\($columnIdForThisVar)\", table: .schema.variablesTable }\n-                          , ON: [ ( .schema.variablesKeyColumns[]\n-                                | { eq: [ { table: \"v.\\($columnIdForThisVar)\", column: . }\n-                                        , { table: \"i.\\($columnIdForThisVar)\", column: . } ] } ) ] }\n-                        ) ]\n-                    } | asSql | asPrettySqlArg)\n-            fi\n-            \" else # factors over boolean variables\n             \"\n             # find distinct weights for the factors into a separate table\n             deepdive create table \\(.weightsTable | @sh) as \\($weightsMaterializeSqlArg)\n-            \" end)\n+            \")",
        "path": "compiler/compile-config/compile-config-2.01-grounding",
        "position": null,
        "original_position": 291,
        "commit_id": "ef844744a017d673f6a1bbf84e0f8c8cf98b9ae4",
        "original_commit_id": "ba7c50215f8a9170f12b320558b8c5719f122128",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "For readability let's inline `$weightsMaterializeSqlArg` since it's no longer repeated.\n",
        "created_at": "2016-06-15T20:07:02Z",
        "updated_at": "2016-06-17T06:56:32Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/540#discussion_r67235683",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/540",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67235683"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/540#discussion_r67235683"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/540"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67235737",
        "pull_request_review_id": null,
        "id": 67235737,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3MjM1NzM3",
        "diff_hunk": "@@ -752,81 +544,39 @@ def withRedefinitionsWithJoinsForIdColumns:\n                 input_sql=\\(\n                 if .function_.isMultinomial then\n                 \"\\\"$(\n-                if [[ -e isDenseMultinomial ]]; then\n-                    # dense multinomial factor needs not join the weight ids,\n-                    # since it exploits the fact that all possible weights are materialized, and\n-                    # each factor can use the variables' domain values to index into the exact one\n-                    echo \\(\n-                    { SELECT:\n-                        [ ( .function_.variables[]\n-                        | { table: \"f\", column: .columnId } )\n-                        , { alias: \"weight_id\", table: \"w\", column: \"wid\" }\n-                        ]\n-                    , FROM:\n-                        [ { alias: \"f\", table: .factorsTable }\n-                        , { alias: \"w\", table: .weightGroupsTable }  # NOTE that we only dump one weight id per group\n-                        ]\n-                    , WHERE:\n-                        [ ( .weight_.params[]\n-                        | { eq: [ { table: \"w\", column: . }\n-                                , { table: \"f\", column: . } ] } )\n-                        ]\n-                    } | asSql | asPrettySqlArg)\n-                else\n                     # sparse multinomial factor need to find exact combinations of category values and\n                     # weight parameters present in the data, hence a lot of joins! but doesn't waste the weights\n+                    # FIXME XXXXX: Assuming all var tables are categorical; TODO: Support mixed head.\n                     echo \\(\n+                        # Order for parallel ARRAY_AGG expressions\n+                        ( [ .function_.variables[] |\n+                        \"\\(.columnPrefix)\\(deepdiveVariableInternalLabelColumn)\" | asSqlIdent\n+                        ] | join(\", \")) as $valueOrder\n+                    |\n                     { SELECT:\n                         [ ( .function_.variables[]\n                         | { table: \"f\", column: .columnId } )\n                         , { alias: \"num_weights\", expr: \"COUNT(w.wid)\" }\n-                        , { alias: \"weight_ids\" , expr: \"ARRAY_AGG(w.wid ORDER BY w.wid)\" }\n+                        , ( .function_.variables[]\n+                        | { alias: \"c\\(.ordinal)_ids\",",
        "path": "compiler/compile-config/compile-config-2.01-grounding",
        "position": null,
        "original_position": 497,
        "commit_id": "ef844744a017d673f6a1bbf84e0f8c8cf98b9ae4",
        "original_commit_id": "ba7c50215f8a9170f12b320558b8c5719f122128",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Maybe add a comment that these two columns are parallel arrays?\n",
        "created_at": "2016-06-15T20:07:22Z",
        "updated_at": "2016-06-17T06:56:32Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/540#discussion_r67235737",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/540",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67235737"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/540#discussion_r67235737"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/540"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67236732",
        "pull_request_review_id": null,
        "id": 67236732,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3MjM2NzMy",
        "diff_hunk": "@@ -427,16 +375,27 @@ def withRedefinitionsWithJoinsForIdColumns:\n \n             # materialize factors using user input_query that pulls in assigned ids to involved variables\n             deepdive create table \\(.factorsTable | @sh) as \\(\n-                # allow .input_query to refer to the assigned variable ids by redefining the variable table names with SQL CTE\n+                # Not using DISTINCT: User decides if they want to have duplicate factors",
        "path": "compiler/compile-config/compile-config-2.01-grounding",
        "position": 199,
        "original_position": 199,
        "commit_id": "ef844744a017d673f6a1bbf84e0f8c8cf98b9ae4",
        "original_commit_id": "ba7c50215f8a9170f12b320558b8c5719f122128",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Note that `factorsTable` can be a view.  Materializing this can happen in the `factors/*/dump` process.  weightsTable below can come directly from the `input_query` instead.  Change can be simple as `deepdive create view ... as ...` or we could move and plug the query into the dump process.\n\nIn general, only the ones up for sequential id assignment have to be materialized as table.  If user's `input_query` is costly we could materialize that instead, but I think factor input_query is expected to generate bloat by design since the result is supposed to be projected and group'ed/distinct'ed, so maybe that's a bad idea.\n",
        "created_at": "2016-06-15T20:13:27Z",
        "updated_at": "2016-06-17T06:56:32Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/540#discussion_r67236732",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/540",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67236732"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/540#discussion_r67236732"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/540"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67258229",
        "pull_request_review_id": null,
        "id": 67258229,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3MjU4MjI5",
        "diff_hunk": "@@ -427,16 +375,27 @@ def withRedefinitionsWithJoinsForIdColumns:\n \n             # materialize factors using user input_query that pulls in assigned ids to involved variables\n             deepdive create table \\(.factorsTable | @sh) as \\(\n-                # allow .input_query to refer to the assigned variable ids by redefining the variable table names with SQL CTE\n+                # Not using DISTINCT: User decides if they want to have duplicate factors",
        "path": "compiler/compile-config/compile-config-2.01-grounding",
        "position": 199,
        "original_position": 199,
        "commit_id": "ef844744a017d673f6a1bbf84e0f8c8cf98b9ae4",
        "original_commit_id": "ba7c50215f8a9170f12b320558b8c5719f122128",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "If it's a view and involves expensive joins, we would evaluate it twice. I'm leaving it as a table for now.\n",
        "created_at": "2016-06-15T22:32:14Z",
        "updated_at": "2016-06-17T06:56:32Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/540#discussion_r67258229",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/540",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67258229"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/540#discussion_r67258229"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/540"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67258718",
        "pull_request_review_id": null,
        "id": 67258718,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3MjU4NzE4",
        "diff_hunk": "@@ -427,16 +375,27 @@ def withRedefinitionsWithJoinsForIdColumns:\n \n             # materialize factors using user input_query that pulls in assigned ids to involved variables\n             deepdive create table \\(.factorsTable | @sh) as \\(\n-                # allow .input_query to refer to the assigned variable ids by redefining the variable table names with SQL CTE\n+                # Not using DISTINCT: User decides if they want to have duplicate factors",
        "path": "compiler/compile-config/compile-config-2.01-grounding",
        "position": 199,
        "original_position": 199,
        "commit_id": "ef844744a017d673f6a1bbf84e0f8c8cf98b9ae4",
        "original_commit_id": "ba7c50215f8a9170f12b320558b8c5719f122128",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Okay.  I have a feeling it'll waste time&space to materialize what's going to be immediately projected out, but we can do this comparison more rigorously and decide later.\n",
        "created_at": "2016-06-15T22:36:22Z",
        "updated_at": "2016-06-17T06:56:32Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/540#discussion_r67258718",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/540",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67258718"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/540#discussion_r67258718"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/540"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67262480",
        "pull_request_review_id": null,
        "id": 67262480,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3MjYyNDgw",
        "diff_hunk": "@@ -73,11 +37,12 @@ def withRedefinitionsWithJoinsForIdColumns:\n         ],\n         style: \"cmd_extractor\", cmd: \"\n         : ${DEEPDIVE_GROUNDING_DIR:=\\\"$DEEPDIVE_APP\\\"/run/model/grounding}\n-        : ${DEEPDIVE_GROUNDING_DENSE_MULTINOMIAL:=false}\n         varPath=\\\"$DEEPDIVE_GROUNDING_DIR\\\"/variable/\\(.variableName | @sh)\n         mkdir -p \\\"$varPath\\\"\n         cd \\\"$varPath\\\"\n \n+        deepdive sql \\\"ANALYZE VERBOSE \\(.variablesTable)\\\"",
        "path": "compiler/compile-config/compile-config-2.01-grounding",
        "position": null,
        "original_position": 52,
        "commit_id": "ef844744a017d673f6a1bbf84e0f8c8cf98b9ae4",
        "original_commit_id": "ba7c50215f8a9170f12b320558b8c5719f122128",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "done\n",
        "created_at": "2016-06-15T23:10:10Z",
        "updated_at": "2016-06-17T06:56:32Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/540#discussion_r67262480",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/540",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67262480"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/540#discussion_r67262480"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/540"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67262487",
        "pull_request_review_id": null,
        "id": 67262487,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3MjYyNDg3",
        "diff_hunk": "@@ -454,65 +413,10 @@ def withRedefinitionsWithJoinsForIdColumns:\n                     | { alias: \"f\", table: .factorsTable }\n                     ]\n                 } | asSql | asPrettySqlArg) as $weightsMaterializeSqlArg |\n-            if .function_.isMultinomial then # factors over categorical variables\n-            \"\n-            # use dense representation only if all variables of the factor are dense\n-            isDenseMultinomial=true\n-            for v in \\([ .function_.variables[] | .schema.variableName | @sh ] | unique | join(\" \")); do\n-                ! [[ -e \\\"$DEEPDIVE_GROUNDING_DIR/variable/$v/isDenseCategorical\\\" ]] || continue\n-                isDenseMultinomial=false\n-                break\n-            done\n-            if $isDenseMultinomial; then\n-                touch isDenseMultinomial\n-                # find distinct weight parameters for factors\n-                # without considering the exact combinations of category values,\n-                # assuming they'll all be useful to put in a dense representation\n-                deepdive create table \\(.weightGroupsTable | @sh) as \\($weightsMaterializeSqlArg)\n-            else\n-                rm -f isDenseMultinomial\n-                # find distinct weight parameters for factors\n-                # considering all the exact combintations of category values present in the factors\n-                # assuming they'll be sparse\n-                deepdive create table \\(.weightsTable | @sh) as \\(\n-                    { SELECT:\n-                        # weight parameters\n-                        [ ( .weight_.params[] | { column: . } )\n-                        # include category columns for every categorical variable\n-                        , ( .function_.variables[]\n-                        | .columnId as $columnIdForThisVar\n-                        | .schema.variablesCategoryColumns[]\n-                        | { alias: \"\\($columnIdForThisVar)_\\(.)\", table: \"v.\\($columnIdForThisVar)\", column: . } )\n-                        # weight attributes\n-                        , { alias: \"isfixed\"  , expr: .weight_.is_fixed      }\n-                        , { alias: \"initvalue\", expr: .weight_.init_value    }\n-                        , { alias: \"wid\"      , expr: \"CAST(NULL AS BIGINT)\" } # to be assigned later by the assign_weight_id process\n-                        ]\n-                    , DISTINCT: true  # XXX this is inevitable\n-                    # weights are exploded for the factor's variables' category values\n-                    , FROM: [ { alias: \"f\", table: .factorsTable } ]\n-                    # FIXME joins below can go away if DDlog could project them as well in the .input_query\n-                    # or we could inject those pushed-down projections if it at least compiled into a sql.jq object\n-# XXX this join prevents us from defining a partial weight distribution\n-                    , JOIN:\n-                        [ ( .function_.variables[]\n-                        | .columnId as $columnIdForThisVar\n-                        | { INNER: { alias: \"i.\\($columnIdForThisVar)\", table: .schema.variablesIdsTable }\n-                          , ON: { eq: [ { table: \"f\"                          , column: $columnIdForThisVar }\n-                                      , { table: \"i.\\($columnIdForThisVar)\", column: deepdiveVariableIdColumn } ] }\n-                          }\n-                        , { INNER: { alias: \"v.\\($columnIdForThisVar)\", table: .schema.variablesTable }\n-                          , ON: [ ( .schema.variablesKeyColumns[]\n-                                | { eq: [ { table: \"v.\\($columnIdForThisVar)\", column: . }\n-                                        , { table: \"i.\\($columnIdForThisVar)\", column: . } ] } ) ] }\n-                        ) ]\n-                    } | asSql | asPrettySqlArg)\n-            fi\n-            \" else # factors over boolean variables\n             \"\n             # find distinct weights for the factors into a separate table\n             deepdive create table \\(.weightsTable | @sh) as \\($weightsMaterializeSqlArg)\n-            \" end)\n+            \")",
        "path": "compiler/compile-config/compile-config-2.01-grounding",
        "position": null,
        "original_position": 291,
        "commit_id": "ef844744a017d673f6a1bbf84e0f8c8cf98b9ae4",
        "original_commit_id": "ba7c50215f8a9170f12b320558b8c5719f122128",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "done\n",
        "created_at": "2016-06-15T23:10:14Z",
        "updated_at": "2016-06-17T06:56:32Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/540#discussion_r67262487",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/540",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67262487"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/540#discussion_r67262487"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/540"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67262499",
        "pull_request_review_id": null,
        "id": 67262499,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3MjYyNDk5",
        "diff_hunk": "@@ -752,81 +544,39 @@ def withRedefinitionsWithJoinsForIdColumns:\n                 input_sql=\\(\n                 if .function_.isMultinomial then\n                 \"\\\"$(\n-                if [[ -e isDenseMultinomial ]]; then\n-                    # dense multinomial factor needs not join the weight ids,\n-                    # since it exploits the fact that all possible weights are materialized, and\n-                    # each factor can use the variables' domain values to index into the exact one\n-                    echo \\(\n-                    { SELECT:\n-                        [ ( .function_.variables[]\n-                        | { table: \"f\", column: .columnId } )\n-                        , { alias: \"weight_id\", table: \"w\", column: \"wid\" }\n-                        ]\n-                    , FROM:\n-                        [ { alias: \"f\", table: .factorsTable }\n-                        , { alias: \"w\", table: .weightGroupsTable }  # NOTE that we only dump one weight id per group\n-                        ]\n-                    , WHERE:\n-                        [ ( .weight_.params[]\n-                        | { eq: [ { table: \"w\", column: . }\n-                                , { table: \"f\", column: . } ] } )\n-                        ]\n-                    } | asSql | asPrettySqlArg)\n-                else\n                     # sparse multinomial factor need to find exact combinations of category values and\n                     # weight parameters present in the data, hence a lot of joins! but doesn't waste the weights\n+                    # FIXME XXXXX: Assuming all var tables are categorical; TODO: Support mixed head.\n                     echo \\(\n+                        # Order for parallel ARRAY_AGG expressions\n+                        ( [ .function_.variables[] |\n+                        \"\\(.columnPrefix)\\(deepdiveVariableInternalLabelColumn)\" | asSqlIdent\n+                        ] | join(\", \")) as $valueOrder\n+                    |\n                     { SELECT:\n                         [ ( .function_.variables[]\n                         | { table: \"f\", column: .columnId } )\n                         , { alias: \"num_weights\", expr: \"COUNT(w.wid)\" }\n-                        , { alias: \"weight_ids\" , expr: \"ARRAY_AGG(w.wid ORDER BY w.wid)\" }\n+                        , ( .function_.variables[]\n+                        | { alias: \"c\\(.ordinal)_ids\",",
        "path": "compiler/compile-config/compile-config-2.01-grounding",
        "position": null,
        "original_position": 497,
        "commit_id": "ef844744a017d673f6a1bbf84e0f8c8cf98b9ae4",
        "original_commit_id": "ba7c50215f8a9170f12b320558b8c5719f122128",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "done\n",
        "created_at": "2016-06-15T23:10:19Z",
        "updated_at": "2016-06-17T06:56:32Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/540#discussion_r67262499",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/540",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67262499"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/540#discussion_r67262499"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/540"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67593365",
        "pull_request_review_id": null,
        "id": 67593365,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3NTkzMzY1",
        "diff_hunk": "@@ -25,20 +48,24 @@ def convert_type_func(ty, ty_rest = \"\"):\n       return convert_other_array\n   else: # non-array, must be primitive type\n     normalized_type_name = {\n+        \"timestamp\": \"timestamp\",",
        "path": "database/pgtsv_to_json",
        "position": null,
        "original_position": 34,
        "commit_id": "f5dc3d37d7158e66543fcbe9a683ab6236b1e08e",
        "original_commit_id": "a3c6404e0044dd2c0e72b9c1c0c1f298218fc042",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "This seems unnecessary, but fine to keep.\n",
        "created_at": "2016-06-18T01:56:17Z",
        "updated_at": "2016-06-19T19:35:24Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/542#discussion_r67593365",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/542",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67593365"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/542#discussion_r67593365"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/542"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67593366",
        "pull_request_review_id": null,
        "id": 67593366,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3NTkzMzY2",
        "diff_hunk": "@@ -3,6 +3,29 @@\n # Usage: pgtsv_to_json COLUMN_NAME1:TYPE1 COLUMN_NAME2:TYPE2 ...\n \n import json, sys, csv, re\n+from datetime import datetime\n+\n+def timestamp(timestamp_str):\n+    \"\"\"Given a timestamp string, return a timestamp string in ISO 8601 format to emulate\n+    Postgres 9.5's to_json timestamp formatting. The given timestamp is assumed UTC.\n+\n+    Time zones are not supported. http://bugs.python.org/issue6641\n+\n+    Examples:\n+\n+        >>> timestamp('2016-06-17 20:10:38')\n+        '2016-06-17T20:10:38+00:00'\n+\n+        >>> timestamp('2016-06-17 20:10:37.9293')\n+        '2016-06-17T20:10:37.929300+00:00'\n+\n+    \"\"\"\n+\n+    try:\n+        parsed = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S.%f')\n+    except ValueError:\n+        parsed = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')\n+    return parsed.isoformat() + '+00:00'",
        "path": "database/pgtsv_to_json",
        "position": null,
        "original_position": 26,
        "commit_id": "f5dc3d37d7158e66543fcbe9a683ab6236b1e08e",
        "original_commit_id": "a3c6404e0044dd2c0e72b9c1c0c1f298218fc042",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "It'd be nice to default to local timezone but sounds like it's not easy to get it or is it just the lack of support for `%:z`?  Does GP store all timestamp in UTC btw?\n",
        "created_at": "2016-06-18T01:56:18Z",
        "updated_at": "2016-06-19T19:35:24Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/542#discussion_r67593366",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/542",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67593366"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/542#discussion_r67593366"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/542"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67604448",
        "pull_request_review_id": null,
        "id": 67604448,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3NjA0NDQ4",
        "diff_hunk": "@@ -3,6 +3,29 @@\n # Usage: pgtsv_to_json COLUMN_NAME1:TYPE1 COLUMN_NAME2:TYPE2 ...\n \n import json, sys, csv, re\n+from datetime import datetime\n+\n+def timestamp(timestamp_str):\n+    \"\"\"Given a timestamp string, return a timestamp string in ISO 8601 format to emulate\n+    Postgres 9.5's to_json timestamp formatting. The given timestamp is assumed UTC.\n+\n+    Time zones are not supported. http://bugs.python.org/issue6641\n+\n+    Examples:\n+\n+        >>> timestamp('2016-06-17 20:10:38')\n+        '2016-06-17T20:10:38+00:00'\n+\n+        >>> timestamp('2016-06-17 20:10:37.9293')\n+        '2016-06-17T20:10:37.929300+00:00'\n+\n+    \"\"\"\n+\n+    try:\n+        parsed = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S.%f')\n+    except ValueError:\n+        parsed = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')\n+    return parsed.isoformat() + '+00:00'",
        "path": "database/pgtsv_to_json",
        "position": null,
        "original_position": 26,
        "commit_id": "f5dc3d37d7158e66543fcbe9a683ab6236b1e08e",
        "original_commit_id": "a3c6404e0044dd2c0e72b9c1c0c1f298218fc042",
        "user": {
            "login": "shahin",
            "id": 20877,
            "node_id": "MDQ6VXNlcjIwODc3",
            "avatar_url": "https://avatars3.githubusercontent.com/u/20877?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shahin",
            "html_url": "https://github.com/shahin",
            "followers_url": "https://api.github.com/users/shahin/followers",
            "following_url": "https://api.github.com/users/shahin/following{/other_user}",
            "gists_url": "https://api.github.com/users/shahin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shahin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shahin/subscriptions",
            "organizations_url": "https://api.github.com/users/shahin/orgs",
            "repos_url": "https://api.github.com/users/shahin/repos",
            "events_url": "https://api.github.com/users/shahin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shahin/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Timezone-aware dates are stored internally in UTC, but they're output in whichever time zone is set for the session. This can be (a) the system time zone, (b) the time zone set in postgresql.conf, (c) the current time zone setting for the psql session, or (d) [something else](https://www.postgresql.org/docs/8.3/static/datatype-datetime.html#DATATYPE-TIMEZONES). Given that, defaulting to \"local\" time seems potentially convenient but very easily incorrect (it's not really clear what \"local\" means in this context).\n\nThis code will raise on any timestamp with a timezone, so it's not handling them incorrectly -- it's refusing to handle them at all.\n\nNow that you mention it, it is pretty odd to output a UTC zone offset in this case. It shouldn't output a zone offset at all. I can remove the `+00:00` on output. How does that sound?\n",
        "created_at": "2016-06-18T18:06:00Z",
        "updated_at": "2016-06-19T19:35:24Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/542#discussion_r67604448",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/542",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67604448"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/542#discussion_r67604448"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/542"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67604491",
        "pull_request_review_id": null,
        "id": 67604491,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3NjA0NDkx",
        "diff_hunk": "@@ -3,6 +3,29 @@\n # Usage: pgtsv_to_json COLUMN_NAME1:TYPE1 COLUMN_NAME2:TYPE2 ...\n \n import json, sys, csv, re\n+from datetime import datetime\n+\n+def timestamp(timestamp_str):\n+    \"\"\"Given a timestamp string, return a timestamp string in ISO 8601 format to emulate\n+    Postgres 9.5's to_json timestamp formatting. The given timestamp is assumed UTC.\n+\n+    Time zones are not supported. http://bugs.python.org/issue6641\n+\n+    Examples:\n+\n+        >>> timestamp('2016-06-17 20:10:38')\n+        '2016-06-17T20:10:38+00:00'\n+\n+        >>> timestamp('2016-06-17 20:10:37.9293')\n+        '2016-06-17T20:10:37.929300+00:00'\n+\n+    \"\"\"\n+\n+    try:\n+        parsed = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S.%f')\n+    except ValueError:\n+        parsed = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')\n+    return parsed.isoformat() + '+00:00'",
        "path": "database/pgtsv_to_json",
        "position": null,
        "original_position": 26,
        "commit_id": "f5dc3d37d7158e66543fcbe9a683ab6236b1e08e",
        "original_commit_id": "a3c6404e0044dd2c0e72b9c1c0c1f298218fc042",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I think appending `+00:00` or `Z` will be safer and more accurate if you can explicitly set the UTC timezone to the session this `pgtsv_to_json` is attached to.\n",
        "created_at": "2016-06-18T18:08:55Z",
        "updated_at": "2016-06-19T19:35:24Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/542#discussion_r67604491",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/542",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67604491"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/542#discussion_r67604491"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/542"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67609242",
        "pull_request_review_id": null,
        "id": 67609242,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3NjA5MjQy",
        "diff_hunk": "@@ -3,6 +3,29 @@\n # Usage: pgtsv_to_json COLUMN_NAME1:TYPE1 COLUMN_NAME2:TYPE2 ...\n \n import json, sys, csv, re\n+from datetime import datetime\n+\n+def timestamp(timestamp_str):\n+    \"\"\"Given a timestamp string, return a timestamp string in ISO 8601 format to emulate\n+    Postgres 9.5's to_json timestamp formatting. The given timestamp is assumed UTC.\n+\n+    Time zones are not supported. http://bugs.python.org/issue6641\n+\n+    Examples:\n+\n+        >>> timestamp('2016-06-17 20:10:38')\n+        '2016-06-17T20:10:38+00:00'\n+\n+        >>> timestamp('2016-06-17 20:10:37.9293')\n+        '2016-06-17T20:10:37.929300+00:00'\n+\n+    \"\"\"\n+\n+    try:\n+        parsed = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S.%f')\n+    except ValueError:\n+        parsed = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')\n+    return parsed.isoformat() + '+00:00'",
        "path": "database/pgtsv_to_json",
        "position": null,
        "original_position": 26,
        "commit_id": "f5dc3d37d7158e66543fcbe9a683ab6236b1e08e",
        "original_commit_id": "a3c6404e0044dd2c0e72b9c1c0c1f298218fc042",
        "user": {
            "login": "shahin",
            "id": 20877,
            "node_id": "MDQ6VXNlcjIwODc3",
            "avatar_url": "https://avatars3.githubusercontent.com/u/20877?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shahin",
            "html_url": "https://github.com/shahin",
            "followers_url": "https://api.github.com/users/shahin/followers",
            "following_url": "https://api.github.com/users/shahin/following{/other_user}",
            "gists_url": "https://api.github.com/users/shahin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shahin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shahin/subscriptions",
            "organizations_url": "https://api.github.com/users/shahin/orgs",
            "repos_url": "https://api.github.com/users/shahin/repos",
            "events_url": "https://api.github.com/users/shahin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shahin/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "The `pgtsv_to_json` module just takes text over STDIN and isn't aware of any database sessions. Inspecting a separate psql process for state seems brittle, especially since there's no guarantee one exists.\n\nI'm getting the impression that you'd like this module to handle time zone offsets, which is totally reasonable. I like the original module's design, so my preference is to avoid:\n- changes to the `pgtsv_to_json` interface,\n- changes outside this Python module,\n- assumptions about the state of a psql session.\n\nTwo options are:\n1. Parse and output time zone offsets where they exist on the input, and output no offset (not even `+00:00`) where they don't, or\n2. Raise a `ValueError` where time zone offsets exist on the input, and output no offsets where they don't.\n\nEither way there's no loss of accuracy, and I'm happy to go in either direction. What do you think?\n",
        "created_at": "2016-06-19T02:23:50Z",
        "updated_at": "2016-06-19T19:35:24Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/542#discussion_r67609242",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/542",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67609242"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/542#discussion_r67609242"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/542"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67609499",
        "pull_request_review_id": null,
        "id": 67609499,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3NjA5NDk5",
        "diff_hunk": "@@ -3,6 +3,29 @@\n # Usage: pgtsv_to_json COLUMN_NAME1:TYPE1 COLUMN_NAME2:TYPE2 ...\n \n import json, sys, csv, re\n+from datetime import datetime\n+\n+def timestamp(timestamp_str):\n+    \"\"\"Given a timestamp string, return a timestamp string in ISO 8601 format to emulate\n+    Postgres 9.5's to_json timestamp formatting. The given timestamp is assumed UTC.\n+\n+    Time zones are not supported. http://bugs.python.org/issue6641\n+\n+    Examples:\n+\n+        >>> timestamp('2016-06-17 20:10:38')\n+        '2016-06-17T20:10:38+00:00'\n+\n+        >>> timestamp('2016-06-17 20:10:37.9293')\n+        '2016-06-17T20:10:37.929300+00:00'\n+\n+    \"\"\"\n+\n+    try:\n+        parsed = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S.%f')\n+    except ValueError:\n+        parsed = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')\n+    return parsed.isoformat() + '+00:00'",
        "path": "database/pgtsv_to_json",
        "position": null,
        "original_position": 26,
        "commit_id": "f5dc3d37d7158e66543fcbe9a683ab6236b1e08e",
        "original_commit_id": "a3c6404e0044dd2c0e72b9c1c0c1f298218fc042",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "1 sounds better. But we are probably over-engineering here. I'd simply output verbatim whatever psql feeds us. Chances are the downstream app would actually prefer verbatim. For the current main consumer of ES, it supports a variety of formats already: https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-date-format.html\n",
        "created_at": "2016-06-19T02:55:22Z",
        "updated_at": "2016-06-19T19:35:24Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/542#discussion_r67609499",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/542",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67609499"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/542#discussion_r67609499"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/542"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67622480",
        "pull_request_review_id": null,
        "id": 67622480,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3NjIyNDgw",
        "diff_hunk": "@@ -3,6 +3,31 @@\n # Usage: pgtsv_to_json COLUMN_NAME1:TYPE1 COLUMN_NAME2:TYPE2 ...\n \n import json, sys, csv, re\n+from datetime import datetime\n+\n+def timestamp(timestamp_str):\n+    \"\"\"Given a timestamp string, return a timestamp string in ISO 8601 format to emulate\n+    Postgres 9.5's to_json timestamp formatting.\n+\n+    This supports the `timestamp without time zone` PostgreSQL type.\n+\n+    Time zone offsets are not supported. http://bugs.python.org/issue6641\n+\n+    Examples:\n+\n+        >>> timestamp('2016-06-17 20:10:38')\n+        '2016-06-17T20:10:38'\n+\n+        >>> timestamp('2016-06-17 20:10:37.9293')\n+        '2016-06-17T20:10:37.929300'\n+\n+    \"\"\"\n+\n+    try:\n+        parsed = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S.%f')\n+    except ValueError:\n+        parsed = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')",
        "path": "database/pgtsv_to_json",
        "position": 27,
        "original_position": 27,
        "commit_id": "f5dc3d37d7158e66543fcbe9a683ab6236b1e08e",
        "original_commit_id": "0a794ff96cec5a9591336c41da77a2ee2d129194",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "If this fails too, can we just return the input string?\n",
        "created_at": "2016-06-19T19:26:12Z",
        "updated_at": "2016-06-19T19:35:24Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/542#discussion_r67622480",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/542",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67622480"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/542#discussion_r67622480"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/542"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67622587",
        "pull_request_review_id": null,
        "id": 67622587,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3NjIyNTg3",
        "diff_hunk": "@@ -3,6 +3,31 @@\n # Usage: pgtsv_to_json COLUMN_NAME1:TYPE1 COLUMN_NAME2:TYPE2 ...\n \n import json, sys, csv, re\n+from datetime import datetime\n+\n+def timestamp(timestamp_str):\n+    \"\"\"Given a timestamp string, return a timestamp string in ISO 8601 format to emulate\n+    Postgres 9.5's to_json timestamp formatting.\n+\n+    This supports the `timestamp without time zone` PostgreSQL type.\n+\n+    Time zone offsets are not supported. http://bugs.python.org/issue6641\n+\n+    Examples:\n+\n+        >>> timestamp('2016-06-17 20:10:38')\n+        '2016-06-17T20:10:38'\n+\n+        >>> timestamp('2016-06-17 20:10:37.9293')\n+        '2016-06-17T20:10:37.929300'\n+\n+    \"\"\"\n+\n+    try:\n+        parsed = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S.%f')\n+    except ValueError:\n+        parsed = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')",
        "path": "database/pgtsv_to_json",
        "position": 27,
        "original_position": 27,
        "commit_id": "f5dc3d37d7158e66543fcbe9a683ab6236b1e08e",
        "original_commit_id": "0a794ff96cec5a9591336c41da77a2ee2d129194",
        "user": {
            "login": "shahin",
            "id": 20877,
            "node_id": "MDQ6VXNlcjIwODc3",
            "avatar_url": "https://avatars3.githubusercontent.com/u/20877?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shahin",
            "html_url": "https://github.com/shahin",
            "followers_url": "https://api.github.com/users/shahin/followers",
            "following_url": "https://api.github.com/users/shahin/following{/other_user}",
            "gists_url": "https://api.github.com/users/shahin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shahin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shahin/subscriptions",
            "organizations_url": "https://api.github.com/users/shahin/orgs",
            "repos_url": "https://api.github.com/users/shahin/repos",
            "events_url": "https://api.github.com/users/shahin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shahin/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Added in f5dc3d3\n",
        "created_at": "2016-06-19T19:35:58Z",
        "updated_at": "2016-06-19T19:35:58Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/542#discussion_r67622587",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/542",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67622587"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/542#discussion_r67622587"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/542"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67624786",
        "pull_request_review_id": null,
        "id": 67624786,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3NjI0Nzg2",
        "diff_hunk": "@@ -378,15 +378,15 @@ def factorWeightDescriptionSqlExpr:\n                 # Not using DISTINCT: User decides if they want to have duplicate factors\n                 { SELECT:\n                     [ ( .function_.variables[] | { table: \"F\", column: .columnId } )\n-                    # TODO XXXX: assuming so-called \"multinomial\" factors' heads have categorical vars only\n-                    , ( .function_ | select(.isMultinomial) | .variables[] |\n+                    # TODO XXXX: assuming categorical factors' heads have categorical vars only",
        "path": "compiler/compile-config/compile-config-2.01-grounding",
        "position": null,
        "original_position": 6,
        "commit_id": "c0d7720f82a3188baa1886856edc8f84ec90ff3a",
        "original_commit_id": "1d098544042dae88ec3803b72915486e8d0ba97e",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I think three-X is the convention: `XXX`.  Editors and IDEs recognize them and highlight just like `TODO` and `FIXME`.  Not sure where they came from but pretty useful to flag comments.\n",
        "created_at": "2016-06-19T22:10:14Z",
        "updated_at": "2016-06-19T22:25:15Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/544#discussion_r67624786",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/544",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67624786"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/544#discussion_r67624786"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/544"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67624973",
        "pull_request_review_id": null,
        "id": 67624973,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3NjI0OTcz",
        "diff_hunk": "@@ -378,15 +378,15 @@ def factorWeightDescriptionSqlExpr:\n                 # Not using DISTINCT: User decides if they want to have duplicate factors\n                 { SELECT:\n                     [ ( .function_.variables[] | { table: \"F\", column: .columnId } )\n-                    # TODO XXXX: assuming so-called \"multinomial\" factors' heads have categorical vars only\n-                    , ( .function_ | select(.isMultinomial) | .variables[] |\n+                    # TODO XXXX: assuming categorical factors' heads have categorical vars only",
        "path": "compiler/compile-config/compile-config-2.01-grounding",
        "position": null,
        "original_position": 6,
        "commit_id": "c0d7720f82a3188baa1886856edc8f84ec90ff3a",
        "original_commit_id": "1d098544042dae88ec3803b72915486e8d0ba97e",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I thought more Xs could get more attention :)\n",
        "created_at": "2016-06-19T22:22:43Z",
        "updated_at": "2016-06-19T22:25:15Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/544#discussion_r67624973",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/544",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/67624973"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/544#discussion_r67624973"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/544"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/68304072",
        "pull_request_review_id": null,
        "id": 68304072,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY4MzA0MDcy",
        "diff_hunk": "@@ -42,13 +42,13 @@ def convert_type_func(ty, ty_rest = \"\"):\n         return re.sub(r\"\\\\(.)\", lambda m: '\"\"' if m.group(1) is '\"' else m.group(1), s)\n     if ty_el == \"text\":\n       def convert_text_array(value):\n-        arr = csv.reader([backslashes_to_csv_escapes(value[1:-1])], delimiter=',', quotechar='\"', escapechar='\\\\').next()\n-        return map(convert, arr)\n+        arr = next(csv.reader([backslashes_to_csv_escapes(value[1:-1])]))",
        "path": "database/pgcsv_to_json",
        "position": null,
        "original_position": 15,
        "commit_id": "d4e95f13d56127406fc6868b272e7a5151a789a5",
        "original_commit_id": "e768fc59b44d3e98b574307644484138f34a3633",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Looks like this `csv.reader` is seeing a newline here?  I think this `backslashes_to_csv_escapes` was depending on the assumption of arrays appearing in a TSV stream.  So if we switch the input to CSV, not sure if we can keep the rest intact.\n",
        "created_at": "2016-06-23T19:59:46Z",
        "updated_at": "2016-07-05T15:39:19Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/551#discussion_r68304072",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/551",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/68304072"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/551#discussion_r68304072"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/551"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/68492954",
        "pull_request_review_id": null,
        "id": 68492954,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY4NDkyOTU0",
        "diff_hunk": "@@ -77,56 +77,12 @@ def convert_type_func(ty, ty_rest = \"\"):\n     else:\n       raise ValueError(\"Unsupported data type %s\" % ty)\n \n-# PostgreSQL COPY TO text Format parser\n-# See: http://www.postgresql.org/docs/9.1/static/sql-copy.html#AEN64302\n-pgTextEscapeSeqs = {\n-    \"b\": \"\\b\",\n-    \"f\": \"\\f\",\n-    \"n\": \"\\n\",\n-    \"r\": \"\\r\",\n-    \"t\": \"\\t\",\n-    \"v\": \"\\v\",\n-    }\n-def decode_pg_text_escapes(m):\n-  c = m.group(1)\n-  if c in pgTextEscapeSeqs:\n-    return pgTextEscapeSeqs[c]\n-  elif c.startswith(\"x\"):\n-    return chr(int(c, base=16))\n-  elif c.startswith(\"0\"):\n-    return chr(int(c, base=8))\n-  else:\n-    return c\n-def unescape_postgres_text_format(s):\n-  # unescape PostgreSQL text format\n-  return re.sub(r\"\\\\(.|0[0-7]{1,2}|x[0-9A-Fa-f]{1,2})\", decode_pg_text_escapes, s)\n-\n-def utf_8_encoder(unicode_csv_data):\n-    for line in unicode_csv_data:\n-        yield line.encode('utf-8')\n-\n-\n-def unicode_csv_reader(unicode_csv_data, **kwargs):\n-    '''A generator for unicode lines from a csv file.\n-\n-    This function takes all the same parameters as `csv.reader`. It was copied\n-    directly from the documentation for that module:\n-\n-    https://docs.python.org/2/library/csv.html#examples\n-\n-    '''\n-    # csv.py doesn't do Unicode; encode temporarily as UTF-8:\n-    csv_reader = csv.reader(utf_8_encoder(unicode_csv_data), **kwargs)\n-    for row in csv_reader:\n-        # decode UTF-8 back to Unicode, cell by cell:\n-        yield [unicode(cell, 'utf-8') for cell in row]\n-\n def main():\n   # parse column names and types from arguments\n   def parseArg(arg):\n       field_name, field_type = re.match(r\"(.+):([^:]+)\", arg).groups()\n       return field_name, convert_type_func(field_type)\n-  names_converters = map(parseArg, sys.argv[1:])\n+  names_converters = list(map(parseArg, sys.argv[1:]))\n \n   # read the PostgreSQL COPY TO text format\n   # XXX With Python's csv.reader, it's impossible to distinguish empty strings from nulls in PostgreSQL's csv output.",
        "path": "database/pgcsv_to_json",
        "position": null,
        "original_position": 93,
        "commit_id": "d4e95f13d56127406fc6868b272e7a5151a789a5",
        "original_commit_id": "d32b57b9d70ff8a01593830bffbfbc7b95a72ab6",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Looks like this is taken care of?\n",
        "created_at": "2016-06-25T19:42:04Z",
        "updated_at": "2016-07-05T15:39:19Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/551#discussion_r68492954",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/551",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/68492954"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/551#discussion_r68492954"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/551"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/68492971",
        "pull_request_review_id": null,
        "id": 68492971,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY4NDkyOTcx",
        "diff_hunk": "@@ -0,0 +1,611 @@\n+# a collection of SQL, PG TSV, CSV, and JSON data that can test a lot of corner case handling\n+\n+###############################################################################\n+## a nasty SQL input to test output formatters\n+NastySQL=\"\n+       SELECT 123::bigint as i\n+            , 45.678 as float\n+            , TRUE as t\n+            , FALSE as f\n+            , 'foo bar baz'::text AS s\n+            , ''::text AS empty_str\n+            , NULL::text AS n\n+            , 'NULL'   AS n1\n+            , 'null'   AS n2\n+            , E'\\\\\\\\N' AS n3\n+            , 'N'      AS n4\n+            , ARRAY[1,2,3] AS num_arr\n+            , ARRAY[1.2,3.45,67.89] AS float_arr\n+            , ARRAY[ 'easy'\n+                   , '123'\n+                   , 'abc'\n+                   , 'two words'\n+                   ] AS text_arr\n+            , ARRAY[ E'\\b'\n+                   , E'\\f'\n+                   , E'\\n'\n+                   , E'\\r'\n+                   , E'\\t'\n+                   , E'\\x1c'\n+                   , E'\\x1d'\n+                   , E'\\x1e \\x1f'\n+                   , E'\\x7f'\n+                   ] AS nonprintable\n+            , ARRAY[ E'\\b\\b'\n+                   , E'\\f\\f'\n+                   , E'\\n\\n'\n+                   , E'\\r\\r'\n+                   , E'\\t\\t'\n+                   , E'\\x1c\\x1c'\n+                   , E'\\x1d\\x1d'\n+                   , E'\\x1e\\x1e'\n+                   , E'\\x1f\\x1f'\n+                   , E'\\x7f\\x7f'\n+                   ] AS nonprintable2\n+            , ARRAY[ E'abc\\bdef\\bghi'\n+                   , E'abc\\fdef\\fghi'\n+                   , E'\\n\\n'\n+                   , E'\\r\\r'\n+                   , E'\\t\\t'\n+                   , E'\\x1c\\x1c'\n+                   , E'\\x1d\\x1d'\n+                   , E'\\x1e\\x1e'\n+                   , E'\\x1f\\x1f'\n+                   , E'\\x7f\\x7f'\n+                   ] AS nonprintable3\n+            , ARRAY[ '.'\n+                   , ','\n+                   , '.'\n+                   , '{'\n+                   , '}'\n+                   , '['\n+                   , ']'\n+                   , '('\n+                   , ')'\n+                   , '\\\"'\n+                   , E'\\\\\\\\' -- XXX Greenplum doesn't like the simpler '\\\\'\n+                   ] AS punctuations\n+            , ARRAY[ '.'\n+                   , ','\n+                   , '.'\n+                   , '{{'\n+                   , '}}'\n+                   , '[['\n+                   , ']]'\n+                   , '(('\n+                   , '))'\n+                   , E'\\\\\\\"'\n+                   , E'\\\\\\\\'\n+                   ] AS punctuations2\n+            , ARRAY[ 'asdf  qwer\"$'\\t'\"zxcv\"$'\\n'\"1234'\n+                   , ''\n+                   , 'NULL'\n+                   , 'null'\n+                   , E'\\\\\\\\N'\n+                   , 'N'\n+                   , '\\\"I''m your father,\\\" said Darth Vader.'\n+                   , E'\"'{\"csv in a json\": \"a,b c,\\\\\",\\\\\",\\\\\"line '\\'\\''1'\\'\\'$'\\n''bogus,NULL,null,\\\\\\\\N,N,line \\\\\"\\\\\"2\\\\\"\\\\\"\",  \"foo\":123,'$'\\n''\"bar\":45.678, \"null\": \"\\\\\\\\N\"}'\"'\n+                     -- XXX Greenplum (or older PostgreSQL 8.x) treats backslashes as escapes in strings '...'\n+                     -- and E'...' is a consistent way to write backslashes in string literal across versions\n+                   ] AS torture_arr\n+    \"\n+\n+# expected TSV output\n+TSVHeader=                         TSV=\n+TSVHeader+=$'\\t''i'                TSV+=$'\\t''123'\n+TSVHeader+=$'\\t''float'            TSV+=$'\\t''45.678'\n+TSVHeader+=$'\\t''t'                TSV+=$'\\t''t'\n+TSVHeader+=$'\\t''f'                TSV+=$'\\t''f'\n+TSVHeader+=$'\\t''s'                TSV+=$'\\t''foo bar baz'\n+TSVHeader+=$'\\t''empty_str'        TSV+=$'\\t'''\n+TSVHeader+=$'\\t''n'                TSV+=$'\\t''\\N'\n+TSVHeader+=$'\\t''n1'               TSV+=$'\\t''NULL'\n+TSVHeader+=$'\\t''n2'               TSV+=$'\\t''null'\n+TSVHeader+=$'\\t''n3'               TSV+=$'\\t''\\\\N'\n+TSVHeader+=$'\\t''n4'               TSV+=$'\\t''N'\n+TSVHeader+=$'\\t''num_arr'          TSV+=$'\\t''{1,2,3}'\n+TSVHeader+=$'\\t''float_arr'        TSV+=$'\\t''{1.2,3.45,67.89}'\n+TSVHeader+=$'\\t''text_arr'         TSV+=$'\\t''{easy,123,abc,\"two words\"}'\n+TSVHeader+=$'\\t''nonprintable'     TSV+=$'\\t''{\\b,\"\\f\",\"\\n\",\"\\r\",\"\\t\",'$'\\x1c'','$'\\x1d'',\"'$'\\x1e'' '$'\\x1f''\",'$'\\x7f''}'\n+TSVHeader+=$'\\t''nonprintable2'    TSV+=$'\\t''{\\b\\b,\"\\f\\f\",\"\\n\\n\",\"\\r\\r\",\"\\t\\t\",'$'\\x1c'$'\\x1c'','$'\\x1d'$'\\x1d'','$'\\x1e'$'\\x1e'','$'\\x1f'$'\\x1f'','$'\\x7f'$'\\x7f''}'\n+TSVHeader+=$'\\t''nonprintable3'    TSV+=$'\\t''{abc\\bdef\\bghi,\"abc\\fdef\\fghi\",\"\\n\\n\",\"\\r\\r\",\"\\t\\t\",'$'\\x1c'$'\\x1c'','$'\\x1d'$'\\x1d'','$'\\x1e'$'\\x1e'','$'\\x1f'$'\\x1f'','$'\\x7f'$'\\x7f''}'\n+TSVHeader+=$'\\t''punctuations'     TSV+=$'\\t''{.,\",\",.,\"{\",\"}\",[,],(,),\"\\\\\"\",\"\\\\\\\\\"}'\n+TSVHeader+=$'\\t''punctuations2'    TSV+=$'\\t''{.,\",\",.,\"{{\",\"}}\",[[,]],((,)),\"\\\\\"\",\"\\\\\\\\\"}'\n+TSVHeader+=$'\\t''torture_arr'      TSV+=$'\\t''{\"asdf  qwer\\tzxcv\\n1234\"'\n+                                        TSV+=',\"\"'\n+                                        TSV+=',\"NULL\"'\n+                                        TSV+=',\"null\"'\n+                                        TSV+=',\"\\\\\\\\N\"'\n+                                        TSV+=',N'\n+                                        TSV+=',\"\\\\\"I'\\''m your father,\\\\\" said Darth Vader.\"'\n+                                        TSV+=',\"{\\\\\"csv in a json\\\\\": \\\\\"a,b c,\\\\\\\\\\\\\",\\\\\\\\\\\\\",\\\\\\\\\\\\\"line '\\''1'\\''\\nbogus,NULL,null,\\\\\\\\\\\\\\\\N,N,line \\\\\\\\\\\\\"\\\\\\\\\\\\\"2\\\\\\\\\\\\\"\\\\\\\\\\\\\"\\\\\",  \\\\\"foo\\\\\":123,\\n\\\\\"bar\\\\\":45.678, \\\\\"null\\\\\": \\\\\"\\\\\\\\\\\\\\\\N\\\\\"}\"'\n+                                        TSV+='}'\n+TSVHeader=${TSVHeader#$'\\t'}       TSV=${TSV#$'\\t'}  # strip the first delimiter\n+NastyTSVHeader=$TSVHeader NastyTSV=$TSV\n+\n+# column types\n+Types=\n+Types+=$'\\t''int'\n+Types+=$'\\t''float'\n+Types+=$'\\t''boolean'\n+Types+=$'\\t''boolean'\n+Types+=$'\\t''text'\n+Types+=$'\\t''text'\n+Types+=$'\\t''text'\n+Types+=$'\\t''text'\n+Types+=$'\\t''text'\n+Types+=$'\\t''text'\n+Types+=$'\\t''text'\n+Types+=$'\\t''int[]'\n+Types+=$'\\t''float[]'\n+Types+=$'\\t''text[]'\n+Types+=$'\\t''text[]'\n+Types+=$'\\t''text[]'\n+Types+=$'\\t''text[]'\n+Types+=$'\\t''text[]'\n+Types+=$'\\t''text[]'\n+Types+=$'\\t''text[]'\n+Types=${Types#$'\\t'}\n+NastyTypes=$Types\n+\n+# columns names with types\n+NastyColumnTypes=$(paste <(tr '\\t' '\\n' <<<\"$TSVHeader\") <(tr '\\t' '\\n' <<<\"$Types\") | tr '\\t' :)\n+\n+# expected CSV output and header\n+CSVHeader=                     CSV=\n+CSVHeader+=,'i'                CSV+=,'123'\n+CSVHeader+=,'float'            CSV+=,'45.678'\n+CSVHeader+=,'t'                CSV+=,'t'\n+CSVHeader+=,'f'                CSV+=,'f'\n+CSVHeader+=,'s'                CSV+=,'foo bar baz'\n+CSVHeader+=,'empty_str'        CSV+=,'\"\"'\n+CSVHeader+=,'n'                CSV+=,''\n+CSVHeader+=,'n1'               CSV+=,'NULL'\n+CSVHeader+=,'n2'               CSV+=,'null'\n+CSVHeader+=,'n3'               CSV+=,'\\N'\n+CSVHeader+=,'n4'               CSV+=,'N'\n+CSVHeader+=,'num_arr'          CSV+=,'\"{1,2,3}\"'\n+CSVHeader+=,'float_arr'        CSV+=,'\"{1.2,3.45,67.89}\"'\n+CSVHeader+=,'text_arr'         CSV+=,'\"{easy,123,abc,\"\"two words\"\"}\"'\n+CSVHeader+=,'nonprintable'     CSV+=,'\"{'$'\\b'',\"\"'$'\\f''\"\",\"\"'$'\\n''\"\",\"\"'$'\\r''\"\",\"\"'$'\\t''\"\",'$'\\x1c'','$'\\x1d'',\"\"'$'\\x1e'' '$'\\x1f''\"\",'$'\\x7f''}\"'\n+CSVHeader+=,'nonprintable2'    CSV+=,'\"{'$'\\b'$'\\b'',\"\"'$'\\f'$'\\f''\"\",\"\"'$'\\n'$'\\n''\"\",\"\"'$'\\r'$'\\r''\"\",\"\"'$'\\t'$'\\t''\"\",'$'\\x1c'$'\\x1c'','$'\\x1d'$'\\x1d'','$'\\x1e'$'\\x1e'','$'\\x1f'$'\\x1f'','$'\\x7f'$'\\x7f''}\"'\n+CSVHeader+=,'nonprintable3'    CSV+=,'\"{abc'$'\\b''def'$'\\b''ghi,\"\"abc'$'\\f''def'$'\\f''ghi\"\",\"\"'$'\\n'$'\\n''\"\",\"\"'$'\\r'$'\\r''\"\",\"\"'$'\\t'$'\\t''\"\",'$'\\x1c'$'\\x1c'','$'\\x1d'$'\\x1d'','$'\\x1e'$'\\x1e'','$'\\x1f'$'\\x1f'','$'\\x7f'$'\\x7f''}\"'\n+CSVHeader+=,'punctuations'     CSV+=,'\"{.,\"\",\"\",.,\"\"{\"\",\"\"}\"\",[,],(,),\"\"\\\"\"\"\",\"\"\\\\\"\"}\"'\n+CSVHeader+=,'punctuations2'    CSV+=,'\"{.,\"\",\"\",.,\"\"{{\"\",\"\"}}\"\",[[,]],((,)),\"\"\\\"\"\"\",\"\"\\\\\"\"}\"'\n+CSVHeader+=,'torture_arr'      CSV+=,'\"{\"\"asdf  qwer'$'\\t''zxcv'$'\\n''1234\"\"'\n+                                 CSV+=',\"\"\"\"'\n+                                 CSV+=',\"\"NULL\"\"'\n+                                 CSV+=',\"\"null\"\"'\n+                                 CSV+=',\"\"\\\\N\"\"'\n+                                 CSV+=',N'\n+                                 CSV+=',\"\"\\\"\"I'\\''m your father,\\\"\" said Darth Vader.\"\"'\n+                                 CSV+=',\"\"{\\\"\"csv in a json\\\"\": \\\"\"a,b c,\\\\\\\"\",\\\\\\\"\",\\\\\\\"\"line '\\''1'\\'$'\\n''bogus,NULL,null,\\\\\\\\N,N,line \\\\\\\"\"\\\\\\\"\"2\\\\\\\"\"\\\\\\\"\"\\\"\",  \\\"\"foo\\\"\":123,'$'\\n''\\\"\"bar\\\"\":45.678, \\\"\"null\\\"\": \\\"\"\\\\\\\\N\\\"\"}\"\"'\n+                                 CSV+='}\"'\n+CSVHeader=${CSVHeader#,}       CSV=${CSV#,}  # strip the first delimiter\n+NastyCSVHeader=$CSVHeader NastyCSV=$CSV\n+\n+# expected JSON output\n+NastyJSON='\n+        {\n+          \"i\": 123,\n+          \"float\": 45.678,\n+          \"t\": true,\n+          \"f\": false,\n+          \"s\": \"foo bar baz\",\n+          \"empty_str\": \"\",\n+          \"n\": null,\n+          \"n1\": \"NULL\",\n+          \"n2\": \"null\",\n+          \"n3\": \"\\\\N\",\n+          \"n4\": \"N\",\n+          \"num_arr\": [\n+            1,\n+            2,\n+            3\n+          ],\n+          \"float_arr\": [\n+            1.2,\n+            3.45,\n+            67.89\n+          ],\n+          \"text_arr\": [\n+            \"easy\",\n+            \"123\",\n+            \"abc\",\n+            \"two words\"\n+          ],\n+          \"nonprintable\": [\n+            \"\\b\",\n+            \"\\f\",\n+            \"\\n\",\n+            \"\\r\",\n+            \"\\t\",\n+            \"\\u001c\",\n+            \"\\u001d\",\n+            \"\\u001e \\u001f\",\n+            \"\\u007f\"\n+          ],\n+          \"nonprintable2\": [\n+            \"\\b\\b\",\n+            \"\\f\\f\",\n+            \"\\n\\n\",\n+            \"\\r\\r\",\n+            \"\\t\\t\",\n+            \"\\u001c\\u001c\",\n+            \"\\u001d\\u001d\",\n+            \"\\u001e\\u001e\",\n+            \"\\u001f\\u001f\",\n+            \"\\u007f\\u007f\"\n+          ],\n+          \"nonprintable3\": [\n+            \"abc\\bdef\\bghi\",\n+            \"abc\\fdef\\fghi\",\n+            \"\\n\\n\",\n+            \"\\r\\r\",\n+            \"\\t\\t\",\n+            \"\\u001c\\u001c\",\n+            \"\\u001d\\u001d\",\n+            \"\\u001e\\u001e\",\n+            \"\\u001f\\u001f\",\n+            \"\\u007f\\u007f\"\n+          ],\n+          \"punctuations\": [\n+            \".\",\n+            \",\",\n+            \".\",\n+            \"{\",\n+            \"}\",\n+            \"[\",\n+            \"]\",\n+            \"(\",\n+            \")\",\n+            \"\\\"\",\n+            \"\\\\\"\n+          ],\n+          \"punctuations2\": [\n+            \".\",\n+            \",\",\n+            \".\",\n+            \"{{\",\n+            \"}}\",\n+            \"[[\",\n+            \"]]\",\n+            \"((\",\n+            \"))\",\n+            \"\\\"\",\n+            \"\\\\\"\n+          ],\n+          \"torture_arr\": [\n+            \"asdf  qwer\\tzxcv\\n1234\",\n+            \"\",\n+            \"NULL\",\n+            \"null\",\n+            \"\\\\N\",\n+            \"N\",\n+            \"\\\"I'\\''m your father,\\\" said Darth Vader.\",\n+            \"{\\\"csv in a json\\\": \\\"a,b c,\\\\\\\",\\\\\\\",\\\\\\\"line '\\''1'\\''\\nbogus,NULL,null,\\\\\\\\N,N,line \\\\\\\"\\\\\\\"2\\\\\\\"\\\\\\\"\\\",  \\\"foo\\\":123,\\n\\\"bar\\\":45.678, \\\"null\\\": \\\"\\\\\\\\N\\\"}\"\n+          ]\n+        }\n+    '\n+\n+###############################################################################\n+## a case where NULL is in an array\n+NullInArraySQL=\"SELECT 1 AS i\n+                     , ARRAY[''\n+                            , NULL\n+                            , 'NULL'\n+                            , 'null'\n+                            , E'\\\\\\\\N'\n+                            , 'N'\n+                            ] AS arr\n+    \"\n+\n+# expected TSV output for NULL in arrays\n+TSVHeader=                         TSV=\n+TSVHeader+=$'\\t''i'                TSV+=$'\\t''1'\n+TSVHeader+=$'\\t''arr'              TSV+=$'\\t''{\"\"'\n+                                        TSV+=',NULL'\n+                                        TSV+=',\"NULL\"'\n+                                        TSV+=',\"null\"'\n+                                        TSV+=',\"\\\\\\\\N\"'\n+                                        TSV+=',N'\n+                                        TSV+='}'\n+TSVHeader=${TSVHeader#$'\\t'}       TSV=${TSV#$'\\t'}  # strip the first delimiter\n+NullInArrayTSVHeader=$TSVHeader NullInArrayTSV=$TSV\n+\n+# column types\n+Types=\n+Types+=$'\\t''int'\n+Types+=$'\\t''text[]'\n+Types=${Types#$'\\t'}\n+NullInArrayTypes=$Types\n+\n+# columns names with types\n+NullInArrayColumnTypes=$(paste <(tr '\\t' '\\n' <<<\"$TSVHeader\") <(tr '\\t' '\\n' <<<\"$Types\") | tr '\\t' :)\n+\n+# expected CSV output for NULL in arrays\n+CSVHeader=                     CSV=\n+CSVHeader+=,'i'                CSV+=,'1'\n+CSVHeader+=,'arr'              CSV+=,'\"{\"\"\"\"'\n+                                 CSV+=',NULL'\n+                                 CSV+=',\"\"NULL\"\"'\n+                                 CSV+=',\"\"null\"\"'\n+                                 CSV+=',\"\"\\\\N\"\"'\n+                                 CSV+=',N'\n+                                 CSV+='}\"'\n+CSVHeader=${CSVHeader#,}       CSV=${CSV#,}  # strip the first delimiter\n+NullInArrayCSVHeader=$CSVHeader NullInArrayCSV=$CSV\n+\n+NullInArrayJSON='\n+        { \"i\": 1,\n+        \"arr\": [\n+            \"\",\n+            null,\n+            \"NULL\",\n+            \"null\",\n+            \"\\\\N\",\n+            \"N\"\n+        ] }\n+    '\n+\n+###############################################################################\n+## a case with nested array\n+NestedArraySQL=\"\n+       SELECT 123::bigint as i\n+            , ARRAY[ ARRAY[ 'not so easy', '123 45', '789 10' ]\n+                   , ARRAY[       E'\\b\\f',  E'\\n\\r',    E'\\t' ]\n+                   , ARRAY[        '.,\\\"', '{}[]()',  E'\\\\\\\\' ]\n+                   , ARRAY[            '',     NULL,   'NULL' ]\n+                   , ARRAY[        'null', E'\\\\\\\\N',      'N' ]\n+                   , ARRAY[ 'asdf  qwer\"$'\\t'\"zxcv\"$'\\n'\"1234'\n+                          , '\\\"I''m your father,\\\" said Darth Vader.'\n+                          , E'\"'{\"csv in a json\": \"a,b c,\\\\\",\\\\\",\\\\\"line '\\'\\''1'\\'\\'$'\\n''bogus,NULL,null,\\\\\\\\N,N,line \\\\\"\\\\\"2\\\\\"\\\\\"\",  \"foo\":123,'$'\\n''\"bar\":45.678, \"null\": \"\\\\\\\\N\"}'\"'\n+                          ]\n+                   ] AS text_arr_arr\n+    \"\n+\n+# expected TSV output\n+TSVHeader=                         TSV=\n+TSVHeader+=$'\\t''i'                TSV+=$'\\t''123'\n+TSVHeader+=$'\\t''text_arr_arr'     TSV+=$'\\t''{{\"not so easy\",\"123 45\",\"789 10\"}'\n+                                        TSV+=',{\"\\b\\f\",\"\\n\\r\",\"\\t\"}'\n+                                        TSV+=',{\".,\\\\\"\",\"{}[]()\",\"\\\\\\\\\"}'\n+                                        TSV+=',{\"\",NULL,\"NULL\"}'\n+                                        TSV+=',{\"null\",\"\\\\\\\\N\",N}'\n+                                        TSV+=',{\"asdf  qwer\\tzxcv\\n1234\"'\n+                                         TSV+=',\"\\\\\"I'\\''m your father,\\\\\" said Darth Vader.\"'\n+                                         TSV+=',\"{\\\\\"csv in a json\\\\\": \\\\\"a,b c,\\\\\\\\\\\\\",\\\\\\\\\\\\\",\\\\\\\\\\\\\"line '\\''1'\\''\\nbogus,NULL,null,\\\\\\\\\\\\\\\\N,N,line \\\\\\\\\\\\\"\\\\\\\\\\\\\"2\\\\\\\\\\\\\"\\\\\\\\\\\\\"\\\\\",  \\\\\"foo\\\\\":123,\\n\\\\\"bar\\\\\":45.678, \\\\\"null\\\\\": \\\\\"\\\\\\\\\\\\\\\\N\\\\\"}\"'\n+                                         TSV+='}'\n+                                        TSV+='}'\n+TSVHeader=${TSVHeader#$'\\t'}       TSV=${TSV#$'\\t'}  # strip the first delimiter\n+NestedArrayTSVHeader=$TSVHeader NestedArrayTSV=$TSV\n+\n+# column types\n+Types=\n+Types+=$'\\t''int'\n+Types+=$'\\t''text[][]'\n+Types=${Types#$'\\t'}\n+NestedArrayTypes=$Types\n+\n+# columns names with types\n+NestedArrayTypes=$(paste <(tr '\\t' '\\n' <<<\"$TSVHeader\") <(tr '\\t' '\\n' <<<\"$Types\") | tr '\\t' :)\n+\n+# expected CSV output and header\n+CSVHeader=                     CSV=\n+CSVHeader+=,'i'                CSV+=,'123'\n+CSVHeader+=,'text_arr_arr'     CSV+=,'\"{{\"\"not so easy\"\",\"\"123 45\"\",\"\"789 10\"\"}'\n+                                 CSV+=',{\"\"'$'\\b'$'\\f''\"\",\"\"'$'\\n'$'\\r''\"\",\"\"'$'\\t''\"\"}'\n+                                 CSV+=',{\"\".,\\\"\"\"\",\"\"{}[]()\"\",\"\"\\\\\"\"}'\n+                                 CSV+=',{\"\"\"\",NULL,\"\"NULL\"\"}'\n+                                 CSV+=',{\"\"null\"\",\"\"\\\\N\"\",N}'\n+                                 CSV+=',{\"\"asdf  qwer'$'\\t''zxcv'$'\\n''1234\"\"'\n+                                  CSV+=',\"\"\\\"\"I'\\''m your father,\\\"\" said Darth Vader.\"\"'\n+                                  CSV+=',\"\"{\\\"\"csv in a json\\\"\": \\\"\"a,b c,\\\\\\\"\",\\\\\\\"\",\\\\\\\"\"line '\\''1'\\'''$'\\n''bogus,NULL,null,\\\\\\\\N,N,line \\\\\\\"\"\\\\\\\"\"2\\\\\\\"\"\\\\\\\"\"\\\"\",  \\\"\"foo\\\"\":123,'$'\\n''\\\"\"bar\\\"\":45.678, \\\"\"null\\\"\": \\\"\"\\\\\\\\N\\\"\"}\"\"'\n+                                  CSV+='}'\n+                                 CSV+='}\"'\n+CSVHeader=${CSVHeader#,}       CSV=${CSV#,}  # strip the first delimiter\n+NestedArrayCSVHeader=$CSVHeader NestedArrayCSV=$CSV\n+\n+# expected JSON output\n+NestedArrayJSON='\n+        {\n+          \"i\": 123,\n+          \"text_arr_arr\": [ [ \"not so easy\", \"123 45\", \"789 10\" ]\n+                          , [        \"\\b\\f\",   \"\\n\\r\",     \"\\t\" ]\n+                          , [        \".,\\\"\", \"{}[]()\",     \"\\\\\" ]\n+                          , [            \"\",     null,   \"NULL\" ]\n+                          , [        \"null\",    \"\\\\N\",      \"N\" ]\n+                          , [ \"asdf  qwer\\tzxcv\\n1234\"\n+                            , \"\\\"I'\\''m your father,\\\" said Darth Vader.\"\n+                            , \"{\\\"csv in a json\\\": \\\"a,b c,\\\\\\\",\\\\\\\",\\\\\\\"line '\\''1'\\''\\nbogus,NULL,null,\\\\\\\\N,N,line \\\\\\\"\\\\\\\"2\\\\\\\"\\\\\\\"\\\",  \\\"foo\\\":123,\\n\\\"bar\\\":45.678, \\\"null\\\": \\\"\\\\\\\\N\\\"}\"\n+                            ]\n+          ]\n+        }\n+    '\n+\n+###############################################################################\n+## a case with unicode escapes\n+\n+# UTF-8 Sampler from http://www.columbia.edu/~fdc/utf8/\n+ICanEatGlass=()",
        "path": "database/test/corner_cases.bash",
        "position": 438,
        "original_position": 430,
        "commit_id": "d4e95f13d56127406fc6868b272e7a5151a789a5",
        "original_commit_id": "d32b57b9d70ff8a01593830bffbfbc7b95a72ab6",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Lovely\n",
        "created_at": "2016-06-25T19:43:45Z",
        "updated_at": "2016-07-05T15:39:19Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/551#discussion_r68492971",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/551",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/68492971"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/551#discussion_r68492971"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/551"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/68493055",
        "pull_request_review_id": null,
        "id": 68493055,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY4NDkzMDU1",
        "diff_hunk": "@@ -77,56 +77,12 @@ def convert_type_func(ty, ty_rest = \"\"):\n     else:\n       raise ValueError(\"Unsupported data type %s\" % ty)\n \n-# PostgreSQL COPY TO text Format parser\n-# See: http://www.postgresql.org/docs/9.1/static/sql-copy.html#AEN64302\n-pgTextEscapeSeqs = {\n-    \"b\": \"\\b\",\n-    \"f\": \"\\f\",\n-    \"n\": \"\\n\",\n-    \"r\": \"\\r\",\n-    \"t\": \"\\t\",\n-    \"v\": \"\\v\",\n-    }\n-def decode_pg_text_escapes(m):\n-  c = m.group(1)\n-  if c in pgTextEscapeSeqs:\n-    return pgTextEscapeSeqs[c]\n-  elif c.startswith(\"x\"):\n-    return chr(int(c, base=16))\n-  elif c.startswith(\"0\"):\n-    return chr(int(c, base=8))\n-  else:\n-    return c\n-def unescape_postgres_text_format(s):\n-  # unescape PostgreSQL text format\n-  return re.sub(r\"\\\\(.|0[0-7]{1,2}|x[0-9A-Fa-f]{1,2})\", decode_pg_text_escapes, s)\n-\n-def utf_8_encoder(unicode_csv_data):\n-    for line in unicode_csv_data:\n-        yield line.encode('utf-8')\n-\n-\n-def unicode_csv_reader(unicode_csv_data, **kwargs):\n-    '''A generator for unicode lines from a csv file.\n-\n-    This function takes all the same parameters as `csv.reader`. It was copied\n-    directly from the documentation for that module:\n-\n-    https://docs.python.org/2/library/csv.html#examples\n-\n-    '''\n-    # csv.py doesn't do Unicode; encode temporarily as UTF-8:\n-    csv_reader = csv.reader(utf_8_encoder(unicode_csv_data), **kwargs)\n-    for row in csv_reader:\n-        # decode UTF-8 back to Unicode, cell by cell:\n-        yield [unicode(cell, 'utf-8') for cell in row]\n-\n def main():\n   # parse column names and types from arguments\n   def parseArg(arg):\n       field_name, field_type = re.match(r\"(.+):([^:]+)\", arg).groups()\n       return field_name, convert_type_func(field_type)\n-  names_converters = map(parseArg, sys.argv[1:])\n+  names_converters = list(map(parseArg, sys.argv[1:]))\n \n   # read the PostgreSQL COPY TO text format\n   # XXX With Python's csv.reader, it's impossible to distinguish empty strings from nulls in PostgreSQL's csv output.",
        "path": "database/pgcsv_to_json",
        "position": null,
        "original_position": 93,
        "commit_id": "d4e95f13d56127406fc6868b272e7a5151a789a5",
        "original_commit_id": "d32b57b9d70ff8a01593830bffbfbc7b95a72ab6",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Nope. I just added some tests around this script. As I said, we should either 1) revert to tsv and handle all escape sequences explicitly, or 2) implement csv parsing to distinguish quoted values from unquoted.\n",
        "created_at": "2016-06-25T19:48:26Z",
        "updated_at": "2016-07-05T15:39:19Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/551#discussion_r68493055",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/551",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/68493055"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/551#discussion_r68493055"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/551"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/68493139",
        "pull_request_review_id": null,
        "id": 68493139,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY4NDkzMTM5",
        "diff_hunk": "@@ -77,56 +77,12 @@ def convert_type_func(ty, ty_rest = \"\"):\n     else:\n       raise ValueError(\"Unsupported data type %s\" % ty)\n \n-# PostgreSQL COPY TO text Format parser\n-# See: http://www.postgresql.org/docs/9.1/static/sql-copy.html#AEN64302\n-pgTextEscapeSeqs = {\n-    \"b\": \"\\b\",\n-    \"f\": \"\\f\",\n-    \"n\": \"\\n\",\n-    \"r\": \"\\r\",\n-    \"t\": \"\\t\",\n-    \"v\": \"\\v\",\n-    }\n-def decode_pg_text_escapes(m):\n-  c = m.group(1)\n-  if c in pgTextEscapeSeqs:\n-    return pgTextEscapeSeqs[c]\n-  elif c.startswith(\"x\"):\n-    return chr(int(c, base=16))\n-  elif c.startswith(\"0\"):\n-    return chr(int(c, base=8))\n-  else:\n-    return c\n-def unescape_postgres_text_format(s):\n-  # unescape PostgreSQL text format\n-  return re.sub(r\"\\\\(.|0[0-7]{1,2}|x[0-9A-Fa-f]{1,2})\", decode_pg_text_escapes, s)\n-\n-def utf_8_encoder(unicode_csv_data):\n-    for line in unicode_csv_data:\n-        yield line.encode('utf-8')\n-\n-\n-def unicode_csv_reader(unicode_csv_data, **kwargs):\n-    '''A generator for unicode lines from a csv file.\n-\n-    This function takes all the same parameters as `csv.reader`. It was copied\n-    directly from the documentation for that module:\n-\n-    https://docs.python.org/2/library/csv.html#examples\n-\n-    '''\n-    # csv.py doesn't do Unicode; encode temporarily as UTF-8:\n-    csv_reader = csv.reader(utf_8_encoder(unicode_csv_data), **kwargs)\n-    for row in csv_reader:\n-        # decode UTF-8 back to Unicode, cell by cell:\n-        yield [unicode(cell, 'utf-8') for cell in row]\n-\n def main():\n   # parse column names and types from arguments\n   def parseArg(arg):\n       field_name, field_type = re.match(r\"(.+):([^:]+)\", arg).groups()\n       return field_name, convert_type_func(field_type)\n-  names_converters = map(parseArg, sys.argv[1:])\n+  names_converters = list(map(parseArg, sys.argv[1:]))\n \n   # read the PostgreSQL COPY TO text format\n   # XXX With Python's csv.reader, it's impossible to distinguish empty strings from nulls in PostgreSQL's csv output.",
        "path": "database/pgcsv_to_json",
        "position": null,
        "original_position": 93,
        "commit_id": "d4e95f13d56127406fc6868b272e7a5151a789a5",
        "original_commit_id": "d32b57b9d70ff8a01593830bffbfbc7b95a72ab6",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "OK, I thought the `csv null as '\\\\N'` would address this ambiguity. I suppose this script is called from more than one place... Would the \"FORCE_QUOTE\" option of COPY also help here?\n\nYou have the best judgement of whether 1 or 2 is better.\n",
        "created_at": "2016-06-25T19:55:17Z",
        "updated_at": "2016-07-05T15:39:19Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/551#discussion_r68493139",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/551",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/68493139"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/551#discussion_r68493139"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/551"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/68793960",
        "pull_request_review_id": null,
        "id": 68793960,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY4NzkzOTYw",
        "diff_hunk": "@@ -135,14 +139,25 @@ def main():\n   # See: http://grokbase.com/t/python/python-ideas/131b0eaykx/csv-dialect-enhancement\n   # See: http://stackoverflow.com/questions/11379300/python-csv-reader-behavior-with-none-and-empty-string\n   # See: https://github.com/JoshClose/CsvHelper/issues/252\n-  reader = unicode_csv_reader(sys.stdin, delimiter='\\t', quotechar=None, quoting=csv.QUOTE_NONE)\n+  reader = unicode_csv_reader(in_file, delimiter='\\t', quotechar=None, quoting=csv.QUOTE_NONE)\n   for line in reader:\n     obj = {}\n     for (name, convert), field in zip(names_converters, line):\n-        obj[name] = None if field == \"\\\\N\" \\\n+        obj[name] = None if field in NULL_STRINGS \\",
        "path": "database/pgtsv_to_json",
        "position": null,
        "original_position": 102,
        "commit_id": "d4e95f13d56127406fc6868b272e7a5151a789a5",
        "original_commit_id": "8a7bda4eeae6f95feeffc7be97882349d4d65388",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I think only `\\N` should be treated as None at the top level?  `NULL`s are None only within arrays.\n",
        "created_at": "2016-06-28T16:38:22Z",
        "updated_at": "2016-07-05T15:39:19Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/551#discussion_r68793960",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/551",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/68793960"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/551#discussion_r68793960"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/551"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/69664850",
        "pull_request_review_id": null,
        "id": 69664850,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY5NjY0ODUw",
        "diff_hunk": "@@ -0,0 +1,38 @@\n+#!/usr/bin/env bash\n+set -euo pipefail -x\n+\n+unset PYTHONPATH  # existing PYTHONPATH can interfere\n+\n+( # install a Python virtualenv\n+if ! type virtualenv &>/dev/null; then\n+    # if virtualenv isn't available, install it locally along with pip\n+    [[ -e bootstrap/virtualenv.py ]] || {\n+        curl -RLO https://bootstrap.pypa.io/get-pip.py &&\n+        python get-pip.py virtualenv --ignore-installed --target bootstrap",
        "path": "extern/bundled/python-virtualenv/install.sh",
        "position": 11,
        "original_position": 11,
        "commit_id": "9ce214705c7c944545e6b261fa444c367d099466",
        "original_commit_id": "13daea5774bf55dce64425b3c4ec7e7cabfb46a9",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Could `python` here be python3? Should we bundle `python` as well?\n",
        "created_at": "2016-07-06T01:34:57Z",
        "updated_at": "2016-07-07T01:37:18Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/558#discussion_r69664850",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/558",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/69664850"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/558#discussion_r69664850"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/558"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/69666557",
        "pull_request_review_id": null,
        "id": 69666557,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY5NjY2NTU3",
        "diff_hunk": "@@ -0,0 +1,24 @@\n+#!/usr/bin/env bash\n+# app-has-nothing-to-compile -- Checks the DeepDive app has no updated source\n+# $ app-has-nothing-to-compile\n+# Checks standard DeepDive app artifacts that can change the result of\n+# `deepdive compile`, such as app.ddlog, deepdive.conf, and returns 0 if\n+# nothing is newer than the previously compiled `run/compiled/config.json`.\n+##\n+set -eu\n+\n+DEEPDIVE_APP=$(find-deepdive-app)\n+export DEEPDIVE_APP\n+cd \"$DEEPDIVE_APP\"\n+\n+# source files to check\n+set -- app.ddlog deepdive.conf schema.json\n+\n+# check if each source file is newer than last compiled result\n+num_updated=0\n+for src; do\n+    [[ \"$src\" -nt \"$DEEPDIVE_APP\"/run/compiled/config.json ]] || continue",
        "path": "compiler/app-has-nothing-to-compile",
        "position": 20,
        "original_position": 20,
        "commit_id": "9ce214705c7c944545e6b261fa444c367d099466",
        "original_commit_id": "13daea5774bf55dce64425b3c4ec7e7cabfb46a9",
        "user": {
            "login": "shahin",
            "id": 20877,
            "node_id": "MDQ6VXNlcjIwODc3",
            "avatar_url": "https://avatars3.githubusercontent.com/u/20877?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shahin",
            "html_url": "https://github.com/shahin",
            "followers_url": "https://api.github.com/users/shahin/followers",
            "following_url": "https://api.github.com/users/shahin/following{/other_user}",
            "gists_url": "https://api.github.com/users/shahin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shahin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shahin/subscriptions",
            "organizations_url": "https://api.github.com/users/shahin/orgs",
            "repos_url": "https://api.github.com/users/shahin/repos",
            "events_url": "https://api.github.com/users/shahin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shahin/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "seems like this is what `make` does but I see that `deepdive-compile` is a custom script instead a makefile. Is this just a preference or does it reflect an incompatibility between our build and `make`?\n",
        "created_at": "2016-07-06T02:03:59Z",
        "updated_at": "2016-07-07T01:37:18Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/558#discussion_r69666557",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/558",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/69666557"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/558#discussion_r69666557"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/558"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/69668476",
        "pull_request_review_id": null,
        "id": 69668476,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY5NjY4NDc2",
        "diff_hunk": "@@ -0,0 +1,24 @@\n+#!/usr/bin/env bash\n+# app-has-nothing-to-compile -- Checks the DeepDive app has no updated source\n+# $ app-has-nothing-to-compile\n+# Checks standard DeepDive app artifacts that can change the result of\n+# `deepdive compile`, such as app.ddlog, deepdive.conf, and returns 0 if\n+# nothing is newer than the previously compiled `run/compiled/config.json`.\n+##\n+set -eu\n+\n+DEEPDIVE_APP=$(find-deepdive-app)\n+export DEEPDIVE_APP\n+cd \"$DEEPDIVE_APP\"\n+\n+# source files to check\n+set -- app.ddlog deepdive.conf schema.json\n+\n+# check if each source file is newer than last compiled result\n+num_updated=0\n+for src; do\n+    [[ \"$src\" -nt \"$DEEPDIVE_APP\"/run/compiled/config.json ]] || continue",
        "path": "compiler/app-has-nothing-to-compile",
        "position": 20,
        "original_position": 20,
        "commit_id": "9ce214705c7c944545e6b261fa444c367d099466",
        "original_commit_id": "13daea5774bf55dce64425b3c4ec7e7cabfb46a9",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Good point. Large portion of it can be rewritten into a makefile but there's a little extra done so we'll still need some custom steps pre/post `make`. \n",
        "created_at": "2016-07-06T02:39:36Z",
        "updated_at": "2016-07-07T01:37:18Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/558#discussion_r69668476",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/558",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/69668476"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/558#discussion_r69668476"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/558"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/69671508",
        "pull_request_review_id": null,
        "id": 69671508,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY5NjcxNTA4",
        "diff_hunk": "@@ -0,0 +1,38 @@\n+#!/usr/bin/env bash\n+set -euo pipefail -x\n+\n+unset PYTHONPATH  # existing PYTHONPATH can interfere\n+\n+( # install a Python virtualenv\n+if ! type virtualenv &>/dev/null; then\n+    # if virtualenv isn't available, install it locally along with pip\n+    [[ -e bootstrap/virtualenv.py ]] || {\n+        curl -RLO https://bootstrap.pypa.io/get-pip.py &&\n+        python get-pip.py virtualenv --ignore-installed --target bootstrap",
        "path": "extern/bundled/python-virtualenv/install.sh",
        "position": 11,
        "original_position": 11,
        "commit_id": "9ce214705c7c944545e6b261fa444c367d099466",
        "original_commit_id": "13daea5774bf55dce64425b3c4ec7e7cabfb46a9",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I guess it'll follow the system default at build time which may be undesirable. `python` also seems to get bundled as part of virtualenv but it's not exposed and only used for the pyhocon command. My intention for the bundled virtualenv was to serve only internal components but we might want to find a nice way to provide a default set of packages to the user. What's lacking here, I've been increasingly worried about, is a clear separation between the environment user's codes run in and deepdive's internal components do. We probably should try to not pollute user's env and document what exactly gets added to it, e.g., ddlib, psycopg2. Ideally users should be able to run deepdive in another virtualenv and their udfs shouldn't get interfered by the bundled virtualenv. Curious to hear any thoughts along the line!\n",
        "created_at": "2016-07-06T03:37:44Z",
        "updated_at": "2016-07-07T01:37:18Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/558#discussion_r69671508",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/558",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/69671508"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/558#discussion_r69671508"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/558"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/69672364",
        "pull_request_review_id": null,
        "id": 69672364,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY5NjcyMzY0",
        "diff_hunk": "@@ -0,0 +1,38 @@\n+#!/usr/bin/env bash\n+set -euo pipefail -x\n+\n+unset PYTHONPATH  # existing PYTHONPATH can interfere\n+\n+( # install a Python virtualenv\n+if ! type virtualenv &>/dev/null; then\n+    # if virtualenv isn't available, install it locally along with pip\n+    [[ -e bootstrap/virtualenv.py ]] || {\n+        curl -RLO https://bootstrap.pypa.io/get-pip.py &&\n+        python get-pip.py virtualenv --ignore-installed --target bootstrap",
        "path": "extern/bundled/python-virtualenv/install.sh",
        "position": 11,
        "original_position": 11,
        "commit_id": "9ce214705c7c944545e6b261fa444c367d099466",
        "original_commit_id": "13daea5774bf55dce64425b3c4ec7e7cabfb46a9",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Agree that it's probably a bad idea to pollute the runtime user's env. For internal use, should we just stick with `python3` (better unicode handling at least, also no ambiguity)? Do we ever need to bundle ddlib (assuming it's the ancient feature lib)?\n",
        "created_at": "2016-07-06T03:56:10Z",
        "updated_at": "2016-07-07T01:37:18Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/558#discussion_r69672364",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/558",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/69672364"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/558#discussion_r69672364"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/558"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/69673234",
        "pull_request_review_id": null,
        "id": 69673234,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY5NjczMjM0",
        "diff_hunk": "@@ -0,0 +1,38 @@\n+#!/usr/bin/env bash\n+set -euo pipefail -x\n+\n+unset PYTHONPATH  # existing PYTHONPATH can interfere\n+\n+( # install a Python virtualenv\n+if ! type virtualenv &>/dev/null; then\n+    # if virtualenv isn't available, install it locally along with pip\n+    [[ -e bootstrap/virtualenv.py ]] || {\n+        curl -RLO https://bootstrap.pypa.io/get-pip.py &&\n+        python get-pip.py virtualenv --ignore-installed --target bootstrap",
        "path": "extern/bundled/python-virtualenv/install.sh",
        "position": 11,
        "original_position": 11,
        "commit_id": "9ce214705c7c944545e6b261fa444c367d099466",
        "original_commit_id": "13daea5774bf55dce64425b3c4ec7e7cabfb46a9",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I'd also like to default to python3 but need to check if virtualenv is standalone or needs a host python3.  ddlib provides some crucial decorators for UDFs (`@tsv_extractor` parsing/formatting PG TSV and `@tsj_extractor` for TSJ in the future) and the new automatic feature lib is not here yet, so yes users want/need ddlib.\n",
        "created_at": "2016-07-06T04:15:14Z",
        "updated_at": "2016-07-07T01:37:18Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/558#discussion_r69673234",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/558",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/69673234"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/558#discussion_r69673234"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/558"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/69353528",
        "pull_request_review_id": null,
        "id": 69353528,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY5MzUzNTI4",
        "diff_hunk": "@@ -0,0 +1,1000 @@\n+1",
        "path": "examples/balance/input/yes.tsv",
        "position": null,
        "original_position": 1,
        "commit_id": "90d67128e3a8f5f5649b7fb610f0138af8967714",
        "original_commit_id": "6807cf50db889604f4dd7dea9469795e1a7dffd3",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "You can replace this with a line in `input/yes.tsv.sh`:\n\n```\nseq 1000\n```\n\nand `chmod +x input/yes.tsv.sh`.  Same thing applies to `no.tsv`.\n",
        "created_at": "2016-07-01T20:44:00Z",
        "updated_at": "2016-07-07T21:38:13Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/555#discussion_r69353528",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/555",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/69353528"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/555#discussion_r69353528"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/555"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/69353815",
        "pull_request_review_id": null,
        "id": 69353815,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY5MzUzODE1",
        "diff_hunk": "@@ -0,0 +1,1000 @@\n+1",
        "path": "examples/balance/input/yes.tsv",
        "position": null,
        "original_position": 1,
        "commit_id": "90d67128e3a8f5f5649b7fb610f0138af8967714",
        "original_commit_id": "6807cf50db889604f4dd7dea9469795e1a7dffd3",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "done\n",
        "created_at": "2016-07-01T20:46:26Z",
        "updated_at": "2016-07-07T21:38:13Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/555#discussion_r69353815",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/555",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/69353815"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/555#discussion_r69353815"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/555"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/69354916",
        "pull_request_review_id": null,
        "id": 69354916,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY5MzU0OTE2",
        "diff_hunk": "@@ -0,0 +1,12 @@\n+yes (vote_id bigint).\n+no  (vote_id bigint).\n+\n+whatever? (\n+    candidate bigint\n+).\n+\n+@weight(1)\n+whatever(p) :- yes(_).",
        "path": "examples/balance/app.ddlog",
        "position": 9,
        "original_position": 9,
        "commit_id": "90d67128e3a8f5f5649b7fb610f0138af8967714",
        "original_commit_id": "8004afe12519f3e33678ab12ce244401b0198d75",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "To match the paper more closely, I think this should be an imply factor rather than a unary (also turning the two others into variables).\n\n```\n@weight(1)\nyes(_) => whatever(p) :- TRUE.\n```\n\nIn the paper we overloaded the datalog head/body syntax with implication, which later has been disambiguated into a separate syntax.  That gives users precise control of mapping data to the model.  (I know, the `:- TRUE` part looks ugly and shouldn't be mandatory)\n",
        "created_at": "2016-07-01T20:57:35Z",
        "updated_at": "2016-07-07T21:38:13Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/555#discussion_r69354916",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/555",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/69354916"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/555#discussion_r69354916"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/555"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/69356382",
        "pull_request_review_id": null,
        "id": 69356382,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY5MzU2Mzgy",
        "diff_hunk": "@@ -0,0 +1,12 @@\n+yes (vote_id bigint).\n+no  (vote_id bigint).\n+\n+whatever? (\n+    candidate bigint\n+).\n+\n+@weight(1)\n+whatever(p) :- yes(_).",
        "path": "examples/balance/app.ddlog",
        "position": 9,
        "original_position": 9,
        "commit_id": "90d67128e3a8f5f5649b7fb610f0138af8967714",
        "original_commit_id": "8004afe12519f3e33678ab12ce244401b0198d75",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Grounding would fail with that syntax... I think currently we require all relations in the inference rule head to be a variable relation.\n",
        "created_at": "2016-07-01T21:12:36Z",
        "updated_at": "2016-07-07T21:38:13Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/555#discussion_r69356382",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/555",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/69356382"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/555#discussion_r69356382"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/555"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/69364881",
        "pull_request_review_id": null,
        "id": 69364881,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY5MzY0ODgx",
        "diff_hunk": "@@ -0,0 +1,12 @@\n+yes (vote_id bigint).\n+no  (vote_id bigint).\n+\n+whatever? (\n+    candidate bigint\n+).\n+\n+@weight(1)\n+whatever(p) :- yes(_).",
        "path": "examples/balance/app.ddlog",
        "position": 9,
        "original_position": 9,
        "commit_id": "90d67128e3a8f5f5649b7fb610f0138af8967714",
        "original_commit_id": "8004afe12519f3e33678ab12ce244401b0198d75",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Right. So they also need the question marks in the schema and can be given labels to act as evidence.\n",
        "created_at": "2016-07-01T23:02:51Z",
        "updated_at": "2016-07-07T21:38:13Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/555#discussion_r69364881",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/555",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/69364881"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/555#discussion_r69364881"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/555"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/69385611",
        "pull_request_review_id": null,
        "id": 69385611,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY5Mzg1NjEx",
        "diff_hunk": "@@ -0,0 +1,12 @@\n+yes (vote_id bigint).\n+no  (vote_id bigint).\n+\n+whatever? (\n+    candidate bigint\n+).\n+\n+@weight(1)\n+whatever(p) :- yes(_).",
        "path": "examples/balance/app.ddlog",
        "position": 9,
        "original_position": 9,
        "commit_id": "90d67128e3a8f5f5649b7fb610f0138af8967714",
        "original_commit_id": "8004afe12519f3e33678ab12ce244401b0198d75",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "OK, I don't think it's necessary to make `yes` / `no` variables or use IMPLY. The ratio semantics concern the counts of \"positive\" and \"negative\" factors.\n",
        "created_at": "2016-07-03T04:55:31Z",
        "updated_at": "2016-07-07T21:38:13Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/555#discussion_r69385611",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/555",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/69385611"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/555#discussion_r69385611"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/555"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/70379787",
        "pull_request_review_id": null,
        "id": 70379787,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcwMzc5Nzg3",
        "diff_hunk": "@@ -1 +1 @@\n-Subproject commit 22f4fdb40dc2e165e501affd9091267f1acfe200\n+Subproject commit 091296cd6e04fc21e9ec384eba6f2b5de7ec90c6",
        "path": "compiler/ddlog",
        "position": null,
        "original_position": 2,
        "commit_id": "384a64f0a6f4c3749edc0b924f15dd3bbdb7829d",
        "original_commit_id": "a093a5744ce1bc36b7ad5a2bb26c607e2b05a484",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Can you update ddlog to master after the merge that just happened?\n",
        "created_at": "2016-07-12T05:40:55Z",
        "updated_at": "2016-07-12T05:41:47Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/561#discussion_r70379787",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/561",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/70379787"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/561#discussion_r70379787"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/561"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/70379796",
        "pull_request_review_id": null,
        "id": 70379796,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcwMzc5Nzk2",
        "diff_hunk": "@@ -4,10 +4,10 @@\n . \"$BATS_TEST_DIRNAME\"/env.sh >&2\n \n : ${CHUNKING_TEST_NUM_WORDS_TRAIN:=1000} ${CHUNKING_TEST_NUM_WORDS_TEST:=200}\n-: ${CHUNKING_TEST_MIN_F1SCORE:=60}\n+: ${CHUNKING_TEST_MIN_F1SCORE:=80}\n \n : ${CHUNKING_TEST_REUSE_NUM_WORDS_TEST:=200}\n-: ${CHUNKING_TEST_REUSE_MIN_F1SCORE:=60}\n+: ${CHUNKING_TEST_REUSE_MIN_F1SCORE:=80}",
        "path": "test/postgresql/chunking_example.bats",
        "position": 9,
        "original_position": 9,
        "commit_id": "384a64f0a6f4c3749edc0b924f15dd3bbdb7829d",
        "original_commit_id": "a093a5744ce1bc36b7ad5a2bb26c607e2b05a484",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Interesting.. the skip chains were actually hurting the quality?\n",
        "created_at": "2016-07-12T05:41:04Z",
        "updated_at": "2016-07-12T05:41:47Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/561#discussion_r70379796",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/561",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/70379796"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/561#discussion_r70379796"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/561"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/70380036",
        "pull_request_review_id": null,
        "id": 70380036,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcwMzgwMDM2",
        "diff_hunk": "@@ -4,10 +4,10 @@\n . \"$BATS_TEST_DIRNAME\"/env.sh >&2\n \n : ${CHUNKING_TEST_NUM_WORDS_TRAIN:=1000} ${CHUNKING_TEST_NUM_WORDS_TEST:=200}\n-: ${CHUNKING_TEST_MIN_F1SCORE:=60}\n+: ${CHUNKING_TEST_MIN_F1SCORE:=80}\n \n : ${CHUNKING_TEST_REUSE_NUM_WORDS_TEST:=200}\n-: ${CHUNKING_TEST_REUSE_MIN_F1SCORE:=60}\n+: ${CHUNKING_TEST_REUSE_MIN_F1SCORE:=80}",
        "path": "test/postgresql/chunking_example.bats",
        "position": 9,
        "original_position": 9,
        "commit_id": "384a64f0a6f4c3749edc0b924f15dd3bbdb7829d",
        "original_commit_id": "a093a5744ce1bc36b7ad5a2bb26c607e2b05a484",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "It was PHONY! It was a mesh ([i, j] for all 0 <= i < j < N), not skip chain! Keeping that rule would drag the F1 to 53. Removing it gets us F1 = 87.\n",
        "created_at": "2016-07-12T05:43:48Z",
        "updated_at": "2016-07-12T05:44:11Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/561#discussion_r70380036",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/561",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/70380036"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/561#discussion_r70380036"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/561"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71075627",
        "pull_request_review_id": null,
        "id": 71075627,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcxMDc1NjI3",
        "diff_hunk": "@@ -0,0 +1,43 @@\n+# Dockerfile to build and test DeepDive inside a container\n+#\n+# `make build-in-container test-in-container` uses master image built by this.\n+# `util/build/docker/` contains utilities relevant to this.\n+FROM ubuntu",
        "path": "Dockerfile",
        "position": 5,
        "original_position": 5,
        "commit_id": "9163b46ffe48f0b7dae299c9fe8b9ecb3e91036d",
        "original_commit_id": "a700a5651bec8060ad1868501fad6fd953b9f931",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Maybe specify a version? say 16.04\n",
        "created_at": "2016-07-17T02:53:30Z",
        "updated_at": "2016-07-19T02:45:40Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/564#discussion_r71075627",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/564",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71075627"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/564#discussion_r71075627"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/564"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71075647",
        "pull_request_review_id": null,
        "id": 71075647,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcxMDc1NjQ3",
        "diff_hunk": "@@ -0,0 +1,43 @@\n+#!/usr/bin/env bash\n+# test-in-container-postgres -- Tests with PostgreSQL container linked\n+#\n+# Uses: https://hub.docker.com/_/postgres/\n+##\n+set -euo pipefail\n+. \"$(dirname \"$0\")\"/config.bash\n+\n+: ${DOCKER_PGNAME:=test-postgres.$$}\n+: ${POSTGRES_PASSWORD:=$RANDOM$RANDOM}\n+: ${POSTGRES_START_TIME:=4} # seconds\n+\n+# run a postgres container\n+trap 'docker rm -f \"$DOCKER_PGNAME\"' EXIT\n+set -x\n+docker run --detach --name \"$DOCKER_PGNAME\" \\\n+    --env POSTGRES_USER=\"$USER\" \\\n+    --env POSTGRES_PASSWORD=\"$POSTGRES_PASSWORD\" \\\n+    postgres \\\n+    #\n+\n+# wait until postgres starts\n+postgres-has-started() {\n+    docker run --rm --name \"$DOCKER_PGNAME.check\" \\\n+        --link \"$DOCKER_PGNAME\" \\\n+        --env PGPASSWORD=\"$POSTGRES_PASSWORD\" \\\n+        postgres \\\n+        psql -h \"$DOCKER_PGNAME\" -U \"$USER\" -l\n+}\n+sleep $POSTGRES_START_TIME  # give it some initial time to start\n+while ! postgres-has-started; do sleep 1.$RANDOM; done",
        "path": "DockerBuild/test-in-container-postgres",
        "position": 31,
        "original_position": 31,
        "commit_id": "9163b46ffe48f0b7dae299c9fe8b9ecb3e91036d",
        "original_commit_id": "a700a5651bec8060ad1868501fad6fd953b9f931",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Not sure if it's worth it for now: if we need more services as fixture in the future, we may want to try docker compose.\n",
        "created_at": "2016-07-17T02:56:04Z",
        "updated_at": "2016-07-19T02:45:40Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/564#discussion_r71075647",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/564",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71075647"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/564#discussion_r71075647"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/564"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71076341",
        "pull_request_review_id": null,
        "id": 71076341,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcxMDc2MzQx",
        "diff_hunk": "@@ -0,0 +1,43 @@\n+#!/usr/bin/env bash\n+# test-in-container-postgres -- Tests with PostgreSQL container linked\n+#\n+# Uses: https://hub.docker.com/_/postgres/\n+##\n+set -euo pipefail\n+. \"$(dirname \"$0\")\"/config.bash\n+\n+: ${DOCKER_PGNAME:=test-postgres.$$}\n+: ${POSTGRES_PASSWORD:=$RANDOM$RANDOM}\n+: ${POSTGRES_START_TIME:=4} # seconds\n+\n+# run a postgres container\n+trap 'docker rm -f \"$DOCKER_PGNAME\"' EXIT\n+set -x\n+docker run --detach --name \"$DOCKER_PGNAME\" \\\n+    --env POSTGRES_USER=\"$USER\" \\\n+    --env POSTGRES_PASSWORD=\"$POSTGRES_PASSWORD\" \\\n+    postgres \\\n+    #\n+\n+# wait until postgres starts\n+postgres-has-started() {\n+    docker run --rm --name \"$DOCKER_PGNAME.check\" \\\n+        --link \"$DOCKER_PGNAME\" \\\n+        --env PGPASSWORD=\"$POSTGRES_PASSWORD\" \\\n+        postgres \\\n+        psql -h \"$DOCKER_PGNAME\" -U \"$USER\" -l\n+}\n+sleep $POSTGRES_START_TIME  # give it some initial time to start\n+while ! postgres-has-started; do sleep 1.$RANDOM; done",
        "path": "DockerBuild/test-in-container-postgres",
        "position": 31,
        "original_position": 31,
        "commit_id": "9163b46ffe48f0b7dae299c9fe8b9ecb3e91036d",
        "original_commit_id": "a700a5651bec8060ad1868501fad6fd953b9f931",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Sure we should definitely switch to the right way like docker compose soon.  I didn't have chance to play with it yet and didn't want to complicate things without fully understanding it, which will take some time.  Btw Greenplum tests should be fixed and included in Travis first (it's not working yet as the clients are missing in the build image).\n",
        "created_at": "2016-07-17T04:29:48Z",
        "updated_at": "2016-07-19T02:45:40Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/564#discussion_r71076341",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/564",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71076341"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/564#discussion_r71076341"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/564"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71076365",
        "pull_request_review_id": null,
        "id": 71076365,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcxMDc2MzY1",
        "diff_hunk": "@@ -0,0 +1,43 @@\n+# Dockerfile to build and test DeepDive inside a container\n+#\n+# `make build-in-container test-in-container` uses master image built by this.\n+# `util/build/docker/` contains utilities relevant to this.\n+FROM ubuntu",
        "path": "Dockerfile",
        "position": 5,
        "original_position": 5,
        "commit_id": "9163b46ffe48f0b7dae299c9fe8b9ecb3e91036d",
        "original_commit_id": "a700a5651bec8060ad1868501fad6fd953b9f931",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "There are pros and cons.  I think the source code should be portable, even for non-ubuntu, so I'd vote against fixing it.  Any compelling reason to do so?\n",
        "created_at": "2016-07-17T04:33:12Z",
        "updated_at": "2016-07-19T02:45:40Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/564#discussion_r71076365",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/564",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71076365"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/564#discussion_r71076365"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/564"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71076413",
        "pull_request_review_id": null,
        "id": 71076413,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcxMDc2NDEz",
        "diff_hunk": "@@ -0,0 +1,43 @@\n+# Dockerfile to build and test DeepDive inside a container\n+#\n+# `make build-in-container test-in-container` uses master image built by this.\n+# `util/build/docker/` contains utilities relevant to this.\n+FROM ubuntu",
        "path": "Dockerfile",
        "position": 5,
        "original_position": 5,
        "commit_id": "9163b46ffe48f0b7dae299c9fe8b9ecb3e91036d",
        "original_commit_id": "a700a5651bec8060ad1868501fad6fd953b9f931",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I don't know how \"ubuntu\" is resolved -- maybe it's resolved to \"latest\" so in October it'll be ubuntu:16.10 right after the release. But if during the next 18 months, production envs stick to 16.04 (which is the most likely scenario), then Travis would be testing against an env different from prod during that time. When DD breaks on 16.04 but not on 16.10, Travis wouldn't catch that.\n",
        "created_at": "2016-07-17T04:38:52Z",
        "updated_at": "2016-07-19T02:45:40Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/564#discussion_r71076413",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/564",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71076413"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/564#discussion_r71076413"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/564"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71076598",
        "pull_request_review_id": null,
        "id": 71076598,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcxMDc2NTk4",
        "diff_hunk": "@@ -0,0 +1,43 @@\n+# Dockerfile to build and test DeepDive inside a container\n+#\n+# `make build-in-container test-in-container` uses master image built by this.\n+# `util/build/docker/` contains utilities relevant to this.\n+FROM ubuntu",
        "path": "Dockerfile",
        "position": 5,
        "original_position": 5,
        "commit_id": "9163b46ffe48f0b7dae299c9fe8b9ecb3e91036d",
        "original_commit_id": "a700a5651bec8060ad1868501fad6fd953b9f931",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "This Dockerfile and DockerBuild/ scripts are for development.  For production and normal users, I think we should have a separate set of Dockerfiles that just installs the built DD binary releases with their runtime dependencies.  For that we can/should certainly fix the ubuntu version.  However, if we're running everything containerized, perhaps the host OS distro doesn't even matter.  We could just mount the DD app and run it to populate another database container.  However, yes for standalone releases, we may want to test it against various flavors of distros and versions, or just explicitly target few of them.\n\nJustification for separate production images: The deepdive-build image is now ~4-5GiB.  However, given that the release binary is ~0.1GiB compressed and as the runtime dependencies are going to be much smaller than build deps, I'd say the production image can easily go under 1GiB.\n",
        "created_at": "2016-07-17T05:05:31Z",
        "updated_at": "2016-07-19T02:45:40Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/564#discussion_r71076598",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/564",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71076598"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/564#discussion_r71076598"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/564"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71076636",
        "pull_request_review_id": null,
        "id": 71076636,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcxMDc2NjM2",
        "diff_hunk": "@@ -0,0 +1,43 @@\n+# Dockerfile to build and test DeepDive inside a container\n+#\n+# `make build-in-container test-in-container` uses master image built by this.\n+# `util/build/docker/` contains utilities relevant to this.\n+FROM ubuntu",
        "path": "Dockerfile",
        "position": 5,
        "original_position": 5,
        "commit_id": "9163b46ffe48f0b7dae299c9fe8b9ecb3e91036d",
        "original_commit_id": "a700a5651bec8060ad1868501fad6fd953b9f931",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "And yes, `ubuntu` is `ubuntu:latest` and I have to check if their policy is to strictly follow LTS or literally any latest release. (I hope the former but may not be the case)\n",
        "created_at": "2016-07-17T05:12:08Z",
        "updated_at": "2016-07-19T02:45:40Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/564#discussion_r71076636",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/564",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71076636"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/564#discussion_r71076636"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/564"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71076951",
        "pull_request_review_id": null,
        "id": 71076951,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcxMDc2OTUx",
        "diff_hunk": "@@ -0,0 +1,43 @@\n+# Dockerfile to build and test DeepDive inside a container\n+#\n+# `make build-in-container test-in-container` uses master image built by this.\n+# `util/build/docker/` contains utilities relevant to this.\n+FROM ubuntu",
        "path": "Dockerfile",
        "position": 5,
        "original_position": 5,
        "commit_id": "9163b46ffe48f0b7dae299c9fe8b9ecb3e91036d",
        "original_commit_id": "a700a5651bec8060ad1868501fad6fd953b9f931",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Sounds good -- we can keep a separate production build process.\n",
        "created_at": "2016-07-17T05:49:06Z",
        "updated_at": "2016-07-19T02:45:40Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/564#discussion_r71076951",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/564",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71076951"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/564#discussion_r71076951"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/564"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/72929754",
        "pull_request_review_id": null,
        "id": 72929754,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcyOTI5NzU0",
        "diff_hunk": "@@ -1,8 +1,8 @@\n-#!/bin/sh\n+#!/bin/bash",
        "path": "shell/jq",
        "position": null,
        "original_position": 2,
        "commit_id": "095ebf973e7c8813d132a96b3e6a2d1288014603",
        "original_commit_id": "5f0136041999116963fc88df9e72c4d0337135dc",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Please keep this `/bin/sh`.  Maybe `type -p -a` below needs to be `which -a`..\n",
        "created_at": "2016-08-01T06:42:47Z",
        "updated_at": "2016-08-05T00:53:18Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/573#discussion_r72929754",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/573",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/72929754"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/573#discussion_r72929754"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/573"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/72929881",
        "pull_request_review_id": null,
        "id": 72929881,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcyOTI5ODgx",
        "diff_hunk": "@@ -0,0 +1,43 @@\n+if [ \"$(uname)\" == \"Darwin\" ]; then\n+  # C++11 finagling for Mac OSX\n+  export CC=clang\n+  export CXX=clang++\n+  export MACOSX_VERSION_MIN=\"10.9\"\n+  CXXFLAGS=\"${CXXFLAGS} -mmacosx-version-min=${MACOSX_VERSION_MIN}\"\n+  CXXFLAGS=\"${CXXFLAGS} -Wno-error=unused-command-line-argument\"\n+  export LDFLAGS=\"${LDFLAGS} -mmacosx-version-min=${MACOSX_VERSION_MIN}\"\n+  export LINKFLAGS=\"${LDFLAGS}\"\n+  export MACOSX_DEPLOYMENT_TARGET=10.9\n+\n+  # make sure clang-format is installed\n+  # e.g. brew install clang-format\n+\n+  # make sure autoreconf can be found\n+  # e.g. brew install autoconf\n+\n+  # for graphviz, also brew install autogen libtool\n+fi\n+\n+make install PREFIX=$PREFIX\n+\n+# add the /util directory to the PATH inside this conda env",
        "path": "conda.recipe/build.sh",
        "position": 23,
        "original_position": 23,
        "commit_id": "095ebf973e7c8813d132a96b3e6a2d1288014603",
        "original_commit_id": "5f0136041999116963fc88df9e72c4d0337135dc",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Not sure what a conda env is, but why do we want to expose everything in util/?  Most of them aren't standalone executables, i.e., won't work outside an env set up by bin/deepdive.. so this will introduce a lot of broken commands to the conda env.\n",
        "created_at": "2016-08-01T06:44:06Z",
        "updated_at": "2016-08-05T00:53:18Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/573#discussion_r72929881",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/573",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/72929881"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/573#discussion_r72929881"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/573"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/72929972",
        "pull_request_review_id": null,
        "id": 72929972,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcyOTI5OTcy",
        "diff_hunk": "@@ -98,7 +98,7 @@ def normalize_type_name(ty):\n   if ty in CANONICAL_TYPE_BY_NAME:\n     return CANONICAL_TYPE_BY_NAME[ty]\n   else:\n-    for patt,ty_canonical in CANONICAL_TYPE_BY_REGEX.iteritems():\n+    for patt,ty_canonical in CANONICAL_TYPE_BY_REGEX.items():",
        "path": "ddlib/ddlib/util.py",
        "position": 5,
        "original_position": 5,
        "commit_id": "095ebf973e7c8813d132a96b3e6a2d1288014603",
        "original_commit_id": "5f0136041999116963fc88df9e72c4d0337135dc",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Seems like we're sacrificing Python2 performance for code compatibility with Python3.. Is there a way to still use generators in py2 for simple iterations?  Or is there proof that the difference is negligible?\n",
        "created_at": "2016-08-01T06:45:01Z",
        "updated_at": "2016-08-05T00:53:18Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/573#discussion_r72929972",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/573",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/72929972"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/573#discussion_r72929972"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/573"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/73014149",
        "pull_request_review_id": null,
        "id": 73014149,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDczMDE0MTQ5",
        "diff_hunk": "@@ -98,7 +98,7 @@ def normalize_type_name(ty):\n   if ty in CANONICAL_TYPE_BY_NAME:\n     return CANONICAL_TYPE_BY_NAME[ty]\n   else:\n-    for patt,ty_canonical in CANONICAL_TYPE_BY_REGEX.iteritems():\n+    for patt,ty_canonical in CANONICAL_TYPE_BY_REGEX.items():",
        "path": "ddlib/ddlib/util.py",
        "position": 5,
        "original_position": 5,
        "commit_id": "095ebf973e7c8813d132a96b3e6a2d1288014603",
        "original_commit_id": "5f0136041999116963fc88df9e72c4d0337135dc",
        "user": {
            "login": "shahin",
            "id": 20877,
            "node_id": "MDQ6VXNlcjIwODc3",
            "avatar_url": "https://avatars3.githubusercontent.com/u/20877?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shahin",
            "html_url": "https://github.com/shahin",
            "followers_url": "https://api.github.com/users/shahin/followers",
            "following_url": "https://api.github.com/users/shahin/following{/other_user}",
            "gists_url": "https://api.github.com/users/shahin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shahin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shahin/subscriptions",
            "organizations_url": "https://api.github.com/users/shahin/orgs",
            "repos_url": "https://api.github.com/users/shahin/repos",
            "events_url": "https://api.github.com/users/shahin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shahin/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "This dictionary has only six things in it and it's a constant, so the risk of impacting performance is small and could go in either direction. We can definitely find some way to use an iterator in both py2 and py3 here, but it'll make the code more complex. In your experience, is this piece of code a hot path?\n",
        "created_at": "2016-08-01T17:03:39Z",
        "updated_at": "2016-08-05T00:53:18Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/573#discussion_r73014149",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/573",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/73014149"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/573#discussion_r73014149"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/573"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/75963012",
        "pull_request_review_id": null,
        "id": 75963012,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDc1OTYzMDEy",
        "diff_hunk": "@@ -180,5 +180,8 @@ done\n # load everything queued so far\n loadQueued\n \n+# always analyze relation after loading\n+deepdive db analyze \"$Relation\"",
        "path": "database/deepdive-load",
        "position": null,
        "original_position": 5,
        "commit_id": "5c7496769dd68a94162c634b947aa53f558f85b2",
        "original_commit_id": "c1340676030f7d3ea3dc477e4005d87c1b1c44f1",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Might be better to do `deepdive-db analyze` given the `deepdive-mark done` pattern?\n",
        "created_at": "2016-08-23T22:24:30Z",
        "updated_at": "2016-08-23T23:07:51Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/584#discussion_r75963012",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/584",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/75963012"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/584#discussion_r75963012"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/584"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71381414",
        "pull_request_review_id": null,
        "id": 71381414,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcxMzgxNDE0",
        "diff_hunk": "@@ -0,0 +1,73 @@\n+#!/usr/bin/env python\n+# db-query-tsj -- Runs SQL against PostgreSQL and formats output rows in TSJ\n+#\n+# $ db-query-tsj SQL COLUMN_TYPE...\n+##\n+import psycopg2, psycopg2.extras, ujson\n+import sys, subprocess, os, re\n+\n+# get arguments from command-line\n+def usage(*msg):\n+    err = subprocess.call([\"usage\", sys.argv[0]] + list(msg))\n+    if err != 0: sys.exit(err)\n+if not len(sys.argv) > 1: usage(\"Missing SQL\")\n+sql = sys.argv[1]\n+if not len(sys.argv) > 2: usage(\"Missing COLUMN_TYPEs\")\n+column_types = sys.argv[2:]\n+\n+# write TSJ to stdout\n+tsj_output = sys.stdout\n+\n+\n+# SQL types that are already formatted in JSON (to bypass ujson.dump)\n+TYPES_TO_PASS_THRU = [ \"json\" ]\n+\n+# PostgreSQL date/time types adapted by psycopg that need special formatting\n+# See: http://initd.org/psycopg/docs/usage.html#adapt-date\n+# See: https://www.postgresql.org/docs/current/static/datatype-datetime.html\n+TYPES_DATETIME_PATTERNS = [re.compile(regex) for regex in (\n+        r'date',\n+        r'interval(?:|.+)',\n+        r'timestamp\\s*(?:\\(\\d\\))?(?:\\s*with(?:out)?\\s+time\\s+zone)?',\n+        )]\n+\n+# generate code to output columns of each row based on their types\n+# See: http://lucumr.pocoo.org/2011/2/1/exec-in-python/\n+stmts = []\n+for i,ty in enumerate(column_types):",
        "path": "database/db-driver/postgresql/db-query-tsj",
        "position": 37,
        "original_position": 37,
        "commit_id": "958864644304e408eb36289c102b086f4bd9a866",
        "original_commit_id": "40bc5a4e25e53050d4e1e8efb4155ed42c8ec8f9",
        "user": {
            "login": "shahin",
            "id": 20877,
            "node_id": "MDQ6VXNlcjIwODc3",
            "avatar_url": "https://avatars3.githubusercontent.com/u/20877?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shahin",
            "html_url": "https://github.com/shahin",
            "followers_url": "https://api.github.com/users/shahin/followers",
            "following_url": "https://api.github.com/users/shahin/following{/other_user}",
            "gists_url": "https://api.github.com/users/shahin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shahin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shahin/subscriptions",
            "organizations_url": "https://api.github.com/users/shahin/orgs",
            "repos_url": "https://api.github.com/users/shahin/repos",
            "events_url": "https://api.github.com/users/shahin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shahin/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Should this be wrapped in a testable function?\n",
        "created_at": "2016-07-19T17:12:10Z",
        "updated_at": "2016-11-03T05:31:52Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/565#discussion_r71381414",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/565",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71381414"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/565#discussion_r71381414"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/565"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71382033",
        "pull_request_review_id": null,
        "id": 71382033,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcxMzgyMDMz",
        "diff_hunk": "@@ -0,0 +1,73 @@\n+#!/usr/bin/env python\n+# db-query-tsj -- Runs SQL against PostgreSQL and formats output rows in TSJ\n+#\n+# $ db-query-tsj SQL COLUMN_TYPE...\n+##\n+import psycopg2, psycopg2.extras, ujson\n+import sys, subprocess, os, re\n+\n+# get arguments from command-line\n+def usage(*msg):\n+    err = subprocess.call([\"usage\", sys.argv[0]] + list(msg))\n+    if err != 0: sys.exit(err)\n+if not len(sys.argv) > 1: usage(\"Missing SQL\")\n+sql = sys.argv[1]\n+if not len(sys.argv) > 2: usage(\"Missing COLUMN_TYPEs\")\n+column_types = sys.argv[2:]\n+\n+# write TSJ to stdout\n+tsj_output = sys.stdout\n+\n+\n+# SQL types that are already formatted in JSON (to bypass ujson.dump)\n+TYPES_TO_PASS_THRU = [ \"json\" ]\n+\n+# PostgreSQL date/time types adapted by psycopg that need special formatting\n+# See: http://initd.org/psycopg/docs/usage.html#adapt-date\n+# See: https://www.postgresql.org/docs/current/static/datatype-datetime.html\n+TYPES_DATETIME_PATTERNS = [re.compile(regex) for regex in (\n+        r'date',\n+        r'interval(?:|.+)',\n+        r'timestamp\\s*(?:\\(\\d\\))?(?:\\s*with(?:out)?\\s+time\\s+zone)?',\n+        )]\n+\n+# generate code to output columns of each row based on their types\n+# See: http://lucumr.pocoo.org/2011/2/1/exec-in-python/\n+stmts = []\n+for i,ty in enumerate(column_types):\n+    ty = ty.lower()\n+    if i > 0: # separate each column by TAB\n+        stmts.append('tsj_output.write(\"\\\\t\")')\n+    if ty in TYPES_TO_PASS_THRU:\n+        # passthru JSON compatible columns\n+        stmts.append('tsj_output.write(columns[%d])' % i)\n+    elif any(pattern.match(ty) for pattern in TYPES_DATETIME_PATTERNS):\n+        # ujson needs us to format datetime\n+        stmts.append('ujson.dump(columns[%d].isoformat(), tsj_output)' % i)\n+    else:  # dump in JSON except a few\n+        stmts.append('ujson.dump(columns[%d], tsj_output)' % i)\n+stmts.append('tsj_output.write(\"\\\\n\")')\n+exec compile(\"def write_tsj_output(columns):\\n\" + \"\\n\".join(\"  \" + s for s in stmts),\n+        \"%s codegen\" % sys.argv[0], \"exec\")\n+\n+# tell psycopg2 to actually skip parsing any JSON\n+psycopg2.extras.register_default_json(loads=lambda x: x)\n+# XXX or use ujson.loads if you really must parse it\n+#psycopg2.extras.register_default_json(loads=ujson.loads)\n+\n+# use pyscopg2 to access the database\n+conn = psycopg2.connect(",
        "path": "database/db-driver/postgresql/db-query-tsj",
        "position": 59,
        "original_position": 59,
        "commit_id": "958864644304e408eb36289c102b086f4bd9a866",
        "original_commit_id": "40bc5a4e25e53050d4e1e8efb4155ed42c8ec8f9",
        "user": {
            "login": "shahin",
            "id": 20877,
            "node_id": "MDQ6VXNlcjIwODc3",
            "avatar_url": "https://avatars3.githubusercontent.com/u/20877?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shahin",
            "html_url": "https://github.com/shahin",
            "followers_url": "https://api.github.com/users/shahin/followers",
            "following_url": "https://api.github.com/users/shahin/following{/other_user}",
            "gists_url": "https://api.github.com/users/shahin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shahin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shahin/subscriptions",
            "organizations_url": "https://api.github.com/users/shahin/orgs",
            "repos_url": "https://api.github.com/users/shahin/repos",
            "events_url": "https://api.github.com/users/shahin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shahin/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "This will try to connect to some database when the module is imported. I know that we're running it as a script in deepdive, but this makes it harder to test, won't play nice with pdb, and makes it difficult to compose with other Python.\n\nWe can have the best of both worlds, though, if we throw all the connection stuff into an `if __name__ == '__main__'`. Would that work here?\n",
        "created_at": "2016-07-19T17:15:42Z",
        "updated_at": "2016-11-03T05:31:52Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/565#discussion_r71382033",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/565",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71382033"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/565#discussion_r71382033"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/565"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71382837",
        "pull_request_review_id": null,
        "id": 71382837,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcxMzgyODM3",
        "diff_hunk": "@@ -0,0 +1,73 @@\n+#!/usr/bin/env python\n+# db-query-tsj -- Runs SQL against PostgreSQL and formats output rows in TSJ\n+#\n+# $ db-query-tsj SQL COLUMN_TYPE...\n+##\n+import psycopg2, psycopg2.extras, ujson\n+import sys, subprocess, os, re\n+\n+# get arguments from command-line\n+def usage(*msg):\n+    err = subprocess.call([\"usage\", sys.argv[0]] + list(msg))\n+    if err != 0: sys.exit(err)\n+if not len(sys.argv) > 1: usage(\"Missing SQL\")",
        "path": "database/db-driver/postgresql/db-query-tsj",
        "position": 13,
        "original_position": 13,
        "commit_id": "958864644304e408eb36289c102b086f4bd9a866",
        "original_commit_id": "40bc5a4e25e53050d4e1e8efb4155ed42c8ec8f9",
        "user": {
            "login": "shahin",
            "id": 20877,
            "node_id": "MDQ6VXNlcjIwODc3",
            "avatar_url": "https://avatars3.githubusercontent.com/u/20877?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shahin",
            "html_url": "https://github.com/shahin",
            "followers_url": "https://api.github.com/users/shahin/followers",
            "following_url": "https://api.github.com/users/shahin/following{/other_user}",
            "gists_url": "https://api.github.com/users/shahin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shahin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shahin/subscriptions",
            "organizations_url": "https://api.github.com/users/shahin/orgs",
            "repos_url": "https://api.github.com/users/shahin/repos",
            "events_url": "https://api.github.com/users/shahin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shahin/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I'm thinking argument checks should go in `if __name__ == __main__` in case someone needs to import this instead of running from the command line.\n",
        "created_at": "2016-07-19T17:20:16Z",
        "updated_at": "2016-11-03T05:31:52Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/565#discussion_r71382837",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/565",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71382837"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/565#discussion_r71382837"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/565"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71383827",
        "pull_request_review_id": null,
        "id": 71383827,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcxMzgzODI3",
        "diff_hunk": "@@ -371,3 +379,45 @@ def tsv_extractor(generator):\n   for row in parser.parse_stdin():\n     for out_row in generator(**row._asdict()):\n       printer.write(out_row)\n+\n+\n+def tsj_extractor(generator):\n+  \"\"\"\n+  When a generator function is decorated with this (i.e., @tsj_extractor\n+  preceding the def line), each standard input line is parsed as\n+  tab-separated JSON (TSJ) values, then the function is applied to the parsed\n+  array to generate output rows, and each output row expected to be an array\n+  is formatted as TSJ.\n+  \"\"\"\n+  reload(sys).setdefaultencoding(\"utf8\")  # to avoid UnicodeEncodeError of JSON values during conversion by str()\n+\n+  input_format, output_format = get_generator_format(generator)\n+  input_names  = [name for name,t in input_format]\n+  num_input_values = len(input_format)\n+  num_input_splits = num_input_values - 1\n+  num_output_values = len(output_format)\n+\n+  def parse_json(column_index, json_value):",
        "path": "ddlib/ddlib/util.py",
        "position": 79,
        "original_position": 79,
        "commit_id": "958864644304e408eb36289c102b086f4bd9a866",
        "original_commit_id": "40bc5a4e25e53050d4e1e8efb4155ed42c8ec8f9",
        "user": {
            "login": "shahin",
            "id": 20877,
            "node_id": "MDQ6VXNlcjIwODc3",
            "avatar_url": "https://avatars3.githubusercontent.com/u/20877?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shahin",
            "html_url": "https://github.com/shahin",
            "followers_url": "https://api.github.com/users/shahin/followers",
            "following_url": "https://api.github.com/users/shahin/following{/other_user}",
            "gists_url": "https://api.github.com/users/shahin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shahin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shahin/subscriptions",
            "organizations_url": "https://api.github.com/users/shahin/orgs",
            "repos_url": "https://api.github.com/users/shahin/repos",
            "events_url": "https://api.github.com/users/shahin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shahin/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Can we make this a top-level, testable/importable function?\n",
        "created_at": "2016-07-19T17:25:32Z",
        "updated_at": "2016-11-03T05:31:52Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/565#discussion_r71383827",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/565",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71383827"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/565#discussion_r71383827"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/565"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71392873",
        "pull_request_review_id": null,
        "id": 71392873,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcxMzkyODcz",
        "diff_hunk": "@@ -0,0 +1,73 @@\n+#!/usr/bin/env python\n+# db-query-tsj -- Runs SQL against PostgreSQL and formats output rows in TSJ\n+#\n+# $ db-query-tsj SQL COLUMN_TYPE...\n+##\n+import psycopg2, psycopg2.extras, ujson\n+import sys, subprocess, os, re\n+\n+# get arguments from command-line\n+def usage(*msg):\n+    err = subprocess.call([\"usage\", sys.argv[0]] + list(msg))\n+    if err != 0: sys.exit(err)\n+if not len(sys.argv) > 1: usage(\"Missing SQL\")\n+sql = sys.argv[1]\n+if not len(sys.argv) > 2: usage(\"Missing COLUMN_TYPEs\")\n+column_types = sys.argv[2:]\n+\n+# write TSJ to stdout\n+tsj_output = sys.stdout\n+\n+\n+# SQL types that are already formatted in JSON (to bypass ujson.dump)\n+TYPES_TO_PASS_THRU = [ \"json\" ]\n+\n+# PostgreSQL date/time types adapted by psycopg that need special formatting\n+# See: http://initd.org/psycopg/docs/usage.html#adapt-date\n+# See: https://www.postgresql.org/docs/current/static/datatype-datetime.html\n+TYPES_DATETIME_PATTERNS = [re.compile(regex) for regex in (\n+        r'date',\n+        r'interval(?:|.+)',\n+        r'timestamp\\s*(?:\\(\\d\\))?(?:\\s*with(?:out)?\\s+time\\s+zone)?',\n+        )]\n+\n+# generate code to output columns of each row based on their types\n+# See: http://lucumr.pocoo.org/2011/2/1/exec-in-python/\n+stmts = []\n+for i,ty in enumerate(column_types):\n+    ty = ty.lower()\n+    if i > 0: # separate each column by TAB\n+        stmts.append('tsj_output.write(\"\\\\t\")')\n+    if ty in TYPES_TO_PASS_THRU:\n+        # passthru JSON compatible columns\n+        stmts.append('tsj_output.write(columns[%d])' % i)\n+    elif any(pattern.match(ty) for pattern in TYPES_DATETIME_PATTERNS):\n+        # ujson needs us to format datetime\n+        stmts.append('ujson.dump(columns[%d].isoformat(), tsj_output)' % i)\n+    else:  # dump in JSON except a few\n+        stmts.append('ujson.dump(columns[%d], tsj_output)' % i)\n+stmts.append('tsj_output.write(\"\\\\n\")')\n+exec compile(\"def write_tsj_output(columns):\\n\" + \"\\n\".join(\"  \" + s for s in stmts),\n+        \"%s codegen\" % sys.argv[0], \"exec\")\n+\n+# tell psycopg2 to actually skip parsing any JSON\n+psycopg2.extras.register_default_json(loads=lambda x: x)\n+# XXX or use ujson.loads if you really must parse it\n+#psycopg2.extras.register_default_json(loads=ujson.loads)\n+\n+# use pyscopg2 to access the database\n+conn = psycopg2.connect(",
        "path": "database/db-driver/postgresql/db-query-tsj",
        "position": 59,
        "original_position": 59,
        "commit_id": "958864644304e408eb36289c102b086f4bd9a866",
        "original_commit_id": "40bc5a4e25e53050d4e1e8efb4155ed42c8ec8f9",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Good point.  My goal was to keep code minimal without any frills just for implementing this executable, but I agree this piece can be a generally useful Python module.  I'm thinking of moving this into somewhere like ddlib.pgtsj, have it installed in our virtualenv, and have this script call the main after doing a bit of argument parsing.  .pyc's can be used that way although I doubt it'd make any big difference for this one.  How does that sound?\n\nBtw Is there a CLI generator you recommend?  It seems pip, virtualenv, etc. were all following a convention and their script seemed generated.\n",
        "created_at": "2016-07-19T18:12:55Z",
        "updated_at": "2016-11-03T05:31:52Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/565#discussion_r71392873",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/565",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71392873"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/565#discussion_r71392873"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/565"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71396033",
        "pull_request_review_id": null,
        "id": 71396033,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcxMzk2MDMz",
        "diff_hunk": "@@ -0,0 +1,73 @@\n+#!/usr/bin/env python\n+# db-query-tsj -- Runs SQL against PostgreSQL and formats output rows in TSJ\n+#\n+# $ db-query-tsj SQL COLUMN_TYPE...\n+##\n+import psycopg2, psycopg2.extras, ujson\n+import sys, subprocess, os, re\n+\n+# get arguments from command-line\n+def usage(*msg):\n+    err = subprocess.call([\"usage\", sys.argv[0]] + list(msg))\n+    if err != 0: sys.exit(err)\n+if not len(sys.argv) > 1: usage(\"Missing SQL\")\n+sql = sys.argv[1]\n+if not len(sys.argv) > 2: usage(\"Missing COLUMN_TYPEs\")\n+column_types = sys.argv[2:]\n+\n+# write TSJ to stdout\n+tsj_output = sys.stdout\n+\n+\n+# SQL types that are already formatted in JSON (to bypass ujson.dump)\n+TYPES_TO_PASS_THRU = [ \"json\" ]\n+\n+# PostgreSQL date/time types adapted by psycopg that need special formatting\n+# See: http://initd.org/psycopg/docs/usage.html#adapt-date\n+# See: https://www.postgresql.org/docs/current/static/datatype-datetime.html\n+TYPES_DATETIME_PATTERNS = [re.compile(regex) for regex in (\n+        r'date',\n+        r'interval(?:|.+)',\n+        r'timestamp\\s*(?:\\(\\d\\))?(?:\\s*with(?:out)?\\s+time\\s+zone)?',\n+        )]\n+\n+# generate code to output columns of each row based on their types\n+# See: http://lucumr.pocoo.org/2011/2/1/exec-in-python/\n+stmts = []\n+for i,ty in enumerate(column_types):\n+    ty = ty.lower()\n+    if i > 0: # separate each column by TAB\n+        stmts.append('tsj_output.write(\"\\\\t\")')\n+    if ty in TYPES_TO_PASS_THRU:\n+        # passthru JSON compatible columns\n+        stmts.append('tsj_output.write(columns[%d])' % i)\n+    elif any(pattern.match(ty) for pattern in TYPES_DATETIME_PATTERNS):\n+        # ujson needs us to format datetime\n+        stmts.append('ujson.dump(columns[%d].isoformat(), tsj_output)' % i)\n+    else:  # dump in JSON except a few\n+        stmts.append('ujson.dump(columns[%d], tsj_output)' % i)\n+stmts.append('tsj_output.write(\"\\\\n\")')\n+exec compile(\"def write_tsj_output(columns):\\n\" + \"\\n\".join(\"  \" + s for s in stmts),\n+        \"%s codegen\" % sys.argv[0], \"exec\")\n+\n+# tell psycopg2 to actually skip parsing any JSON\n+psycopg2.extras.register_default_json(loads=lambda x: x)\n+# XXX or use ujson.loads if you really must parse it\n+#psycopg2.extras.register_default_json(loads=ujson.loads)\n+\n+# use pyscopg2 to access the database\n+conn = psycopg2.connect(",
        "path": "database/db-driver/postgresql/db-query-tsj",
        "position": 59,
        "original_position": 59,
        "commit_id": "958864644304e408eb36289c102b086f4bd9a866",
        "original_commit_id": "40bc5a4e25e53050d4e1e8efb4155ed42c8ec8f9",
        "user": {
            "login": "shahin",
            "id": 20877,
            "node_id": "MDQ6VXNlcjIwODc3",
            "avatar_url": "https://avatars3.githubusercontent.com/u/20877?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shahin",
            "html_url": "https://github.com/shahin",
            "followers_url": "https://api.github.com/users/shahin/followers",
            "following_url": "https://api.github.com/users/shahin/following{/other_user}",
            "gists_url": "https://api.github.com/users/shahin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shahin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shahin/subscriptions",
            "organizations_url": "https://api.github.com/users/shahin/orgs",
            "repos_url": "https://api.github.com/users/shahin/repos",
            "events_url": "https://api.github.com/users/shahin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shahin/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Yeah, a separate package that we install in our virtualenv sounds great. I agree that we don't need to do anything special w/ .pyc's here.\n\nWhen you say \"CLI generator\" I think of two different things:\n1. For exposing executable commands from an installed Python package, consider using `entry_points`. Here's [a general overview](https://chriswarrick.com/blog/2014/09/15/python-apps-the-right-way-entry_points-and-scripts/) and here's [an example in memex](https://github.com/LatticeData/memex/tree/71b8ccb0730392c7ae442b44413eb6dae6ec3c69/queryjson). \n2. For parsing arguments, [`argparse`](https://docs.python.org/3/library/argparse.html) is the most common and straightfoward. When I have a really complex interface with lots of options I tend to use [`docopt`](https://github.com/docopt/docopt/blob/master/examples/arguments_example.py).\n\nLet me know if I misunderstood and you were actually asking about something else.\n",
        "created_at": "2016-07-19T18:28:52Z",
        "updated_at": "2016-11-03T05:31:52Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/565#discussion_r71396033",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/565",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71396033"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/565#discussion_r71396033"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/565"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71401133",
        "pull_request_review_id": null,
        "id": 71401133,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcxNDAxMTMz",
        "diff_hunk": "@@ -0,0 +1,73 @@\n+#!/usr/bin/env python\n+# db-query-tsj -- Runs SQL against PostgreSQL and formats output rows in TSJ\n+#\n+# $ db-query-tsj SQL COLUMN_TYPE...\n+##\n+import psycopg2, psycopg2.extras, ujson\n+import sys, subprocess, os, re\n+\n+# get arguments from command-line\n+def usage(*msg):\n+    err = subprocess.call([\"usage\", sys.argv[0]] + list(msg))\n+    if err != 0: sys.exit(err)\n+if not len(sys.argv) > 1: usage(\"Missing SQL\")\n+sql = sys.argv[1]\n+if not len(sys.argv) > 2: usage(\"Missing COLUMN_TYPEs\")\n+column_types = sys.argv[2:]\n+\n+# write TSJ to stdout\n+tsj_output = sys.stdout\n+\n+\n+# SQL types that are already formatted in JSON (to bypass ujson.dump)\n+TYPES_TO_PASS_THRU = [ \"json\" ]\n+\n+# PostgreSQL date/time types adapted by psycopg that need special formatting\n+# See: http://initd.org/psycopg/docs/usage.html#adapt-date\n+# See: https://www.postgresql.org/docs/current/static/datatype-datetime.html\n+TYPES_DATETIME_PATTERNS = [re.compile(regex) for regex in (\n+        r'date',\n+        r'interval(?:|.+)',\n+        r'timestamp\\s*(?:\\(\\d\\))?(?:\\s*with(?:out)?\\s+time\\s+zone)?',\n+        )]\n+\n+# generate code to output columns of each row based on their types\n+# See: http://lucumr.pocoo.org/2011/2/1/exec-in-python/\n+stmts = []\n+for i,ty in enumerate(column_types):\n+    ty = ty.lower()\n+    if i > 0: # separate each column by TAB\n+        stmts.append('tsj_output.write(\"\\\\t\")')\n+    if ty in TYPES_TO_PASS_THRU:\n+        # passthru JSON compatible columns\n+        stmts.append('tsj_output.write(columns[%d])' % i)\n+    elif any(pattern.match(ty) for pattern in TYPES_DATETIME_PATTERNS):\n+        # ujson needs us to format datetime\n+        stmts.append('ujson.dump(columns[%d].isoformat(), tsj_output)' % i)\n+    else:  # dump in JSON except a few\n+        stmts.append('ujson.dump(columns[%d], tsj_output)' % i)\n+stmts.append('tsj_output.write(\"\\\\n\")')\n+exec compile(\"def write_tsj_output(columns):\\n\" + \"\\n\".join(\"  \" + s for s in stmts),\n+        \"%s codegen\" % sys.argv[0], \"exec\")\n+\n+# tell psycopg2 to actually skip parsing any JSON\n+psycopg2.extras.register_default_json(loads=lambda x: x)\n+# XXX or use ujson.loads if you really must parse it\n+#psycopg2.extras.register_default_json(loads=ujson.loads)\n+\n+# use pyscopg2 to access the database\n+conn = psycopg2.connect(",
        "path": "database/db-driver/postgresql/db-query-tsj",
        "position": 59,
        "original_position": 59,
        "commit_id": "958864644304e408eb36289c102b086f4bd9a866",
        "original_commit_id": "40bc5a4e25e53050d4e1e8efb4155ed42c8ec8f9",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Thanks for the pointers. I was referring to `entry_points`.  Maybe I'll have to keep the script with the comments for `usage` and a bit of sys.argv mapping for consistency within DD.  I don't think having a main that does little is very important for the library.\n",
        "created_at": "2016-07-19T18:55:22Z",
        "updated_at": "2016-11-03T05:31:52Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/565#discussion_r71401133",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/565",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71401133"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/565#discussion_r71401133"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/565"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71406317",
        "pull_request_review_id": null,
        "id": 71406317,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcxNDA2MzE3",
        "diff_hunk": "@@ -0,0 +1,73 @@\n+#!/usr/bin/env python\n+# db-query-tsj -- Runs SQL against PostgreSQL and formats output rows in TSJ\n+#\n+# $ db-query-tsj SQL COLUMN_TYPE...\n+##\n+import psycopg2, psycopg2.extras, ujson\n+import sys, subprocess, os, re\n+\n+# get arguments from command-line\n+def usage(*msg):\n+    err = subprocess.call([\"usage\", sys.argv[0]] + list(msg))\n+    if err != 0: sys.exit(err)\n+if not len(sys.argv) > 1: usage(\"Missing SQL\")\n+sql = sys.argv[1]\n+if not len(sys.argv) > 2: usage(\"Missing COLUMN_TYPEs\")\n+column_types = sys.argv[2:]\n+\n+# write TSJ to stdout\n+tsj_output = sys.stdout\n+\n+\n+# SQL types that are already formatted in JSON (to bypass ujson.dump)\n+TYPES_TO_PASS_THRU = [ \"json\" ]\n+\n+# PostgreSQL date/time types adapted by psycopg that need special formatting\n+# See: http://initd.org/psycopg/docs/usage.html#adapt-date\n+# See: https://www.postgresql.org/docs/current/static/datatype-datetime.html\n+TYPES_DATETIME_PATTERNS = [re.compile(regex) for regex in (\n+        r'date',\n+        r'interval(?:|.+)',\n+        r'timestamp\\s*(?:\\(\\d\\))?(?:\\s*with(?:out)?\\s+time\\s+zone)?',\n+        )]\n+\n+# generate code to output columns of each row based on their types\n+# See: http://lucumr.pocoo.org/2011/2/1/exec-in-python/\n+stmts = []\n+for i,ty in enumerate(column_types):\n+    ty = ty.lower()\n+    if i > 0: # separate each column by TAB\n+        stmts.append('tsj_output.write(\"\\\\t\")')\n+    if ty in TYPES_TO_PASS_THRU:\n+        # passthru JSON compatible columns\n+        stmts.append('tsj_output.write(columns[%d])' % i)\n+    elif any(pattern.match(ty) for pattern in TYPES_DATETIME_PATTERNS):\n+        # ujson needs us to format datetime\n+        stmts.append('ujson.dump(columns[%d].isoformat(), tsj_output)' % i)\n+    else:  # dump in JSON except a few\n+        stmts.append('ujson.dump(columns[%d], tsj_output)' % i)\n+stmts.append('tsj_output.write(\"\\\\n\")')\n+exec compile(\"def write_tsj_output(columns):\\n\" + \"\\n\".join(\"  \" + s for s in stmts),\n+        \"%s codegen\" % sys.argv[0], \"exec\")\n+\n+# tell psycopg2 to actually skip parsing any JSON\n+psycopg2.extras.register_default_json(loads=lambda x: x)\n+# XXX or use ujson.loads if you really must parse it\n+#psycopg2.extras.register_default_json(loads=ujson.loads)\n+\n+# use pyscopg2 to access the database\n+conn = psycopg2.connect(",
        "path": "database/db-driver/postgresql/db-query-tsj",
        "position": 59,
        "original_position": 59,
        "commit_id": "958864644304e408eb36289c102b086f4bd9a866",
        "original_commit_id": "40bc5a4e25e53050d4e1e8efb4155ed42c8ec8f9",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "On second thought I think we should defer this to when we have at least two more concrete use cases.  While attempting, I realized not many Python code will want to generate TSJ files directly from database as it can just delegate to this executable, and to make any smaller unit useful by itself, we have to generalize and add significant bloat to this highly specialized code.  I can't easily figure a way to expose meaningful knobs but still keep it performant.  Currently the SLOC < 50, but I think making it testable, reusable at finer granularity will just make it 2x or even longer, making it ironically less maintainable.  The executable unit is already tested fairly extensively from `deepdive_sql.bats`, and I don't think we need tests for every few lines of the code.  Maybe you can take a stab later when some other code needs to output TSJ after executing a SQL query.\n",
        "created_at": "2016-07-19T19:25:39Z",
        "updated_at": "2016-11-03T05:31:52Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/565#discussion_r71406317",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/565",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71406317"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/565#discussion_r71406317"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/565"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71408030",
        "pull_request_review_id": null,
        "id": 71408030,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcxNDA4MDMw",
        "diff_hunk": "@@ -371,3 +379,45 @@ def tsv_extractor(generator):\n   for row in parser.parse_stdin():\n     for out_row in generator(**row._asdict()):\n       printer.write(out_row)\n+\n+\n+def tsj_extractor(generator):\n+  \"\"\"\n+  When a generator function is decorated with this (i.e., @tsj_extractor\n+  preceding the def line), each standard input line is parsed as\n+  tab-separated JSON (TSJ) values, then the function is applied to the parsed\n+  array to generate output rows, and each output row expected to be an array\n+  is formatted as TSJ.\n+  \"\"\"\n+  reload(sys).setdefaultencoding(\"utf8\")  # to avoid UnicodeEncodeError of JSON values during conversion by str()\n+\n+  input_format, output_format = get_generator_format(generator)\n+  input_names  = [name for name,t in input_format]\n+  num_input_values = len(input_format)\n+  num_input_splits = num_input_values - 1\n+  num_output_values = len(output_format)\n+\n+  def parse_json(column_index, json_value):",
        "path": "ddlib/ddlib/util.py",
        "position": 79,
        "original_position": 79,
        "commit_id": "958864644304e408eb36289c102b086f4bd9a866",
        "original_commit_id": "40bc5a4e25e53050d4e1e8efb4155ed42c8ec8f9",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "You mean `parse_json`?  It's just a `json.loads` with context-dependent error message translation.. that I doubt would be reusable or worth testing.  It's just to make the for loop body look cleaner with list comprehension syntax.  If the nested `def`s disturbs you in general, I can inline the code and make the following for loop longer.  I also hate `def`s but Python's dichotomy btwn statements and expressions forces you to use them.\n",
        "created_at": "2016-07-19T19:36:05Z",
        "updated_at": "2016-11-03T05:31:52Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/565#discussion_r71408030",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/565",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71408030"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/565#discussion_r71408030"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/565"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71417790",
        "pull_request_review_id": null,
        "id": 71417790,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcxNDE3Nzkw",
        "diff_hunk": "@@ -0,0 +1,73 @@\n+#!/usr/bin/env python\n+# db-query-tsj -- Runs SQL against PostgreSQL and formats output rows in TSJ\n+#\n+# $ db-query-tsj SQL COLUMN_TYPE...\n+##\n+import psycopg2, psycopg2.extras, ujson\n+import sys, subprocess, os, re\n+\n+# get arguments from command-line\n+def usage(*msg):\n+    err = subprocess.call([\"usage\", sys.argv[0]] + list(msg))\n+    if err != 0: sys.exit(err)\n+if not len(sys.argv) > 1: usage(\"Missing SQL\")\n+sql = sys.argv[1]\n+if not len(sys.argv) > 2: usage(\"Missing COLUMN_TYPEs\")\n+column_types = sys.argv[2:]\n+\n+# write TSJ to stdout\n+tsj_output = sys.stdout\n+\n+\n+# SQL types that are already formatted in JSON (to bypass ujson.dump)\n+TYPES_TO_PASS_THRU = [ \"json\" ]\n+\n+# PostgreSQL date/time types adapted by psycopg that need special formatting\n+# See: http://initd.org/psycopg/docs/usage.html#adapt-date\n+# See: https://www.postgresql.org/docs/current/static/datatype-datetime.html\n+TYPES_DATETIME_PATTERNS = [re.compile(regex) for regex in (\n+        r'date',\n+        r'interval(?:|.+)',\n+        r'timestamp\\s*(?:\\(\\d\\))?(?:\\s*with(?:out)?\\s+time\\s+zone)?',\n+        )]\n+\n+# generate code to output columns of each row based on their types\n+# See: http://lucumr.pocoo.org/2011/2/1/exec-in-python/\n+stmts = []\n+for i,ty in enumerate(column_types):\n+    ty = ty.lower()\n+    if i > 0: # separate each column by TAB\n+        stmts.append('tsj_output.write(\"\\\\t\")')\n+    if ty in TYPES_TO_PASS_THRU:\n+        # passthru JSON compatible columns\n+        stmts.append('tsj_output.write(columns[%d])' % i)\n+    elif any(pattern.match(ty) for pattern in TYPES_DATETIME_PATTERNS):\n+        # ujson needs us to format datetime\n+        stmts.append('ujson.dump(columns[%d].isoformat(), tsj_output)' % i)\n+    else:  # dump in JSON except a few\n+        stmts.append('ujson.dump(columns[%d], tsj_output)' % i)\n+stmts.append('tsj_output.write(\"\\\\n\")')\n+exec compile(\"def write_tsj_output(columns):\\n\" + \"\\n\".join(\"  \" + s for s in stmts),\n+        \"%s codegen\" % sys.argv[0], \"exec\")\n+\n+# tell psycopg2 to actually skip parsing any JSON\n+psycopg2.extras.register_default_json(loads=lambda x: x)\n+# XXX or use ujson.loads if you really must parse it\n+#psycopg2.extras.register_default_json(loads=ujson.loads)\n+\n+# use pyscopg2 to access the database\n+conn = psycopg2.connect(\n+        # NOTE these environment variables are supposed to set by db-parse\n+        database=os.environ.get(\"DBNAME\"),\n+        password=os.environ.get(\"DBPASSWORD\"),\n+        user=os.environ.get(\"DBUSER\"),\n+        host=os.environ.get(\"DBHOST\"),\n+        port=os.environ.get(\"DBPORT\"),",
        "path": "database/db-driver/postgresql/db-query-tsj",
        "position": 65,
        "original_position": 65,
        "commit_id": "958864644304e408eb36289c102b086f4bd9a866",
        "original_commit_id": "40bc5a4e25e53050d4e1e8efb4155ed42c8ec8f9",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "We currently use the DBOPTIONS env var (`search_path=abc,public`) to achieve specify which schema DD should use. That's not a good design but has been used in many places. Can we make this pick that up?\n",
        "created_at": "2016-07-19T20:34:26Z",
        "updated_at": "2016-11-03T05:31:52Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/565#discussion_r71417790",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/565",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71417790"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/565#discussion_r71417790"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/565"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71418808",
        "pull_request_review_id": null,
        "id": 71418808,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcxNDE4ODA4",
        "diff_hunk": "@@ -0,0 +1,73 @@\n+#!/usr/bin/env python\n+# db-query-tsj -- Runs SQL against PostgreSQL and formats output rows in TSJ\n+#\n+# $ db-query-tsj SQL COLUMN_TYPE...\n+##\n+import psycopg2, psycopg2.extras, ujson\n+import sys, subprocess, os, re\n+\n+# get arguments from command-line\n+def usage(*msg):\n+    err = subprocess.call([\"usage\", sys.argv[0]] + list(msg))\n+    if err != 0: sys.exit(err)\n+if not len(sys.argv) > 1: usage(\"Missing SQL\")\n+sql = sys.argv[1]\n+if not len(sys.argv) > 2: usage(\"Missing COLUMN_TYPEs\")\n+column_types = sys.argv[2:]\n+\n+# write TSJ to stdout\n+tsj_output = sys.stdout\n+\n+\n+# SQL types that are already formatted in JSON (to bypass ujson.dump)\n+TYPES_TO_PASS_THRU = [ \"json\" ]\n+\n+# PostgreSQL date/time types adapted by psycopg that need special formatting\n+# See: http://initd.org/psycopg/docs/usage.html#adapt-date\n+# See: https://www.postgresql.org/docs/current/static/datatype-datetime.html\n+TYPES_DATETIME_PATTERNS = [re.compile(regex) for regex in (\n+        r'date',\n+        r'interval(?:|.+)',\n+        r'timestamp\\s*(?:\\(\\d\\))?(?:\\s*with(?:out)?\\s+time\\s+zone)?',\n+        )]\n+\n+# generate code to output columns of each row based on their types\n+# See: http://lucumr.pocoo.org/2011/2/1/exec-in-python/\n+stmts = []\n+for i,ty in enumerate(column_types):\n+    ty = ty.lower()\n+    if i > 0: # separate each column by TAB\n+        stmts.append('tsj_output.write(\"\\\\t\")')\n+    if ty in TYPES_TO_PASS_THRU:\n+        # passthru JSON compatible columns\n+        stmts.append('tsj_output.write(columns[%d])' % i)\n+    elif any(pattern.match(ty) for pattern in TYPES_DATETIME_PATTERNS):\n+        # ujson needs us to format datetime\n+        stmts.append('ujson.dump(columns[%d].isoformat(), tsj_output)' % i)\n+    else:  # dump in JSON except a few\n+        stmts.append('ujson.dump(columns[%d], tsj_output)' % i)\n+stmts.append('tsj_output.write(\"\\\\n\")')\n+exec compile(\"def write_tsj_output(columns):\\n\" + \"\\n\".join(\"  \" + s for s in stmts),\n+        \"%s codegen\" % sys.argv[0], \"exec\")\n+\n+# tell psycopg2 to actually skip parsing any JSON\n+psycopg2.extras.register_default_json(loads=lambda x: x)\n+# XXX or use ujson.loads if you really must parse it\n+#psycopg2.extras.register_default_json(loads=ujson.loads)\n+\n+# use pyscopg2 to access the database\n+conn = psycopg2.connect(\n+        # NOTE these environment variables are supposed to set by db-parse\n+        database=os.environ.get(\"DBNAME\"),\n+        password=os.environ.get(\"DBPASSWORD\"),\n+        user=os.environ.get(\"DBUSER\"),\n+        host=os.environ.get(\"DBHOST\"),\n+        port=os.environ.get(\"DBPORT\"),",
        "path": "database/db-driver/postgresql/db-query-tsj",
        "position": 65,
        "original_position": 65,
        "commit_id": "958864644304e408eb36289c102b086f4bd9a866",
        "original_commit_id": "40bc5a4e25e53050d4e1e8efb4155ed42c8ec8f9",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Sure. Is it `PGOPTIONS` or `DBOPTIONS`?  I think the underlying libpq is supposed to pick up a bunch of env vars and psycopg2 also claims that but it didn't work, hence these lines.\n",
        "created_at": "2016-07-19T20:40:13Z",
        "updated_at": "2016-11-03T05:31:52Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/565#discussion_r71418808",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/565",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71418808"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/565#discussion_r71418808"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/565"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71419179",
        "pull_request_review_id": null,
        "id": 71419179,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcxNDE5MTc5",
        "diff_hunk": "@@ -0,0 +1,73 @@\n+#!/usr/bin/env python\n+# db-query-tsj -- Runs SQL against PostgreSQL and formats output rows in TSJ\n+#\n+# $ db-query-tsj SQL COLUMN_TYPE...\n+##\n+import psycopg2, psycopg2.extras, ujson\n+import sys, subprocess, os, re\n+\n+# get arguments from command-line\n+def usage(*msg):\n+    err = subprocess.call([\"usage\", sys.argv[0]] + list(msg))\n+    if err != 0: sys.exit(err)\n+if not len(sys.argv) > 1: usage(\"Missing SQL\")\n+sql = sys.argv[1]\n+if not len(sys.argv) > 2: usage(\"Missing COLUMN_TYPEs\")\n+column_types = sys.argv[2:]\n+\n+# write TSJ to stdout\n+tsj_output = sys.stdout\n+\n+\n+# SQL types that are already formatted in JSON (to bypass ujson.dump)\n+TYPES_TO_PASS_THRU = [ \"json\" ]\n+\n+# PostgreSQL date/time types adapted by psycopg that need special formatting\n+# See: http://initd.org/psycopg/docs/usage.html#adapt-date\n+# See: https://www.postgresql.org/docs/current/static/datatype-datetime.html\n+TYPES_DATETIME_PATTERNS = [re.compile(regex) for regex in (\n+        r'date',\n+        r'interval(?:|.+)',\n+        r'timestamp\\s*(?:\\(\\d\\))?(?:\\s*with(?:out)?\\s+time\\s+zone)?',\n+        )]\n+\n+# generate code to output columns of each row based on their types\n+# See: http://lucumr.pocoo.org/2011/2/1/exec-in-python/\n+stmts = []\n+for i,ty in enumerate(column_types):\n+    ty = ty.lower()\n+    if i > 0: # separate each column by TAB\n+        stmts.append('tsj_output.write(\"\\\\t\")')\n+    if ty in TYPES_TO_PASS_THRU:\n+        # passthru JSON compatible columns\n+        stmts.append('tsj_output.write(columns[%d])' % i)\n+    elif any(pattern.match(ty) for pattern in TYPES_DATETIME_PATTERNS):\n+        # ujson needs us to format datetime\n+        stmts.append('ujson.dump(columns[%d].isoformat(), tsj_output)' % i)\n+    else:  # dump in JSON except a few\n+        stmts.append('ujson.dump(columns[%d], tsj_output)' % i)\n+stmts.append('tsj_output.write(\"\\\\n\")')\n+exec compile(\"def write_tsj_output(columns):\\n\" + \"\\n\".join(\"  \" + s for s in stmts),\n+        \"%s codegen\" % sys.argv[0], \"exec\")\n+\n+# tell psycopg2 to actually skip parsing any JSON\n+psycopg2.extras.register_default_json(loads=lambda x: x)\n+# XXX or use ujson.loads if you really must parse it\n+#psycopg2.extras.register_default_json(loads=ujson.loads)\n+\n+# use pyscopg2 to access the database\n+conn = psycopg2.connect(\n+        # NOTE these environment variables are supposed to set by db-parse\n+        database=os.environ.get(\"DBNAME\"),\n+        password=os.environ.get(\"DBPASSWORD\"),\n+        user=os.environ.get(\"DBUSER\"),\n+        host=os.environ.get(\"DBHOST\"),\n+        port=os.environ.get(\"DBPORT\"),",
        "path": "database/db-driver/postgresql/db-query-tsj",
        "position": 65,
        "original_position": 65,
        "commit_id": "958864644304e408eb36289c102b086f4bd9a866",
        "original_commit_id": "40bc5a4e25e53050d4e1e8efb4155ed42c8ec8f9",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Sorry, it's `PGOPTIONS`...\n",
        "created_at": "2016-07-19T20:42:22Z",
        "updated_at": "2016-11-03T05:31:52Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/565#discussion_r71419179",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/565",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/71419179"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/565#discussion_r71419179"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/565"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/94291599",
        "pull_request_review_id": 14819738,
        "id": 94291599,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDk0MjkxNTk5",
        "diff_hunk": "@@ -56,6 +56,7 @@ case $format in\n             }\n             trap cleanup EXIT\n \n+            gpfdist --help &>/dev/null ||  { echo 'gpfdist not working! make it work!'; exit 1; }",
        "path": "database/db-driver/greenplum/db-unload",
        "position": 4,
        "original_position": 4,
        "commit_id": "2ea7fc435e2bb78849859f8df825bc2f47c52324",
        "original_commit_id": "2ea7fc435e2bb78849859f8df825bc2f47c52324",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "`error` command can be used in these cases.",
        "created_at": "2017-01-02T02:01:34Z",
        "updated_at": "2017-01-02T02:02:00Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/615#discussion_r94291599",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/615",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/94291599"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/615#discussion_r94291599"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/615"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/95068247",
        "pull_request_review_id": 15614450,
        "id": 95068247,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDk1MDY4MjQ3",
        "diff_hunk": "@@ -191,8 +191,8 @@ if .deepdive_.execution.processes | has(\"process/grounding/combine_factorgraph\")\n             for pid in $DEEPDIVE_FACTORGRAPH_SHARDS; do\n                 cd \\\"$DEEPDIVE_RESULTS_DIR\\\"/$pid\n                 cat inference_result.out.text |\n-                # restoring shard ID to the vids\n-                awk -v SHARD_BASE=$(($pid << 48)) '\n+                # restoring shard ID to the vids. Note that awk --bignum flag is required for precision\n+                awk --bignum -v SHARD_BASE=$(($pid << 48)) '",
        "path": "compiler/compile-config/compile-config-2.02-learning_inference",
        "position": null,
        "original_position": 7,
        "commit_id": "63ea31a5f0e9c4dd2316086be1179256cc71af58",
        "original_commit_id": "e91ee573859b109a2ccd73e2f7d9397819bce991",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "This flag seems to be something relatively new?  I can't find it from my Mac or Linux's gawk, mawk, or BSD awk. AWK has too many forks and versions out there, so maybe we should use something simpler.  I'll find a better solution and fill it in here.  Thanks for suggesting the fix and hope this one works for you for now.",
        "created_at": "2017-01-07T23:48:37Z",
        "updated_at": "2017-01-10T11:48:39Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/617#discussion_r95068247",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/617",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/95068247"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/617#discussion_r95068247"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/617"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/95931038",
        "pull_request_review_id": 16502478,
        "id": 95931038,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDk1OTMxMDM4",
        "diff_hunk": "@@ -197,10 +197,11 @@ if .deepdive_.execution.processes | has(\"process/grounding/combine_factorgraph\")\n                 cd \\\"$DEEPDIVE_RESULTS_DIR\\\"/$pid\n                 cat inference_result.out.text |\n                 # restoring shard ID to the vids\n-                jq -R -r --argjson SHARD_BASE $(($pid << 48)) '\n-                    split(\\\" \\\") |\n-                    [ (.[0] | tonumber + $SHARD_BASE | tostring)\n-                    , .[1:][] ] | join(\\\"\\\\t\\\")' |\n+                python3 -c \"",
        "path": "compiler/compile-config/compile-config-2.02-learning_inference",
        "position": null,
        "original_position": 8,
        "commit_id": "d1fe52aabe3dc392b099f63c62f6d0ef2f512db5",
        "original_commit_id": "ec70d363ca927ba97b8c9ad3613fb1f45eb672f0",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I think you need to escape the double quote (with backslash). Travis is complaining about jq compilation.\r\n\r\nAlso, @netj should we just use `python` here -- which should be compatible with both 2 and 3.\r\n\r\n@zifeishan let's do some perf test for this?",
        "created_at": "2017-01-13T03:08:36Z",
        "updated_at": "2017-01-13T07:45:52Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/621#discussion_r95931038",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/621",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/95931038"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/621#discussion_r95931038"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/621"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/95947433",
        "pull_request_review_id": 16518415,
        "id": 95947433,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDk1OTQ3NDMz",
        "diff_hunk": "@@ -197,10 +197,11 @@ if .deepdive_.execution.processes | has(\"process/grounding/combine_factorgraph\")\n                 cd \\\"$DEEPDIVE_RESULTS_DIR\\\"/$pid\n                 cat inference_result.out.text |\n                 # restoring shard ID to the vids\n-                jq -R -r --argjson SHARD_BASE $(($pid << 48)) '\n-                    split(\\\" \\\") |\n-                    [ (.[0] | tonumber + $SHARD_BASE | tostring)\n-                    , .[1:][] ] | join(\\\"\\\\t\\\")' |\n+                python3 -c \"",
        "path": "compiler/compile-config/compile-config-2.02-learning_inference",
        "position": null,
        "original_position": 8,
        "commit_id": "d1fe52aabe3dc392b099f63c62f6d0ef2f512db5",
        "original_commit_id": "ec70d363ca927ba97b8c9ad3613fb1f45eb672f0",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "I couldn't resist doing a [benchmark notebook](https://gist.github.com/netj/20e00e1719684ce08f38edff519f575a)\r\nand the winner is.... Perl!\r\n\r\njq was actually the slowest, ~7x slower than Perl and 3x than AWK.  Python is ~1.7x, AWK is ~2.2x.\r\njq and AWK uses little memory ~10MB, ~1/3 of Python (35-45MB), but so does Perl (14MB).\r\nSo Perl is a clear winner.",
        "created_at": "2017-01-13T07:19:10Z",
        "updated_at": "2017-01-13T07:45:52Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/621#discussion_r95947433",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/621",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/95947433"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/621#discussion_r95947433"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/621"
            }
        },
        "in_reply_to_id": 95931038
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/95948718",
        "pull_request_review_id": 16519701,
        "id": 95948718,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDk1OTQ4NzE4",
        "diff_hunk": "@@ -197,10 +197,11 @@ if .deepdive_.execution.processes | has(\"process/grounding/combine_factorgraph\")\n                 cd \\\"$DEEPDIVE_RESULTS_DIR\\\"/$pid\n                 cat inference_result.out.text |\n                 # restoring shard ID to the vids\n-                jq -R -r --argjson SHARD_BASE $(($pid << 48)) '\n-                    split(\\\" \\\") |\n-                    [ (.[0] | tonumber + $SHARD_BASE | tostring)\n-                    , .[1:][] ] | join(\\\"\\\\t\\\")' |\n+                python3 -c \"",
        "path": "compiler/compile-config/compile-config-2.02-learning_inference",
        "position": null,
        "original_position": 8,
        "commit_id": "d1fe52aabe3dc392b099f63c62f6d0ef2f512db5",
        "original_commit_id": "ec70d363ca927ba97b8c9ad3613fb1f45eb672f0",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "If perl can handle all 2^64 numbers and that's not subject to version differences on different OSes, we can go with perl.",
        "created_at": "2017-01-13T07:34:22Z",
        "updated_at": "2017-01-13T07:45:52Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/621#discussion_r95948718",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/621",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/95948718"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/621#discussion_r95948718"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/621"
            }
        },
        "in_reply_to_id": 95931038
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/96128079",
        "pull_request_review_id": 16705644,
        "id": 96128079,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDk2MTI4MDc5",
        "diff_hunk": "@@ -6,15 +6,15 @@\n # parallel connections, which MAY RESULT IN INCORRECT RESULT UNLESS QUERY IS\n # TOTALLY ORDERED BY COLUMNS WITH NO DUPLICATE VALUES!\n #\n-# GPFDIST_PORT_BASE (defaults to 10000) is the base port number for the gpfdist\n+# GPFDIST_PORT_BASE (defaults to 12000) is the base port number for the gpfdist\n # processes.\n # GPFDIST_MAX_LENGTH (defaults to 1MiB) may need to be increased for larger\n # records to be unloaded through gpfdist.\n # GPFDIST_LOG_PATH can be set to a path prefix for logging.\n ##\n set -eu\n \n-: ${GPFDIST_DISABLE:=} ${GPFDIST_PORT_BASE:=10000} ${GPFDIST_MAX_LENGTH:=$((2**20))} ${GPFDIST_LOG_PATH:=}\n+: ${GPFDIST_DISABLE:=} ${GPFDIST_PORT_BASE:=12000} ${GPFDIST_MAX_LENGTH:=$((2**20))} ${GPFDIST_LOG_PATH:=}",
        "path": "database/db-driver/greenplum/db-unload",
        "position": 14,
        "original_position": 14,
        "commit_id": "d6a9d070249c73f6393e5056fb64bc9054c13bd9",
        "original_commit_id": "d6a9d070249c73f6393e5056fb64bc9054c13bd9",
        "user": {
            "login": "netj",
            "id": 53634,
            "node_id": "MDQ6VXNlcjUzNjM0",
            "avatar_url": "https://avatars2.githubusercontent.com/u/53634?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netj",
            "html_url": "https://github.com/netj",
            "followers_url": "https://api.github.com/users/netj/followers",
            "following_url": "https://api.github.com/users/netj/following{/other_user}",
            "gists_url": "https://api.github.com/users/netj/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/netj/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/netj/subscriptions",
            "organizations_url": "https://api.github.com/users/netj/orgs",
            "repos_url": "https://api.github.com/users/netj/repos",
            "events_url": "https://api.github.com/users/netj/events{/privacy}",
            "received_events_url": "https://api.github.com/users/netj/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "While we're at this let's also randomize the base?\r\n`$(( 100*($RANDOM % 500 + 100) ))` which would give us 10000 through 59900",
        "created_at": "2017-01-15T01:24:26Z",
        "updated_at": "2017-01-15T01:26:18Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/622#discussion_r96128079",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/622",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/96128079"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/622#discussion_r96128079"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/622"
            }
        }
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/96128137",
        "pull_request_review_id": 16705692,
        "id": 96128137,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDk2MTI4MTM3",
        "diff_hunk": "@@ -6,15 +6,15 @@\n # parallel connections, which MAY RESULT IN INCORRECT RESULT UNLESS QUERY IS\n # TOTALLY ORDERED BY COLUMNS WITH NO DUPLICATE VALUES!\n #\n-# GPFDIST_PORT_BASE (defaults to 10000) is the base port number for the gpfdist\n+# GPFDIST_PORT_BASE (defaults to 12000) is the base port number for the gpfdist\n # processes.\n # GPFDIST_MAX_LENGTH (defaults to 1MiB) may need to be increased for larger\n # records to be unloaded through gpfdist.\n # GPFDIST_LOG_PATH can be set to a path prefix for logging.\n ##\n set -eu\n \n-: ${GPFDIST_DISABLE:=} ${GPFDIST_PORT_BASE:=10000} ${GPFDIST_MAX_LENGTH:=$((2**20))} ${GPFDIST_LOG_PATH:=}\n+: ${GPFDIST_DISABLE:=} ${GPFDIST_PORT_BASE:=12000} ${GPFDIST_MAX_LENGTH:=$((2**20))} ${GPFDIST_LOG_PATH:=}",
        "path": "database/db-driver/greenplum/db-unload",
        "position": 14,
        "original_position": 14,
        "commit_id": "d6a9d070249c73f6393e5056fb64bc9054c13bd9",
        "original_commit_id": "d6a9d070249c73f6393e5056fb64bc9054c13bd9",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "We should make it pick ports based on availability. I moved it to 12000 for now because I used 10000 somewhere else...",
        "created_at": "2017-01-15T01:28:48Z",
        "updated_at": "2017-01-15T01:28:48Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/622#discussion_r96128137",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/622",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/96128137"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/622#discussion_r96128137"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/622"
            }
        },
        "in_reply_to_id": 96128079
    },
    {
        "url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/96128437",
        "pull_request_review_id": 16705978,
        "id": 96128437,
        "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDk2MTI4NDM3",
        "diff_hunk": "@@ -6,15 +6,15 @@\n # parallel connections, which MAY RESULT IN INCORRECT RESULT UNLESS QUERY IS\n # TOTALLY ORDERED BY COLUMNS WITH NO DUPLICATE VALUES!\n #\n-# GPFDIST_PORT_BASE (defaults to 10000) is the base port number for the gpfdist\n+# GPFDIST_PORT_BASE (defaults to 12000) is the base port number for the gpfdist\n # processes.\n # GPFDIST_MAX_LENGTH (defaults to 1MiB) may need to be increased for larger\n # records to be unloaded through gpfdist.\n # GPFDIST_LOG_PATH can be set to a path prefix for logging.\n ##\n set -eu\n \n-: ${GPFDIST_DISABLE:=} ${GPFDIST_PORT_BASE:=10000} ${GPFDIST_MAX_LENGTH:=$((2**20))} ${GPFDIST_LOG_PATH:=}\n+: ${GPFDIST_DISABLE:=} ${GPFDIST_PORT_BASE:=12000} ${GPFDIST_MAX_LENGTH:=$((2**20))} ${GPFDIST_LOG_PATH:=}",
        "path": "database/db-driver/greenplum/db-unload",
        "position": 14,
        "original_position": 14,
        "commit_id": "d6a9d070249c73f6393e5056fb64bc9054c13bd9",
        "original_commit_id": "d6a9d070249c73f6393e5056fb64bc9054c13bd9",
        "user": {
            "login": "alldefector",
            "id": 5607717,
            "node_id": "MDQ6VXNlcjU2MDc3MTc=",
            "avatar_url": "https://avatars0.githubusercontent.com/u/5607717?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/alldefector",
            "html_url": "https://github.com/alldefector",
            "followers_url": "https://api.github.com/users/alldefector/followers",
            "following_url": "https://api.github.com/users/alldefector/following{/other_user}",
            "gists_url": "https://api.github.com/users/alldefector/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/alldefector/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/alldefector/subscriptions",
            "organizations_url": "https://api.github.com/users/alldefector/orgs",
            "repos_url": "https://api.github.com/users/alldefector/repos",
            "events_url": "https://api.github.com/users/alldefector/events{/privacy}",
            "received_events_url": "https://api.github.com/users/alldefector/received_events",
            "type": "User",
            "site_admin": false
        },
        "body": "Closing for now. We can change that later.",
        "created_at": "2017-01-15T01:54:33Z",
        "updated_at": "2017-01-15T01:54:33Z",
        "html_url": "https://github.com/HazyResearch/deepdive/pull/622#discussion_r96128437",
        "pull_request_url": "https://api.github.com/repos/HazyResearch/deepdive/pulls/622",
        "author_association": "CONTRIBUTOR",
        "_links": {
            "self": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/comments/96128437"
            },
            "html": {
                "href": "https://github.com/HazyResearch/deepdive/pull/622#discussion_r96128437"
            },
            "pull_request": {
                "href": "https://api.github.com/repos/HazyResearch/deepdive/pulls/622"
            }
        },
        "in_reply_to_id": 96128079
    }
]